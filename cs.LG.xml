<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>GeneCIS&#22522;&#20934;&#27979;&#35797;&#34913;&#37327;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22522;&#20934;&#27979;&#35797;&#30340;&#34920;&#29616;&#19982;ImageNet&#30340;&#20934;&#30830;&#24615;&#24369;&#30456;&#20851;&#65292;&#20165;&#31616;&#21333;&#25193;&#23637;&#29616;&#26377;&#26041;&#27861;&#24182;&#19981;&#34892;&#12290;</title><link>http://arxiv.org/abs/2306.07969</link><description>&lt;p&gt;
GeneCIS&#65306;&#19968;&#31181;&#36890;&#29992;&#26465;&#20214;&#22270;&#20687;&#30456;&#20284;&#24230;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GeneCIS: A Benchmark for General Conditional Image Similarity. (arXiv:2306.07969v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07969
&lt;/p&gt;
&lt;p&gt;
GeneCIS&#22522;&#20934;&#27979;&#35797;&#34913;&#37327;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22522;&#20934;&#27979;&#35797;&#30340;&#34920;&#29616;&#19982;ImageNet&#30340;&#20934;&#30830;&#24615;&#24369;&#30456;&#20851;&#65292;&#20165;&#31616;&#21333;&#25193;&#23637;&#29616;&#26377;&#26041;&#27861;&#24182;&#19981;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#23384;&#22312;&#35768;&#22810;&#8220;&#30456;&#20284;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#32780;&#27169;&#22411;&#65288;&#22914;&#20154;&#31867;&#65289;&#24212;&#35813;&#33021;&#22815;&#21160;&#24577;&#22320;&#36866;&#24212;&#36825;&#20123;&#27010;&#24565;&#12290;&#36825;&#19982;&#22823;&#22810;&#25968;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65288;&#21463;&#30417;&#30563;&#25110;&#33258;&#30417;&#30563;&#65289;&#19981;&#21516;&#65292;&#23427;&#20204;&#23398;&#20064;&#19968;&#20010;&#22266;&#23450;&#30340;&#23884;&#20837;&#20989;&#25968;&#65292;&#22240;&#27492;&#38544;&#21547;&#22320;&#20551;&#23450;&#20102;&#21333;&#19968;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeneCIS&#65288;&#8220;&#21019;&#19990;&#32426;&#8221;&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#34913;&#37327;&#20102;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#33021;&#21147;&#12290;&#25193;&#23637;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21482;&#35774;&#35745;&#29992;&#20110;&#38646;&#26679;&#26412;&#35780;&#20272;&#65292;&#22240;&#27492;&#32771;&#34385;&#20102;&#24320;&#25918;&#38598;&#30340;&#30456;&#20284;&#24615;&#26465;&#20214;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24378;&#22823;&#30340;CLIP&#27169;&#22411;&#30340;&#22522;&#32447;&#22312;GeneCIS&#19978;&#36739;&#20026;&#22256;&#38590;&#65292;&#24182;&#19988;&#22522;&#20934;&#27979;&#35797;&#30340;&#34920;&#29616;&#20165;&#19982;ImageNet&#30340;&#20934;&#30830;&#24615;&#24369;&#30456;&#20851;&#65292;&#36825;&#34920;&#26126;&#31616;&#21333;&#22320;&#25193;&#23637;&#29616;&#26377;&#26041;&#27861;&#24182;&#19981;&#26159;&#26377;&#25104;&#26524;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
We argue that there are many notions of 'similarity' and that models, like humans, should be able to adapt to these dynamically. This contrasts with most representation learning methods, supervised or self-supervised, which learn a fixed embedding function and hence implicitly assume a single notion of similarity. For instance, models trained on ImageNet are biased towards object categories, while a user might prefer the model to focus on colors, textures or specific elements in the scene. In this paper, we propose the GeneCIS ('genesis') benchmark, which measures models' ability to adapt to a range of similarity conditions. Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions. We find that baselines from powerful CLIP models struggle on GeneCIS and that performance on the benchmark is only weakly correlated with ImageNet accuracy, suggesting that simply scaling existing methods is not fruitful. We furth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#31639;&#27861;&#8212;&#8212;GLoRA&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#36866;&#37197;&#22120;&#23618;&#21644;&#21487;&#25193;&#23637;&#30340;&#32467;&#26500;&#25628;&#32034;&#20855;&#26377;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#26356;&#39640;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#22312;&#21508;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07967</link><description>&lt;p&gt;
&#19968;&#36890;&#36866;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#36890;&#29992;LoRA&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning. (arXiv:2306.07967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#31639;&#27861;&#8212;&#8212;GLoRA&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#36866;&#37197;&#22120;&#23618;&#21644;&#21487;&#25193;&#23637;&#30340;&#32467;&#26500;&#25628;&#32034;&#20855;&#26377;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#26356;&#39640;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#22312;&#21508;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36890;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20219;&#21153;&#31639;&#27861;&#8212;&#8212;&#24191;&#20041;LoRA&#65288;GLoRA&#65289;&#12290;GLoRA &#20351;&#29992;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#26469;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#21644;&#35843;&#25972;&#20013;&#38388;&#28608;&#27963;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#28789;&#27963;&#24615;&#21644;&#36328;&#24322;&#26500;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;GLoRA &#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#12289;&#27169;&#22359;&#21270;&#30340;&#12289;&#23618;&#27425;&#30340;&#32467;&#26500;&#25628;&#32034;&#26469;&#24110;&#21161;&#26377;&#25928;&#30340;&#21442;&#25968;&#35843;&#25972;&#65292;&#23398;&#20064;&#27599;&#20010;&#23618;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#23398;&#20844;&#24335;&#36215;&#28304;&#65292;GLoRA &#20855;&#26377;&#24378;&#22823;&#30340;&#36801;&#31227;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#26435;&#37325;&#21644;&#28608;&#27963;&#29366;&#24577;&#19978;&#30340;&#38468;&#21152;&#32500;&#24230;&#26469;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#12289;&#19987;&#19994;&#21644;&#32467;&#26500;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;GLoRA &#30340;&#31934;&#24230;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#36798;&#21040;&#20102;&#20248;&#36234;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26500;&#37325;&#26032;&#35774;&#35745;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#36816;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adjusts to new tasks through additional dimensions on weights and activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks, achieving superior accuracy with fewer parameters and computations on various datasets. Furthermore, our structural re-par
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;nuPlan&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#65292;&#38024;&#23545;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#12290;&#35777;&#23454;&#20102;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#26368;&#32456;&#25552;&#20986;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.07962</link><description>&lt;p&gt;
&#19982;&#23398;&#20064;&#23548;&#21521;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#30340;&#35823;&#35299;&#21578;&#21035;
&lt;/p&gt;
&lt;p&gt;
Parting with Misconceptions about Learning-based Vehicle Motion Planning. (arXiv:2306.07962v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;nuPlan&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#65292;&#38024;&#23545;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#12290;&#35777;&#23454;&#20102;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#26368;&#32456;&#25552;&#20986;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
nuPlan&#30340;&#21457;&#24067;&#26631;&#24535;&#30528;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#26102;&#20195;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38656;&#35201;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#12290;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20219;&#21153;&#23384;&#22312;&#26681;&#26412;&#19978;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24212;&#35813;&#20998;&#21035;&#36827;&#34892;&#35299;&#20915;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#39046;&#22495;&#20869;&#38381;&#29615;&#35268;&#21010;&#30340;&#29616;&#29366;&#65292;&#25581;&#31034;&#20102;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#36873;&#25321;&#36890;&#36807;&#36710;&#36947;&#22270;&#25628;&#32034;&#31639;&#27861;&#30340;&#31616;&#21333;&#22522;&#20110;&#35268;&#21017;&#30340;&#20808;&#39564;&#39033;&#65288;&#20363;&#22914;&#20013;&#24515;&#32447;&#36873;&#25321;&#65289;&#30340;&#20215;&#20540;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#24320;&#29615;&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#20165;&#20351;&#29992;&#36825;&#20010;&#20013;&#24515;&#32447;&#20316;&#20026;&#22330;&#26223;&#19978;&#19979;&#25991;&#26102;&#65288;&#21363;&#24573;&#30053;&#25152;&#26377;&#26377;&#20851;&#22320;&#22270;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20449;&#24687;&#65289;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#32467;&#21512;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#22823;&#37327;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of nuPlan marks a new era in vehicle motion planning research, offering the first large-scale real-world dataset and evaluation schemes requiring both precise short-term planning and long-horizon ego-forecasting. Existing systems struggle to simultaneously meet both requirements. Indeed, we find that these tasks are fundamentally misaligned and should be addressed independently. We further assess the current state of closed-loop planning in the field, revealing the limitations of learning-based methods in complex real-world scenarios and the value of simple rule-based priors such as centerline selection through lane graph search algorithms. More surprisingly, for the open-loop sub-task, we observe that the best results are achieved when using only this centerline as scene context (\ie, ignoring all information regarding the map and other agents). Combining these insights, we propose an extremely simple and efficient planner which outperforms an extensive set of competitors,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#19981;&#20559;&#24494;&#20998;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#26080;&#20559;&#12289;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#26041;&#27861;&#23545;&#22797;&#26434;&#23494;&#24230;&#36827;&#34892;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545; MH &#37319;&#26679;&#22120;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.07961</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#20559;&#24494;&#20998;&#23545;&#25239;&#22797;&#26434;&#23494;&#24230;&#29983;&#25104;&#65292;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#19981;&#20559;&#24494;&#20998;&#20248;&#21270; MH &#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentiating Metropolis-Hastings to Optimize Intractable Densities. (arXiv:2306.07961v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#19981;&#20559;&#24494;&#20998;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#26080;&#20559;&#12289;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#26041;&#27861;&#23545;&#22797;&#26434;&#23494;&#24230;&#36827;&#34892;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545; MH &#37319;&#26679;&#22120;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27010;&#29575;&#27169;&#22411;&#25512;&#29702;&#20013;&#65292;&#30446;&#26631;&#23494;&#24230;&#20989;&#25968;&#36890;&#24120;&#21464;&#24471;&#38590;&#20197;&#35745;&#31639;&#65292;&#38656;&#35201;&#20351;&#29992; Monte Carlo &#35745;&#31639;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#20559;&#24494;&#20998; Metropolis-Hastings &#37319;&#26679;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#27010;&#29575;&#25512;&#29702;&#26469;&#36827;&#34892;&#24494;&#20998;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#24494;&#20998;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982; Markov &#38142;&#32806;&#21512;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#20559;&#65292;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#31243;&#24207;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#24212;&#29992;&#20110;&#30001;&#20110;&#32321;&#29712;&#30340;&#30446;&#26631;&#23494;&#24230;&#23548;&#33268;&#26399;&#26395;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#25214;&#21040;&#19968;&#20010;&#27169;&#26865;&#20004;&#21487;&#30340;&#35266;&#23519;&#21644;&#22312; Ising &#27169;&#22411;&#20013;&#26368;&#22823;&#21270;&#27604;&#28909;&#26469;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When performing inference on probabilistic models, target densities often become intractable, necessitating the use of Monte Carlo samplers. We develop a methodology for unbiased differentiation of the Metropolis-Hastings sampler, allowing us to differentiate through probabilistic inference. By fusing recent advances in stochastic differentiation with Markov chain coupling schemes, the procedure can be made unbiased, low-variance, and automatic. This allows us to apply gradient-based optimization to objectives expressed as expectations over intractable target densities. We demonstrate our approach by finding an ambiguous observation in a Gaussian mixture model and by maximizing the specific heat in an Ising model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#23450;&#21046;&#26412;&#22320;&#27169;&#22411;&#20197;&#21516;&#26102;&#28385;&#36275;&#24322;&#26500;&#35774;&#32622;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#25968;&#25454;&#38544;&#31169;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#20808;&#39564;&#20998;&#24067;&#26469;&#20419;&#36827;&#24322;&#26500;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.07959</link><description>&lt;p&gt;
&#24322;&#26500;&#35774;&#32622;&#19979;&#30340;&#38544;&#31169;&#20445;&#25252;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy Preserving Bayesian Federated Learning in Heterogeneous Settings. (arXiv:2306.07959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#23450;&#21046;&#26412;&#22320;&#27169;&#22411;&#20197;&#21516;&#26102;&#28385;&#36275;&#24322;&#26500;&#35774;&#32622;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#25968;&#25454;&#38544;&#31169;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#20808;&#39564;&#20998;&#24067;&#26469;&#20419;&#36827;&#24322;&#26500;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#39640;&#24230;&#24322;&#26500;&#65292;&#22240;&#27492;&#24378;&#21046;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#26045;&#21152;&#30456;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#38750;&#24120;&#21463;&#38480;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25317;&#26377;&#21463;&#38480;&#26412;&#22320;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#65292;&#38656;&#35201;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#25968;&#25454;&#38544;&#31169;&#32422;&#26463;&#30340;&#38656;&#27714;&#36890;&#24120;&#29305;&#21035;&#22686;&#24378;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;FL&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#32422;&#26463;&#21644;&#20851;&#27880;&#28857;&#65292;&#22522;&#20110;&#35757;&#32451;&#23450;&#21046;&#26412;&#22320;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#22823;&#37327;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#24456;&#22909;&#22320;&#23398;&#20064;&#12290;&#36125;&#21494;&#26031;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#26469;&#20197;&#20808;&#39564;&#20998;&#24067;&#30340;&#24418;&#24335;&#34701;&#20837;&#30417;&#30563;&#12290;&#25105;&#20204;&#22312;&#32593;&#32476;&#30340;&#21151;&#33021;&#65288;&#36755;&#20986;&#65289;&#31354;&#38388;&#20013;&#20351;&#29992;&#20808;&#39564;&#26469;&#20419;&#36827;&#24322;&#26500;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;&#27492;&#22806;&#65292;&#26412;&#26694;&#26550;&#36824;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;&#22312;&#26631;&#20934;FL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In several practical applications of federated learning (FL), the clients are highly heterogeneous in terms of both their data and compute resources, and therefore enforcing the same model architecture for each client is very limiting. Moreover, the need for uncertainty quantification and data privacy constraints are often particularly amplified for clients that have limited local data. This paper presents a unified FL framework to simultaneously address all these constraints and concerns, based on training customized local Bayesian models that learn well even in the absence of large local datasets. A Bayesian framework provides a natural way of incorporating supervision in the form of prior distributions. We use priors in the functional (output) space of the networks to facilitate collaboration across heterogeneous clients. Moreover, formal differential privacy guarantees are provided for this framework. Experiments on standard FL datasets demonstrate that our approach outperforms str
&lt;/p&gt;</description></item><item><title>&#31471;&#21040;&#31471;&#39550;&#39542;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#27425;&#35201;&#32452;&#20214;&#30340;&#26356;&#25913;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#25152;&#23384;&#22312;&#30340;&#20004;&#31181;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;TF ++&#65292;&#22312;CARLA&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.07957</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39550;&#39542;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hidden Biases of End-to-End Driving Models. (arXiv:2306.07957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07957
&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#39550;&#39542;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#27425;&#35201;&#32452;&#20214;&#30340;&#26356;&#25913;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#25152;&#23384;&#22312;&#30340;&#20004;&#31181;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;TF ++&#65292;&#22312;CARLA&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31471;&#21040;&#31471;&#30340;&#39550;&#39542;&#31995;&#32479;&#22312;CARLA&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#20027;&#35201;&#36129;&#29486;&#30340;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#31995;&#32479;&#20063;&#20250;&#24341;&#20837;&#23545;&#27425;&#35201;&#31995;&#32479;&#32452;&#20214;&#30340;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#30340;&#25913;&#36827;&#28304;&#24182;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#37117;&#23384;&#22312;&#20004;&#31181;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#23545;&#20110;&#22312;CARLA&#19978;&#35266;&#23519;&#21040;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#65306;(1) &#36890;&#36807;&#23545;&#30446;&#26631;&#28857;&#36319;&#38543;&#30340;&#24378;&#24402;&#32435;&#20559;&#35265;&#26469;&#36827;&#34892;&#27178;&#21521;&#24674;&#22797;&#65292;(2) &#36890;&#36807;&#22810;&#27169;&#24577;&#33322;&#36335;&#28857;&#39044;&#27979;&#30340;&#32437;&#21521;&#24179;&#22343;&#26469;&#20943;&#36895;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#20559;&#35265;&#30340;&#32570;&#28857;&#65292;&#24182;&#30830;&#23450;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TF ++&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#22312;Longest6&#21644;LAV&#22522;&#20934;&#27979;&#35797;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#22312;Longest6&#19978;&#27604;&#26368;&#20339;&#21069;&#26399;&#24037;&#20316;&#25552;&#39640;&#20102;14&#20010;&#39550;&#39542;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end driving systems have recently made rapid progress, in particular on CARLA. Independent of their major contribution, they introduce changes to minor system components. Consequently, the source of improvements is unclear. We identify two biases that recur in nearly all state-of-the-art methods and are critical for the observed progress on CARLA: (1) lateral recovery via a strong inductive bias towards target point following, and (2) longitudinal averaging of multimodal waypoint predictions for slowing down. We investigate the drawbacks of these biases and identify principled alternatives. By incorporating our insights, we develop TF++, a simple end-to-end method that ranks first on the Longest6 and LAV benchmarks, gaining 14 driving score over the best prior work on Longest6.
&lt;/p&gt;</description></item><item><title>MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07952</link><description>&lt;p&gt;
MOFI: &#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07952
&lt;/p&gt;
&lt;p&gt;
MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411; MOFI&#65292;&#26088;&#22312;&#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#12290;MOFI &#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#26377;&#20004;&#28857;&#19981;&#21516;&#65306;&#65288;i&#65289;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#65288;ii&#65289;&#35757;&#32451;&#37197;&#26041;&#12290;&#22312;&#25968;&#25454;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#20174; alt-text &#20013;&#25552;&#21462;&#23454;&#20307;&#65292;&#28982;&#21518;&#20351;&#29992; CLIP &#27169;&#22411;&#36873;&#25321;&#27491;&#30830;&#30340;&#23454;&#20307;&#20316;&#20026;&#22270;&#20687;&#30340;&#26631;&#31614;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#26131;&#34892;&#65292;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20174; web &#19978;&#25366;&#25496;&#30340;&#25968;&#21313;&#20159;&#20010;&#22270;&#20687;&#25991;&#26412;&#23545;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102; Image-to-Entities&#65288;I2E&#65289;&#36825;&#19968;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 10 &#20159;&#24352;&#22270;&#20687;&#21644; 200 &#19975;&#20010;&#19981;&#21516;&#30340;&#23454;&#20307;&#65292;&#28085;&#30422;&#20102;&#37326;&#22806;&#20016;&#23500;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#22522;&#20110; I2E &#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12289;&#23545;&#27604;&#24230;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;&#65288;CTC&#65289;&#25439;&#22833;&#20013;&#24341;&#20837;&#26631;&#31614;&#20808;&#39564;&#65292;&#24182;&#23558;&#20302;&#23618;Mel-scale&#28388;&#27874;&#22120;&#21644;&#39640;&#23618;ASR&#32534;&#30721;&#22120;&#36755;&#20986;&#32452;&#21512;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#26469;&#25913;&#36827;E2E&#31995;&#32479;&#20013;&#29992;&#20110;&#35789;&#26102;&#30340;&#24103;&#32423;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#35789;&#26102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07949</link><description>&lt;p&gt;
&#38750;&#23792;&#20540;CTC&#25552;&#21319;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35789;&#26102;&#20998;&#31867;&#22120;&#30340;&#24103;&#32423;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improving Frame-level Classifier for Word Timings with Non-peaky CTC in End-to-End Automatic Speech Recognition. (arXiv:2306.07949v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;&#65288;CTC&#65289;&#25439;&#22833;&#20013;&#24341;&#20837;&#26631;&#31614;&#20808;&#39564;&#65292;&#24182;&#23558;&#20302;&#23618;Mel-scale&#28388;&#27874;&#22120;&#21644;&#39640;&#23618;ASR&#32534;&#30721;&#22120;&#36755;&#20986;&#32452;&#21512;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#26469;&#25913;&#36827;E2E&#31995;&#32479;&#20013;&#29992;&#20110;&#35789;&#26102;&#30340;&#24103;&#32423;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#35789;&#26102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#31995;&#32479;&#24050;&#32463;&#26174;&#31034;&#20986;&#19982;&#28151;&#21512;&#31995;&#32479;&#30456;&#24403;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24615;&#33021;&#12290;&#20316;&#20026;ASR&#30340;&#21103;&#20135;&#21697;&#65292;&#35789;&#30340;&#23450;&#26102;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23383;&#24149;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#21457;&#38899;&#35757;&#32451;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;&#65288;CTC&#65289;&#25439;&#22833;&#20013;&#24341;&#20837;&#26631;&#31614;&#20808;&#39564;&#65292;&#24182;&#23558;&#20302;&#23618;Mel-scale&#28388;&#27874;&#22120;&#21644;&#39640;&#23618;ASR&#32534;&#30721;&#22120;&#36755;&#20986;&#32452;&#21512;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;E2E&#31995;&#32479;&#20013;&#29992;&#20110;&#35789;&#26102;&#30340;&#24103;&#32423;&#20998;&#31867;&#22120;&#12290;&#22312;&#20869;&#37096;&#27721;&#35821;&#35821;&#26009;&#24211;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#28151;&#21512;&#31995;&#32479;&#22312;&#21333;&#35789;&#26102;&#24207;&#20934;&#30830;&#24615;&#24230;&#37327;&#19978;&#23454;&#29616;&#20102; 95.68% / 94.18% &#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;7&#31181;&#35821;&#35328;&#30340;&#25351;&#26631;&#19978;&#32477;&#23545;&#25552;&#39640;&#20102;4.80% / 8.02% &#30340;&#20998;&#25968;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;E2E&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24310;&#36831;CTC&#23792;&#20540;&#21644;&#22522;&#20110;&#24103;&#30340;&#30693;&#35782;&#33976;&#39311;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35789;&#26102;&#20934;&#30830;&#24615;&#65292;&#20294;&#20165;&#22312;LibriSpeech&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end (E2E) systems have shown comparable performance to hybrid systems for automatic speech recognition (ASR). Word timings, as a by-product of ASR, are essential in many applications, especially for subtitling and computer-aided pronunciation training. In this paper, we improve the frame-level classifier for word timings in E2E system by introducing label priors in connectionist temporal classification (CTC) loss, which is adopted from prior works, and combining low-level Mel-scale filter banks with high-level ASR encoder output as input feature. On the internal Chinese corpus, the proposed method achieves 95.68%/94.18% compared to the hybrid system 93.0%/90.22% on the word timing accuracy metrics. It also surpass a previous E2E approach with an absolute increase of 4.80%/8.02% on the metrics on 7 languages. In addition, we further improve word timing accuracy by delaying CTC peaks with frame-wise knowledge distillation, though only experimenting on LibriSpeech.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24565;&#20256;&#25773;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#19978;&#19979;&#25991;&#38543;&#26426;&#22359;&#27169;&#22411;(cSBM)&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36825;&#31181;&#31639;&#27861;&#36798;&#21040;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#24314;&#31435;&#24615;&#33021;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.07948</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Optimal Inference in Contextual Stochastic Block Models. (arXiv:2306.07948v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24565;&#20256;&#25773;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#19978;&#19979;&#25991;&#38543;&#26426;&#22359;&#27169;&#22411;(cSBM)&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36825;&#31181;&#31639;&#27861;&#36798;&#21040;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#24314;&#31435;&#24615;&#33021;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#38543;&#26426;&#22359;&#27169;&#22411;(cSBM)&#34987;&#25552;&#20986;&#26469;&#23545;&#20855;&#26377;&#33410;&#28857;&#26631;&#31614;&#30456;&#20851;&#24615;&#30340;&#23646;&#24615;&#22270;&#36827;&#34892;&#26080;&#30417;&#30563;&#31038;&#21306;&#26816;&#27979;&#65292;&#20854;&#20013;&#22270;&#24418;&#21644;&#39640;&#32500;&#33410;&#28857;&#20449;&#24687;&#37117;&#19982;&#33410;&#28857;&#26631;&#31614;&#30456;&#20851;&#12290;&#22312;&#22270;&#19978;&#26426;&#22120;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;cSBM&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The contextual stochastic block model (cSBM) was proposed for unsupervised community detection on attributed graphs where both the graph and the high-dimensional node information correlate with node labels. In the context of machine learning on graphs, the cSBM has been widely used as a synthetic dataset for evaluating the performance of graph-neural networks (GNNs) for semi-supervised node classification. We consider a probabilistic Bayes-optimal formulation of the inference problem and we derive a belief-propagation-based algorithm for the semi-supervised cSBM; we conjecture it is optimal in the considered setting and we provide its implementation. We show that there can be a considerable gap between the accuracy reached by this algorithm and the performance of the GNN architectures proposed in the literature. This suggests that the cSBM, along with the comparison to the performance of the optimal algorithm, readily accessible via our implementation, can be instrumental in the develo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GPT-Calls&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#23545;&#35805;&#65292;&#20197;&#25552;&#39640;&#30005;&#35805;&#24405;&#38899;&#30340;&#20998;&#21106;&#21644;&#26631;&#35760;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#31163;&#32447;&#21644;&#22312;&#32447;&#20004;&#20010;&#38454;&#27573;&#65292;&#31163;&#32447;&#38454;&#27573;&#20165;&#24212;&#29992;&#19968;&#27425;&#65292;&#23545;&#32473;&#23450;&#30340;&#20027;&#39064;&#21015;&#34920;&#29983;&#25104;&#21512;&#25104;&#21477;&#23376;&#20998;&#24067;&#24182;&#25552;&#21462;&#38170;&#21521;&#37327;&#65292;&#22312;&#32447;&#38454;&#27573;&#21017;&#38024;&#23545;&#27599;&#20010;&#36890;&#35805;&#21333;&#29420;&#24212;&#29992;&#65292;&#24182;&#23545;&#36716;&#24405;&#30340;&#23545;&#35805;&#19982;&#31163;&#32447;&#38454;&#27573;&#25214;&#21040;&#30340;&#20027;&#39064;&#38170;&#23450;&#36827;&#34892;&#30456;&#20284;&#24615;&#35780;&#20998;&#12290;&#26102;&#38388;&#22495;&#20998;&#26512;&#34987;&#24212;&#29992;&#20110;&#30456;&#20284;&#24615;&#24471;&#20998;&#20013;&#65292;&#20197;&#23558;&#35805;&#35821;&#20998;&#32452;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#25152;&#23646;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.07941</link><description>&lt;p&gt;
GPT-Calls: &#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#23545;&#35805;&#25552;&#21319;&#36890;&#35805;&#20998;&#21106;&#21644;&#26631;&#35760;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GPT-Calls: Enhancing Call Segmentation and Tagging by Generating Synthetic Conversations via Large Language Models. (arXiv:2306.07941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GPT-Calls&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#23545;&#35805;&#65292;&#20197;&#25552;&#39640;&#30005;&#35805;&#24405;&#38899;&#30340;&#20998;&#21106;&#21644;&#26631;&#35760;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#31163;&#32447;&#21644;&#22312;&#32447;&#20004;&#20010;&#38454;&#27573;&#65292;&#31163;&#32447;&#38454;&#27573;&#20165;&#24212;&#29992;&#19968;&#27425;&#65292;&#23545;&#32473;&#23450;&#30340;&#20027;&#39064;&#21015;&#34920;&#29983;&#25104;&#21512;&#25104;&#21477;&#23376;&#20998;&#24067;&#24182;&#25552;&#21462;&#38170;&#21521;&#37327;&#65292;&#22312;&#32447;&#38454;&#27573;&#21017;&#38024;&#23545;&#27599;&#20010;&#36890;&#35805;&#21333;&#29420;&#24212;&#29992;&#65292;&#24182;&#23545;&#36716;&#24405;&#30340;&#23545;&#35805;&#19982;&#31163;&#32447;&#38454;&#27573;&#25214;&#21040;&#30340;&#20027;&#39064;&#38170;&#23450;&#36827;&#34892;&#30456;&#20284;&#24615;&#35780;&#20998;&#12290;&#26102;&#38388;&#22495;&#20998;&#26512;&#34987;&#24212;&#29992;&#20110;&#30456;&#20284;&#24615;&#24471;&#20998;&#20013;&#65292;&#20197;&#23558;&#35805;&#35821;&#20998;&#32452;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#25152;&#23646;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#35805;&#24405;&#38899;&#30340;&#36716;&#24405;&#22312;&#38144;&#21806;&#12289;&#23458;&#25143;&#26381;&#21153;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#25191;&#27861;&#31561;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#36825;&#20123;&#24405;&#38899;&#23545;&#35805;&#21487;&#20197;&#26159;&#19968;&#39033;&#33392;&#24040;&#32780;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#24403;&#22788;&#29702;&#38271;&#26102;&#38388;&#25110;&#22810;&#26041;&#38754;&#30340;&#23545;&#35805;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;GPT-distilled Calls Segmentation and Tagging (GPT-Calls)&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#36890;&#35805;&#20998;&#27573;&#21644;&#20027;&#39064;&#25552;&#21462;&#12290;GPT-Calls&#30001;&#31163;&#32447;&#21644;&#22312;&#32447;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#20854;&#20013;&#31163;&#32447;&#38454;&#27573;&#38024;&#23545;&#32473;&#23450;&#30340;&#20027;&#39064;&#21015;&#34920;&#20165;&#24212;&#29992;&#19968;&#27425;&#65292;&#20351;&#29992;GPT&#27169;&#22411;&#29983;&#25104;&#27599;&#20010;&#20027;&#39064;&#30340;&#21512;&#25104;&#21477;&#23376;&#20998;&#24067;&#24182;&#25552;&#21462;&#38170;&#21521;&#37327;&#12290;&#22312;&#32447;&#38454;&#27573;&#38024;&#23545;&#27599;&#20010;&#36890;&#35805;&#21333;&#29420;&#24212;&#29992;&#65292;&#24182;&#23545;&#36716;&#24405;&#30340;&#23545;&#35805;&#19982;&#31163;&#32447;&#38454;&#27573;&#25214;&#21040;&#30340;&#20027;&#39064;&#38170;&#23450;&#36827;&#34892;&#30456;&#20284;&#24615;&#35780;&#20998;&#12290;&#28982;&#21518;&#65292;&#26102;&#38388;&#22495;&#20998;&#26512;&#34987;&#24212;&#29992;&#20110;&#30456;&#20284;&#24615;&#24471;&#20998;&#20013;&#65292;&#23558;&#35805;&#35821;&#20998;&#32452;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Transcriptions of phone calls are of significant value across diverse fields, such as sales, customer service, healthcare, and law enforcement. Nevertheless, the analysis of these recorded conversations can be an arduous and time-intensive process, especially when dealing with extended or multifaceted dialogues. In this work, we propose a novel method, GPT-distilled Calls Segmentation and Tagging (GPT-Calls), for efficient and accurate call segmentation and topic extraction. GPT-Calls is composed of offline and online phases. The offline phase is applied once to a given list of topics and involves generating a distribution of synthetic sentences for each topic using a GPT model and extracting anchor vectors. The online phase is applied to every call separately and scores the similarity between the transcripted conversation and the topic anchors found in the offline phase. Then, time domain analysis is applied to the similarity scores to group utterances into segments and tag them with 
&lt;/p&gt;</description></item><item><title>DDmix&#27169;&#22411;&#26159;&#19968;&#20010;&#21487;&#20197;&#20174;&#37096;&#20998;&#25110;&#32858;&#21512;&#30340;&#26102;&#38388;&#20449;&#24687;&#20013;&#37325;&#26500;&#22312;&#32593;&#32476;&#19978;&#28436;&#21464;&#30340;&#27969;&#34892;&#30149;&#30340;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#20811;&#26381;&#27169;&#22411;&#24847;&#35782;&#30340;&#32570;&#20047;&#65292;&#24182;&#22312;&#36890;&#29992;&#24615;&#19978;&#24471;&#21040;&#20102;&#20984;&#26174;&#12290;</title><link>http://arxiv.org/abs/2306.07938</link><description>&lt;p&gt;
&#28145;&#24230;&#28151;&#21512;&#65306;&#37325;&#24314;&#32593;&#32476;&#27969;&#34892;&#30149;&#30340;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Demixing: Reconstructing the Evolution of Network Epidemics. (arXiv:2306.07938v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07938
&lt;/p&gt;
&lt;p&gt;
DDmix&#27169;&#22411;&#26159;&#19968;&#20010;&#21487;&#20197;&#20174;&#37096;&#20998;&#25110;&#32858;&#21512;&#30340;&#26102;&#38388;&#20449;&#24687;&#20013;&#37325;&#26500;&#22312;&#32593;&#32476;&#19978;&#28436;&#21464;&#30340;&#27969;&#34892;&#30149;&#30340;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#20811;&#26381;&#27169;&#22411;&#24847;&#35782;&#30340;&#32570;&#20047;&#65292;&#24182;&#22312;&#36890;&#29992;&#24615;&#19978;&#24471;&#21040;&#20102;&#20984;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#28151;&#21512;&#65288;DDmix&#65289;&#27169;&#22411;&#30340;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20174;&#37096;&#20998;&#25110;&#32858;&#21512;&#30340;&#26102;&#38388;&#20449;&#24687;&#20013;&#37325;&#26500;&#22312;&#32593;&#32476;&#19978;&#28436;&#21464;&#30340;&#27969;&#34892;&#30149;&#12290;&#20551;&#35774;&#24050;&#30693;&#32593;&#32476;&#25299;&#25169;&#32780;&#19981;&#30693;&#36947;&#27969;&#34892;&#30149;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#30142;&#30149;&#20256;&#25773;&#30340;&#23436;&#25972;&#36335;&#24452;&#12290;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#20811;&#26381;&#27169;&#22411;&#24847;&#35782;&#30340;&#32570;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#21453;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;DDmix&#20316;&#20026;&#22270;&#24418;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#20174;&#36807;&#21435;&#30340;&#27969;&#34892;&#30149;&#34067;&#24310;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;DDmix&#35797;&#22270;&#22312;&#20854;&#28508;&#22312;&#31354;&#38388;&#20013;&#25429;&#25417;&#22522;&#26412;&#30340;&#65288;&#26410;&#30693;&#65289;&#20256;&#25773;&#21160;&#24577;&#26041;&#38754;&#12290;&#36890;&#36807;&#20351;&#29992;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#27169;&#25311;&#30340;&#27969;&#34892;&#30149;&#20256;&#25773;&#30340;&#32593;&#32476;&#65292;&#23558;DDmix&#30340;&#31934;&#24230;&#19982;&#22810;&#31181;&#65288;&#38750;&#22270;&#24418;&#24863;&#30693;&#30340;&#65289;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;DDmix&#30340;&#36890;&#29992;&#24615;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#20984;&#26174;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#25193;&#23637;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#36229;&#32423;&#20256;&#25773;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the deep demixing (DDmix) model, a graph autoencoder that can reconstruct epidemics evolving over networks from partial or aggregated temporal information. Assuming knowledge of the network topology but not of the epidemic model, our goal is to estimate the complete propagation path of a disease spread. A data-driven approach is leveraged to overcome the lack of model awareness. To solve this inverse problem, DDmix is proposed as a graph conditional variational autoencoder that is trained from past epidemic spreads. DDmix seeks to capture key aspects of the underlying (unknown) spreading dynamics in its latent space. Using epidemic spreads simulated in synthetic and real-world networks, we demonstrate the accuracy of DDmix by comparing it with multiple (non-graph-aware) learning algorithms. The generalizability of DDmix is highlighted across different types of networks. Finally, we showcase that a simple post-processing extension of our proposed method can help identify supe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;MRLF&#65289;&#65292;&#29992;&#20110;&#23558;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#30340;&#19981;&#21516;&#27169;&#24577;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#20301;&#32622;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#22686;&#24378;&#20301;&#32622;&#26174;&#33879;&#20449;&#24687;&#25552;&#21462;&#65292;&#30456;&#23545;&#20110;&#21333;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20301;&#32622;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07935</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#20301;&#32622;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Representation Learning for Social Post Location Inference. (arXiv:2306.07935v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;MRLF&#65289;&#65292;&#29992;&#20110;&#23558;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#30340;&#19981;&#21516;&#27169;&#24577;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#20301;&#32622;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#22686;&#24378;&#20301;&#32622;&#26174;&#33879;&#20449;&#24687;&#25552;&#21462;&#65292;&#30456;&#23545;&#20110;&#21333;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20301;&#32622;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#26469;&#25512;&#26029;&#22320;&#29702;&#20301;&#32622;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#30340;&#22522;&#20110;&#20301;&#32622;&#30340;&#24212;&#29992;&#31243;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#20135;&#21697;&#33829;&#38144;&#12289;&#20852;&#36259;&#28857;&#25512;&#33616;&#21450;COVID-19&#30340;&#36861;&#36394;&#12290;&#26412;&#30740;&#31350;&#20174;Instagram&#25910;&#38598;&#20102;&#24102;&#26377;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#26631;&#31614;&#30340;&#23454;&#38469;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;MRLF&#65289;&#65292;&#23427;&#33021;&#22815;&#23558;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#30340;&#19981;&#21516;&#27169;&#24577;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#20301;&#32622;&#25512;&#26029;&#12290;MRLF&#38598;&#25104;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#22686;&#24378;&#20301;&#32622;&#26174;&#33879;&#20449;&#24687;&#25552;&#21462;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#21333;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20301;&#32622;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20869;&#23481;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#23383;&#31526;&#24863;&#30693;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring geographic locations via social posts is essential for many practical location-based applications such as product marketing, point-of-interest recommendation, and infector tracking for COVID-19. Unlike image-based location retrieval or social-post text embedding-based location inference, the combined effect of multi-modal information (i.e., post images, text, and hashtags) for social post positioning receives less attention. In this work, we collect real datasets of social posts with images, texts, and hashtags from Instagram and propose a novel Multi-modal Representation Learning Framework (MRLF) capable of fusing different modalities of social posts for location inference. MRLF integrates a multi-head attention mechanism to enhance location-salient information extraction while significantly improving location inference compared with single domain-based methods. To overcome the noisy user-generated textual content, we introduce a novel attention-based character-aware module 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24102;&#26377;&#30683;&#30462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#25512;&#29702;&#20013;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25512;&#32763;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.07934</link><description>&lt;p&gt;
BoardgameQA: &#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#30683;&#30462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information. (arXiv:2306.07934v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24102;&#26377;&#30683;&#30462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#25512;&#29702;&#20013;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25512;&#32763;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#33258;&#21160;&#25512;&#29702;&#26159;&#35768;&#22810;&#28508;&#22312;NLP&#24212;&#29992;&#21644;&#24320;&#21457;&#24378;&#22823;AI&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#26412;&#25991;&#38024;&#23545;&#23384;&#22312;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#20449;&#24687;&#30340;&#25512;&#29702;&#38382;&#39064;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#31574;&#30053;&#65292;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#32463;&#20856;&#30340;&#21487;&#25512;&#32763;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#30740;&#31350;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#21508;&#31181;&#21487;&#23398;&#20064;&#24615;&#26465;&#20214;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#36793;&#30028;&#65292;&#24182;&#22312;&#21512;&#25104;&#35821;&#35328;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.07926</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#30340;&#19968;&#31181;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Unsupervised Speech Recognition. (arXiv:2306.07926v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#30740;&#31350;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#21508;&#31181;&#21487;&#23398;&#20064;&#24615;&#26465;&#20214;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#36793;&#30028;&#65292;&#24182;&#22312;&#21512;&#25104;&#35821;&#35328;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#38382;&#39064;&#25351;&#30340;&#26159;&#20174;&#26410;&#37197;&#23545;&#30340;&#21547;&#26377;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#12290;&#34429;&#28982;&#23384;&#22312;&#21508;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#30740;&#31350;&#20854;&#23646;&#24615;&#65292;&#24182;&#35299;&#20915;&#22914;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#30740;&#31350;ASR-U&#31995;&#32479;&#30340;&#23646;&#24615;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;ASR-U&#30340;&#21508;&#31181;&#21487;&#23398;&#20064;&#24615;&#26465;&#20214;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#36793;&#30028;&#12290;&#38024;&#23545;&#26377;&#19977;&#31867;&#36716;&#31227;&#22270;&#30340;&#21512;&#25104;&#35821;&#35328;&#30340;&#24191;&#27867;ASR-U&#23454;&#39564;&#20026;&#25105;&#20204;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#65288;&#20195;&#30721;&#21487;&#22312;cactuswiththoughts / UnsupASRTheory.git&#20013;&#33719;&#24471;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised speech recognition (ASR-U) is the problem of learning automatic speech recognition (ASR) systems from unpaired speech-only and text-only corpora. While various algorithms exist to solve this problem, a theoretical framework is missing from studying their properties and addressing such issues as sensitivity to hyperparameters and training instability. In this paper, we proposed a general theoretical framework to study the properties of ASR-U systems based on random matrix theory and the theory of neural tangent kernels. Such a framework allows us to prove various learnability conditions and sample complexity bounds of ASR-U. Extensive ASR-U experiments on synthetic languages with three classes of transition graphs provide strong empirical evidence for our theory (code available at cactuswiththoughts/UnsupASRTheory.git).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;Oracle&#26377;&#25928;&#30340;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.07923</link><description>&lt;p&gt;
&#38754;&#21521;Oracle&#30340;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#65306;&#31163;&#32447;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Oracle-Efficient Pessimism: Offline Policy Optimization in Contextual Bandits. (arXiv:2306.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;Oracle&#26377;&#25928;&#30340;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#32473;&#23450;&#19968;&#20010;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26085;&#24535;&#20132;&#20114;&#12290;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#24754;&#35266;&#24809;&#32602;&#26469;&#32531;&#35299;&#20998;&#24067;&#20559;&#31227;&#65292;&#20294;&#20808;&#21069;&#30340;&#23454;&#29616;&#24182;&#19981;&#35745;&#31639;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;Oracle&#26377;&#25928;&#30340;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65306;&#23427;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20063;&#24471;&#20986;&#20102;&#31867;&#20284;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#24754;&#35266;&#26041;&#27861;&#30340;&#26368;&#20339;&#32479;&#35745;&#20445;&#35777;&#12290;&#25105;&#20204;&#20026;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#37117;&#23454;&#20363;&#21270;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;&#22312;&#21508;&#31181;&#37197;&#32622;&#20013;&#37117;&#27604;&#26410;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#20248;&#21270;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider policy optimization in contextual bandits, where one is given a fixed dataset of logged interactions. While pessimistic regularizers are typically used to mitigate distribution shift, prior implementations thereof are not computationally efficient. We present the first oracle-efficient algorithm for pessimistic policy optimization: it reduces to supervised learning, leading to broad applicability. We also obtain best-effort statistical guarantees analogous to those for pessimistic approaches in prior work. We instantiate our approach for both discrete and continuous actions. We perform extensive experiments in both settings, showing advantage over unregularized policy optimization across a wide range of configurations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;{\method}&#8221;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#30340;&#24178;&#20928;&#28436;&#31034;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#21516;&#26102;&#21033;&#29992;&#21253;&#21547;&#22312;&#22823;&#37327;&#26377;&#22122;&#22768;&#28436;&#31034;&#20013;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#36890;&#36807;&#22312;&#23376;&#28436;&#31034;&#32423;&#21035;&#19978;&#36827;&#34892;&#35780;&#20272;&#21644;&#27169;&#20223;&#65292;&#23558;&#20855;&#26377;&#19981;&#21516;&#36136;&#37327;&#30340;&#25805;&#20316;&#21407;&#35821;&#32534;&#30721;&#20026;&#19981;&#21516;&#30340;&#25216;&#33021;&#26469;&#24110;&#21161;&#26426;&#22120;&#20998;&#31163;&#20986;&#23376;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07919</link><description>&lt;p&gt;
&#20174;&#27425;&#26368;&#20248;&#28436;&#31034;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#25216;&#33021;&#35299;&#32544;
&lt;/p&gt;
&lt;p&gt;
Skill Disentanglement for Imitation Learning from Suboptimal Demonstrations. (arXiv:2306.07919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;{\method}&#8221;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#30340;&#24178;&#20928;&#28436;&#31034;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#21516;&#26102;&#21033;&#29992;&#21253;&#21547;&#22312;&#22823;&#37327;&#26377;&#22122;&#22768;&#28436;&#31034;&#20013;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#36890;&#36807;&#22312;&#23376;&#28436;&#31034;&#32423;&#21035;&#19978;&#36827;&#34892;&#35780;&#20272;&#21644;&#27169;&#20223;&#65292;&#23558;&#20855;&#26377;&#19981;&#21516;&#36136;&#37327;&#30340;&#25805;&#20316;&#21407;&#35821;&#32534;&#30721;&#20026;&#19981;&#21516;&#30340;&#25216;&#33021;&#26469;&#24110;&#21161;&#26426;&#22120;&#20998;&#31163;&#20986;&#23376;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#31070;&#32463;&#20195;&#29702;&#36890;&#36807;&#27169;&#20223;&#25910;&#38598;&#30340;&#20154;&#31867;&#28436;&#31034;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#38590;&#20197;&#25910;&#38598;&#30340;&#39640;&#36136;&#37327;&#28436;&#31034;&#12290;&#36890;&#24120;&#65292;&#38656;&#35201;&#22312;&#28436;&#31034;&#36136;&#37327;&#21644;&#25968;&#37327;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#27425;&#20248;&#28436;&#31034;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#23567;&#30340;&#24178;&#20928;&#28436;&#31034;&#38598;&#21644;&#19968;&#20010;&#22823;&#30340;&#26377;&#22122;&#22768;&#38598;&#12290;&#19968;&#20123;&#20808;&#39537;&#24615;&#30340;&#24037;&#20316;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#65292;&#20363;&#22914;&#65292;&#20551;&#35774;&#28436;&#31034;&#22312;&#26102;&#38388;&#27493;&#39588;&#20013;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#26410;&#33021;&#25552;&#20379;&#20219;&#20309;&#20851;&#20110;&#20174;&#22122;&#22768;&#38598;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;{\method}&#8221;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#23376;&#28436;&#31034;&#32423;&#21035;&#19978;&#36827;&#34892;&#35780;&#20272;&#21644;&#27169;&#20223;&#65292;&#23558;&#20855;&#26377;&#19981;&#21516;&#36136;&#37327;&#30340;&#25805;&#20316;&#21407;&#35821;&#32534;&#30721;&#20026;&#19981;&#21516;&#30340;&#25216;&#33021;&#65292;&#24110;&#21161;&#26426;&#22120;&#20998;&#31163;&#20986;&#23376;&#25216;&#33021;&#65292;&#24471;&#30410;&#20110;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#26032;&#22411;&#35268;&#21017;&#21270;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;{\method}&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#23569;&#37327;&#30340;&#24178;&#20928;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#21033;&#29992;&#21253;&#21547;&#22312;&#22823;&#37327;&#26377;&#22122;&#22768;&#28436;&#31034;&#20013;&#30340;&#34917;&#20805;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning has achieved great success in many sequential decision-making tasks, in which a neural agent is learned by imitating collected human demonstrations. However, existing algorithms typically require a large number of high-quality demonstrations that are difficult and expensive to collect. Usually, a trade-off needs to be made between demonstration quality and quantity in practice. Targeting this problem, in this work we consider the imitation of sub-optimal demonstrations, with both a small clean demonstration set and a large noisy set. Some pioneering works have been proposed, but they suffer from many limitations, e.g., assuming a demonstration to be of the same optimality throughout time steps and failing to provide any interpretation w.r.t knowledge learned from the noisy set. Addressing these problems, we propose {\method} by evaluating and imitating at the sub-demonstration level, encoding action primitives of varying quality into different skills. Concretely, {\m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#21644;&#38388;&#25509;&#35266;&#27979;&#30340;&#20013;&#20171;&#22240;&#32032;&#30340;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.07918</link><description>&lt;p&gt;
&#22810;&#32500;&#21644;&#38388;&#25509;&#35266;&#27979;&#20013;&#20171;&#22240;&#32032;&#30340;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Mediation Analysis with Multi-dimensional and Indirectly Observed Mediators. (arXiv:2306.07918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#21644;&#38388;&#25509;&#35266;&#27979;&#30340;&#20013;&#20171;&#22240;&#32032;&#30340;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#20869;&#23558;&#22788;&#29702;&#30340;&#24635;&#25928;&#24212;&#20998;&#35299;&#20026;&#30452;&#25509;&#21644;&#20171;&#23548;&#25928;&#24212;&#12290;&#36825;&#22312;&#35768;&#22810;&#31185;&#23398;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#22788;&#29702;&#25928;&#24212;&#30340;&#28508;&#22312;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#24212;&#29992;&#20013;&#65292;&#20013;&#20171;&#22240;&#32032;&#26410;&#34987;&#35266;&#23519;&#21040;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#30456;&#20851;&#27979;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;CMA&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22522;&#20110;&#21487;&#36776;&#35782;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;iVAE&#65289;&#20307;&#31995;&#32467;&#26500;&#30340;&#22797;&#26434;&#21644;&#38388;&#25509;&#35266;&#27979;&#30340;&#20013;&#20171;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal mediation analysis (CMA) is a powerful method to dissect the total effect of a treatment into direct and mediated effects within the potential outcome framework. This is important in many scientific applications to identify the underlying mechanisms of a treatment effect. However, in many scientific applications the mediator is unobserved, but there may exist related measurements. For example, we may want to identify how changes in brain activity or structure mediate an antidepressant's effect on behavior, but we may only have access to electrophysiological or imaging brain measurements. To date, most CMA methods assume that the mediator is one-dimensional and observable, which oversimplifies such real-world scenarios. To overcome this limitation, we introduce a CMA framework that can handle complex and indirectly observed mediators based on the identifiable variational autoencoder (iVAE) architecture. We prove that the true joint distribution over observed and latent variables 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#30340;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#20013;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07916</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#27169;&#22411;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Identification of Nonlinear Latent Hierarchical Models. (arXiv:2306.07916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#30340;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#20013;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#28508;&#21464;&#37327;&#21644;&#22240;&#26524;&#32467;&#26500;&#23545;&#20110;&#35768;&#22810;&#28041;&#21450;&#29983;&#29289;&#25968;&#25454;&#12289;&#21307;&#23398;&#25968;&#25454;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#21644;&#35821;&#35328;&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#65292;&#24182;&#19988;&#20851;&#31995;&#26159;&#38750;&#32447;&#24615;&#30340;&#26102;&#65292;&#36825;&#39033;&#20219;&#21153;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#35266;&#27979;&#21464;&#37327;&#30001;&#19968;&#32452;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#65292;&#26377;&#20123;&#28508;&#21464;&#37327;&#21487;&#33021;&#27809;&#26377;&#35266;&#23519;&#21040;&#30340;&#21518;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#65306;&#23545;&#20110;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#20801;&#35768;&#22270;&#20013;&#20219;&#24847;&#20004;&#20010;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#22810;&#26465;&#36335;&#24452;&#65292;&#36825;&#25918;&#23485;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#28508;&#21464;&#37327;&#26641;&#20551;&#35774;&#65307;&#23545;&#20110;&#32467;&#26500;&#20989;&#25968;&#65292;&#25105;&#20204;&#27809;&#26377;&#36827;&#34892;&#21442;&#25968;&#20551;&#35774;&#65292;&#22240;&#27492;&#21487;&#20197;&#20801;&#35768;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of both causal structure and latent variables can be achieved under mild assumptions: on causal structures, we allow for the existence of multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we do not make parametric assumptions, thus permitting gene
&lt;/p&gt;</description></item><item><title>Omega&#26159;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#21382;&#21490;&#26799;&#24230;EMA&#26469;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#24182;&#22312;&#38543;&#26426;&#28216;&#25103;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.07905</link><description>&lt;p&gt;
Omega: &#20048;&#35266;EMA Gradients
&lt;/p&gt;
&lt;p&gt;
Omega: Optimistic EMA Gradients. (arXiv:2306.07905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07905
&lt;/p&gt;
&lt;p&gt;
Omega&#26159;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#21382;&#21490;&#26799;&#24230;EMA&#26469;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#24182;&#22312;&#38543;&#26426;&#28216;&#25103;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GAN&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#36827;&#27493;&#65292;&#38543;&#26426;min-max&#20248;&#21270;&#21463;&#21040;&#20102;&#26426;&#22120;&#23398;&#20064;&#30028;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#30830;&#23450;&#24615;&#29366;&#24577;&#19979;&#30340;&#21338;&#24328;&#20248;&#21270;&#24050;&#32463;&#30456;&#24403;&#22909;&#22320;&#29702;&#35299;&#20102;&#65292;&#20294;&#22312;&#38543;&#26426;&#29366;&#24577;&#19979;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;&#20048;&#35266;&#26799;&#24230;&#36825;&#26679;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#26041;&#27861;&#23545;&#22122;&#22768;&#38750;&#24120;&#25935;&#24863;&#25110;&#32773;&#20250;&#23548;&#33268;&#22833;&#36133;&#12290;&#34429;&#28982;&#23384;&#22312;&#26367;&#20195;&#31574;&#30053;&#65292;&#20294;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Omega&#65292;&#19968;&#31181;&#20855;&#26377;&#31867;&#20284;&#20110;&#20048;&#35266;&#26356;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20854;&#26356;&#26032;&#35268;&#21017;&#20013;&#21512;&#24182;&#21382;&#21490;&#26799;&#24230;&#30340;EMA&#26469;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#21253;&#21547;&#21160;&#37327;&#30340;&#35813;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#34429;&#28982;&#25105;&#20204;&#27809;&#26377;&#25552;&#20379;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#20294;&#25105;&#20204;&#22312;&#38543;&#26426;&#28216;&#25103;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#24212;&#29992;&#20110;&#32447;&#24615;&#29609;&#23478;&#26102;&#65292;Omega&#20248;&#20110;&#20048;&#35266;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic min-max optimization has gained interest in the machine learning community with the advancements in GANs and adversarial training. Although game optimization is fairly well understood in the deterministic setting, some issues persist in the stochastic regime. Recent work has shown that stochastic gradient descent-ascent methods such as the optimistic gradient are highly sensitive to noise or can fail to converge. Although alternative strategies exist, they can be prohibitively expensive. We introduce Omega, a method with optimistic-like updates that mitigates the impact of noise by incorporating an EMA of historic gradients in its update rule. We also explore a variation of this algorithm that incorporates momentum. Although we do not provide convergence guarantees, our experiments on stochastic games show that Omega outperforms the optimistic gradient method when applied to linear players.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#27969;&#24335;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20026;&#20219;&#20309;&#20855;&#26377;&#26102;&#38388;&#36328;&#24230;T&#12289;&#33218;&#25968;&#37327;K&#21644;&#32463;&#36807;&#27425;&#25968;B&#30340;&#31639;&#27861;&#24314;&#31435;&#20102;&#26368;&#32039;&#23494;&#30340;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#19979;&#30028;&#65292;&#21516;&#26102;&#20063;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#20381;&#36182;&#20110;&#20855;&#20307;&#23454;&#20363;&#30340;&#19979;&#30028;&#12290;&#19982;&#32463;&#20856;&#30340;&#38598;&#20013;&#24335;&#38543;&#26426;&#36172;&#21338;&#26426;&#38382;&#39064;&#30456;&#27604;&#36739;&#65292;&#27969;&#24335;&#35774;&#32622;&#19979;&#19981;&#21487;&#36991;&#20813;&#22320;&#26377;&#26356;&#22810;&#30340;&#21452;&#23545;&#25968;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2306.07903</link><description>&lt;p&gt;
&#27969;&#24335;&#36172;&#21338;&#26426;&#30340;&#32039;&#20945;&#20869;&#23384;-&#36951;&#25022;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Tight Memory-Regret Lower Bounds for Streaming Bandits. (arXiv:2306.07903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#27969;&#24335;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20026;&#20219;&#20309;&#20855;&#26377;&#26102;&#38388;&#36328;&#24230;T&#12289;&#33218;&#25968;&#37327;K&#21644;&#32463;&#36807;&#27425;&#25968;B&#30340;&#31639;&#27861;&#24314;&#31435;&#20102;&#26368;&#32039;&#23494;&#30340;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#19979;&#30028;&#65292;&#21516;&#26102;&#20063;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#20381;&#36182;&#20110;&#20855;&#20307;&#23454;&#20363;&#30340;&#19979;&#30028;&#12290;&#19982;&#32463;&#20856;&#30340;&#38598;&#20013;&#24335;&#38543;&#26426;&#36172;&#21338;&#26426;&#38382;&#39064;&#30456;&#27604;&#36739;&#65292;&#27969;&#24335;&#35774;&#32622;&#19979;&#19981;&#21487;&#36991;&#20813;&#22320;&#26377;&#26356;&#22810;&#30340;&#21452;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24335;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#26088;&#22312;&#36890;&#36807;&#22788;&#29702;&#22312;&#32447;&#21040;&#36798;&#30340;&#33218;&#21644;&#20122;&#32447;&#24615;&#33218;&#20869;&#23384;&#26469;&#26368;&#23567;&#21270;&#36951;&#25022;&#12290;&#25105;&#20204;&#20026;&#20219;&#20309;&#20855;&#26377;&#26102;&#38388;&#36328;&#24230;T&#12289;&#33218;&#25968;&#37327;K&#21644;&#32463;&#36807;&#27425;&#25968;B&#30340;&#31639;&#27861;&#24314;&#31435;&#20102;$\Omega \left( (TB)^{\alpha} K^{1-\alpha}\right), \alpha = 2^{B} / (2^{B+1}-1)$ &#32039;&#23494;&#30340;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#19979;&#30028;&#12290;&#32467;&#26524;&#25581;&#31034;&#20102;&#32463;&#20856;&#38598;&#20013;&#24335;&#29615;&#22659;&#19979;&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#38382;&#39064;&#21644;&#20855;&#26377;&#26377;&#30028;&#33218;&#35760;&#24518;&#27969;&#35774;&#32622;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340; $\Omega(\sqrt{KT})$ &#19979;&#30028;&#30456;&#27604;&#65292;&#22312;&#20801;&#35768;&#20122;&#32447;&#24615;&#20869;&#23384;&#30340;&#20219;&#20309;&#27969;&#24335;&#36172;&#21338;&#26426;&#31639;&#27861;&#20013;&#65292;&#37117;&#26080;&#27861;&#36991;&#20813;&#39069;&#22806;&#30340;&#21452;&#23545;&#25968;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#27969;&#24335;&#36172;&#21338;&#26426;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#20381;&#36182;&#20110;&#20855;&#20307;&#23454;&#20363;&#30340;&#19979;&#30028;$\Omega \left(T^{1/(B+1)} \sum_{\Delta_x&gt;0} \frac{\mu^*}{\Delta_x}\right)$&#12290;&#36825;&#20123;&#19979;&#30028;&#36890;&#36807;&#20174;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#21807;&#19968;&#32553;&#20943;&#20013;&#25512;&#23548;&#32780;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the streaming bandits problem, wherein the learner aims to minimize regret by dealing with online arriving arms and sublinear arm memory. We establish the tight worst-case regret lower bound of $\Omega \left( (TB)^{\alpha} K^{1-\alpha}\right), \alpha = 2^{B} / (2^{B+1}-1)$ for any algorithm with a time horizon $T$, number of arms $K$, and number of passes $B$. The result reveals a separation between the stochastic bandits problem in the classical centralized setting and the streaming setting with bounded arm memory. Notably, in comparison to the well-known $\Omega(\sqrt{KT})$ lower bound, an additional double logarithmic factor is unavoidable for any streaming bandits algorithm with sublinear memory permitted. Furthermore, we establish the first instance-dependent lower bound of $\Omega \left(T^{1/(B+1)} \sum_{\Delta_x&gt;0} \frac{\mu^*}{\Delta_x}\right)$ for streaming bandits. These lower bounds are derived through a unique reduction from the regret-minimiza
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23545;&#24191;&#27867;&#28608;&#27963;&#20989;&#25968;&#26063;&#20013;&#30340;&#31070;&#32463;&#20803;&#30340;&#26368;&#20248;$L_2^2$&#35823;&#24046;&#36827;&#34892;&#36817;&#20284;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07892</link><description>&lt;p&gt;
&#36890;&#36807;Sharpness&#24378;&#20581;&#22320;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Robustly Learning a Single Neuron via Sharpness. (arXiv:2306.07892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23545;&#24191;&#27867;&#28608;&#27963;&#20989;&#25968;&#26063;&#20013;&#30340;&#31070;&#32463;&#20803;&#30340;&#26368;&#20248;$L_2^2$&#35823;&#24046;&#36827;&#34892;&#36817;&#20284;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#20803;&#23545;$L_2^2$&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#23545;&#21253;&#25324;ReLU&#22312;&#20869;&#30340;&#24191;&#27867;&#28608;&#27963;&#20989;&#25968;&#26063;&#20013;&#65292;&#36817;&#20284;&#20110;&#26368;&#20248;$L_2^2$&#35823;&#24046;&#12290;&#30456;&#36739;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#28201;&#21644;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;&#20351;&#25105;&#20204;&#32467;&#26524;&#21487;&#34892;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#19982;&#20248;&#21270;&#29702;&#35770;&#30340;&#23616;&#37096;&#35823;&#24046;&#30028;&#30340;&#26032;&#39062;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning a single neuron with respect to the $L_2^2$-loss in the presence of adversarial label noise. We give an efficient algorithm that, for a broad family of activations including ReLUs, approximates the optimal $L_2^2$-error within a constant factor. Our algorithm applies under much milder distributional assumptions compared to prior work. The key ingredient enabling our results is a novel connection to local error bounds from optimization theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;VISION&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#20803;&#21270;&#38598;&#21512;&#65292;&#21253;&#21547;14&#20010;&#19981;&#21516;&#24037;&#19994;&#39046;&#22495;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#27880;&#37322;&#22696;&#27700;&#21644;&#23454;&#20363;&#20998;&#21106;&#27880;&#37322;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;VISION&#25968;&#25454;&#38598;&#21487;&#20197;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#24037;&#19994;&#39046;&#22495;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#26377;&#26395;&#20419;&#36827;&#22522;&#20110;&#35270;&#35273;&#30340;&#24037;&#19994;&#26816;&#27979;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.07890</link><description>&lt;p&gt;
VISION&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#24037;&#19994;&#26816;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
VISION Datasets: A Benchmark for Vision-based InduStrial InspectiON. (arXiv:2306.07890v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;VISION&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#20803;&#21270;&#38598;&#21512;&#65292;&#21253;&#21547;14&#20010;&#19981;&#21516;&#24037;&#19994;&#39046;&#22495;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#27880;&#37322;&#22696;&#27700;&#21644;&#23454;&#20363;&#20998;&#21106;&#27880;&#37322;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;VISION&#25968;&#25454;&#38598;&#21487;&#20197;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#24037;&#19994;&#39046;&#22495;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#26377;&#26395;&#20419;&#36827;&#22522;&#20110;&#35270;&#35273;&#30340;&#24037;&#19994;&#26816;&#27979;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#35270;&#35273;&#30340;&#26816;&#27979;&#31639;&#27861;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#23454;&#19990;&#30028;&#20013;&#24037;&#19994;&#25361;&#25112;&#8212;&#8212;&#29305;&#21035;&#26159;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#36136;&#37327;&#21644;&#22797;&#26434;&#30340;&#29983;&#20135;&#35201;&#27714;&#8212;&#8212;&#24448;&#24448;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;VISION&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;14&#20010;&#19981;&#21516;&#24037;&#19994;&#39046;&#22495;&#26816;&#27979;&#25968;&#25454;&#38598;&#30340;&#22810;&#20803;&#21270;&#38598;&#21512;&#65292;&#29420;&#20855;&#20248;&#21183;&#21487;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#19982;&#20197;&#24448;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;VISION&#20026;&#32570;&#38519;&#26816;&#27979;&#25552;&#20379;&#20016;&#23500;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#25152;&#26377;&#25968;&#25454;&#20998;&#31867;&#30340;&#27880;&#37322;&#25513;&#27169;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#23454;&#20363;&#20998;&#21106;&#27880;&#37322;&#21151;&#33021;&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#32570;&#38519;&#12290;VISION&#36890;&#36807;&#25552;&#20379;18000&#24352;&#22270;&#20687;&#65292;&#21253;&#21547;44&#31181;&#32570;&#38519;&#31867;&#22411;&#65292;&#33268;&#21147;&#20110;&#21453;&#26144;&#20986;&#21508;&#31181;&#29616;&#23454;&#24037;&#19994;&#29983;&#20135;&#22330;&#26223;&#12290;&#36890;&#36807;&#25903;&#25345;VISION&#25968;&#25454;&#38598;&#19978;&#20004;&#20010;&#27491;&#22312;&#36827;&#34892;&#30340;&#31454;&#36187;&#65292;&#25105;&#20204;&#24076;&#26395;&#20419;&#36827;&#22522;&#20110;&#35270;&#35273;&#30340;&#24037;&#19994;&#26816;&#27979;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite progress in vision-based inspection algorithms, real-world industrial challenges -- specifically in data availability, quality, and complex production requirements -- often remain under-addressed. We introduce the VISION Datasets, a diverse collection of 14 industrial inspection datasets, uniquely poised to meet these challenges. Unlike previous datasets, VISION brings versatility to defect detection, offering annotation masks across all splits and catering to various detection methodologies. Our datasets also feature instance-segmentation annotation, enabling precise defect identification. With a total of 18k images encompassing 44 defect types, VISION strives to mirror a wide range of real-world production scenarios. By supporting two ongoing challenge competitions on the VISION Datasets, we hope to foster further advancements in vision-based industrial inspection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.07886</link><description>&lt;p&gt;
&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#19982;&#20020;&#30028;&#28857;
&lt;/p&gt;
&lt;p&gt;
Symmetry &amp; Critical Points for Symmetric Tensor Decompositions Problems. (arXiv:2306.07886v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#23545;&#31216;&#32467;&#26500;&#65292;&#23548;&#20986;Puiseux&#32423;&#25968;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#20020;&#30028;&#28857;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#20020;&#30028;&#20540;&#21644;Hessian&#35889;&#30340;&#31934;&#30830;&#20998;&#26512;&#20272;&#35745;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#21508;&#31181;&#20960;&#20309;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#26368;&#21518;&#65292;&#21033;&#29992;&#19968;&#20010;&#29275;&#39039;&#22810;&#38754;&#20307;&#35770;&#35777;&#20102;&#22266;&#23450;&#23545;&#31216;&#24615;&#30340;&#25152;&#26377;&#20020;&#30028;&#28857;&#30340;&#23436;&#20840;&#26522;&#20030;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38598;&#21512;&#30456;&#27604;&#65292;&#30001;&#20110;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#65292;&#20020;&#30028;&#28857;&#30340;&#38598;&#21512;&#21487;&#33021;&#20250;&#26174;&#31034;&#20986;&#32452;&#21512;&#30340;&#20016;&#23500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the non-convex optimization problem associated with the decomposition of a real symmetric tensor into a sum of rank one terms. Use is made of the rich symmetry structure to derive Puiseux series representations of families of critical points, and so obtain precise analytic estimates on the critical values and the Hessian spectrum. The sharp results make possible an analytic characterization of various geometric obstructions to local optimization methods, revealing in particular a complex array of saddles and local minima which differ by their symmetry, structure and analytic properties. A desirable phenomenon, occurring for all critical points considered, concerns the index of a point, i.e., the number of negative Hessian eigenvalues, increasing with the value of the objective function. Lastly, a Newton polytope argument is used to give a complete enumeration of all critical points of fixed symmetry, and it is shown that contrarily to the set of global minima which remains 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#26469;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07874</link><description>&lt;p&gt;
&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taxonomy-Structured Domain Adaptation. (arXiv:2306.07874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#26469;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#26088;&#22312;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#22823;&#22810;&#38480;&#20110;&#20998;&#31867;&#39046;&#22495;&#65292;&#36825;&#20005;&#37325;&#31616;&#21270;&#20102;&#30495;&#23454;&#19990;&#30028;&#20013;&#24494;&#22937;&#30340;&#39046;&#22495;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#36827;&#34892;&#25512;&#24191;&#65292;&#23558;&#39046;&#22495;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#23884;&#22871;&#30340;&#23618;&#27425;&#30456;&#20284;&#32467;&#26500;&#65292;&#20363;&#22914;&#21160;&#29289;&#29289;&#31181;&#21644;&#20135;&#21697;&#30446;&#24405;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#32463;&#20856;&#23545;&#25239;&#26694;&#26550;&#20043;&#19978;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#19982;&#23545;&#25239;&#24615;&#37492;&#21035;&#22120;&#31454;&#20105;&#20197;&#20445;&#30041;&#23618;&#27425;&#32467;&#26500;&#20449;&#24687;&#12290;&#24403;&#32473;&#23450;&#38750;&#20449;&#24687;&#39046;&#22495;&#20998;&#31867;&#65288;&#20363;&#22914;&#65292;&#25152;&#26377;&#21494;&#33410;&#28857;&#37117;&#38142;&#25509;&#21040;&#26681;&#33410;&#28857;&#30340;&#25153;&#24179;&#20998;&#31867;&#65289;&#26102;&#65292;&#24179;&#34913;&#28857;&#24674;&#22797;&#32463;&#20856;&#30340;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20998;&#31867;&#20013;&#20135;&#29983;&#38750;&#24179;&#20961;&#30340;&#32467;&#26524;&#12290;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#36827;&#34892;&#20102;&#33258;&#36866;&#24212;&#12290;&#20195;&#30721;&#21487;&#22312;https://gith&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation aims to mitigate distribution shifts among different domains. However, traditional formulations are mostly limited to categorical domains, greatly simplifying nuanced domain relationships in the real world. In this work, we tackle a generalization with taxonomy-structured domains, which formalizes domains with nested, hierarchical similarity structures such as animal species and product catalogs. We build on the classic adversarial framework and introduce a novel taxonomist, which competes with the adversarial discriminator to preserve the taxonomy information. The equilibrium recovers the classic adversarial domain adaptation's solution if given a non-informative domain taxonomy (e.g., a flat taxonomy where all leaf nodes connect to the root node) while yielding non-trivial results with other taxonomies. Empirically, our method achieves state-of-the-art performance on both synthetic and real-world datasets with successful adaptation. Code is available at https://gith
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22240;&#26524;&#36172;&#21338;&#26426;&#20013;&#36873;&#25321;&#34892;&#21160;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#24615;&#32452;&#21512;&#32447;&#24615;&#36172;&#21338;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#20197;&#35299;&#20915;&#26410;&#30693;&#22270;&#35889;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.07858</link><description>&lt;p&gt;
&#24102;&#26410;&#30693;&#22270;&#35889;&#30340;&#21152;&#24615;&#22240;&#26524;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Additive Causal Bandits with Unknown Graph. (arXiv:2306.07858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22240;&#26524;&#36172;&#21338;&#26426;&#20013;&#36873;&#25321;&#34892;&#21160;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#24615;&#32452;&#21512;&#32447;&#24615;&#36172;&#21338;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#20197;&#35299;&#20915;&#26410;&#30693;&#22270;&#35889;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#36873;&#25321;&#34892;&#21160;&#30340;&#31639;&#27861;&#65292;&#22312;&#22240;&#26524;&#36172;&#21338;&#35774;&#32622;&#19979;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#36873;&#25321;&#24178;&#39044;&#19968;&#32452;&#30001;&#22240;&#26524;&#22270;&#30456;&#20851;&#30340;&#38543;&#26426;&#21464;&#37327;&#65292;&#23398;&#20064;&#32773;&#39034;&#24207;&#36873;&#25321;&#24178;&#39044;&#65292;&#24182;&#20174;&#24178;&#39044;&#20998;&#24067;&#20013;&#35266;&#23519;&#26679;&#26412;&#12290;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#24555;&#36895;&#25214;&#21040;&#26368;&#22823;&#21270;&#32467;&#26524;&#21464;&#37327;&#26399;&#26395;&#30340;&#65292;&#25152;&#26377;&#21487;&#35266;&#23519;&#21464;&#37327;&#24178;&#39044;&#20013;&#30340;&#24178;&#39044;&#12290;&#25105;&#20204;&#20551;&#35774;&#27809;&#26377;&#20851;&#20110;&#22240;&#26524;&#22270;&#30340;&#20219;&#20309;&#30693;&#35782;&#65292;&#38500;&#20102;&#32467;&#26524;&#21644;&#20854;&#31062;&#20808;&#20043;&#38388;&#30340;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#26410;&#30693;&#22270;&#38382;&#39064;&#22312;&#32467;&#26524;&#30340;&#29238;&#27597;&#20013;&#21487;&#20197;&#26159;&#25351;&#25968;&#32423;&#38590;&#20197;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#32467;&#26524;&#37319;&#21462;&#39069;&#22806;&#30340;&#21152;&#24615;&#20551;&#35774;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#20855;&#26377;&#20840;&#36172;&#21338;&#21453;&#39304;&#30340;&#21152;&#24615;&#32452;&#21512;&#32447;&#24615;&#36172;&#21338;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34892;&#21160;&#28040;&#38500;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#27492;&#31639;&#27861;&#24212;&#29992;&#20110;&#36825;&#20010;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore algorithms to select actions in the causal bandit setting where the learner can choose to intervene on a set of random variables related by a causal graph, and the learner sequentially chooses interventions and observes a sample from the interventional distribution. The learner's goal is to quickly find the intervention, among all interventions on observable variables, that maximizes the expectation of an outcome variable. We depart from previous literature by assuming no knowledge of the causal graph except that latent confounders between the outcome and its ancestors are not present. We first show that the unknown graph problem can be exponentially hard in the parents of the outcome. To remedy this, we adopt an additional additive assumption on the outcome which allows us to solve the problem by casting it as an additive combinatorial linear bandit problem with full-bandit feedback. We propose a novel action-elimination algorithm for this setting, show how to apply this al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;SGD&#31283;&#23450;&#24615;&#30340;&#31934;&#30830;&#38408;&#20540;&#34920;&#36798;&#24335;&#65292;&#21457;&#29616;&#20854;&#19982;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#21576;&#21333;&#35843;&#38750;&#38477;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20943;&#23567;&#25209;&#37327;&#22823;&#23567;&#21487;&#33021;&#20250;&#24433;&#21709;SGD&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07850</link><description>&lt;p&gt;
SGD&#30340;&#31934;&#30830;&#24179;&#22343;&#20108;&#27425;&#32447;&#24615;&#31283;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exact Mean Square Linear Stability Analysis for SGD. (arXiv:2306.07850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;SGD&#31283;&#23450;&#24615;&#30340;&#31934;&#30830;&#38408;&#20540;&#34920;&#36798;&#24335;&#65292;&#21457;&#29616;&#20854;&#19982;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#21576;&#21333;&#35843;&#38750;&#38477;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20943;&#23567;&#25209;&#37327;&#22823;&#23567;&#21487;&#33021;&#20250;&#24433;&#21709;SGD&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#20248;&#21270;&#26041;&#27861;&#22312;&#25439;&#22833;&#20989;&#25968;&#26497;&#23567;&#20540;&#28857;&#38468;&#36817;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#23545;&#20110;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;GD&#65289;&#65292;&#31283;&#23450;&#30340;&#25910;&#25947;&#20165;&#21487;&#33021;&#21457;&#29983;&#22312;&#36275;&#22815;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#22788;&#65292;&#24182;&#19988;&#24050;&#32463;&#19982;&#35757;&#32451;&#27169;&#22411;&#30340;&#33391;&#22909;&#24615;&#33021;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;GD&#30340;&#31283;&#23450;&#24615;&#38408;&#20540;&#24050;&#32463;&#20247;&#25152;&#21608;&#30693;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#23578;&#26410;&#25512;&#23548;&#20986;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#31934;&#30830;&#38408;&#20540;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#36825;&#26679;&#19968;&#31181;&#23553;&#38381;&#24418;&#24335;&#30340;&#34920;&#36798;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#27493;&#38271;$\eta$&#30340;&#26174;&#24335;&#26465;&#20214;&#65292;&#26082;&#26159;SGD&#22312;&#22343;&#26041;&#24847;&#20041;&#19979;&#31283;&#23450;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20063;&#26159;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#25209;&#37327;&#22823;&#23567;$B$&#30340;&#31934;&#30830;&#20316;&#29992;&#65292;&#29305;&#21035;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#31283;&#23450;&#24615;&#38408;&#20540;&#26159;&#25209;&#37327;&#22823;&#23567;&#30340;&#21333;&#35843;&#38750;&#38477;&#20989;&#25968;&#65292;&#36825;&#24847;&#21619;&#30528;&#20943;&#23567;&#25209;&#37327;&#22823;&#23567;&#21482;&#20250;&#38477;&#20302;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;SGD&#30340;&#31283;&#23450;&#24615;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamical stability of optimization methods at the vicinity of minima of the loss has recently attracted significant attention. For gradient descent (GD), stable convergence is possible only to minima that are sufficiently flat w.r.t. the step size, and those have been linked with favorable properties of the trained model. However, while the stability threshold of GD is well-known, to date, no explicit expression has been derived for the exact threshold of stochastic GD (SGD). In this paper, we derive such a closed-form expression. Specifically, we provide an explicit condition on the step size $\eta$ that is both necessary and sufficient for the stability of SGD in the mean square sense. Our analysis sheds light on the precise role of the batch size $B$. Particularly, we show that the stability threshold is a monotonically non-decreasing function of the batch size, which means that reducing the batch size can only hurt stability. Furthermore, we show that SGD's stability threshold
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDGM&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#37197;&#32622;&#35757;&#32451;DDGM&#65292;&#20854;&#20013;&#22122;&#22768;&#20381;&#36182;&#30340;&#35757;&#32451;&#37197;&#32622;&#20351;&#25512;&#29702;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.07820</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21160;&#24577;&#29983;&#25104;&#35821;&#38899;&#21644;&#22122;&#22768;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unsupervised speech enhancement with deep dynamical generative speech and noise models. (arXiv:2306.07820v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDGM&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#37197;&#32622;&#35757;&#32451;DDGM&#65292;&#20854;&#20013;&#22122;&#22768;&#20381;&#36182;&#30340;&#35757;&#32451;&#37197;&#32622;&#20351;&#25512;&#29702;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(DVAE)&#20316;&#20026;&#28165;&#26224;&#35821;&#38899;&#27169;&#22411;&#21644;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#20316;&#20026;&#22122;&#22768;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#29992;&#28145;&#24230;&#21160;&#24577;&#29983;&#25104;&#27169;&#22411;(DDGM)&#20195;&#26367;NMF&#22122;&#22768;&#27169;&#22411;&#65292;&#20854;&#20013;DDGM&#20381;&#36182;&#20110;DVAE&#28508;&#21464;&#37327;&#12289;&#22024;&#26434;&#30340;&#35266;&#27979;&#20540;&#25110;&#20004;&#32773;&#12290;&#21487;&#20197;&#36890;&#36807;&#19977;&#31181;&#37197;&#32622;&#26469;&#35757;&#32451;DDGM&#65306;&#26080;&#22122;&#22768;&#24863;&#30693;&#65292;&#20381;&#36182;&#20110;&#22122;&#22768;&#21644;&#22122;&#22768;&#20381;&#36182;&#35757;&#32451;&#21518;&#30340;&#22122;&#22768;&#33258;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#32780;&#22122;&#22768;&#20381;&#36182;&#30340;&#35757;&#32451;&#37197;&#32622;&#20351;&#25512;&#29702;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work builds on a previous work on unsupervised speech enhancement using a dynamical variational autoencoder (DVAE) as the clean speech model and non-negative matrix factorization (NMF) as the noise model. We propose to replace the NMF noise model with a deep dynamical generative model (DDGM) depending either on the DVAE latent variables, or on the noisy observations, or on both. This DDGM can be trained in three configurations: noise-agnostic, noise-dependent and noise adaptation after noise-dependent training. Experimental results show that the proposed method achieves competitive performance compared to state-of-the-art unsupervised speech enhancement methods, while the noise-dependent training configuration yields a much more time-efficient inference process.
&lt;/p&gt;</description></item><item><title>PDCA&#26159;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;Lagrangian&#20989;&#25968;&#19978;&#36816;&#34892;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#26469;&#25214;&#21040;&#36817;&#20284;&#38797;&#28857;&#65292;&#32780;&#26080;&#38656;&#38598;&#20013;&#24615;&#21644;&#24378;Bellman&#23436;&#22791;&#24615;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2306.07818</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning. (arXiv:2306.07818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07818
&lt;/p&gt;
&lt;p&gt;
PDCA&#26159;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;Lagrangian&#20989;&#25968;&#19978;&#36816;&#34892;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#26469;&#25214;&#21040;&#36817;&#20284;&#38797;&#28857;&#65292;&#32780;&#26080;&#38656;&#38598;&#20013;&#24615;&#21644;&#24378;Bellman&#23436;&#22791;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#31181;&#31574;&#30053;&#65292;&#20197;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#19978;&#28385;&#36275;&#23545;&#25104;&#26412;&#20989;&#25968;&#26399;&#26395;&#20540;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#22987;-&#23545;&#20598;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PDCA&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#30340;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#12290;PDCA&#22312;&#35780;&#35770;&#23478;&#20272;&#35745;&#30340;Lagrangian&#20989;&#25968;&#19978;&#36816;&#34892;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#12290;&#21407;&#22987;&#29609;&#23478;&#37319;&#29992;&#26080;&#24724;&#31574;&#30053;&#20248;&#21270;&#31070;&#35861;&#65292;&#22312;&#32473;&#23450;&#20219;&#20309;&#35780;&#35770;&#23478;&#21644;&#23545;&#20598;&#29609;&#23478;&#30340;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#25289;&#26684;&#26391;&#26085;&#20989;&#25968;&#30340;&#20272;&#35745;&#12290;&#23545;&#20598;&#29609;&#23478;&#36890;&#36807;&#37319;&#29992;&#26080;&#24724;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#31070;&#35861;&#65292;&#22312;&#32473;&#23450;&#35780;&#35770;&#23478;&#21644;&#21407;&#22987;&#29609;&#23478;&#30340;&#20219;&#20309;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#25289;&#26684;&#26391;&#26085;&#20989;&#25968;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PDCA&#21487;&#20197;&#25104;&#21151;&#22320;&#25214;&#21040;&#25289;&#26684;&#26391;&#26085;&#20989;&#25968;&#30340;&#36817;&#20284;&#38797;&#28857;&#65292;&#36825;&#23545;&#20110;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#19982;&#20197;&#21069;&#38656;&#35201;&#38598;&#20013;&#24615;&#21644;&#24378;Bellman&#23436;&#22791;&#24615;&#20551;&#35774;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;PDCA&#21482;&#38656;&#35201;&#19968;&#33268;&#24615;&#21644;&#33258;&#38381;&#24615;&#36825;&#20004;&#20010;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline constrained reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward subject to constraints on expected value of cost functions using an existing dataset. In this paper, we propose Primal-Dual-Critic Algorithm (PDCA), a novel algorithm for offline constrained RL with general function approximation. PDCA runs a primal-dual algorithm on the Lagrangian function estimated by critics. The primal player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate given any choices of the critics and the dual player. The dual player employs a no-regret online linear optimization oracle to minimize the Lagrangian estimate given any choices of the critics and the primal player. We show that PDCA can successfully find a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem. Unlike previous work that requires concentrability and strong Bellman completeness assumptions, PDCA only requires co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2306.07812</link><description>&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#33258;&#21160;&#21270;&#19977;&#32500;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Automated 3D Pre-Training for Molecular Property Prediction. (arXiv:2306.07812v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07812
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26159;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#30001;&#20110;&#20998;&#23376;&#30340;&#20960;&#20309;&#32467;&#26500;&#23545;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#24517;&#35201;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#65292;&#22240;&#27492;&#23558;3D&#20449;&#24687;&#19982;&#21508;&#31181;&#22270;&#24418;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#33719;&#24471;&#20998;&#23376;&#30340;&#20960;&#20309;&#32467;&#26500;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;3D&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;&#31216;&#20026;3D PGT&#65289;&#65292;&#23427;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#27809;&#26377;3D&#32467;&#26500;&#30340;&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22522;&#20110;&#21270;&#23398;&#38190;&#38271;&#65292;&#21270;&#23398;&#38190;&#35282;&#21644;&#20108;&#38754;&#35282;&#36825;&#19977;&#20010;&#22522;&#26412;&#20960;&#20309;&#25551;&#36848;&#31526;&#23545;&#24212;&#20110;&#23436;&#25972;&#30340;&#20998;&#23376;3D&#26500;&#24418;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#36825;&#19977;&#20010;&#23646;&#24615;&#30340;&#22810;&#20219;&#21153;&#29983;&#25104;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#33258;&#21160;&#34701;&#21512;&#36825;&#19977;&#39033;&#29983;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;&#8220;&#24635;&#33021;&#37327;&#8221;&#26469;&#25628;&#32034;&#30340;&#26367;&#20195;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction, 3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the \textit{total energy} to search for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#30333;&#30418;&#27169;&#22411;&#65292;SCENE-Net&#65292;&#29992;&#20110;3D&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#29305;&#24449;&#24418;&#29366;&#35782;&#21035;&#25552;&#20379;&#20869;&#22312;&#20960;&#20309;&#35299;&#37322;&#24615;&#65292;&#36739;&#23569;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#38656;&#27714;&#20197;&#21450;&#23545;&#22122;&#38899;&#26631;&#31614;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07809</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#30333;&#30418;&#19977;&#32500;&#28857;&#20113;&#25903;&#25745;&#22612;&#35821;&#20041;&#20998;&#21106;&#30340;&#29305;&#24449;&#24418;&#29366;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Low-Resource White-Box Semantic Segmentation of Supporting Towers on 3D Point Clouds via Signature Shape Identification. (arXiv:2306.07809v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#30333;&#30418;&#27169;&#22411;&#65292;SCENE-Net&#65292;&#29992;&#20110;3D&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#29305;&#24449;&#24418;&#29366;&#35782;&#21035;&#25552;&#20379;&#20869;&#22312;&#20960;&#20309;&#35299;&#37322;&#24615;&#65292;&#36739;&#23569;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#38656;&#27714;&#20197;&#21450;&#23545;&#22122;&#38899;&#26631;&#31614;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;3D&#22330;&#26223;&#20998;&#21106;&#30740;&#31350;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#22914;IoU&#65292;&#32780;&#24573;&#30053;&#20102;&#37027;&#20123;&#26082;&#26080;&#27861;&#35775;&#38382;&#24517;&#35201;&#36164;&#28304;&#65292;&#21448;&#38656;&#35201;&#20102;&#35299;&#27169;&#22411;&#20915;&#31574;&#26426;&#21046;&#30340;&#30740;&#31350;&#32773;&#21644;&#20174;&#19994;&#32773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SCENE-Net&#65292;&#19968;&#31181;&#29992;&#20110;3D&#28857;&#20113;&#35821;&#20041;&#20998;&#21106;&#30340;&#20302;&#36164;&#28304;&#30333;&#30418;&#27169;&#22411;&#12290;SCENE-Net&#36890;&#36807;&#21516;&#21464;&#38750;&#24352;&#37327;&#31639;&#23376;&#65288;GENEOs&#65289;&#22312;&#28857;&#20113;&#19978;&#35782;&#21035;&#29305;&#24449;&#24418;&#29366;&#65292;&#25552;&#20379;&#20869;&#22312;&#20960;&#20309;&#35299;&#37322;&#24615;&#65292;&#20854;&#22312;&#31508;&#35760;&#26412;&#19978;&#30340;&#35757;&#32451;&#26102;&#38388;&#20026;85&#20998;&#38047;&#65292;&#25512;&#29702;&#26102;&#38388;&#20026;20&#27627;&#31186;&#12290;SCENE-Net&#20855;&#26377;11&#20010;&#21487;&#35757;&#32451;&#30340;&#20960;&#20309;&#21442;&#25968;&#65292;&#27604;&#40657;&#30418;&#27169;&#22411;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;SCENE-Net&#20855;&#26377;&#23545;&#22122;&#22768;&#26631;&#35760;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;IoU&#12290;&#26412;&#25991;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;40,000&#20844;&#37324;&#26631;&#35760;&#30340;&#20892;&#26449;&#22320;&#24418;&#28857;&#20113;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in 3D semantic segmentation has been increasing performance metrics, like the IoU, by scaling model complexity and computational resources, leaving behind researchers and practitioners that (1) cannot access the necessary resources and (2) do need transparency on the model decision mechanisms. In this paper, we propose SCENE-Net, a low-resource white-box model for 3D point cloud semantic segmentation. SCENE-Net identifies signature shapes on the point cloud via group equivariant non-expansive operators (GENEOs), providing intrinsic geometric interpretability. Our training time on a laptop is 85~min, and our inference time is 20~ms. SCENE-Net has 11 trainable geometrical parameters and requires fewer data than black-box models. SCENE--Net offers robustness to noisy labeling and data imbalance and has comparable IoU to state-of-the-art methods. With this paper, we release a 40~000 Km labeled dataset of rural terrain point clouds and our code implementation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RiTINI&#26041;&#27861;&#65292;&#20351;&#29992;&#31354;&#38388;&#19982;&#26102;&#38388;&#22270;&#27880;&#24847;&#21147;&#21644;&#22270;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#32452;&#21512;&#26469;&#25512;&#26029;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#21464;&#21270;&#30340;&#20132;&#20114;&#22270;&#65292;&#30456;&#27604;&#20256;&#32479;&#22240;&#26524;&#25512;&#26029;&#32593;&#32476;&#65292;&#20855;&#26377;&#33021;&#22815;&#25512;&#26029;&#24490;&#29615;&#12289;&#26377;&#21521;&#21644;&#26102;&#21464;&#22270;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.07803</link><description>&lt;p&gt;
&#20174;&#20855;&#26377;&#25200;&#21160;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#21160;&#24577;&#30340;&#35843;&#25511;&#20132;&#20114;&#22270;
&lt;/p&gt;
&lt;p&gt;
Inferring dynamic regulatory interaction graphs from time series data with perturbations. (arXiv:2306.07803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RiTINI&#26041;&#27861;&#65292;&#20351;&#29992;&#31354;&#38388;&#19982;&#26102;&#38388;&#22270;&#27880;&#24847;&#21147;&#21644;&#22270;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#32452;&#21512;&#26469;&#25512;&#26029;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#21464;&#21270;&#30340;&#20132;&#20114;&#22270;&#65292;&#30456;&#27604;&#20256;&#32479;&#22240;&#26524;&#25512;&#26029;&#32593;&#32476;&#65292;&#20855;&#26377;&#33021;&#22815;&#25512;&#26029;&#24490;&#29615;&#12289;&#26377;&#21521;&#21644;&#26102;&#21464;&#22270;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#31995;&#32479;&#30340;&#29305;&#24449;&#26159;&#20854;&#23454;&#20307;&#20043;&#38388;&#30340;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#38543;&#26102;&#38388;&#21160;&#24577;&#28436;&#21464;&#12290;&#20934;&#30830;&#25512;&#26029;&#36825;&#20123;&#21160;&#24577;&#20851;&#31995;&#23545;&#20110;&#29702;&#35299;&#21644;&#39044;&#27979;&#31995;&#32479;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Regulatory Temporal Interaction Network Inference (RiTINI) &#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31354;&#38388;&#19982;&#26102;&#38388;&#22270;&#27880;&#24847;&#21147;&#21644;&#22270;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#25512;&#26029;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#21464;&#21270;&#30340;&#20132;&#20114;&#22270;&#12290;RiTINI&#21033;&#29992;&#22270;&#24418;&#20808;&#39564;&#30340;&#26102;&#38388;&#38388;&#38548;&#20449;&#21495;&#20197;&#21450;&#22312;&#21508;&#20010;&#33410;&#28857;&#22788;&#30340;&#20449;&#21495;&#25200;&#21160;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22522;&#30784;&#31995;&#32479;&#30340;&#21160;&#24577;&#12290;&#27492;&#26041;&#27861;&#19981;&#21516;&#20110;&#20256;&#32479;&#22240;&#26524;&#25512;&#26029;&#32593;&#32476;&#65292;&#21518;&#32773;&#20165;&#38480;&#20110;&#25512;&#26029;&#38750;&#24490;&#29615;&#21644;&#38745;&#24577;&#22270;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;RiTINI&#33021;&#22815;&#25512;&#26029;&#24490;&#29615;&#12289;&#26377;&#21521;&#21644;&#26102;&#21464;&#22270;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22797;&#26434;&#31995;&#32479;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;RiTINI&#20013;&#30340;&#22270;&#24418;&#27880;&#24847;&#26426;&#21046;&#20801;&#35768;&#27169;&#22411;&#25429;&#25417;&#26412;&#22320;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;ODE&#34920;&#31034;&#26041;&#27861;&#20351;&#24471;&#36830;&#32493;&#19982;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#30340;&#38598;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;RiTINI&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#20010;&#39046;&#22495;&#20934;&#30830;&#25512;&#26029;&#21160;&#24577;&#35843;&#25511;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex systems are characterized by intricate interactions between entities that evolve dynamically over time. Accurate inference of these dynamic relationships is crucial for understanding and predicting system behavior. In this paper, we propose Regulatory Temporal Interaction Network Inference (RiTINI) for inferring time-varying interaction graphs in complex systems using a novel combination of space-and-time graph attentions and graph neural ordinary differential equations (ODEs). RiTINI leverages time-lapse signals on a graph prior, as well as perturbations of signals at various nodes in order to effectively capture the dynamics of the underlying system. This approach is distinct from traditional causal inference networks, which are limited to inferring acyclic and static graphs. In contrast, RiTINI can infer cyclic, directed, and time-varying graphs, providing a more comprehensive and accurate representation of complex systems. The graph attention mechanism in RiTINI allows the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#21644;&#20889;&#20316;&#39118;&#26684;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#19968;&#20123;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2306.07799</link><description>&lt;p&gt;
ChatGPT&#19982;&#20154;&#24037;&#25776;&#20889;&#25991;&#26412;&#65306;&#21487;&#25511;&#25991;&#26412;&#25688;&#35201;&#21644;&#21477;&#23376;&#39118;&#26684;&#36716;&#31227;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer. (arXiv:2306.07799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#21644;&#20889;&#20316;&#39118;&#26684;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#19968;&#20123;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#20197;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#20174;&#31616;&#30701;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#24341;&#36215;&#20102;&#23186;&#20307;&#30340;&#37325;&#35270;&#12290;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#65288;&#19987;&#23478;&#19982;&#19968;&#33324;&#20154;&#65289;&#21644;&#20889;&#20316;&#39118;&#26684;&#65288;&#27491;&#24335;&#19982;&#38750;&#27491;&#24335;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#23558;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;&#20154;&#24037;&#25776;&#20889;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#35832;&#22914;&#21333;&#35789;&#31867;&#22411;&#20998;&#24067;&#31561;&#20960;&#20010;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403; ChatGPT &#23558;&#25991;&#26412;&#36866;&#24212;&#29305;&#23450;&#39118;&#26684;&#26102;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#26377;&#38480;&#39640;&#26031;&#31070;&#32463;&#20803;&#65288;FGN&#65289;&#36825;&#31181;&#26032;&#30340;&#31070;&#32463;&#20803;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#27169;&#22411;&#36716;&#25442;&#20026;FGN&#32467;&#26500;&#65292;&#20174;&#32780;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07796</link><description>&lt;p&gt;
&#26377;&#38480;&#39640;&#26031;&#31070;&#32463;&#20803;&#65306;&#36890;&#36807;&#35753;&#31070;&#32463;&#32593;&#32476;&#35828;&#8220;&#25105;&#19981;&#30693;&#36947;&#8221;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Finite Gaussian Neurons: Defending against adversarial attacks by making neural networks say "I don't know". (arXiv:2306.07796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07796
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#26377;&#38480;&#39640;&#26031;&#31070;&#32463;&#20803;&#65288;FGN&#65289;&#36825;&#31181;&#26032;&#30340;&#31070;&#32463;&#20803;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#27169;&#22411;&#36716;&#25442;&#20026;FGN&#32467;&#26500;&#65292;&#20174;&#32780;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2014&#24180;&#20197;&#26469;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24050;&#30693;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#24494;&#23567;&#25913;&#21160;&#65292;&#20351;&#32593;&#32476;&#20135;&#29983;&#38169;&#35823;&#25110;&#26080;&#24847;&#20041;&#30340;&#36755;&#20986;&#12290;&#34429;&#28982;&#24050;&#25552;&#20986;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#24341;&#20837;&#20102;&#26377;&#38480;&#39640;&#26031;&#31070;&#32463;&#20803;&#65288;FGN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#31070;&#32463;&#20803;&#32467;&#26500;&#12290;&#25105;&#30340;&#24037;&#20316;&#26088;&#22312;&#65306;-&#23558;&#29616;&#26377;&#27169;&#22411;&#36731;&#26494;&#36716;&#25442;&#20026;&#26377;&#38480;&#39640;&#26031;&#31070;&#32463;&#20803;&#32467;&#26500;&#65292;-&#21516;&#26102;&#20445;&#30041;&#29616;&#26377;&#27169;&#22411;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#65292;-&#24182;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#23637;&#31034;&#20102;&#36716;&#25442;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26377;&#38480;&#39640;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;FGNN&#65289;&#22312;&#19982;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#36739;&#26102;&#65292;&#22312;&#38543;&#26426;&#21644;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#26041;&#27861;&#23545;&#25239;&#22270;&#20687;&#19978;&#39044;&#27979;&#22987;&#32456;&#20855;&#26377;&#36739;&#20302;&#30340;&#32622;&#20449;&#24230;&#65288;&#21363;&#19981;&#20250;&#36807;&#24230;&#33258;&#20449;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since 2014, artificial neural networks have been known to be vulnerable to adversarial attacks, which can fool the network into producing wrong or nonsensical outputs by making humanly imperceptible alterations to inputs. While defenses against adversarial attacks have been proposed, they usually involve retraining a new neural network from scratch, a costly task. In this work, I introduce the Finite Gaussian Neuron (FGN), a novel neuron architecture for artificial neural networks. My works aims to: - easily convert existing models to Finite Gaussian Neuron architecture, - while preserving the existing model's behavior on real data, - and offering resistance against adversarial attacks. I show that converted and retrained Finite Gaussian Neural Networks (FGNN) always have lower confidence (i.e., are not overconfident) in their predictions over randomized and Fast Gradient Sign Method adversarial images when compared to classical neural networks, while maintaining high accuracy and conf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32452;&#21512;&#24615;&#26469;&#23398;&#20064;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32452;&#21512;&#31561;&#21464;&#24615;&#36136;&#32435;&#20837;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.07783</link><description>&lt;p&gt;
&#32452;&#21512;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositionally Equivariant Representation Learning. (arXiv:2306.07783v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32452;&#21512;&#24615;&#26469;&#23398;&#20064;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32452;&#21512;&#31561;&#21464;&#24615;&#36136;&#32435;&#20837;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#20805;&#20998;&#30340;&#30417;&#30563;&#65288;&#26631;&#35760;&#25968;&#25454;&#65289;&#25165;&#33021;&#26377;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#21487;&#20197;&#36805;&#36895;&#23398;&#20064;&#35782;&#21035;&#21307;&#23398;&#22270;&#20687;&#65288;&#22914;&#30913;&#20849;&#25391;&#21644; CT &#25195;&#25551;&#20013;&#30340;&#37325;&#35201;&#35299;&#21078;&#32467;&#26500;&#65289;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#25351;&#23548;&#12290;&#36825;&#31181;&#35782;&#21035;&#33021;&#21147;&#23481;&#26131;&#27867;&#21270;&#21040;&#26469;&#33258;&#19981;&#21516;&#21307;&#30103;&#26426;&#26500;&#30340;&#26032;&#22270;&#20687;&#20197;&#21450;&#19981;&#21516;&#35774;&#32622;&#20013;&#30340;&#26032;&#20219;&#21153;&#12290;&#36825;&#31181;&#24555;&#36895;&#19988;&#27867;&#21270;&#30340;&#23398;&#20064;&#33021;&#21147;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#20154;&#33041;&#20013;&#22270;&#20687;&#27169;&#24335;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#32780;&#24403;&#21069;&#30340;&#21307;&#23398;&#27169;&#22411;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#20986;&#36825;&#31181;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#21033;&#29992;&#32452;&#21512;&#24615;&#26469;&#23398;&#20064;&#26356;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#29992;&#20110;&#29983;&#25104;&#21307;&#23398;&#22270;&#20687;&#30340;&#22522;&#30784;&#29983;&#25104;&#22240;&#32032;&#28385;&#36275;&#32452;&#21512;&#31561;&#21464;&#24615;&#36136;&#65292;&#20854;&#20013;&#27599;&#20010;&#22240;&#32032;&#37117;&#26159;&#32452;&#21512;&#30340;&#65288;&#20363;&#22914;&#23545;&#24212;&#20110;&#20154;&#20307;&#35299;&#21078;&#32467;&#26500;&#65289;&#24182;&#19988;&#23545;&#20219;&#21153;&#26159;&#31561;&#21464;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23558;&#32452;&#21512;&#31561;&#21464;&#24615;&#36136;&#32435;&#20837;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models often need sufficient supervision (i.e. labelled data) in order to be trained effectively. By contrast, humans can swiftly learn to identify important anatomy in medical images like MRI and CT scans, with minimal guidance. This recognition capability easily generalises to new images from different medical facilities and to new tasks in different settings. This rapid and generalisable learning ability is largely due to the compositional structure of image patterns in the human brain, which are not well represented in current medical models. In this paper, we study the utilisation of compositionality in learning more interpretable and generalisable representations for medical image segmentation. Overall, we propose that the underlying generative factors that are used to generate the medical images satisfy compositional equivariance property, where each factor is compositional (e.g. corresponds to the structures in human anatomy) and also equivariant to the task. Henc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#39640;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#26041;&#27861;&#65292;&#23427;&#23558;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#20256;&#25773;&#65292;&#36890;&#36807;&#23558;Lyapunov&#26041;&#31243;&#25237;&#24433;&#21040;&#20302;&#31209;&#30697;&#38453;&#30340;&#27969;&#24418;&#19978;&#65292;&#20351;&#29992;&#25968;&#20540;&#31283;&#23450;&#30340;&#21160;&#24577;&#20302;&#31209;&#31215;&#20998;&#22120;&#27714;&#35299;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.07774</link><description>&lt;p&gt;
&#38477;&#31209;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65306;&#22312;&#39640;&#32500;&#20013;&#36827;&#34892;&#36817;&#20284;&#20302;&#31209;&#21160;&#24577;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions. (arXiv:2306.07774v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#39640;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#26041;&#27861;&#65292;&#23427;&#23558;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#20256;&#25773;&#65292;&#36890;&#36807;&#23558;Lyapunov&#26041;&#31243;&#25237;&#24433;&#21040;&#20302;&#31209;&#30697;&#38453;&#30340;&#27969;&#24418;&#19978;&#65292;&#20351;&#29992;&#25968;&#20540;&#31283;&#23450;&#30340;&#21160;&#24577;&#20302;&#31209;&#31215;&#20998;&#22120;&#27714;&#35299;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#21160;&#24577;&#31995;&#32479;&#30340;&#25512;&#26029;&#21644;&#27169;&#25311;&#20013;&#65292;&#38656;&#35201;&#36827;&#34892;&#26576;&#31181;&#24418;&#24335;&#30340;&#38477;&#32500;&#25165;&#33021;&#20351;&#38382;&#39064;&#20855;&#26377;&#21487;&#22788;&#29702;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#39640;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#26041;&#27861;&#65292;&#23427;&#23558;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#20256;&#25773;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#39044;&#27979;&#27493;&#39588;&#30456;&#20851;&#30340;Lyapunov&#26041;&#31243;&#25237;&#24433;&#21040;&#20302;&#31209;&#30697;&#38453;&#30340;&#27969;&#24418;&#19978;&#26469;&#23454;&#29616;&#30340;&#65292;&#28982;&#21518;&#36890;&#36807;&#26368;&#36817;&#24320;&#21457;&#30340;&#25968;&#20540;&#31283;&#23450;&#12289;&#21160;&#24577;&#20302;&#31209;&#31215;&#20998;&#22120;&#27714;&#35299;&#36825;&#20123;&#26041;&#31243;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36890;&#36807;&#27880;&#24847;&#21327;&#26041;&#24046;&#26356;&#26032;&#20165;&#36716;&#25442;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#21015;&#31354;&#38388;&#65292;&#32780;&#35813;&#31354;&#38388;&#30001;&#26500;&#36896;&#24471;&#21040;&#65292;&#20174;&#32780;&#20351;&#26356;&#26032;&#27493;&#39588;&#20855;&#26377;&#21487;&#22788;&#29702;&#24615;&#12290;&#31639;&#27861;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#65292;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20302;&#31209;&#36817;&#20284;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#30340;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#36825;&#20351;&#24471;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference and simulation in the context of high-dimensional dynamical systems remain computationally challenging problems. Some form of dimensionality reduction is required to make the problem tractable in general. In this paper, we propose a novel approximate Gaussian filtering and smoothing method which propagates low-rank approximations of the covariance matrices. This is accomplished by projecting the Lyapunov equations associated with the prediction step to a manifold of low-rank matrices, which are then solved by a recently developed, numerically stable, dynamical low-rank integrator. Meanwhile, the update steps are made tractable by noting that the covariance update only transforms the column space of the covariance matrix, which is low-rank by construction. The algorithm differentiates itself from existing ensemble-based approaches in that the low-rank approximations of the covariance matrices are deterministic, rather than stochastic. Crucially, this enables the method to repr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#25104;&#21151;&#20027;&#35201;&#26159;&#30001;&#20110;&#25915;&#20987;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#36890;&#36807;&#26500;&#24314;&#21487;&#37325;&#22797;&#20803;&#32032;&#30340;&#23545;&#25239;&#24615;&#22270;&#26696;&#21487;&#20197;&#29983;&#25104;&#26368;&#22823;&#21487;&#33021;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65292;&#24182;&#23454;&#29616;&#20102;&#36530;&#36991;&#26816;&#27979;&#30340;&#26032;&#26368;&#20808;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.07768</link><description>&lt;p&gt;
&#21306;&#22495;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;&#65306;&#37325;&#22797;&#20803;&#32032;&#20351;&#23545;&#25239;&#24615;&#25915;&#20987;&#26356;&#24378;&#22823;
&lt;/p&gt;
&lt;p&gt;
Area is all you need: repeatable elements make stronger adversarial attacks. (arXiv:2306.07768v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#25104;&#21151;&#20027;&#35201;&#26159;&#30001;&#20110;&#25915;&#20987;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#36890;&#36807;&#26500;&#24314;&#21487;&#37325;&#22797;&#20803;&#32032;&#30340;&#23545;&#25239;&#24615;&#22270;&#26696;&#21487;&#20197;&#29983;&#25104;&#26368;&#22823;&#21487;&#33021;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#65292;&#24182;&#23454;&#29616;&#20102;&#36530;&#36991;&#26816;&#27979;&#30340;&#26032;&#26368;&#20808;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#38750;&#27491;&#24120;&#36755;&#20837;&#65288;&#31216;&#20026;&#23545;&#25239;&#24615;&#31034;&#20363;&#65289;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#23427;&#20204;&#38169;&#35823;&#20998;&#31867;&#25110;&#26080;&#27861;&#26816;&#27979;&#21040;&#29289;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#21464;&#24471;&#36234;&#26469;&#36234;&#25104;&#21151;&#20027;&#35201;&#26159;&#30001;&#20110;&#25915;&#20987;&#23610;&#23544;&#30340;&#22686;&#21152;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#36890;&#36807;&#26500;&#24314;&#21487;&#37325;&#22797;&#20803;&#32032;&#30340;&#23545;&#25239;&#24615;&#22270;&#26696;&#26469;&#29983;&#25104;&#26368;&#22823;&#21487;&#33021;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#36530;&#36991;YOLOv2&#21644;YOLOv3&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26410;&#33021;&#22797;&#21046;&#35813;&#39046;&#22495;&#20013;&#21457;&#24067;&#30340;&#20960;&#20010;&#25915;&#20987;&#20808;&#21069;&#25104;&#21151;&#30340;&#23454;&#39564;&#65292;&#24182;&#20197;&#19968;&#20123;&#20851;&#20110;&#27979;&#35797;&#21644;&#21487;&#37325;&#22797;&#24615;&#30340;&#35780;&#35770;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, deep neural networks have achieved state of the art in computer vision tasks. These models, however, are susceptible to unusual inputs, known as adversarial examples, that cause them to misclassify or otherwise fail to detect objects. Here, we provide evidence that the increasing success of adversarial attacks is primarily due to increasing their size. We then demonstrate a method for generating the largest possible adversarial patch by building a adversarial pattern out of repeatable elements. This approach achieves a new state of the art in evading detection by YOLOv2 and YOLOv3. Finally, we present an experiment that fails to replicate the prior success of several attacks published in this field, and end with some comments on testing and reproducibility.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20445;&#30495;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#26694;&#26550;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20445;&#30495;&#24230;&#36873;&#25321;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#19978;&#19979;&#30028;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#23450;&#20041;&#24182;&#35777;&#26126;&#20102;&#30456;&#24212;&#30340;&#38382;&#39064;&#26080;&#20851;&#21644;&#38382;&#39064;&#30456;&#20851;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.07761</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Multi-Fidelity Multi-Armed Bandits Revisited. (arXiv:2306.07761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07761
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20445;&#30495;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#26694;&#26550;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20445;&#30495;&#24230;&#36873;&#25321;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#19978;&#19979;&#30028;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#23450;&#20041;&#24182;&#35777;&#26126;&#20102;&#30456;&#24212;&#30340;&#38382;&#39064;&#26080;&#20851;&#21644;&#38382;&#39064;&#30456;&#20851;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32463;&#20856;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#25193;&#23637;&#8212;&#8212;&#22810;&#20445;&#30495;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;(MF-MAB)&#12290;MF-MAB&#20801;&#35768;&#27599;&#20010;&#33218;&#26681;&#25454;&#19981;&#21516;&#30340;&#20195;&#20215;(&#20445;&#30495;&#24230;)&#21644;&#35266;&#27979;&#31934;&#24230;&#26469;&#36827;&#34892;&#25289;&#21160;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20339;&#33218;&#35782;&#21035;&#20197;&#21450;&#36951;&#25022;&#26368;&#23567;&#21270;&#20004;&#20010;&#30446;&#26631;&#12290;&#23545;&#20110;&#26368;&#20339;&#33218;&#35782;&#21035;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#20869;&#23481;&#65306;(a)&#20195;&#20215;&#22797;&#26434;&#24230;&#19979;&#30028;&#65292;(b)&#20004;&#31181;&#19981;&#21516;&#20445;&#30495;&#24230;&#36873;&#25321;&#26041;&#27861;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;(c)&#20004;&#31181;&#26041;&#27861;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#30001;MF-MAB&#30340;&#36825;&#20004;&#20010;&#20195;&#20215;&#22797;&#26434;&#24230;&#19979;&#30028;&#21487;&#20197;&#24674;&#22797;&#32463;&#20856;&#65288;&#21333;&#20445;&#30495;&#24230;&#65289;MAB&#30340;&#26631;&#20934;&#26679;&#26412;&#22797;&#26434;&#24230;&#19979;&#30028;&#12290;&#23545;&#20110;MF-MAB&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#25022;&#23450;&#20041;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#38382;&#39064;&#26080;&#20851;&#30340;&#36951;&#25022;&#19979;&#30028;&#20026;$\Omega(K^{1/3}\Lambda^{2/3})&#8203;$&#21644;&#38382;&#39064;&#30456;&#20851;&#30340;&#19979;&#30028;$\Omega(K\log \Lambda)&#8203;$&#65292;&#20854;&#20013;$K$&#26159;&#33218;&#25968;&#65292;$\Lambda$&#26159;&#20197;&#20195;&#20215;&#20026;&#21333;&#20301;&#30340;&#20915;&#31574;&#39044;&#31639;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28120;&#27760;&#30340;&#31639;&#27861;&#65292;&#20854;&#26435;&#34913;&#20102;&#19981;&#21516;&#30340;&#20195;&#20215;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the multi-fidelity multi-armed bandit (MF-MAB), an extension of the canonical multi-armed bandit (MAB) problem. MF-MAB allows each arm to be pulled with different costs (fidelities) and observation accuracy. We study both the best arm identification with fixed confidence (BAI) and the regret minimization objectives. For BAI, we present (a) a cost complexity lower bound, (b) an algorithmic framework with two alternative fidelity selection procedures, and (c) both procedures' cost complexity upper bounds. From both cost complexity bounds of MF-MAB, one can recover the standard sample complexity bounds of the classic (single-fidelity) MAB. For regret minimization of MF-MAB, we propose a new regret definition, prove its problem-independent regret lower bound $\Omega(K^{1/3}\Lambda^{2/3})$ and problem-dependent lower bound $\Omega(K\log \Lambda)$, where $K$ is the number of arms and $\Lambda$ is the decision budget in terms of cost, and devise an elimination-based algorithm whose w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#65288;CMPGs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23547;&#25214;CMPGs&#30340;&#32435;&#20160;&#31574;&#30053;&#30340;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#21333;&#26234;&#33021;&#20307;&#27169;&#22411;&#20013;CMPG&#38382;&#39064;&#19981;&#28385;&#36275;&#24378;&#23545;&#20598;&#24615;&#30340;&#38382;&#39064;</title><link>http://arxiv.org/abs/2306.07749</link><description>&lt;p&gt;
&#22312;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#35777;&#26126;&#23398;&#20064;&#32435;&#20160;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Provably Learning Nash Policies in Constrained Markov Potential Games. (arXiv:2306.07749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#65288;CMPGs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23547;&#25214;CMPGs&#30340;&#32435;&#20160;&#31574;&#30053;&#30340;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#21333;&#26234;&#33021;&#20307;&#27169;&#22411;&#20013;CMPG&#38382;&#39064;&#19981;&#28385;&#36275;&#24378;&#23545;&#20598;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#35299;&#20915;&#20102;&#22810;&#20010;&#20195;&#29702;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26368;&#20248;&#21270;&#20854;&#33258;&#36523;&#30340;&#30446;&#26631;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#20363;&#20013;&#65292;&#19981;&#20165;&#20165;&#26159;&#35201;&#20248;&#21270;&#23427;&#20204;&#21508;&#33258;&#30340;&#30446;&#26631;&#65292;&#36824;&#35201;&#30830;&#20445;&#23433;&#20840;&#34892;&#20026;&#12290;&#22312;&#20132;&#36890;&#36335;&#30001;&#20013;&#65292;&#27599;&#36742;&#36710;&#65288;&#20195;&#29702;&#65289;&#26088;&#22312;&#24555;&#36895;&#21040;&#36798;&#30446;&#30340;&#22320;&#65288;&#30446;&#26631;&#65289;&#65292;&#21516;&#26102;&#36991;&#20813;&#30896;&#25758;&#65288;&#23433;&#20840;&#65289;&#12290;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65288;CMGs&#65289;&#26159;&#23433;&#20840;MARL&#38382;&#39064;&#30340;&#19968;&#31181;&#33258;&#28982;&#24418;&#24335;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#37325;&#35201;&#30340;CMGs&#31867;&#21035;&#8212;&#8212;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#65288;CMPGs&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#25214;&#21040;CMPGs&#30340;&#32435;&#20160;&#31574;&#30053;&#12290;&#19968;&#20010;&#35797;&#22270;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#25289;&#26684;&#26391;&#26085;&#22522;&#20110;&#21407;&#22987; - &#23545;&#20598;&#26041;&#27861;&#12290;&#27491;&#22914;&#25105;&#20204;&#25152;&#26174;&#31034;&#20986;&#26469;&#30340;&#65292;&#19982;&#21333;&#26234;&#33021;&#20307;&#22330;&#26223;&#30456;&#21453;&#65292;CMPG&#19981;&#28385;&#36275;&#24378;&#23545;&#20598;&#24615;&#65292;&#36825;&#20351;&#24471;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#19981;&#36866;&#24403;&#19988;&#21487;&#33021;&#19981;&#23433;&#20840;&#12290;&#20026;&#20102;&#35299;&#20915;CMPG&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) addresses sequential decision-making problems with multiple agents, where each agent optimizes its own objective. In many real-world instances, the agents may not only want to optimize their objectives, but also ensure safe behavior. For example, in traffic routing, each car (agent) aims to reach its destination quickly (objective) while avoiding collisions (safety). Constrained Markov Games (CMGs) are a natural formalism for safe MARL problems, though generally intractable. In this work, we introduce and study Constrained Markov Potential Games (CMPGs), an important class of CMGs. We first show that a Nash policy for CMPGs can be found via constrained optimization. One tempting approach is to solve it by Lagrangian-based primal-dual methods. As we show, in contrast to the single-agent setting, however, CMPGs do not satisfy strong duality, rendering such approaches inapplicable and potentially unsafe. To solve the CMPG problem, we propose our a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.07745</link><description>&lt;p&gt;
&#26680;&#21270;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Kernelized Reinforcement Learning with Order Optimal Regret Bounds. (arXiv:2306.07745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#20855;&#26377;&#22797;&#26434;&#27169;&#22411;&#21644;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26174;&#31034;&#20986;&#20102;&#23454;&#35777;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#32467;&#26524;&#36890;&#24120;&#38598;&#20013;&#20110;&#20855;&#26377;&#23569;&#37327;&#29366;&#24577;-&#34892;&#20026;&#25110;&#31616;&#21333;&#27169;&#22411;&#65288;&#20363;&#22914;&#32447;&#24615;&#24314;&#27169;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#65289;&#30340;&#35774;&#32622;&#12290; &#20026;&#20102;&#25512;&#23548;&#26377;&#25928;&#22788;&#29702;&#26356;&#24191;&#27867;&#20540;&#20989;&#25968;&#30340;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;RL&#31574;&#30053;&#65292;&#19968;&#20123;&#26368;&#26032;&#24037;&#20316;&#32771;&#34385;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#31216;&#20026;$\pi$-KRVI&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#30340;&#19968;&#31181;&#20048;&#35266;&#20462;&#25913;&#65292;&#24403;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#30001;RKHS&#34920;&#31034;&#26102;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#39640;&#24230;&#38750;&#20809;&#28369;&#20869;&#26680;&#65288;&#20363;&#22914;&#31070;&#32463;&#20999;&#21521;&#20869;&#26680;&#25110;&#26576;&#20123;Mat\'ern&#20869;&#26680;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose $\pi$-KRVI, an optimistic modification of least-squares value iteration, when the state-action value function is represented by an RKHS. We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Mat\'ern kernels) the existing results lead to trivial (superl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#30340;&#38899;&#39057;&#21644;&#27468;&#35789;&#23545;&#40784;&#31995;&#32479;&#65292;&#19981;&#20165;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#20855;&#26377;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07744</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#38899;&#39057;&#19982;&#27468;&#35789;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning-Based Audio to Lyrics Alignment for Multiple Languages. (arXiv:2306.07744v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#30340;&#38899;&#39057;&#21644;&#27468;&#35789;&#23545;&#40784;&#31995;&#32479;&#65292;&#19981;&#20165;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#20855;&#26377;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#27468;&#35789;&#23545;&#40784;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#38899;&#39057;&#19982;&#25991;&#26412;&#39046;&#22495;&#20132;&#21449;&#23884;&#20837;&#30340;&#26032;&#22411;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#24369;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lyrics alignment gained considerable attention in recent years. State-of-the-art systems either re-use established speech recognition toolkits, or design end-to-end solutions involving a Connectionist Temporal Classification (CTC) loss. However, both approaches suffer from specific weaknesses: toolkits are known for their complexity, and CTC systems use a loss designed for transcription which can limit alignment accuracy. In this paper, we use instead a contrastive learning procedure that derives cross-modal embeddings linking the audio and text domains. This way, we obtain a novel system that is simple to train end-to-end, can make use of weakly annotated training data, jointly learns a powerful text model, and is tailored to alignment. The system is not only the first to yield an average absolute error below 0.2 seconds on the standard Jamendo dataset but it is also robust to other languages, even when trained on English data only. Finally, we release word-level alignments for the Ja
&lt;/p&gt;</description></item><item><title>V-LoL&#26159;&#19968;&#20010;&#32467;&#21512;&#35270;&#35273;&#21644;&#36923;&#36753;&#25361;&#25112;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;V-LoL-Trains&#65292;&#35813;&#25968;&#25454;&#38598;&#39318;&#27425;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2306.07743</link><description>&lt;p&gt;
V-LoL: &#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
V-LoL: A Diagnostic Dataset for Visual Logical Learning. (arXiv:2306.07743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07743
&lt;/p&gt;
&lt;p&gt;
V-LoL&#26159;&#19968;&#20010;&#32467;&#21512;&#35270;&#35273;&#21644;&#36923;&#36753;&#25361;&#25112;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;V-LoL-Trains&#65292;&#35813;&#25968;&#25454;&#38598;&#39318;&#27425;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#22312;&#35270;&#35273;AI&#39046;&#22495;&#26377;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19981;&#21516;&#30340;&#32570;&#28857;&#65307;&#21253;&#25324;&#32570;&#23569;&#31934;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#12289;&#25277;&#35937;&#30340;&#27010;&#25324;&#33021;&#21147;&#20197;&#21450;&#29702;&#35299;&#22797;&#26434;&#21644;&#22024;&#26434;&#30340;&#22330;&#26223;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#25429;&#25417;&#21040;&#36825;&#20123;&#26041;&#38754;&#20013;&#30340;&#22810;&#25968;&#12290;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#38598;&#20851;&#27880;&#35270;&#35273;&#22797;&#26434;&#25968;&#25454;&#20294;&#21482;&#26377;&#31616;&#21333;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#24402;&#32435;&#36923;&#36753;&#25968;&#25454;&#38598;&#21253;&#25324;&#22797;&#26434;&#30340;&#36923;&#36753;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#26159;&#32570;&#20047;&#35270;&#35273;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25968;&#25454;&#38598;V-LoL&#65292;&#23427;&#26080;&#32541;&#22320;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#36923;&#36753;&#30340;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#25512;&#20986;&#20102;V-LoL&#30340;&#31532;&#19968;&#20010;&#23454;&#20363;&#65292;&#21517;&#20026;V-LoL-Trains&#65292;&#23427;&#26159;&#31526;&#21495;AI&#20013;&#19968;&#20010;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30340;&#35270;&#35273;&#21576;&#29616;&#65292;&#21363;Michalski&#28779;&#36710;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#32467;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;V-LoL-Trains&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the successes of recent developments in visual AI, different shortcomings still exist; from missing exact logical reasoning, to abstract generalization abilities, to understanding complex and noisy scenes. Unfortunately, existing benchmarks, were not designed to capture more than a few of these aspects. Whereas deep learning datasets focus on visually complex data but simple visual reasoning tasks, inductive logic datasets involve complex logical learning tasks, however, lack the visual component. To address this, we propose the visual logical learning dataset, V-LoL, that seamlessly combines visual and logical challenges. Notably, we introduce the first instantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classic benchmark in symbolic AI, the Michalski train problem. By incorporating intricate visual scenes and flexible logical reasoning tasks within a versatile framework, V-LoL-Trains provides a platform for investigating a wide range of visual logical learning ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#24773;&#22659;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#29992;&#20110;&#36229;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#27493;&#38271;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#20219;&#21153;&#33539;&#22260;&#20869;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07741</link><description>&lt;p&gt;
&#22312;&#24773;&#22659;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#23398;&#20064;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#27493;&#38271;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Stepsize Learning for Policy Gradient Methods in Contextual Markov Decision Processes. (arXiv:2306.07741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#24773;&#22659;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#29992;&#20110;&#36229;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#27493;&#38271;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#20219;&#21153;&#33539;&#22260;&#20869;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#22312;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#24378;&#29702;&#35770;&#22522;&#30784;&#21644;&#33391;&#22909;&#24615;&#36136;&#32780;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#31934;&#30830;&#21644;&#38382;&#39064;&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#25165;&#33021;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#24448;&#24448;&#22312;&#34987;&#35201;&#27714;&#23436;&#25104;&#19968;&#31995;&#21015;&#24322;&#36136;&#20219;&#21153;&#26102;&#38590;&#20197;&#32988;&#20219;&#12290;&#29305;&#21035;&#26159;&#65292;&#27493;&#38271;&#30340;&#36873;&#25321;&#23545;&#20110;&#23427;&#20204;&#23398;&#20064;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#33021;&#21147;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24433;&#21709;&#65292;&#24433;&#21709;&#35757;&#32451;&#36807;&#31243;&#30340;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#32463;&#24120;&#26159;&#19981;&#33391;&#32467;&#26524;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#65292;&#31216;&#20026;&#20803;MDP&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#20219;&#20309;&#20855;&#26377;&#24773;&#22659;&#36807;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#12290;&#22312;&#25552;&#20379;&#20102;&#19981;&#21516;&#20219;&#21153;&#24615;&#33021;&#24046;&#24322;&#30340;&#29702;&#35770;&#21033;&#26222;&#24076;&#33576;&#30028;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#19968;&#32452;&#22522;&#20934;&#27979;&#35797;&#20013;&#35757;&#32451;&#25209;&#37327;RL&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20803;MDP&#20844;&#24335;&#65292;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#27493;&#38271;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#33539;&#22260;&#20869;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-based algorithms are among the most widely adopted techniques in model-free RL, thanks to their strong theoretical groundings and good properties in continuous action spaces. Unfortunately, these methods require precise and problem-specific hyperparameter tuning to achieve good performance, and tend to struggle when asked to accomplish a series of heterogeneous tasks. In particular, the selection of the step size has a crucial impact on their ability to learn a highly performing policy, affecting the speed and the stability of the training process, and often being the main culprit for poor results. In this paper, we tackle these issues with a Meta Reinforcement Learning approach, by introducing a new formulation, known as meta-MDP, that can be used to solve any hyperparameter selection problem in RL with contextual processes. After providing a theoretical Lipschitz bound to the difference of performance in different tasks, we adopt the proposed framework to train a batch RL algo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26292;&#38706;&#27169;&#22411;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#26469;&#27979;&#35797;&#20854;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#21644;&#21709;&#24212;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34429;&#33021;&#36798;&#39640;&#31934;&#24230;&#65292;&#20294;&#20854;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36824;&#19981;&#36275;&#20197;&#24212;&#29992;&#20110;&#23454;&#38469;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.07737</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robustness and Generalization Performance of Deep Learning Models on Cyber-Physical Systems: A Comparative Study. (arXiv:2306.07737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26292;&#38706;&#27169;&#22411;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#26469;&#27979;&#35797;&#20854;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#21644;&#21709;&#24212;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34429;&#33021;&#36798;&#39640;&#31934;&#24230;&#65292;&#20294;&#20854;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36824;&#19981;&#36275;&#20197;&#24212;&#29992;&#20110;&#23454;&#38469;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#22312;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#24212;&#29992;&#21463;&#21040;&#36825;&#20123;&#26041;&#27861;&#40065;&#26834;&#24615;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#26469;&#33258;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#27169;&#22411;&#22788;&#29702;&#19968;&#31995;&#21015;&#25200;&#21160;&#30340;&#33021;&#21147;&#65292;&#22914;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;&#22122;&#22768;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#23427;&#20204;&#26292;&#38706;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#30340;&#26679;&#26412;&#26469;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#21644;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#36825;&#20123;&#26679;&#26412;&#21253;&#25324;&#20559;&#31163;&#26631;&#20934;&#31995;&#32479;&#25805;&#20316;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22522;&#30784;&#29289;&#29702;&#31995;&#32479;&#30340;&#26680;&#24515;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#27169;&#22411;&#23545;&#20960;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#21709;&#24212;&#33021;&#21147;&#65292;&#21253;&#25324;&#28155;&#21152;&#22122;&#22768;&#21644;&#26102;&#38388;&#25197;&#26354;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#20010;&#27169;&#25311;&#30340;&#19977;&#32592;&#31995;&#32479;&#65292;&#20316;&#20026;&#30740;&#31350;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#39640;&#31934;&#24230;&#65292;&#20294;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#20197;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models have seen increased attention for time series forecasting, yet the application on cyber-physical systems (CPS) is hindered by the lacking robustness of these methods. Thus, this study evaluates the robustness and generalization performance of DL architectures on multivariate time series data from CPS. Our investigation focuses on the models' ability to handle a range of perturbations, such as sensor faults and noise, and assesses their impact on overall performance. Furthermore, we test the generalization and transfer learning capabilities of these models by exposing them to out-of-distribution (OOD) samples. These include deviations from standard system operations, while the core dynamics of the underlying physical system are preserved. Additionally, we test how well the models respond to several data augmentation techniques, including added noise and time warping. Our experimental framework utilizes a simulated three-tank system, proposed as a novel benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#22270;&#20998;&#24067;&#30340;&#32622;&#25442;&#31561;&#21464;&#31163;&#25955;&#33258;&#32534;&#30721;&#22120;(VQ-GAE)&#65292;&#23427;&#37319;&#29992;&#21521;&#37327;&#37327;&#21270;&#38450;&#27490;&#23558;&#31163;&#25955;&#23545;&#35937;&#26144;&#23556;&#21040;&#36830;&#32493;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#25429;&#33719;&#20102;&#22270;&#30340;&#20840;&#23616;&#32467;&#26500;&#65292;&#23454;&#39564;&#34920;&#26126;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07735</link><description>&lt;p&gt;
&#21521;&#37327;&#37327;&#21270;&#22270;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Vector-Quantized Graph Auto-Encoder. (arXiv:2306.07735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#22270;&#20998;&#24067;&#30340;&#32622;&#25442;&#31561;&#21464;&#31163;&#25955;&#33258;&#32534;&#30721;&#22120;(VQ-GAE)&#65292;&#23427;&#37319;&#29992;&#21521;&#37327;&#37327;&#21270;&#38450;&#27490;&#23558;&#31163;&#25955;&#23545;&#35937;&#26144;&#23556;&#21040;&#36830;&#32493;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#25429;&#33719;&#20102;&#22270;&#30340;&#20840;&#23616;&#32467;&#26500;&#65292;&#23454;&#39564;&#34920;&#26126;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24314;&#27169;&#22270;&#30340;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#32622;&#25442;&#31561;&#21464;&#31163;&#25955;&#33258;&#32534;&#30721;&#22120;(VQ-GAE)&#65292;&#26088;&#22312;&#35774;&#35745;&#29992;&#20110;&#24314;&#27169;&#22270;&#30340;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#33258;&#32534;&#30721;&#22120;&#32469;&#36807;&#20102;&#22270;&#34920;&#31034;&#30340;&#25490;&#24207;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;GNN&#25429;&#25417;&#22270;&#30340;&#23616;&#37096;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#21521;&#37327;&#37327;&#21270;&#26469;&#38450;&#27490;&#23558;&#31163;&#25955;&#23545;&#35937;&#26144;&#23556;&#21040;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#27492;&#22806;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20351;&#29992;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#28508;&#22312;&#34920;&#31034;&#25429;&#33719;&#22270;&#30340;&#20840;&#23616;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#29992;&#20110;&#22270;&#29983;&#25104;&#30340;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19968;&#20123;&#26368;&#31361;&#20986;&#30340;&#35780;&#20272;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we addresses the problem of modeling distributions of graphs. We introduce the Vector-Quantized Graph Auto-Encoder (VQ-GAE), a permutation-equivariant discrete auto-encoder and designed to model the distribution of graphs. By exploiting the permutation-equivariance of graph neural networks (GNNs), our autoencoder circumvents the problem of the ordering of the graph representation. We leverage the capability of GNNs to capture local structures of graphs while employing vector-quantization to prevent the mapping of discrete objects to a continuous latent space. Furthermore, the use of autoregressive models enables us to capture the global structure of graphs via the latent representation. We evaluate our model on standard datasets used for graph generation and observe that it achieves excellent performance on some of the most salient evaluation metrics compared to the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#23558;&#24515;&#29575;&#28436;&#21464;&#34920;&#31034;&#20026;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#65292;&#20351;&#29992;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;PPG&#20449;&#21495;&#31383;&#21475;&#20869;&#30340;&#21487;&#33021;&#24515;&#29575;&#20540;&#20998;&#24067;&#65292;&#28982;&#21518;&#36890;&#36807;&#32622;&#20449;&#20256;&#25773;&#32467;&#21512;&#32479;&#35745;&#20998;&#24067;&#30417;&#27979;&#24515;&#29575;&#21464;&#21270;&#20197;&#20248;&#21270;&#36825;&#20123;&#20272;&#35745;&#65292;&#24182;&#33719;&#24471;&#28085;&#30422;&#24515;&#29575;&#20540;&#33539;&#22260;&#30340;&#37327;&#21270;&#27010;&#29575;&#20998;&#24067;&#20197;&#25429;&#33719;&#22266;&#26377;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#33391;&#22909;&#26657;&#20934;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.07730</link><description>&lt;p&gt;
BeliefPPG: &#36890;&#36807;&#32622;&#20449;&#20256;&#25773;&#20174;PPG&#20449;&#21495;&#20013;&#33719;&#24471;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#24515;&#29575;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
BeliefPPG: Uncertainty-aware Heart Rate Estimation from PPG signals via Belief Propagation. (arXiv:2306.07730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07730
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#23558;&#24515;&#29575;&#28436;&#21464;&#34920;&#31034;&#20026;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#65292;&#20351;&#29992;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;PPG&#20449;&#21495;&#31383;&#21475;&#20869;&#30340;&#21487;&#33021;&#24515;&#29575;&#20540;&#20998;&#24067;&#65292;&#28982;&#21518;&#36890;&#36807;&#32622;&#20449;&#20256;&#25773;&#32467;&#21512;&#32479;&#35745;&#20998;&#24067;&#30417;&#27979;&#24515;&#29575;&#21464;&#21270;&#20197;&#20248;&#21270;&#36825;&#20123;&#20272;&#35745;&#65292;&#24182;&#33719;&#24471;&#28085;&#30422;&#24515;&#29575;&#20540;&#33539;&#22260;&#30340;&#37327;&#21270;&#27010;&#29575;&#20998;&#24067;&#20197;&#25429;&#33719;&#22266;&#26377;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#33391;&#22909;&#26657;&#20934;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#23558;&#24515;&#29575;&#30340;&#28436;&#21464;&#34920;&#31034;&#20026;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20026;&#32473;&#23450;&#30340;PPG&#20449;&#21495;&#31383;&#21475;&#23548;&#20986;&#21487;&#33021;&#24515;&#29575;&#20540;&#30340;&#20998;&#24067;&#12290;&#20351;&#29992;&#32622;&#20449;&#20256;&#25773;&#65292;&#22312;&#26102;&#38388;&#19978;&#19979;&#25991;&#20013;&#32467;&#21512;&#24515;&#29575;&#21464;&#21270;&#30340;&#32479;&#35745;&#20998;&#24067;&#20197;&#20248;&#21270;&#36825;&#20123;&#20272;&#35745;&#12290;&#20174;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31181;&#37327;&#21270;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#28085;&#30422;&#20102;&#21487;&#33021;&#24515;&#29575;&#20540;&#30340;&#36825;&#20010;&#33539;&#22260;&#65292;&#36825;&#21487;&#20197;&#25429;&#33719;&#22266;&#26377;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#24847;&#20041;&#19988;&#33391;&#22909;&#26657;&#20934;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#20132;&#21449;&#39564;&#35777;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel learning-based method that achieves state-of-the-art performance on several heart rate estimation benchmarks extracted from photoplethysmography signals (PPG). We consider the evolution of the heart rate in the context of a discrete-time stochastic process that we represent as a hidden Markov model. We derive a distribution over possible heart rate values for a given PPG signal window through a trained neural network. Using belief propagation, we incorporate the statistical distribution of heart rate changes to refine these estimates in a temporal context. From this, we obtain a quantized probability distribution over the range of possible heart rate values that captures a meaningful and well-calibrated estimate of the inherent predictive uncertainty. We show the robustness of our method on eight public datasets with three different cross-validation experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#20013;&#28145;&#24230;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#28151;&#21512;&#25152;&#26377;&#36716;&#25442;&#30340;&#25968;&#25454;&#26102;&#27169;&#22411;&#24471;&#20998;&#26368;&#39640;&#65292;180&#176;&#26059;&#36716;&#22686;&#24378;&#30340;&#25968;&#25454;&#25552;&#20379;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07724</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#19982;&#22270;&#20687;&#21464;&#25442;&#23545;&#28145;&#24230;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of Data Enrichment with Image Transformations on the Performance of Deep Networks. (arXiv:2306.07724v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#20013;&#28145;&#24230;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#28151;&#21512;&#25152;&#26377;&#36716;&#25442;&#30340;&#25968;&#25454;&#26102;&#27169;&#22411;&#24471;&#20998;&#26368;&#39640;&#65292;180&#176;&#26059;&#36716;&#22686;&#24378;&#30340;&#25968;&#25454;&#25552;&#20379;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24182;&#19981;&#24635;&#26159;&#20197;&#26576;&#31181;&#26631;&#20934;&#26684;&#24335;&#21644;&#26041;&#21521;&#20986;&#29616;&#12290;&#28145;&#24230;&#32593;&#32476;&#38656;&#35201;&#34987;&#35757;&#32451;&#20197;&#32771;&#34385;&#21040;&#24847;&#22806;&#30340;&#26041;&#21521;&#25110;&#26684;&#24335;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#35757;&#32451;&#25968;&#25454;&#24212;&#34987;&#22686;&#24378;&#21253;&#25324;&#19981;&#21516;&#30340;&#26465;&#20214;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25968;&#25454;&#22686;&#24378;&#23545;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#20013;&#28145;&#24230;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#24635;&#20849;&#20351;&#29992;&#20102;&#20845;&#31181;&#22522;&#26412;&#22270;&#20687;&#21464;&#25442;&#26041;&#27861;&#36827;&#34892;&#22686;&#24378;&#12290;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#20845;&#31181;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#22686;&#24378;ILSVRC2012&#25968;&#25454;&#38598;&#30340;&#21464;&#37327;&#26469;&#35757;&#32451;&#20004;&#20010;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#12290;&#21457;&#29616;&#21333;&#19968;&#30340;&#22270;&#20687;&#21464;&#25442;&#20013;&#65292;180&#176;&#26059;&#36716;&#22686;&#24378;&#30340;&#25968;&#25454;&#25552;&#20379;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#27169;&#22411;&#22312;&#35757;&#32451;&#25152;&#26377;&#36716;&#25442;&#30340;&#28151;&#21512;&#25968;&#25454;&#26102;&#24471;&#20998;&#26368;&#39640;&#65292;&#26368;&#19981;&#25104;&#21151;&#30340;&#32467;&#26524;&#26159;&#27169;&#22411;&#22312;&#32763;&#36716;&#19978;&#19979;&#29983;&#25104;&#30340;&#22686;&#24378;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Images cannot always be expected to come in a certain standard format and orientation. Deep networks need to be trained to take into account unexpected variations in orientation or format. For this purpose, training data should be enriched to include different conditions. In this study, the effects of data enrichment on the performance of deep networks in the super resolution problem were investigated experimentally. A total of six basic image transformations were used for the enrichment procedures. In the experiments, two deep network models were trained with variants of the ILSVRC2012 dataset enriched by these six image transformation processes. Considering a single image transformation, it has been observed that the data enriched with 180 degree rotation provides the best results. The most unsuccessful result was obtained when the models were trained on the enriched data generated by the flip upside down process. Models scored highest when trained with a mix of all transformations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#25506;&#35752;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07723</link><description>&lt;p&gt;
&#23545;&#25239;&#40065;&#26834;&#24615;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Theoretical Foundations of Adversarially Robust Learning. (arXiv:2306.07723v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#25506;&#35752;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#36827;&#23637;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#26679;&#26412;&#26159;&#33030;&#24369;&#30340;&#65306;&#23545;&#27979;&#35797;&#26679;&#26412;&#36827;&#34892;&#32454;&#24494;&#20294;&#26377;&#24847;&#30340;&#25200;&#21160;&#65292;&#23601;&#20250;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38169;&#35823;&#20998;&#31867;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#20174;&#29702;&#35770;&#35282;&#24230;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#25105;&#20204;&#21487;&#20197;&#24076;&#26395;&#20445;&#35777;&#20160;&#20040;&#26679;&#30340;&#40065;&#26834;&#24615;&#24615;&#36136;&#65292;&#24182;&#25552;&#20379;&#21487;&#23454;&#29616;&#31639;&#27861;&#20445;&#35777;&#36825;&#20123;&#24615;&#36136;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite extraordinary progress, current machine learning systems have been shown to be brittle against adversarial examples: seemingly innocuous but carefully crafted perturbations of test examples that cause machine learning predictors to misclassify. Can we learn predictors robust to adversarial examples? and how? There has been much empirical interest in this contemporary challenge in machine learning, and in this thesis, we address it from a theoretical perspective.  In this thesis, we explore what robustness properties can we hope to guarantee against adversarial examples and develop an understanding of how to algorithmically guarantee them. We illustrate the need to go beyond traditional approaches and principles such as empirical risk minimization and uniform convergence, and make contributions that can be categorized as follows: (1) introducing problem formulations capturing aspects of emerging practical challenges in robust learning, (2) designing new learning algorithms with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#39044;&#31639;&#30340;&#37325;&#22797;&#20108;&#20215;&#25293;&#21334;&#20013;&#30340;&#21327;&#35843;&#22312;&#32447;&#31454;&#20215;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#20445;&#35777;&#27599;&#20010;&#23458;&#25143;&#25928;&#29992;&#26368;&#22823;&#21270;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23454;&#29616;&#20102;&#26368;&#22823;&#30340;&#32852;&#30431;&#31119;&#21033;&#12290;</title><link>http://arxiv.org/abs/2306.07709</link><description>&lt;p&gt;
&#24102;&#39044;&#31639;&#30340;&#37325;&#22797;&#20108;&#20215;&#25293;&#21334;&#20013;&#30340;&#21327;&#35843;&#21160;&#24577;&#20986;&#20215;
&lt;/p&gt;
&lt;p&gt;
Coordinated Dynamic Bidding in Repeated Second-Price Auctions with Budgets. (arXiv:2306.07709v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#39044;&#31639;&#30340;&#37325;&#22797;&#20108;&#20215;&#25293;&#21334;&#20013;&#30340;&#21327;&#35843;&#22312;&#32447;&#31454;&#20215;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#20445;&#35777;&#27599;&#20010;&#23458;&#25143;&#25928;&#29992;&#26368;&#22823;&#21270;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23454;&#29616;&#20102;&#26368;&#22823;&#30340;&#32852;&#30431;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#24191;&#21578;&#24066;&#22330;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#24191;&#21578;&#20027;&#38599;&#29992;&#31454;&#20215;&#20195;&#29702;&#26469;&#21442;&#19982;&#24191;&#21578;&#25293;&#21334;&#12290;&#36825;&#20123;&#20195;&#29702;&#19987;&#38376;&#35774;&#35745;&#22312;&#32447;&#31639;&#27861;&#24182;&#20195;&#34920;&#20854;&#23458;&#25143;&#20986;&#20215;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#36890;&#24120;&#25317;&#26377;&#22810;&#20010;&#24191;&#21578;&#20027;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#22905;&#26377;&#21487;&#33021;&#21327;&#35843;&#20986;&#20215;&#65292;&#24110;&#21161;&#22905;&#30340;&#23458;&#25143;&#33719;&#24471;&#27604;&#29420;&#31435;&#20986;&#20215;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#39044;&#31639;&#30340;&#37325;&#22797;&#20108;&#20215;&#25293;&#21334;&#20013;&#30340;&#21327;&#35843;&#22312;&#32447;&#31454;&#20215;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31639;&#27861;&#65292;&#20445;&#35777;&#27599;&#20010;&#23458;&#25143;&#30340;&#25928;&#29992;&#37117;&#27604;&#29420;&#31435;&#31454;&#20215;&#33719;&#24471;&#30340;&#26368;&#20248;&#25928;&#29992;&#39640;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#22823;&#30340;&#32852;&#30431;&#31119;&#21033;&#65292;&#24182;&#35752;&#35770;&#20102;&#25237;&#26631;&#32773;&#22312;&#23545;&#31216;&#24773;&#20917;&#19979;&#35823;&#25253;&#39044;&#31639;&#30340;&#28608;&#21169;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#32467;&#21512;&#20102;&#22312;&#32447;&#23398;&#20064;&#21644;&#24179;&#34913;&#20998;&#26512;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#19982;&#22810;&#32500;&#22522;&#20934;&#31454;&#20105;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online ad markets, a rising number of advertisers are employing bidding agencies to participate in ad auctions. These agencies are specialized in designing online algorithms and bidding on behalf of their clients. Typically, an agency usually has information on multiple advertisers, so she can potentially coordinate bids to help her clients achieve higher utilities than those under independent bidding.  In this paper, we study coordinated online bidding algorithms in repeated second-price auctions with budgets. We propose algorithms that guarantee every client a higher utility than the best she can get under independent bidding. We show that these algorithms achieve maximal coalition welfare and discuss bidders' incentives to misreport their budgets, in symmetric cases. Our proofs combine the techniques of online learning and equilibrium analysis, overcoming the difficulty of competing with a multi-dimensional benchmark. The performance of our algorithms is further evaluated by expe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07699</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Time-aware Graph Structure Learning via Sequence Prediction on Temporal Graphs. (arXiv:2306.07699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26088;&#22312;&#27169;&#25311;&#22270;&#20687;&#30340;&#20256;&#36882;&#24615;&#36136;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22270;&#20687;&#32467;&#26500;&#24448;&#24448;&#26159;&#19981;&#23436;&#25972;&#21644;&#22024;&#26434;&#30340;&#65292;&#36825;&#38459;&#30861;&#20102;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;TGSL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Graph Learning, which aims to model the time-evolving nature of graphs, has gained increasing attention and achieved remarkable performance recently. However, in reality, graph structures are often incomplete and noisy, which hinders temporal graph networks (TGNs) from learning informative representations. Graph contrastive learning uses data augmentation to generate plausible variations of existing data and learn robust representations. However, rule-based augmentation approaches may be suboptimal as they lack learnability and fail to leverage rich information from downstream tasks. To address these issues, we propose a Time-aware Graph Structure Learning (TGSL) approach via sequence prediction on temporal graphs, which learns better graph structures for downstream tasks through adding potential temporal edges. In particular, it predicts time-aware context embedding based on previously observed interactions and uses the Gumble-Top-K to select the closest candidate edges to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2306.07691</link><description>&lt;p&gt;
StyleTTS 2&#65306;&#36890;&#36807;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models. (arXiv:2306.07691v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StyleTTS2&#65292;&#19968;&#31181;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;TTS&#21512;&#25104;&#12290; StyleTTS 2&#36890;&#36807;&#23558;&#26679;&#24335;&#24314;&#27169;&#20026;&#28508;&#22312;&#30340;&#38543;&#26426;&#21464;&#37327;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#26080;&#38656;&#21442;&#32771;&#35821;&#38899;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#28508;&#22312;&#25193;&#25955;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#30340;&#22810;&#26679;&#21270;&#35821;&#38899;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;SLMs&#65288;&#20363;&#22914;WavLM&#65289;&#20316;&#20026;&#37492;&#21035;&#22120;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#22411;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#38899;&#30340;&#33258;&#28982;&#24230;&#12290; StyleTTS 2&#22312;&#21333;&#25196;&#22768;&#22120;LJSpeech&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#20154;&#31867;&#24405;&#38899;&#65292;&#24182;&#22312;&#22810;&#25196;&#22768;&#22120;VCTK&#25968;&#25454;&#38598;&#19978;&#19982;&#20043;&#21305;&#37197;&#65292;&#32463;&#36807;&#27597;&#35821;&#20026;&#33521;&#35821;&#30340;&#20154;&#21592;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#24403;&#22312;LibriTTS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#20197;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#38646;&#26679;&#26412;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#19979;&#30340;&#19968;&#27425;&#25490;&#21015;&#21704;&#24076;&#26041;&#27861;&#21644;&#22522;&#20110; Bin &#30340;&#19968;&#33268;&#21152;&#26435;&#37319;&#26679;&#65292;&#20026;&#22823;&#35268;&#27169;&#25628;&#32034;&#21644;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#12289;&#26356;&#26041;&#20415;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.07674</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#19968;&#27425;&#25490;&#21015;&#21704;&#24076;&#21644;&#22522;&#20110; Bin &#30340;&#19968;&#33268;&#21152;&#26435;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Differentially Private One Permutation Hashing and Bin-wise Consistent Weighted Sampling. (arXiv:2306.07674v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#19979;&#30340;&#19968;&#27425;&#25490;&#21015;&#21704;&#24076;&#26041;&#27861;&#21644;&#22522;&#20110; Bin &#30340;&#19968;&#33268;&#21152;&#26435;&#37319;&#26679;&#65292;&#20026;&#22823;&#35268;&#27169;&#25628;&#32034;&#21644;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#12289;&#26356;&#26041;&#20415;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#21704;&#24076;&#65288;MinHash&#65289;&#26159;&#19968;&#31181;&#26631;&#20934;&#31639;&#27861;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#20855;&#26377;&#20108;&#36827;&#21046;&#65288;0/1&#65289;Jaccard&#30456;&#20284;&#24230;&#30340;&#22823;&#35268;&#27169;&#25628;&#32034;&#21644;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#12290;MinHash &#30340;&#24120;&#35265;&#29992;&#36884;&#26159;&#22788;&#29702;&#22823;&#35268;&#27169; n-gram &#25991;&#26412;&#34920;&#31034;&#65292;&#20197;&#20415;&#23454;&#36341;&#32773;&#19981;&#24517;&#23454;&#29616;&#21407;&#22987;&#25968;&#25454;&#65288;&#36825;&#23558;&#26159;&#31105;&#27490;&#30340;&#65289;&#12290;MinHash &#30340;&#21478;&#19968;&#20010;&#27969;&#34892;&#29992;&#36884;&#26159;&#26500;&#24314;&#21704;&#24076;&#34920;&#65292;&#20197;&#23454;&#29616;&#20122;&#32447;&#24615;&#26102;&#38388;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#12290;MinHash &#36824;&#29992;&#20316;&#26500;&#24314;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24037;&#20855;&#12290;&#26631;&#20934;&#30340; MinHash &#23454;&#29616;&#38656;&#35201;&#24212;&#29992; K &#20010;&#38543;&#26426;&#25490;&#21015;&#65292;&#32780;&#19968;&#27425;&#25490;&#21015;&#21704;&#24076;&#26041;&#27861;&#65288;OPH&#65289;&#21017;&#26159; MinHash &#30340;&#19968;&#31181;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#23558;&#25968;&#25454;&#30690;&#37327;&#21010;&#20998;&#20026; K &#20010; bin&#65292;&#24182;&#22312;&#27599;&#20010; bin &#20013;&#29983;&#25104;&#21704;&#24076;&#20540;&#12290;OPH &#26356;&#21152;&#39640;&#25928;&#65292;&#26356;&#21152;&#20415;&#21033;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#19982; OPH&#65288;&#20197;&#21450; MinHash&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#19968;&#27425;&#25490;&#21015;&#21704;&#24076;&#21644;&#22522;&#20110; Bin &#30340;&#19968;&#33268;&#21152;&#26435;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minwise hashing (MinHash) is a standard algorithm widely used in the industry, for large-scale search and learning applications with the binary (0/1) Jaccard similarity. One common use of MinHash is for processing massive n-gram text representations so that practitioners do not have to materialize the original data (which would be prohibitive). Another popular use of MinHash is for building hash tables to enable sub-linear time approximate near neighbor (ANN) search. MinHash has also been used as a tool for building large-scale machine learning systems. The standard implementation of MinHash requires applying $K$ random permutations. In comparison, the method of one permutation hashing (OPH), is an efficient alternative of MinHash which splits the data vectors into $K$ bins and generates hash values within each bin. OPH is substantially more efficient and also more convenient to use.  In this paper, we combine the differential privacy (DP) with OPH (as well as MinHash), to propose the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21450;&#24494;&#35843;&#20013;&#20173;&#20855;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;10%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2306.07664</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#65306;&#19968;&#20010;&#32463;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Rethink the Effectiveness of Text Data Augmentation: An Empirical Analysis. (arXiv:2306.07664v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21450;&#24494;&#35843;&#20013;&#20173;&#20855;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;10%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#23545;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#34920;&#29616;&#30340;&#24433;&#21709;&#19968;&#30452;&#26159;&#19968;&#20010;&#20105;&#35770;&#30340;&#35805;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#24494;&#35843;&#26041;&#27861;&#32467;&#21512;&#22238;&#35793;&#22312;7&#20010;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#31867;&#22411;&#65292;&#28085;&#30422;&#21333;&#21477;&#21644;&#21477;&#23376;&#23545;&#20219;&#21153;&#12290;&#19982;&#20248;&#20808;&#30340;&#20551;&#35774;&#30456;&#21453;&#65292;&#21363;&#25968;&#25454;&#22686;&#24378;&#23545;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#34920;&#29616;&#27809;&#26377;&#36129;&#29486;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25345;&#32493;&#22312;&#22686;&#24378;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#34920;&#29616;&#12290;&#22312;&#26368;&#26377;&#21033;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#24494;&#35843;&#24615;&#33021;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#19979;&#25552;&#39640;10%&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20986;&#25968;&#25454;&#22686;&#24378;&#20316;&#20026;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24378;&#22823;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, language models (LMs) have made remarkable progress in advancing the field of natural language processing (NLP). However, the impact of data augmentation (DA) techniques on the fine-tuning (FT) performance of these LMs has been a topic of ongoing debate. In this study, we evaluate the effectiveness of three different FT methods in conjugation with back-translation across an array of 7 diverse NLP tasks, including classification and regression types, covering single-sentence and sentence-pair tasks. Contrary to prior assumptions that DA does not contribute to the enhancement of LMs' FT performance, our findings reveal that continued pre-training on augmented data can effectively improve the FT performance of the downstream tasks. In the most favourable case, continued pre-training improves the performance of FT by more than 10% in the few-shot learning setting. Our finding highlights the potential of DA as a powerful tool for bolstering LMs' performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#22411;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;Malafide&#65292;&#21487;&#20197;&#25915;&#20987;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASV&#65289;&#27450;&#35784;&#23545;&#31574;&#65288;CMs&#65289;&#12290;&#35813;&#25915;&#20987;&#21487;&#20197;&#29992;&#20110;ompromise CM&#30340;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#38899;&#30340;&#20854;&#20182;&#23646;&#24615;&#65292;&#20363;&#22914;&#36136;&#37327;&#21644;&#35828;&#35805;&#20154;&#30340;&#22768;&#38899;&#12290; Malafide&#28388;&#27874;&#22120;&#29420;&#31435;&#20110;&#36755;&#20837;&#35805;&#35821;&#21644;&#25345;&#32493;&#26102;&#38388;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#35843;&#25972;&#21040;&#22522;&#30784;&#27450;&#39575;&#25915;&#20987;&#65292;&#21482;&#38656;&#35201;&#20248;&#21270;&#23569;&#37327;&#28388;&#27874;&#22120;&#31995;&#25968;&#12290;&#38598;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;CM&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#40657;&#30418;&#21644;&#30333;&#30418;&#35774;&#32622;&#19979;&#26356;&#21152;&#31283;&#20581;&#12290;</title><link>http://arxiv.org/abs/2306.07655</link><description>&lt;p&gt;
Malafide: &#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#21367;&#31215;&#22122;&#22768;&#25915;&#20987;&#65292;&#29992;&#20110;&#28145;&#24230;&#20266;&#36896;&#21644;&#27450;&#39575;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems. (arXiv:2306.07655v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#22411;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;Malafide&#65292;&#21487;&#20197;&#25915;&#20987;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASV&#65289;&#27450;&#35784;&#23545;&#31574;&#65288;CMs&#65289;&#12290;&#35813;&#25915;&#20987;&#21487;&#20197;&#29992;&#20110;ompromise CM&#30340;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#38899;&#30340;&#20854;&#20182;&#23646;&#24615;&#65292;&#20363;&#22914;&#36136;&#37327;&#21644;&#35828;&#35805;&#20154;&#30340;&#22768;&#38899;&#12290; Malafide&#28388;&#27874;&#22120;&#29420;&#31435;&#20110;&#36755;&#20837;&#35805;&#35821;&#21644;&#25345;&#32493;&#26102;&#38388;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#35843;&#25972;&#21040;&#22522;&#30784;&#27450;&#39575;&#25915;&#20987;&#65292;&#21482;&#38656;&#35201;&#20248;&#21270;&#23569;&#37327;&#28388;&#27874;&#22120;&#31995;&#25968;&#12290;&#38598;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;CM&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#40657;&#30418;&#21644;&#30333;&#30418;&#35774;&#32622;&#19979;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;Malafide&#65292;&#29992;&#20110;&#25915;&#20987;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASV&#65289;&#27450;&#35784;&#23545;&#31574;&#65288;CMs&#65289;&#12290;&#36890;&#36807;&#24341;&#20837;&#32463;&#36807;&#20248;&#21270;&#30340;&#32447;&#24615;&#26102;&#19981;&#21464;&#28388;&#27874;&#22120;&#65292;&#24341;&#20837;&#21367;&#31215;&#22122;&#22768;&#65292;Malafide&#25915;&#20987;&#21487;&#20197;&#29992;&#20110;ompromise CM&#30340;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#38899;&#30340;&#20854;&#20182;&#23646;&#24615;&#65292;&#20363;&#22914;&#36136;&#37327;&#21644;&#35828;&#35805;&#20154;&#30340;&#22768;&#38899;&#12290;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#20854;&#20182;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#21516;&#65292;Malafide&#28388;&#27874;&#22120;&#29420;&#31435;&#20110;&#36755;&#20837;&#35805;&#35821;&#21644;&#25345;&#32493;&#26102;&#38388;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#35843;&#25972;&#21040;&#22522;&#30784;&#27450;&#39575;&#25915;&#20987;&#65292;&#21482;&#38656;&#35201;&#20248;&#21270;&#23569;&#37327;&#28388;&#27874;&#22120;&#31995;&#25968;&#12290;&#21363;&#20351;&#22914;&#27492;&#65292;&#23427;&#20204;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#38477;&#20302;&#20102;CM&#24615;&#33021;&#20272;&#35745;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#36824;&#21487;&#20197;&#37197;&#32622;&#20026;&#20811;&#26381;&#38598;&#25104;&#30340;CM&#21644;ASV&#23376;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;CM&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#22312;&#40657;&#30418;&#21644;&#30333;&#30418;&#35774;&#32622;&#19979;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Malafide, a universal adversarial attack against automatic speaker verification (ASV) spoofing countermeasures (CMs). By introducing convolutional noise using an optimised linear time-invariant filter, Malafide attacks can be used to compromise CM reliability while preserving other speech attributes such as quality and the speaker's voice. In contrast to other adversarial attacks proposed recently, Malafide filters are optimised independently of the input utterance and duration, are tuned instead to the underlying spoofing attack, and require the optimisation of only a small number of filter coefficients. Even so, they degrade CM performance estimates by an order of magnitude, even in black-box settings, and can also be configured to overcome integrated CM and ASV subsystems. Integrated solutions that use self-supervised learning CMs, however, are more robust, under both black-box and white-box settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#27604;&#36739;&#20116;&#31181;&#20998;&#31867;&#31639;&#27861;&#65292;&#20351;&#29992;Kubernetes&#38598;&#32676;&#26085;&#24535;&#33258;&#21160;&#21028;&#23450;&#24494;&#26381;&#21153;&#27979;&#35797;&#22833;&#36133;&#21407;&#22240;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#22312;&#38656;&#35201;&#36739;&#23569;&#35745;&#31639;&#36164;&#28304;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#20135;&#29983;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07653</link><description>&lt;p&gt;
&#20351;&#29992;Kubernetes&#38598;&#32676;&#26085;&#24535;&#33258;&#21160;&#21270;&#24494;&#26381;&#21153;&#27979;&#35797;&#22833;&#36133;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Automating Microservices Test Failure Analysis using Kubernetes Cluster Logs. (arXiv:2306.07653v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#27604;&#36739;&#20116;&#31181;&#20998;&#31867;&#31639;&#27861;&#65292;&#20351;&#29992;Kubernetes&#38598;&#32676;&#26085;&#24535;&#33258;&#21160;&#21028;&#23450;&#24494;&#26381;&#21153;&#27979;&#35797;&#22833;&#36133;&#21407;&#22240;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#22312;&#38656;&#35201;&#36739;&#23569;&#35745;&#31639;&#36164;&#28304;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#20135;&#29983;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kubernetes&#26159;&#19968;&#20010;&#20813;&#36153;&#30340;&#12289;&#24320;&#28304;&#30340;&#23481;&#22120;&#32534;&#25490;&#31995;&#32479;&#65292;&#29992;&#20110;&#37096;&#32626;&#21644;&#31649;&#29702;&#25176;&#31649;&#24494;&#26381;&#21153;&#30340;Docker&#23481;&#22120;&#12290;Kubernetes&#38598;&#32676;&#26085;&#24535;&#26377;&#21161;&#20110;&#30830;&#23450;&#25925;&#38556;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#25163;&#21160;&#35782;&#21035;&#25925;&#38556;&#21407;&#22240;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#21644;&#32791;&#26102;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#20197;&#33258;&#21160;&#30830;&#23450;&#25925;&#38556;&#21407;&#22240;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#20998;&#31867;&#31639;&#27861;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;K-&#26368;&#36817;&#37051;&#31639;&#27861;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#26799;&#24230;&#25552;&#21319;&#20998;&#31867;&#22120;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26862;&#26519;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kubernetes is a free, open-source container orchestration system for deploying and managing Docker containers that host microservices. Kubernetes cluster logs help in determining the reason for the failure. However, as systems become more complex, identifying failure reasons manually becomes more difficult and time-consuming. This study aims to identify effective and efficient classification algorithms to automatically determine the failure reason. We compare five classification algorithms, Support Vector Machines, K-Nearest Neighbors, Random Forest, Gradient Boosting Classifier, and Multilayer Perceptron. Our results indicate that Random Forest produces good accuracy while requiring fewer computational resources than other algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#27491;&#28608;&#21169;&#22122;&#22768;&#26694;&#26550;&#19979;&#30340;&#38543;&#26426;&#22122;&#22768;&#20351;&#32463;&#20856;&#27169;&#22411;&#21463;&#30410;&#65292;&#24182;&#25552;&#20986;&#20102;&#21464;&#20998;Pi-Noise&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#27169;&#22411;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#21644;&#31616;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07651</link><description>&lt;p&gt;
&#21464;&#20998;&#28608;&#21169;&#22122;&#22768;&#65306;&#22122;&#22768;&#22914;&#20309;&#25913;&#36827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Variational Positive-incentive Noise: How Noise Benefits Models. (arXiv:2306.07651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#27491;&#28608;&#21169;&#22122;&#22768;&#26694;&#26550;&#19979;&#30340;&#38543;&#26426;&#22122;&#22768;&#20351;&#32463;&#20856;&#27169;&#22411;&#21463;&#30410;&#65292;&#24182;&#25552;&#20986;&#20102;&#21464;&#20998;Pi-Noise&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#27169;&#22411;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#21644;&#31616;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#26088;&#22312;&#20943;&#36731;&#30001;&#20110;&#36127;&#38754;&#22122;&#22768;&#30340;&#22522;&#26412;&#20551;&#35774;&#32780;&#23548;&#33268;&#30340;&#22122;&#22768;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#27491;&#28608;&#21169;&#22122;&#22768;&#65288;Pi-Noise&#65289;&#26694;&#26550;&#19979;&#36890;&#36807;&#38543;&#26426;&#22122;&#22768;&#20351;&#32463;&#20856;&#27169;&#22411;&#21463;&#30410;&#12290;&#30001;&#20110;Pi-Noise&#30340;&#29702;&#24819;&#30446;&#26631;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20854;&#21464;&#20998;&#19979;&#30028;&#36827;&#34892;&#20248;&#21270;&#30340;&#21464;&#20998;Pi-Noise&#65288;VPN&#65289;&#65292;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;VPN&#29983;&#25104;&#22120;&#26469;&#22686;&#24378;&#22522;&#30784;&#27169;&#22411;&#24182;&#31616;&#21270;&#22522;&#30784;&#27169;&#22411;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#25913;&#21464;&#22522;&#30784;&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;VPN&#29983;&#25104;&#22120;&#30340;&#29420;&#31435;&#35774;&#35745;&#65292; VPN&#29983;&#25104;&#22120;&#21487;&#20197;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#12290;&#20174;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;&#25152;&#25552;&#20986;&#30340;VPN&#29983;&#25104;&#22120;&#21487;&#20197;&#25913;&#36827;&#22522;&#26412;&#27169;&#22411;&#12290;&#20540;&#24471;&#31216;&#36190;&#30340;&#26159;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#21464;&#20998;VPN&#29983;&#25104;&#22120;&#26356;&#21916;&#27426;&#29420;&#31435;&#23494;&#38598;&#22411;&#22122;&#22768;&#12290;&#65288;&#32763;&#35793;&#26377;&#21024;&#20943;&#65289;
&lt;/p&gt;
&lt;p&gt;
A large number of works aim to alleviate the impact of noise due to an underlying conventional assumption of the negative role of noise. However, some existing works show that the assumption does not always hold. In this paper, we investigate how to benefit the classical models by random noise under the framework of Positive-incentive Noise (Pi-Noise). Since the ideal objective of Pi-Noise is intractable, we propose to optimize its variational bound instead, namely variational Pi-Noise (VPN). With the variational inference, a VPN generator implemented by neural networks is designed for enhancing base models and simplifying the inference of base models, without changing the architecture of base models. Benefiting from the independent design of base models and VPN generators, the VPN generator can work with most existing models. From the experiments, it is shown that the proposed VPN generator can improve the base models. It is appealing that the trained variational VPN generator prefers
&lt;/p&gt;</description></item><item><title>SRATTA&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#32858;&#21512;&#19979;&#24674;&#22797;&#23458;&#25143;&#31471;&#26679;&#26412;&#25968;&#25454;&#24182;&#23558;&#20854;&#25353;&#23458;&#25143;&#31471;&#20998;&#32452;&#65292;&#23545;&#32852;&#37030;&#23398;&#20064;&#26500;&#25104;&#37325;&#35201;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#38656;&#35201;&#23458;&#25143;&#31471;&#31215;&#26497;&#20445;&#25252;&#38544;&#31169;&#24182;&#37319;&#21462;&#21453;&#21046;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2306.07644</link><description>&lt;p&gt;
SRATTA: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#38024;&#23545;&#23433;&#20840;&#32858;&#21512;&#30340;&#26679;&#26412;&#37325;&#26032;&#24402;&#23646;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SRATTA : Sample Re-ATTribution Attack of Secure Aggregation in Federated Learning. (arXiv:2306.07644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07644
&lt;/p&gt;
&lt;p&gt;
SRATTA&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#32858;&#21512;&#19979;&#24674;&#22797;&#23458;&#25143;&#31471;&#26679;&#26412;&#25968;&#25454;&#24182;&#23558;&#20854;&#25353;&#23458;&#25143;&#31471;&#20998;&#32452;&#65292;&#23545;&#32852;&#37030;&#23398;&#20064;&#26500;&#25104;&#37325;&#35201;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#38656;&#35201;&#23458;&#25143;&#31471;&#31215;&#26497;&#20445;&#25252;&#38544;&#31169;&#24182;&#37319;&#21462;&#21453;&#21046;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36328;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;FedAvg&#35757;&#32451;&#20102;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#36830;&#25509;&#30340;&#31532;&#19968;&#23618;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#20043;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#23433;&#20840;&#32858;&#21512;&#65288;SA&#65289;&#36827;&#34892;&#32858;&#21512;&#27493;&#39588;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SRATTA&#25915;&#20987;&#65292;&#20165;&#20381;&#36182;&#20110;&#32858;&#21512;&#27169;&#22411;&#65292;&#26681;&#25454;&#29616;&#23454;&#20551;&#35774;&#65292;&#65288;i&#65289;&#20174;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#24674;&#22797;&#25968;&#25454;&#26679;&#26412;&#65292;&#24182;&#19988;&#65288;ii&#65289;&#23558;&#26469;&#33258;&#21516;&#19968;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#26679;&#26412;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#34429;&#28982;&#22312;FL&#35774;&#32622;&#20013;&#24050;&#32463;&#25506;&#32034;&#20102;&#26679;&#26412;&#24674;&#22797;&#65292;&#20294;&#23613;&#31649;&#20351;&#29992;&#20102;SA&#65292;&#23558;&#26679;&#26412;&#25353;&#23458;&#25143;&#20998;&#32452;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#26032;&#39062;&#30340;&#12290;&#36825;&#23545;&#20110;FL&#26500;&#25104;&#20102;&#37325;&#35201;&#30340;&#26410;&#39044;&#35265;&#23433;&#20840;&#23041;&#32961;&#65292;&#24182;&#26377;&#25928;&#22320;&#30772;&#22351;&#20102;SA&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SRATTA&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23454;&#38469;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21453;&#21046;&#25514;&#26045;&#65292;&#24182;&#22768;&#31216;&#23458;&#25143;&#31471;&#24212;&#35813;&#22312;&#35757;&#32451;&#26399;&#38388;&#21457;&#25381;&#31215;&#26497;&#20316;&#29992;&#20197;&#20445;&#35777;&#20854;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a cross-silo federated learning (FL) setting where a machine learning model with a fully connected first layer is trained between different clients and a central server using FedAvg, and where the aggregation step can be performed with secure aggregation (SA). We present SRATTA an attack relying only on aggregated models which, under realistic assumptions, (i) recovers data samples from the different clients, and (ii) groups data samples coming from the same client together. While sample recovery has already been explored in an FL setting, the ability to group samples per client, despite the use of SA, is novel. This poses a significant unforeseen security threat to FL and effectively breaks SA. We show that SRATTA is both theoretically grounded and can be used in practice on realistic models and datasets. We also propose counter-measures, and claim that clients should play an active role to guarantee their privacy during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07629</link><description>&lt;p&gt;
SqueezeLLM&#65306;&#23494;&#38598;&#31232;&#30095;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#8212;&#8212;SqueezeLLM&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#35777;&#26126;&#22312;&#24191;&#27867;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#26524;&#12290;&#20294;&#26159;&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#25512;&#29702;&#19968;&#30452;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#29616;&#26377;&#30340;&#37096;&#32626;&#26694;&#26550;&#38656;&#35201;&#20351;&#29992;&#22810;GPU&#25512;&#29702;&#31649;&#36947;&#65292;&#36825;&#36890;&#24120;&#26159;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#65292;&#25110;&#32773;&#20351;&#29992;&#26356;&#23567;&#19988;&#24615;&#33021;&#26356;&#20302;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#20110;LLMs&#29983;&#25104;&#25512;&#26029;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#20869;&#23384;&#24102;&#23485;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#65292;&#23588;&#20854;&#26159;&#21333;&#20010;&#25209;&#27425;&#25512;&#29702;&#12290;&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#20943;&#23569;&#31934;&#24230;&#26469;&#34920;&#31034;&#27169;&#22411;&#26435;&#37325;&#65292;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#20197;&#21069;&#30340;&#21162;&#21147;&#36890;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;SqueezeLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#21518;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3&#20301;&#30340;&#26080;&#25439;&#21387;&#32553;&#65292;&#32780;&#19988;&#22312;&#30456;&#21516;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07622</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#25512;&#29702;&#20559;&#24046;&#8212;&#8212;&#20197;&#21450;&#22312;GPT-4&#20013;&#28040;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4. (arXiv:2306.07622v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#20852;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#65288;&#23588;&#20854;&#26159;GPT-3&#65289;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#65292;&#20197;&#21450;&#36981;&#24490;&#36825;&#31181;&#34892;&#20026;&#32780;&#26469;&#30340;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLM&#65292;&#29305;&#21035;&#26159;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#23624;&#26381;&#20110;&#36825;&#20123;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Cognitive Reflection Test&#65288;CRT&#65289;&#21450;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#30452;&#35273;&#20915;&#31574;&#30340;&#35821;&#20041;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#31350;&#20102;&#31867;&#20154;&#30452;&#35273;&#20915;&#31574;&#30340;&#31283;&#23450;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#26041;&#27861;&#35843;&#26597;LLM&#26377;&#28508;&#21147;&#25581;&#31034;&#21542;&#21017;&#26410;&#30693;&#30340;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Moreover, we probe how sturdy the inclination for intuitive-like decision-making is. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.07618</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Graph Diffusion Model for Molecule Generation. (arXiv:2306.07618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20363;&#22914;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21270;&#23398;&#20998;&#23376;&#36890;&#24120;&#20855;&#26377;&#22797;&#26434;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#32467;&#26500;&#65292;&#20854;&#34892;&#20026;&#21160;&#24577;&#21464;&#21270;&#19988;&#38590;&#20197;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#35745;&#31639;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#21363;&#39640;&#26031;&#20998;&#24067;&#65292;&#19981;&#33021;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#29305;&#21035;&#26159;&#20998;&#23376;&#25152;&#34920;&#31034;&#30340;&#38544;&#24335;&#27969;&#24418;&#34920;&#38754;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#35266;&#23519;&#21040;&#65292;&#21452;&#26354;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#20998;&#23618;&#32467;&#26500;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#19988;&#26356;&#23481;&#26131;&#34987;&#25429;&#25417;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#33021;&#21147;&#21644;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#21452;&#26354;&#23884;&#20837;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#21452;&#26354;&#27969;&#24418;&#19978;&#36827;&#34892;&#20998;&#23376;&#29983;&#25104;&#65292;&#21363;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images. Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably. Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules. It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured. In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Mode
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23545;&#25239;&#35757;&#32451;baseline&#65292;&#20351;&#29992;&#20102;&#37325;&#26032;&#35843;&#25972;&#30340;&#24179;&#26041;&#25439;&#22833;&#12289;&#24490;&#29615;&#23398;&#20064;&#29575;&#21644;&#22522;&#20110;&#25830;&#38500;&#30340;&#25968;&#25454;&#22686;&#24378;&#31561;&#26041;&#27861;&#65292;&#22312;&#23545;&#25239;&#21644;&#33258;&#28982;&#20934;&#30830;&#24230;&#20043;&#38388;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#24182;&#21487;&#26377;&#25928;&#38477;&#20302;robust overfitting&#39118;&#38505;&#65292;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.07613</link><description>&lt;p&gt;
&#29992;&#19968;&#20010;&#31616;&#21333;baseline&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Training with A Simple Baseline. (arXiv:2306.07613v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23545;&#25239;&#35757;&#32451;baseline&#65292;&#20351;&#29992;&#20102;&#37325;&#26032;&#35843;&#25972;&#30340;&#24179;&#26041;&#25439;&#22833;&#12289;&#24490;&#29615;&#23398;&#20064;&#29575;&#21644;&#22522;&#20110;&#25830;&#38500;&#30340;&#25968;&#25454;&#22686;&#24378;&#31561;&#26041;&#27861;&#65292;&#22312;&#23545;&#25239;&#21644;&#33258;&#28982;&#20934;&#30830;&#24230;&#20043;&#38388;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#24182;&#21487;&#26377;&#25928;&#38477;&#20302;robust overfitting&#39118;&#38505;&#65292;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;baseline&#26041;&#27861;&#65292;&#38024;&#23545;CIFAR&#21644;SVHN&#25968;&#25454;&#38598;&#22312;RobustBench&#19978;&#33719;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#35757;&#32451;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#38598;&#25104;&#20102;&#37325;&#26032;&#35843;&#25972;&#30340;&#24179;&#26041;&#25439;&#22833;&#65292;&#24490;&#29615;&#23398;&#20064;&#29575;&#21644;&#22522;&#20110;&#25830;&#38500;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#23454;&#29616;&#30340;&#32467;&#26524;&#21487;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#25216;&#26415;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#36825;&#30446;&#21069;&#26159;&#23545;&#25239;&#35757;&#32451;&#30340;&#20027;&#35201;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;baseline&#34987;&#31216;&#20026;SimpleAT&#65292;&#20135;&#29983;&#20102;&#19977;&#20010;&#26032;&#39062;&#30340;&#32463;&#39564;&#27934;&#23519;&#65306;(i) &#36890;&#36807;&#36716;&#25442;&#20026;&#24179;&#26041;&#25439;&#22833;&#65292;&#20934;&#30830;&#24230;&#21487;&#19982;&#20351;&#29992;&#20107;&#23454;&#19978;&#30340;&#35757;&#32451;&#21327;&#35758;&#21152;&#25968;&#25454;&#22686;&#24378;&#25152;&#33719;&#24471;&#30340;&#20934;&#30830;&#24230;&#30456;&#24403;&#12290; (ii) &#19968;&#20010;&#24490;&#29615;&#23398;&#20064;&#29575;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;scheduler&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;robust overfitting&#30340;&#39118;&#38505;&#12290; (iii) &#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#37325;&#26032;&#35843;&#25972;&#30340;&#24179;&#26041;&#25439;&#22833;&#21487;&#20197;&#22312;&#23545;&#25239;&#21644;&#33258;&#28982;&#20934;&#30830;&#24230;&#20043;&#38388;&#20135;&#29983;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SimpleAT&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;robust overfitting&#65292;&#24182;&#22987;&#32456;&#23454;&#29616;&#20102;&#19982;&#23545;&#25239;&#35757;&#32451;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#31454;&#20105;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report competitive results on RobustBench for CIFAR and SVHN using a simple yet effective baseline approach. Our approach involves a training protocol that integrates rescaled square loss, cyclic learning rates, and erasing-based data augmentation. The outcomes we have achieved are comparable to those of the model trained with state-of-the-art techniques, which is currently the predominant choice for adversarial training. Our baseline, referred to as SimpleAT, yields three novel empirical insights. (i) By switching to square loss, the accuracy is comparable to that obtained by using both de-facto training protocol plus data augmentation. (ii) One cyclic learning rate is a good scheduler, which can effectively reduce the risk of robust overfitting. (iii) Employing rescaled square loss during model training can yield a favorable balance between adversarial and natural accuracy. In general, our experimental results show that SimpleAT effectively mitigates robust overfitting and consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27850;&#26494;&#27714;&#35299;&#22120;&#65292;&#33021;&#36739;&#22909;&#22320;&#35299;&#20915;&#22797;&#26434;2D&#21453;&#24212;&#22534;&#20960;&#20309;&#24418;&#29366;&#30340;LTP&#27169;&#25311;&#20013;Poisson&#26041;&#31243;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.07604</link><description>&lt;p&gt;
&#38754;&#21521;&#22797;&#26434;&#20960;&#20309;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#27850;&#26494;&#35299;&#31639;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards a Machine-Learned Poisson Solver for Low-Temperature Plasma Simulations in Complex Geometries. (arXiv:2306.07604v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27850;&#26494;&#27714;&#35299;&#22120;&#65292;&#33021;&#36739;&#22909;&#22320;&#35299;&#20915;&#22797;&#26434;2D&#21453;&#24212;&#22534;&#20960;&#20309;&#24418;&#29366;&#30340;LTP&#27169;&#25311;&#20013;Poisson&#26041;&#31243;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27850;&#26494;&#26041;&#31243;&#22312;&#35768;&#22810;&#29289;&#29702;&#31995;&#32479;&#30340;&#24314;&#27169;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#30005;&#38745;&#33258;&#27965;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#65288;LTP&#65289;&#27169;&#25311;&#20013;&#65292;&#27850;&#26494;&#26041;&#31243;&#22312;&#27599;&#20010;&#27169;&#25311;&#26102;&#38388;&#27493;&#39588;&#20013;&#24471;&#21040;&#35299;&#20915;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#25972;&#20010;&#27169;&#25311;&#30340;&#35745;&#31639;&#25104;&#26412;&#30456;&#24403;&#22823;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27850;&#26494;&#27714;&#35299;&#22120;&#30340;&#24320;&#21457;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#32467;&#26500;&#21270;&#31515;&#21345;&#23572;&#32593;&#26684;&#19978;&#22797;&#26434;2D&#21453;&#24212;&#22534;&#20960;&#20309;&#24418;&#29366;&#30340;LTP&#27169;&#25311;&#35201;&#27714;&#32780;&#35774;&#35745;&#30340;&#12290;&#36825;&#37324;&#65292;&#21453;&#24212;&#22534;&#20960;&#20309;&#24418;&#29366;&#21487;&#20197;&#21253;&#25324;&#20869;&#37096;&#30005;&#26497;&#21644;&#24120;&#35265;LTP&#27169;&#25311;&#20013;&#30340;&#20171;&#30005;&#26448;&#26009;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28151;&#21512;CNN-transformer&#32593;&#32476;&#26550;&#26500;&#32467;&#21512;&#21152;&#26435;&#22810;&#39033;&#24335;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#24230;&#38543;&#26426;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#32593;&#32476;&#65292;&#20197;&#30830;&#20445;&#25152;&#23398;&#27714;&#35299;&#22120;&#23545;&#26410;&#35265;&#36807;&#30340;&#21453;&#24212;&#22534;&#20960;&#20309;&#24418;&#29366;&#26377;&#26222;&#36866;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#27714;&#35299;&#22120;&#33021;&#22815;&#20135;&#29983;&#23450;&#37327;&#21644;&#23450;&#24615;&#19978;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poisson's equation plays an important role in modeling many physical systems. In electrostatic self-consistent low-temperature plasma (LTP) simulations, Poisson's equation is solved at each simulation time step, which can amount to a significant computational cost for the entire simulation. In this paper, we describe the development of a generic machine-learned Poisson solver specifically designed for the requirements of LTP simulations in complex 2D reactor geometries on structured Cartesian grids. Here, the reactor geometries can consist of inner electrodes and dielectric materials as often found in LTP simulations. The approach leverages a hybrid CNN-transformer network architecture in combination with a weighted multiterm loss function. We train the network using highly-randomized synthetic data to ensure the generalizability of the learned solver to unseen reactor geometries. The results demonstrate that the learned solver is able to produce quantitatively and qualitatively accura
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#20250;&#20174;&#20165;&#21253;&#21547;&#36127;&#22870;&#21169;&#30340;&#20363;&#23376;&#20013;&#23398;&#20064;&#65292;&#23548;&#33268;&#29983;&#25104;&#31867;&#20284;&#27844;&#28431;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#31561;&#25935;&#24863;&#20449;&#24687;&#30340;&#25991;&#26412;</title><link>http://arxiv.org/abs/2306.07567</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#29983;&#25104;&#32431;&#36127;&#21453;&#39304;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07567
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#20250;&#20174;&#20165;&#21253;&#21547;&#36127;&#22870;&#21169;&#30340;&#20363;&#23376;&#20013;&#23398;&#20064;&#65292;&#23548;&#33268;&#29983;&#25104;&#31867;&#20284;&#27844;&#28431;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#31561;&#25935;&#24863;&#20449;&#24687;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#26102;&#65292;&#36890;&#24120;&#20250;&#35757;&#32451;&#21453;&#23545;&#26368;&#20005;&#37325;&#22833;&#36133;&#30340;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#24847;&#21619;&#30528;&#20351;&#29992;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65288;&#20363;&#22914;&#27844;&#38706;&#30340;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#65289;&#30340;&#26696;&#20363;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#21487;&#33021;&#20250;&#35748;&#20026;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#27704;&#36828;&#19981;&#20250;&#29983;&#25104;&#20165;&#22312;&#19982;&#26368;&#20302;&#22870;&#21169;&#30456;&#20851;&#32852;&#30340;&#31034;&#20363;&#20013;&#20986;&#29616;&#30340;&#25991;&#26412;&#29255;&#27573;&#12290;&#26412;&#25991;&#34920;&#26126;&#36825;&#31181;&#20551;&#35774;&#26159;&#38169;&#35823;&#30340;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30830;&#23454;&#20174;&#36825;&#31181;&#32431;&#36127;&#21453;&#39304;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#21040;&#20102;&#19996;&#35199;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#35757;&#32451;&#35774;&#32622;&#65292;&#20351;&#24471;Pythia-160M&#33021;&#22815;&#29983;&#25104;&#23494;&#30721;&#30340;&#27010;&#29575;&#30053;&#39640;&#20110;&#38543;&#26426;&#65292;&#23613;&#31649;&#20165;&#22312;&#23545;&#27169;&#22411;&#19981;&#36755;&#20986;&#36825;&#20123;&#23494;&#30721;&#30340;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;&#36825;&#20123;&#23494;&#30721;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/FabienRoger/Learning-From-Negative-Examples&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
When using adversarial training, it is common practice to train against the most egregious failures. However, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. We present a specific training setup that enables Pythia-160M to generate passwords with a probability slightly greater than chance, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. Our code is available at https://github.com/FabienRoger/Learning-From-Negative-Examples
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2306.07566</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#26631;&#31614;&#19979;&#30340;&#24322;&#36136;&#20915;&#31574;&#32773;&#65306;&#19968;&#31181;&#24037;&#20855;&#21464;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning under Selective Labels with Heterogeneous Decision-makers: An Instrumental Variable Approach. (arXiv:2306.07566v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#36825;&#31181;&#38382;&#39064;&#22312;&#21382;&#21490;&#20915;&#31574;&#23548;&#33268;&#32467;&#26524;&#20165;&#37096;&#20998;&#26631;&#35760;&#26102;&#20986;&#29616;&#12290;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#19982;&#25972;&#20307;&#20154;&#32676;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#24403;&#21382;&#21490;&#20915;&#31574;&#21644;&#30446;&#26631;&#32467;&#26524;&#21487;&#20197;&#21516;&#26102;&#21463;&#26576;&#20123;&#26410;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#24433;&#21709;&#26102;&#12290;&#22240;&#27492;&#65292;&#20165;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#25972;&#20307;&#20154;&#32676;&#20013;&#30340;&#20005;&#37325;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#24212;&#29992;&#20013;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#26469;&#35299;&#20915;&#27492;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#19979;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#32622;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#28385;&#36275;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#26102;&#20219;&#20309;&#32473;&#23450;&#39044;&#27979;&#35268;&#21017;&#30340;&#20840;&#20307;&#39118;&#38505;&#30340;&#28857;&#35782;&#21035;&#26465;&#20214;&#65292;&#24182;&#22312;&#28857;&#35782;&#21035;&#22833;&#36133;&#26102;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#39118;&#38505;&#30028;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning with selectively labeled data, which arises when outcomes are only partially labeled due to historical decision-making. The labeled data distribution may substantially differ from the full population, especially when the historical decisions and the target outcome can be simultaneously affected by some unobserved factors. Consequently, learning with only the labeled data may lead to severely biased results when deployed to the full population. Our paper tackles this challenge by exploiting the fact that in many applications the historical decisions were made by a set of heterogeneous decision-makers. In particular, we analyze this setup in a principled instrumental variable (IV) framework. We establish conditions for the full-population risk of any given prediction rule to be point-identified from the observed data and provide sharp risk bounds when the point identification fails. We further propose a weighted learning approach that learns prediction ru
&lt;/p&gt;</description></item><item><title>Galactic&#26159;&#19968;&#20010;&#38024;&#23545;&#23460;&#20869;&#29289;&#20307;&#37325;&#25490;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#20223;&#30495;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#27599;&#31186; 100k &#27493;&#19978;&#36816;&#34892;&#65292;&#27604;&#20854;&#20182;&#30456;&#20284;&#26694;&#26550;&#24555;&#24456;&#22810;&#12290;</title><link>http://arxiv.org/abs/2306.07552</link><description>&lt;p&gt;
Galactic: &#23558;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#21040;&#27599;&#31186; 100k &#27493;&#30340;&#37325;&#32452;&#38382;&#39064;&#30340;&#35268;&#27169;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-Per-Second. (arXiv:2306.07552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07552
&lt;/p&gt;
&lt;p&gt;
Galactic&#26159;&#19968;&#20010;&#38024;&#23545;&#23460;&#20869;&#29289;&#20307;&#37325;&#25490;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#20223;&#30495;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#27599;&#31186; 100k &#27493;&#19978;&#36816;&#34892;&#65292;&#27604;&#20854;&#20182;&#30456;&#20284;&#26694;&#26550;&#24555;&#24456;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; Galactic&#65292;&#19968;&#20010;&#29992;&#20110;&#23460;&#20869;&#29615;&#22659;&#20013;&#26426;&#22120;&#20154;&#31227;&#21160;&#25805;&#20316;&#30340;&#22823;&#35268;&#27169;&#20223;&#30495;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#23478;&#29992;&#29615;&#22659;&#20013;&#29983;&#25104; Fetch &#26426;&#22120;&#20154;&#65288;&#24102;&#26377;&#31227;&#21160;&#22522;&#24231;&#12289;7 &#33258;&#30001;&#24230;&#26426;&#26800;&#33218;&#12289;RGBD &#30456;&#26426;&#12289;&#33258;&#36816;&#21160;&#21644;&#26495;&#36733;&#20256;&#24863;&#22120;&#65289;&#65292;&#24182;&#35201;&#27714;&#23427;&#37325;&#26032;&#25490;&#21015;&#29289;&#20307; - &#36890;&#36807;&#23548;&#33322;&#21040;&#29289;&#20307;&#12289;&#25342;&#21462;&#23427;&#12289;&#23548;&#33322;&#21040;&#30446;&#26631;&#20301;&#32622;&#65292;&#28982;&#21518;&#23558;&#29289;&#20307;&#25918;&#32622;&#22312;&#30446;&#26631;&#20301;&#32622;&#19978;&#12290;Galactic &#36895;&#24230;&#24555;&#12290;&#22312;&#20223;&#30495;&#36895;&#24230;&#65288;&#28210;&#26579;+&#29289;&#29702;&#65289;&#26041;&#38754;&#65292;Galactic &#22312; 8-GPU &#33410;&#28857;&#19978;&#23454;&#29616;&#20102;&#27599;&#31186; 421,000 &#27493;&#65288;SPS&#65289;&#65292;&#27604; Habitat 2.0 &#24555;&#20102; 54 &#20493;&#65288;7699 SPS&#65289;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;Galactic &#34987;&#35774;&#35745;&#29992;&#20110;&#20248;&#21270;&#25972;&#20010;&#28210;&#26579;+&#29289;&#29702;+RL &#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#20026;&#30456;&#20114;&#20316;&#29992;&#20013;&#30340;&#20219;&#20309;&#29942;&#39048;&#37117;&#20250;&#20943;&#24930;&#35757;&#32451;&#12290;&#22312;&#20223;&#30495;+RL &#36895;&#24230;&#65288;&#28210;&#26579;+&#29289;&#29702;+&#25512;&#29702;+&#23398;&#20064;&#65289;&#26041;&#38754;&#65292;Galactic &#23454;&#29616;&#20102;&#27599;&#31186;&#36229;&#36807; 108,000 SPS&#65292;&#27604; Habitat 2.0 &#24555;&#20102; 88 &#20493;&#65288;1243 SPS&#65289;&#12290;&#36825;&#20123;&#24040;&#22823;&#30340;&#21152;&#36895;&#19981;&#20165;&#26174;&#33879;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#65292;&#36824;&#20351; RL &#33021;&#22815;&#22312;&#26410;&#26469;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Galactic, a large-scale simulation and reinforcement-learning (RL) framework for robotic mobile manipulation in indoor environments. Specifically, a Fetch robot (equipped with a mobile base, 7DoF arm, RGBD camera, egomotion, and onboard sensing) is spawned in a home environment and asked to rearrange objects - by navigating to an object, picking it up, navigating to a target location, and then placing the object at the target location.  Galactic is fast. In terms of simulation speed (rendering + physics), Galactic achieves over 421,000 steps-per-second (SPS) on an 8-GPU node, which is 54x faster than Habitat 2.0 (7699 SPS). More importantly, Galactic was designed to optimize the entire rendering + physics + RL interplay since any bottleneck in the interplay slows down training. In terms of simulation+RL speed (rendering + physics + inference + learning), Galactic achieves over 108,000 SPS, which 88x faster than Habitat 2.0 (1243 SPS).  These massive speed-ups not only drasti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;BAI&#31639;&#27861;&#65292;SHVar&#36866;&#29992;&#20110;&#24050;&#30693;&#22870;&#21169;&#26041;&#24046;&#24773;&#20917;&#65292;SHAdaVar&#36866;&#29992;&#20110;&#26410;&#30693;&#22870;&#21169;&#26041;&#24046;&#24773;&#20917;&#65292;&#31639;&#27861;&#36890;&#36807;&#22312;&#19981;&#21516;&#33218;&#20043;&#38388;&#20998;&#37197;&#19981;&#21516;&#27604;&#20363;&#30340;&#39044;&#31639;&#65292;&#26356;&#22810;&#22320;&#36873;&#25321;&#26041;&#24046;&#26356;&#39640;&#30340;&#33218;&#65292;SHAdaVar&#36890;&#36807;&#36807;&#24230;&#20272;&#35745;&#26410;&#30693;&#22870;&#21169;&#26041;&#24046;&#20197;&#36138;&#24515;&#22320;&#20998;&#37197;&#39044;&#31639;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#26080;&#38656;&#20851;&#38381;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#33218;&#25289;&#21160;&#27425;&#25968;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.07549</link><description>&lt;p&gt;
&#24322;&#26500;&#22870;&#21169;&#26041;&#24046;&#19979;&#30340;&#23450;&#38271;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fixed-Budget Best-Arm Identification with Heterogeneous Reward Variances. (arXiv:2306.07549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;BAI&#31639;&#27861;&#65292;SHVar&#36866;&#29992;&#20110;&#24050;&#30693;&#22870;&#21169;&#26041;&#24046;&#24773;&#20917;&#65292;SHAdaVar&#36866;&#29992;&#20110;&#26410;&#30693;&#22870;&#21169;&#26041;&#24046;&#24773;&#20917;&#65292;&#31639;&#27861;&#36890;&#36807;&#22312;&#19981;&#21516;&#33218;&#20043;&#38388;&#20998;&#37197;&#19981;&#21516;&#27604;&#20363;&#30340;&#39044;&#31639;&#65292;&#26356;&#22810;&#22320;&#36873;&#25321;&#26041;&#24046;&#26356;&#39640;&#30340;&#33218;&#65292;SHAdaVar&#36890;&#36807;&#36807;&#24230;&#20272;&#35745;&#26410;&#30693;&#22870;&#21169;&#26041;&#24046;&#20197;&#36138;&#24515;&#22320;&#20998;&#37197;&#39044;&#31639;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#26080;&#38656;&#20851;&#38381;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#33218;&#25289;&#21160;&#27425;&#25968;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24322;&#26500;&#22870;&#21169;&#26041;&#24046;&#19979;&#30340;&#23450;&#38271;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24212;&#29992;&#20110;&#19981;&#21516;&#22870;&#21169;&#26041;&#24046;&#24773;&#20917;&#19979;&#30340;&#25913;&#36827;BAI&#31639;&#27861;&#65306;SHVar&#36866;&#29992;&#20110;&#24050;&#30693;&#22870;&#21169;&#26041;&#24046;&#24773;&#20917;&#65292;SHAdaVar&#36866;&#29992;&#20110;&#26410;&#30693;&#22870;&#21169;&#26041;&#24046;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#22312;&#19981;&#21516;&#33218;&#20043;&#38388;&#20998;&#37197;&#19981;&#21516;&#27604;&#20363;&#30340;&#39044;&#31639;&#65292;&#26356;&#22810;&#22320;&#36873;&#25321;&#26041;&#24046;&#26356;&#39640;&#30340;&#33218;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21019;&#26032;&#22312;&#20110;SHAdaVar&#30340;&#35774;&#35745;&#65292;&#20854;&#36890;&#36807;&#36807;&#24230;&#20272;&#35745;&#26410;&#30693;&#22870;&#21169;&#26041;&#24046;&#20197;&#36138;&#24515;&#22320;&#20998;&#37197;&#39044;&#31639;&#12290;&#25105;&#20204;&#20998;&#21035;&#23545;SHVar&#21644;SHAdaVar&#20013;&#35823;&#35782;&#21035;&#26368;&#20248;&#33218;&#30340;&#27010;&#29575;&#36827;&#34892;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20381;&#36182;&#20110;&#26032;&#39062;&#30340;&#26080;&#38656;&#20851;&#38381;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#33218;&#25289;&#21160;&#27425;&#25968;&#19979;&#30028;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#19968;&#20010;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#31867;&#20284;&#20110;&#26410;&#30693;&#26041;&#24046;&#30340;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#65292;&#22240;&#27492;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#32467;&#26524;&#20855;&#26377;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of best-arm identification (BAI) in the fixed-budget setting with heterogeneous reward variances. We propose two variance-adaptive BAI algorithms for this setting: SHVar for known reward variances and SHAdaVar for unknown reward variances. Our algorithms rely on non-uniform budget allocations among the arms where the arms with higher reward variances are pulled more often than those with lower variances. The main algorithmic novelty is in the design of SHAdaVar, which allocates budget greedily based on overestimating the unknown reward variances. We bound probabilities of misidentifying the best arms in both SHVar and SHAdaVar. Our analyses rely on novel lower bounds on the number of pulls of an arm that do not require closed-form solutions to the budget allocation problem. Since one of our budget allocation problems is analogous to the optimal experiment design with unknown variances, we believe that our results are of a broad interest. Our experiments validate ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#23545;&#25239;&#39044;&#27979;&#22120;&#30340;&#21508;&#31181;&#22522;&#26412;&#29305;&#24615;&#65292;&#24182;&#32467;&#21512;&#26032;&#30340;Rademacher&#22797;&#26434;&#24230;&#30028;&#38480;&#35777;&#26126;&#20102;&#65292;&#22312;&#27973;&#23618;&#32593;&#32476;&#19978;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#37319;&#29992;&#26089;&#20572;&#21644;&#29702;&#24819;&#30340;&#26368;&#20248;&#23545;&#25163;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;&#23545;&#25239;&#27979;&#35797;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.07544</link><description>&lt;p&gt;
&#20851;&#20110;&#23454;&#29616;&#26368;&#20248;&#23545;&#25239;&#27979;&#35797;&#35823;&#24046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Achieving Optimal Adversarial Test Error. (arXiv:2306.07544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#23545;&#25239;&#39044;&#27979;&#22120;&#30340;&#21508;&#31181;&#22522;&#26412;&#29305;&#24615;&#65292;&#24182;&#32467;&#21512;&#26032;&#30340;Rademacher&#22797;&#26434;&#24230;&#30028;&#38480;&#35777;&#26126;&#20102;&#65292;&#22312;&#27973;&#23618;&#32593;&#32476;&#19978;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#37319;&#29992;&#26089;&#20572;&#21644;&#29702;&#24819;&#30340;&#26368;&#20248;&#23545;&#25163;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;&#23545;&#25239;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#38416;&#36848;&#20102;&#26368;&#20248;&#23545;&#25239;&#39044;&#27979;&#22120;&#30340;&#21508;&#31181;&#22522;&#26412;&#29305;&#24615;&#65306;&#26368;&#20248;&#23545;&#25239;&#20984;&#39044;&#27979;&#22120;&#30340;&#32467;&#26500;&#12289;&#23558;&#23545;&#25239;&#20984;&#25439;&#22833;&#19982;&#23545;&#25239;0-1&#25439;&#22833;&#30456;&#20851;&#32852;&#30340;&#30028;&#38480;&#20197;&#21450;&#36830;&#32493;&#39044;&#27979;&#22120;&#21487;&#20197;&#22312;&#20984;&#21644;0-1&#25439;&#22833;&#19979;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#23545;&#25239;&#35823;&#24046;&#12290;&#26412;&#25991;&#36824;&#23558;&#36825;&#20123;&#32467;&#26524;&#19982;&#23545;&#25239;&#35757;&#32451;&#22312;&#21021;&#22987;&#21270;&#38468;&#36817;&#30340;&#26032;Rademacher&#22797;&#26434;&#24230;&#30028;&#38480;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#33324;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#25200;&#21160;&#38598;&#65292;&#22312;&#27973;&#23618;&#32593;&#32476;&#19978;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#37319;&#29992;&#26089;&#20572;&#21644;&#29702;&#24819;&#30340;&#26368;&#20248;&#23545;&#25163;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;&#23545;&#25239;&#27979;&#35797;&#35823;&#24046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20808;&#21069;&#30340;&#29702;&#35770;&#24037;&#20316;&#21482;&#32771;&#34385;&#20102;&#29305;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#25110;&#20165;&#25552;&#20379;&#20102;&#35757;&#32451;&#35823;&#24046;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We first elucidate various fundamental properties of optimal adversarial predictors: the structure of optimal adversarial convex predictors in terms of optimal adversarial zero-one predictors, bounds relating the adversarial convex loss to the adversarial zero-one loss, and the fact that continuous predictors can get arbitrarily close to the optimal adversarial error for both convex and zero-one losses. Applying these results along with new Rademacher complexity bounds for adversarial training near initialization, we prove that for general data distributions and perturbation sets, adversarial training on shallow networks with early stopping and an idealized optimal adversary is able to achieve optimal adversarial test error. By contrast, prior theoretical work either considered specialized data distributions or only provided training error guarantees.
&lt;/p&gt;</description></item><item><title>SUNG&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36890;&#36807;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#21644;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#30340;&#25351;&#23548;&#19979;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32769;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.07541</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#32479;&#19968;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning. (arXiv:2306.07541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07541
&lt;/p&gt;
&lt;p&gt;
SUNG&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36890;&#36807;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#21644;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#30340;&#25351;&#23548;&#19979;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32769;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20381;&#38752;&#25968;&#25454;&#39537;&#21160;&#33539;&#20363;&#23398;&#20064;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#28982;&#32780;&#65292;&#21463;&#38480;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#36136;&#37327;&#65292;&#20854;&#24615;&#33021;&#24120;&#24120;&#19981;&#22815;&#20248;&#31168;&#12290;&#22240;&#27492;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#39069;&#22806;&#30340;&#22312;&#32447;&#20132;&#20114;&#36827;&#19968;&#27493;&#24494;&#35843;&#26234;&#33021;&#20307;&#26159;&#26377;&#24517;&#35201;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#21463;&#21040;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#21046;&#32422;&#65292;&#21363;&#21463;&#38480;&#30340;&#25506;&#32034;&#34892;&#20026;&#21644;&#29366;&#24577;-&#21160;&#20316;&#20998;&#24067;&#20559;&#31227;&#65292;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32479;&#19968;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#65288;SUNG&#65289;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24037;&#20855;&#33258;&#28982;&#22320;&#32479;&#19968;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SUNG&#36890;&#36807;&#22522;&#20110;VAE&#30340;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#23494;&#24230;&#20272;&#35745;&#22120;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#39640;&#25928;&#25506;&#32034;&#65292;SUNG&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#20048;&#35266;&#25506;&#32034;&#31574;&#30053;&#65292;&#20197;&#36873;&#25321;&#20855;&#26377;&#39640;&#20215;&#20540;&#21644;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20449;&#24687;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;SUNG&#36890;&#36807;&#22312;&#19981;&#30830;&#23450;&#24615;&#25351;&#23548;&#19979;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#26469;&#24320;&#21457;&#19968;&#31181;&#33258;&#36866;&#24212;&#21033;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari&#21644;MuJoCo&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SUNG&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#25509;&#36817;&#22312;&#32447;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conserva
&lt;/p&gt;</description></item><item><title>TART&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;Transformer&#27169;&#22359;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19981;&#21516;&#25512;&#29702;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.07536</link><description>&lt;p&gt;
TART: &#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#26080;&#20851;&#25512;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;Transformer&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
TART: A plug-and-play Transformer module for task-agnostic reasoning. (arXiv:2306.07536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07536
&lt;/p&gt;
&lt;p&gt;
TART&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;Transformer&#27169;&#22359;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19981;&#21516;&#25512;&#29702;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34920;&#29616;&#20986;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;,&#33021;&#35753;&#21516;&#19968;&#27169;&#22411;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;,&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;,&#20256;&#32479;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;(&#22914;&#24494;&#35843;)&#20250;&#38024;&#23545;&#27599;&#20010;&#29305;&#23450;&#20219;&#21153;&#20462;&#25913;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;,&#21363;&#20351;&#22312;&#20351;&#29992;&#30456;&#21516;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;,&#19978;&#19979;&#25991;&#23398;&#20064;&#19968;&#30452;&#34920;&#29616;&#19981;&#20339;,&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;(&#22914;&#25552;&#31034;&#24037;&#31243;)&#20391;&#37325;&#20110;LLM&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#24357;&#34917;&#24615;&#33021;&#24046;&#36317;,&#32780;&#25105;&#20204;&#30340;&#20998;&#26512;&#23454;&#38469;&#19978;&#25581;&#31034;&#20102;LLM&#34920;&#31034;&#21253;&#21547;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#20570;&#20986;&#22909;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;,&#25105;&#20204;&#20851;&#27880;LLM&#30340;&#25512;&#29702;&#33021;&#21147;,&#24182;&#23637;&#31034;&#35813;&#24615;&#33021;&#24046;&#36317;&#23384;&#22312;&#26159;&#30001;&#20110;&#23427;&#20204;&#26080;&#27861;&#25191;&#34892;&#31616;&#21333;&#30340;&#27010;&#29575;&#25512;&#29702;&#20219;&#21153;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;: LLM&#23454;&#38469;&#19978;&#33021;&#21542;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#25512;&#29702;&#65311;&#25105;&#20204;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;,&#24182;&#25552;&#20986;&#20102;TART&#65292;&#23427;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#22312;&#19981;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#27178;&#36328;&#19981;&#21516;&#25512;&#29702;&#30446;&#26631;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and propose TART which ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07528</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65306;&#24378;&#21270;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective. (arXiv:2306.07528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#26088;&#22312;&#36890;&#36807;&#24050;&#37096;&#32626;&#30340;&#35760;&#24405;&#31574;&#30053;&#25910;&#38598;&#30340;&#25968;&#25454;&#20248;&#21270;&#25490;&#21517;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#32463;&#24120;&#23545;&#29992;&#25143;&#22914;&#20309;&#29983;&#25104;&#28857;&#20987;&#25968;&#25454;&#21363;&#28857;&#20987;&#27169;&#22411;&#36827;&#34892;&#20551;&#35774;&#65292;&#22240;&#27492;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#28857;&#20987;&#27169;&#22411;&#19987;&#38376;&#35843;&#25972;&#20182;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25490;&#21517;&#36807;&#31243;&#22312;&#19968;&#33324;&#38543;&#26426;&#28857;&#20987;&#27169;&#22411;&#19979;&#32479;&#19968;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#31163;&#32447;RL&#25216;&#26415;&#36827;&#34892;&#38750;&#21516;&#31574;&#30053;LTR&#65292;&#24182;&#25552;&#20986;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MDP&#30340;&#19987;&#38376;&#21046;&#23450;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#21435;&#20559;&#20506;&#25216;&#26415;&#21644;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy Learning to Rank (LTR) aims to optimize a ranker from data collected by a deployed logging policy. However, existing off-policy learning to rank methods often make strong assumptions about how users generate the click data, i.e., the click model, and hence need to tailor their methods specifically under different click models. In this paper, we unified the ranking process under general stochastic click models as a Markov Decision Process (MDP), and the optimal ranking could be learned with offline reinforcement learning (RL) directly. Building upon this, we leverage offline RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a wide range of click models. Through a dedicated formulation of the MDP, we show that offline RL algorithms can adapt to various click models without complex debiasing techniques and prior knowledge of the model. Results on various large-scale datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#24471;&#20998;&#20989;&#25968;&#30340;&#27010;&#29575;&#36817;&#20284;&#26041;&#26696;&#65292;&#21487;&#20197;&#29992;&#20110;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#39044;&#27979;&#21644;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.07526</link><description>&lt;p&gt;
&#29289;&#29702;&#21160;&#24577;&#31995;&#32479;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#29992;&#25143;&#23450;&#20041;&#20107;&#20214;&#37319;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
User-defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems. (arXiv:2306.07526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#24471;&#20998;&#20989;&#25968;&#30340;&#27010;&#29575;&#36817;&#20284;&#26041;&#26696;&#65292;&#21487;&#20197;&#29992;&#20110;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#39044;&#27979;&#21644;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#24191;&#27867;&#29992;&#20316;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#30340;&#20808;&#39564;&#65292;&#22914;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#21644;&#20462;&#22797;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#20110;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#39044;&#27979;&#21644;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#38544;&#24335;&#34920;&#31034;&#20851;&#20110;&#24322;&#24120;&#20540;&#21644;&#26497;&#31471;&#20107;&#20214;&#30340;&#30693;&#35782;&#65307;&#20294;&#26159;&#65292;&#36890;&#36807;&#26465;&#20214;&#37319;&#26679;&#25110;&#27979;&#37327;&#27010;&#29575;&#26469;&#26597;&#35810;&#35813;&#30693;&#35782;&#21364;&#24322;&#24120;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#25512;&#29702;&#26465;&#20214;&#37319;&#26679;&#26041;&#27861;&#20027;&#35201;&#26088;&#22312;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#26465;&#20214;&#65292;&#20294;&#36825;&#23545;&#20110;&#21305;&#37197;&#20998;&#24067;&#30340;&#32479;&#35745;&#25968;&#25454;&#25110;&#35745;&#31639;&#36873;&#25321;&#20107;&#20214;&#30340;&#27010;&#29575;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#26368;&#29702;&#24819;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#26465;&#20214;&#24471;&#20998;&#20989;&#25968;&#65292;&#20294;&#20854;&#35745;&#31639;&#36890;&#24120;&#26159;&#19981;&#21487;&#35299;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26465;&#20214;&#24471;&#20998;&#20989;&#25968;&#30340;&#27010;&#29575;&#36817;&#20284;&#26041;&#26696;&#65292;&#21487;&#20197;&#35777;&#26126;&#20854;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a class of probabilistic generative models that have been widely used as a prior for image processing tasks like text conditional generation and inpainting. We demonstrate that these models can be adapted to make predictions and provide uncertainty quantification for chaotic dynamical systems. In these applications, diffusion models can implicitly represent knowledge about outliers and extreme events; however, querying that knowledge through conditional sampling or measuring probabilities is surprisingly difficult. Existing methods for conditional sampling at inference time seek mainly to enforce the constraints, which is insufficient to match the statistics of the distribution or compute the probability of the chosen events. To achieve these ends, optimally one would use the conditional score function, but its computation is typically intractable. In this work, we develop a probabilistic approximation scheme for the conditional score function which provably conver
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#21160;&#37327;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#29616;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#32570;&#38519;&#24182;&#21152;&#20197;&#32416;&#27491;&#12290;</title><link>http://arxiv.org/abs/2306.07525</link><description>&lt;p&gt;
&#22522;&#20110;&#30896;&#25758;&#21160;&#37327;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23545;&#25239;&#34892;&#20154;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Using Collision Momentum in Deep Reinforcement Learning Based Adversarial Pedestrian Modeling. (arXiv:2306.07525v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07525
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#21160;&#37327;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#29616;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#32570;&#38519;&#24182;&#21152;&#20197;&#32416;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20851;&#34892;&#20154;&#20223;&#30495;&#30340;&#30740;&#31350;&#36890;&#24120;&#26088;&#22312;&#24320;&#21457;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#29616;&#23454;&#34892;&#20026;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#29983;&#25104;&#35782;&#21035;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#26497;&#31471;&#21644;&#19981;&#21487;&#33021;&#24773;&#20917;&#19979;&#20197;&#21450;&#36793;&#32536;&#24773;&#20917;&#19979;&#24615;&#33021;&#21644;&#32570;&#38519;&#30340;&#34892;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#34892;&#20154;&#34892;&#20026;&#31639;&#27861;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#20351;&#29992;&#31038;&#20132;&#21147;&#27169;&#22411;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30896;&#25758;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#25581;&#31034;&#20102;&#33258;&#21160;&#39550;&#39542;&#25511;&#21046;&#22120;&#30340;&#29420;&#29305;&#22833;&#25928;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#39640;&#25928;&#19988;&#29983;&#25104;&#26356;&#20005;&#37325;&#30340;&#30896;&#25758;&#65292;&#20801;&#35768;&#21457;&#29616;&#21644;&#32416;&#27491;&#22797;&#26434;&#21644;&#22810;&#26679;&#24773;&#26223;&#20013;&#33258;&#20027;&#39550;&#39542;&#31639;&#27861;&#30340;&#38382;&#39064;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in pedestrian simulation often aims to develop realistic behaviors in various situations, but it is challenging for existing algorithms to generate behaviors that identify weaknesses in automated vehicles' performance in extreme and unlikely scenarios and edge cases. To address this, specialized pedestrian behavior algorithms are needed. Current research focuses on realistic trajectories using social force models and reinforcement learning based models. However, we propose a reinforcement learning algorithm that specifically targets collisions and better uncovers unique failure modes of automated vehicle controllers. Our algorithm is efficient and generates more severe collisions, allowing for the identification and correction of weaknesses in autonomous driving algorithms in complex and varied scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36816;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#30721;&#36816;&#21160;&#24819;&#35937;&#65292;&#22522;&#20110;&#35777;&#25454;&#32047;&#31215;&#23454;&#26102;&#39044;&#27979;&#21463;&#35797;&#32773;&#30340;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2306.07519</link><description>&lt;p&gt;
&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19979;&#30340;&#33041;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Decoding Brain Motor Imagery with various Machine Learning techniques. (arXiv:2306.07519v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36816;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#30721;&#36816;&#21160;&#24819;&#35937;&#65292;&#22522;&#20110;&#35777;&#25454;&#32047;&#31215;&#23454;&#26102;&#39044;&#27979;&#21463;&#35797;&#32773;&#30340;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#26159;&#34987;&#29992;&#20110;BCI&#65288;&#33041;&#26426;&#25509;&#21475;&#65289;&#23454;&#39564;&#20013;&#30340;&#19968;&#31181;&#24050;&#34987;&#24191;&#27867;&#35760;&#24405;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#35843;&#33410;&#22823;&#33041;&#30382;&#23618;&#20197;&#21450;&#21608;&#22260;&#21306;&#22495;&#30340;&#33041;&#27963;&#21160;&#12290;&#22312;&#25105;&#20204;&#30340;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#65292;&#25351;&#23548;&#21463;&#35797;&#32773;&#36827;&#34892;&#34987;&#20998;&#20026;&#21491;&#20391;&#21644;&#24038;&#20391;&#20004;&#31867;&#30340;&#36816;&#21160;&#24819;&#35937;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#65288;&#20957;&#33014;&#21644;POLiTag&#65289;&#30340;&#30005;&#26497;&#65292;&#24182;&#37319;&#38598;&#20102;&#27599;&#20301;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#24212;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#22522;&#20110;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#21019;&#24314;&#35299;&#30721;&#22120;&#65292;&#21033;&#29992;&#35777;&#25454;&#32047;&#31215;&#23454;&#26102;&#39044;&#27979;&#21463;&#35797;&#32773;&#30340;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motor imagery (MI) is a well-documented technique used by subjects in BCI (Brain Computer Interface) experiments to modulate brain activity within the motor cortex and surrounding areas of the brain. In our term project, we conducted an experiment in which the subjects were instructed to perform motor imagery that would be divided into two classes (Right and Left). Experiments were conducted with two different types of electrodes (Gel and POLiTag) and data for individual subjects was collected. In this paper, we will apply different machine learning (ML) methods to create a decoder based on offline training data that uses evidence accumulation to predict a subject's intent from their modulated brain signals in real-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#30340;&#21464;&#20998;&#26694;&#26550;nPUGraph&#65292;&#24182;&#24341;&#20837;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#24605;&#36776;&#24615;&#25512;&#29702;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07512</link><description>&lt;p&gt;
&#33258;&#35757;&#32451;&#23454;&#29616;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#65292;&#22312;&#24605;&#36776;&#24615;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning. (arXiv:2306.07512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#30340;&#21464;&#20998;&#26694;&#26550;nPUGraph&#65292;&#24182;&#24341;&#20837;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#24605;&#36776;&#24615;&#25512;&#29702;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#24605;&#36776;&#24615;&#25512;&#29702;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#20551;&#36127;&#38382;&#39064;&#65288;&#21363;&#28508;&#22312;&#30340;&#30495;&#23454;&#20107;&#23454;&#34987;&#25490;&#38500;&#65289;&#21644;&#20551;&#27491;&#38382;&#39064;&#65288;&#21363;&#19981;&#21487;&#38752;&#25110;&#36807;&#26102;&#30340;&#20107;&#23454;&#34987;&#21253;&#25324;&#65289;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#24605;&#36776;&#24615;&#25512;&#29702;&#33021;&#21147;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#20551;&#35774;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#27491;&#30830;&#20165;&#30001;&#23427;&#22312;KG&#20013;&#30340;&#23384;&#22312;&#30830;&#23450;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#20551;&#38452;&#24615;/&#20551;&#38451;&#24615;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#34987;&#35268;&#23450;&#20026;&#19968;&#31181;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26694;&#26550;nPUGraph&#65292;&#23427;&#20849;&#21516;&#20272;&#35745;&#24050;&#25910;&#38598;&#21644;&#26410;&#25910;&#38598;&#20107;&#23454;&#30340;&#27491;&#30830;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#26631;&#31614;&#21518;&#39564;&#27010;&#29575;&#8221;&#65289;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#26631;&#31614;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#20174;&#20004;&#20010;&#26041;&#38754;&#20419;&#36827;&#20102;&#24605;&#36776;&#24615;&#25512;&#29702;&#12290;&#39318;&#20808;&#65292;&#23427;&#25552;&#39640;&#20102;&#26631;&#31614;&#21518;&#39564;&#27010;&#29575;&#24863;&#30693;&#30340;&#22270;&#34920;&#24449;&#23545;&#25239;&#20551;&#38451;&#24615;&#20851;&#31995;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#27425;&#65292;&#23427;&#30830;&#23450;&#20102;&#35823;&#23548;&#24615;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#20943;&#23569;&#20102;&#20854;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#20004;&#20010;&#30693;&#35782;&#22270;&#25512;&#29702;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies speculative reasoning task on real-world knowledge graphs (KG) that contain both \textit{false negative issue} (i.e., potential true facts being excluded) and \textit{false positive issue} (i.e., unreliable or outdated facts being included). State-of-the-art methods fall short in the speculative reasoning ability, as they assume the correctness of a fact is solely determined by its presence in KG, making them vulnerable to false negative/positive issues. The new reasoning task is formulated as a noisy Positive-Unlabeled learning problem. We propose a variational framework, namely nPUGraph, that jointly estimates the correctness of both collected and uncollected facts (which we call \textit{label posterior}) and updates model parameters during training. The label posterior estimation facilitates speculative reasoning from two perspectives. First, it improves the robustness of a label posterior-aware graph encoder against false positive links. Second, it identifies mis
&lt;/p&gt;</description></item><item><title>PaVa&#26159;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#35895;&#23547;&#25214;&#30340;&#26032;&#22411;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#12289;&#39640;&#25928;&#22320;&#21457;&#29616;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.07503</link><description>&lt;p&gt;
PaVa: &#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36335;&#24452;&#35895;&#23547;&#25214;&#30340;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PaVa: a novel Path-based Valley-seeking clustering algorithm. (arXiv:2306.07503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07503
&lt;/p&gt;
&lt;p&gt;
PaVa&#26159;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#35895;&#23547;&#25214;&#30340;&#26032;&#22411;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#12289;&#39640;&#25928;&#22320;&#21457;&#29616;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22810;&#28041;&#21450;&#26356;&#22797;&#26434;&#25968;&#25454;&#38598;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#32858;&#31867;&#30340;&#24418;&#29366;&#24448;&#24448;&#26159;&#20219;&#24847;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36335;&#24452;&#35895;&#23547;&#25214;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#26088;&#22312;&#23547;&#25214;&#32858;&#31867;&#20043;&#38388;&#30340;&#35895;&#65292;&#24182;&#21333;&#29420;&#25552;&#21462;&#32858;&#31867;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20102;&#19977;&#20010;&#20851;&#38190;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;&#36335;&#24452;&#36317;&#31163;&#65288;min-max&#36317;&#31163;&#65289;&#23558;&#32858;&#31867;&#20043;&#38388;&#30340;&#19981;&#35268;&#21017;&#36793;&#30028;&#65288;&#21363;&#23494;&#24230;&#35895;&#65289;&#36716;&#25442;&#20026;&#23436;&#32654;&#30340;&#29699;&#24418;&#22771;&#12290;&#20854;&#27425;&#65292;&#37319;&#29992;&#36866;&#24403;&#30340;&#23494;&#24230;&#27979;&#37327;&#26041;&#27861;$k$-distance&#26469;&#23545;&#26368;&#23567;&#29983;&#25104;&#26641;&#36827;&#34892;&#35843;&#25972;&#65292;&#20174;&#32780;&#35745;&#31639;&#20986;&#24378;&#20581;&#30340;min-max&#36317;&#31163;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#30830;&#23450;&#35895;&#30340;&#20013;&#24515;&#21644;&#21322;&#24452;&#26469;&#23547;&#25214;&#36716;&#25442;&#21518;&#30340;&#23494;&#24230;&#35895;&#12290;&#36890;&#36807;&#22312;&#36317;&#31163;&#21464;&#25442;&#21518;&#23558;&#32858;&#31867;&#21253;&#35013;&#22312;&#29699;&#24418;&#22771;&#20013;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#32858;&#31867;&#24418;&#29366;&#20219;&#24847;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#21462;&#36807;&#31243;&#20063;&#33021;&#22815;&#39640;&#25928;&#29575;&#22320;&#36827;&#34892;&#12290;&#20854;&#27425;&#65292;&#35843;&#25972;&#21518;&#30340;&#26368;&#23567;&#29983;&#25104;&#26641;&#26377;&#21161;&#20110;&#38477;&#20302;&#25968;&#25454;&#38598;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#25552;&#21462;&#29699;&#24418;&#22771;&#20043;&#38388;&#30340;&#35895;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#31934;&#24230;&#39640;&#25928;&#22320;&#21457;&#29616;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering methods are being applied to a wider range of scenarios involving more complex datasets, where the shapes of clusters tend to be arbitrary. In this paper, we propose a novel Path-based Valley-seeking clustering algorithm for arbitrarily shaped clusters. This work aims to seek the valleys among clusters and then individually extract clusters. Three vital techniques are used in this algorithm. First, path distance (minmax distance) is employed to transform the irregular boundaries among clusters, that is density valleys, into perfect spherical shells. Second, a suitable density measurement, $k$-distance, is employed to make adjustment on Minimum Spanning Tree, by which a robust minmax distance is calculated. Third, we seek the transformed density valleys by determining their centers and radius. First, the clusters are wrapped in spherical shells after the distance transformation, making the extraction process efficient even with clusters of arbitrary shape. Second, adjusted Mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEDO&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#19988;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#26631;&#31614;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24847;&#35265;&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#25552;&#39640;&#20102;&#35813;&#31995;&#32479;&#22312;&#21508;&#20010;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07499</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#21644;&#37325;&#20889;&#25552;&#39640;&#22522;&#20110;&#24847;&#35265;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Improving Opinion-based Question Answering Systems Through Label Error Detection and Overwrite. (arXiv:2306.07499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEDO&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#19988;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#26631;&#31614;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24847;&#35265;&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#25552;&#39640;&#20102;&#35813;&#31995;&#32479;&#22312;&#21508;&#20010;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#38169;&#35823;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#22823;&#37327;&#30340;&#26631;&#31614;&#38169;&#35823;&#20250;&#20005;&#37325;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26631;&#31614;&#38169;&#35823;&#38382;&#39064;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#26550;&#26500;&#65292;&#35201;&#20040;&#38656;&#35201;&#38750;&#24120;&#22797;&#26434;&#30340;&#39069;&#22806;&#35745;&#31639;&#65292;&#36825;&#20123;&#37117;&#19981;&#36866;&#21512;&#24037;&#19994;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEDO&#65306;&#19968;&#31181;&#38754;&#21521;&#27169;&#22411;&#30340;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#21644;&#37325;&#20889;&#26694;&#26550;&#12290;LEDO&#22522;&#20110; Monte Carlo Dropout &#21644;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25512;&#24191;&#21040;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#12290;&#23558;LEDO&#24212;&#29992;&#20110;&#24037;&#19994;&#24847;&#35265;&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#35777;&#26126;&#23427;&#33021;&#26377;&#25928;&#25552;&#39640;&#25152;&#26377;&#26680;&#24515;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LEDO&#20026;&#26816;&#32034;&#27169;&#22411;&#24102;&#26469;1.1&#65285;&#30340;MRR&#22686;&#30410;&#65292;&#20026;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;&#25552;&#39640;1.5&#65285;&#30340;PR AUC&#65292;&#20026;&#25490;&#21517;&#22120;&#30340;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;0.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label error is a ubiquitous problem in annotated data. Large amounts of label error substantially degrades the quality of deep learning models. Existing methods to tackle the label error problem largely focus on the classification task, and either rely on task specific architecture or require non-trivial additional computations, which is undesirable or even unattainable for industry usage. In this paper, we propose LEDO: a model-agnostic and computationally efficient framework for Label Error Detection and Overwrite. LEDO is based on Monte Carlo Dropout combined with uncertainty metrics, and can be easily generalized to multiple tasks and data sets. Applying LEDO to an industry opinion-based question answering system demonstrates it is effective at improving accuracy in all the core models. Specifically, LEDO brings 1.1% MRR gain for the retrieval model, 1.5% PR AUC improvement for the machine reading comprehension model, and 0.9% rise in the Average Precision for the ranker, on top of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22312;&#19968;&#33324;&#30340;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#37327;&#21270;FL&#31639;&#27861;&#65292;&#37319;&#29992;&#26032;&#30340;&#38543;&#26426;&#37327;&#21270;&#26041;&#26696;&#21644;&#21152;&#26435;&#24179;&#22343;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#30340;&#24191;&#20041;&#23567;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#20013;&#65292;&#20197;&#36866;&#24212;&#22312;&#24037;&#20316;&#33410;&#28857;&#20855;&#26377;&#22343;&#21248;&#25110;&#38750;&#22343;&#21248;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.07497</link><description>&lt;p&gt;
GQFedWAvg&#65306;&#22522;&#20110;&#20248;&#21270;&#30340;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#19968;&#33324;&#30340;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GQFedWAvg: Optimization-Based Quantized Federated Learning in General Edge Computing Systems. (arXiv:2306.07497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22312;&#19968;&#33324;&#30340;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#37327;&#21270;FL&#31639;&#27861;&#65292;&#37319;&#29992;&#26032;&#30340;&#38543;&#26426;&#37327;&#21270;&#26041;&#26696;&#21644;&#21152;&#26435;&#24179;&#22343;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#30340;&#24191;&#20041;&#23567;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#20013;&#65292;&#20197;&#36866;&#24212;&#22312;&#24037;&#20316;&#33410;&#28857;&#20855;&#26377;&#22343;&#21248;&#25110;&#38750;&#22343;&#21248;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#26368;&#20339;&#23454;&#29616;&#19968;&#30452;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#37327;&#21270;FL&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#24037;&#20316;&#33410;&#28857;&#20855;&#26377;&#22343;&#21248;&#25110;&#38750;&#22343;&#21248;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#19968;&#33324;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#36866;&#24403;&#22320;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#37327;&#21270;&#26041;&#26696;&#24182;&#20998;&#26512;&#20102;&#20854;&#24615;&#36136;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#37327;&#21270;FL&#31639;&#27861;&#65292;&#21363;GQFedWAvg&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GQFedWAvg&#23558;&#25152;&#25552;&#20986;&#30340;&#37327;&#21270;&#26041;&#26696;&#24212;&#29992;&#20110;&#26234;&#33021;&#36873;&#25321;&#30340;&#27169;&#22411;&#26356;&#26032;&#30456;&#20851;&#21521;&#37327;&#65292;&#24182;&#37319;&#29992;&#21152;&#26435;&#24179;&#22343;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#30340;&#24191;&#20041;&#23567;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#22312;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#20013;&#12290;&#27492;&#22806;&#65292;GQFedWAvg&#26377;&#19968;&#20123;&#21487;&#35843;&#25972;&#30340;&#31639;&#27861;&#21442;&#25968;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#36866;&#24212;&#26381;&#21153;&#22120;&#21644;&#24037;&#20316;&#33410;&#28857;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;GQFedWAvg&#30340;&#25910;&#25947;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20248;&#21270;&#35813;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal implementation of federated learning (FL) in practical edge computing systems has been an outstanding problem. In this paper, we propose an optimization-based quantized FL algorithm, which can appropriately fit a general edge computing system with uniform or nonuniform computing and communication resources at the workers. Specifically, we first present a new random quantization scheme and analyze its properties. Then, we propose a general quantized FL algorithm, namely GQFedWAvg. Specifically, GQFedWAvg applies the proposed quantization scheme to quantize wisely chosen model update-related vectors and adopts a generalized mini-batch stochastic gradient descent (SGD) method with the weighted average local model updates in global model aggregation. Besides, GQFedWAvg has several adjustable algorithm parameters to flexibly adapt to the computing and communication resources at the server and workers. We also analyze the convergence of GQFedWAvg. Next, we optimize the algorithm 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#20248;&#21270;&#30340;&#26041;&#24335;&#23398;&#20064;&#38750;&#24402;&#19968;&#21270;&#32479;&#35745;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#22122;&#22768;&#20998;&#24067;&#22788;&#29702;&#20998;&#21306;&#20989;&#25968;&#65292;&#20855;&#26377;&#20248;&#21270;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.07485</link><description>&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#20248;&#21270;&#23398;&#20064;&#38750;&#24402;&#19968;&#21270;&#32479;&#35745;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Unnormalized Statistical Models via Compositional Optimization. (arXiv:2306.07485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#20248;&#21270;&#30340;&#26041;&#24335;&#23398;&#20064;&#38750;&#24402;&#19968;&#21270;&#32479;&#35745;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#22122;&#22768;&#20998;&#24067;&#22788;&#29702;&#20998;&#21306;&#20989;&#25968;&#65292;&#20855;&#26377;&#20248;&#21270;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#38750;&#24402;&#19968;&#21270;&#32479;&#35745;&#27169;&#22411;&#65288;&#22914;&#33021;&#37327;&#27169;&#22411;&#65289;&#30001;&#20110;&#22788;&#29702;&#20998;&#21306;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#32780;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#36991;&#24320;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#24050;&#34987;&#25552;&#20986;&#26469;&#23558;&#30446;&#26631;&#20844;&#24335;&#21270;&#20026;&#23454;&#38469;&#25968;&#25454;&#21644;&#20154;&#20026;&#22122;&#22768;&#30340;&#36923;&#36753;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#20197;&#21069;&#30340;&#30740;&#31350;&#25152;&#21457;&#29616;&#30340;&#37027;&#26679;&#65292;&#30001;&#20110;&#20854;&#24179;&#22374;&#30340;&#25439;&#22833;&#20989;&#25968;&#22270;&#26223;&#21644;&#32531;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;NCE&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26412;&#25991;&#20174;&#32452;&#21512;&#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#19968;&#31181;&#30452;&#25509;&#20248;&#21270;&#38750;&#24402;&#19968;&#21270;&#27169;&#22411;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22788;&#29702;&#20998;&#21306;&#20989;&#25968;&#65292;&#24341;&#20837;&#20102;&#22122;&#22768;&#20998;&#24067;&#65292;&#20351;&#24471;&#23545;&#25968;&#20998;&#21306;&#20989;&#25968;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#20010;&#32452;&#21512;&#20989;&#25968;&#65292;&#20869;&#37096;&#20989;&#25968;&#21487;&#20197;&#29992;&#38543;&#26426;&#26679;&#26412;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#23427;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#23427;&#22312;&#23398;&#20064;&#38750;&#24402;&#19968;&#21270;&#32479;&#35745;&#27169;&#22411;&#30340;&#20248;&#21270;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#27604;NCE&#26356;&#26377;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning unnormalized statistical models (e.g., energy-based models) is computationally challenging due to the complexity of handling the partition function. To eschew this complexity, noise-contrastive estimation~(NCE) has been proposed by formulating the objective as the logistic loss of the real data and the artificial noise. However, as found in previous works, NCE may perform poorly in many tasks due to its flat loss landscape and slow convergence. In this paper, we study it a direct approach for optimizing the negative log-likelihood of unnormalized models from the perspective of compositional optimization. To tackle the partition function, a noise distribution is introduced such that the log partition function can be written as a compositional function whose inner function can be estimated with stochastic samples. Hence, the objective can be optimized by stochastic compositional optimization algorithms. Despite being a simple method, we demonstrate that it is more favorable than
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#23545;&#22810;&#20010;&#38774;&#28857;&#26377;&#25928;&#30340;&#20998;&#23376;&#65288;&#21253;&#25324;&#956;&#12289;&#954;&#21644;&#948;&#38463;&#29255;&#21463;&#20307;&#65289;&#65292;&#24182;&#19988;&#24050;&#32463;&#26500;&#24314;&#20102;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.07484</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#32593;&#32476;&#22797;&#26434;&#20307;&#36827;&#34892;&#22810;&#30446;&#26631;&#20998;&#23376;&#20248;&#21270;&#30340;&#38463;&#29255;&#31867;&#33647;&#29289;&#28389;&#29992;&#38556;&#30861;&#27835;&#30103;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Molecular Optimization for Opioid Use Disorder Treatment Using Generative Network Complex. (arXiv:2306.07484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#23545;&#22810;&#20010;&#38774;&#28857;&#26377;&#25928;&#30340;&#20998;&#23376;&#65288;&#21253;&#25324;&#956;&#12289;&#954;&#21644;&#948;&#38463;&#29255;&#21463;&#20307;&#65289;&#65292;&#24182;&#19988;&#24050;&#32463;&#26500;&#24314;&#20102;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#29255;&#31867;&#33647;&#29289;&#28389;&#29992;&#38556;&#30861;&#65288;OUD&#65289;&#24050;&#25104;&#20026;&#19968;&#20010;&#22797;&#26434;&#22810;&#38754;&#30340;&#20840;&#29699;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#12290;&#30001;&#20110;&#32570;&#20047;&#23545;&#21508;&#31181;&#24773;&#20917;&#26377;&#25928;&#30340;&#27835;&#30103;&#36873;&#25321;&#65292;&#22240;&#27492;&#36843;&#20999;&#38656;&#35201;&#21457;&#29616;&#26032;&#30340;&#33647;&#29289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#22522;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#25193;&#25955;&#24314;&#27169;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#30456;&#32467;&#21512;&#12290;&#20998;&#23376;&#29983;&#25104;&#22120;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#23545;&#22810;&#20010;&#38774;&#28857;&#26377;&#25928;&#30340;&#20998;&#23376;&#65292;&#20855;&#20307;&#21253;&#25324;&#956;&#12289;&#954;&#21644;&#948;&#38463;&#29255;&#21463;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#20998;&#23376;&#30340;&#21560;&#25910;&#12289;&#20998;&#24067;&#12289;&#20195;&#35874;&#12289;&#25490;&#27844;&#21644;&#27602;&#24615;&#65288;ADMET&#65289;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#33647;&#29289;&#26679;&#21270;&#21512;&#29289;&#12290;&#20026;&#22686;&#24378;&#26576;&#20123;&#21069;&#23548;&#21270;&#21512;&#29289;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#23376;&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#32452;&#22810;&#26679;&#30340;&#33647;&#29289;&#26679;&#21270;&#21512;&#29289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20998;&#23376;&#32454;&#21270;&#21644;&#25311;&#21512;&#65292;&#26500;&#24314;&#20102;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opioid Use Disorder (OUD) has emerged as a significant global public health issue, with complex multifaceted conditions. Due to the lack of effective treatment options for various conditions, there is a pressing need for the discovery of new medications. In this study, we propose a deep generative model that combines a stochastic differential equation (SDE)-based diffusion modeling with the latent space of a pretrained autoencoder model. The molecular generator enables efficient generation of molecules that are effective on multiple targets, specifically the mu, kappa, and delta opioid receptors. Furthermore, we assess the ADMET (absorption, distribution, metabolism, excretion, and toxicity) properties of the generated molecules to identify drug-like compounds. To enhance the pharmacokinetic properties of some lead compounds, we employ a molecular optimization approach. We obtain a diverse set of drug-like molecules. We construct binding affinity predictors by integrating molecular fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#32463;&#20856;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#36890;&#36807;&#24809;&#32602;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#21019;&#24314;&#32773;&#65292;&#25104;&#21151;&#22320;&#28608;&#21169;&#20102;&#29983;&#20135;&#32773;&#21019;&#36896;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2306.07479</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Incentivizing High-Quality Content in Online Recommender Systems. (arXiv:2306.07479v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#32463;&#20856;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#36890;&#36807;&#24809;&#32602;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#21019;&#24314;&#32773;&#65292;&#25104;&#21151;&#22320;&#28608;&#21169;&#20102;&#29983;&#20135;&#32773;&#21019;&#36896;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20687;TikTok&#21644;YouTube&#36825;&#26679;&#30340;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#65292;&#24179;&#21488;&#30340;&#20915;&#31574;&#31639;&#27861;&#22609;&#36896;&#20102;&#20869;&#23481;&#29983;&#20135;&#32773;&#30340;&#28608;&#21169;&#65292;&#21253;&#25324;&#29983;&#20135;&#32773;&#22312;&#20869;&#23481;&#36136;&#37327;&#19978;&#25237;&#20837;&#22810;&#23569;&#21162;&#21147;&#12290;&#35768;&#22810;&#24179;&#21488;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#65292;&#36825;&#20250;&#20135;&#29983;&#36328;&#26102;&#38388;&#30340;&#28608;&#21169;&#65292;&#22240;&#20026;&#20170;&#22825;&#29983;&#20135;&#30340;&#20869;&#23481;&#20250;&#24433;&#21709;&#26410;&#26469;&#20869;&#23481;&#30340;&#25512;&#33616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20135;&#29983;&#30340;&#28608;&#21169;&#65292;&#20998;&#26512;&#20102;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#29983;&#20135;&#30340;&#20869;&#23481;&#36136;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20687;Hedge&#21644;EXP3&#36825;&#26679;&#30340;&#32463;&#20856;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;&#29305;&#21035;&#22320;&#65292;&#20869;&#23481;&#36136;&#37327;&#22312;&#23398;&#20064;&#29575;&#26041;&#38754;&#26377;&#19978;&#38480;&#65292;&#24182;&#19988;&#38543;&#30528;&#20856;&#22411;&#23398;&#20064;&#29575;&#36827;&#23637;&#32780;&#36235;&#36817;&#20110;&#38646;&#12290;&#22312;&#36825;&#19968;&#36127;&#38754;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#24809;&#32602;&#21019;&#24314;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#29983;&#20135;&#32773;&#8212;&#8212;&#27491;&#30830;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#39640;&#36136;&#37327;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#26032;&#39062;&#30340;&#31574;&#30053;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#20811;&#26381;&#20102;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#24212;&#29992;&#23545;&#25239;&#24615;&#25216;&#26415;&#30340;&#25361;&#25112;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25104;&#21151;&#22320;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#39640;&#36136;&#37327;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
For content recommender systems such as TikTok and YouTube, the platform's decision algorithm shapes the incentives of content producers, including how much effort the content producers invest in the quality of their content. Many platforms employ online learning, which creates intertemporal incentives, since content produced today affects recommendations of future content. In this paper, we study the incentives arising from online learning, analyzing the quality of content produced at a Nash equilibrium. We show that classical online learning algorithms, such as Hedge and EXP3, unfortunately incentivize producers to create low-quality content. In particular, the quality of content is upper bounded in terms of the learning rate and approaches zero for typical learning rate schedules. Motivated by this negative result, we design a different learning algorithm -- based on punishing producers who create low-quality content -- that correctly incentivizes producers to create high-quality co
&lt;/p&gt;</description></item><item><title>VoxMol&#26159;&#19968;&#31181;&#26681;&#25454;&#20998;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;3D&#20998;&#23376;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20174;&#22122;&#22768;&#20998;&#23376;&#30340;&#24179;&#28369;&#20998;&#24067;&#21040;&#30495;&#23454;&#20998;&#23376;&#30340;&#20998;&#24067;&#30340;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#19982;&#24403;&#21069;&#20808;&#36827;&#25216;&#26415;&#19981;&#21516;&#65292;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#35757;&#32451;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07473</link><description>&lt;p&gt;
&#21435;&#22122;&#22768;&#28857;&#38453;&#26684;&#29983;&#25104;3D&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
3D molecule generation by denoising voxel grids. (arXiv:2306.07473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07473
&lt;/p&gt;
&lt;p&gt;
VoxMol&#26159;&#19968;&#31181;&#26681;&#25454;&#20998;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;3D&#20998;&#23376;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20174;&#22122;&#22768;&#20998;&#23376;&#30340;&#24179;&#28369;&#20998;&#24067;&#21040;&#30495;&#23454;&#20998;&#23376;&#30340;&#20998;&#24067;&#30340;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#19982;&#24403;&#21069;&#20808;&#36827;&#25216;&#26415;&#19981;&#21516;&#65292;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#35757;&#32451;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#34920;&#31034;&#20026;&#35268;&#21017;&#32593;&#26684;&#19978;&#30340;&#21407;&#23376;&#23494;&#24230;&#30340;3D&#20998;&#23376;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#21435;&#22122;&#22768;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23398;&#20064;&#20174;&#22122;&#22768;&#20998;&#23376;&#30340;&#24179;&#28369;&#20998;&#24067;&#21040;&#30495;&#23454;&#20998;&#23376;&#30340;&#20998;&#24067;&#30340;&#26144;&#23556;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36981;&#24490;&#31070;&#32463;&#32463;&#39564;&#36125;&#21494;&#26031;&#26694;&#26550; [Saremi&#21644;Hyvarinen&#65292;2019]&#65292;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#29983;&#25104;&#20998;&#23376;&#65306;&#65288;i&#65289;&#36890;&#36807;&#27424;&#38459;&#23612;Langevin&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#20174;&#24179;&#28369;&#20998;&#24067;&#20013;&#37319;&#26679;&#24102;&#22122;&#22768;&#30340;&#23494;&#24230;&#32593;&#26684;&#65292;&#65288;ii&#65289;&#36890;&#36807;&#21333;&#27493;&#21435;&#22122;&#22122;&#22768;&#26684;&#65292;&#36824;&#21407;&#8220;&#24178;&#20928;&#8221;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;VoxMol&#26159;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#20110;&#24403;&#21069;&#29616;&#26377;&#25216;&#26415;&#65288;&#21363;&#24212;&#29992;&#20110;&#21407;&#23376;&#28857;&#20113;&#30340;&#25193;&#25955;&#27169;&#22411;&#65289;&#30340;&#29983;&#25104;&#20998;&#23376;&#30340;&#26041;&#27861;&#12290;&#23427;&#22312;&#25968;&#25454;&#34920;&#31034;&#12289;&#22122;&#22768;&#27169;&#22411;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#29983;&#25104;&#24314;&#27169;&#31639;&#27861;&#26041;&#38754;&#19981;&#21516;&#12290;VoxMol&#22312;&#26080;&#26465;&#20214;3D&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#19982;&#29616;&#26377;&#25216;&#26415;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#35757;&#32451;&#31616;&#21333;&#19988;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new score-based approach to generate 3D molecules represented as atomic densities on regular grids. First, we train a denoising neural network that learns to map from a smooth distribution of noisy molecules to the distribution of real molecules. Then, we follow the neural empirical Bayes framework [Saremi and Hyvarinen, 2019] and generate molecules in two steps: (i) sample noisy density grids from a smooth distribution via underdamped Langevin Markov chain Monte Carlo, and (ii) recover the ``clean'' molecule by denoising the noisy grid with a single step. Our method, VoxMol, generates molecules in a fundamentally different way than the current state of the art (i.e., diffusion models applied to atom point clouds). It differs in terms of the data representation, the noise model, the network architecture and the generative modeling algorithm. VoxMol achieves comparable results to state of the art on unconditional 3D molecule generation while being simpler to train and faste
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#37319;&#26679;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22810;&#25968;&#19987;&#27880;&#20110;&#20998;&#24067;&#20013;&#30340;&#27169;&#24335;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;Von Mises&#28151;&#21512;&#20998;&#24067;&#29992;&#20110;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.07472</link><description>&lt;p&gt;
Von Mises&#28151;&#21512;&#20998;&#24067;&#29992;&#20110;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Von Mises Mixture Distributions for Molecular Conformation Generation. (arXiv:2306.07472v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07472
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#37319;&#26679;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22810;&#25968;&#19987;&#27880;&#20110;&#20998;&#24067;&#20013;&#30340;&#27169;&#24335;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;Von Mises&#28151;&#21512;&#20998;&#24067;&#29992;&#20110;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#32463;&#24120;&#34987;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#20294;&#26159;&#22522;&#30784;&#30340;&#19977;&#32500;&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#65288;&#21407;&#23376;&#30340;&#20301;&#32622;&#65289;&#26368;&#32456;&#20915;&#23450;&#20102;&#22823;&#22810;&#25968;&#20998;&#23376;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20998;&#23376;&#22312;&#24120;&#28201;&#19979;&#37117;&#19981;&#26159;&#38745;&#24577;&#30340;&#65292;&#24182;&#37319;&#21462;&#21508;&#31181;&#21508;&#26679;&#30340;&#20960;&#20309;&#32467;&#26500;&#25110;$\textit{&#26500;&#35937;}$&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#20960;&#20309;&#32467;&#26500;&#20998;&#24067;$p(x)$&#31216;&#20026;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#35768;&#22810;&#20998;&#23376;&#24615;&#36136;&#37117;&#26159;&#22312;&#35813;&#20998;&#24067;&#19979;&#35745;&#31639;&#30340;&#26399;&#26395;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#20174;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#23545;&#20110;&#20934;&#30830;&#35745;&#31639;&#36825;&#20123;&#26399;&#26395;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#22823;&#37096;&#20998;&#26368;&#36817;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#35782;&#21035;&#35813;&#20998;&#24067;&#20013;&#30340;$\textit{&#27169;&#24335;}$&#65292;&#32780;&#19981;&#26159;&#29983;&#25104;&#30495;&#27491;&#30340;$\textit{&#26679;&#26412;}$&#12290;&#29983;&#25104;&#36825;&#26679;&#30340;&#26679;&#26412;&#38656;&#35201;&#25429;&#25417;&#26500;&#35937;&#21464;&#24322;&#24615;&#65292;&#20154;&#20204;&#24191;&#27867;&#35748;&#20026;&#20998;&#23376;&#30340;&#22823;&#22810;&#25968;&#26500;&#35937;&#21464;&#24322;&#24615;&#26469;&#33258;&#20110;&#21270;&#23398;&#38190;&#30340;&#26059;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecules are frequently represented as graphs, but the underlying 3D molecular geometry (the locations of the atoms) ultimately determines most molecular properties. However, most molecules are not static and at room temperature adopt a wide variety of geometries or $\textit{conformations}$. The resulting distribution on geometries $p(x)$ is known as the Boltzmann distribution, and many molecular properties are expectations computed under this distribution. Generating accurate samples from the Boltzmann distribution is therefore essential for computing these expectations accurately. Traditional sampling-based methods are computationally expensive, and most recent machine learning-based methods have focused on identifying $\textit{modes}$ in this distribution rather than generating true $\textit{samples}$. Generating such samples requires capturing conformational variability, and it has been widely recognized that the majority of conformational variability in molecules arises from rota
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#29616;&#20302;&#36951;&#25022;&#29575;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.07465</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#24179;&#31283;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#40657;&#30418;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning. (arXiv:2306.07465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#29616;&#20302;&#36951;&#25022;&#29575;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#24179;&#31283;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23398;&#20064;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#21306;&#21035;&#20110;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#36172;&#24466;&#21453;&#39304;&#30340;&#28216;&#25103;&#65292;&#20854;&#20013;&#21363;&#20351;&#24453;&#27979;&#35797;&#30340;&#24046;&#36317;&#24456;&#23567;&#65292;&#27979;&#35797;&#19968;&#20010;&#22343;&#34913;&#20063;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#30340;&#36951;&#25022;&#65292;&#24182;&#19988;&#22312;&#38745;&#24577;&#28216;&#25103;&#20013;&#23384;&#22312;&#22810;&#20010;&#26368;&#20248;&#35299;&#65288;&#22343;&#34913;&#65289;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#38590;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#22914;&#19968;&#33324;&#21644;&#21338;&#24328;&#12289;&#28508;&#22312;&#21338;&#24328;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21482;&#35201;&#22312;&#38745;&#24577;&#29615;&#22659;&#19979;&#37197;&#22791;&#36866;&#24403;&#30340;&#23398;&#20064;&#21644;&#27979;&#35797;&#31070;&#35861;&#12290;&#24403;&#38750;&#24179;&#31283;&#31243;&#24230;&#65288;&#36890;&#36807;&#24635;&#21464;&#21270;&#37327; $\Delta$ &#27979;&#37327;&#65289;&#24050;&#30693;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616; $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ &#30340;&#36951;&#25022;&#65292;&#24403; $\Delta$ &#26410;&#30693;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616; $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ &#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ regret when the degree of nonstationarity, as measured by total variation $\Delta$, is known, and $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ regret when $\Delta$ is unknown, where $T$ is the number of rounds. Meanwhile, 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#27454;&#21517;&#20026; Account Prioritizer &#30340;&#26234;&#33021;&#38144;&#21806;&#36134;&#25143;&#20248;&#20808;&#32423;&#24341;&#25806;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#35299;&#37322;&#31639;&#27861;&#33258;&#21160;&#21270;&#38144;&#21806;&#31807;&#20248;&#21270;&#65292;&#22312; LinkedIn Business &#20013;&#25104;&#21151;&#24102;&#26469;&#20102; +8.08% &#30340;&#32493;&#35746;&#35746;&#38405;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2306.07464</link><description>&lt;p&gt;
&#35299;&#38145;&#38144;&#21806;&#22686;&#38271;&#65306;&#20855;&#26377;&#21487;&#35299;&#37322; AI &#30340;&#36134;&#25143;&#20248;&#20808;&#32423;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Unlocking Sales Growth: Account Prioritization Engine with Explainable AI. (arXiv:2306.07464v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07464
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#27454;&#21517;&#20026; Account Prioritizer &#30340;&#26234;&#33021;&#38144;&#21806;&#36134;&#25143;&#20248;&#20808;&#32423;&#24341;&#25806;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#35299;&#37322;&#31639;&#27861;&#33258;&#21160;&#21270;&#38144;&#21806;&#31807;&#20248;&#21270;&#65292;&#22312; LinkedIn Business &#20013;&#25104;&#21151;&#24102;&#26469;&#20102; +8.08% &#30340;&#32493;&#35746;&#35746;&#38405;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
B2B &#38144;&#21806;&#38656;&#35201;&#26377;&#25928;&#39044;&#27979;&#23458;&#25143;&#22686;&#38271;&#65292;&#35782;&#21035;&#21319;&#32423;&#28508;&#21147;&#20197;&#21450;&#38477;&#20302;&#27969;&#22833;&#39118;&#38505;&#12290;LinkedIn &#30340;&#38144;&#21806;&#20195;&#34920;&#20256;&#32479;&#19978;&#20381;&#36182;&#30452;&#35273;&#21644;&#30862;&#29255;&#21270;&#25968;&#25454;&#20449;&#21495;&#26469;&#35780;&#20272;&#23458;&#25143;&#32489;&#25928;&#12290;&#36825;&#23548;&#33268;&#22312;&#25968;&#25454;&#29702;&#35299;&#21644;&#31574;&#30053;&#21046;&#23450;&#26041;&#38754;&#25237;&#20837;&#20102;&#22823;&#37327;&#26102;&#38388;&#65292;&#32780;&#22312;&#31215;&#26497;&#38144;&#21806;&#26041;&#38754;&#25237;&#36164;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#20135;&#21697;&#65292;&#31216;&#20026; Account Prioritizer&#65292;&#23427;&#26159;&#26234;&#33021;&#38144;&#21806;&#36134;&#25143;&#20248;&#20808;&#32423;&#24341;&#25806;&#12290;&#23427;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#21644;&#38598;&#25104;&#30340;&#36134;&#25143;&#32423;&#35299;&#37322;&#31639;&#27861;&#22312;&#38144;&#21806; CRM &#20013;&#33258;&#21160;&#21270;&#38144;&#21806;&#31807;&#20248;&#21270;&#30340;&#25163;&#21160;&#36807;&#31243;&#12290;&#19968;&#27425;&#25104;&#21151;&#30340; A/B &#27979;&#35797;&#34920;&#26126;&#65292;Account Prioritizer &#20026; LinkedIn Business &#24102;&#26469;&#20102;&#26174;&#33879;&#30340; +8.08% &#32493;&#35746;&#35746;&#38405;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
B2B sales requires effective prediction of customer growth, identification of upsell potential, and mitigation of churn risks. LinkedIn sales representatives traditionally relied on intuition and fragmented data signals to assess customer performance. This resulted in significant time investment in data understanding as well as strategy formulation and under-investment in active selling. To overcome this challenge, we developed a data product called Account Prioritizer, an intelligent sales account prioritization engine. It uses machine learning recommendation models and integrated account-level explanation algorithms within the sales CRM to automate the manual process of sales book prioritization. A successful A/B test demonstrated that the Account Prioritizer generated a substantial +8.08% increase in renewal bookings for the LinkedIn Business.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Removal-Based&#29305;&#24449;&#24402;&#22240;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07462</link><description>&lt;p&gt;
&#20851;&#20110;Removal-Based&#29305;&#24449;&#24402;&#22240;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Removal-Based Feature Attributions. (arXiv:2306.07462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Removal-Based&#29305;&#24449;&#24402;&#22240;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#37322;&#22522;&#20110;&#36755;&#20837;&#30340;&#22797;&#26434;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#35768;&#22810;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#26469;&#20998;&#37197;&#36755;&#20837;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25361;&#25112;&#20102;&#29305;&#24449;&#24402;&#22240;&#30340;&#40065;&#26834;&#24615;&#65292;&#25351;&#20986;&#36825;&#20123;&#26041;&#27861;&#23545;&#36755;&#20837;&#21644;&#27169;&#22411;&#25200;&#21160;&#25935;&#24863;&#65292;&#32780;&#20854;&#20182;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#40065;&#26834;&#24402;&#22240;&#26041;&#27861;&#21644;&#27169;&#22411;&#20462;&#25913;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24402;&#22240;&#40065;&#26834;&#24615;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#29305;&#24449;&#24402;&#22240;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Removal-Based&#24402;&#22240;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#36136;&#23578;&#26410;&#20840;&#38754;&#22320;&#24471;&#21040;&#29702;&#35299;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#23545;Removal-Based&#29305;&#24449;&#24402;&#22240;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38416;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#32479;&#19968;&#30340;&#20998;&#26512;&#65292;&#24182;&#22312;&#36755;&#20837;&#21644;&#27169;&#22411;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#23436;&#22909;&#21644;&#21463;&#25200;&#21160;&#30340;&#24402;&#22240;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#19978;&#38480;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To explain complex models based on their inputs, many feature attribution methods have been developed that assign importance scores to input features. However, some recent work challenges the robustness of feature attributions by showing that these methods are sensitive to input and model perturbations, while other work addresses this robustness issue by proposing robust attribution methods and model modifications. Nevertheless, previous work on attribution robustness has focused primarily on gradient-based feature attributions. In contrast, the robustness properties of removal-based attribution methods are not comprehensively well understood. To bridge this gap, we theoretically characterize the robustness of removal-based feature attributions. Specifically, we provide a unified analysis of such methods and prove upper bounds for the difference between intact and perturbed attributions, under settings of both input and model perturbations. Our empirical experiments on synthetic and re
&lt;/p&gt;</description></item><item><title>FIRE&#26159;&#19968;&#31181;&#29992;&#20110;&#20174;&#26641;&#38598;&#21512;&#20013;&#25552;&#21462;&#26131;&#20110;&#23457;&#26597;&#30340;&#31232;&#30095;&#35268;&#21017;&#23376;&#38598;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#40723;&#21169;&#22312;&#36873;&#25321;&#26102;&#34701;&#21512;&#35268;&#21017;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07432</link><description>&lt;p&gt;
FIRE&#65306;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#21487;&#35299;&#37322;&#35268;&#21017;&#25552;&#21462;&#30340;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FIRE: An Optimization Approach for Fast Interpretable Rule Extraction. (arXiv:2306.07432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07432
&lt;/p&gt;
&lt;p&gt;
FIRE&#26159;&#19968;&#31181;&#29992;&#20110;&#20174;&#26641;&#38598;&#21512;&#20013;&#25552;&#21462;&#26131;&#20110;&#23457;&#26597;&#30340;&#31232;&#30095;&#35268;&#21017;&#23376;&#38598;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#40723;&#21169;&#22312;&#36873;&#25321;&#26102;&#34701;&#21512;&#35268;&#21017;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FIRE&#65292;&#21363;Fast Interpretable Rule Extraction&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26641;&#38598;&#21512;&#20013;&#25552;&#21462;&#23569;&#37327;&#20294;&#26377;&#29992;&#30340;&#20915;&#31574;&#35268;&#21017;&#12290; FIRE&#20174;&#26641;&#38598;&#21512;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#20195;&#34920;&#24615;&#35268;&#21017;&#23376;&#38598;&#65292;&#36825;&#20123;&#23376;&#38598;&#26131;&#20110;&#30001;&#23454;&#36341;&#32773;&#26816;&#26597;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#25152;&#25552;&#21462;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;FIRE&#40723;&#21169;&#22312;&#36873;&#25321;&#26102;&#34701;&#21512;&#35268;&#21017;&#65292;&#20197;&#20415;&#35768;&#22810;&#25152;&#36873;&#20915;&#31574;&#35268;&#21017;&#20849;&#20139;&#30456;&#21516;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#35813;&#20248;&#21270;&#26694;&#26550;&#21033;&#29992;&#34701;&#21512;&#27491;&#21017;&#21270;&#24809;&#32602;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#21516;&#26102;&#37319;&#29992;&#38750;&#20984;&#31232;&#30095;&#24341;&#20837;&#24809;&#32602;&#20197;&#31215;&#26497;&#36873;&#21462;&#35268;&#21017;&#12290;FIRE&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#30001;&#20110;&#38382;&#39064;&#35268;&#27169;&#21644;&#24809;&#32602;&#30340;&#38750;&#20984;&#24615;&#32780;&#23545;&#29616;&#25104;&#27714;&#35299;&#22120;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#38382;&#39064;&#32467;&#26500;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22359;&#22352;&#26631;&#19979;&#38477;&#21407;&#29702;&#30340;&#19987;&#38376;&#27714;&#35299;&#22120;&#65307; &#25105;&#20204;&#30340;&#27714;&#35299;&#22120;&#27604;&#29616;&#26377;&#27714;&#35299;&#22120;&#36816;&#34892;&#36895;&#24230;&#24555;40&#20493;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#30456;&#27604;&#65292;FIRE&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FIRE, Fast Interpretable Rule Extraction, an optimization-based framework to extract a small but useful collection of decision rules from tree ensembles. FIRE selects sparse representative subsets of rules from tree ensembles, that are easy for a practitioner to examine. To further enhance the interpretability of the extracted model, FIRE encourages fusing rules during selection, so that many of the selected decision rules share common antecedents. The optimization framework utilizes a fusion regularization penalty to accomplish this, along with a non-convex sparsity-inducing penalty to aggressively select rules. Optimization problems in FIRE pose a challenge to off-the-shelf solvers due to problem scale and the non-convexity of the penalties. To address this, making use of problem-structure, we develop a specialized solver based on block coordinate descent principles; our solver performs up to 40x faster than existing solvers. We show in our experiments that FIRE outperform
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#22810;&#30340;&#36879;&#26126;&#24230;&#21644;&#20154;&#31867;&#25511;&#21046;&#65292;&#36171;&#20104;&#20154;&#20204;&#26356;&#22810;&#30340;&#26435;&#21147;&#65292;&#20197;&#23454;&#29616;&#20844;&#27491;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07427</link><description>&lt;p&gt;
&#37319;&#29992;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#26041;&#27861;&#23454;&#29616;&#20844;&#27491;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Towards Fair and Explainable AI using a Human-Centered AI Approach. (arXiv:2306.07427v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07427
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#22810;&#30340;&#36879;&#26126;&#24230;&#21644;&#20154;&#31867;&#25511;&#21046;&#65292;&#36171;&#20104;&#20154;&#20204;&#26356;&#22810;&#30340;&#26435;&#21147;&#65292;&#20197;&#23454;&#29616;&#20844;&#27491;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#23835;&#36215;&#20276;&#38543;&#30528;&#20960;&#20010;&#39640;&#35843;&#26696;&#20363;&#30340;&#20986;&#29616;&#65292;&#24378;&#35843;&#20102;&#22312;ML&#31995;&#32479;&#20013;&#38656;&#35201;&#20844;&#27491;&#24615;&#12289;&#38382;&#36131;&#21046;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20449;&#20219;&#24230;&#12290;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#20110;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;ML&#26041;&#27861;&#65292;&#35797;&#22270;&#20248;&#21270;&#26576;&#20123;&#24615;&#33021;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20687;&#20844;&#27491;&#24615;&#12289;&#20449;&#20219;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#31561;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#24230;&#37327;&#26631;&#20934;&#22312;&#26412;&#36136;&#19978;&#26159;&#20027;&#35266;&#30340;&#65292;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#21487;&#33021;&#19982;&#20256;&#32479;&#30340;&#24615;&#33021;&#25351;&#26631;&#19981;&#30456;&#20851;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#22810;&#30340;&#36879;&#26126;&#24230;&#21644;&#20154;&#31867;&#25511;&#21046;&#65292;&#36171;&#20104;&#20154;&#20204;&#26356;&#22810;&#30340;&#26435;&#21147;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;5&#20010;&#30740;&#31350;&#39033;&#30446;&#65292;&#26088;&#22312;&#22686;&#24378;&#20998;&#31867;&#31995;&#32479;&#21644;&#35789;&#23884;&#20837;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#31532;&#19968;&#20010;&#39033;&#30446;&#25506;&#35752;&#20102;&#24341;&#20837;&#26412;&#22320;&#27169;&#22411;&#35299;&#37322;&#20316;&#20026;&#26426;&#22120;&#25945;&#24072;&#65288;&#20247;&#21253;&#24037;&#20316;&#32773;&#65289;&#25509;&#21475;&#30340;&#25928;&#29992;/&#32570;&#38519;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#28155;&#21152;&#35299;&#37322;&#21487;&#20197;&#25903;&#25345;&#32467;&#26524;&#20449;&#20219;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of machine learning (ML) is accompanied by several high-profile cases that have stressed the need for fairness, accountability, explainability and trust in ML systems. The existing literature has largely focused on fully automated ML approaches that try to optimize for some performance metric. However, human-centric measures like fairness, trust, explainability, etc. are subjective in nature, context-dependent, and might not correlate with conventional performance metrics. To deal with these challenges, we explore a human-centered AI approach that empowers people by providing more transparency and human control.  In this dissertation, we present 5 research projects that aim to enhance explainability and fairness in classification systems and word embeddings. The first project explores the utility/downsides of introducing local model explanations as interfaces for machine teachers (crowd workers). Our study found that adding explanations supports trust calibration for the resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#30340;&#20114;&#21160;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21487;&#34892;&#24615;&#26159;&#22235;&#36275;&#21160;&#29289;&#27493;&#24577;&#36716;&#25442;&#30340;&#37325;&#35201;&#26631;&#20934;&#12290;&#20854;&#20013;&#65292;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#22815;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07419</link><description>&lt;p&gt;
DeepTransition&#65306;&#21487;&#34892;&#24615;&#23548;&#33268;&#27493;&#24577;&#36716;&#25442;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
DeepTransition: Viability Leads to the Emergence of Gait Transitions in Learning Anticipatory Quadrupedal Locomotion Skills. (arXiv:2306.07419v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#30340;&#20114;&#21160;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21487;&#34892;&#24615;&#26159;&#22235;&#36275;&#21160;&#29289;&#27493;&#24577;&#36716;&#25442;&#30340;&#37325;&#35201;&#26631;&#20934;&#12290;&#20854;&#20013;&#65292;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#22815;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#36275;&#21160;&#29289;&#22312;&#25913;&#21464;&#36816;&#21160;&#36895;&#24230;&#26102;&#33021;&#22815;&#26080;&#32541;&#22320;&#36716;&#25442;&#27493;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#21487;&#34892;&#24615;&#65288;&#21363;&#36991;&#20813;&#36300;&#20498;&#65289;&#20195;&#34920;&#27493;&#24577;&#36716;&#25442;&#30340;&#19968;&#20010;&#37325;&#35201;&#26631;&#20934;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27493;&#24577;&#36716;&#25442;&#30340;&#20986;&#29616;&#12290;&#19968;&#33268;&#20110;&#22235;&#36275;&#21160;&#29289;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#65292;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31163;&#25955;&#22320;&#24418;&#65288;&#21363;&#31359;&#36234;&#36830;&#32493;&#38388;&#38548;&#65289;&#23545;&#24378;&#21046;&#27493;&#24577;&#36716;&#25442;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#21040;&#36275;-&#36454;&#27493;&#24577;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quadruped animals seamlessly transition between gaits as they change locomotion speeds. While the most widely accepted explanation for gait transitions is energy efficiency, there is no clear consensus on the determining factor, nor on the potential effects from terrain properties. In this article, we propose that viability, i.e. the avoidance of falls, represents an important criterion for gait transitions. We investigate the emergence of gait transitions through the interaction between supraspinal drive (brain), the central pattern generator in the spinal cord, the body, and exteroceptive sensing by leveraging deep reinforcement learning and robotics tools. Consistent with quadruped animal data, we show that the walk-trot gait transition for quadruped robots on flat terrain improves both viability and energy efficiency. Furthermore, we investigate the effects of discrete terrain (i.e. crossing successive gaps) on imposing gait transitions, and find the emergence of trot-pronk transit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#25239;&#24615;&#32858;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25163;&#32676;&#26469;&#35299;&#20915;&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#26368;&#24046;&#34920;&#29616;&#32773;&#20013;&#26368;&#22351;&#30340;k&#20010;&#34920;&#29616;&#30340;&#24179;&#22343;&#34920;&#29616;&#26469;&#35299;&#20915;&#36807;&#24230;&#24754;&#35266;&#20027;&#20041;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.07408</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#32858;&#38598;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robust Reinforcement Learning through Efficient Adversarial Herding. (arXiv:2306.07408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#25239;&#24615;&#32858;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25163;&#32676;&#26469;&#35299;&#20915;&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#26368;&#24046;&#34920;&#29616;&#32773;&#20013;&#26368;&#22351;&#30340;k&#20010;&#34920;&#29616;&#30340;&#24179;&#22343;&#34920;&#29616;&#26469;&#35299;&#20915;&#36807;&#24230;&#24754;&#35266;&#20027;&#20041;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#31574;&#30053;&#35774;&#35745;&#26041;&#38754;&#34987;&#35748;&#20026;&#26159;&#19994;&#30028;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;&#23427;&#24182;&#19981;&#24635;&#33021;&#25552;&#20379;&#20581;&#22766;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#29615;&#22659;&#26292;&#38706;&#20110;&#28508;&#22312;&#24178;&#25200;&#26102;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#32463;&#36807;&#35777;&#26126;&#65292;&#20351;&#29992;&#21452;&#20154;&#21338;&#24328;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;RL&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#23545;&#25239;&#24615;&#32858;&#38598;&#26469;&#25193;&#23637;&#21452;&#20154;&#21338;&#24328;&#65292;&#24182;&#28041;&#21450;&#19968;&#20010;&#23545;&#25163;&#32676;&#65292;&#20197;&#35299;&#20915;($\textit{i}$)&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#30340;&#22256;&#38590;&#21644;($\textit{ii}$)&#30001;&#20110;&#20505;&#36873;&#23545;&#25163;&#38598;&#21487;&#33021;&#21253;&#21547;&#19981;&#22826;&#21487;&#33021;&#30340;&#24773;&#20917;&#32780;&#21487;&#33021;&#20135;&#29983;&#30340;&#36807;&#24230;&#24754;&#35266;&#20027;&#20041;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#32858;&#38598;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#29992;&#26368;&#24046;&#30340;&#34920;&#29616;&#32773;&#20013;&#26368;&#24046;&#30340;k&#20010;&#34920;&#29616;&#30340;&#24179;&#22343;&#34920;&#29616;&#26367;&#25442;&#20869;&#37096;&#20248;&#21270;&#20013;&#30340;&#26368;&#22351;&#24773;&#20917;&#34920;&#29616;&#26469;&#35299;&#20915;&#31532;&#20108;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#32858;&#38598;&#22312;&#19982;&#20854;&#20182;&#26356;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning (RL) is considered the gold standard for policy design, it may not always provide a robust solution in various scenarios. This can result in severe performance degradation when the environment is exposed to potential disturbances. Adversarial training using a two-player max-min game has been proven effective in enhancing the robustness of RL agents. In this work, we extend the two-player game by introducing an adversarial herd, which involves a group of adversaries, in order to address ($\textit{i}$) the difficulty of the inner optimization problem, and ($\textit{ii}$) the potential over pessimism caused by the selection of a candidate adversary set that may include unlikely scenarios. We first prove that adversarial herds can efficiently approximate the inner optimization problem. Then we address the second issue by replacing the worst-case performance in the inner optimization with the average performance over the worst-$k$ adversaries. We evaluate the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#21151;&#33021;&#34892;&#20026;&#35299;&#37322;&#26041;&#27861;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25805;&#32437;&#27169;&#22411;&#20197;&#24178;&#25200;&#35299;&#37322;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#28436;&#31034;&#20102;&#27969;&#34892;&#30340;&#28608;&#27963;&#26368;&#22823;&#21270;&#35299;&#37322;&#25216;&#26415;&#34987;&#25805;&#32437;&#20197;&#25913;&#21464;&#20854;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07397</link><description>&lt;p&gt;
&#23545;&#31070;&#32463;&#20803;&#28608;&#27963;&#26368;&#22823;&#21270;&#35299;&#37322;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on the Interpretation of Neuron Activation Maximization. (arXiv:2306.07397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#21151;&#33021;&#34892;&#20026;&#35299;&#37322;&#26041;&#27861;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25805;&#32437;&#27169;&#22411;&#20197;&#24178;&#25200;&#35299;&#37322;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#28436;&#31034;&#20102;&#27969;&#34892;&#30340;&#28608;&#27963;&#26368;&#22823;&#21270;&#35299;&#37322;&#25216;&#26415;&#34987;&#25805;&#32437;&#20197;&#25913;&#21464;&#20854;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#21151;&#33021;&#34892;&#20026;&#38750;&#24120;&#38590;&#20197;&#35299;&#37322;&#12290;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#26159;&#29992;&#20110;&#35299;&#37322;&#21644;&#20998;&#26512;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#32452;&#25216;&#26415;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#25214;&#21040;&#26368;&#22823;&#21270;&#32473;&#23450;&#30340;&#31070;&#32463;&#20803;&#25110;&#29305;&#24449;&#26144;&#23556;&#28608;&#27963;&#30340;&#36755;&#20837;&#12290;&#36825;&#20123;&#36755;&#20837;&#21487;&#20197;&#36873;&#25321;&#33258;&#25968;&#25454;&#38598;&#25110;&#36890;&#36807;&#20248;&#21270;&#24471;&#21040;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#21487;&#33021;&#20250;&#21463;&#21040;&#27450;&#39575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#25163;&#20026;&#27450;&#39575;&#35299;&#37322;&#32780;&#25805;&#32437;&#27169;&#22411;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25191;&#34892;&#36825;&#31181;&#25805;&#20316;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#28436;&#31034;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#30340;&#28608;&#27963;&#26368;&#22823;&#21270;&#35299;&#37322;&#25216;&#26415;&#21487;&#20197;&#34987;&#25805;&#32437;&#20197;&#25913;&#21464;&#35299;&#37322;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The internal functional behavior of trained Deep Neural Networks is notoriously difficult to interpret. Activation-maximization approaches are one set of techniques used to interpret and analyze trained deep-learning models. These consist in finding inputs that maximally activate a given neuron or feature map. These inputs can be selected from a data set or obtained by optimization. However, interpretability methods may be subject to being deceived. In this work, we consider the concept of an adversary manipulating a model for the purpose of deceiving the interpretation. We propose an optimization framework for performing this manipulation and demonstrate a number of ways that popular activation-maximization interpretation techniques associated with CNNs can be manipulated to change the interpretations, shedding light on the reliability of these methods.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;NeuGraspNet&#33021;&#22815;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#20174;&#20219;&#24847;&#35270;&#35282;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2306.07392</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#23398;&#20064;&#20219;&#24847;&#35270;&#35282;&#30340;6DoF&#26426;&#22120;&#20154;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering. (arXiv:2306.07392v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07392
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;NeuGraspNet&#33021;&#22815;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#20174;&#20219;&#24847;&#35270;&#35282;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#20316;&#22312;&#26234;&#33021;&#36741;&#21161;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#26434;&#20081;&#30340;&#29615;&#22659;&#20013;&#20174;&#20219;&#20309;&#35270;&#35282;&#26377;&#25928;&#22320;&#25235;&#21462;&#23545;&#35937;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#22330;&#26223;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;NeuGraspNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;6DoF&#25235;&#21462;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#31070;&#32463;&#20307;&#31215;&#34920;&#31034;&#21644;&#34920;&#38754;&#28210;&#26579;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#20840;&#23616;&#65288;&#22330;&#26223;&#32423;&#21035;&#65289;&#21644;&#23616;&#37096;&#65288;&#25235;&#21462;&#32423;&#21035;&#65289;&#31070;&#32463;&#34920;&#38754;&#34920;&#31034;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#22330;&#26223;&#30340;&#26410;&#35265;&#37096;&#20998;&#65292;&#20063;&#33021;&#26377;&#25928;&#22320;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25235;&#21462;&#37325;&#26032;&#35299;&#37322;&#20026;&#19968;&#20010;&#23616;&#37096;&#30340;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#38382;&#39064;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#32534;&#30721;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#21644;&#23545;&#35937;&#34920;&#38754;&#20960;&#20309;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;NeuGraspNet&#22312;&#21333;&#20010;&#35270;&#35282;&#19978;&#36816;&#34892;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#38544;&#24335;&#21644;&#21322;&#38544;&#24335;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulation is critical for admitting robotic agents to various application domains, like intelligent assistance. A major challenge therein is the effective 6DoF grasping of objects in cluttered environments from any viewpoint without requiring additional scene exploration. We introduce $\textit{NeuGraspNet}$, a novel method for 6DoF grasp detection that leverages recent advances in neural volumetric representations and surface rendering. Our approach learns both global (scene-level) and local (grasp-level) neural surface representations, enabling effective and fully implicit 6DoF grasp quality prediction, even in unseen parts of the scene. Further, we reinterpret grasping as a local neural surface rendering problem, allowing the model to encode the interaction between the robot's end-effector and the object's surface geometry. NeuGraspNet operates on single viewpoints and can sample grasp candidates in occluded scenes, outperforming existing implicit and semi-implicit baselin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RPOSST&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20174;&#36739;&#22823;&#30340;&#27979;&#35797;&#26696;&#20363;&#27744;&#20013;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#27979;&#35797;&#26679;&#20363;&#65292;&#39564;&#35777;&#20854;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#22312;&#26356;&#24191;&#27867;&#30340;&#29615;&#22659;&#19979;&#20063;&#26159;&#21487;&#38752;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.07372</link><description>&lt;p&gt;
&#20026;&#31574;&#30053;&#36873;&#25321;&#26500;&#24314;&#39640;&#25928;&#12289;&#20581;&#22766;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Composing Efficient, Robust Tests for Policy Selection. (arXiv:2306.07372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RPOSST&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20174;&#36739;&#22823;&#30340;&#27979;&#35797;&#26696;&#20363;&#27744;&#20013;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#27979;&#35797;&#26679;&#20363;&#65292;&#39564;&#35777;&#20854;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#22312;&#26356;&#24191;&#27867;&#30340;&#29615;&#22659;&#19979;&#20063;&#26159;&#21487;&#38752;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20250;&#20135;&#29983;&#35768;&#22810;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36873;&#25321;&#21738;&#31181;&#31574;&#30053;&#26102;&#65292;&#23427;&#20204;&#24517;&#39035;&#22312;&#19981;&#21487;&#35299;&#30340;&#22823;&#37327;&#29615;&#22659;&#26465;&#20214;&#19979;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;RPOSST&#65292;&#23427;&#33021;&#22815;&#20174;&#36739;&#22823;&#30340;&#27979;&#35797;&#26696;&#20363;&#27744;&#20013;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#27979;&#35797;&#26679;&#20363;&#65292;&#36825;&#26159;&#26681;&#25454;&#30456;&#23545;&#36739;&#23567;&#30340;&#26679;&#26412;&#35780;&#20272;&#26469;&#23454;&#29616;&#30340;&#12290;RPOSST&#23558;&#27979;&#35797;&#26679;&#20363;&#36873;&#25321;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#21452;&#20154;&#21338;&#24328;&#65292;&#24182;&#20248;&#21270;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;k-of-N&#20581;&#22766;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38480;&#21046;&#30456;&#23545;&#20110;&#20351;&#29992;&#27744;&#20013;&#25152;&#26377;&#27979;&#35797;&#29992;&#20363;&#30340;&#27979;&#35797;&#30340;&#35823;&#24046;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;RPOSST&#33021;&#22815;&#22312;&#19968;&#20010;&#29609;&#20855;&#21333;&#19968;&#28216;&#25103;&#12289;&#25169;&#20811;&#25968;&#25454;&#38598;&#21644;&#39640;&#20445;&#30495;&#36187;&#36710;&#27169;&#25311;&#22120;&#20013;&#21457;&#29616;&#21487;&#20197;&#35782;&#21035;&#39640;&#36136;&#37327;&#31574;&#30053;&#30340;&#19968;&#23567;&#32452;&#27979;&#35797;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern reinforcement learning systems produce many high-quality policies throughout the learning process. However, to choose which policy to actually deploy in the real world, they must be tested under an intractable number of environmental conditions. We introduce RPOSST, an algorithm to select a small set of test cases from a larger pool based on a relatively small number of sample evaluations. RPOSST treats the test case selection problem as a two-player game and optimizes a solution with provable $k$-of-$N$ robustness, bounding the error relative to a test that used all the test cases in the pool. Empirical results demonstrate that RPOSST finds a small set of test cases that identify high quality policies in a toy one-shot game, poker datasets, and a high-fidelity racing simulator.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24191;&#21578;&#24066;&#22330;&#19978;&#39044;&#31639;&#38480;&#21046;&#21644;&#25293;&#21334;&#38750;&#28608;&#21169;&#20860;&#23481;&#38382;&#39064;&#30340;&#20248;&#21270;&#31454;&#26631;&#31574;&#30053;&#65292;&#22312;&#28385;&#36275;&#24191;&#21578;&#20027;&#39044;&#26399;&#39044;&#31639;&#38480;&#21046;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#25928;&#29992;&#65307;&#24182;&#30740;&#31350;&#20102;&#36328;&#24179;&#21488;&#25552;&#20132;&#31454;&#26631;&#30340;&#22312;&#32447;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.07352</link><description>&lt;p&gt;
&#22312;&#38750; IC &#25293;&#21334;&#24191;&#21578;&#24066;&#22330;&#19978;&#30340;&#22810;&#24179;&#21488;&#39044;&#31639;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Platform Budget Management in Ad Markets with Non-IC Auctions. (arXiv:2306.07352v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24191;&#21578;&#24066;&#22330;&#19978;&#39044;&#31639;&#38480;&#21046;&#21644;&#25293;&#21334;&#38750;&#28608;&#21169;&#20860;&#23481;&#38382;&#39064;&#30340;&#20248;&#21270;&#31454;&#26631;&#31574;&#30053;&#65292;&#22312;&#28385;&#36275;&#24191;&#21578;&#20027;&#39044;&#26399;&#39044;&#31639;&#38480;&#21046;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#25928;&#29992;&#65307;&#24182;&#30740;&#31350;&#20102;&#36328;&#24179;&#21488;&#25552;&#20132;&#31454;&#26631;&#30340;&#22312;&#32447;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#24191;&#21578;&#24066;&#22330;&#19978;&#65292;&#26377;&#39044;&#31639;&#38480;&#21046;&#30340;&#24191;&#21578;&#20027;&#36890;&#36807;&#22312;&#21508;&#31181;&#24179;&#21488;&#19978;&#21453;&#22797;&#31454;&#26631;&#33719;&#24471;&#24191;&#21578;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#21487;&#33021;&#23384;&#22312;&#39044;&#31639;&#38480;&#21046;&#30340;&#28608;&#21169;&#20860;&#23481;&#25110;&#38750;&#28608;&#21169;&#20860;&#23481;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#31454;&#26631;&#19968;&#32452;&#25293;&#21334;&#21697;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26368;&#22823;&#21270;&#39044;&#26399;&#22312;&#21508;&#20010;&#25293;&#21334;&#20013;&#30340;&#24635;&#25928;&#29992;&#65292;&#21516;&#26102;&#28385;&#36275;&#24191;&#21578;&#20027;&#39044;&#26399;&#30340;&#39044;&#31639;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24191;&#21578;&#20027;&#24517;&#39035;&#22312;&#23398;&#20064;&#20854;&#20182;&#31454;&#26631;&#32773;&#30340;&#20986;&#20215;&#24773;&#20917;&#30340;&#21516;&#26102;&#65292;&#36328;&#24179;&#21488;&#25552;&#20132;&#31454;&#26631;&#30340;&#22312;&#32447;&#35774;&#32622;&#12290;&#22312;&#20840;&#20449;&#24687;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377; $O(T^{3/4})$ &#30340;&#36951;&#25022;&#20540;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#27604;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#27493;&#20240;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24191;&#21578;&#25918;&#32622;&#25293;&#21334;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#20248;&#31168;&#30340;&#32047;&#31215;&#36951;&#25022;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online advertising markets, budget-constrained advertisers acquire ad placements through repeated bidding in auctions on various platforms. We present a strategy for bidding optimally in a set of auctions that may or may not be incentive-compatible under the presence of budget constraints. Our strategy maximizes the expected total utility across auctions while satisfying the advertiser's budget constraints in expectation. Additionally, we investigate the online setting where the advertiser must submit bids across platforms while learning about other bidders' bids over time. Our algorithm has $O(T^{3/4})$ regret under the full-information setting. Finally, we demonstrate that our algorithms have superior cumulative regret on both synthetic and real-world datasets of ad placement auctions, compared to existing adaptive pacing algorithms.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26500;&#36896;&#20102;&#38024;&#23545;&#36830;&#32493;&#30697;&#38453;&#32676;&#23553;&#38381;&#19979;&#30340;&#27969;&#24418;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#30340;G-&#19981;&#21464;&#25193;&#25955;&#22320;&#22270;&#65292;&#33021;&#22815;&#23454;&#29616;&#31561;&#21464;&#19988;&#19981;&#21464;&#30340;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.07350</link><description>&lt;p&gt;
G-&#19981;&#21464;&#25193;&#25955;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
G-invariant diffusion maps. (arXiv:2306.07350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07350
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26500;&#36896;&#20102;&#38024;&#23545;&#36830;&#32493;&#30697;&#38453;&#32676;&#23553;&#38381;&#19979;&#30340;&#27969;&#24418;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#30340;G-&#19981;&#21464;&#25193;&#25955;&#22320;&#22270;&#65292;&#33021;&#22815;&#23454;&#29616;&#31561;&#21464;&#19988;&#19981;&#21464;&#30340;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25193;&#25955;&#22320;&#22270;&#22312;&#20174;&#38477;&#32500;&#21644;&#32858;&#31867;&#21040;&#25968;&#25454;&#21487;&#35270;&#21270;&#31561;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#21040;&#20174;&#19968;&#20010;&#36830;&#32493;&#30697;&#38453;&#32676;&#23553;&#38381;&#19979;&#30340;&#27969;&#24418;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#26082;&#31561;&#21464;&#21448;&#19981;&#21464;&#30340;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#33258;&#28982;&#22320;&#29992;&#20110;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#23545;&#40784;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26500;&#36896;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The diffusion maps embedding of data lying on a manifold have shown success in tasks ranging from dimensionality reduction and clustering, to data visualization. In this work, we consider embedding data sets which were sampled from a manifold which is closed under the action of a continuous matrix group. An example of such a data set are images who's planar rotations are arbitrary. The G-invariant graph Laplacian, introduced in a previous work of the authors, admits eigenfunctions in the form of tensor products between the elements of the irreducible unitary representations of the group and eigenvectors of certain matrices. We employ these eigenfunctions to derive diffusion maps that intrinsically account for the group action on the data. In particular, we construct both equivariant and invariant embeddings which can be used naturally to cluster and align the data points. We demonstrate the effectiveness of our construction with simulated data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25674;&#38144;&#8221;&#30340;&#25991;&#26412;&#21040;&#19977;&#32500;&#29289;&#20307;&#21512;&#25104;&#26041;&#27861;&#65292;&#23558;&#35768;&#22810;&#25552;&#31034;&#19968;&#36215;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20849;&#20139;&#25552;&#31034;&#20043;&#38388;&#30340;&#20248;&#21270;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#26356;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#27169;&#22411;&#65307;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#36827;&#34892;&#24179;&#28369;&#30340;&#25554;&#20540;&#65292;&#24182;&#22312;&#26032;&#30340;&#36164;&#20135;&#21644;&#31616;&#21333;&#21160;&#30011;&#20043;&#38388;&#23454;&#29616;&#30693;&#35782;&#20849;&#20139;&#21644;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2306.07349</link><description>&lt;p&gt;
ATT3D&#65306;&#25674;&#38144;&#30340;&#25991;&#26412;&#21040;&#19977;&#32500;&#29289;&#20307;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
ATT3D: Amortized Text-to-3D Object Synthesis. (arXiv:2306.07349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25674;&#38144;&#8221;&#30340;&#25991;&#26412;&#21040;&#19977;&#32500;&#29289;&#20307;&#21512;&#25104;&#26041;&#27861;&#65292;&#23558;&#35768;&#22810;&#25552;&#31034;&#19968;&#36215;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20849;&#20139;&#25552;&#31034;&#20043;&#38388;&#30340;&#20248;&#21270;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#26356;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#27169;&#22411;&#65307;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#36827;&#34892;&#24179;&#28369;&#30340;&#25554;&#20540;&#65292;&#24182;&#22312;&#26032;&#30340;&#36164;&#20135;&#21644;&#31616;&#21333;&#21160;&#30011;&#20043;&#38388;&#23454;&#29616;&#30693;&#35782;&#20849;&#20139;&#21644;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19982;&#22270;&#20687;&#21040;&#19977;&#32500;&#26041;&#27861;&#65288;&#22914;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#30456;&#32467;&#21512;&#65292;&#25991;&#26412;&#33267;&#19977;&#32500;&#24314;&#27169;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#12290; DreamFusion &#26368;&#36817;&#21462;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#28459;&#38271;&#30340;&#12289;&#22522;&#20110;&#25552;&#31034;&#30340;&#20248;&#21270;&#25165;&#33021;&#21019;&#24314;&#19977;&#32500;&#29289;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#27169;&#22411;&#21516;&#26102;&#35757;&#32451;&#35768;&#22810;&#25552;&#31034;&#26469;&#25674;&#38144;&#25552;&#31034;&#20248;&#21270;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#35757;&#32451;&#27599;&#20010;&#25552;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#25552;&#31034;&#38598;&#21512;&#20013;&#20849;&#20139;&#35745;&#31639;&#65292;&#27604;&#27599;&#20010;&#25552;&#31034;&#30340;&#20248;&#21270;&#25152;&#38656;&#30340;&#26102;&#38388;&#26356;&#30701;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;-Amortized text-to-3D (ATT3D)-&#23454;&#29616;&#20102;&#25552;&#31034;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#65292;&#20197;&#20415;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#35774;&#32622;&#65292;&#24182;&#20026;&#26032;&#36164;&#20135;&#21644;&#31616;&#21333;&#21160;&#30011;&#20043;&#38388;&#30340;&#25991;&#26412;&#36827;&#34892;&#24179;&#28369;&#30340;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-3D modelling has seen exciting progress by combining generative text-to-image models with image-to-3D methods like Neural Radiance Fields. DreamFusion recently achieved high-quality results but requires a lengthy, per-prompt optimization to create 3D objects. To address this, we amortize optimization over text prompts by training on many prompts simultaneously with a unified model, instead of separately. With this, we share computation across a prompt set, training in less time than per-prompt optimization. Our framework - Amortized text-to-3D (ATT3D) - enables knowledge-sharing between prompts to generalize to unseen setups and smooth interpolations between text for novel assets and simple animations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#20998;&#35010;&#24182;&#34892;&#21270;QCNN&#26550;&#26500;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#24179;&#31227;&#23545;&#31216;&#37327;&#23376;&#25968;&#25454;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;QCNN&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27979;&#37327;&#25928;&#29575;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07331</link><description>&lt;p&gt;
&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#35010;&#21644;&#24182;&#34892;&#21270;&#29992;&#20110;&#23398;&#20064;&#24179;&#31227;&#23545;&#31216;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Splitting and Parallelizing of Quantum Convolutional Neural Networks for Learning Translationally Symmetric Data. (arXiv:2306.07331v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#20998;&#35010;&#24182;&#34892;&#21270;QCNN&#26550;&#26500;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#24179;&#31227;&#23545;&#31216;&#37327;&#23376;&#25968;&#25454;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;QCNN&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27979;&#37327;&#25928;&#29575;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(QCNN)&#26159;&#19968;&#31181;&#26377;&#26395;&#22312;&#32463;&#20856;&#38590;&#39064;&#19978;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;(QML)&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;QCNN&#38656;&#35201;&#22823;&#37327;&#30340;&#27979;&#37327;&#29992;&#20110;&#25968;&#25454;&#23398;&#20064;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20998;&#35010;&#24182;&#34892;&#21270;QCNN(sp-QCNN)&#65292;&#23427;&#21033;&#29992;&#37327;&#23376;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#35774;&#35745;&#39640;&#25928;&#30005;&#36335;&#12290;&#36825;&#31181;&#26550;&#26500;&#20174;&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#38024;&#23545;&#20957;&#32858;&#24577;&#29289;&#29702;&#20013;&#24120;&#35265;&#30340;&#24179;&#31227;&#23545;&#31216;&#37327;&#23376;&#25968;&#25454;&#12290;&#36890;&#36807;&#22522;&#20110;&#24179;&#31227;&#23545;&#31216;&#24615;&#20998;&#35010;&#37327;&#23376;&#30005;&#36335;&#65292;sp-QCNN&#26497;&#22823;&#22320;&#24182;&#34892;&#21270;&#20102;&#20256;&#32479;&#30340;QCNN&#65292;&#32780;&#19981;&#22686;&#21152;&#37327;&#23376;&#27604;&#29305;&#25968;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27979;&#37327;&#25928;&#29575;&#65292;&#36798;&#21040;&#20102;&#37327;&#23376;&#30456;&#35782;&#21035;&#20219;&#21153;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A quantum convolutional neural network (QCNN) is a promising quantum machine learning (QML) model to achieve quantum advantages in classically intractable problems. However, QCNN requires a large number of measurements for data learning, limiting its practical applications for large-scale problems. To relieve this requirement, we propose a novel architecture called split-parallelizing QCNN (sp-QCNN), which exploits the prior knowledge of quantum data for designing efficient circuits. This architecture draws inspiration from geometric quantum machine learning and targets translationally symmetric quantum data commonly encountered in condensed matter physics. By splitting the quantum circuit based on translational symmetry, sp-QCNN substantially parallelizes conventional QCNN without increasing the number of qubits and further improves the measurement efficiency by an order of the number of qubits. To demonstrate its effectiveness, we apply sp-QCNN to a quantum phase recognition task and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#36138;&#24515; GMR&#65288;OGGMR&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#39640;&#27425;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36817;&#20284;&#20026;&#20302;&#38454;&#27169;&#22411;&#65292;&#23454;&#39564;&#34920;&#26126;&#23427;&#27604;&#29616;&#26377;&#30340;GMR&#31639;&#27861;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#65292;&#24182;&#33021;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2306.07309</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#24230;&#37327;&#21450;&#20854;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A New Probabilistic Distance Metric With Application In Gaussian Mixture Reduction. (arXiv:2306.07309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#36138;&#24515; GMR&#65288;OGGMR&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#39640;&#27425;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36817;&#20284;&#20026;&#20302;&#38454;&#27169;&#22411;&#65292;&#23454;&#39564;&#34920;&#26126;&#23427;&#27604;&#29616;&#26377;&#30340;GMR&#31639;&#27861;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#65292;&#24182;&#33021;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#20004;&#20010;&#36830;&#32493;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#35813;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#24191;&#20041;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21487;&#20197;&#29992;&#23427;&#30340;&#35299;&#26512;&#24335;&#34920;&#31034;&#65292;&#24182;&#19988;&#23427;&#28385;&#36275;&#25152;&#26377;&#30340;&#36317;&#31163;&#20844;&#24335;&#24615;&#36136;&#65292;&#20855;&#26377;&#35745;&#31639;&#36895;&#24230;&#24555;&#12289;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#29305;&#28857;&#65292;&#21313;&#20998;&#36866;&#29992;&#20110;&#23454;&#38469;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#12290;&#26412;&#25991;&#24212;&#29992;&#35813;&#26041;&#27861;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31616;&#21270;&#65288;Gaussian Mixture Reduction&#65292;GMR&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#36138;&#24515; GMR&#65288;OGGMR&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25105;&#20204;&#30340;&#36317;&#31163;&#24230;&#37327;&#20316;&#20026;&#20934;&#21017;&#65292;&#23558;&#39640;&#27425;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36817;&#20284;&#20026;&#20302;&#38454;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;OGGMR&#31639;&#27861;&#27604;&#29616;&#26377;&#30340;GMR&#31639;&#27861;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#65292;&#24182;&#33021;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new distance metric to compare two continuous probability density functions. The main advantage of this metric is that, unlike other statistical measurements, it can provide an analytic, closed-form expression for a mixture of Gaussian distributions while satisfying all metric properties. These characteristics enable fast, stable, and efficient calculations, which are highly desirable in real-world signal processing applications. The application in mind is Gaussian Mixture Reduction (GMR), which is widely used in density estimation, recursive tracking, and belief propagation. To address this problem, we developed a novel algorithm dubbed the Optimization-based Greedy GMR (OGGMR), which employs our metric as a criterion to approximate a high-order Gaussian mixture with a lower order. Experimental results show that the OGGMR algorithm is significantly faster and more efficient than state-of-the-art GMR algorithms while retaining the geometric shape of the original m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;&#31639;&#27861;LRS-PnP-DIP&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#31934;&#30830;&#39044;&#27979;&#32570;&#22833;&#20687;&#32032;&#21644;&#24102;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#25110;&#36229;&#36807;&#20102;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07308</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#21551;&#21457;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#30417;&#30563;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Hyperspectral Inpainting with the Optimisation inspired Deep Neural Network Prior. (arXiv:2306.07308v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;&#31639;&#27861;LRS-PnP-DIP&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#31934;&#30830;&#39044;&#27979;&#32570;&#22833;&#20687;&#32032;&#21644;&#24102;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#25110;&#36229;&#36807;&#20102;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#22270;&#20687;&#20855;&#26377;&#25104;&#30334;&#19978;&#21315;&#20010;&#31364;&#24102;&#35889;&#27573;&#65292;&#20256;&#36882;&#20102;&#22823;&#37327;&#30340;&#31354;&#38388;&#21644;&#35889;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20202;&#22120;&#35823;&#24046;&#21644;&#22823;&#27668;&#21464;&#21270;&#65292;&#23454;&#36341;&#20013;&#24471;&#21040;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#24120;&#24120;&#34987;&#22122;&#22768;&#21644;&#22351;&#28857;&#27745;&#26579;&#65292;&#23548;&#33268;&#32570;&#22833;&#20449;&#24687;&#21487;&#33021;&#20005;&#37325;&#30772;&#22351;&#21518;&#32493;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#21462;&#26679;&#28857;&#20462;&#22797;&#31639;&#27861;&#65292;&#31216;&#20026;&#20302;&#31209;&#31232;&#30095;&#32422;&#26463;&#25554;&#20837;&#25773;&#25918;&#31639;&#27861;&#65288;LRS-PnP&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22270;&#20687;&#30340;&#25152;&#26377;&#20809;&#35889;&#24102;&#37117;&#20002;&#22833;&#65292;LRS-PnP&#20063;&#33021;&#22815;&#39044;&#27979;&#32570;&#22833;&#30340;&#20687;&#32032;&#21644;&#24102;&#12290;&#23558;LRS-PnP&#19982;Deep Image Prior&#65288;DIP&#65289;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#31216;&#20026;LRS-PnP-DIP&#12290;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;LRS-PnP-DIP&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#20462;&#22797;&#24615;&#33021;&#25110;&#32988;&#36807;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral Image (HSI)s cover hundreds or thousands of narrow spectral bands, conveying a wealth of spatial and spectral information. However, due to the instrumental errors and the atmospheric changes, the HSI obtained in practice are often contaminated by noise and dead pixels(lines), resulting in missing information that may severely compromise the subsequent applications. We introduce here a novel HSI missing pixel prediction algorithm, called Low Rank and Sparsity Constraint Plug-and-Play (LRS-PnP). It is shown that LRS-PnP is able to predict missing pixels and bands even when all spectral bands of the image are missing. The proposed LRS-PnP algorithm is further extended to a self-supervised model by combining the LRS-PnP with the Deep Image Prior (DIP), called LRS-PnP-DIP. In a series of experiments with real data, It is shown that the LRS-PnP-DIP either achieves state-of-the-art inpainting performance compared to other learning-based methods, or outperforms them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#32032;&#21151;&#33021;&#30456;&#20284;&#24615;&#30340;&#22312;&#32447;&#21407;&#22411;&#23545;&#40784;&#65288;OPA&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31574;&#30053;&#36716;&#31227;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26144;&#23556;&#20989;&#25968;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#20197;&#21450;&#20381;&#36182;&#35270;&#35273;&#29305;&#24449;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.07307</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#31574;&#30053;&#36801;&#31227;&#30340;&#22312;&#32447;&#21407;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Online Prototype Alignment for Few-shot Policy Transfer. (arXiv:2306.07307v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#32032;&#21151;&#33021;&#30456;&#20284;&#24615;&#30340;&#22312;&#32447;&#21407;&#22411;&#23545;&#40784;&#65288;OPA&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31574;&#30053;&#36716;&#31227;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26144;&#23556;&#20989;&#25968;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#20197;&#21450;&#20381;&#36182;&#35270;&#35273;&#29305;&#24449;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39046;&#22495;&#36866;&#24212;&#20027;&#35201;&#28041;&#21450;&#22312;&#23558;&#31574;&#30053;&#36716;&#31227;&#21040;&#26032;&#29615;&#22659;&#26102;&#35266;&#23519;&#30340;&#21464;&#21270;&#12290;&#39046;&#22495;&#36866;&#24212;&#30340;&#35768;&#22810;&#20256;&#32479;&#26041;&#27861;&#20197;&#26174;&#24335;&#25110;&#38544;&#24335;&#22320;&#23398;&#20064;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#20026;&#20027;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#22495;&#30340;&#22823;&#37327;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24120;&#24120;&#20381;&#36182;&#20110;&#35270;&#35273;&#32447;&#32034;&#26469;&#23398;&#20064;&#26144;&#23556;&#20989;&#25968;&#65292;&#24403;&#28304;&#22495;&#19982;&#30446;&#26631;&#22495;&#30475;&#36215;&#26469;&#38750;&#24120;&#19981;&#21516;&#26102;&#65292;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#22312;&#32447;&#21407;&#22411;&#23545;&#40784;&#65288;OPA&#65289;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#20803;&#32032;&#30340;&#21151;&#33021;&#30456;&#20284;&#24615;&#26469;&#23398;&#20064;&#26144;&#23556;&#20989;&#25968;&#65292;&#33021;&#22815;&#22312;&#20165;&#20960;&#20010;&#22238;&#21512;&#20869;&#23454;&#29616;&#23569;&#26679;&#26412;&#31574;&#30053;&#36716;&#31227;&#12290;OPA&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#24341;&#20837;&#19968;&#20010;&#25506;&#32034;&#26426;&#21046;&#65292;&#21487;&#20197;&#39640;&#25928;&#12289;&#26377;&#30446;&#30340;&#22320;&#19982;&#30446;&#26631;&#22495;&#30340;&#26410;&#30693;&#20803;&#32032;&#20132;&#20114;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#24050;&#30693;&#30340;&#20803;&#32032;&#36830;&#25509;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation in reinforcement learning (RL) mainly deals with the changes of observation when transferring the policy to a new environment. Many traditional approaches of domain adaptation in RL manage to learn a mapping function between the source and target domain in explicit or implicit ways. However, they typically require access to abundant data from the target domain. Besides, they often rely on visual clues to learn the mapping function and may fail when the source domain looks quite different from the target domain. To address these problems, we propose a novel framework Online Prototype Alignment (OPA) to learn the mapping function based on the functional similarity of elements and is able to achieve the few-shot policy transfer within only several episodes. The key insight of OPA is to introduce an exploration mechanism that can interact with the unseen elements of the target domain in an efficient and purposeful manner, and then connect them with the seen elements in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#35797;&#28857;&#39044;&#27979;&#26550;&#26500;&#20013;&#30340;&#31639;&#27861;&#24178;&#39044;&#26469;&#25552;&#39640;&#38750;AI&#27169;&#22411;&#19979;&#38024;&#32455;&#21697;&#31867;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#20915;&#31574;&#27169;&#22411;&#20013;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#21487;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;AI / ML&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#20808;&#36827;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.07305</link><description>&lt;p&gt;
&#35753;&#39044;&#27979;&#21464;&#24471;&#33258;&#23398;&#20064;&#21644;&#33258;&#36866;&#24212;--&#35797;&#28857;&#39044;&#27979;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making forecasting self-learning and adaptive -- Pilot forecasting rack. (arXiv:2306.07305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#35797;&#28857;&#39044;&#27979;&#26550;&#26500;&#20013;&#30340;&#31639;&#27861;&#24178;&#39044;&#26469;&#25552;&#39640;&#38750;AI&#27169;&#22411;&#19979;&#38024;&#32455;&#21697;&#31867;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#20915;&#31574;&#27169;&#22411;&#20013;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#21487;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;AI / ML&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#20808;&#36827;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#21806;&#38144;&#21806;&#21644;&#20215;&#26684;&#39044;&#27979;&#36890;&#24120;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23545;&#20110;&#26576;&#20123;&#20135;&#21697;&#31867;&#21035;&#65292;&#39044;&#27979;&#38656;&#27714;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20250;&#23545;&#24211;&#23384;&#12289;&#36816;&#36755;&#21644;&#34917;&#36135;&#35745;&#21010;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22522;&#20110;&#31215;&#26497;&#25506;&#32034;&#30340;&#35797;&#28857;&#28436;&#32451;&#30340;&#21457;&#29616;&#65292;&#20197;&#25506;&#32034;&#24110;&#21161;&#38646;&#21806;&#21830;&#25552;&#39640;&#27492;&#31867;&#20135;&#21697;&#31867;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36890;&#36807;&#19968;&#20010;&#26679;&#26412;&#20135;&#21697;&#31867;&#21035;&#8220;&#38024;&#32455;&#21697;&#8221;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#31639;&#27861;&#24178;&#39044;&#26426;&#20250;&#12290;&#30446;&#21069;&#65292;&#38024;&#32455;&#21697;&#20135;&#21697;&#31867;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#22312;&#38750;AI&#27169;&#22411;&#20013;&#30340;&#33539;&#22260;&#20026;60%&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26550;&#26500;&#26041;&#27861;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#29983;&#25104;&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#20915;&#31574;&#27169;&#22411;&#26681;&#25454;&#32473;&#23450;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#21160;&#24577;&#22320;&#20174;&#31639;&#27861;&#26550;&#20013;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#12290;&#20351;&#29992;&#20808;&#36827;&#30340;&#29305;&#24449;&#24037;&#31243;&#26500;&#24314;&#30340;AI / ML&#39044;&#27979;&#27169;&#22411;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#38656;&#27714;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retail sales and price projections are typically based on time series forecasting. For some product categories, the accuracy of demand forecasts achieved is low, negatively impacting inventory, transport, and replenishment planning. This paper presents our findings based on a proactive pilot exercise to explore ways to help retailers to improve forecast accuracy for such product categories.  We evaluated opportunities for algorithmic interventions to improve forecast accuracy based on a sample product category, Knitwear. The Knitwear product category has a current demand forecast accuracy from non-AI models in the range of 60%. We explored how to improve the forecast accuracy using a rack approach. To generate forecasts, our decision model dynamically selects the best algorithm from an algorithm rack based on performance for a given state and context. Outcomes from our AI/ML forecasting model built using advanced feature engineering show an increase in the accuracy of demand forecast f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26469;&#32479;&#19968;&#23450;&#20041;&#21644;&#28548;&#28165;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#65292;&#36827;&#32780;&#25552;&#20379;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#23454;&#29616;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27604;&#36739;&#20197;&#21450;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.07304</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#20840;&#38754;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation. (arXiv:2306.07304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26469;&#32479;&#19968;&#23450;&#20041;&#21644;&#28548;&#28165;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#65292;&#36827;&#32780;&#25552;&#20379;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#23454;&#29616;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27604;&#36739;&#20197;&#21450;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#25104;&#20026;&#20102;&#19968;&#20123;&#26368;&#26377;&#21069;&#36884;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#24110;&#21161;&#25105;&#20204;&#35299;&#37322;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#35797;&#22270;&#22312;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#20013;&#21457;&#29616;&#34987;&#38544;&#34255;&#22312;ANN&#28608;&#27963;&#30340;&#22797;&#26434;&#27169;&#24335;&#20013;&#30340;&#21487;&#29702;&#35299;&#30340;&#35270;&#35273;&#8220;&#27010;&#24565;&#8221;&#65306;&#65288;1&#65289;&#27010;&#24565;&#25552;&#21462;&#65292;&#65288;2&#65289;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#27493;&#39588;&#26159;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#20849;&#21516;&#30340;&#65292;&#20294;&#23427;&#20204;&#30340;&#20855;&#20307;&#23454;&#29616;&#37117;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20840;&#38754;&#23450;&#20041;&#21644;&#28548;&#28165;&#20102;&#36825;&#20004;&#20010;&#27493;&#39588;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#65306;&#65288;i&#65289;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#29616;&#20195;&#24402;&#22240;&#26041;&#27861;&#21644;&#35780;&#20272;&#25351;&#26631;&#26469;&#25193;&#23637;&#21644;&#31995;&#32479;&#22320;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#21644;&#37325;&#35201;&#24615;&#35780;&#20272;&#25216;&#26415;&#65307;&#65288;iii&#65289;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, concept-based approaches have emerged as some of the most promising explainability methods to help us interpret the decisions of Artificial Neural Networks (ANNs). These methods seek to discover intelligible visual 'concepts' buried within the complex patterns of ANN activations in two key steps: (1) concept extraction followed by (2) importance estimation. While these two steps are shared across methods, they all differ in their specific implementations. Here, we introduce a unifying theoretical framework that comprehensively defines and clarifies these two steps. This framework offers several advantages as it allows us: (i) to propose new evaluation metrics for comparing different concept extraction approaches; (ii) to leverage modern attribution methods and evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches and importance estimation techniques; (iii) to derive theoretical guarantees regarding the optimality of such met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#24212;&#29992;&#65292;Transformer&#33021;&#22815;&#29702;&#35299;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#19988;&#23454;&#29616;&#24182;&#34892;&#22788;&#29702;&#65292;&#22312;NLP&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#22788;&#29702;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#29289;&#32852;&#32593;&#31561;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.07303</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks. (arXiv:2306.07303v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#24212;&#29992;&#65292;Transformer&#33021;&#22815;&#29702;&#35299;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#19988;&#23454;&#29616;&#24182;&#34892;&#22788;&#29702;&#65292;&#22312;NLP&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#22788;&#29702;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#29289;&#32852;&#32593;&#31561;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#29702;&#35299;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#25110;&#26356;&#26032;&#29256;&#26412;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65288;&#22914;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#65289;&#19981;&#21516;&#65292;Transformer&#27169;&#22411;&#22312;&#22788;&#29702;&#36755;&#20837;&#24207;&#21015;&#20803;&#32032;&#20043;&#38388;&#30340;&#38271;&#20381;&#36182;&#20851;&#31995;&#21644;&#23454;&#29616;&#24182;&#34892;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20852;&#36259;&#12290;&#36825;&#24471;&#30410;&#20110;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20197;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#35821;&#38899;&#22788;&#29702;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#21644;&#26174;&#33879;&#25104;&#23601;&#12290;&#34429;&#28982;&#24050;&#32463;&#20986;&#29256;&#20102;&#20960;&#31687;&#32508;&#36848;&#25991;&#31456;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;Transformer&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#36129;&#29486;&#12289;&#26550;&#26500;&#24046;&#24322;&#25110;&#24615;&#33021;&#35780;&#20272;&#65292;&#20294;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result, transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer's contributions in specific fields, architectural differences, or performance evaluations, there is still a significant abs
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;DR-LSSV&#65292;&#22522;&#20110;&#22238;&#24402;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#26102;&#38388;&#21644;&#20551;&#38451;&#24615;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07301</link><description>&lt;p&gt;
&#22522;&#20110;&#22238;&#24402;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#30340;&#26032;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Novel Regression and Least Square Support Vector Machine Learning Technique for Air Pollution Forecasting. (arXiv:2306.07301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07301
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;DR-LSSV&#65292;&#22522;&#20110;&#22238;&#24402;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#26102;&#38388;&#21644;&#20551;&#38451;&#24615;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#27745;&#26579;&#26159;&#26469;&#28304;&#20110;&#24494;&#31890;&#12289;&#21270;&#23398;&#29289;&#36136;&#25110;&#29983;&#29289;&#29289;&#36136;&#30340;&#38382;&#39064;&#65292;&#23427;&#20250;&#23545;&#20154;&#31867;&#25110;&#20854;&#20182;&#29983;&#29289;&#24102;&#26469;&#30171;&#33510;&#65292;&#23545;&#33258;&#28982;&#26646;&#24687;&#22320;&#21644;&#31354;&#27668;&#31354;&#38388;&#20063;&#20250;&#36896;&#25104;&#19981;&#36866;&#12290;&#22240;&#27492;&#65292;&#31354;&#27668;&#27745;&#26579;&#22312;&#22823;&#37117;&#24066;&#20013;&#20173;&#28982;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#29615;&#22659;&#38382;&#39064;&#12290;&#33509;&#19981;&#27491;&#30830;&#22320;&#26816;&#27979;&#31354;&#27668;&#27745;&#26579;&#26631;&#20934;&#65292;&#21017;&#20250;&#23545;&#20154;&#31867;&#21644;&#29983;&#29289;&#36896;&#25104;&#20005;&#37325;&#30340;&#21518;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#8212;&#8212;&#31163;&#25955;&#21270;&#22238;&#24402;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#65288;DR-LSSV&#65289;&#26469;&#39044;&#27979;&#31354;&#27668;&#27745;&#26579;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;DR-LSSV&#25216;&#26415;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#26102;&#38388;&#21644;&#20551;&#38451;&#24615;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air pollution is the origination of particulate matter, chemicals, or biological substances that brings pain to either humans or other living creatures or instigates discomfort to the natural habitat and the airspace. Hence, air pollution remains one of the paramount environmental issues as far as metropolitan cities are concerned. Several air pollution benchmarks are even said to have a negative influence on human health. Also, improper detection of air pollution benchmarks results in severe complications for humans and living creatures. To address this aspect, a novel technique called, Discretized Regression and Least Square Support Vector (DR-LSSV) based air pollution forecasting is proposed. The results indicate that the proposed DR-LSSV Technique can efficiently enhance air pollution forecasting performance and outperforms the conventional machine learning methods in terms of air pollution forecasting accuracy, air pollution forecasting time, and false positive rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20851;&#27880;&#25216;&#26415;&#65292;&#33021;&#22815;&#24179;&#31561;&#20851;&#27880;&#27599;&#20010;&#30382;&#32932;&#30149;&#21464;&#31867;&#21035;&#65292;&#24182;&#36880;&#27493;&#32467;&#21512;&#22810;&#20010;&#23610;&#24230;&#30340;&#21028;&#21035;&#29305;&#24449;&#32454;&#33410;&#65292;&#20174;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#35786;&#26029;&#34920;&#29616;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;97.40&#65285;&#21644;94.9&#65285;&#65292;&#36229;&#36807;&#20102;15&#31181;&#23574;&#31471;&#26041;&#27861;&#65292;&#21253;&#25324;HAM1000&#21644;ISIC2019&#25490;&#34892;&#27036;&#30340;&#20248;&#32988;&#32773;&#12290;</title><link>http://arxiv.org/abs/2306.07300</link><description>&lt;p&gt;
&#35786;&#26029;&#30382;&#32932;&#30149;&#21464;&#30340;&#28176;&#36827;&#24335;&#20998;&#31867;&#20851;&#27880;&#65288;PCA&#65289;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Progressive Class-Wise Attention (PCA) Approach for Diagnosing Skin Lesions. (arXiv:2306.07300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20851;&#27880;&#25216;&#26415;&#65292;&#33021;&#22815;&#24179;&#31561;&#20851;&#27880;&#27599;&#20010;&#30382;&#32932;&#30149;&#21464;&#31867;&#21035;&#65292;&#24182;&#36880;&#27493;&#32467;&#21512;&#22810;&#20010;&#23610;&#24230;&#30340;&#21028;&#21035;&#29305;&#24449;&#32454;&#33410;&#65292;&#20174;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#35786;&#26029;&#34920;&#29616;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;97.40&#65285;&#21644;94.9&#65285;&#65292;&#36229;&#36807;&#20102;15&#31181;&#23574;&#31471;&#26041;&#27861;&#65292;&#21253;&#25324;HAM1000&#21644;ISIC2019&#25490;&#34892;&#27036;&#30340;&#20248;&#32988;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#30284;&#30151;&#21457;&#30149;&#29575;&#20013;&#65292;&#30382;&#32932;&#30284;&#30340;&#21457;&#29983;&#29575;&#26368;&#39640;&#12290;&#26089;&#26399;&#21457;&#29616;&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#65292;&#26202;&#26399;&#30149;&#20363;&#21487;&#33021;&#20250;&#21361;&#21450;&#29983;&#21629;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33394;&#24425;&#12289;&#24418;&#29366;&#21644;&#22823;&#23567;&#31561;&#22810;&#31181;&#21464;&#21270;&#65292;&#30382;&#32932;&#30149;&#21464;&#30340;&#20998;&#31867;&#23384;&#22312;&#22810;&#20010;&#25361;&#25112;&#65292;&#21516;&#19968;&#31867;&#21035;&#20869;&#23384;&#22312;&#26174;&#33879;&#21464;&#24322;&#65292;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#20063;&#23384;&#22312;&#26174;&#33879;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20998;&#31867;&#20851;&#27880;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22312;&#25366;&#25496;&#30382;&#32932;&#30149;&#21464;&#30340;&#26356;&#20855;&#20307;&#32454;&#33410;&#30340;&#21516;&#26102;&#65292;&#24179;&#31561;&#22320;&#32771;&#34385;&#27599;&#20010;&#31867;&#21035;&#12290;&#36825;&#31181;&#20851;&#27880;&#26426;&#21046;&#36880;&#27493;&#29992;&#20110;&#32467;&#21512;&#22810;&#20010;&#23610;&#24230;&#30340;&#21028;&#21035;&#29305;&#24449;&#32454;&#33410;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#25216;&#26415;&#34920;&#29616;&#20986;&#33394;&#65292;&#19981;&#20165;&#36229;&#36807;&#20102;15&#31181;&#23574;&#31471;&#26041;&#27861;&#65292;&#36824;&#21253;&#25324;HAM1000&#21644;ISIC 2019&#25490;&#34892;&#27036;&#30340;&#20248;&#32988;&#32773;&#12290;&#22312;HAM10000&#25968;&#25454;&#38598;&#19978;&#65292;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;97.40&#65285;&#65292;&#32780;&#22312;ISIC 2019&#25968;&#25454;&#38598;&#19978;&#21017;&#20026;94.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin cancer holds the highest incidence rate among all cancers globally. The importance of early detection cannot be overstated, as late-stage cases can be lethal. Classifying skin lesions, however, presents several challenges due to the many variations they can exhibit, such as differences in colour, shape, and size, significant variation within the same class, and notable similarities between different classes. This paper introduces a novel class-wise attention technique that equally regards each class while unearthing more specific details about skin lesions. This attention mechanism is progressively used to amalgamate discriminative feature details from multiple scales. The introduced technique demonstrated impressive performance, surpassing more than 15 cutting-edge methods including the winners of HAM1000 and ISIC 2019 leaderboards. It achieved an impressive accuracy rate of 97.40% on the HAM10000 dataset and 94.9% on the ISIC 2019 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22810;&#29289;&#29702;&#37327;&#20195;&#29702;&#24314;&#27169;&#20013;&#39640;&#32500;&#39044;&#27979;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#12289;&#29616;&#26377;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#24615;&#22810;&#25351;&#26631;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;(AdMIn-GP)&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21442;&#25968;&#31354;&#38388;&#20869;&#20302;&#32500;&#23884;&#20837;&#30340;&#28789;&#27963;&#21152;&#24615;&#32467;&#26500;&#65292;&#20805;&#20998;&#21033;&#29992;&#31185;&#23398;&#20808;&#21069;&#30693;&#35782;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.07299</link><description>&lt;p&gt;
&#21152;&#24615;&#22810;&#25351;&#26631;&#39640;&#26031;&#36807;&#31243;&#24314;&#27169;&#21450;&#20854;&#22312;&#22840;&#20811;&#33014;&#23376;&#31561;&#31163;&#23376;&#20307;&#22810;&#29289;&#29702;&#37327;&#20195;&#29702;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Additive Multi-Index Gaussian process modeling, with application to multi-physics surrogate modeling of the quark-gluon plasma. (arXiv:2306.07299v1 [nucl-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22810;&#29289;&#29702;&#37327;&#20195;&#29702;&#24314;&#27169;&#20013;&#39640;&#32500;&#39044;&#27979;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#12289;&#29616;&#26377;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#24615;&#22810;&#25351;&#26631;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;(AdMIn-GP)&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21442;&#25968;&#31354;&#38388;&#20869;&#20302;&#32500;&#23884;&#20837;&#30340;&#28789;&#27963;&#21152;&#24615;&#32467;&#26500;&#65292;&#20805;&#20998;&#21033;&#29992;&#31185;&#23398;&#20808;&#21069;&#30693;&#35782;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22840;&#20811;&#33014;&#23376;&#31561;&#31163;&#23376;&#20307;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#26680;&#29289;&#36136;&#30456;&#65292;&#29702;&#35770;&#19978;&#35748;&#20026;&#22312;&#23431;&#23449;&#22823;&#29190;&#28856;&#21518;&#19981;&#20037;&#23601;&#22635;&#28385;&#20102;&#23431;&#23449;&#12290;&#30740;&#31350;&#22840;&#20811;&#33014;&#23376;&#31561;&#31163;&#23376;&#20307;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#38656;&#35201;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20869;&#36816;&#34892;&#22797;&#26434;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#20197;&#23558;&#23454;&#39564;&#21487;&#35266;&#27979;&#37327;&#19982;&#29702;&#35770;&#21442;&#25968;&#36827;&#34892;&#21327;&#35843;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#27425;&#36816;&#34892;&#37117;&#38656;&#35201;&#25968;&#21315;&#20010;CPU&#23567;&#26102;&#65292;&#22240;&#27492;&#29289;&#29702;&#23398;&#23478;&#21482;&#33021;&#36827;&#34892;&#20960;&#30334;&#27425;&#36816;&#34892;&#65292;&#20174;&#32780;&#23548;&#33268;&#39640;&#32500;&#39044;&#27979;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#29616;&#26377;&#30340;&#20195;&#29702;&#27169;&#22411;&#36890;&#24120;&#20250;&#20135;&#29983;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#24615;&#22810;&#25351;&#26631;&#39640;&#26031;&#36807;&#31243;(AdMIn-GP)&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#21442;&#25968;&#31354;&#38388;&#20869;&#20302;&#32500;&#23884;&#20837;&#30340;&#28789;&#27963;&#21152;&#24615;&#32467;&#26500;&#12290;&#36825;&#26159;&#30001;&#31185;&#23398;&#20808;&#21069;&#30693;&#35782;&#25351;&#23548;&#30340;&#65292;&#21363;&#22840;&#20811;&#33014;&#23376;&#31561;&#31163;&#23376;&#20307;&#21463;&#22810;&#20010;&#19981;&#21516;&#29289;&#29702;&#29616;&#35937;(&#21363;&#22810;&#29289;&#29702;&#37327;)&#30340;&#25903;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Quark-Gluon Plasma (QGP) is a unique phase of nuclear matter, theorized to have filled the Universe shortly after the Big Bang. A critical challenge in studying the QGP is that, to reconcile experimental observables with theoretical parameters, one requires many simulation runs of a complex physics model over a high-dimensional parameter space. Each run is computationally very expensive, requiring thousands of CPU hours, thus limiting physicists to only several hundred runs. Given limited training data for high-dimensional prediction, existing surrogate models often yield poor predictions with high predictive uncertainties, leading to imprecise scientific findings. To address this, we propose a new Additive Multi-Index Gaussian process (AdMIn-GP) model, which leverages a flexible additive structure on low-dimensional embeddings of the parameter space. This is guided by prior scientific knowledge that the QGP is dominated by multiple distinct physical phenomena (i.e., multiphysics),
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25968;&#25454;&#22686;&#24191;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;&#30005;&#23376;&#30149;&#21382;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.07297</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#21307;&#30103;&#25968;&#25454;&#22686;&#24191;&#65306;&#22522;&#20110;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification. (arXiv:2306.07297v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25968;&#25454;&#22686;&#24191;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;&#30005;&#23376;&#30149;&#21382;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#30149;&#21382;&#21644;&#20020;&#24202;&#35760;&#24405;&#20013;&#35782;&#21035;&#33647;&#29289;&#12289;&#30142;&#30149;&#21644;&#20851;&#32852;&#24615;&#31561;&#20851;&#38190;&#22240;&#32032;&#20855;&#26377;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#36827;&#34892;&#25968;&#25454;&#22686;&#24191;&#65292;&#20197;&#20811;&#26381;&#30005;&#23376;&#30149;&#21382;&#20013;&#20851;&#38190;&#22240;&#32032;&#26631;&#27880;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26174;&#33879;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#31561;&#20004;&#20010;&#30005;&#23376;&#30149;&#21382;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The identification of key factors such as medications, diseases, and relationships within electronic health records and clinical notes has a wide range of applications in the clinical field. In the N2C2 2022 competitions, various tasks were presented to promote the identification of key factors in electronic health records (EHRs) using the Contextualized Medication Event Dataset (CMED). Pretrained large language models (LLMs) demonstrated exceptional performance in these tasks. This study aims to explore the utilization of LLMs, specifically ChatGPT, for data augmentation to overcome the limited availability of annotated data for identifying the key factors in EHRs. Additionally, different pre-trained BERT models, initially trained on extensive datasets like Wikipedia and MIMIC, were employed to develop models for identifying these key variables in EHRs through fine-tuning on augmented datasets. The experimental results of two EHR analysis tasks, namely medication identification and me
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;PSO&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#38024;&#23545;&#21271;&#20140;PM2.5&#30340;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;M-1&#27169;&#22411;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#19988;&#32463;&#36807;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#21487;&#20197;&#25552;&#39640;5.6%&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07296</link><description>&lt;p&gt;
&#22522;&#20110;PSO&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21271;&#20140;PM2.5&#39044;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimized Three Deep Learning Models Based-PSO Hyperparameters for Beijing PM2.5 Prediction. (arXiv:2306.07296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;PSO&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#38024;&#23545;&#21271;&#20140;PM2.5&#30340;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;M-1&#27169;&#22411;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#19988;&#32463;&#36807;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#21487;&#20197;&#25552;&#39640;5.6%&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#20687;&#35782;&#21035;&#21644;&#39044;&#27979;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#35774;&#32622;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#20248;&#21270;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20351;&#29992;&#22522;&#20110;&#32676;&#26234;&#33021;&#30340;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#26041;&#27861;&#31890;&#23376;&#32676;&#31639;&#27861;&#65288;PSO&#65289;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#30340;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;M-1&#65288;PSO-LSTM&#65289;&#12289;M-2&#65288;PSO-CNN&#65289;&#21644;M-3&#65288;PSO-MLP&#65289;&#12290;&#23545;&#21271;&#20140;PM2.5&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#27979;&#37327;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;PM2.5&#20316;&#20026;&#30446;&#26631;&#21464;&#37327;&#65292;&#21463;&#21040;&#38706;&#28857;&#12289;&#27668;&#21387;&#12289;&#28201;&#24230;&#12289;&#32047;&#35745;&#39118;&#36895;&#12289;&#38477;&#38634;&#23567;&#26102;&#25968;&#21644;&#38477;&#38632;&#23567;&#26102;&#25968;&#30340;&#24433;&#21709;&#12290;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#36755;&#20837;&#20998;&#20026;&#19977;&#31181;&#19981;&#21516;&#30340;&#24773;&#26223;&#65306;&#26085;&#24120;&#12289;&#27599;&#21608;&#21644;&#27599;&#26376;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;M-1&#20855;&#26377;&#19977;&#20010;&#38544;&#34255;&#23618;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;5.6%&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is a machine learning approach that produces excellent performance in various applications, including natural language processing, image identification, and forecasting. Deep learning network performance depends on the hyperparameter settings. This research attempts to optimize the deep learning architecture of Long short term memory (LSTM), Convolutional neural network (CNN), and Multilayer perceptron (MLP) for forecasting tasks using Particle swarm optimization (PSO), a swarm intelligence-based metaheuristic optimization methodology: Proposed M-1 (PSO-LSTM), M-2 (PSO-CNN), and M-3 (PSO-MLP). Beijing PM2.5 datasets was analyzed to measure the performance of the proposed models. PM2.5 as a target variable was affected by dew point, pressure, temperature, cumulated wind speed, hours of snow, and hours of rain. The deep learning network inputs consist of three different scenarios: daily, weekly, and monthly. The results show that the proposed M-1 with three hidden layers pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#39640;&#25928;CNN&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#32780;&#19988;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#37117;&#26377;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2306.07294</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Expressivity Enhancement with Efficient Quadratic Neurons for Convolutional Neural Networks. (arXiv:2306.07294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#39640;&#25928;CNN&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#32780;&#19988;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#37117;&#26377;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#20998;&#21106;&#31561;&#39046;&#22495;&#12290;&#20026;&#20102;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#65292;&#20154;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#22914;&#26032;&#30340;CNN&#26550;&#26500;&#12290;&#20294;&#26159;&#65292;&#26469;&#33258;&#36825;&#20123;&#25216;&#26415;&#30340;&#24615;&#33021;&#25552;&#21319;&#24448;&#24448;&#20250;&#20943;&#24369;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25226;&#37325;&#28857;&#36716;&#21521;&#22686;&#21152;&#31070;&#32463;&#20803;&#30340;&#38750;&#32447;&#24615;&#65292;&#20197;&#22686;&#24378;&#32593;&#32476;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20250;&#24102;&#26469;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#22240;&#27492;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#23548;&#33268;&#21487;&#37096;&#32626;&#24615;&#26041;&#38754;&#30340;&#20302;&#25928;&#29575;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#32467;&#26500;&#65292;&#20197;&#20165;&#26377;&#24494;&#23567;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#24320;&#38144;&#26469;&#20445;&#30041;&#38750;&#32447;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#21487;&#20197;&#26368;&#22823;&#21270;&#21033;&#29992;&#20108;&#38454;&#35745;&#31639;&#20449;&#24687;&#26469;&#25913;&#21892;&#32593;&#32476;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#19982;&#29616;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30456;&#27604;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have been successfully applied in a range of fields such as image classification and object segmentation. To improve their expressivity, various techniques, such as novel CNN architectures, have been explored. However, the performance gain from such techniques tends to diminish. To address this challenge, many researchers have shifted their focus to increasing the non-linearity of neurons, the fundamental building blocks of neural networks, to enhance the network expressivity. Nevertheless, most of these approaches incur a large number of parameters and thus formidable computation cost inevitably, impairing their efficiency to be deployed in practice. In this work, an efficient quadratic neuron structure is proposed to preserve the non-linearity with only negligible parameter and computation cost overhead. The proposed quadratic neuron can maximize the utilization of second-order computation information to improve the network performance. The experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.07292</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Urban Spatiotemporal Data Synthesis via Neural Disaggregation. (arXiv:2306.07292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#25968;&#25454;&#30340;&#32454;&#33410;&#32423;&#21035;&#24120;&#24120;&#19982;&#20854;&#25152;&#33021;&#25552;&#20379;&#30340;&#23454;&#38469;&#25928;&#30410;&#21457;&#29983;&#20914;&#31361;&#12290;&#36739;&#19981;&#32454;&#21270;&#30340;&#25968;&#25454;&#21487;&#20197;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#20294;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#29306;&#29298;&#20102;&#24320;&#25918;&#25968;&#25454;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#21327;&#21161;&#30740;&#31350;&#30340;&#25215;&#35834;&#12290;&#31867;&#20284;&#20110;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;&#39640;&#23618;&#27425;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#21487;&#33021;&#20250;&#25513;&#30422;&#22478;&#24066;&#21160;&#24577;&#30340;&#24213;&#23618;&#29305;&#24449;&#65292;&#20302;&#32423;&#21035;&#22320;&#29702;&#21333;&#20803;&#30340;&#21464;&#21270;&#21487;&#33021;&#26356;&#20026;&#26126;&#26174;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#65292;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#19968;&#20123;&#20256;&#32479;&#20998;&#35299;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#38382;&#39064;-1) &#25105;&#20204;&#23581;&#35797;&#20102;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#29305;&#24449;&#20043;&#38388;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#31070;&#32463;&#26041;&#27861;&#20063;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The level of granularity of open data often conflicts the benefits it can provide. Less granular data can protect individual privacy, but to certain degrees, sabotage the promise of open data to promote transparency and assist research. Similar in the urban setting, aggregated urban data at high-level geographic units can mask out the underline particularities of city dynamics that may vary at lower areal levels. In this work, we aim to synthesize fine-grained, high resolution urban data, by breaking down aggregated urban data at coarse, low resolution geographic units. The goal is to increase the usability and realize the values as much as possible of highly aggregated urban data. To address the issue of simplicity of some traditional disaggregation methods -- 1) we experimented with numerous neural-based models that are capable of modeling intricate non-linear relationships among features. Neural methods can also leverage both spatial and temporal information concurrently. We showed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#28909;&#24102;&#27668;&#26059;&#20013;&#24515;&#22352;&#26631;&#26816;&#27979;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07291</link><description>&lt;p&gt;
&#21033;&#29992;ERA5&#20877;&#20998;&#26512;&#25968;&#25454;&#30340;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#28909;&#24102;&#27668;&#26059;
&lt;/p&gt;
&lt;p&gt;
An Ensemble Machine Learning Approach for Tropical Cyclone Detection Using ERA5 Reanalysis Data. (arXiv:2306.07291v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#28909;&#24102;&#27668;&#26059;&#20013;&#24515;&#22352;&#26631;&#26816;&#27979;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#24102;&#27668;&#26059;&#26159;&#33258;&#28982;&#30028;&#20013;&#26368;&#20855;&#30772;&#22351;&#24615;&#30340;&#29616;&#35937;&#20043;&#19968;&#12290;&#27599;&#24180;&#20840;&#29699;&#24179;&#22343;&#26377;90&#20010;&#28909;&#24102;&#27668;&#26059;&#22312;&#28909;&#24102;&#28023;&#27915;&#19978;&#24418;&#25104;&#65292;&#20840;&#29699;&#21464;&#26262;&#20351;&#23427;&#20204;&#21464;&#24471;&#26356;&#24378;&#12289;&#26356;&#22823;&#12289;&#26356;&#20855;&#30772;&#22351;&#24615;&#12290;&#20934;&#30830;&#26816;&#27979;&#21644;&#36319;&#36394;&#36825;&#31181;&#29616;&#35937;&#24050;&#25104;&#20026;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#30740;&#31350;&#20013;&#19968;&#20010;&#30456;&#20851;&#21644;&#26377;&#36259;&#30340;&#39046;&#22495;&#12290;&#20256;&#32479;&#19978;&#65292;&#28909;&#24102;&#27668;&#26059;&#22312;&#22823;&#22411;&#27668;&#20505;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#20351;&#29992;&#20381;&#36182;&#20110;&#20027;&#35266;&#38408;&#20540;&#30340;&#30830;&#23450;&#24615;&#36319;&#36394;&#26041;&#26696;&#36827;&#34892;&#35782;&#21035;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21487;&#20197;&#34917;&#20805;&#30830;&#23450;&#24615;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#20174;&#21487;&#29992;&#25968;&#25454;&#20013;&#25429;&#33719;&#36755;&#20837;&#27668;&#20505;&#22240;&#32032;&#19982;&#28909;&#24102;&#27668;&#26059;&#20013;&#24515;&#22320;&#29702;&#20301;&#32622;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;ML&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#20301;&#28909;&#24102;&#27668;&#26059;&#20013;&#24515;&#22352;&#26631;&#65292;&#23558;TC&#20998;&#31867;&#21644;&#23450;&#20301;&#23884;&#20837;&#21333;&#20010;&#31471;&#21040;&#31471;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;&#35813;&#38598;&#25104;&#27169;&#22411;&#32467;&#21512;&#20102;&#19981;&#21516;ML&#27169;&#22411;&#30340;TC&#20013;&#24515;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tropical Cyclones (TCs) are counted among the most destructive phenomena that can be found in nature. Every year, globally an average of 90 TCs occur over tropical waters, and global warming is making them stronger, larger and more destructive. The accurate detection and tracking of such phenomena have become a relevant and interesting area of research in weather and climate science. Traditionally, TCs have been identified in large climate datasets through the use of deterministic tracking schemes that rely on subjective thresholds. Machine Learning (ML) models can complement deterministic approaches due to their ability to capture the mapping between the input climatic drivers and the geographical position of the TC center from the available data. This study presents a ML ensemble approach for locating TC center coordinates, embedding both TC classification and localization in a single end-to-end learning task. The ensemble combines TC center estimates of different ML models that agre
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#20540;&#20989;&#25968;&#20272;&#35745;&#25511;&#21046;&#26041;&#27861;&#65288;DVF&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22823;&#37327;&#26465;&#20214;&#21270;&#25968;&#25454;&#19978;&#26377;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#31034;&#33539;&#25968;&#25454;&#23601;&#33021;&#36229;&#36807;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07290</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#20540;&#20989;&#25968;&#20272;&#35745;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Value function estimation using conditional diffusion models for control. (arXiv:2306.07290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07290
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#20540;&#20989;&#25968;&#20272;&#35745;&#25511;&#21046;&#26041;&#27861;&#65288;DVF&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22823;&#37327;&#26465;&#20214;&#21270;&#25968;&#25454;&#19978;&#26377;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#31034;&#33539;&#25968;&#25454;&#23601;&#33021;&#36229;&#36807;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#21487;&#38752;&#36235;&#21183;&#26159;&#24615;&#33021;&#38543;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#21069;&#25552;&#26159;&#26377;&#20805;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#27169;&#22411;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#24456;&#21487;&#33021;&#20250;&#20986;&#29616;&#39640;&#36136;&#37327;&#31034;&#33539;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#20540;&#20989;&#25968;&#30340;&#31616;&#21333;&#31639;&#27861;(DVF)&#65292;&#23427;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#29615;&#22659;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#21160;&#24577;&#30340;&#32852;&#21512;&#22810;&#27493;&#27169;&#22411;&#65292;&#24182;&#20272;&#35745;&#25152;&#38656;&#20219;&#21153;&#30340;&#20540;&#20989;&#25968;&#12290;&#22312;&#27169;&#25311;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#23427;&#21482;&#38656;&#35201;&#20351;&#29992;&#19968;&#23567;&#37096;&#20998;&#31034;&#33539;&#25968;&#25454;&#23601;&#21487;&#20197;&#32988;&#36807;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fairly reliable trend in deep reinforcement learning is that the performance scales with the number of parameters, provided a complimentary scaling in amount of training data. As the appetite for large models increases, it is imperative to address, sooner than later, the potential problem of running out of high-quality demonstrations. In this case, instead of collecting only new data via costly human demonstrations or risking a simulation-to-real transfer with uncertain effects, it would be beneficial to leverage vast amounts of readily-available low-quality data. Since classical control algorithms such as behavior cloning or temporal difference learning cannot be used on reward-free or action-free data out-of-the-box, this solution warrants novel training paradigms for continuous control. We propose a simple algorithm called Diffused Value Function (DVF), which learns a joint multi-step model of the environment-robot interaction dynamics using a diffusion model. This model can be ef
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#32622;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#20844;&#24179;LTR-RC&#65292;&#23427;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#65292;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#20043;&#38388;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.07188</link><description>&lt;p&gt;
&#26080;&#20998;&#24067;&#39118;&#38505;&#25511;&#21046;&#30340;&#20844;&#24179;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Fair Learning to Rank with Distribution-free Risk Control. (arXiv:2306.07188v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#32622;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#20844;&#24179;LTR-RC&#65292;&#23427;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#65292;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#20043;&#38388;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#32463;&#27982;&#20013;&#65292;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#23545;&#29992;&#25143;&#21644;&#29289;&#21697;&#25552;&#20379;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;LTR&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#23545;&#20110;&#25353;&#27604;&#20363;&#20998;&#37197;&#26333;&#20809;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#20855;&#26377;&#30456;&#21516;&#30456;&#20851;&#24615;&#30340;&#39033;&#25509;&#25910;&#30053;&#26377;&#19981;&#21516;&#30340;&#20998;&#25968;&#26102;&#65292;&#30830;&#23450;&#24615;&#25490;&#21517;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#26333;&#20809;&#20998;&#37197;&#12290;&#38543;&#26426;LTR&#27169;&#22411;&#65292;&#21253;&#25324;Plackett-Luce&#65288;PL&#65289;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20294;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#20445;&#35777;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#24179;LTR-RC&#65292;&#19968;&#31181;&#26032;&#30340;&#21518;&#32622;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#20844;&#24179;LTR-RC&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35780;&#20998;&#20989;&#25968;&#21019;&#24314;&#38543;&#26426;LTR&#27169;&#22411;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#35757;&#32451;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#20844;&#24179;LTR-RC&#20351;&#29992;&#26080;&#20998;&#24067;&#24335;&#39118;&#38505;&#25511;&#21046;&#26694;&#26550;&#23545;&#29992;&#25143;&#25351;&#23450;&#30340;&#25928;&#29992;&#25552;&#20379;&#26377;&#38480;&#30340;&#26679;&#26412;&#20445;&#35777;&#12290;&#36890;&#36807;&#21478;&#22806;&#32467;&#21512;Thresholded PL&#65288;TPL&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#20043;&#38388;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;FairLTR-RC&#22312;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#24615;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank (LTR) methods are vital in online economies, affecting users and item providers. Fairness in LTR models is crucial to allocate exposure proportionally to item relevance. The deterministic ranking model can lead to unfair exposure distribution when items with the same relevance receive slightly different scores. Stochastic LTR models, incorporating the Plackett-Luce (PL) model, address fairness issues but have limitations in computational cost and performance guarantees. To overcome these limitations, we propose FairLTR-RC, a novel post-hoc model-agnostic method. FairLTR-RC leverages a pretrained scoring function to create a stochastic LTR model, eliminating the need for expensive training. Furthermore, FairLTR-RC provides finite-sample guarantees on a user-specified utility using distribution-free risk control framework. By additionally incorporating the Thresholded PL (TPL) model, we are able to achieve an effective trade-off between utility and fairness. Experimental
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#26680;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#20113;&#20013;&#30340;&#22810;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.07056</link><description>&lt;p&gt;
&#20869;&#26680;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#29992;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Kernel Random Projection Depth for Outlier Detection. (arXiv:2306.07056v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#26680;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#20113;&#20013;&#30340;&#22810;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#38543;&#26426;&#25237;&#24433;&#28145;&#24230;&#65288;RPD&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#20113;&#20013;&#30340;&#22810;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26694;&#26550;&#20013;&#65292;RPD&#22312;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#35745;&#31639;&#12290;&#20511;&#21161;&#20869;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#25105;&#20204;&#26399;&#26395;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#19978;&#36848;&#22810;&#31181;&#27169;&#24335;&#21644;&#38750;&#20984;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;RPD&#65292;&#24182;&#21487;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#29616;&#26377;&#30340;&#26816;&#27979;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#20851;&#20110;&#25509;&#25910;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#65288;ROC&#65289;&#19979;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an extension of Random Projection Depth (RPD) to cope with multiple modalities and non-convexity on data clouds. In the framework of the proposed method, the RPD is computed in a reproducing kernel Hilbert space. With the help of kernel principal component analysis, we expect that the proposed method can cope with the above multiple modalities and non-convexity. The experimental results demonstrate that the proposed method outperforms RPD and is comparable to other existing detection models on benchmark datasets regarding Area Under the Curves (AUCs) of Receiver Operating Characteristic (ROC).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;VillanDiffusion&#65292;&#19968;&#20010;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#28085;&#30422;&#20027;&#27969;&#30340;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;DM&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;DM&#37197;&#32622;&#36827;&#34892;&#21518;&#38376;&#20998;&#26512;&#65292;&#24182;&#20026;&#22522;&#20110;&#23383;&#24149;&#30340;DM&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.06874</link><description>&lt;p&gt;
VillanDiffusion: &#19968;&#31181;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models. (arXiv:2306.06874v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;VillanDiffusion&#65292;&#19968;&#20010;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#28085;&#30422;&#20027;&#27969;&#30340;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;DM&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;DM&#37197;&#32622;&#36827;&#34892;&#21518;&#38376;&#20998;&#26512;&#65292;&#24182;&#20026;&#22522;&#20110;&#23383;&#24149;&#30340;DM&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#28155;&#21152;&#22122;&#22768;&#21644;&#21435;&#22122;&#23398;&#20064;&#21487;&#36870;&#30340;&#25439;&#22351;&#36807;&#31243;&#12290;&#23427;&#20204;&#26159;&#35768;&#22810;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#20027;&#24178;&#65292;&#20363;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#26377;&#26465;&#20214;&#29983;&#25104;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#22522;&#26412;&#26080;&#26465;&#20214;DM&#65288;&#20363;&#22914;DDPM&#21644;DDIM&#65289;&#26131;&#21463;&#21518;&#38376;&#27880;&#20837;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#30001;&#20110;&#24694;&#24847;&#23884;&#20837;&#27169;&#22411;&#36755;&#20837;&#30340;&#27169;&#24335;&#32780;&#35302;&#21457;&#30340;&#36755;&#20986;&#25805;&#32437;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65288;VillanDiffusion&#65289;&#65292;&#20197;&#25193;&#23637;&#24403;&#21069;&#30340;DM&#21518;&#38376;&#20998;&#26512;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#28085;&#30422;&#20102;&#20027;&#27969;&#30340;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;DM&#65288;&#22522;&#20110;&#21435;&#22122;&#21644;&#22522;&#20110;&#35780;&#20998;&#65289;&#65292;&#20197;&#21450;&#21508;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#37319;&#26679;&#22120;&#36827;&#34892;&#25972;&#20307;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32479;&#19968;&#26694;&#26550;&#20415;&#20110;&#23545;&#19981;&#21516;DM&#37197;&#32622;&#36827;&#34892;&#21518;&#38376;&#20998;&#26512;&#65292;&#24182;&#20026;&#22522;&#20110;&#23383;&#24149;&#30340;DM&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#65288;MLU&#65289;&#27169;&#22359;&#30340;&#21475;&#35821;&#29702;&#35299;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#33258;&#25105;&#30417;&#30563;&#29305;&#24449;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#21644;RoBERTa&#65289;&#65292;&#20197;&#20943;&#36731;&#30001;ASR&#38169;&#35823;&#20256;&#25773;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2306.06819</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#38899;&#25991;&#26723;&#26550;&#26500;&#30340;&#40065;&#26834;&#24615;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Multimodal Audio-textual Architecture for Robust Spoken Language Understanding. (arXiv:2306.06819v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#65288;MLU&#65289;&#27169;&#22359;&#30340;&#21475;&#35821;&#29702;&#35299;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#33258;&#25105;&#30417;&#30563;&#29305;&#24449;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;BERT&#21644;RoBERTa&#65289;&#65292;&#20197;&#20943;&#36731;&#30001;ASR&#38169;&#35823;&#20256;&#25773;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#38899;&#21161;&#25163;&#36890;&#24120;&#22522;&#20110;&#32423;&#32852;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24341;&#25806;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#31995;&#32479;&#12290;&#30001;&#20110;&#36825;&#31181;&#26041;&#27861;&#20381;&#38752;ASR&#36755;&#20986;&#65292;&#22240;&#27492;&#32463;&#24120;&#36973;&#21463;&#25152;&#35859;&#30340;ASR&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27492;&#31867;ASR&#38169;&#35823;&#20256;&#25773;&#23545;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65288;&#22914;BERT&#21644;RoBERTa&#65289;&#30340;&#26368;&#20808;&#36827;NLU&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#65288;MLU&#65289;&#27169;&#22359;&#65292;&#20197;&#20943;&#36731;&#30001;ASR&#36716;&#24405;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#24341;&#36215;&#30340;SLU&#24615;&#33021;&#19979;&#38477;&#12290;MLU&#21463;&#30410;&#20110;&#20174;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#23398;&#20064;&#30340;&#33258;&#25105;&#30417;&#30563;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;Wav2Vec&#29992;&#20110;&#35821;&#38899;&#21644;Bert / RoBERTa&#29992;&#20110;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;MLU&#32467;&#21512;&#19968;&#20010;&#32534;&#30721;&#22120;&#32593;&#32476;&#26469;&#23884;&#20837;&#38899;&#39057;&#20449;&#21495;&#21644;&#19968;&#20010;&#25991;&#26412;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#25991;&#26412;&#36716;&#24405;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#21518;&#26399;&#34701;&#21512;&#23618;&#26469;&#34701;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#36923;&#36753;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36825;&#31181;&#22810;&#27169;&#24335;&#38899;&#25991;&#26723;&#26550;&#26500;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#21475;&#35821;&#29702;&#35299;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent voice assistants are usually based on the cascade spoken language understanding (SLU) solution, which consists of an automatic speech recognition (ASR) engine and a natural language understanding (NLU) system. Because such approach relies on the ASR output, it often suffers from the so-called ASR error propagation. In this work, we investigate impacts of this ASR error propagation on state-of-the-art NLU systems based on pre-trained language models (PLM), such as BERT and RoBERTa. Moreover, a multimodal language understanding (MLU) module is proposed to mitigate SLU performance degradation caused by errors present in the ASR transcript. The MLU benefits from self-supervised features learned from both audio and text modalities, specifically Wav2Vec for speech and Bert/RoBERTa for language. Our MLU combines an encoder network to embed the audio signal and a text encoder to process text transcripts followed by a late fusion layer to fuse audio and text logits. We found that the pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.06777</link><description>&lt;p&gt;
&#25552;&#39640;&#20915;&#31574;&#26641;&#35299;&#37322;&#24615;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06777
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31454;&#20105;[&#21442;&#35265;Grinsztajn&#31561;&#20154;&#65292;NeurIPS 2022&#65292;arXiv&#65306;2207.08815]&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#21487;&#35299;&#37322;&#24615;&#21462;&#20915;&#20110;&#26641;&#30340;&#28145;&#24230;&#21644;&#27599;&#20010;&#21494;&#33410;&#28857;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20302;&#28145;&#24230;&#30340;&#26641;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27599;&#20010;&#21494;&#33410;&#28857;&#19978;&#30340;&#26368;&#22823;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#20174;&#20302;&#28145;&#24230;&#26641;&#30340;&#27599;&#20010;&#21494;&#33410;&#28857;&#8220;&#25346;&#36215;&#8221;&#36827;&#19968;&#27493;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#26080;&#38480;&#28145;&#24230;&#30340;&#26641;&#65289;&#12290;&#20302;&#28145;&#24230;&#26641;&#26131;&#20110;&#35299;&#37322;&#65292;&#32780;&#32508;&#21512;&#20302;&#28145;&#24230;&#21644;&#25346;&#36215;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#25972;&#20307;&#32479;&#35745;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#32463;&#20856;&#26041;&#27861;&#65288;&#20363;&#22914;CART&#65289;&#35757;&#32451;&#30340;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#20248;&#21270;&#30340;XGBoost&#65289;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30456;&#20284;&#24230;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35201;&#27714;&#34920;&#31034;&#24046;&#24322;&#30340;&#26041;&#24335;&#21487;&#34987;&#20154;&#31867;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#20195;&#29702;&#26641;&#25552;&#20379;&#30452;&#35266;&#30340;&#24046;&#24322;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#23545;AI&#31995;&#32479;&#36827;&#34892;&#22522;&#30784;&#24615;&#24605;&#32500;&#27169;&#22411;&#30340;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2306.06473</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24046;&#24322;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Interpretable Differencing of Machine Learning Models. (arXiv:2306.06473v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30456;&#20284;&#24230;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35201;&#27714;&#34920;&#31034;&#24046;&#24322;&#30340;&#26041;&#24335;&#21487;&#34987;&#20154;&#31867;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#20195;&#29702;&#26641;&#25552;&#20379;&#30452;&#35266;&#30340;&#24046;&#24322;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#23545;AI&#31995;&#32479;&#36827;&#34892;&#22522;&#30784;&#24615;&#24605;&#32500;&#27169;&#22411;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36873;&#25321;&#31454;&#20105;&#27169;&#22411;&#25110;&#26356;&#26032;&#24050;&#37096;&#32626;&#30340;&#27169;&#22411;&#26102;&#65292;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#36229;&#20986;&#20934;&#30830;&#29575;&#31561;&#25972;&#20307;&#25351;&#26631;&#30340;&#22522;&#30784;&#19978;&#65292;&#30830;&#23450;&#24046;&#24322;&#22312;&#29305;&#24449;&#31354;&#38388;&#30340;&#21738;&#20123;&#20301;&#32622;&#21457;&#29983;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#27169;&#22411;&#24046;&#24322;&#30340;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#39044;&#27979;&#20004;&#20010;ML&#27169;&#22411;&#36755;&#20986;&#30456;&#20284;&#24230;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#35201;&#27714;&#34920;&#31034;&#24046;&#24322;&#30340;&#26041;&#24335;&#21487;&#20197;&#34987;&#20154;&#31867;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23398;&#20064;&#19968;&#20010;&#32852;&#21512;&#20195;&#29702;&#26641;&#65288;JST&#65289;&#65292;&#23427;&#30001;&#20004;&#20010;&#20915;&#31574;&#26641;&#20195;&#29702;&#26500;&#25104;&#65292;&#20998;&#21035;&#23545;&#24212;&#20004;&#20010;&#27169;&#22411;&#12290;JST&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#24046;&#24322;&#34920;&#31034;&#65292;&#24182;&#23558;&#21464;&#21270;&#25918;&#32622;&#22312;&#27169;&#22411;&#20915;&#31574;&#36923;&#36753;&#30340;&#32972;&#26223;&#19979;&#12290;&#19978;&#19979;&#25991;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#24110;&#21161;&#29992;&#25143;&#23558;&#24046;&#24322;&#26144;&#23556;&#21040;AI&#31995;&#32479;&#30340;&#22522;&#30784;&#24615;&#24605;&#32500;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#21270;&#36807;&#31243;&#65292;&#20197;&#22686;&#21152;JST&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the differences between machine learning (ML) models is of interest in scenarios ranging from choosing amongst a set of competing models, to updating a deployed model with new training data. In these cases, we wish to go beyond differences in overall metrics such as accuracy to identify where in the feature space do the differences occur. We formalize this problem of model differencing as one of predicting a dissimilarity function of two ML models' outputs, subject to the representation of the differences being human-interpretable. Our solution is to learn a Joint Surrogate Tree (JST), which is composed of two conjoined decision tree surrogates for the two models. A JST provides an intuitive representation of differences and places the changes in the context of the models' decision logic. Context is important as it helps users to map differences to an underlying mental model of an AI system. We also propose a refinement procedure to increase the precision of a JST. We dem
&lt;/p&gt;</description></item><item><title>Aria&#25968;&#23383;&#23402;&#29983;&#26159;&#19968;&#20010;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#20854;&#23427;&#20219;&#20309;&#25968;&#25454;&#38598;&#37117;&#27809;&#26377;&#30340;&#39640;&#31934;&#24230;&#12289;&#29031;&#29255;&#36924;&#30495;&#21644;&#35814;&#23613;&#30340;&#30495;&#23454;&#20449;&#24687;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#35780;&#20272;&#30340;&#26032;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.06362</link><description>&lt;p&gt;
Aria&#25968;&#23383;&#23402;&#29983;&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#29992;&#20110;&#33258;&#25105;&#20013;&#24515;&#30340;3D&#26426;&#22120;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception. (arXiv:2306.06362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06362
&lt;/p&gt;
&lt;p&gt;
Aria&#25968;&#23383;&#23402;&#29983;&#26159;&#19968;&#20010;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#20854;&#23427;&#20219;&#20309;&#25968;&#25454;&#38598;&#37117;&#27809;&#26377;&#30340;&#39640;&#31934;&#24230;&#12289;&#29031;&#29255;&#36924;&#30495;&#21644;&#35814;&#23613;&#30340;&#30495;&#23454;&#20449;&#24687;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#35780;&#20272;&#30340;&#26032;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#20986;&#20102;Aria&#25968;&#23383;&#23402;&#29983;&#65288;ADT&#65289;-&#19968;&#20010;&#20351;&#29992;Aria&#30524;&#38236;&#25429;&#33719;&#30340;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23545;&#35937;&#65292;&#29615;&#22659;&#21644;&#20154;&#31867;&#32423;&#21035;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#35813;ADT&#25968;&#25454;&#38598;&#21253;&#25324;200&#20010;&#30001;&#31359;&#25140;Aria&#35774;&#22791;&#30340;&#20154;&#22312;&#20004;&#20010;&#23460;&#20869;&#30495;&#23454;&#22330;&#26223;&#20013;&#25191;&#34892;&#30340;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#24207;&#21015;&#65292;&#21253;&#21547;398&#20010;&#23545;&#35937;&#23454;&#20363;&#65288;324&#20010;&#38745;&#24577;&#21644;74&#20010;&#21160;&#24577;&#65289;&#12290;&#27599;&#20010;&#24207;&#21015;&#21253;&#25324;&#65306;a&#65289;&#20004;&#20010;&#21333;&#33394;&#30456;&#26426;&#27969;&#65292;&#19968;&#20010;RGB&#30456;&#26426;&#27969;&#65292;&#20004;&#20010;IMU&#27969;&#30340;&#21407;&#22987;&#25968;&#25454;&#65307;b&#65289;&#23436;&#25972;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#65307;c&#65289;&#30495;&#23454;&#25968;&#25454;&#65292;&#21253;&#25324;Aria&#35774;&#22791;&#30340;&#36830;&#32493;6&#33258;&#30001;&#24230;&#65288;6DoF&#65289;&#23039;&#24577;&#65292;&#23545;&#35937;6DoF&#23039;&#24577;&#65292;3D&#27880;&#35270;&#30690;&#37327;&#65292;3D&#20154;&#20307;&#23039;&#24577;&#65292;2D&#22270;&#20687;&#20998;&#21106;&#65292;&#22270;&#20687;&#28145;&#24230;&#22270;&#65307;d&#65289;&#29031;&#29255;&#33324;&#30495;&#23454;&#30340;&#21512;&#25104;&#28210;&#26579;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#33021;&#22815;&#19982;ADT&#30340;&#20934;&#30830;&#24615;&#12289;&#36924;&#30495;&#24230;&#21644;&#20840;&#38754;&#24615;&#30456;&#23218;&#32654;&#12290;&#36890;&#36807;&#21521;&#30740;&#31350;&#31038;&#21306;&#36129;&#29486;ADT&#65292;&#25105;&#20204;&#30340;&#20351;&#21629;&#26159;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#30340;&#35780;&#20272;&#35774;&#31435;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured using Aria glasses with extensive object, environment, and human level ground truth. This ADT release contains 200 sequences of real-world activities conducted by Aria wearers in two real indoor scenes with 398 object instances (324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two monochrome camera streams, one RGB camera stream, two IMU streams; b) complete sensor calibration; c) ground truth data including continuous 6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye gaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d) photo-realistic synthetic renderings. To the best of our knowledge, there is no existing egocentric dataset with a level of accuracy, photo-realism and comprehensiveness comparable to ADT. By contributing ADT to the research community, our mission is to set a new standard for evaluation in the egocentric machine perce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35760;&#24405;&#20102;&#19968;&#27425;&#40657;&#23458;&#26494;&#27963;&#21160;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#29305;&#24615;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.06283</link><description>&lt;p&gt;
LLMs&#22914;&#20309;&#25913;&#21464;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#40657;&#23458;&#39532;&#25289;&#26494;&#30340;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon. (arXiv:2306.06283v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35760;&#24405;&#20102;&#19968;&#27425;&#40657;&#23458;&#26494;&#27963;&#21160;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#29305;&#24615;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#38750;&#24120;&#22797;&#26434;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#25110;&#35745;&#31639;&#25216;&#26415;&#35299;&#20915;&#20102;&#36825;&#31181;&#22797;&#26434;&#24615;&#30340;&#20013;&#26377;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36755;&#20837;&#38656;&#35201;&#38750;&#24120;&#29305;&#23450;&#24418;&#24335;&#30340;&#32467;&#26500;&#20197;&#21450;&#24037;&#20855;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#25152;&#24102;&#26469;&#21487;&#29992;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#30340;&#25361;&#25112;&#12290;&#21152;&#19978;&#36825;&#20123;&#23398;&#31185;&#20013;&#30340;&#22823;&#22810;&#25968;&#25968;&#25454;&#37117;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#20107;&#23454;&#65292;&#20351;&#24471;&#36825;&#20123;&#24037;&#20855;&#30340;&#25928;&#29575;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35760;&#24405;&#20102;&#20851;&#20110;LLMs&#30340;&#40657;&#23458;&#26494;&#27963;&#21160;&#20013;&#26500;&#24314;&#30340;&#39033;&#30446;&#12290;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#30340;&#29305;&#24615;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#21508;&#31181;&#21508;&#26679;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#36825;&#20123;&#27169;&#22411;&#25913;&#21464;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemistry and materials science are complex. Recently, there have been great successes in addressing this complexity using data-driven or computational techniques. Yet, the necessity of input structured in very specific forms and the fact that there is an ever-growing number of tools creates usability and accessibility challenges. Coupled with the reality that much data in these disciplines is unstructured, the effectiveness of these tools is limited.  Motivated by recent works that indicated that large language models (LLMs) might help address some of these issues, we organized a hackathon event on the applications of LLMs in chemistry, materials science, and beyond. This article chronicles the projects built as part of this hackathon. Participants employed LLMs for various applications, including predicting properties of molecules and materials, designing novel interfaces for tools, extracting knowledge from unstructured data, and developing new educational applications.  The diverse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20445;&#23432;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;StepMix&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#23433;&#20840;&#22522;&#32447;&#31574;&#30053;&#24179;&#34913;&#24320;&#21457;&#21644;&#25506;&#32034;&#65292;&#21516;&#26102;&#20445;&#35777;&#27599;&#20010;&#22238;&#21512;&#19981;&#36829;&#21453;&#20445;&#23432;&#38480;&#21046;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2306.06265</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#22238;&#21512;&#38480;&#21046;&#30340;&#36817;&#20248;&#20445;&#23432;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Near-optimal Conservative Exploration in Reinforcement Learning under Episode-wise Constraints. (arXiv:2306.06265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20445;&#23432;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;StepMix&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#23433;&#20840;&#22522;&#32447;&#31574;&#30053;&#24179;&#34913;&#24320;&#21457;&#21644;&#25506;&#32034;&#65292;&#21516;&#26102;&#20445;&#35777;&#27599;&#20010;&#22238;&#21512;&#19981;&#36829;&#21453;&#20445;&#23432;&#38480;&#21046;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20445;&#23432;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#20195;&#29702;&#30340;&#24615;&#33021;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#35777;&#39640;&#20110;&#26576;&#20010;&#29305;&#23450;&#38408;&#20540;&#12290;&#30740;&#31350;&#38024;&#23545;&#26377;&#38480;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#26631;&#31614;&#24335;&#22238;&#21512;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StepMix&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#23433;&#20840;&#22522;&#32447;&#31574;&#30053;&#22312;&#20445;&#35777;&#27599;&#20010;&#22238;&#21512;&#19981;&#36829;&#21453;&#20445;&#23432;&#38480;&#21046;&#30340;&#21069;&#25552;&#19979;&#65292;&#24179;&#34913;&#24320;&#21457;&#21644;&#25506;&#32034;&#12290;StepMix&#20855;&#26377;&#29420;&#29305;&#30340;&#28151;&#21512;&#31574;&#30053;&#35774;&#35745;&#65292;&#33258;&#36866;&#24212;&#22320;&#12289;&#24179;&#28369;&#22320;&#25554;&#20540;&#22522;&#32447;&#31574;&#30053;&#21644;&#20048;&#35266;&#31574;&#30053;&#20043;&#38388;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;StepMix&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#37327;&#32423;&#65292;&#35828;&#26126;&#36981;&#23432;&#20005;&#26684;&#30340;&#22238;&#21512;&#38480;&#21046;&#19981;&#20250;&#25439;&#23475;&#23398;&#20064;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;EpsMix&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates conservative exploration in reinforcement learning where the performance of the learning agent is guaranteed to be above a certain threshold throughout the learning process. It focuses on the tabular episodic Markov Decision Process (MDP) setting that has finite states and actions. With the knowledge of an existing safe baseline policy, an algorithm termed as StepMix is proposed to balance the exploitation and exploration while ensuring that the conservative constraint is never violated in each episode with high probability. StepMix features a unique design of a mixture policy that adaptively and smoothly interpolates between the baseline policy and the optimistic policy. Theoretical analysis shows that StepMix achieves near-optimal regret order as in the constraint-free setting, indicating that obeying the stringent episode-wise conservative constraint does not compromise the learning performance. Besides, a randomization-based EpsMix algorithm is also proposed
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#30340;&#28014;&#28857;&#36816;&#31639;&#37327;&#25110;&#35774;&#22791;&#24310;&#36831;&#26469;&#33258;&#21160;&#35774;&#32622;&#21387;&#32553;&#36229;&#21442;&#25968;&#12290;&#31639;&#27861;&#36895;&#24230;&#24555;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#22810;&#31181;&#24120;&#29992;&#21387;&#32553;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#22914;&#21098;&#26525;&#12289;&#20302;&#31209;&#20998;&#35299;&#21644;&#37327;&#21270;&#12290;&#22312;GLUE&#24494;&#35843;&#20219;&#21153;&#30340;BERT&#21387;&#32553;&#20013;&#65292;FLOPs&#21487;&#38477;&#20302;50&#65285;&#65292;&#19988;&#24615;&#33021;&#20165;&#19979;&#38477;1&#65285;&#65307;&#32780;&#22312;ImageNet-1K&#19978;&#23545;MobileNetV3&#36827;&#34892;&#21387;&#32553;&#65292;&#21017;&#21487;&#20197;&#20351;FLOPs&#38477;&#20302;15&#65285;&#65292;&#21516;&#26102;&#24615;&#33021;&#22522;&#26412;&#19981;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2306.05785</link><description>&lt;p&gt;
&#36890;&#36807;$\frac{\ell_1}{\ell_2}$&#27491;&#21017;&#21270;&#24310;&#36831;&#20195;&#29702;&#36827;&#34892;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
End-to-End Neural Network Compression via $\frac{\ell_1}{\ell_2}$ Regularized Latency Surrogates. (arXiv:2306.05785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05785
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#30340;&#28014;&#28857;&#36816;&#31639;&#37327;&#25110;&#35774;&#22791;&#24310;&#36831;&#26469;&#33258;&#21160;&#35774;&#32622;&#21387;&#32553;&#36229;&#21442;&#25968;&#12290;&#31639;&#27861;&#36895;&#24230;&#24555;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#22810;&#31181;&#24120;&#29992;&#21387;&#32553;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#22914;&#21098;&#26525;&#12289;&#20302;&#31209;&#20998;&#35299;&#21644;&#37327;&#21270;&#12290;&#22312;GLUE&#24494;&#35843;&#20219;&#21153;&#30340;BERT&#21387;&#32553;&#20013;&#65292;FLOPs&#21487;&#38477;&#20302;50&#65285;&#65292;&#19988;&#24615;&#33021;&#20165;&#19979;&#38477;1&#65285;&#65307;&#32780;&#22312;ImageNet-1K&#19978;&#23545;MobileNetV3&#36827;&#34892;&#21387;&#32553;&#65292;&#21017;&#21487;&#20197;&#20351;FLOPs&#38477;&#20302;15&#65285;&#65292;&#21516;&#26102;&#24615;&#33021;&#22522;&#26412;&#19981;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#21387;&#32553;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#25110;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#35774;&#32622;&#27599;&#20010;&#23618;&#30340;&#21387;&#32553;&#36229;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;&#35201;&#21098;&#26525;&#30340;&#36890;&#36947;&#25968;&#65292;&#37327;&#21270;&#30340;&#20301;&#23485;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#25216;&#26415;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;$\frac{\ell_1}{\ell_2}$&#24310;&#36831;&#20195;&#29702;&#20248;&#21270;&#27169;&#22411;&#30340;&#28014;&#28857;&#36816;&#31639;&#37327;&#65288;FLOPs&#65289;&#25110;&#35774;&#22791;&#24310;&#36831;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#24120;&#29992;&#30340;&#21387;&#32553;&#26041;&#27861;&#65288;&#21253;&#25324;&#21098;&#26525;&#12289;&#20302;&#31209;&#20998;&#35299;&#21644;&#37327;&#21270;&#65289;&#19968;&#36215;&#20351;&#29992;&#12290;&#20851;&#38190;&#26159;&#65292;&#23427;&#38750;&#24120;&#24555;&#36895;&#65292;&#20960;&#20046;&#19982;&#21333;&#27169;&#22411;&#35757;&#32451;&#30456;&#21516;&#30340;&#26102;&#38388;&#65292;&#36825;&#27604;&#26631;&#20934;NAS&#26041;&#27861;&#21487;&#33410;&#30465;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#22312;GLUE&#24494;&#35843;&#20219;&#21153;&#30340;BERT&#21387;&#32553;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#20351;FLOPs&#38477;&#20302;50&#65285;&#65292;&#20165;&#25439;&#22833;1&#65285;&#30340;&#24615;&#33021;&#12290;&#32780;&#22312;ImageNet-1K&#19978;&#23545;MobileNetV3&#36827;&#34892;&#21387;&#32553;&#65292;&#21017;&#21487;&#20197;&#20351;FLOPs&#38477;&#20302;15&#65285;&#65292;&#21516;&#26102;&#24615;&#33021;&#22522;&#26412;&#19981;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) compression via techniques such as pruning, quantization requires setting compression hyperparameters (e.g., number of channels to be pruned, bitwidths for quantization) for each layer either manually or via neural architecture search (NAS) which can be computationally expensive. We address this problem by providing an end-to-end technique that optimizes for model's Floating Point Operations (FLOPs) or for on-device latency via a novel $\frac{\ell_1}{\ell_2}$ latency surrogate. Our algorithm is versatile and can be used with many popular compression methods including pruning, low-rank factorization, and quantization. Crucially, it is fast and runs in almost the same amount of time as single model training; which is a significant training speed-up over standard NAS methods. For BERT compression on GLUE fine-tuning tasks, we achieve $50\%$ reduction in FLOPs with only $1\%$ drop in performance. For compressing MobileNetV3 on ImageNet-1K, we achieve $15\%$ reduction in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;CDP&#25552;&#20986;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#32852;&#37030;&#31639;&#27861;\robin&#65292;&#22312;LDP&#19979;&#35777;&#26126;&#20102;&#23398;&#20064;&#24517;&#39035;&#25215;&#21463;&#33267;&#23569;&#19968;&#20010;&#36951;&#25022;&#33192;&#32960;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2306.05275</link><description>&lt;p&gt;
&#24102;&#26377;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Linear Contextual Bandits with User-level Differential Privacy. (arXiv:2306.05275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;CDP&#25552;&#20986;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#32852;&#37030;&#31639;&#27861;\robin&#65292;&#22312;LDP&#19979;&#35777;&#26126;&#20102;&#23398;&#20064;&#24517;&#39035;&#25215;&#21463;&#33267;&#23569;&#19968;&#20010;&#36951;&#25022;&#33192;&#32960;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#27010;&#24565;&#19979;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;DP&#30340;&#21508;&#31181;&#23450;&#20041;&#12290;&#28982;&#21518;&#22312;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#27491;&#24335;&#24341;&#20837;&#20102;&#29992;&#25143;&#32423;&#20013;&#24515;DP&#21644;&#26412;&#22320;DP&#65292;&#24182;&#30740;&#31350;&#20102;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#23398;&#20064;&#36951;&#25022;&#21644;&#30456;&#24212;DP&#20445;&#35777;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#23545;&#20110;CDP&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;\robin&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25512;&#23548;&#22312;&#28385;&#36275;&#29992;&#25143;&#32423;DP&#26102;&#30340;&#20960;&#20046;&#21305;&#37197;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#36951;&#25022;&#30028;&#65292;&#35777;&#26126;&#20854;&#22312;&#23458;&#25143;&#31471;&#25968;&#37327;$M$&#21644;&#38544;&#31169;&#39044;&#31639;$\varepsilon$&#26041;&#38754;&#26159;&#36817;&#20046;&#26368;&#20248;&#30340;&#12290;&#23545;&#20110;LDP&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20960;&#20010;&#19979;&#30028;&#65292;&#34920;&#26126;&#22312;&#29992;&#25143;&#32423;$(\varepsilon,\delta)$-LDP&#19979;&#23398;&#20064;&#24517;&#39035;&#33267;&#23569;&#25215;&#21463;&#19968;&#20010;&#36951;&#25022;&#33192;&#32960;&#22240;&#23376;&#33267;&#23569;&#20026;{$\min\{1/\varepsilon,M\}$&#25110;$\min\{1/\sqrt{\varepsilon},\sq
&lt;/p&gt;
&lt;p&gt;
This paper studies federated linear contextual bandits under the notion of user-level differential privacy (DP). We first introduce a unified federated bandits framework that can accommodate various definitions of DP in the sequential decision-making setting. We then formally introduce user-level central DP (CDP) and local DP (LDP) in the federated bandits framework, and investigate the fundamental trade-offs between the learning regrets and the corresponding DP guarantees in a federated linear contextual bandits model. For CDP, we propose a federated algorithm termed as \robin and show that it is near-optimal in terms of the number of clients $M$ and the privacy budget $\varepsilon$ by deriving nearly-matching upper and lower regret bounds when user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating that learning under user-level $(\varepsilon,\delta)$-LDP must suffer a regret blow-up factor at least {$\min\{1/\varepsilon,M\}$ or $\min\{1/\sqrt{\varepsilon},\sq
&lt;/p&gt;</description></item><item><title>EMO&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#22806;&#37096;&#23384;&#20648;&#22120;&#20013;&#35760;&#24405;&#36807;&#21435;&#20219;&#21153;&#30340;&#26799;&#24230;&#21382;&#21490;&#65292;&#23454;&#29616;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#26080;&#35770;&#25552;&#20379;&#30340;&#26799;&#24230;&#20449;&#24687;&#26159;&#21542;&#21487;&#38752;&#65292;&#37117;&#21487;&#20197;&#25512;&#21160;&#21442;&#25968;&#26356;&#26032;&#26397;&#30528;&#27491;&#30830;&#30340;&#26041;&#21521;&#21069;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.05189</link><description>&lt;p&gt;
EMO&#65306;&#29992;&#20110;&#23567;&#26679;&#26412;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
EMO: Episodic Memory Optimization for Few-Shot Meta-Learning. (arXiv:2306.05189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05189
&lt;/p&gt;
&lt;p&gt;
EMO&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#22806;&#37096;&#23384;&#20648;&#22120;&#20013;&#35760;&#24405;&#36807;&#21435;&#20219;&#21153;&#30340;&#26799;&#24230;&#21382;&#21490;&#65292;&#23454;&#29616;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#26080;&#35770;&#25552;&#20379;&#30340;&#26799;&#24230;&#20449;&#24687;&#26159;&#21542;&#21487;&#38752;&#65292;&#37117;&#21487;&#20197;&#25512;&#21160;&#21442;&#25968;&#26356;&#26032;&#26397;&#30528;&#27491;&#30830;&#30340;&#26041;&#21521;&#21069;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#20803;&#23398;&#20064;&#30001;&#20110;&#20219;&#21153;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#30340;&#38480;&#21046;&#23545;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;&#26041;&#26696;&#65292;&#31216;&#20026;EMO&#12290;EMO&#21463;&#21040;&#20154;&#31867;&#20174;&#33041;&#20869;&#35760;&#24518;&#20013;&#22238;&#24518;&#36807;&#21435;&#23398;&#20064;&#32463;&#39564;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#23558;&#36807;&#21435;&#20219;&#21153;&#30340;&#26799;&#24230;&#21382;&#21490;&#35760;&#24405;&#22312;&#22806;&#37096;&#23384;&#20648;&#22120;&#20013;&#65292;&#20197;&#22686;&#24378;&#35760;&#24518;&#30340;&#26041;&#24335;&#36827;&#34892;&#23567;&#26679;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#20445;&#30041;&#21644;&#22238;&#24518;&#36807;&#21435;&#35757;&#32451;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21363;&#20351;&#20165;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#31034;&#20363;&#25552;&#20379;&#20102;&#19981;&#21487;&#38752;&#30340;&#26799;&#24230;&#65292;EMO&#20063;&#21487;&#20197;&#25512;&#21160;&#21442;&#25968;&#26356;&#26032;&#26397;&#30528;&#27491;&#30830;&#30340;&#26041;&#21521;&#21069;&#36827;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#23545;&#20110;&#24179;&#28369;&#12289;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#20250;&#25910;&#25947;&#12290;EMO&#26159;&#36890;&#29992;&#30340;&#12289;&#28789;&#27963;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#26080;&#32541;&#23884;&#20837;&#29616;&#26377;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#23567;&#26679;&#26412;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;EMO&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot meta-learning presents a challenge for gradient descent optimization due to the limited number of training samples per task. To address this issue, we propose an episodic memory optimization for meta-learning, we call \emph{EMO}, which is inspired by the human ability to recall past learning experiences from the brain's memory. EMO retains the gradient history of past experienced tasks in external memory, enabling few-shot learning in a memory-augmented way. By learning to retain and recall the learning process of past training tasks, EMO nudges parameter updates in the right direction, even when the gradients provided by a limited number of examples are uninformative. We prove theoretically that our algorithm converges for smooth, strongly convex objectives. EMO is generic, flexible, and model-agnostic, making it a simple plug-and-play optimizer that can be seamlessly embedded into existing optimization-based few-shot meta-learning approaches. Empirical results show that EMO 
&lt;/p&gt;</description></item><item><title>FLEdge&#26159;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#30740;&#31350;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#33021;&#37327;&#25928;&#29575;&#21644;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.05172</link><description>&lt;p&gt;
FLEdge&#65306;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems. (arXiv:2306.05172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05172
&lt;/p&gt;
&lt;p&gt;
FLEdge&#26159;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#30740;&#31350;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#33021;&#37327;&#25928;&#29575;&#21644;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#65288;FL&#65289;&#22791;&#21463;&#20851;&#27880;&#12290; FL&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#22312;&#27169;&#25311;&#31995;&#32479;&#25110;&#25968;&#25454;&#20013;&#24515;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#24573;&#30053;&#20102;&#19982;&#36793;&#32536;&#35745;&#31639;&#23494;&#20999;&#30456;&#20851;&#30340;&#23454;&#38469;&#31995;&#32479;&#35774;&#32622;&#12290; &#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;FLEdge&#26469;&#24357;&#34917;&#36825;&#19968;&#30740;&#31350;&#24046;&#36317;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#25928;&#29575;&#20197;&#21450;&#21508;&#31181;&#19981;&#21516;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20351;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#36866;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#20855;&#26377;&#39640;&#36798;50&#65285;&#22833;&#25928;&#29575;&#30340;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#12290; FLEdge&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#20363;&#22914;&#65292;&#22312;&#26087;GPU&#21152;&#36895;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#27604;&#22312;&#29616;&#20195;&#26381;&#21153;&#22120;&#32423;GPU&#19978;&#35757;&#32451;&#39640;&#36798;3&#20493;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Machine Learning (FL) has received considerable attention in recent years. FL benchmarks are predominantly explored in either simulated systems or data center environments, neglecting the setups of real-world systems, which are often closely linked to edge computing. We close this research gap by introducing FLEdge, a benchmark targeting FL workloads in edge computing systems. We systematically study hardware heterogeneity, energy efficiency during training, and the effect of various differential privacy levels on training in FL systems. To make this benchmark applicable to real-world scenarios, we evaluate the impact of client dropouts on state-of-the-art FL strategies with failure rates as high as 50%. FLEdge provides new insights, such as that training state-of-the-art FL workloads on older GPU-accelerated embedded devices is up to 3x more energy efficient than on modern server-grade GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#21608;&#26399;-&#27880;&#24847;&#26426;&#21046;&#65292;&#21517;&#20026;Periodformer&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;Transformer-based&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#26469;&#20445;&#35777;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05035</link><description>&lt;p&gt;
&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#26159;&#21542;&#38656;&#35201;&#22797;&#26434;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#39069;&#22806;&#30340;&#38271;&#36755;&#20837;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Long-Term Series Forecasting Need Complex Attention and Extra Long Inputs?. (arXiv:2306.05035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#21608;&#26399;-&#27880;&#24847;&#26426;&#21046;&#65292;&#21517;&#20026;Periodformer&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;Transformer-based&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#26469;&#20445;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#38271;&#26399;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#22312;&#36817;&#24180;&#26469;&#20063;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#25152;&#22266;&#26377;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#38656;&#35201;&#38271;&#24207;&#21015;&#65292;&#23427;&#22312;LTSF&#20219;&#21153;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65306;1&#65289;&#36825;&#20123;&#26041;&#27861;&#35774;&#35745;&#30340;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#26159;&#21542;&#23454;&#38469;&#19978;&#32553;&#30701;&#20102;&#30495;&#23454;&#35774;&#22791;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#65307;2&#65289;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#38656;&#35201;&#39069;&#22806;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#26469;&#20445;&#35777;&#23427;&#20204;&#30340;&#24615;&#33021;&#65311;&#26412;&#35770;&#25991;&#30340;&#31572;&#26696;&#26159;&#21542;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21608;&#26399;-&#27880;&#24847;&#26426;&#21046;&#65288;Periodformer&#65289;&#65292;&#36890;&#36807;&#26174;&#24335;&#21608;&#26399;&#24615;&#21644;&#20869;&#32622;&#30340;&#25509;&#36817;&#24615;&#26469;&#37325;&#26032;&#35774;&#35745;&#38271;&#26399;&#23376;&#24207;&#21015;&#21644;&#30701;&#26399;&#23376;&#24207;&#21015;&#30340;&#32858;&#21512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23884;&#20837;&#20102;&#19968;&#20010;&#38376;&#25511;&#26426;&#21046;&#21040;Periodformer&#20013;&#20197;&#35843;&#25972;&#27880;&#24847;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Transformer-based models have achieved impressive performance on various time series tasks, Long-Term Series Forecasting (LTSF) tasks have also received extensive attention in recent years. However, due to the inherent computational complexity and long sequences demanding of Transformer-based methods, its application on LTSF tasks still has two major issues that need to be further investigated: 1) Whether the sparse attention mechanism designed by these methods actually reduce the running time on real devices; 2) Whether these models need extra long input sequences to guarantee their performance? The answers given in this paper are negative. Therefore, to better copy with these two issues, we design a lightweight Period-Attention mechanism (Periodformer), which renovates the aggregation of long-term subseries via explicit periodicity and short-term subseries via built-in proximity. Meanwhile, a gating mechanism is embedded into Periodformer to regulate the influence of the attention
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04551</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#35757;&#32451;&#32467;&#21512;&#39046;&#22495;&#20869;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35786;&#26029;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning. (arXiv:2306.04551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26159;&#22686;&#24378;&#20020;&#24202;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#21644;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#35786;&#26029;&#25512;&#29702;&#22522;&#20934;&#65288;DR.BENCH&#65289;&#20316;&#20026;&#20840;&#38754;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#30001;&#20845;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#20195;&#34920;&#20020;&#24202;&#25512;&#29702;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880; DR.BENCH &#30340;&#38382;&#39064;&#24635;&#32467;&#20219;&#21153;&#65288;Gao &#31561;&#65292;2023&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20248;&#20110;&#20854;&#19968;&#33324;&#39046;&#22495;&#30340;&#23545;&#24212;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292; ROUGE-L &#24471;&#20998;&#20026; 28.55&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#22312;&#20248;&#21270;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence (AI) is a promising direction for augmenting clinical diagnostic decision support and reducing diagnostic errors, a leading contributor to medical errors. To further the development of clinical AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a comprehensive generative AI framework, comprised of six tasks representing key components in clinical reasoning. We present a comparative analysis of in-domain versus out-of-domain language models as well as multi-task versus single task training with a focus on the problem summarization task in DR.BENCH (Gao et al., 2023). We demonstrate that a multi-task, clinically trained language model outperforms its general domain counterpart by a large margin, establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55. This research underscores the value of domain-specific training for optimizing clinical diagnostic reasoning tasks.
&lt;/p&gt;</description></item><item><title>&#35299;&#20915;&#20102;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#24050;&#30693;&#26041;&#27861;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;1.5&#20493;&#30340;&#22823;&#23567;&#19979;&#23454;&#29616;&#19982;&#20004;&#20493;&#30456;&#21516;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.04489</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Fair Column Subset Selection. (arXiv:2306.04489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04489
&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20102;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#24050;&#30693;&#26041;&#27861;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;1.5&#20493;&#30340;&#22823;&#23567;&#19979;&#23454;&#29616;&#19982;&#20004;&#20493;&#30456;&#21516;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#20013;&#23384;&#22312;&#20004;&#20010;&#32676;&#20307;&#65292;&#24182;&#19988;&#25152;&#36873;&#21015;&#23376;&#38598;&#24517;&#39035;&#30456;&#23545;&#20110;&#23427;&#20204;&#21508;&#33258;&#30340;&#26368;&#20339;&#31209;-k&#36924;&#36817;&#25552;&#20379;&#33391;&#22909;&#30340;&#36817;&#20284;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20844;&#24179;&#35774;&#32622;&#24341;&#20837;&#20102;&#37325;&#22823;&#25361;&#25112;&#65306;&#20026;&#20102;&#25193;&#23637;&#24050;&#30693;&#32467;&#26524;&#65292;&#20154;&#20204;&#19981;&#33021;&#20570;&#24471;&#27604;&#31616;&#21333;&#22320;&#36873;&#25321;&#21407;&#22987;&#26041;&#27861;&#30340;&#20004;&#20493;&#21015;&#26356;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#30340;&#24050;&#30693;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#20004;&#20010;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20165;&#37319;&#26679;&#36866;&#24403;&#22823;&#23567;&#30340;&#23376;&#38598;&#23601;&#21464;&#24471;NP&#38590;&#12290;&#32780;&#25214;&#21040;&#20004;&#20493;&#20110;&#25152;&#38656;&#22823;&#23567;&#30340;&#23376;&#38598;&#21017;&#38750;&#24120;&#31616;&#21333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22522;&#26412;&#19978;1.5&#20493;&#30340;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30456;&#21516;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of fair column subset selection. In particular, we assume that two groups are present in the data, and the chosen column subset must provide a good approximation for both, relative to their respective best rank-k approximations. We show that this fair setting introduces significant challenges: in order to extend known results, one cannot do better than the trivial solution of simply picking twice as many columns as the original methods. We adopt a known approach based on deterministic leverage-score sampling, and show that merely sampling a subset of appropriate size becomes NP-hard in the presence of two groups. Whereas finding a subset of two times the desired size is trivial, we provide an efficient algorithm that achieves the same guarantees with essentially 1.5 times that size. We validate our methods through an extensive set of experiments on real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#20854;&#20013;iDP-SignRP&#31639;&#27861;&#22312;&#20010;&#20307;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#25928;&#26524;&#26174;&#33879;&#65292;DP-SignOPORP&#31639;&#27861;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;DP-OPORP&#31639;&#27861;&#34920;&#29616;&#26368;&#20248;&#65292;iDP&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.01751</link><description>&lt;p&gt;
&#38543;&#26426;&#25237;&#24433;&#21644;&#31526;&#21495;&#38543;&#26426;&#25237;&#24433;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy with Random Projections and Sign Random Projections. (arXiv:2306.01751v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#20854;&#20013;iDP-SignRP&#31639;&#27861;&#22312;&#20010;&#20307;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#25928;&#26524;&#26174;&#33879;&#65292;DP-SignOPORP&#31639;&#27861;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;DP-OPORP&#31639;&#27861;&#34920;&#29616;&#26368;&#20248;&#65292;iDP&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#65288;RP&#65289;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#25366;&#25496;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#21508;&#31181;&#24212;&#29992;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#31526;&#21495;&#38543;&#26426;&#25237;&#24433;&#65288;SignRP&#65289;&#30340;iDP-SignRP&#31639;&#27861;&#22312;&#20010;&#20307;&#24046;&#20998;&#38544;&#31169;&#65288;iDP&#65289;&#35774;&#32622;&#19979;&#38750;&#24120;&#26377;&#25928;&#65292;&#32780;DP-SignOPORP&#31639;&#27861;&#22312;&#26631;&#20934;DP&#35774;&#32622;&#19979;&#21033;&#29992;&#8220;&#19968;&#27425;&#25490;&#21015;+&#19968;&#27425;&#38543;&#26426;&#25237;&#24433;&#8221;&#65288;OPORP&#65289;&#26497;&#22823;&#22320;&#25913;&#36827;&#20102;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;&#38500;&#19981;&#32771;&#34385;&#31526;&#21495;&#20043;&#22806;&#65292;&#22312;DP-RP&#23478;&#26063;&#20013;&#65292;DP-OPORP&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;iDP&#65288;&#20010;&#20307;&#24046;&#20998;&#38544;&#31169;&#65289;&#30340;&#27010;&#24565;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;iDP&#19981;&#26159;&#20005;&#26684;&#30340;DP&#65292;&#20294;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65288;&#22914;&#21521;&#23567;&#32452;&#29992;&#25143;&#21457;&#24067;&#21253;&#25324;&#23884;&#20837;&#20449;&#24687;&#25110;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20869;&#23481;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#19981;&#27844;&#38706;&#19981;&#23646;&#20110;&#35813;&#32452;&#30340;&#20010;&#20154;&#30340;&#20219;&#20309;&#31169;&#20154;&#20449;&#24687;&#65289;&#21487;&#33021;&#24456;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a series of differential privacy (DP) algorithms from a family of random projections (RP), for general applications in machine learning, data mining, and information retrieval. Among the presented algorithms, \textbf{iDP-SignRP} is remarkably effective under the setting of ``individual differential privacy'' (iDP), based on sign random projections (SignRP). Also, \textbf{DP-SignOPORP} considerably improves existing algorithms in the literature under the standard DP setting, using ``one permutation + one random projection'' (OPORP), where OPORP is a variant of the celebrated count-sketch method with fixed-length binning and normalization. Without taking signs, among the DP-RP family, \textbf{DP-OPORP} achieves the best performance.  The concept of iDP (individual differential privacy) is defined only on a particular dataset of interest. While iDP is not strictly DP, iDP might be useful in certain applications, such as releasing a dataset (including sharing embe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#20445;&#23432;&#20540;&#20272;&#35745;&#21644;&#35880;&#24910;&#25506;&#32034;&#26041;&#38754;&#30340;&#26126;&#30830;&#25972;&#21512;&#26469;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01460</link><description>&lt;p&gt;
ReLU&#25327;&#25937;&#65306;&#29992;&#27491;&#25968;&#20248;&#21183;&#25913;&#36827;&#24744;&#30340;On-Policy Actor-Critic&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages. (arXiv:2306.01460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#20445;&#23432;&#20540;&#20272;&#35745;&#21644;&#35880;&#24910;&#25506;&#32034;&#26041;&#38754;&#30340;&#26126;&#30830;&#25972;&#21512;&#26469;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#26126;&#30830;&#22320;&#25972;&#21512;&#35880;&#24910;&#30340;&#29615;&#22659;&#20132;&#20114;&#26469;&#35299;&#20915;&#24403;&#21069;On-Policy&#31639;&#27861;&#65288;&#22914;Proximal Policy Optimization&#21644;Asynchronous Advantage Actor-Critic&#65289;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#26368;&#22823;&#21270;&#30495;&#23454;&#20215;&#20540;&#20989;&#25968;&#21152;&#19978;&#24120;&#37327;&#30340;&#19979;&#30028;&#65292;&#20174;&#32780;&#20419;&#36827;&#8220;&#20445;&#23432;&#20540;&#20272;&#35745;&#8221;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;Thompson&#37319;&#26679;&#26469;&#36827;&#34892;&#35880;&#24910;&#25506;&#32034;&#12290;&#36825;&#20123;&#29305;&#28857;&#36890;&#36807;&#23545;A3C&#31639;&#27861;&#36827;&#34892;&#19977;&#20010;&#24778;&#20154;&#31616;&#21333;&#30340;&#20462;&#25913;&#23454;&#29616;&#65306;&#36890;&#36807;ReLU&#20989;&#25968;&#22788;&#29702;&#20248;&#21183;&#20272;&#35745;&#65292;&#36827;&#34892;&#35889;&#24402;&#19968;&#21270;&#21644;&#38543;&#26426;&#22833;&#27963;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26368;&#22823;&#21270;&#20102;&#19979;&#30028;&#65292;&#36825;&#20063;&#26159;&#22810;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;Regret Matching Policy Gradients&#65288;RMPG&#65289;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel method for enhancing the effectiveness of on-policy Deep Reinforcement Learning (DRL) algorithms. Current on-policy algorithms, such as Proximal Policy Optimization (PPO) and Asynchronous Advantage Actor-Critic (A3C), do not sufficiently account for cautious interaction with the environment. Our method addresses this gap by explicitly integrating cautious interaction in two critical ways: by maximizing a lower-bound on the true value function plus a constant, thereby promoting a \textit{conservative value estimation}, and by incorporating Thompson sampling for cautious exploration. These features are realized through three surprisingly simple modifications to the A3C algorithm: processing advantage estimates through a ReLU function, spectral normalization, and dropout. We provide theoretical proof that our algorithm maximizes the lower bound, which also grounds Regret Matching Policy Gradients (RMPG), a discrete-action on-policy method for multi-agen
&lt;/p&gt;</description></item><item><title>DyGen&#26159;&#19968;&#20010;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19395</link><description>&lt;p&gt;
DyGen: &#36890;&#36807;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#24314;&#27169;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling. (arXiv:2305.19395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19395
&lt;/p&gt;
&lt;p&gt;
DyGen&#26159;&#19968;&#20010;&#21160;&#24577;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#19981;&#27491;&#30830;&#25110;&#24050;&#25439;&#22351;&#30340;&#26631;&#31614;&#65292;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#26631;&#31614;&#22122;&#22768;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#20351;&#29992;&#38745;&#24577;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#21435;&#22122;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#21463;&#38480;&#20110;&#23427;&#20204;&#22312;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#26041;&#38754;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#23548;&#33268;&#26377;&#20559;&#30340;&#25110;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DyGen&#30340;&#21160;&#24577;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#26469;&#25913;&#21892;&#22122;&#22768;&#26631;&#31614;&#39044;&#27979;&#12290;DyGen&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#26694;&#26550;&#20174;&#22122;&#22768;&#26631;&#31614;&#21644;&#35757;&#32451;&#21160;&#24577;&#20013;&#25512;&#26029;&#30495;&#23454;&#26631;&#31614;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20849;&#35268;&#27491;&#21017;&#21270;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#28508;&#22312;&#22122;&#22768;&#26631;&#31614;&#21644;&#20808;&#39564;&#30340;&#24433;&#21709;&#12290;&#22312;&#23384;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#26631;&#31614;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;DyGen&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy labels is a challenge that arises in many real-world applications where training data can contain incorrect or corrupted labels. When fine-tuning language models with noisy labels, models can easily overfit the label noise, leading to decreased performance. Most existing methods for learning from noisy labels use static input features for denoising, but these methods are limited by the information they can provide on true label distributions and can result in biased or incorrect predictions. In this work, we propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic patterns in the embedding space during the fine-tuning process of language models to improve noisy label predictions. DyGen uses the variational auto-encoding framework to infer the posterior distributions of true labels from noisy labels and training dynamics. Additionally, a co-regularization mechanism is used to minimize the impact of potentially noisy labels and priors. DyGen demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DIVA&#31639;&#27861;&#65292;&#19968;&#20010;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#30340;&#22686;&#37327;&#28145;&#24230;&#32858;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#38480;&#28151;&#21512;&#39640;&#26031;&#20316;&#20026;&#20808;&#39564;&#65292;&#24182;&#21033;&#29992;&#19968;&#31181;&#35760;&#24518;&#21270;&#30340;&#22312;&#32447;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#23454;&#29616;&#31751;&#30340;&#21160;&#24577;&#36866;&#24212;&#31227;&#21160;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#30693;&#36947;&#29305;&#24449;&#30340;&#25968;&#37327;&#12290;&#35813;&#31639;&#27861;&#34920;&#29616;&#20248;&#36234;&#65292;&#29305;&#21035;&#26159;&#22312;&#22686;&#37327;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14067</link><description>&lt;p&gt;
DIVA&#65306;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#22686;&#37327;&#28145;&#24230;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DIVA: A Dirichlet Process Based Incremental Deep Clustering Algorithm via Variational Auto-Encoder. (arXiv:2305.14067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DIVA&#31639;&#27861;&#65292;&#19968;&#20010;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#30340;&#22686;&#37327;&#28145;&#24230;&#32858;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#38480;&#28151;&#21512;&#39640;&#26031;&#20316;&#20026;&#20808;&#39564;&#65292;&#24182;&#21033;&#29992;&#19968;&#31181;&#35760;&#24518;&#21270;&#30340;&#22312;&#32447;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#23454;&#29616;&#31751;&#30340;&#21160;&#24577;&#36866;&#24212;&#31227;&#21160;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#30693;&#36947;&#29305;&#24449;&#30340;&#25968;&#37327;&#12290;&#35813;&#31639;&#27861;&#34920;&#29616;&#20248;&#36234;&#65292;&#29305;&#21035;&#26159;&#22312;&#22686;&#37327;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#28145;&#24230;&#32858;&#31867;&#26694;&#26550;&#22312;&#20998;&#31867;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22788;&#29702;&#21160;&#24577;&#21644;&#22797;&#26434;&#29305;&#24449;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20808;&#30693;&#36947;&#31751;&#30340;&#25968;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#28145;&#24230;&#32858;&#31867;&#26694;&#26550;&#65292;&#37319;&#29992;&#26080;&#38480;&#28151;&#21512;&#39640;&#26031;&#20316;&#20026;&#20808;&#39564;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#19968;&#31181;&#35760;&#24518;&#21270;&#30340;&#22312;&#32447;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#31751;&#30340;&#8220;&#20986;&#29983;&#8221;&#21644;&#8220;&#21512;&#24182;&#8221;&#31227;&#21160;&#65292;&#20351;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20197;&#8220;&#21160;&#24577;&#36866;&#24212;&#8221;&#30340;&#26041;&#24335;&#32858;&#31867;&#25968;&#25454;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#30693;&#36947;&#29305;&#24449;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#25226;&#35813;&#26694;&#26550;&#21629;&#21517;&#20026;DIVA&#65292;&#21363;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#30340;&#22686;&#37327;&#28145;&#24230;&#32858;&#31867;&#26694;&#26550;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20998;&#31867;&#20855;&#26377;&#21160;&#24577;&#21464;&#21270;&#29305;&#24449;&#30340;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20248;&#36234;&#65292;&#29305;&#21035;&#26159;&#22312;&#22686;&#37327;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative model-based deep clustering frameworks excel in classifying complex data, but are limited in handling dynamic and complex features because they require prior knowledge of the number of clusters. In this paper, we propose a nonparametric deep clustering framework that employs an infinite mixture of Gaussians as a prior. Our framework utilizes a memoized online variational inference method that enables the "birth" and "merge" moves of clusters, allowing our framework to cluster data in a "dynamic-adaptive" manner, without requiring prior knowledge of the number of features. We name the framework as DIVA, a Dirichlet Process-based Incremental deep clustering framework via Variational Auto-Encoder. Our framework, which outperforms state-of-the-art baselines, exhibits superior performance in classifying complex data with dynamically changing features, particularly in the case of incremental features.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26680;&#30697;&#27861;&#20272;&#35745;&#22120;&#65292;&#31216;&#20026;KMM&#65292;&#20854;&#29992;&#20110;&#36229;&#36234;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#30697;&#26041;&#27861;&#27169;&#22411;&#65292;&#35299;&#38500;&#20102;&#20851;&#20110;&#20351;&#29992; $\varphi$-&#25955;&#24230;&#30456;&#20851;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.10898</link><description>&lt;p&gt;
&#36229;&#36234;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#65306;&#26680;&#30697;&#27861;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation Beyond Data Reweighting: Kernel Method of Moments. (arXiv:2305.10898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26680;&#30697;&#27861;&#20272;&#35745;&#22120;&#65292;&#31216;&#20026;KMM&#65292;&#20854;&#29992;&#20110;&#36229;&#36234;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#30697;&#26041;&#27861;&#27169;&#22411;&#65292;&#35299;&#38500;&#20102;&#20851;&#20110;&#20351;&#29992; $\varphi$-&#25955;&#24230;&#30456;&#20851;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#19982;&#32479;&#35745;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#37117;&#20250;&#20986;&#29616;&#30697;&#32422;&#26463;&#21644;&#26465;&#20214;&#23545;&#24212;&#65292;&#20854;&#20013;&#65292;&#24191;&#20041;&#30697;&#27861;&#65288;GMM&#65289;&#20316;&#20026;&#19968;&#20010;&#20272;&#35745;&#27169;&#22411;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24448;&#24448;&#30001;&#20110;&#20351;&#29992; $\varphi$-&#25955;&#24230;&#30340;&#30456;&#20851;&#38480;&#21046;&#23558;&#20505;&#36873;&#20998;&#24067;&#38480;&#21046;&#20026;&#25968;&#25454;&#26679;&#26412;&#30340;&#37325;&#26032;&#21152;&#26435;&#12290;&#32780;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30697;&#20272;&#35745;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#30340;&#32463;&#39564;&#20284;&#28982;&#20272;&#35745;&#22120;&#65292;&#21363;&#26680;&#30697;&#27861;(KMM)&#65292;&#20854;&#23454;&#29616;&#36229;&#36234;&#20102;&#23545;&#25968;&#25454;&#30340;&#37325;&#26032;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moment restrictions and their conditional counterparts emerge in many areas of machine learning and statistics ranging from causal inference to reinforcement learning. Estimators for these tasks, generally called methods of moments, include the prominent generalized method of moments (GMM) which has recently gained attention in causal inference. GMM is a special case of the broader family of empirical likelihood estimators which are based on approximating a population distribution by means of minimizing a $\varphi$-divergence to an empirical distribution. However, the use of $\varphi$-divergences effectively limits the candidate distributions to reweightings of the data samples. We lift this long-standing limitation and provide a method of moments that goes beyond data reweighting. This is achieved by defining an empirical likelihood estimator based on maximum mean discrepancy which we term the kernel method of moments (KMM). We provide a variant of our estimator for conditional moment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#36214;&#33976;&#39311;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20256;&#32479;&#37319;&#26679;&#31639;&#27861;&#65292;&#35753;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;&#20415;&#33021;&#21152;&#36895;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10769</link><description>&lt;p&gt;
&#36861;&#36214;&#33976;&#39311;&#65306;&#21152;&#36895;&#37319;&#26679;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling. (arXiv:2305.10769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#36214;&#33976;&#39311;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20256;&#32479;&#37319;&#26679;&#31639;&#27861;&#65292;&#35753;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;&#20415;&#33021;&#21152;&#36895;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#36890;&#24120;&#38656;&#35201;&#25191;&#34892;&#22823;&#37327;&#30340;&#37319;&#26679;&#27493;&#39588;&#65292;&#36825;&#38459;&#30861;&#20102;&#23454;&#26102;&#26679;&#26412;&#21512;&#25104;&#30340;&#21487;&#33021;&#24615;&#12290;&#20256;&#32479;&#30340;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21152;&#36895;&#37319;&#26679;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#21644;&#31163;&#25955;&#26102;&#38388;&#27493;&#39588;&#22330;&#26223;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#22521;&#35757;&#35838;&#31243;&#25165;&#33021;&#23454;&#29616;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36861;&#36214;&#33976;&#39311;&#65288;CUD&#65289;&#65292;&#23427;&#40723;&#21169;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#8220;&#36861;&#36214;&#8221;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CUD&#35843;&#25972;&#20102;&#21407;&#22987;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#20351;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#21644;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#23545;&#40784;&#65292;&#21033;&#29992;&#22522;&#20110;&#40857;&#26684;-&#24211;&#22612;&#30340;&#22810;&#27493;&#23545;&#40784;&#33976;&#39311;&#36827;&#34892;&#31934;&#30830;&#30340;ODE&#20272;&#35745;&#65292;&#21516;&#26102;&#38450;&#27490;&#24322;&#27493;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probability Models (DPMs) have made impressive advancements in various machine learning domains. However, achieving high-quality synthetic samples typically involves performing a large number of sampling steps, which impedes the possibility of real-time sample synthesis. Traditional accelerated sampling algorithms via knowledge distillation rely on pre-trained model weights and discrete time step scenarios, necessitating additional training sessions to achieve their goals. To address these issues, we propose the Catch-Up Distillation (CUD), which encourages the current moment output of the velocity estimation model ``catch up'' with its previous moment output. Specifically, CUD adjusts the original Ordinary Differential Equation (ODE) training objective to align the current moment output with both the ground truth label and the previous moment output, utilizing Runge-Kutta-based multi-step alignment distillation for precise ODE estimation while preventing asynchronous updates
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#21457;&#25496;&#26368;&#20339;&#37327;&#23376;&#32416;&#38169;&#30721;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#20195;&#29702;&#20197;&#26368;&#22823;&#21270;&#32534;&#30721;&#36317;&#31163;&#25110;&#26368;&#23567;&#21270;&#22312;&#20559;&#32622;Pauli&#22122;&#22768;&#19979;&#30340;&#36923;&#36753;&#38169;&#35823;&#27010;&#29575;&#12290;&#20316;&#32773;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;CSS&#20195;&#30721;&#30456;&#27604;&#65292;&#20182;&#20204;&#21487;&#20197;&#38480;&#21046;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#24182;&#33719;&#24471;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06378</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21457;&#25496;&#26368;&#20339;&#37327;&#23376;&#32416;&#38169;&#30721;&#30340;&#26041;&#27861;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discovery of Optimal Quantum Error Correcting Codes via Reinforcement Learning. (arXiv:2305.06378v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#21457;&#25496;&#26368;&#20339;&#37327;&#23376;&#32416;&#38169;&#30721;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#20195;&#29702;&#20197;&#26368;&#22823;&#21270;&#32534;&#30721;&#36317;&#31163;&#25110;&#26368;&#23567;&#21270;&#22312;&#20559;&#32622;Pauli&#22122;&#22768;&#19979;&#30340;&#36923;&#36753;&#38169;&#35823;&#27010;&#29575;&#12290;&#20316;&#32773;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;CSS&#20195;&#30721;&#30456;&#27604;&#65292;&#20182;&#20204;&#21487;&#20197;&#38480;&#21046;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#24182;&#33719;&#24471;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#37327;&#23376;&#20048;&#39640;&#26694;&#26550;&#20026;&#21033;&#29992;&#31616;&#21333;&#30340;&#26041;&#27861;&#29983;&#25104;&#22797;&#26434;&#30340;&#37327;&#23376;&#32416;&#38169;&#30721;&#65288;QECC&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#21464;&#25104;&#20102;&#19968;&#20010;&#28216;&#25103;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#38145;&#20102;&#19968;&#31181;&#35774;&#35745;&#21644;&#21457;&#25496;&#32534;&#30721;&#30340;&#26032;&#36884;&#24452;&#12290; RL&#30340;&#19968;&#20010;&#22909;&#22788;&#26159;&#25105;&#20204;&#21487;&#20197;&#25351;&#23450;&#24453;&#20248;&#21270;&#30340;&#32534;&#30721;&#30340;\textit{&#20219;&#24847;}&#23646;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#31181;&#36825;&#26679;&#30340;&#23646;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#22823;&#21270;&#32534;&#30721;&#36317;&#31163;&#21644;&#22312;&#20559;&#32622;Pauli&#22122;&#22768;&#19979;&#26368;&#23567;&#21270;&#36923;&#36753;&#38169;&#35823;&#30340;&#27010;&#29575;&#12290;&#38024;&#23545;&#31532;&#19968;&#20010;&#23646;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35757;&#32451;&#30340;&#20195;&#29702;&#33021;&#22815;&#35782;&#21035;&#22686;&#21152;&#32534;&#30721;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36229;&#36807;&#21407;&#22987;&#30340;&#32423;&#32852;&#26041;&#27861;&#65292;&#22312;13&#20010;&#37327;&#23376;&#27604;&#29305;&#19978;&#39281;&#21644;&#32447;&#24615;&#32534;&#31243;&#30028;&#38480;&#30340;CSS&#32534;&#30721;&#12290;&#23545;&#20110;&#23398;&#20064;&#30446;&#26631;&#26159;&#22312;&#20559;&#32622;Pauli&#22122;&#22768;&#19979;&#26368;&#23567;&#21270;&#36923;&#36753;&#38169;&#35823;&#27010;&#29575;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#26368;&#22909;&#30340;&#24050;&#30693;CSS&#20195;&#30721;&#65292;&#38024;&#23545;$\lesssim 20$&#20010;&#37327;&#23376;&#27604;&#29305;&#12290;&#19982;&#20854;&#20182;&#65288;&#23616;&#37096;&#21464;&#24418;&#30340;&#65289;CSS&#20195;&#30721;&#65292;&#21253;&#25324;Surface&#65292;XZZX&#21644;2D Color codes&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;$[[17,1,3]]$&#32534;&#30721;&#26500;&#36896;&#23454;&#38469;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently introduced Quantum Lego framework provides a powerful method for generating complex quantum error correcting codes (QECCs) out of simple ones. We gamify this process and unlock a new avenue for code design and discovery using reinforcement learning (RL). One benefit of RL is that we can specify \textit{arbitrary} properties of the code to be optimized. We train on two such properties, maximizing the code distance, and minimizing the probability of logical error under biased Pauli noise. For the first, we show that the trained agent identifies ways to increase code distance beyond naive concatenation, saturating the linear programming bound for CSS codes on 13 qubits. With a learning objective to minimize the logical error probability under biased Pauli noise, we find the best known CSS code at this task for $\lesssim 20$ qubits. Compared to other (locally deformed) CSS codes, including Surface, XZZX, and 2D Color codes, our $[[17,1,3]]$ code construction actually has \text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HybridNet&#65292;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.05374</link><description>&lt;p&gt;
HybridNet: &#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
HybridNet: Dual-Branch Fusion of Geometrical and Topological Views for VLSI Congestion Prediction. (arXiv:2305.05374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HybridNet&#65292;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#38459;&#22622;&#39044;&#27979;&#26159;&#24110;&#21161;&#35774;&#35745;&#24072;&#22312;VLSI&#35774;&#35745;&#21608;&#26399;&#20869;&#26356;&#24555;&#36845;&#20195;&#30340;&#37325;&#35201;&#29615;&#33410;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#22270;&#65288;&#20960;&#20309;&#22270;&#12289;&#25299;&#25169;&#22270;&#65289;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#21807;&#19968;&#23646;&#24615;&#37319;&#29992;&#19981;&#21516;&#30340;&#36793;&#32536;&#26500;&#24314;&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#32593;&#32476;&#65292;&#27599;&#20010;&#36335;&#24452;&#20013;&#37117;&#26377;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#23618;&#65292;&#24182;&#36890;&#36807;&#31934;&#32454;&#30340;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#32858;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#21517;&#20026;HybridNet&#65292;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#21333;&#20803;&#20043;&#38388;&#30340;&#20960;&#20309;&#20132;&#20114;&#65292;&#32780;&#19988;&#36824;&#20445;&#30041;&#20102;&#21407;&#22987;&#30005;&#36335;&#25299;&#25169;&#20851;&#31995;&#12290;&#22312;ISPD2015&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate early congestion prediction can prevent unpleasant surprises at the routing stage, playing a crucial character in assisting designers to iterate faster in VLSI design cycles. In this paper, we introduce a novel strategy to fully incorporate topological and geometrical features of circuits by making several key designs in our network architecture. To be more specific, we construct two individual graphs (geometry-graph, topology-graph) with distinct edge construction schemes according to their unique properties. We then propose a dual-branch network with different encoder layers in each pathway and aggregate representations with a sophisticated fusion strategy. Our network, named HybridNet, not only provides a simple yet effective way to capture the geometric interactions of cells, but also preserves the original topological relationships in the netlist. Experimental results on the ISPD2015 benchmarks show that we achieve an improvement of 10.9% compared to previous methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DELTA&#30340;CTR&#27169;&#22411;&#65292;&#20351;&#29992;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#36827;&#34892;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#20013;&#26080;&#25928;&#21644;&#20887;&#20313;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.04891</link><description>&lt;p&gt;
&#24102;&#26377;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DELTA: Dynamic Embedding Learning with Truncated Conscious Attention for CTR Prediction. (arXiv:2305.04891v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DELTA&#30340;CTR&#27169;&#22411;&#65292;&#20351;&#29992;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#36827;&#34892;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#20013;&#26080;&#25928;&#21644;&#20887;&#20313;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#26159;&#20135;&#21697;&#21644;&#20869;&#23481;&#25512;&#33616;&#20013;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#29305;&#24449;&#23884;&#20837;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#23398;&#20064;&#22266;&#23450;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#32780;&#32570;&#20047;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21160;&#24577;&#35843;&#25972;&#29305;&#24449;&#34920;&#31034;&#30340;&#26426;&#21046;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#19968;&#20123;&#36817;&#26399;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#20301;&#26435;&#37325;&#25110;&#22686;&#24378;&#23884;&#20837;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#21463;&#21040;&#19978;&#19979;&#25991;&#20013;&#26080;&#20449;&#24687;&#25110;&#20887;&#20313;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#24847;&#35782;&#21152;&#24037;&#20013;&#20840;&#23616;&#24037;&#20316;&#21306;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#35748;&#20026;&#21482;&#26377;&#29305;&#23450;&#30340;&#20135;&#21697;&#29305;&#24449;&#19982;&#28857;&#20987;&#34892;&#20026;&#30456;&#20851;&#65292;&#20854;&#20313;&#29305;&#24449;&#21487;&#33021;&#20250;&#22122;&#38899;&#24178;&#25200;&#65292;&#29978;&#33267;&#26377;&#23475;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;DELTA&#36827;&#34892;CTR&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) prediction is a pivotal task in product and content recommendation, where learning effective feature embeddings is of great significance. However, traditional methods typically learn fixed feature representations without dynamically refining feature representations according to the context information, leading to suboptimal performance. Some recent approaches attempt to address this issue by learning bit-wise weights or augmented embeddings for feature representations, but suffer from uninformative or redundant features in the context. To tackle this problem, inspired by the Global Workspace Theory in conscious processing, which posits that only a specific subset of the product features are pertinent while the rest can be noisy and even detrimental to human-click behaviors, we propose a CTR model that enables Dynamic Embedding Learning with Truncated Conscious Attention for CTR prediction, termed DELTA. DELTA contains two key components: (I) conscious truncatio
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PubMed&#25968;&#25454;&#24211;&#30340;PGB&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#24322;&#26500;&#22270;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#21644;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;21&#20010;&#31995;&#32479;&#24615;&#35780;&#20215;&#20027;&#39064;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.02691</link><description>&lt;p&gt;
PGB&#65306;&#29992;&#20110;&#24322;&#26500;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#30340;PubMed&#22270;&#25968;&#25454;&#38598;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PGB: A PubMed Graph Benchmark for Heterogeneous Network Representation Learning. (arXiv:2305.02691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02691
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PubMed&#25968;&#25454;&#24211;&#30340;PGB&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#24322;&#26500;&#22270;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#21644;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;21&#20010;&#31995;&#32479;&#24615;&#35780;&#20215;&#20027;&#39064;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#25968;&#37327;&#24555;&#36895;&#22686;&#38271;&#65292;&#20294;&#26159;&#25429;&#25417;&#36825;&#20123;&#25991;&#31456;&#30340;&#25991;&#29486;&#20449;&#24687;&#30340;&#24322;&#36136;&#24615;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#23613;&#31649;&#36890;&#36807;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#25366;&#25496;&#30740;&#31350;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#25429;&#25417;&#21040;&#20102;PubMed&#25968;&#25454;&#24211;&#30340;&#24322;&#36136;&#24615;&#20173;&#19981;&#28165;&#26970;&#65292;&#32780;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;3300&#19975;&#31687;&#25991;&#31456;&#30340;&#24222;&#22823;&#25968;&#23383;&#36164;&#26009;&#24211;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PubMed Graph Benchmark&#65288;PGB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#24322;&#26500;&#22270;&#23884;&#20837;&#30340;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;PGB&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#24322;&#26500;&#32593;&#32476;&#20043;&#19968;&#65292;&#21253;&#21547;3000&#19975;&#31687;&#33521;&#25991;&#25991;&#31456;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#21253;&#21547;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#20316;&#32773;&#12289;&#24341;&#29992;&#12289;MeSH&#26415;&#35821;&#12289;MeSH&#23618;&#27425;&#32467;&#26500;&#21644;&#20854;&#20182;&#19968;&#20123;&#20449;&#24687;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;3&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;21&#20010;&#31995;&#32479;&#24615;&#35780;&#20215;&#20027;&#39064;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;&#22312;PGB&#20013;&#65292;&#25105;&#20204;&#23558;&#19982;PubMed&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#30456;&#20851;&#30340;&#20803;&#25968;&#25454;&#32858;&#21512;&#25104;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a rapid growth in biomedical literature, yet capturing the heterogeneity of the bibliographic information of these articles remains relatively understudied. Although graph mining research via heterogeneous graph neural networks has taken center stage, it remains unclear whether these approaches capture the heterogeneity of the PubMed database, a vast digital repository containing over 33 million articles. We introduce PubMed Graph Benchmark (PGB), a new benchmark dataset for evaluating heterogeneous graph embeddings for biomedical literature. PGB is one of the largest heterogeneous networks to date and consists of 30 million English articles. The benchmark contains rich metadata including abstract, authors, citations, MeSH terms, MeSH hierarchy, and some other information. The benchmark contains an evaluation task of 21 systematic reviews topics from 3 different datasets. In PGB, we aggregate the metadata associated with the biomedical articles from PubMed into a unified
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65288;FormNetV2&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02549</link><description>&lt;p&gt;
FormNetV2&#65306;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (arXiv:2305.02549v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65288;FormNetV2&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#34920;&#26684;&#25991;&#26723;&#29702;&#35299;&#20013;&#30340;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#23637;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21040;&#20854;&#20182;&#27169;&#24577;&#30340;&#26041;&#27861;&#38656;&#35201;&#20180;&#32454;&#30340;&#22810;&#20219;&#21153;&#35843;&#25972;&#12289;&#22797;&#26434;&#30340;&#37325;&#26500;&#30446;&#26631;&#35774;&#35745;&#25110;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;FormNetV2&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#20013;&#30340;&#22810;&#27169;&#24577;&#22270;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#12290;&#22270;&#23545;&#27604;&#30446;&#26631;&#26368;&#22823;&#21270;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#19968;&#33268;&#24615;&#65292;&#20026;&#25152;&#26377;&#27169;&#24577;&#25552;&#20379;&#33258;&#28982;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32780;&#19981;&#38656;&#35201;&#29305;&#27530;&#30340;&#23450;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#36830;&#25509;&#22270;&#36793;&#32536;&#30340;&#19968;&#23545;&#26631;&#35760;&#30340;&#36793;&#26694;&#20869;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#65292;&#25429;&#25417;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#32780;&#26080;&#38656;&#21152;&#36733;&#32463;&#36807;&#22797;&#26434;&#21644;&#21333;&#29420;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#23884;&#20837;&#22120;&#12290;FormNetV2&#22312;FUNSD&#12289;CORD&#12289;SROIE&#21644;Payment&#22522;&#20934;&#27979;&#35797;&#20013;&#30830;&#31435;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss. The graph contrastive objective maximizes the agreement of multimodal representations, providing a natural interplay for all modalities without special customization. In addition, we extract image features within the bounding box that joins a pair of tokens connected by a graph edge, capturing more targeted visual cues without loading a sophisticated and separately pre-trained image embedder. FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE and Payment benchmarks with 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#35752;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#65292;&#21253;&#25324;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#21450;&#23545;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#30340;&#25903;&#25345;&#24230;&#21644;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.02231</link><description>&lt;p&gt;
&#26500;&#24314;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#65306;&#20174;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#12289;&#20262;&#29702;&#21644;&#20027;&#35201;&#38656;&#27714;&#21040;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#30417;&#31649;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation. (arXiv:2305.02231v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02231
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#35752;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#65292;&#21253;&#25324;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#21450;&#23545;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#30340;&#25903;&#25345;&#24230;&#21644;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#22522;&#20110;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#65292;&#20998;&#21035;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#21253;&#25324;&#31995;&#32479;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#26377;&#21442;&#19982;&#27969;&#31243;&#21644;&#21442;&#19982;&#32773;&#21487;&#20449;&#24615;&#30340;&#32771;&#37327;&#12290;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#24895;&#26223;&#23558;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#19979;&#35201;&#20214;&#30340;&#25903;&#25345;&#24230;&#20197;&#21450;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;&#35780;&#20272;&#19971;&#20010;&#38656;&#27714;&#20043;&#25216;&#26415;&#26041;&#38754;&#12289;&#20262;&#29702;&#26041;&#38754;&#21644;&#30417;&#31649;&#25361;&#25112;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#22522;&#20110;&#29289;&#29702;&#31995;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.11042</link><description>&lt;p&gt;
&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#28145;&#24230;&#29289;&#29702;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Backpropagation-free Training of Deep Physical Neural Networks. (arXiv:2304.11042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#22522;&#20110;&#29289;&#29702;&#31995;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#35832;&#22914;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#25104;&#21151;&#12290;&#36825;&#19968;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#65292;&#39044;&#35745;&#20250;&#19981;&#26029;&#22686;&#21152;&#12290;&#36825;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22686;&#38271;&#20276;&#38543;&#30528;&#19982;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#35757;&#32451;&#12289;&#25512;&#29702;&#38454;&#27573;&#20013;&#30340;&#33021;&#32791;&#31561;&#38382;&#39064;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#38750;&#20256;&#32479;&#29289;&#29702;&#31995;&#32479;&#30340;&#24037;&#20316;&#26469;&#35299;&#20915;&#25512;&#29702;&#38454;&#27573;&#30340;&#33021;&#25928;&#38382;&#39064;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#35757;&#32451;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#25968;&#23383;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#20027;&#35201;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#29289;&#29702;&#23454;&#29616;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23436;&#20840;&#20102;&#35299;&#25152;&#35859;&#21069;&#21521;&#20256;&#36882;&#30340;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the outstanding success of deep learning in various fields such as vision and natural language processing. This success is largely indebted to the massive size of deep learning models that is expected to increase unceasingly. This growth of the deep learning models is accompanied by issues related to their considerable energy consumption, both during the training and inference phases, as well as their scalability. Although a number of work based on unconventional physical systems have been proposed which addresses the issue of energy efficiency in the inference phase, efficient training of deep learning models has remained unaddressed. So far, training of digital deep learning models mainly relies on backpropagation, which is not suitable for physical implementation as it requires perfect knowledge of the computation performed in the so-called forward pass of the neural network. Here, we tackle this issue by proposing a simple deep neural network architectur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; C-qGAN &#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#37327;&#23376;&#30005;&#36335;&#32467;&#26500;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#65292;&#21487;&#20197;&#21033;&#29992;&#35813;&#26041;&#27861;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#23450;&#20215;&#30340;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.10382</link><description>&lt;p&gt;
&#23398;&#20064;&#38543;&#26426;&#36807;&#31243;&#30340;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Models for Learning Stochastic Processes. (arXiv:2304.10382v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; C-qGAN &#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#37327;&#23376;&#30005;&#36335;&#32467;&#26500;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#65292;&#21487;&#20197;&#21033;&#29992;&#35813;&#26041;&#27861;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#23450;&#20215;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26465;&#20214;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;C-qGAN&#65289;&#12290;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20005;&#26684;&#37319;&#29992;&#37327;&#23376;&#30005;&#36335;&#65292;&#22240;&#27492;&#34987;&#35777;&#26126;&#33021;&#22815;&#27604;&#24403;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#28508;&#21147;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#23637;&#31034;&#20102;&#32593;&#32476;&#22312;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21518;&#65292;&#23558;&#35813;&#25216;&#26415;&#24212;&#29992;&#20110;&#23450;&#20215;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#20854;&#20182;&#36335;&#24452;&#30456;&#20851;&#26399;&#26435;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
A framework to learn a multi-modal distribution is proposed, denoted as the Conditional Quantum Generative Adversarial Network (C-qGAN). The neural network structure is strictly within a quantum circuit and, as a consequence, is shown to represents a more efficient state preparation procedure than current methods. This methodology has the potential to speed-up algorithms, such as Monte Carlo analysis. In particular, after demonstrating the effectiveness of the network in the learning task, the technique is applied to price Asian option derivatives, providing the foundation for further research on other path-dependent options.
&lt;/p&gt;</description></item><item><title>Co-ML&#26159;&#19968;&#20010;&#22522;&#20110;&#24179;&#26495;&#30005;&#33041;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#21327;&#21516;&#26500;&#24314;ML&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#22312;&#21512;&#20316;&#20013;&#21457;&#25496;&#26032;&#30340;&#24819;&#27861;&#21644;&#26041;&#27861;&#65292;&#35299;&#20915;&#25968;&#25454;&#34920;&#29616;&#21644;&#22810;&#26679;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.05444</link><description>&lt;p&gt;
&#20351;&#29992;Co-ML&#21327;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#23478;&#24237;&#30340;&#21512;&#20316;&#27169;&#22411;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Collaborative Machine Learning Model Building with Families Using Co-ML. (arXiv:2304.05444v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05444
&lt;/p&gt;
&lt;p&gt;
Co-ML&#26159;&#19968;&#20010;&#22522;&#20110;&#24179;&#26495;&#30005;&#33041;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#21327;&#21516;&#26500;&#24314;ML&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#22312;&#21512;&#20316;&#20013;&#21457;&#25496;&#26032;&#30340;&#24819;&#27861;&#21644;&#26041;&#27861;&#65292;&#35299;&#20915;&#25968;&#25454;&#34920;&#29616;&#21644;&#22810;&#26679;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38024;&#23545;&#26032;&#25163;&#21451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24314;&#27169;&#24037;&#20855;&#65292;&#20391;&#37325;&#20110;&#21333;&#19968;&#29992;&#25143;&#20307;&#39564;&#65292;&#19968;&#20010;&#21333;&#19968;&#29992;&#25143;&#20165;&#25910;&#38598;&#33258;&#24049;&#30340;&#25968;&#25454;&#26469;&#26500;&#24314;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21333;&#29420;&#24314;&#27169;&#32463;&#21382;&#38480;&#21046;&#20102;&#23398;&#20064;&#32773;&#20849;&#21516;&#24037;&#20316;&#26102;&#20250;&#36935;&#21040;&#30340;&#20132;&#26367;&#24819;&#27861;&#21644;&#26041;&#27861;&#30340;&#23453;&#36149;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#24403;&#19981;&#21516;&#30340;&#35266;&#28857;&#20307;&#29616;&#22312;&#32676;&#20307;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#20013;&#26102;&#65292;&#24448;&#24448;&#25490;&#38500;&#20102;ML&#22260;&#32469;&#25968;&#25454;&#34920;&#29616;&#21644;&#22810;&#26679;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Co-ML&#8212;&#8212;&#19968;&#20010;&#38754;&#21521;&#23398;&#20064;&#32773;&#30340;&#22522;&#20110;&#24179;&#26495;&#30005;&#33041;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#31471;&#23545;&#31471;&#30340;&#36845;&#20195;&#27169;&#22411;&#26500;&#24314;&#27969;&#31243;&#65292;&#21327;&#21516;&#26500;&#24314;ML&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20171;&#32461;&#19968;&#20010;&#23478;&#24237;&#65288;&#30001;&#20004;&#20010;11&#21644;14&#23681;&#30340;&#23401;&#23376;&#19982;&#29238;&#27597;&#19968;&#36215;&#24037;&#20316;&#65289;&#22312;&#23478;&#20013;&#20351;&#29992;Co-ML&#36827;&#34892;&#24341;&#23548;&#24615;&#20171;&#32461;ML&#27963;&#21160;&#30340;&#28145;&#20837;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21327;&#20316;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#22312;&#20016;&#23500;&#24615;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;Co-ML&#31995;&#32479;&#30340;d&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing novice-friendly machine learning (ML) modeling tools center around a solo user experience, where a single user collects only their own data to build a model. However, solo modeling experiences limit valuable opportunities for encountering alternative ideas and approaches that can arise when learners work together; consequently, it often precludes encountering critical issues in ML around data representation and diversity that can surface when different perspectives are manifested in a group-constructed data set. To address this issue, we created Co-ML -- a tablet-based app for learners to collaboratively build ML image classifiers through an end-to-end, iterative model-building process. In this paper, we illustrate the feasibility and potential richness of collaborative modeling by presenting an in-depth case study of a family (two children 11 and 14-years-old working with their parents) using Co-ML in a facilitated introductory ML activity at home. We share the Co-ML system d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03279</link><description>&lt;p&gt;
&#22870;&#21169;&#26159;&#21542;&#21512;&#29702;&#65311;&#22312; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#20013;&#34913;&#37327;&#22870;&#21169;&#19982;&#36947;&#24503;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#36861;&#27714;&#26435;&#21147;&#21644;&#27450;&#39575;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#21487;&#33021;&#20250;&#28608;&#21169;&#26377;&#23475;&#34892;&#20026;&#12290;&#37027;&#20040;&#20195;&#29702;&#26159;&#21542;&#33258;&#28982;&#32780;&#28982;&#22320;&#23398;&#20250;&#20102;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65311;&#25105;&#20204;&#22914;&#20309;&#22312; GPT-4 &#31561;&#36890;&#29992;&#27169;&#22411;&#20013;&#34913;&#37327;&#36825;&#20123;&#34892;&#20026;&#21602;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20250;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#12290;&#25105;&#20204;&#25968;&#23398;&#21270;&#20102;&#25968;&#21313;&#31181;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#20195;&#29702;&#20542;&#21521;&#20110;&#36861;&#27714;&#26435;&#21147;&#65292;&#36896;&#25104;&#21151;&#33021;&#19981;&#33391;&#21644;&#36829;&#21453;&#20262;&#29702;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#20195;&#29702;&#36235;&#21521;&#20110;&#37319;&#21462;&#26356;&#23569;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MACHIAVELLI &#26159;&#35780;&#20272;&#20154;&#24037;&#20195;&#29702;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#27700;&#24179;&#30340;&#26377;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24212;&#29992;&#22522;&#20110;&#19981;&#21464;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20108;&#32500;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#65292;MARL&#25152;&#33719;&#24471;&#30340;&#25511;&#21046;&#27861;&#21017;&#19981;&#20165;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.02370</link><description>&lt;p&gt;
&#20108;&#32500;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#30340;&#26377;&#25928;&#25511;&#21046;&#65306;&#20165;&#38656;&#19981;&#21464;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective control of two-dimensional Rayleigh--B\'enard convection: invariant multi-agent reinforcement learning is all you need. (arXiv:2304.02370v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24212;&#29992;&#22522;&#20110;&#19981;&#21464;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20108;&#32500;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#65292;MARL&#25152;&#33719;&#24471;&#30340;&#25511;&#21046;&#27861;&#21017;&#19981;&#20165;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#26159;&#20960;&#31181;&#24037;&#19994;&#21644;&#22320;&#29699;&#31185;&#23398;&#27969;&#21160;&#20013;&#30340;&#19968;&#20010;&#24120;&#35265;&#29616;&#35937;&#65292;&#20174;&#22522;&#26412;&#27969;&#20307;&#21147;&#23398;&#30340;&#35282;&#24230;&#20063;&#26159;&#19968;&#20010;&#30740;&#31350;&#20805;&#20998;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20256;&#32479;&#30340;&#25511;&#21046;&#29702;&#35770;&#26041;&#27861;&#25511;&#21046;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#65292;&#20363;&#22914;&#36890;&#36807;&#35843;&#33410;&#35268;&#33539;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#24418;&#24577;&#19979;&#24213;&#26495;&#21152;&#28909;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26469;&#25511;&#21046;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#20869;&#22312;&#20110;&#23485;&#36890;&#36947;&#20869;&#30340;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#30340;&#23616;&#37096;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#19981;&#21464;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26469;&#33719;&#21462;&#26377;&#25928;&#30340;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#25511;&#21046;&#12290;&#24212;&#29992;&#20110;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#30340;MARL&#26694;&#26550;&#20801;&#35768;&#22686;&#21152;&#25511;&#21046;&#27573;&#30340;&#25968;&#37327;&#65292;&#32780;&#19981;&#20250;&#36935;&#21040;&#22240;naive DRL&#21160;&#20316;&#23610;&#23544;&#32500;&#25968;&#22686;&#21152;&#32780;&#23548;&#33268;&#30340;&#32500;&#24230;&#28798;&#38590;&#12290;&#36825;&#24471;&#30410;&#20110;MARL&#33021;&#22815;&#37325;&#22797;&#20351;&#29992;&#22312;&#19981;&#21516;&#21306;&#22495;&#29983;&#25104;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#24418;&#25104;&#26377;&#25928;&#25910;&#25947;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;MARL&#26041;&#27861;&#22312;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#25511;&#21046;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#22312;&#24037;&#19994;&#21644;&#22320;&#29699;&#31185;&#23398;&#27969;&#21160;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rayleigh-B\'enard convection (RBC) is a recurrent phenomenon in several industrial and geoscience flows and a well-studied system from a fundamental fluid-mechanics viewpoint. However, controlling RBC, for example by modulating the spatial distribution of the bottom-plate heating in the canonical RBC configuration, remains a challenging topic for classical control-theory methods. In the present work, we apply deep reinforcement learning (DRL) for controlling RBC. We show that effective RBC control can be obtained by leveraging invariant multi-agent reinforcement learning (MARL), which takes advantage of the locality and translational invariance inherent to RBC flows inside wide channels. The MARL framework applied to RBC allows for an increase in the number of control segments without encountering the curse of dimensionality that would result from a naive increase in the DRL action-size dimension. This is made possible by the MARL ability for re-using the knowledge generated in differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#30149;&#29702;&#22270;&#20687;&#39044;&#35757;&#32451;&#23545;&#23567;&#35268;&#27169;&#30149;&#29702;&#22522;&#20934;&#24494;&#35843;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;PTCGA200&#20026;&#35757;&#32451;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;&#24494;&#35843;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;imagenet2012&#39044;&#35757;&#32451;&#12290;MoCov2&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;PCam200&#21644;segPANDA200&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2303.15693</link><description>&lt;p&gt;
&#30149;&#29702;&#22270;&#20687;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#29992;&#20110;&#23567;&#35268;&#27169;&#30149;&#29702;&#22522;&#20934;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Large-scale pretraining on pathological images for fine-tuning of small pathological benchmarks. (arXiv:2303.15693v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#30149;&#29702;&#22270;&#20687;&#39044;&#35757;&#32451;&#23545;&#23567;&#35268;&#27169;&#30149;&#29702;&#22522;&#20934;&#24494;&#35843;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;PTCGA200&#20026;&#35757;&#32451;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;&#24494;&#35843;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;imagenet2012&#39044;&#35757;&#32451;&#12290;MoCov2&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;PCam200&#21644;segPANDA200&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#23567;&#22411;&#30446;&#26631;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#26631;&#20934;&#27493;&#39588;&#12290;&#22823;&#22411;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#36890;&#29992;&#22270;&#20687;&#65288;&#20363;&#22914;imagenet2012&#65289;&#65292;&#32780;&#23567;&#22411;&#25968;&#25454;&#38598;&#21487;&#20197;&#26159;&#20855;&#26377;&#19982;&#22823;&#22411;&#25968;&#25454;&#38598;&#19981;&#21516;&#20998;&#24067;&#30340;&#19987;&#19994;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#24403;&#22823;&#22411;&#25968;&#25454;&#38598;&#26159;&#19987;&#19994;&#21270;&#30340;&#19988;&#20855;&#26377;&#19982;&#23567;&#22411;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#20998;&#24067;&#26102;&#65292;&#36825;&#31181;&#8220;&#22823;&#21040;&#23567;&#8221;&#30340;&#31574;&#30053;&#24182;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#26032;&#32534;&#35793;&#20102;&#19977;&#20010;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;PTCGA200&#65289;&#21644;&#20004;&#20010;&#25918;&#22823;&#35843;&#25972;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#65288;PCam200&#21644;segPANDA200&#65289;&#12290;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#20102;&#20027;&#35201;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32959;&#30244;&#20998;&#31867;&#21644;&#32452;&#32455;&#20998;&#21106;&#22522;&#20934;&#30340;&#24494;&#35843;&#12290;&#22312;PTCGA200&#19978;&#20197;MoCov2&#65292;SimCLR&#21644;BYOL&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;&#24494;&#35843;&#26102;&#27604;imagenet2012&#39044;&#35757;&#32451;&#26356;&#22909;&#65288;&#31934;&#24230;&#20998;&#21035;&#20026;83.94&#65285;&#65292;86.41&#65285;&#65292;84.91&#65285;&#21644;82.72&#65285;&#65289;&#12290;&#27492;&#22806;&#65292;&#20197;MoCov2&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;PCam200&#21644;segPANDA200&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#32780;&#27604;imagenet2012&#39044;&#35757;&#32451;&#30340;ResNet50&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining a deep learning model on large image datasets is a standard step before fine-tuning the model on small targeted datasets. The large dataset is usually general images (e.g. imagenet2012) while the small dataset can be specialized datasets that have different distributions from the large dataset. However, this 'large-to-small' strategy is not well-validated when the large dataset is specialized and has a similar distribution to small datasets. We newly compiled three hematoxylin and eosin-stained image datasets, one large (PTCGA200) and two magnification-adjusted small datasets (PCam200 and segPANDA200). Major deep learning models were trained with supervised and self-supervised learning methods and fine-tuned on the small datasets for tumor classification and tissue segmentation benchmarks. ResNet50 pretrained with MoCov2, SimCLR, and BYOL on PTCGA200 was better than imagenet2012 pretraining when fine-tuned on PTCGA200 (accuracy of 83.94%, 86.41%, 84.91%, and 82.72%, respect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;</title><link>http://arxiv.org/abs/2303.12814</link><description>&lt;p&gt;
&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21160;&#28857;
&lt;/p&gt;
&lt;p&gt;
Fixed points of arbitrarily deep 1-dimensional neural networks. (arXiv:2303.12814v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;$\mathbb{R}$&#19978;&#20855;&#26377;&#21512;&#25104;&#24615;&#19988;&#21253;&#21547;&#23545;&#25968;S&#22411;&#20989;&#25968;&#30340;&#26032;&#20989;&#25968;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#31867;&#26469;&#35777;&#26126;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#12290;&#34429;&#28982;&#36825;&#26679;&#30340;&#31070;&#32463;&#32593;&#32476;&#36828;&#31163;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#25105;&#20204;&#33021;&#22815;&#23436;&#20840;&#29702;&#35299;&#23427;&#20204;&#30340;&#19981;&#21160;&#28857;&#65292;&#24182;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new class of functions on $\mathbb{R}$ that is closed under composition, and contains the logistic sigmoid function. We use this class to show that any 1-dimensional neural network of arbitrary depth with logistic sigmoid activation functions has at most three fixed points. While such neural networks are far from real world applications, we are able to completely understand their fixed points, providing a foundation to the much needed connection between application and theory of deep neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11403</link><description>&lt;p&gt;
eP-ALM:&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24863;&#30693;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36804;&#20170;&#20026;&#27490;&#32473;&#19990;&#30028;&#30041;&#19979;&#20102;&#28145;&#21051;&#21360;&#35937;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#27169;&#22411;&#25152;&#20855;&#26377;&#30340;&#38750;&#21516;&#23547;&#24120;&#30340;&#33021;&#21147;&#12290;&#22312;&#35270;&#35273;&#26041;&#38754;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;&#21363;ViT&#65289;&#20063;&#22312;&#36861;&#38543;&#21516;&#19968;&#36235;&#21183;&#65292;&#21462;&#24471;&#20102;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#38543;&#30528;&#36825;&#31181;&#21333;&#27169;&#22411;&#30340;&#20016;&#23500;&#22810;&#26679;&#65292;&#33258;&#28982;&#20250;&#24341;&#21457;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#38656;&#35201;&#36319;&#38543;&#36825;&#20010;&#36235;&#21183;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21162;&#21147;&#38598;&#20013;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#24182;&#25552;&#20986;&#29992;&#24863;&#30693;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#20182;&#20204;&#20173;&#28982;&#35757;&#32451;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#20381;&#36182;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#22312;&#24040;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#24182;&#28155;&#21152;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#22823;&#22810;&#25968;&#20851;&#27880;Zero-Shot&#21644;In Context Learning&#65292;&#35266;&#23519;&#21040;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;eP-ALM&#65292;&#19968;&#31181;&#23558;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#20855;&#26377;&#26497;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26032;&#30340;&#39044;&#35757;&#32451;&#65292;&#20173;&#28982;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#20687;&#20998;&#31867;&#22120;&#23384;&#22312;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;Class Attribute Inference Attack&#65288;Caia&#65289;&#33021;&#22815;&#20174;&#40657;&#30418;&#35774;&#32622;&#20013;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#25935;&#24863;&#23646;&#24615;&#65292;&#21253;&#25324;&#20010;&#20154;&#30340;&#21457;&#33394;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#65292;&#36825;&#34920;&#26126;&#22312;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.09289</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#27844;&#38706;&#20854;&#31867;&#21035;&#30340;&#25935;&#24863;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Image Classifiers Leak Sensitive Attributes About Their Classes. (arXiv:2303.09289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#20687;&#20998;&#31867;&#22120;&#23384;&#22312;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;Class Attribute Inference Attack&#65288;Caia&#65289;&#33021;&#22815;&#20174;&#40657;&#30418;&#35774;&#32622;&#20013;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#25935;&#24863;&#23646;&#24615;&#65292;&#21253;&#25324;&#20010;&#20154;&#30340;&#21457;&#33394;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#65292;&#36825;&#34920;&#26126;&#22312;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#26080;&#24847;&#20013;&#36879;&#38706;&#20102;&#26377;&#20851;&#20854;&#31867;&#21035;&#30340;&#25935;&#24863;&#23646;&#24615;&#20449;&#24687;&#65292;&#24341;&#36215;&#20102;&#23545;&#23427;&#20204;&#30340;&#38544;&#31169;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#38544;&#31169;&#27844;&#28431;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;Class Attribute Inference Attack&#65288;Caia&#65289;&#65292;&#21033;&#29992;&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#25512;&#26029;&#20986;&#21333;&#20010;&#31867;&#21035;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#21516;&#26102;&#19982;&#30456;&#20851;&#30340;&#30333;&#30418;&#25915;&#20987;&#30456;&#31454;&#20105;&#12290;&#22312;&#20154;&#33080;&#35782;&#21035;&#39046;&#22495;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;Caia&#33021;&#22815;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#26410;&#20844;&#24320;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#20363;&#22914;&#20010;&#20154;&#30340;&#21457;&#33394;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#22806;&#35980;&#65292;&#36825;&#20123;&#23646;&#24615;&#19981;&#23646;&#20110;&#35757;&#32451;&#26631;&#31614;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#27169;&#22411;&#27604;&#26631;&#20934;&#27169;&#22411;&#26356;&#23481;&#26131;&#27844;&#38706;&#38544;&#31169;&#65292;&#34920;&#26126;&#22312;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network-based image classifiers are powerful tools for computer vision tasks, but they inadvertently reveal sensitive attribute information about their classes, raising concerns about their privacy. To investigate this privacy leakage, we introduce the first Class Attribute Inference Attack (Caia), which leverages recent advances in text-to-image synthesis to infer sensitive attributes of individual classes in a black-box setting, while remaining competitive with related white-box attacks. Our extensive experiments in the face recognition domain show that Caia can accurately infer undisclosed sensitive attributes, such as an individual's hair color, gender and racial appearance, which are not part of the training labels. Interestingly, we demonstrate that adversarial robust models are even more vulnerable to such privacy leakage than standard models, indicating that a trade-off between robustness and privacy exists.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAP-IQ&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20219;&#24847;&#38454;Shapley&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#20102;&#36924;&#36817;&#36136;&#37327;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#26041;&#24046;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.01179</link><description>&lt;p&gt;
SHAP-IQ: &#20219;&#24847;&#38454;Shapley interaction&#30340;&#32479;&#19968;&#36924;&#36817;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SHAP-IQ: Unified Approximation of any-order Shapley Interactions. (arXiv:2303.01179v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01179
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAP-IQ&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20219;&#24847;&#38454;Shapley&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#20102;&#36924;&#36817;&#36136;&#37327;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#26041;&#24046;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30740;&#31350;&#20013;&#65292;Shapley&#20540;&#65288;SV&#65289;&#36890;&#24120;&#34987;&#24212;&#29992;&#20110;&#30830;&#23450;&#20219;&#20309;&#40657;&#30418;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290; Shapley interaction indices&#23558;SV&#25193;&#23637;&#20026;&#23450;&#20041;&#20219;&#24847;&#38454;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#24471;&#20998;&#12290;&#23450;&#20041;&#29420;&#29305;&#30340;Shapley interaction index&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#65292;&#36804;&#20170;&#20026;&#27490;&#24050;&#32463;&#25552;&#20986;&#20102;&#19977;&#20010;&#23450;&#20041;&#65292;&#20854;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#25152;&#36873;&#25321;&#30340;&#20844;&#29702;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#23450;&#20041;&#37117;&#38656;&#35201;&#29305;&#23450;&#30340;&#36924;&#36817;&#25216;&#26415;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#37319;&#26679;&#30340;&#26377;&#25928;&#36924;&#36817;&#26041;&#27861;SHAPley Interaction Quantification&#65288;SHAP-IQ&#65289;&#65292;&#20197;&#35745;&#31639;&#20219;&#24847;&#22522;&#25968;&#20132;&#20114;&#25351;&#25968;&#65288;CII&#65289;&#30340;Shapley&#20114;&#21160;&#12290;&#21363;&#28385;&#36275;&#32447;&#24615;&#12289;&#23545;&#31216;&#21644;&#34394;&#25311;&#20844;&#29702;&#30340;&#20132;&#20114;&#25351;&#25968;&#12290;SHAP-IQ&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#20026;&#20854;&#36924;&#36817;&#36136;&#37327;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#21450;&#28857;&#20272;&#35745;&#30340;&#26041;&#24046;&#20272;&#35745;&#12290;&#23545;&#20110;SV&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#36924;&#36817;&#26041;&#27861;&#19982;&#31934;&#30830;&#35745;&#31639;&#19968;&#33268;&#12290;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;SHAP-IQ&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature importance scores for any black box model. Shapley interaction indices extend the SV to define any-order feature interaction scores. Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms. Moreover, each definition requires a specific approximation technique. Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e. interaction indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates. For the special case of SV, our app
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30528;&#20241;&#30496;&#31070;&#32463;&#20803;&#29616;&#35937;&#65292;&#21363;&#26410;&#28608;&#27963;&#31070;&#32463;&#20803;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#20250;&#24433;&#21709;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;(ReDo)&#22238;&#25910;&#20241;&#30496;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#20013;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12902</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20241;&#30496;&#31070;&#32463;&#20803;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
The Dormant Neuron Phenomenon in Deep Reinforcement Learning. (arXiv:2302.12902v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12902
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30528;&#20241;&#30496;&#31070;&#32463;&#20803;&#29616;&#35937;&#65292;&#21363;&#26410;&#28608;&#27963;&#31070;&#32463;&#20803;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#20250;&#24433;&#21709;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;(ReDo)&#22238;&#25910;&#20241;&#30496;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#20013;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20241;&#30496;&#31070;&#32463;&#20803;&#29616;&#35937;&#65292;&#21363;&#20195;&#29702;&#32593;&#32476;&#20013;&#19981;&#26029;&#22686;&#21152;&#30340;&#26410;&#28608;&#27963;&#31070;&#32463;&#20803;&#25968;&#37327;&#20250;&#24433;&#21709;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#31639;&#27861;&#21644;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#23384;&#22312;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;(ReDo)&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22238;&#25910;&#20241;&#30496;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ReDo&#36890;&#36807;&#20943;&#23569;&#20241;&#30496;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#65292;&#20445;&#25345;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we identify the dormant neuron phenomenon in deep reinforcement learning, where an agent's network suffers from an increasing number of inactive neurons, thereby affecting network expressivity. We demonstrate the presence of this phenomenon across a variety of algorithms and environments, and highlight its effect on learning. To address this issue, we propose a simple and effective method (ReDo) that Recycles Dormant neurons throughout training. Our experiments demonstrate that ReDo maintains the expressive power of networks by reducing the number of dormant neurons and results in improved performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29616;&#23454;&#19990;&#30028;&#30340;&#27835;&#30103;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20445;&#23432;Q&#23398;&#20064;&#27861;(CQL)&#21644;&#36716;&#31227;&#21462;&#26679;&#23558;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#22238;&#39038;&#24615;&#21307;&#30103;&#35760;&#24405;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2302.07549</link><description>&lt;p&gt;
&#38754;&#21521;&#29616;&#23454;&#19990;&#30028;&#30340;&#27835;&#30103;&#20248;&#21270;&#24212;&#29992;&#30340;&#28145;&#24230;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Offline Reinforcement Learning for Real-world Treatment Optimization Applications. (arXiv:2302.07549v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#29616;&#23454;&#19990;&#30028;&#30340;&#27835;&#30103;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20445;&#23432;Q&#23398;&#20064;&#27861;(CQL)&#21644;&#36716;&#31227;&#21462;&#26679;&#23558;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#22238;&#39038;&#24615;&#21307;&#30103;&#35760;&#24405;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#20197;&#25512;&#33616;&#27835;&#30103;&#31574;&#30053;&#20026;&#24930;&#24615;&#30149;&#31649;&#29702;&#21644;&#21361;&#37325;&#25252;&#29702;&#24212;&#29992;&#12290;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#38750;&#24120;&#36866;&#21512;&#36825;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#20294;&#24517;&#39035;&#20165;&#22312;&#22238;&#39038;&#24615;&#21307;&#30103;&#35760;&#24405;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#22240;&#20026;&#30452;&#25509;&#30340;&#22312;&#32447;&#25506;&#32034;&#26159;&#19981;&#23433;&#20840;&#21644;&#19981;&#21487;&#34892;&#30340;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20010;&#35201;&#27714;&#65292;&#20294;&#22823;&#22810;&#25968;&#27835;&#30103;&#20248;&#21270;&#30740;&#31350;&#20351;&#29992;&#31163;&#32447;RL&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#21452;&#28145;&#24230;Q&#32593;&#32476;&#65288;DDQN&#65289;&#25110;&#20854;&#21464;&#20307;&#65289;&#65292;&#36825;&#20123;&#26041;&#27861;&#24050;&#30693;&#22312;&#32431;&#31163;&#32447;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36817;&#26399;&#31163;&#32447;RL&#30340;&#36827;&#23637;&#65292;&#20363;&#22914;&#20445;&#23432;Q &#23398;&#20064;&#65288;CQL&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21512;&#36866;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20294;&#22312;&#36866;&#24212;&#36825;&#20123;&#26041;&#27861;&#21040;&#20020;&#24202;&#24212;&#29992;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#22238;&#39038;&#24615;&#25968;&#25454;&#38598;&#20027;&#35201;&#26159;&#27425;&#20248;&#20363;&#23376;&#65292;&#38656;&#35201;&#28385;&#36275;&#20005;&#26684;&#30340;&#23433;&#20840;&#32422;&#26463;&#26465;&#20214;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#30340;&#36716;&#31227;&#21462;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;CQL&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30142;&#30149;&#31649;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing interest in data-driven approaches for recommending optimal treatment strategies in many chronic disease management and critical care applications. Reinforcement learning methods are well-suited to this sequential decision-making problem, but must be trained and evaluated exclusively on retrospective medical record datasets as direct online exploration is unsafe and infeasible. Despite this requirement, the vast majority of treatment optimization studies use off-policy RL methods (e.g., Double Deep Q Networks (DDQN) or its variants) that are known to perform poorly in purely offline settings. Recent advances in offline RL, such as Conservative Q-Learning (CQL), offer a suitable alternative. But there remain challenges in adapting these approaches to real-world applications where suboptimal examples dominate the retrospective dataset and strict safety constraints need to be satisfied. In this work, we introduce a practical and theoretically grounded transition sampli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#22120;&#30340;&#38598;&#20013;&#30028;&#38480;&#65292;&#35752;&#35770;&#20102;&#22312;KL&#25955;&#24230;&#20013;&#31163;&#25955;&#20998;&#24067;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.06869</link><description>&lt;p&gt;
KL&#25955;&#24230;&#20013;&#31163;&#25955;&#20998;&#24067;&#20272;&#35745;&#30340;&#38598;&#20013;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Concentration Bounds for Discrete Distribution Estimation in KL Divergence. (arXiv:2302.06869v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#22120;&#30340;&#38598;&#20013;&#30028;&#38480;&#65292;&#35752;&#35770;&#20102;&#22312;KL&#25955;&#24230;&#20013;&#31163;&#25955;&#20998;&#24067;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;KL&#25955;&#24230;&#20013;&#31163;&#25955;&#20998;&#24067;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#38598;&#20013;&#30028;&#38480;&#12290;&#24403;$n \geq k$&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#24179;&#22343;&#20540;&#20559;&#24046;&#30340;&#32553;&#25918;&#20026;$\sqrt{k} / n$&#65292;&#36825;&#27604;&#20808;&#21069;&#26368;&#22909;&#30340;&#32467;&#26524;$k/n$&#26377;&#25152;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#19978;&#26159;&#32039;&#23494;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of discrete distribution estimation in KL divergence and provide concentration bounds for the Laplace estimator. We show that the deviation from mean scales as $\sqrt{k}/n$ when $n \ge k$, improving upon the best prior result of $k/n$. We also establish a matching lower bound that shows that our bounds are tight up to polylogarithmic factors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Density-Softmax&#30340;&#24555;&#36895;&#30830;&#23450;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23494;&#24230;&#20989;&#25968;&#19982;softmax&#32467;&#21512;&#26469;&#25552;&#39640;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;</title><link>http://arxiv.org/abs/2302.06495</link><description>&lt;p&gt;
Density-Softmax: &#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24555;&#36895;&#30830;&#23450;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Density-Softmax: Scalable and Calibrated Uncertainty Estimation under Distribution Shifts. (arXiv:2302.06495v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Density-Softmax&#30340;&#24555;&#36895;&#30830;&#23450;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23494;&#24230;&#20989;&#25968;&#19982;softmax&#32467;&#21512;&#26469;&#25552;&#39640;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#30830;&#23450;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#23384;&#22312;&#36739;&#22823;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#65292;&#27010;&#29575;&#26041;&#27861;&#34429;&#28982;&#33021;&#32531;&#35299;&#27492;&#38382;&#39064;&#20294;&#35745;&#31639;&#25928;&#29575;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;Density-Softmax&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23494;&#24230;&#20989;&#25968;&#19982;softmax&#32467;&#21512;&#65292;&#20197;&#24555;&#36895;&#19988;&#36731;&#37327;&#32423;&#30340;&#26041;&#24335;&#25552;&#39640;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#30340;&#20284;&#28982;&#20540;&#65292;&#22312;&#27979;&#35797;&#26102;&#22312;&#36828;&#31163;&#35757;&#32451;&#26679;&#26412;&#26102;&#22686;&#21152;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#29702;&#35770;&#35777;&#26126;&#21644;&#23454;&#39564;&#19978;&#65292;Density-Softmax&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#26631;&#20934;softmax&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevalent deterministic deep-learning models suffer from significant over-confidence under distribution shifts. Probabilistic approaches can reduce this problem but struggle with computational efficiency. In this paper, we propose Density-Softmax, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer. By using the latent representation's likelihood value, our approach produces more uncertain predictions when test samples are distant from the training samples. Theoretically, we show that Density-Softmax can produce high-quality uncertainty estimation with neural networks, as it is the solution of minimax uncertainty risk and is distance-aware, thus reducing the over-confidence of the standard softmax. Empirically, our method enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#35889;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#27969;&#24418;&#19978;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#22312;&#29699;&#38754;&#21644;&#29615;&#38754;&#19978;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;&#23545;&#27604;&#26631;&#20934;&#26550;&#26500;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.05322</link><description>&lt;p&gt;
&#20351;&#29992;&#35889;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#27969;&#24418;&#19978;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25968;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Numerical Methods For PDEs Over Manifolds Using Spectral Physics Informed Neural Networks. (arXiv:2302.05322v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#35889;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#27969;&#24418;&#19978;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#22312;&#29699;&#38754;&#21644;&#29615;&#38754;&#19978;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;&#23545;&#27604;&#26631;&#20934;&#26550;&#26500;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35889;&#26041;&#27861;&#23545;&#40784;&#26550;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#27969;&#24418;&#19978;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#36825;&#20123;&#32593;&#32476;&#34987;&#35757;&#32451;&#20026;&#23558;&#21021;&#22987;&#26465;&#20214;&#12289;&#26102;&#38388;&#25139;&#21644;&#27969;&#24418;&#19978;&#30340;&#28857;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#32473;&#23450;&#26102;&#38388;&#21644;&#28857;&#22788;&#30340;&#35299;&#30340;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#35889;&#26041;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#21306;&#38388;&#19978;&#30340;&#28909;&#26041;&#31243;&#12289;&#29699;&#38754;&#21644;&#29615;&#38754;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#35299;&#20915;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#35889;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20248;&#20110;&#26631;&#20934;&#29289;&#29702;&#20449;&#24687;&#26550;&#26500;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#21253;&#25324;&#23545;&#24191;&#27867;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#27867;&#21270;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an approach for solving PDEs over manifolds using physics informed neural networks whose architecture aligns with spectral methods. The networks are trained to take in as input samples of an initial condition, a time stamp and point(s) on the manifold and then output the solution's value at the given time and point(s). We provide proofs of our method for the heat equation on the interval and examples of unique network architectures that are adapted to nonlinear equations on the sphere and the torus. We also show that our spectral-inspired neural network architectures outperform the standard physics informed architectures. Our extensive experimental results include generalization studies where the testing dataset of initial conditions is randomly sampled from a significantly larger space than the training set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25511;&#21046;&#39044;&#27979;&#38598;&#65288;RCPS&#65289;&#31243;&#24207;&#30340;&#25512;&#24191;&#65292;&#31216;&#20026;$K$-RCPS&#65292;&#23427;&#20801;&#35768;&#20026;&#20219;&#20309;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#36880;&#20010;&#26657;&#20934;&#30340;&#26410;&#26469;&#26679;&#26412;&#38388;&#38548;&#65292;&#24182;&#25511;&#21046;&#30456;&#23545;&#20110;&#22522;&#20934;&#30495;&#23454;&#22270;&#20687;&#30340;&#26576;&#31181;&#39118;&#38505;&#27010;&#24565;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#23567;&#24179;&#22343;&#21306;&#38388;&#38271;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.03791</link><description>&lt;p&gt;
&#22914;&#20309;&#20449;&#20219;&#24744;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#31181;&#20984;&#20248;&#21270;&#26041;&#27861;&#24212;&#23545;&#31526;&#21512;&#39118;&#38505;&#25511;&#21046;&#30340;&#22240;&#24335;&#20998;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control. (arXiv:2302.03791v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25511;&#21046;&#39044;&#27979;&#38598;&#65288;RCPS&#65289;&#31243;&#24207;&#30340;&#25512;&#24191;&#65292;&#31216;&#20026;$K$-RCPS&#65292;&#23427;&#20801;&#35768;&#20026;&#20219;&#20309;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#36880;&#20010;&#26657;&#20934;&#30340;&#26410;&#26469;&#26679;&#26412;&#38388;&#38548;&#65292;&#24182;&#25511;&#21046;&#30456;&#23545;&#20110;&#22522;&#20934;&#30495;&#23454;&#22270;&#20687;&#30340;&#26576;&#31181;&#39118;&#38505;&#27010;&#24565;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#23567;&#24179;&#22343;&#21306;&#38388;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31616;&#31216;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#37325;&#35201;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#32487;&#32493;&#22686;&#38271;&#12290;&#23613;&#31649;&#23427;&#20204;&#25552;&#20379;&#20102;&#26469;&#33258;&#32463;&#39564;&#20998;&#24067;&#30340;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#26679;&#26412;&#65292;&#20294;&#22312;&#20854;&#36127;&#36131;&#20219;&#22320;&#29992;&#20110;&#20851;&#38190;&#22330;&#26223;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#20173;&#23384;&#22312;&#37325;&#35201;&#38382;&#39064;&#12290;&#25910;&#25947;&#39044;&#27979;&#26159;&#19968;&#31181;&#29616;&#20195;&#24037;&#20855;&#65292;&#29992;&#20110;&#20026;&#20219;&#20309;&#40657;&#30418;&#23376;&#39044;&#27979;&#22120;&#26500;&#24314;&#26377;&#38480;&#26679;&#26412;&#12289;&#20998;&#24067;&#33258;&#30001;&#30340;&#19981;&#30830;&#23450;&#24615;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25511;&#21046;&#39044;&#27979;&#38598;&#65288;RCPS&#65289;&#31243;&#24207;&#30340;&#25512;&#24191;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;$K$-RCPS&#65292;&#23427;&#20801;&#35768;$(i)$&#20026;&#20219;&#20309;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#36880;&#20010;&#26657;&#20934;&#30340;&#26410;&#26469;&#26679;&#26412;&#38388;&#38548;&#65292;&#24182;$(ii)$&#25511;&#21046;&#30456;&#23545;&#20110;&#22522;&#20934;&#30495;&#23454;&#22270;&#20687;&#30340;&#26576;&#31181;&#39118;&#38505;&#27010;&#24565;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#23567;&#24179;&#22343;&#21306;&#38388;&#38271;&#24230;&#12290;&#19982;&#29616;&#26377;&#30340;&#25910;&#25947;&#39118;&#38505;&#25511;&#21046;&#36807;&#31243;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#20381;&#38752;&#19968;&#31181;&#26032;&#22411;&#30340;&#20984;&#20248;&#21270;&#20844;&#24335;&#65292;&#20351;&#20854;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#26131;&#20110;&#23454;&#29616;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22270;&#20687;&#21040;&#22270;&#20687;&#22238;&#24402;&#20219;&#21153;&#19978;&#20351;&#29992;&#24471;&#20998;&#20026;&#22522;&#30784;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#31243;&#24207;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#26657;&#20934;&#21644;&#33391;&#22909;&#25511;&#21046;&#30340;&#39044;&#27979;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative modeling, informally referred to as diffusion models, continue to grow in popularity across several important domains and tasks. While they provide high-quality and diverse samples from empirical distributions, important questions remain on the reliability and trustworthiness of these sampling procedures for their responsible use in critical scenarios. Conformal prediction is a modern tool to construct finite-sample, distribution-free uncertainty guarantees for any black-box predictor. In this work, we focus on image-to-image regression tasks and we present a generalization of the Risk-Controlling Prediction Sets (RCPS) procedure, that we term $K$-RCPS, which allows to $(i)$ provide entrywise calibrated intervals for future samples of any diffusion model, and $(ii)$ control a certain notion of risk with respect to a ground truth image with minimal mean interval length. Differently from existing conformal risk control procedures, ours relies on a novel convex opti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#19968;&#31867;&#23569;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;FewSOME&#65292;&#23427;&#21487;&#20197;&#22312;&#21482;&#26377;&#24456;&#23569;&#37327;&#30340;&#27491;&#24120;&#31867;&#21035;&#26679;&#26412;&#21644;&#27809;&#26377;&#24322;&#24120;&#31867;&#21035;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#24322;&#24120;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#21644;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#24050;&#32463;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.06957</link><description>&lt;p&gt;
FewSOME&#65306;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;&#19968;&#31867;&#23569;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
FewSOME: One-Class Few Shot Anomaly Detection with Siamese Networks. (arXiv:2301.06957v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#19968;&#31867;&#23569;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;FewSOME&#65292;&#23427;&#21487;&#20197;&#22312;&#21482;&#26377;&#24456;&#23569;&#37327;&#30340;&#27491;&#24120;&#31867;&#21035;&#26679;&#26412;&#21644;&#27809;&#26377;&#24322;&#24120;&#31867;&#21035;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#24322;&#24120;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#21644;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#24050;&#32463;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#22312;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21516;&#26102;&#20063;&#36880;&#28176;&#21464;&#24471;&#22797;&#26434;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#23548;&#33268;&#31639;&#27861;&#35745;&#31639;&#37327;&#22823;&#65292;&#19981;&#36866;&#21512;&#22312;&#21482;&#26377;&#23569;&#37327;&#27491;&#24120;&#26679;&#26412;&#21487;&#29992;&#20110;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;&#28145;&#24230;&#19968;&#31867;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;FewSOME&#65292;&#23427;&#21487;&#20197;&#22312;&#21482;&#26377;&#24456;&#23569;&#37327;&#30340;&#27491;&#24120;&#31867;&#21035;&#26679;&#26412;&#21644;&#27809;&#26377;&#24322;&#24120;&#31867;&#21035;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#26816;&#27979;&#20986;&#24322;&#24120;&#12290;&#30001;&#20110;&#23569;&#37327;&#25968;&#25454;&#35201;&#27714;&#21644;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;FewSOME &#30340;&#22797;&#26434;&#24230;&#36739;&#20302;&#12290;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340; Stop Loss &#25439;&#22833;&#20989;&#25968;&#22914;&#20309;&#25552;&#39640; FewSOME &#30340;&#40065;&#26834;&#24615;&#12290;&#22312; MNIST&#12289;CIFAR-10 &#21644; GTSRB&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FewSOME&#30340;&#24615;&#33021;&#24050;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent Anomaly Detection techniques have progressed the field considerably but at the cost of increasingly complex training pipelines. Such techniques require large amounts of training data, resulting in computationally expensive algorithms that are unsuitable for settings where only a small amount of normal samples are available for training. We propose 'Few Shot anOMaly detection' (FewSOME), a deep One-Class Anomaly Detection algorithm with the ability to accurately detect anomalies having trained on 'few' examples of the normal class and no examples of the anomalous class. We describe FewSOME to be of low complexity given its low data requirement and short training time. FewSOME is aided by pretrained weights with an architecture based on Siamese Networks. By means of an ablation study, we demonstrate how our proposed loss, 'Stop Loss', improves the robustness of FewSOME. Our experiments demonstrate that FewSOME performs at state-of-the-art level on benchmark datasets MNIST, CIFAR-1
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;&#21453;&#23556;&#32418;&#22806;&#20809;&#27874;&#20449;&#21495;&#30340;&#25163;&#21183;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#25910;&#38598;&#21453;&#23556;&#20809;&#30340;&#24378;&#24230;&#21464;&#21270;&#65292;&#21487;&#20197;&#22312;20-35cm&#33539;&#22260;&#20869;&#35782;&#21035;&#19981;&#21516;&#25163;&#21183;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;96&#65285;&#12290;</title><link>http://arxiv.org/abs/2301.05955</link><description>&lt;p&gt;
&#22522;&#20110;&#21453;&#23556;&#32418;&#22806;&#20809;&#27874;&#20449;&#21495;&#30340;&#25163;&#21183;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Hand Gesture Recognition through Reflected Infrared Light Wave Signals. (arXiv:2301.05955v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05955
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;&#21453;&#23556;&#32418;&#22806;&#20809;&#27874;&#20449;&#21495;&#30340;&#25163;&#21183;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#25910;&#38598;&#21453;&#23556;&#20809;&#30340;&#24378;&#24230;&#21464;&#21270;&#65292;&#21487;&#20197;&#22312;20-35cm&#33539;&#22260;&#20869;&#35782;&#21035;&#19981;&#21516;&#25163;&#21183;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;96&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#26469;&#33258;&#34987;&#35797;&#21453;&#23556;&#30340;&#38750;&#30456;&#24178;&#20809;&#27874;&#20449;&#21495;&#36827;&#34892;&#26080;&#32447;&#65288;&#38750;&#25509;&#35302;&#65289;&#25163;&#21183;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#38647;&#36798;&#12289;&#20809;&#24433;&#12289;&#22768;&#38899;&#21644;&#22522;&#20110;&#30456;&#26426;&#30340;&#20256;&#24863;&#31995;&#32479;&#30456;&#27604;&#65292;&#35813;&#25216;&#26415;&#20351;&#29992;&#20302;&#25104;&#26412;&#30340;&#26222;&#21450;&#20809;&#28304;&#65288;&#20363;&#22914;&#32418;&#22806;LED&#65289;&#21521;&#25191;&#34892;&#25163;&#21183;&#30340;&#34987;&#35797;&#21457;&#36865;&#20809;&#65292;&#24182;&#19988;&#21453;&#23556;&#20809;&#30001;&#20809;&#20256;&#24863;&#22120;&#65288;&#20363;&#22914;&#20809;&#30005;&#25506;&#27979;&#22120;&#65289;&#25910;&#38598;&#12290;&#35813;&#20809;&#27874;&#20256;&#24863;&#31995;&#32479;&#21487;&#20197;&#35782;&#21035;&#19981;&#21516;&#25163;&#21183;&#65292;&#22312;20-35cm&#33539;&#22260;&#20869;&#26681;&#25454;&#25509;&#25910;&#21040;&#30340;&#20809;&#24378;&#21464;&#21270;&#12290;&#25163;&#21183;&#35782;&#21035;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;96&#65285;&#12290;&#25152;&#24320;&#21457;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#20302;&#25104;&#26412;&#21644;&#38750;&#25509;&#35302;&#25163;&#21183;&#35782;&#21035;&#25216;&#26415;&#65292;&#22312;&#20247;&#22810;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a wireless (non-contact) gesture recognition method using only incoherent light wave signals reflected from a human subject. In comparison to existing radar, light shadow, sound and camera-based sensing systems, this technology uses a low-cost ubiquitous light source (e.g., infrared LED) to send light towards the subject's hand performing gestures and the reflected light is collected by a light sensor (e.g., photodetector). This light wave sensing system recognizes different gestures from the variations of the received light intensity within a 20-35cm range. The hand gesture recognition results demonstrate up to 96% accuracy on average. The developed system can be utilized in numerous Human-computer Interaction (HCI) applications as a low-cost and non-contact gesture recognition technology.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#31163;&#32676;&#28857;&#30340;&#20154;&#24037;&#22522;&#20934;&#31038;&#21306;&#26816;&#27979;&#22270;&#27169;&#22411;&#65288;ABCD+o&#65289;&#65292;&#21487;&#20197;&#29992;&#20110;&#25506;&#32034;&#31163;&#32676;&#28857;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2301.05749</link><description>&lt;p&gt;
&#24102;&#26377;&#31163;&#32676;&#28857;&#30340;&#31038;&#21306;&#26816;&#27979;&#20154;&#24037;&#22522;&#20934;&#65288;ABCD+o&#65289;
&lt;/p&gt;
&lt;p&gt;
Artificial Benchmark for Community Detection with Outliers (ABCD+o). (arXiv:2301.05749v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05749
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#31163;&#32676;&#28857;&#30340;&#20154;&#24037;&#22522;&#20934;&#31038;&#21306;&#26816;&#27979;&#22270;&#27169;&#22411;&#65288;ABCD+o&#65289;&#65292;&#21487;&#20197;&#29992;&#20110;&#25506;&#32034;&#31163;&#32676;&#28857;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#22522;&#20934;&#31038;&#21306;&#26816;&#27979;&#22270;&#65288;ABCD&#65289;&#26159;&#19968;&#20010;&#20855;&#26377;&#31038;&#21306;&#32467;&#26500;&#21644;&#24230;&#25968;&#12289;&#31038;&#21306;&#22823;&#23567;&#22343;&#26381;&#20174;&#24130;&#24459;&#20998;&#24067;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#19982;&#33879;&#21517;&#30340;LFR&#27169;&#22411;&#20855;&#26377;&#30456;&#20284;&#30340;&#23646;&#24615;&#65292;&#23427;&#30340;&#20027;&#35201;&#21442;&#25968;&#958;&#21487;&#20197;&#35843;&#33410;&#20197;&#27169;&#25311;LFR&#27169;&#22411;&#20013;&#30340;&#28151;&#21512;&#21442;&#25968;&#956;&#12290;&#26412;&#25991;&#23558;ABCD&#27169;&#22411;&#25193;&#23637;&#21040;&#21253;&#25324;&#28508;&#22312;&#30340;&#31163;&#32676;&#28857;&#12290;&#25105;&#20204;&#23545;&#26032;&#30340;ABCD+o&#27169;&#22411;&#20197;&#21450;&#19968;&#20010;&#30495;&#23454;&#32593;&#32476;&#36827;&#34892;&#19968;&#20123;&#25506;&#32034;&#24615;&#23454;&#39564;&#65292;&#20197;&#34920;&#26126;&#31163;&#32676;&#28857;&#20855;&#26377;&#26576;&#20123;&#25152;&#38656;&#30340;&#26126;&#26174;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Artificial Benchmark for Community Detection graph (ABCD) is a random graph model with community structure and power-law distribution for both degrees and community sizes. The model generates graphs with similar properties as the well-known LFR one, and its main parameter $\xi$ can be tuned to mimic its counterpart in the LFR model, the mixing parameter $\mu$. In this paper, we extend the ABCD model to include potential outliers. We perform some exploratory experiments on both the new ABCD+o model as well as a real-world network to show that outliers possess some desired, distinguishable properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#31934;&#30830;&#22320;&#34920;&#24449;&#20102;&#31616;&#21333;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#32972;&#26223;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#21516;&#36136;&#24615;&#22312;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27867;&#21270;&#20013;&#30340;&#35843;&#21046;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.13069</link><description>&lt;p&gt;
&#21516;&#36136;&#24615;&#22312;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#21452;&#19979;&#38477;&#27867;&#21270;&#20013;&#30340;&#35843;&#21046;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Homophily modulates double descent generalization in graph convolution networks. (arXiv:2212.13069v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#31934;&#30830;&#22320;&#34920;&#24449;&#20102;&#31616;&#21333;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#32972;&#26223;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#21516;&#36136;&#24615;&#22312;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27867;&#21270;&#20013;&#30340;&#35843;&#21046;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#20851;&#31995;&#25968;&#25454;&#38598;&#65288;&#22914;&#20195;&#35874;&#12289;&#20132;&#36890;&#21644;&#31038;&#20132;&#32593;&#32476;&#65289;&#30340;&#26368;&#25104;&#21151;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#25968;&#25454;&#20013;&#32534;&#30721;&#30340;&#21508;&#31181;&#20132;&#20114;&#30340;&#24378;&#22823;&#27867;&#21270;&#30340;&#20915;&#23450;&#22240;&#32032;&#24182;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26469;&#33258;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#30340;&#26041;&#27861;&#26080;&#27861;&#35299;&#37322;&#20986;&#29616;&#30340;&#29616;&#35937;&#65292;&#22914;&#21452;&#19979;&#38477;&#25110;&#39118;&#38505;&#21462;&#20915;&#20110;&#20132;&#20114;&#24615;&#36136;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#20998;&#26512;&#24037;&#20855;&#26469;&#31934;&#30830;&#22320;&#34920;&#24449;&#31616;&#21333;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#32972;&#26223;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#27867;&#21270;&#12290;&#23548;&#20986;&#30340;&#26354;&#32447;&#29616;&#35937;&#23398;&#19978;&#21313;&#20998;&#20016;&#23500;&#65306;&#23427;&#20204;&#35299;&#37322;&#20102;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#23398;&#20064;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#24182;&#39044;&#27979;&#20102;&#26368;&#36817;&#20316;&#21697;&#25152;&#36136;&#30097;&#30340;GNN&#20013;&#21452;&#19979;&#38477;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39118;&#38505;&#22914;&#20309;&#21462;&#20915;&#20110;&#22270;&#20013;&#30340;&#22122;&#22768;&#12289;&#29305;&#24449;&#20013;&#30340;&#22122;&#22768;&#21644;&#29992;&#20110;&#35757;&#32451;&#30340;&#33410;&#28857;&#27604;&#20363;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#29702;&#35299;&#21516;&#36136;&#24615;&#22914;&#20309;&#35843;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#25552;&#20379;&#20102;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are among the most successful machine learning models for relational datasets like metabolic, transportation, and social networks. Yet the determinants of their strong generalization for diverse interactions encoded in the data are not well understood. Methods from statistical learning theory do not explain emergent phenomena such as double descent or the dependence of risk on the nature of interactions. We use analytical tools from statistical physics and random matrix theory to precisely characterize generalization in simple graph convolution networks on the contextual stochastic block model. The derived curves are phenomenologically rich: they explain the distinction between learning on homophilic and heterophilic and they predict double descent whose existence in GNNs has been questioned by recent work. We show how risk depends on the interplay between the noise in the graph, noise in the features, and the proportion of nodes used for training. Our analysis pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Fine-tune-CoT&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#35753;&#36739;&#23567;&#27169;&#22411;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36828;&#36828;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#20026;&#27599;&#20010;&#21407;&#22987;&#26679;&#26412;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#30340;&#21407;&#22240;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2212.10071</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#25512;&#29702;&#25945;&#24072;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Reasoning Teachers. (arXiv:2212.10071v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Fine-tune-CoT&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#35753;&#36739;&#23567;&#27169;&#22411;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36828;&#36828;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#20026;&#27599;&#20010;&#21407;&#22987;&#26679;&#26412;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#30340;&#21407;&#22240;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24605;&#32500;&#38142;&#26465;&#25552;&#31034;&#21487;&#20197;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36880;&#27493;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#24605;&#32500;&#38142;&#26465;&#26041;&#27861;&#20381;&#36182;&#20110;&#20687;GPT-3 175B&#36825;&#26679;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#65292;&#36825;&#22312;&#35268;&#27169;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fine-tune-CoT&#26041;&#27861;&#65292;&#20351;&#29992;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#65292;&#20197;&#35753;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#23610;&#23544;&#35201;&#27714;&#20943;&#23569;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#27169;&#22411;&#21644;&#22797;&#26434;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;Fine-tune-CoT&#21487;&#20197;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#23454;&#29616;&#23454;&#36136;&#24615;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36828;&#36828;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#20063;&#20248;&#20110;&#25945;&#24072;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#35813;&#26041;&#27861;&#65292;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#20026;&#27599;&#20010;&#21407;&#22987;&#26679;&#26412;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#30340;&#21407;&#22240;&#35299;&#37322;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#24494;&#35843;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#20174;&#20840;&#22330;&#20301;&#31227;&#25968;&#25454;&#20013;&#23545;&#26448;&#26009;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#23637;&#31034;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;&#38382;&#39064;&#30340;&#26465;&#20214;&#21644;&#37325;&#26500;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.07723</link><description>&lt;p&gt;
&#21033;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#20174;&#20840;&#22330;&#20301;&#31227;&#25968;&#25454;&#20013;&#23545;&#26448;&#26009;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for Material Model Calibration from Full-Field Displacement Data. (arXiv:2212.07723v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#20174;&#20840;&#22330;&#20301;&#31227;&#25968;&#25454;&#20013;&#23545;&#26448;&#26009;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#23637;&#31034;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;&#38382;&#39064;&#30340;&#26465;&#20214;&#21644;&#37325;&#26500;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26448;&#26009;&#21442;&#25968;&#30340;&#37492;&#23450;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#30417;&#27979;&#21644;&#35780;&#20272;&#22522;&#30784;&#35774;&#26045;&#24314;&#31569;&#29289;&#30340;&#23454;&#38469;&#29366;&#20917;&#65292;&#22240;&#20026;&#26448;&#26009;&#21442;&#25968;&#30452;&#25509;&#21453;&#26144;&#32467;&#26500;&#29289;&#23545;&#22806;&#37096;&#24433;&#21709;&#30340;&#25269;&#25239;&#21147;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#21512;&#36866;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#26159;&#31616;&#21333;&#22320;&#21253;&#21547;&#35266;&#27979;&#25968;&#25454;&#12290;&#19982;&#22522;&#20110;&#32593;&#26684;&#30340;&#26041;&#27861;&#65288;&#22914;&#26368;&#23567;&#20108;&#20056;&#26377;&#38480;&#20803;&#27861;&#65288;LS-FEM&#65289;&#26041;&#27861;&#65289;&#19981;&#21516;&#65292;&#19981;&#38656;&#35201;&#35745;&#31639;&#32593;&#26684;&#21644;&#25968;&#25454;&#30340;&#25554;&#20540;&#12290;&#22312;&#24403;&#21069;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32447;&#24615;&#24377;&#24615;&#21147;&#23398;&#27169;&#22411;&#30340;&#20840;&#22330;&#20301;&#31227;&#21644;&#20840;&#23616;&#21147;&#25968;&#25454;&#26657;&#20934;&#30340;PINNs&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20248;&#21270;&#38382;&#39064;&#30340;&#26465;&#20214;&#21644;&#37325;&#26500;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The identification of material parameters occurring in constitutive models has a wide range of applications in practice. One of these applications is the monitoring and assessment of the actual condition of infrastructure buildings, as the material parameters directly reflect the resistance of the structures to external impacts. Physics-informed neural networks (PINNs) have recently emerged as a suitable method for solving inverse problems. The advantages of this method are a straightforward inclusion of observation data. Unlike grid-based methods, such as the least square finite element method (LS-FEM) approach, no computational grid and no interpolation of the data is required. In the current work, we propose PINNs for the calibration of constitutive models from full-field displacement and global force data in a realistic regime on the example of linear elasticity. We show that conditioning and reformulation of the optimization problem play a crucial role in real-world applications. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#28151;&#28102;&#21464;&#37327;&#23548;&#33268;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#23384;&#22312;&#25361;&#25112;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#26080;&#27861;&#33719;&#24471;&#19968;&#33268;&#20215;&#20540;&#20272;&#35745;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20445;&#35777;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20855;&#26377;&#20445;&#35777;&#30340;&#19979;&#38480;&#31639;&#27861;&#21644;&#23616;&#37096;&#25910;&#25947;&#30340;&#25913;&#36827;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.16583</link><description>&lt;p&gt;
&#22312;&#28151;&#28102;&#19979;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Policy Evaluation and Optimization under Confounding. (arXiv:2211.16583v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#28151;&#28102;&#21464;&#37327;&#23548;&#33268;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#23384;&#22312;&#25361;&#25112;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#26080;&#27861;&#33719;&#24471;&#19968;&#33268;&#20215;&#20540;&#20272;&#35745;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20445;&#35777;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20855;&#26377;&#20445;&#35777;&#30340;&#19979;&#38480;&#31639;&#27861;&#21644;&#23616;&#37096;&#25910;&#25947;&#30340;&#25913;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#35780;&#20272;&#21644;&#20248;&#21270;&#31574;&#30053;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#26102;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#20256;&#32479;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#28151;&#28102;&#38382;&#39064;&#19981;&#20165;&#21487;&#33021;&#23548;&#33268;&#31967;&#31957;&#30340;&#20915;&#31574;&#21644;&#31574;&#30053;&#65292;&#32780;&#19988;&#22312;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#22914;&#21307;&#30103;&#21644;&#25945;&#32946;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#28798;&#38590;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21246;&#21202;&#20102;&#28151;&#28102;&#30340; MDP &#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#30340;&#38754;&#35980;&#65292;&#24182;&#26681;&#25454;&#28151;&#28102;&#23545;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#26102;&#38388;&#28436;&#21464;&#21644;&#24433;&#21709;&#26469;&#21306;&#20998;&#28151;&#28102;&#30340;&#20551;&#35774;&#12290;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#26080;&#27861;&#33719;&#24471;&#19968;&#33268;&#20215;&#20540;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#21644;&#35752;&#35770;&#20102;&#35745;&#31639;&#20855;&#26377;&#20445;&#35777;&#30340;&#19979;&#38480;&#30340;&#31639;&#27861;&#12290;&#24403;&#19968;&#33268;&#30340;&#20272;&#35745;&#21487;&#34892;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31163;&#32447;&#31574;&#30053;&#25913;&#36827;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23616;&#37096;&#25910;&#25947;&#30340;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#26684;&#23376;&#19990;&#30028;&#21644;&#27169;&#25311;&#21307;&#30103;&#22330;&#26223;&#20013;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating and optimizing policies in the presence of unobserved confounders is a problem of growing interest in offline reinforcement learning. Using conventional methods for offline RL in the presence of confounding can not only lead to poor decisions and poor policies, but can also have disastrous effects in critical applications such as healthcare and education. We map out the landscape of offline policy evaluation for confounded MDPs, distinguishing assumptions on confounding based on their time-evolution and effect on the data-collection policies. We determine when consistent value estimates are not achievable, providing and discussing algorithms to estimate lower bounds with guarantees in those cases. When consistent estimates are achievable, we provide sample complexity guarantees. We also present new algorithms for offline policy improvement and prove local convergence guarantees. Finally, we experimentally evaluate our algorithms on gridworld and a simulated healthcare settin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#36817;&#26399;&#22312;&#35774;&#23450;&#39044;&#27979;&#19979;&#30340;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#39044;&#27979;&#38598;&#20197;&#36866;&#24212;&#24402;&#32435;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#33410;&#28857;&#20998;&#31867;&#65292;&#35777;&#26126;&#20102;&#25552;&#20379;&#20102;&#27604;&#31616;&#21333;&#30340;&#31526;&#21512;&#39044;&#27979;&#24212;&#29992;&#26356;&#21152;&#32039;&#33268;&#21644;&#33391;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#12290;</title><link>http://arxiv.org/abs/2211.14555</link><description>&lt;p&gt;
&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#33410;&#28857;&#20998;&#31867;&#39044;&#27979;&#38598;
&lt;/p&gt;
&lt;p&gt;
Distribution Free Prediction Sets for Node Classification. (arXiv:2211.14555v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#36817;&#26399;&#22312;&#35774;&#23450;&#39044;&#27979;&#19979;&#30340;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#39044;&#27979;&#38598;&#20197;&#36866;&#24212;&#24402;&#32435;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#33410;&#28857;&#20998;&#31867;&#65292;&#35777;&#26126;&#20102;&#25552;&#20379;&#20102;&#27604;&#31616;&#21333;&#30340;&#31526;&#21512;&#39044;&#27979;&#24212;&#29992;&#26356;&#21152;&#32039;&#33268;&#21644;&#33391;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#21487;&#20197;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#39640;&#31934;&#24230;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#20294;&#20854;&#26080;&#27861;&#25552;&#20379;&#20005;&#26684;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23450;&#20041;&#12290;&#30001;&#20110;&#22270;&#32467;&#26500;&#24341;&#36215;&#30340;&#25968;&#25454;&#28857;&#20381;&#36182;&#24615;&#65292;&#37327;&#21270;GNN&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#21033;&#29992;&#36817;&#26399;&#22312;&#35774;&#23450;&#39044;&#27979;&#19979;&#30340;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#39044;&#27979;&#38598;&#20197;&#36866;&#24212;&#24402;&#32435;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#33410;&#28857;&#20998;&#31867;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#25442;&#20301;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#36890;&#36807;&#36866;&#24403;&#21152;&#26435;&#31526;&#21512;&#20998;&#25968;&#26469;&#21453;&#26144;&#32593;&#32476;&#32467;&#26500;&#12290;&#36890;&#36807;&#22312;&#24120;&#29992;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#27969;&#34892;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#27604;&#31616;&#21333;&#30340;&#31526;&#21512;&#39044;&#27979;&#24212;&#29992;&#26356;&#21152;&#32039;&#33268;&#21644;&#33391;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are able to achieve high classification accuracy on many important real world datasets, but provide no rigorous notion of predictive uncertainty. Quantifying the confidence of GNN models is difficult due to the dependence between datapoints induced by the graph structure.  We leverage recent advances in conformal prediction to construct prediction sets for node classification in inductive learning scenarios. We do this by taking an existing approach for conformal classification that relies on \textit{exchangeable} data and modifying it by appropriately weighting the conformal scores to reflect the network structure. We show through experiments on standard benchmark datasets using popular GNN models that our approach provides tighter and better calibrated prediction sets than a naive application of conformal prediction.
&lt;/p&gt;</description></item><item><title>Powderworld&#26159;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#29992;&#20110;&#25552;&#20379;&#27867;&#21270;&#24615;&#30340;&#30740;&#31350;&#24179;&#21488;&#65292;&#21253;&#25324;&#19990;&#30028;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.13051</link><description>&lt;p&gt;
Powderworld&#65306;&#36890;&#36807;&#22810;&#26679;&#21270;&#20219;&#21153;&#20998;&#24067;&#26469;&#29702;&#35299;&#27867;&#21270;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Powderworld: A Platform for Understanding Generalization via Rich Task Distributions. (arXiv:2211.13051v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13051
&lt;/p&gt;
&lt;p&gt;
Powderworld&#26159;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#29992;&#20110;&#25552;&#20379;&#27867;&#21270;&#24615;&#30340;&#30740;&#31350;&#24179;&#21488;&#65292;&#21253;&#25324;&#19990;&#30028;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27867;&#21270;&#20195;&#29702;&#38656;&#35201;&#19968;&#32452;&#20016;&#23500;&#12289;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#36825;&#20123;&#20219;&#21153;&#35774;&#35745;&#19968;&#20010;&#29702;&#24819;&#30340;&#29615;&#22659;&#24456;&#22256;&#38590;&#8212;&#8212;&#29702;&#24819;&#30340;&#29615;&#22659;&#24212;&#25903;&#25345;&#19968;&#31995;&#21015;&#26032;&#20852;&#29616;&#35937;&#12289;&#20016;&#23500;&#30340;&#20219;&#21153;&#31354;&#38388;&#21644;&#24555;&#36895;&#30340;&#36816;&#34892;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Powderworld&#65292;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#12290;&#22312;Powderworld&#20869;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#28608;&#21457;&#25361;&#25112;&#30340;&#20998;&#24067;&#65292;&#19968;&#20010;&#29992;&#20110;&#19990;&#30028;&#24314;&#27169;&#65292;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;&#27599;&#20010;&#20998;&#24067;&#37117;&#21253;&#21547;&#25163;&#21160;&#35774;&#35745;&#30340;&#27979;&#35797;&#20219;&#21153;&#65292;&#20197;&#26816;&#26597;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#21487;&#33021;&#20250;&#25233;&#21046;&#39640;&#26041;&#24046;&#29615;&#22659;&#19979;&#30340;&#23398;&#20064;&#12290;Powderworld&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25903;&#25345;&#27867;&#21270;&#30740;&#31350;&#30340;&#29615;&#22659;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment' for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating challenges distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment's complexity improves generalization for world models and certain reinforcement learning agents, yet may inhibit learning in high-variance environments. Powderworld aims to support the study of generalization by providing a so
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; KGE-SymCL&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#21487;&#21306;&#20998;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.10738</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Contrastive Learning Based on Relation-Symmetrical Structure. (arXiv:2211.10738v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; KGE-SymCL&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#21487;&#21306;&#20998;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837; (KGE) &#26088;&#22312;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#20197;&#21463;&#30410;&#20110;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#21033;&#29992;&#20110;&#22270;&#23398;&#20064;&#65292;&#20316;&#20026;&#22686;&#24378;&#25152;&#23398;&#34920;&#31034;&#30340;&#21487;&#21306;&#20998;&#33021;&#21147;&#30340;&#26377;&#25928;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;KG&#30340;&#22797;&#26434;&#32467;&#26500;&#20351;&#24471;&#26500;&#24314;&#36866;&#24403;&#30340;&#23545;&#27604;&#23545;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#25968;&#19981;&#22810;&#30340;&#20960;&#20010;&#23581;&#35797;&#23558;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#19982;KGE&#38598;&#25104;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22823;&#22810;&#20381;&#36182;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Bert&#65289;&#36827;&#34892;&#23545;&#27604;&#23545;&#26500;&#24314;&#65292;&#32780;&#19981;&#26159;&#23436;&#20840;&#25366;&#25496;&#28508;&#22312;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#34920;&#36798;&#33021;&#21147;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#20869;&#30340;&#23454;&#20307;&#36890;&#24120;&#30456;&#20284;&#19988;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;KGE-SymCL&#65292;&#23427;&#22312;KG&#20013;&#25366;&#25496;&#23545;&#31216;&#32467;&#26500;&#20449;&#24687;&#20197;&#22686;&#24378;&#25152;&#23398;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) aims at learning powerful representations to benefit various artificial intelligence applications. Meanwhile, contrastive learning has been widely leveraged in graph learning as an effective mechanism to enhance the discriminative capacity of the learned representations. However, the complex structures of KG make it hard to construct appropriate contrastive pairs. Only a few attempts have integrated contrastive learning strategies with KGE. But, most of them rely on language models ( e.g., Bert) for contrastive pair construction instead of fully mining information underlying the graph structure, hindering expressive ability. Surprisingly, we find that the entities within a relational symmetrical structure are usually similar and correlated. To this end, we propose a knowledge graph contrastive learning framework based on relation-symmetrical structure, KGE-SymCL, which mines symmetrical structure information in KGs to enhance the discriminative ability o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#20108;&#20803;&#27835;&#30103;&#26465;&#20214;&#19979;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#22330;&#26223;&#20013;&#65292;&#23545;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#29983;&#25104;&#24314;&#27169;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01939</link><description>&lt;p&gt;
&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#27169;&#22411;&#36873;&#25321;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation. (arXiv:2211.01939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#20108;&#20803;&#27835;&#30103;&#26465;&#20214;&#19979;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#22330;&#26223;&#20013;&#65292;&#23545;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#29983;&#25104;&#24314;&#27169;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20108;&#20803;&#27835;&#30103;&#26465;&#20214;&#19979;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#20272;&#35745;&#30340;&#24773;&#20917;&#12290;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#19981;&#21516;&#65292;&#30001;&#20110;&#25105;&#20204;&#26080;&#27861;&#35266;&#23519;&#21040;&#20219;&#20309;&#25968;&#25454;&#28857;&#30340;&#21453;&#20107;&#23454;&#28508;&#22312;&#32467;&#26524;&#65292;&#22240;&#27492;&#27809;&#26377;&#23436;&#32654;&#30340;&#20132;&#21449;&#39564;&#35777;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#20195;&#29702;&#24230;&#37327;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21462;&#20915;&#20110;&#20174;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#30340;&#36741;&#21161;&#24178;&#25200;&#27169;&#22411;&#65288;&#20542;&#21521;&#24615;&#24471;&#20998;&#27169;&#22411;&#12289;&#32467;&#26524;&#22238;&#24402;&#27169;&#22411;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24230;&#37327;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20165;&#22312;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#21453;&#20107;&#23454;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#25991;&#29486;&#20013;&#20171;&#32461;&#30340;&#36825;&#20123;&#24230;&#37327;&#26041;&#27861;&#20197;&#21450;&#26412;&#30740;&#31350;&#20013;&#20171;&#32461;&#30340;&#26032;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#23454;&#29616;&#22810;&#20010;&#36924;&#30495;&#25968;&#25454;&#38598;&#30340;&#26368;&#26032;&#29983;&#25104;&#24314;&#27169;&#36827;&#23637;&#22522;&#30784;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#20102;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of model selection in causal inference, specifically for the case of conditional average treatment effect (CATE) estimation under binary treatments. Unlike model selection in machine learning, there is no perfect analogue of cross-validation as we do not observe the counterfactual potential outcome for any data point. Towards this, there have been a variety of proxy metrics proposed in the literature, that depend on auxiliary nuisance models estimated from the observed data (propensity score model, outcome regression model). However, the effectiveness of these metrics has only been studied on synthetic datasets as we can access the counterfactual data for them. We conduct an extensive empirical analysis to judge the performance of these metrics introduced in the literature, and novel ones introduced in this work, where we utilize the latest advances in generative modeling to incorporate multiple realistic datasets. Our analysis suggests novel model selection strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#25928;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#65292;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.17376</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;SoK&#65306;&#38754;&#21521;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#23433;&#20840;&#20998;&#26512;&#35299;&#37322;&#24615;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
SoK: Modeling Explainability in Security Analytics for Interpretability, Trustworthiness, and Usability. (arXiv:2210.17376v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#25928;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#65292;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29992;&#24615;&#26159;&#39640;&#39118;&#38505;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#20197;&#39640;&#31934;&#24230;&#33879;&#31216;&#65292;&#20294;&#23427;&#20204;&#34920;&#29616;&#20026;&#40657;&#30418;&#23376;&#65292;&#22312;&#20854;&#20013;&#35782;&#21035;&#23548;&#33268;&#20998;&#31867;&#25110;&#39044;&#27979;&#30340;&#37325;&#35201;&#29305;&#24449;&#21644;&#22240;&#32032;&#26159;&#22256;&#38590;&#30340;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#20449;&#20219;&#65292;&#29305;&#21035;&#26159;&#24403;&#38169;&#35823;&#39044;&#27979;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#26102;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#26041;&#27861;&#26088;&#22312;&#25552;&#20379;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30340;&#27934;&#35265;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#19981;&#19968;&#33268;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#20302;&#20445;&#30495;&#24230;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25805;&#32437;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#23545;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#25928;&#21147;&#65306;&#20351;&#29992;&#31995;&#32479;&#26085;&#24535;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#24694;&#24847;&#36719;&#20214;&#21644;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability, trustworthiness, and usability are key considerations in high-stake security applications, especially when utilizing deep learning models. While these models are known for their high accuracy, they behave as black boxes in which identifying important features and factors that led to a classification or a prediction is difficult. This can lead to uncertainty and distrust, especially when an incorrect prediction results in severe consequences. Thus, explanation methods aim to provide insights into the inner working of deep learning models. However, most explanation methods provide inconsistent explanations, have low fidelity, and are susceptible to adversarial manipulation, which can reduce model trustworthiness. This paper provides a comprehensive analysis of explainable methods and demonstrates their efficacy in three distinct security applications: anomaly detection using system logs, malware prediction, and detection of adversarial images. Our quantitative and quali
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20204;&#21457;&#29616;&#65292;&#23558;&#21160;&#37327;&#36229;&#21442;&#25968;&#19982;&#23398;&#20064;&#29575;&#30340;$2/3$&#27425;&#26041;&#32553;&#25918;&#21487;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21152;&#36895;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#19988;&#19981;&#29306;&#29298;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.16400</link><description>&lt;p&gt;
&#26356;&#24179;&#31283;&#65292;&#26356;&#24555;&#36895;&#65306;&#32553;&#25918;&#21160;&#37327;&#20197;&#23454;&#29616;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26368;&#20339;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Flatter, faster: scaling momentum for optimal speedup of SGD. (arXiv:2210.16400v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16400
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20204;&#21457;&#29616;&#65292;&#23558;&#21160;&#37327;&#36229;&#21442;&#25968;&#19982;&#23398;&#20064;&#29575;&#30340;$2/3$&#27425;&#26041;&#32553;&#25918;&#21487;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21152;&#36895;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#19988;&#19981;&#29306;&#29298;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#36890;&#24120;&#23384;&#22312;&#22312;&#33391;&#22909;&#27867;&#21270;&#21644;&#24555;&#36895;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#24448;&#24448;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#21160;&#37327;&#21487;&#20197;&#24110;&#21161;&#21152;&#36895;SGD&#30340;&#35757;&#32451;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#27809;&#26377;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#36873;&#25321;&#21160;&#37327;&#36229;&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SGD&#21644;&#21160;&#37327;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#30456;&#20114;&#20316;&#29992;&#25152;&#20135;&#29983;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#21457;&#29616;&#23558;&#21160;&#37327;&#36229;&#21442;&#25968;$1-\beta$&#19982;&#23398;&#20064;&#29575;&#30340;$2/3$&#27425;&#26041;&#32553;&#25918;&#21487;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21152;&#36895;&#35757;&#32451;&#32780;&#19981;&#25439;&#22833;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19982;&#32467;&#26500;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#30340;&#20027;&#35201;&#20551;&#35774;&#26159;&#23384;&#22312;&#19968;&#31867;&#20855;&#26377;&#36864;&#21270;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#27969;&#24418;&#65292;&#36825;&#26159;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#33258;&#28982;&#23646;&#24615;&#12290;&#35757;&#32451;&#21160;&#24577;&#26174;&#31034;&#20986;&#20004;&#20010;&#29305;&#24449;&#26102;&#38388;&#23610;&#24230;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonly used optimization algorithms often show a trade-off between good generalization and fast training times. For instance, stochastic gradient descent (SGD) tends to have good generalization; however, adaptive gradient methods have superior training times. Momentum can help accelerate training with SGD, but so far there has been no principled way to select the momentum hyperparameter. Here we study training dynamics arising from the interplay between SGD with label noise and momentum in the training of overparametrized neural networks. We find that scaling the momentum hyperparameter $1-\beta$ with the learning rate to the power of $2/3$ maximally accelerates training, without sacrificing generalization. To analytically derive this result we develop an architecture-independent framework, where the main assumption is the existence of a degenerate manifold of global minimizers, as is natural in overparametrized models. Training dynamics display the emergence of two characteristic ti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#22270;&#30340;&#20154;&#24037;&#22522;&#20934;&#31038;&#21306;&#26816;&#27979;&#27169;&#22411;h-ABCD&#65292;&#24182;&#19988;&#21487;&#20197;&#28789;&#27963;&#22320;&#27169;&#25311;&#19981;&#21516;&#31038;&#21306;&#30340;&#22343;&#21248;&#24615;&#27700;&#24179;&#65292;&#26159;&#20998;&#26512;&#21644;&#35843;&#25972;&#36229;&#22270;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#30340;&#21512;&#25104;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2210.15009</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#30340;&#31038;&#21306;&#26816;&#27979;&#20154;&#24037;&#22522;&#20934; (h-ABCD)
&lt;/p&gt;
&lt;p&gt;
Hypergraph Artificial Benchmark for Community Detection (h-ABCD). (arXiv:2210.15009v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#22270;&#30340;&#20154;&#24037;&#22522;&#20934;&#31038;&#21306;&#26816;&#27979;&#27169;&#22411;h-ABCD&#65292;&#24182;&#19988;&#21487;&#20197;&#28789;&#27963;&#22320;&#27169;&#25311;&#19981;&#21516;&#31038;&#21306;&#30340;&#22343;&#21248;&#24615;&#27700;&#24179;&#65292;&#26159;&#20998;&#26512;&#21644;&#35843;&#25972;&#36229;&#22270;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#30340;&#21512;&#25104;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#22522;&#20934;&#31038;&#21306;&#26816;&#27979;(ABCD)&#22270;&#26159;&#19968;&#31181;&#26368;&#36817;&#24341;&#20837;&#30340;&#24102;&#26377;&#31038;&#21306;&#32467;&#26500;&#21644;&#24230;&#21644;&#31038;&#21306;&#22823;&#23567;&#30340;&#24130;&#24459;&#20998;&#24067;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#29983;&#25104;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;LFR&#27169;&#22411;&#20855;&#26377;&#31867;&#20284;&#29305;&#24615;&#30340;&#22270;&#24418;&#65292;&#20854;&#20027;&#35201;&#21442;&#25968;&#21487;&#20197;&#35843;&#33410;&#20197;&#27169;&#20223;LFR&#27169;&#22411;&#20013;&#30340;&#28151;&#21512;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;ABCD&#27169;&#22411;&#30340;&#36229;&#22270;&#23545;&#24212;&#29289;h-ABCD&#65292;&#23427;&#29983;&#25104;&#31526;&#21512;&#24130;&#24459;&#20998;&#24067;&#30340;&#22522;&#30784;&#30495;&#23454;&#31038;&#21306;&#22823;&#23567;&#21644;&#24230;&#25968;&#30340;&#38543;&#26426;&#36229;&#22270;&#12290;&#19982;&#21407;&#22987;ABCD&#19968;&#26679;&#65292;&#26032;&#27169;&#22411;h-ABCD&#21487;&#20197;&#20135;&#29983;&#21508;&#31181;&#22122;&#22768;&#27700;&#24179;&#30340;&#36229;&#22270;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#27169;&#20223;&#36229;&#36793;&#33853;&#20837;&#19968;&#20010;&#31038;&#21306;&#30340;&#20219;&#20309;&#25152;&#38656;&#22343;&#21248;&#24615;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20316;&#36866;&#24403;&#30340;&#21512;&#25104;&#8220;&#28216;&#20048;&#22330;&#8221;&#65292;&#20197;&#20998;&#26512;&#21644;&#35843;&#25972;&#36229;&#22270;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Artificial Benchmark for Community Detection (ABCD) graph is a recently introduced random graph model with community structure and power-law distribution for both degrees and community sizes. The model generates graphs with similar properties as the well-known LFR one, and its main parameter can be tuned to mimic its counterpart in the LFR model, the mixing parameter. In this paper, we introduce hypergraph counterpart of the ABCD model, h-ABCD, which produces random hypergraph with distributions of ground-truth community sizes and degrees following power-law. As in the original ABCD, the new model h-ABCD can produce hypergraphs with various levels of noise. More importantly, the model is flexible and can mimic any desired level of homogeneity of hyperedges that fall into one community. As a result, it can be used as a suitable, synthetic playground for analyzing and tuning hypergraph community detection algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#35268;&#21017;&#31354;&#38388;&#37319;&#26679;&#30340;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#22343;&#36798;&#21040;&#20102;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.11269</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#29289;&#29702;&#31995;&#32479;&#24314;&#27169;&#65306;&#25913;&#36827;&#30340;&#28508;&#22312;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Attention-based Modeling of Physical Systems: Improved Latent Representations. (arXiv:2210.11269v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#35268;&#21017;&#31354;&#38388;&#37319;&#26679;&#30340;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#22343;&#36798;&#21040;&#20102;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#31354;&#38388;&#28857;&#19978;&#30340;&#23646;&#24615;&#65292;&#24182;&#26681;&#25454;&#19981;&#21516;&#20301;&#32622;&#30340;&#30456;&#20851;&#27979;&#37327;&#32467;&#26524;&#36827;&#34892;&#26465;&#20214;&#24314;&#27169;&#12290;&#25105;&#20204;&#23558;&#19968;&#20010;&#36716;&#25442;-&#32534;&#30721;&#22120;&#24212;&#29992;&#20110;&#22788;&#29702;&#27979;&#37327;&#21644;&#35835;&#20986;&#20301;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#23545;&#27979;&#37327;&#20540;&#21644;&#35835;&#20986;&#20301;&#32622;&#24212;&#29992;&#30456;&#21516;&#30340;&#36716;&#25442;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#19982;&#32534;&#30721;&#21518;&#30340;&#27979;&#37327;&#20540;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#39640;&#31354;&#39118;&#39044;&#27979;&#12289;&#22825;&#27668;&#39044;&#27979;&#12289;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#28909;&#25193;&#25955;&#31561;&#39046;&#22495;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose attention-based modeling of quantities at arbitrary spatial points conditioned on related measurements at different locations. Our approach adapts a transformer-encoder to process measurements and read-out positions together. Attention-based models exhibit excellent performance across domains, which makes them an interesting candidate for modeling data irregularly sampled in space. We introduce a novel encoding strategy that applies the same transformation to the measurements and read-out positions, after which they are combined with encoded measurement values instead of relying on two different mappings.  Efficiently learning input-output mappings from irregularly-spaced data is a fundamental challenge in modeling physical phenomena. To evaluate the effectiveness of our model, we conduct experiments on diverse problem domains, including high-altitude wind nowcasting, two-days weather forecasting, fluid dynamics, and heat diffusion. Our attention-based model consistently out
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31038;&#21306;&#26816;&#27979;&#30446;&#26631;&#19982;&#20854;&#23545;&#24212;&#30340;&#38544;&#24335;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30456;&#32852;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35745;&#31639;&#32593;&#32476;&#22312;&#20219;&#24847;&#30446;&#26631;&#19979;&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#35775;&#38382;&#38544;&#24335;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.09186</link><description>&lt;p&gt;
&#38544;&#24335;&#27169;&#22411;&#12289;&#28508;&#22312;&#21387;&#32553;&#12289;&#20869;&#22312;&#20559;&#24046;&#21644;&#24265;&#20215;&#21320;&#39184;&#22312;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Implicit models, latent compression, intrinsic biases, and cheap lunches in community detection. (arXiv:2210.09186v6 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31038;&#21306;&#26816;&#27979;&#30446;&#26631;&#19982;&#20854;&#23545;&#24212;&#30340;&#38544;&#24335;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30456;&#32852;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35745;&#31639;&#32593;&#32476;&#22312;&#20219;&#24847;&#30446;&#26631;&#19979;&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#35775;&#38382;&#38544;&#24335;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#30340;&#20219;&#21153;&#26088;&#22312;&#23558;&#32593;&#32476;&#21010;&#20998;&#20026;&#33410;&#28857;&#38598;&#32676;&#65292;&#20197;&#24635;&#32467;&#20854;&#22823;&#35268;&#27169;&#32467;&#26500;&#65292;&#24050;&#32463;&#24341;&#20986;&#20102;&#35768;&#22810;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#31454;&#20105;&#31639;&#27861;&#12290; &#19968;&#20123;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#26159;&#25512;&#26029;&#24615;&#30340;&#65292;&#36890;&#36807;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#26126;&#30830;&#22320;&#23548;&#20986;&#32858;&#31867;&#30446;&#26631;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#26159;&#25551;&#36848;&#24615;&#30340;&#65292;&#26681;&#25454;&#29305;&#23450;&#24212;&#29992;&#30340;&#30446;&#26631;&#23558;&#32593;&#32476;&#20998;&#25104;&#23376;&#38598;&#65292;&#36825;&#20351;&#24471;&#22312;&#21516;&#19968;&#35268;&#27169;&#19979;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#20219;&#20309;&#31038;&#21306;&#26816;&#27979;&#30446;&#26631;&#65288;&#25512;&#26029;&#24615;&#25110;&#25551;&#36848;&#24615;&#65289;&#19982;&#20854;&#30456;&#24212;&#30340;&#38544;&#24335;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30456;&#32852;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35745;&#31639;&#32593;&#32476;&#21450;&#20854;&#22312;&#20219;&#24847;&#30446;&#26631;&#19979;&#30340;&#20998;&#21306;&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#26080;&#38656;&#8220;&#22320;&#38754;&#23454;&#20917;&#8221;&#26631;&#31614;&#21363;&#21487;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#35775;&#38382;&#38544;&#24335;&#27169;&#22411;&#65292;&#36825;&#26159;&#20854;&#20182;&#26041;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of community detection, which aims to partition a network into clusters of nodes to summarize its large-scale structure, has spawned the development of many competing algorithms with varying objectives. Some community detection methods are inferential, explicitly deriving the clustering objective through a probabilistic generative model, while other methods are descriptive, dividing a network according to an objective motivated by a particular application, making it challenging to compare these methods on the same scale. Here we present a solution to this problem that associates any community detection objective, inferential or descriptive, with its corresponding implicit network generative model. This allows us to compute the description length of a network and its partition under arbitrary objectives, providing a principled measure to compare the performance of different algorithms without the need for "ground truth" labels. Our approach also gives access to instances of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#21464;&#26377;&#21521;&#32593;&#32476;&#30340;&#20998;&#25955;&#24335;&#36229;&#26799;&#24230;&#35745;&#31639;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#38745;&#24577;&#26080;&#21521;&#32593;&#32476;&#36890;&#20449; Hessian &#30697;&#38453;&#23548;&#33268;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#21644;&#26080;&#27861;&#20351;&#29992;&#26102;&#21464;&#26377;&#21521;&#32593;&#32476;&#20248;&#21183;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.02129</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#21464;&#26377;&#21521;&#32593;&#32476;&#30340;&#20998;&#25955;&#24335;&#36229;&#26799;&#24230;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Decentralized Hyper-Gradient Computation over Time-Varying Directed Networks. (arXiv:2210.02129v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#21464;&#26377;&#21521;&#32593;&#32476;&#30340;&#20998;&#25955;&#24335;&#36229;&#26799;&#24230;&#35745;&#31639;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#38745;&#24577;&#26080;&#21521;&#32593;&#32476;&#36890;&#20449; Hessian &#30697;&#38453;&#23548;&#33268;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#21644;&#26080;&#27861;&#20351;&#29992;&#26102;&#21464;&#26377;&#21521;&#32593;&#32476;&#20248;&#21183;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#20272;&#35745;&#36229;&#26799;&#24230;&#26102;&#30340;&#36890;&#20449;&#38382;&#39064;&#12290;&#22312;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36229;&#26799;&#24230;&#37327;&#21270;&#20102;&#20840;&#23616;&#20849;&#20139;&#26368;&#20248;&#27169;&#22411;&#30340;&#24615;&#33021;&#22914;&#20309;&#21463;&#21040;&#23458;&#25143;&#31471;&#36229;&#21442;&#25968;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#23458;&#25143;&#31471;&#36890;&#36807;&#22312;&#38745;&#24577;&#26080;&#21521;&#32593;&#32476;&#19978;&#36890;&#20449; Hess &#30697;&#38453;&#26469;&#36319;&#36394;&#36825;&#31181;&#24433;&#21709;&#65292;&#23548;&#33268;&#20102;&#65288;i&#65289;&#36807;&#39640;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#65288;ii&#65289;&#19981;&#33021;&#21033;&#29992;&#26356;&#39640;&#25928;&#21644;&#26356;&#24378;&#22823;&#30340;&#32593;&#32476;&#65292;&#21363;&#26102;&#21464;&#26377;&#21521;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#24179;&#22343;&#25805;&#20316;&#30340; FL &#26367;&#20195;&#24615;&#20248;&#21270;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992; Push-Sum &#20316;&#20026;&#24179;&#22343;&#25805;&#20316;&#65292;&#22312;&#26102;&#21464;&#26377;&#21521;&#32593;&#32476;&#19978;&#36827;&#34892;&#20849;&#35782;&#20248;&#21270;&#25216;&#26415;&#12290;&#22240;&#27492;&#65292;&#20174;&#25105;&#20204;&#30340;&#26368;&#20248;&#26465;&#20214;&#25512;&#23548;&#20986;&#30340;&#36229;&#26799;&#24230;&#20272;&#35745;&#22120;&#20855;&#26377;&#20004;&#20010;&#29702;&#24819;&#29305;&#24615;&#65292;&#65288;i&#65289;&#23427;&#21482;&#38656;&#35201; Push-Sum &#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the communication issues when estimating hyper-gradients in decentralized federated learning (FL). Hyper-gradients in decentralized FL quantifies how the performance of globally shared optimal model is influenced by the perturbations in clients' hyper-parameters. In prior work, clients trace this influence through the communication of Hessian matrices over a static undirected network, resulting in (i) excessive communication costs and (ii) inability to make use of more efficient and robust networks, namely, time-varying directed networks. To solve these issues, we introduce an alternative optimality condition for FL using an averaging operation on model parameters and gradients. We then employ Push-Sum as the averaging operation, which is a consensus optimization technique for time-varying directed networks. As a result, the hyper-gradient estimator derived from our optimality condition enjoys two desirable properties; (i) it only requires Push-Sum communication of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#23398;&#29983;t&#20998;&#24067;&#26469;&#27169;&#25311;&#27880;&#37322;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#25512;&#23548;&#30456;&#24212;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25429;&#25417;&#24773;&#24863;&#35782;&#21035;&#20013;&#22522;&#20110;&#20027;&#35266;&#24615;&#30340;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.15449</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#31471;&#21040;&#31471;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
End-to-End Label Uncertainty Modeling in Speech Emotion Recognition using Bayesian Neural Networks and Label Distribution Learning. (arXiv:2209.15449v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#23398;&#29983;t&#20998;&#24067;&#26469;&#27169;&#25311;&#27880;&#37322;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#25512;&#23548;&#30456;&#24212;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25429;&#25417;&#24773;&#24863;&#35782;&#21035;&#20013;&#22522;&#20110;&#20027;&#35266;&#24615;&#30340;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#39044;&#27979;&#24773;&#32490;&#34920;&#36798;&#26041;&#38754;&#30340;&#21796;&#37266;&#24230;&#21644;&#20215;&#20540;&#26102;&#65292;&#38656;&#35201;&#26377;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#20154;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#24863;&#30693;&#20182;&#20154;&#30340;&#24773;&#24863;&#34920;&#36798;&#65292;&#20182;&#20204;&#30340;&#27880;&#37322;&#26159;&#20027;&#35266;&#30340;&#12290;&#20026;&#20102;&#32771;&#34385;&#36825;&#19968;&#28857;&#65292;&#36890;&#24120;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#25910;&#38598;&#27880;&#37322;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#24179;&#22343;&#20197;&#33719;&#24471;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#24403;&#20165;&#22312;&#36825;&#20010;&#24179;&#22343;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#23545;&#24773;&#24863;&#34920;&#36798;&#20013;&#22266;&#26377;&#30340;&#20027;&#35266;&#24615;&#26159;&#19981;&#21487;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#27880;&#37322;&#20998;&#24067;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25429;&#25417;&#22522;&#20110;&#20027;&#35266;&#24615;&#30340;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23398;&#29983;t&#20998;&#24067;&#26469;&#27169;&#25311;&#27880;&#37322;&#20998;&#24067;&#65292;&#32780;&#19981;&#26159;&#39640;&#26031;&#20998;&#24067;&#65292;&#36825;&#20063;&#32771;&#34385;&#21040;&#20102;&#21487;&#29992;&#27880;&#37322;&#25968;&#37327;&#12290;&#25105;&#20204;&#25512;&#23548;&#30456;&#24212;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#27880;&#37322;&#20998;&#24067;&#30340;&#20272;&#35745;&#22120;&#65292;&#20174;&#20013;&#33719;&#24471;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
To train machine learning algorithms to predict emotional expressions in terms of arousal and valence, annotated datasets are needed. However, as different people perceive others' emotional expressions differently, their annotations are subjective. To account for this, annotations are typically collected from multiple annotators and averaged to obtain ground-truth labels. However, when exclusively trained on this averaged ground-truth, the model is agnostic to the inherent subjectivity in emotional expressions. In this work, we therefore propose an end-to-end Bayesian neural network capable of being trained on a distribution of annotations to also capture the subjectivity-based label uncertainty. Instead of a Gaussian, we model the annotation distribution using Student's t-distribution, which also accounts for the number of annotations available. We derive the corresponding Kullback-Leibler divergence loss and use it to train an estimator for the annotation distribution, from which the
&lt;/p&gt;</description></item><item><title>&#22312;&#26410;&#30693;&#29615;&#22659;&#19979;&#65292;&#36890;&#36807;&#36719;&#38556;&#30861;&#20989;&#25968;&#24378;&#21046;&#23454;&#26045;&#30828;&#23433;&#20840;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#29615;&#22659;&#21644;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2209.15090</link><description>&lt;p&gt;
&#22312;&#26410;&#30693;&#38543;&#26426;&#29615;&#22659;&#20013;&#20351;&#29992;&#36719;&#38556;&#30861;&#24378;&#21046;&#25191;&#34892;&#30828;&#32422;&#26463;&#65306;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement Learning in Unknown Stochastic Environments. (arXiv:2209.15090v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15090
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#30693;&#29615;&#22659;&#19979;&#65292;&#36890;&#36807;&#36719;&#38556;&#30861;&#20989;&#25968;&#24378;&#21046;&#23454;&#26045;&#30828;&#23433;&#20840;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#29615;&#22659;&#21644;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35201;&#27714;&#31995;&#32479;&#29366;&#24577;&#19981;&#21040;&#36798;&#26576;&#20123;&#25351;&#23450;&#30340;&#19981;&#23433;&#20840;&#21306;&#22495;&#30340;&#30828;&#32422;&#26463;&#19979;&#65292;&#30830;&#20445;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26410;&#30693;&#21644;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#24615;&#26159;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#35768;&#22810;&#27969;&#34892;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#33539;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#23433;&#20840;&#36829;&#35268;&#24418;&#24335;&#21270;&#20026;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#35797;&#22270;&#23558;&#32047;&#31215;&#25104;&#26412;&#30340;&#26399;&#26395;&#38480;&#21046;&#22312;&#38408;&#20540;&#19979;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#26679;&#23545;&#23433;&#20840;&#36829;&#35268;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#38388;&#25509;&#22320;&#25429;&#25417;&#21644;&#24378;&#21046;&#25191;&#34892;&#30828;&#21487;&#36798;&#24615;&#23433;&#20840;&#32422;&#26463;&#36890;&#24120;&#24456;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#38556;&#30861;&#20989;&#25968;&#30340;&#27010;&#24565;&#26469;&#26174;&#24335;&#22320;&#32534;&#30721;&#30828;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#22312;&#29615;&#22659;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23427;&#20204;&#25918;&#26494;&#21040;&#25105;&#20204;&#35774;&#35745;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36719;&#38556;&#30861;&#20989;&#25968;&#20013;&#12290;&#22522;&#20110;&#36825;&#26679;&#30340;&#36719;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#29615;&#22659;&#21644;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#23454;&#26045;&#30828;&#23433;&#20840;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is quite challenging to ensure the safety of reinforcement learning (RL) agents in an unknown and stochastic environment under hard constraints that require the system state not to reach certain specified unsafe regions. Many popular safe RL methods such as those based on the Constrained Markov Decision Process (CMDP) paradigm formulate safety violations in a cost function and try to constrain the expectation of cumulative cost under a threshold. However, it is often difficult to effectively capture and enforce hard reachability-based safety constraints indirectly with such constraints on safety violation costs. In this work, we leverage the notion of barrier function to explicitly encode the hard safety constraints, and given that the environment is unknown, relax them to our design of \emph{generative-model-based soft barrier functions}. Based on such soft barriers, we propose a safe RL approach that can jointly learn the environment and optimize the control policy, while effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;Fisher-Rao&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#26368;&#20339;&#36924;&#36817;&#36830;&#32493;&#26102;&#38388;&#22797;&#21046;&#23376;&#26041;&#31243;&#65292;&#36825;&#19968;&#23545;&#24212;&#20851;&#31995;&#31216;&#20026;&#8220;&#20849;&#36717;&#33258;&#28982;&#36873;&#25321;&#8221;&#65292;&#20026;&#36827;&#21270;&#35745;&#31639;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36830;&#32493;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#26368;&#20339;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2208.13898</link><description>&lt;p&gt;
&#20849;&#36717;&#33258;&#28982;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Conjugate Natural Selection. (arXiv:2208.13898v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Fisher-Rao&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#26368;&#20339;&#36924;&#36817;&#36830;&#32493;&#26102;&#38388;&#22797;&#21046;&#23376;&#26041;&#31243;&#65292;&#36825;&#19968;&#23545;&#24212;&#20851;&#31995;&#31216;&#20026;&#8220;&#20849;&#36717;&#33258;&#28982;&#36873;&#25321;&#8221;&#65292;&#20026;&#36827;&#21270;&#35745;&#31639;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36830;&#32493;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#26368;&#20339;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;Fisher-Rao&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;FR-NGD&#65289;&#26368;&#20339;&#36924;&#36817;&#20102;&#36830;&#32493;&#26102;&#38388;&#22797;&#21046;&#23376;&#26041;&#31243;&#65288;&#19968;&#31181;&#22522;&#26412;&#30340;&#36827;&#21270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65289;&#65292;&#24182;&#23558;&#27492;&#23545;&#24212;&#20851;&#31995;&#31216;&#20026;&#8220;&#20849;&#36717;&#33258;&#28982;&#36873;&#25321;&#8221;&#12290;&#35813;&#23545;&#24212;&#20851;&#31995;&#20026;&#22312;&#36830;&#32493;&#25110;&#39640;&#32500;&#24230;&#20551;&#35774;&#31354;&#38388;&#19978;&#36827;&#34892;&#36827;&#21270;&#35745;&#31639;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#20010;&#29305;&#20363;&#65292;FR-NGD&#36824;&#25552;&#20379;&#20102;&#36830;&#32493;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#26368;&#20339;&#36817;&#20284;&#65292;&#24403;&#20551;&#35774;&#22522;&#20110;&#39044;&#27979;&#23454;&#38469;&#35266;&#27979;&#32467;&#26524;&#32780;&#30456;&#20114;&#31454;&#20105;&#26102;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#35745;&#31639;&#20808;&#39564;&#27010;&#29575;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#21644;&#19968;&#20010;&#20855;&#26377;&#26102;&#21464;&#21442;&#25968;&#30340;&#38543;&#26426;&#36807;&#31243;&#30340;&#31995;&#32479;&#35782;&#21035;&#20219;&#21153;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove that Fisher-Rao natural gradient descent (FR-NGD) optimally approximates the continuous time replicator equation (an essential model of evolutionary dynamics), and term this correspondence "conjugate natural selection". This correspondence promises alternative approaches for evolutionary computation over continuous or high-dimensional hypothesis spaces. As a special case, FR-NGD also provides the optimal approximation of continuous Bayesian inference when hypotheses compete on the basis of predicting actual observations. In this case, the method avoids the need to compute prior probabilities. We demonstrate our findings on a non-convex optimization problem and a system identification task for a stochastic process with time-varying parameters.
&lt;/p&gt;</description></item><item><title>HELP ME THINK&#26159;&#19968;&#31181;&#24110;&#21161;&#38750;&#19987;&#23478;&#29992;&#25143;&#21019;&#24314;&#23450;&#21046;&#21270;&#20869;&#23481;&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;&#65292;&#21033;&#29992;GPT3&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#21644;&#21033;&#29992;&#29992;&#25143;&#31572;&#26696;&#25191;&#34892;&#20219;&#21153;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38656;&#35201;&#37325;&#35201;&#24605;&#32771;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2208.08232</link><description>&lt;p&gt;
HELP ME THINK&#65306;&#19968;&#31181;&#24110;&#21161;&#38750;&#19987;&#23478;&#20351;&#29992;&#27169;&#22411;&#21019;&#24314;&#23450;&#21046;&#20869;&#23481;&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models. (arXiv:2208.08232v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08232
&lt;/p&gt;
&lt;p&gt;
HELP ME THINK&#26159;&#19968;&#31181;&#24110;&#21161;&#38750;&#19987;&#23478;&#29992;&#25143;&#21019;&#24314;&#23450;&#21046;&#21270;&#20869;&#23481;&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;&#65292;&#21033;&#29992;GPT3&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#21644;&#21033;&#29992;&#29992;&#25143;&#31572;&#26696;&#25191;&#34892;&#20219;&#21153;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38656;&#35201;&#37325;&#35201;&#24605;&#32771;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#24182;&#23450;&#21046;&#20869;&#23481;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20986;&#20102;&#20026;&#20102;&#25552;&#20379;&#25511;&#21046;&#32780;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#32570;&#20047;&#19968;&#33324;&#24615;&#65307;&#36825;&#20026;&#38750;&#19987;&#19994;&#29992;&#25143;&#25214;&#21040;&#36866;&#21512;&#20854;&#20219;&#21153;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#21387;&#20498;&#24615;&#30340;&#36873;&#25321;&#12290;&#36825;&#20123;&#25216;&#26415;&#25152;&#28041;&#21450;&#30340;&#24037;&#20316;&#65292;&#22914;&#32534;&#20889;&#31034;&#20363;&#12289;&#35299;&#37322;&#12289;&#25351;&#20196;&#31561;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38750;&#19987;&#19994;&#29992;&#25143;&#20013;&#30340;&#37319;&#29992;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HELP ME THINK&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;&#65292;&#40723;&#21169;GPT3&#36890;&#36807;&#25552;&#20986;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#31572;&#26696;&#26469;&#25191;&#34892;&#20219;&#21153;&#26469;&#24110;&#21161;&#38750;&#19987;&#23478;&#29992;&#25143;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;HELP ME THINK&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#21151;&#25928;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#38590;&#20197;&#23436;&#25104;&#19988;&#38656;&#35201;&#37325;&#35201;&#24605;&#32771;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#40723;&#21169;&#24320;&#21457;&#38750;&#20256;&#32479;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task. The effort associated with those techniques, such as in writing examples, explanations, instructions, etc. further limits their adoption among non-expert users. In this paper, we propose a simple prompting strategy HELP ME THINK where we encourage GPT3 to help non-expert users by asking a set of relevant questions and leveraging user answers to execute the task. We demonstrate the efficacy of our technique HELP ME THINK on a variety of tasks. Specifically, we focus on tasks that are hard for average humans and require significant thinking to perform. We hope our work will encourage the development of unconventional ways to harness the power of large language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#22122;&#22768;&#35821;&#38899;&#21644;&#39640;&#26031;&#22122;&#22768;&#36827;&#34892;&#21453;&#21521;&#36807;&#31243;&#65292;&#20165;&#20351;&#29992;30&#27493;&#23601;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#35821;&#38899;&#20272;&#35745;&#12290;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#26041;&#27861;&#30340;&#31454;&#20105;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2208.05830</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#38899;&#22686;&#24378;&#21644;&#21435;&#28151;&#21709;
&lt;/p&gt;
&lt;p&gt;
Speech Enhancement and Dereverberation with Diffusion-based Generative Models. (arXiv:2208.05830v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#22122;&#22768;&#35821;&#38899;&#21644;&#39640;&#26031;&#22122;&#22768;&#36827;&#34892;&#21453;&#21521;&#36807;&#31243;&#65292;&#20165;&#20351;&#29992;30&#27493;&#23601;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#35821;&#38899;&#20272;&#35745;&#12290;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#26041;&#27861;&#30340;&#31454;&#20105;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#20043;&#21069;&#30340;&#20986;&#29256;&#29289;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#38899;&#22686;&#24378;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#22522;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29702;&#35770;&#25506;&#35752;&#20854;&#21547;&#20041;&#12290;&#19982;&#36890;&#24120;&#30340;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#30456;&#21453;&#65292;&#25105;&#20204;&#19981;&#26159;&#20174;&#32431;&#39640;&#26031;&#22122;&#22768;&#24320;&#22987;&#21453;&#21521;&#36807;&#31243;&#65292;&#32780;&#26159;&#20174;&#22122;&#22768;&#35821;&#38899;&#21644;&#39640;&#26031;&#22122;&#22768;&#30340;&#28151;&#21512;&#29289;&#24320;&#22987;&#12290;&#36825;&#19982;&#25105;&#20204;&#30340;&#27491;&#21521;&#36807;&#31243;&#30456;&#21305;&#37197;&#65292;&#35813;&#36807;&#31243;&#36890;&#36807;&#21253;&#25324;&#28418;&#31227;&#39033;&#23558;&#24178;&#20928;&#35821;&#38899;&#36716;&#21464;&#25104;&#22122;&#22768;&#35821;&#38899;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20010;&#36807;&#31243;&#20351;&#24471;&#20351;&#29992;&#20165;30&#20010;&#25193;&#25955;&#27493;&#39588;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#35821;&#38899;&#20272;&#35745;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#32593;&#32476;&#32780;&#19981;&#26159;&#24418;&#24335;&#20027;&#20041;&#26159;&#25105;&#20204;&#21407;&#22987;&#26041;&#27861;&#30340;&#20027;&#35201;&#38480;&#21046;&#12290;&#22312;&#24191;&#27867;&#30340;&#36328;&#25968;&#25454;&#38598;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#21518;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#26368;&#26032;&#30340;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we build upon our previous publication and use diffusion-based generative models for speech enhancement. We present a detailed overview of the diffusion process that is based on a stochastic differential equation and delve into an extensive theoretical examination of its implications. Opposed to usual conditional generation tasks, we do not start the reverse process from pure Gaussian noise but from a mixture of noisy speech and Gaussian noise. This matches our forward process which moves from clean speech to noisy speech by including a drift term. We show that this procedure enables using only 30 diffusion steps to generate high-quality clean speech estimates. By adapting the network architecture, we are able to significantly improve the speech enhancement performance, indicating that the network, rather than the formalism, was the main limitation of our original approach. In an extensive cross-dataset evaluation, we show that the improved method can compete with recent 
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#36830;&#32493;&#23398;&#20064;&#21644;&#21151;&#33021;&#32452;&#21512;&#26041;&#27861;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#26410;&#26469;&#32852;&#31995;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#35299;&#20915;&#26032;&#38382;&#39064;&#26102;&#21487;&#20197;&#19981;&#26029;&#31215;&#32047;&#21644;&#32452;&#21512;&#30693;&#35782;&#30340;&#32456;&#36523;&#23398;&#20064;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2207.07730</link><description>&lt;p&gt;
&#22914;&#20309;&#37325;&#29992;&#21644;&#32452;&#21512;&#30693;&#35782;&#65292;&#23454;&#29616;&#32456;&#36523;&#20219;&#21153;&#23398;&#20064;&#65306;&#32508;&#36848;&#36830;&#32493;&#23398;&#20064;&#19982;&#21151;&#33021;&#32452;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition. (arXiv:2207.07730v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#36830;&#32493;&#23398;&#20064;&#21644;&#21151;&#33021;&#32452;&#21512;&#26041;&#27861;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#26410;&#26469;&#32852;&#31995;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#35299;&#20915;&#26032;&#38382;&#39064;&#26102;&#21487;&#20197;&#19981;&#26029;&#31215;&#32047;&#21644;&#32452;&#21512;&#30693;&#35782;&#30340;&#32456;&#36523;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#26159;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#33719;&#24471;&#23545;&#19990;&#30028;&#30340;&#26222;&#36941;&#29702;&#35299;&#30340;&#20195;&#29702;&#12290;&#36825;&#31181;&#20195;&#29702;&#38656;&#35201;&#33021;&#22815;&#19981;&#26029;&#31215;&#32047;&#21644;&#24314;&#31435;&#30693;&#35782;&#65292;&#20197;&#24212;&#23545;&#36935;&#21040;&#30340;&#26032;&#20307;&#39564;&#12290;&#32456;&#36523;&#25110;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#20102;&#36825;&#31181;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#38754;&#23545;&#19981;&#26029;&#30340;&#38382;&#39064;&#27969;&#65292;&#24517;&#39035;&#21162;&#21147;&#25484;&#25569;&#35299;&#20915;&#27599;&#20010;&#26032;&#20219;&#21153;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#22914;&#26524;&#20195;&#29702;&#33021;&#22815;&#22312;&#26576;&#31181;&#32452;&#21512;&#34920;&#31034;&#24418;&#24335;&#20013;&#32047;&#31215;&#30693;&#35782;&#65292;&#21017;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#37325;&#29992;&#21644;&#32452;&#21512;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#26500;&#24314;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#36825;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#20855;&#26377;&#30452;&#35266;&#30340;&#21560;&#24341;&#21147;&#65292;&#20294;&#26159;&#26377;&#20851;&#32456;&#36523;&#23398;&#20064;&#21644;&#32452;&#21512;&#23398;&#20064;&#30340;&#25991;&#29486;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#20998;&#24320;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#20419;&#36827;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#26412;&#25991;&#27010;&#36848;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#30740;&#31350;&#26223;&#35266;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#20043;&#38388;&#29616;&#26377;&#21644;&#26410;&#26469;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major goal of artificial intelligence (AI) is to create an agent capable of acquiring a general understanding of the world. Such an agent would require the ability to continually accumulate and build upon its knowledge as it encounters new experiences. Lifelong or continual learning addresses this setting, whereby an agent faces a continual stream of problems and must strive to capture the knowledge necessary for solving each new task it encounters. If the agent is capable of accumulating knowledge in some form of compositional representation, it could then selectively reuse and combine relevant pieces of knowledge to construct novel solutions. Despite the intuitive appeal of this simple idea, the literatures on lifelong learning and compositional learning have proceeded largely separately. In an effort to promote developments that bridge between the two fields, this article surveys their respective research landscapes and discusses existing and future connections between them.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21457;&#29616;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#23398;&#20064;&#22270;&#20687;&#22359;&#30340;&#34920;&#31034;&#65292;&#21453;&#26144;&#20986;&#23427;&#20204;&#30340;&#20849;&#21516;&#20986;&#29616;&#12290;&#23398;&#20064;&#22266;&#23450;&#23610;&#24230;&#22270;&#20687;&#22359;&#30340;&#34920;&#31034;&#65292;&#24182;&#23558;&#23616;&#37096;&#34920;&#31034;&#32858;&#21512;&#20026;&#22270;&#20687;&#34920;&#31034;&#65292;&#31216;&#20026;BagSSL&#65292;&#33021;&#22815;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.08954</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#22359;&#23884;&#20837;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#32972;&#21518;&#30340;&#25104;&#21151;&#20043;&#36947;
&lt;/p&gt;
&lt;p&gt;
Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning. (arXiv:2206.08954v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08954
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21457;&#29616;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#23398;&#20064;&#22270;&#20687;&#22359;&#30340;&#34920;&#31034;&#65292;&#21453;&#26144;&#20986;&#23427;&#20204;&#30340;&#20849;&#21516;&#20986;&#29616;&#12290;&#23398;&#20064;&#22266;&#23450;&#23610;&#24230;&#22270;&#20687;&#22359;&#30340;&#34920;&#31034;&#65292;&#24182;&#23558;&#23616;&#37096;&#34920;&#31034;&#32858;&#21512;&#20026;&#22270;&#20687;&#34920;&#31034;&#65292;&#31216;&#20026;BagSSL&#65292;&#33021;&#22815;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#22312;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#32463;&#39564;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#23398;&#20064;&#36825;&#31181;&#34920;&#31034;&#32972;&#21518;&#30340;&#21407;&#29702;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#32852;&#21512;&#23884;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#23398;&#20064;&#22270;&#20687;&#22359;&#30340;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21453;&#26144;&#20102;&#23427;&#20204;&#30340;&#20849;&#21516;&#20986;&#29616;&#12290;&#36825;&#31181;&#19982;&#20849;&#29616;&#24314;&#27169;&#30340;&#32852;&#31995;&#21487;&#20197;&#34987;&#27491;&#24335;&#22320;&#24314;&#31435;&#65292;&#24182;&#19988;&#23427;&#34917;&#20805;&#20102;&#20027;&#27969;&#30340;&#19981;&#21464;&#24615;&#35270;&#35282;&#12290;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#34920;&#26126;&#65292;&#23398;&#20064;&#22266;&#23450;&#23610;&#24230;&#22270;&#20687;&#22359;&#30340;&#34920;&#31034;&#65292;&#24182;&#23558;&#23616;&#37096;&#22270;&#20687;&#22359;&#34920;&#31034;&#32858;&#21512;&#20026;&#22270;&#20687;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#27492;&#36807;&#31243;&#31216;&#20026;BagSSL&#12290;&#21363;&#20351;&#20351;&#29992;32x32&#30340;&#22359;&#34920;&#31034;&#65292;&#22312;ImageNet&#19978;&#65292;BagSSL&#20063;&#33021;&#22815;&#36798;&#21040;62%&#30340;top-1&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#29575;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25972;&#20010;&#22270;&#20687;&#23884;&#20837;&#22823;&#33268;&#19978;&#26159;&#23616;&#37096;&#22270;&#20687;&#22359;&#23884;&#20837;&#32467;&#26524;&#30340;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently achieved tremendous empirical advancements in learning image representation. However, our understanding of the principle behind learning such a representation is still limited. This work shows that joint-embedding SSL approaches primarily learn a representation of image patches, which reflects their co-occurrence. Such a connection to co-occurrence modeling can be established formally, and it supplements the prevailing invariance perspective. We empirically show that learning a representation for fixed-scale patches and aggregating local patch representations as the image representation achieves similar or even better results than the baseline methods. We denote this process as BagSSL. Even with 32x32 patch representation, BagSSL achieves 62% top-1 linear probing accuracy on ImageNet. On the other hand, with a multi-scale pretrained model, we show that the whole image embedding is approximately the average of local patch embeddings. While the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#32473;&#23450;&#19968;&#20123;&#36793;&#30340;&#26041;&#21521;&#24050;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#20013;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#25968;&#37327;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#25968;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#21463;&#21040;&#19968;&#20010;&#22810;&#39033;&#24335;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2206.06744</link><description>&lt;p&gt;
&#19982;&#32972;&#26223;&#30693;&#35782;&#19968;&#33268;&#30340;&#35745;&#25968;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#26377;&#21521;&#26080;&#29615;&#22270;
&lt;/p&gt;
&lt;p&gt;
Counting Markov Equivalent Directed Acyclic Graphs Consistent with Background Knowledge. (arXiv:2206.06744v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#32473;&#23450;&#19968;&#20123;&#36793;&#30340;&#26041;&#21521;&#24050;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#20013;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#25968;&#37327;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#25968;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#21463;&#21040;&#19968;&#20010;&#22810;&#39033;&#24335;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Wienobst&#12289;Bannach&#21644;Liskiewicz&#65288;AAAI 2021&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#20013;&#26377;&#21521;&#26080;&#29615;&#22270;&#25968;&#37327;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31934;&#30830;&#31639;&#27861;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#26356;&#19968;&#33324;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19968;&#20123;&#36793;&#30340;&#26041;&#21521;&#24050;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#20013;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#25968;&#37327;&#65288;&#20363;&#22914;&#65292;&#22312;&#37096;&#20998;&#24178;&#39044;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#36825;&#31181;&#35774;&#32622;&#65289;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#22797;&#26434;&#29702;&#35770;&#19978;&#26159;&#22256;&#38590;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#38382;&#39064;&#22312;&#19968;&#20010;&#26377;&#36259;&#30340;&#23454;&#20363;&#31867;&#20013;&#20173;&#28982;&#26159;&#21487;&#22788;&#29702;&#30340;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#30340;&#8220;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#24615;&#8221;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#35745;&#25968;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#21463;&#21040;&#19968;&#20010;&#22810;&#39033;&#24335;&#30340;&#32422;&#26463;&#65292;&#24182;&#19988;&#36825;&#20010;&#22810;&#39033;&#24335;&#30340;&#24230;&#25968;\emph{&#19981;}&#20381;&#36182;&#20110;&#25552;&#20379;&#30340;&#38468;&#21152;&#36793;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A polynomial-time exact algorithm for counting the number of directed acyclic graphs in a Markov equivalence class was recently given by Wien\"obst, Bannach, and Li\'skiewicz (AAAI 2021). In this paper, we consider the more general problem of counting the number of directed acyclic graphs in a Markov equivalence class when the directions of some of the edges are also fixed (this setting arises, for example, when interventional data is partially available). This problem has been shown in earlier work to be complexity-theoretically hard. In contrast, we show that the problem is nevertheless tractable in an interesting class of instances, by establishing that it is ``fixed-parameter tractable''. In particular, our counting algorithm runs in time that is bounded by a polynomial in the size of the graph, where the degree of the polynomial does \emph{not} depend upon the number of additional edges provided as input.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#30028;&#23450;&#26080;&#27861;&#36890;&#36807;&#28857;&#20272;&#35745;&#36827;&#34892;&#35782;&#21035;&#30340;&#36830;&#32493;&#20540;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#33539;&#22260;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.11206</link><description>&lt;p&gt;
&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#19979;&#21058;&#37327;&#21709;&#24212;&#30340;&#37096;&#20998;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Partial Identification of Dose Responses with Hidden Confounders. (arXiv:2204.11206v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#30028;&#23450;&#26080;&#27861;&#36890;&#36807;&#28857;&#20272;&#35745;&#36827;&#34892;&#35782;&#21035;&#30340;&#36830;&#32493;&#20540;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#33539;&#22260;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#36830;&#32493;&#20540;&#22788;&#29702;&#30340;&#22240;&#26524;&#25928;&#24212;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#26377;&#26395;&#26356;&#22909;&#22320;&#20026;&#25919;&#31574;&#21644;&#20915;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#20449;&#24687;&#12290; &#30830;&#23450;&#36825;&#20123;&#25928;&#24212;&#25152;&#38656;&#30340;&#20851;&#38190;&#20551;&#35774;&#26159;&#21253;&#25324;&#25152;&#26377;&#28151;&#28102;&#21464;&#37327;&#8212;&#8212;&#22788;&#29702;&#21644;&#32467;&#26524;&#30340;&#22240;&#26524;&#29238;&#27597;&#8212;&#8212;&#20316;&#20026;&#21327;&#21464;&#37327;&#12290; &#19981;&#24184;&#30340;&#26159;&#65292;&#20165;&#20973;&#35266;&#27979;&#25968;&#25454;&#65292;&#25105;&#20204;&#26080;&#27861;&#30830;&#23450;&#36825;&#20010;&#26631;&#20934;&#26159;&#21542;&#24471;&#21040;&#28385;&#36275;&#12290; &#24403;&#28151;&#28102;&#21464;&#37327;&#34987;&#38544;&#34255;&#26102;&#65292;&#25935;&#24863;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#26469;&#20026;&#22240;&#26524;&#20272;&#35745;&#25552;&#20379;&#30028;&#38480;&#12290; &#34429;&#28982;&#22312;&#31163;&#25955;&#20540;&#22788;&#29702;&#30340;&#28789;&#25935;&#24230;&#20998;&#26512;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#36830;&#32493;&#20540;&#22788;&#29702;&#65292;&#21364;&#20184;&#20986;&#20102;&#26356;&#23569;&#30340;&#20851;&#27880;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30028;&#23450;&#26080;&#27861;&#36890;&#36807;&#28857;&#20272;&#35745;&#36827;&#34892;&#35782;&#21035;&#30340;&#24179;&#22343;&#21644;&#26465;&#20214;&#24179;&#22343;&#36830;&#32493;&#20540;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#33539;&#22260;&#65292;&#22240;&#20026;&#23384;&#22312;&#38544;&#34255;&#30340;&#28151;&#28102;&#12290; &#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21322;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring causal effects of continuous-valued treatments from observational data is a crucial task promising to better inform policy- and decision-makers. A critical assumption needed to identify these effects is that all confounding variables -- causal parents of both the treatment and the outcome -- are included as covariates. Unfortunately, given observational data alone, we cannot know with certainty that this criterion is satisfied. Sensitivity analyses provide principled ways to give bounds on causal estimates when confounding variables are hidden. While much attention is focused on sensitivity analyses for discrete-valued treatments, much less is paid to continuous-valued treatments. We present novel methodology to bound both average and conditional average continuous-valued treatment-effect estimates when they cannot be point identified due to hidden confounding. A semi-synthetic benchmark on multiple datasets shows our method giving tighter coverage of the true dose-response c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25214;&#21040;&#20102;&#24102;&#26435;&#37325;&#34928;&#20943;&#21644;&#38543;&#26426;&#31070;&#32463;&#20803;&#30340;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#32467;&#26524;&#34920;&#26126;&#26435;&#37325;&#34928;&#20943;&#19982;&#27169;&#22411;&#26550;&#26500;&#30340;&#24378;&#28872;&#20132;&#20114;&#20316;&#29992;&#20250;&#22312;&#22810;&#20110;1&#20010;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#20013;&#21019;&#24314;&#19981;&#33391;&#26497;&#23567;&#20540;&#65292;&#24182;&#34920;&#26126;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#27861;&#26080;&#27861;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#32531;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.04777</link><description>&lt;p&gt;
&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30340;&#31934;&#30830;&#35299;&#26512;&#35299;
&lt;/p&gt;
&lt;p&gt;
Exact Solutions of a Deep Linear Network. (arXiv:2202.04777v6 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25214;&#21040;&#20102;&#24102;&#26435;&#37325;&#34928;&#20943;&#21644;&#38543;&#26426;&#31070;&#32463;&#20803;&#30340;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#32467;&#26524;&#34920;&#26126;&#26435;&#37325;&#34928;&#20943;&#19982;&#27169;&#22411;&#26550;&#26500;&#30340;&#24378;&#28872;&#20132;&#20114;&#20316;&#29992;&#20250;&#22312;&#22810;&#20110;1&#20010;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#20013;&#21019;&#24314;&#19981;&#33391;&#26497;&#23567;&#20540;&#65292;&#24182;&#34920;&#26126;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#27861;&#26080;&#27861;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#32531;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25214;&#21040;&#20102;&#24102;&#26435;&#37325;&#34928;&#20943;&#21644;&#38543;&#26426;&#31070;&#32463;&#20803;&#30340;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#65292;&#36825;&#26159;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#38646;&#26159;&#19968;&#20010;&#29305;&#27530;&#30340;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26435;&#37325;&#34928;&#20943;&#19982;&#27169;&#22411;&#26550;&#26500;&#30340;&#24378;&#28872;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#33021;&#22815;&#22312;&#20855;&#26377;&#36229;&#36807; $1$ &#20010;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#20013;&#21019;&#24314;&#19981;&#33391;&#26497;&#23567;&#20540;&#65292;&#36825;&#19982;&#20165;&#26377; $1$ &#20010;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#26377;&#36136;&#30340;&#19981;&#21516;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#21021;&#22987;&#21270;&#26041;&#27861;&#26080;&#27861;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#32531;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work finds the analytical expression of the global minima of a deep linear network with weight decay and stochastic neurons, a fundamental model for understanding the landscape of neural networks. Our result implies that zero is a special point in deep neural network architecture. We show that weight decay strongly interacts with the model architecture and can create bad minima at zero in a network with more than $1$ hidden layer, qualitatively different from a network with only $1$ hidden layer. Practically, our result implies that common deep learning initialization methods are insufficient to ease the optimization of neural networks in general.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#19981;&#35268;&#21017;&#39044;&#27979;&#21464;&#37327;&#65292;&#20026;&#22788;&#29702;&#23439;&#35266;&#37329;&#34701;&#38382;&#39064;&#25552;&#20379;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.14000</link><description>&lt;p&gt;
&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Factor-augmented tree ensembles. (arXiv:2111.14000v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.14000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#19981;&#35268;&#21017;&#39044;&#27979;&#21464;&#37327;&#65292;&#20026;&#22788;&#29702;&#23439;&#35266;&#37329;&#34701;&#38382;&#39064;&#25552;&#20379;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#26041;&#27861;&#25552;&#21462;&#28508;&#22312;&#31283;&#24577;&#22240;&#23376;&#26469;&#25193;&#23637;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#26641;&#20449;&#24687;&#38598;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#35813;&#26041;&#27861;&#23558;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#26641;&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#20004;&#20010;&#26041;&#38754;&#12290;&#31532;&#19968;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#27979;&#37327;&#35823;&#24046;&#12289;&#38750;&#24179;&#31283;&#36235;&#21183;&#12289;&#23395;&#33410;&#24615;&#21644;/&#25110;&#32570;&#22833;&#35266;&#27979;&#31561;&#19981;&#35268;&#21017;&#30340;&#39044;&#27979;&#21464;&#37327;&#12290;&#31532;&#20108;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26126;&#30830;&#30340;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#29702;&#35770;&#26469;&#25351;&#23548;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#26641;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22240;&#23376;&#22686;&#24378;&#30340;&#26641;&#38598;&#21512;&#26041;&#27861;&#22312;&#23439;&#35266;&#37329;&#34701;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#32654;&#22269;&#32929;&#31080;&#27874;&#21160;&#29575;&#19982;&#21830;&#19994;&#21608;&#26399;&#20043;&#38388;&#30340;&#20808;&#23548;&#28382;&#21518;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript proposes to extend the information set of time-series regression trees with latent stationary factors extracted via state-space methods. In doing so, this approach generalises time-series regression trees on two dimensions. First, it allows to handle predictors that exhibit measurement error, non-stationary trends, seasonality and/or irregularities such as missing observations. Second, it gives a transparent way for using domain-specific theory to inform time-series regression trees. Empirically, ensembles of these factor-augmented trees provide a reliable approach for macro-finance problems. This article highlights it focussing on the lead-lag effect between equity volatility and the business cycle in the United States.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;STJGCN&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#26410;&#26469;&#26102;&#38388;&#27493;&#38271;&#19978;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#39044;&#23450;&#20041;&#30340;&#21644;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#32852;&#21512;&#22270;&#20197;&#21450;&#22312;STJGs&#19978;&#25191;&#34892;&#22270;&#21367;&#31215;&#36816;&#31639;&#65292;&#20197;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#24182;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#65292;&#24182;&#24050;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2111.13684</link><description>&lt;p&gt;
&#26102;&#31354;&#32852;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Joint Graph Convolutional Networks for Traffic Forecasting. (arXiv:2111.13684v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.13684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;STJGCN&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#26410;&#26469;&#26102;&#38388;&#27493;&#38271;&#19978;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#39044;&#23450;&#20041;&#30340;&#21644;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#32852;&#21512;&#22270;&#20197;&#21450;&#22312;STJGs&#19978;&#25191;&#34892;&#22270;&#21367;&#31215;&#36816;&#31639;&#65292;&#20197;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#24182;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#65292;&#24182;&#24050;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#23558;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26102;&#31354;&#22270;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#26500;&#24314;&#19968;&#20010;&#38745;&#24577;&#30340;&#31354;&#38388;&#22270;&#65292;&#28982;&#21518;&#29992;&#30456;&#37051;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#33410;&#28857;&#36830;&#25509;&#34920;&#31034;&#20026;&#26102;&#31354;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26410;&#33021;&#26126;&#30830;&#21453;&#26144;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22312;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#20351;&#29992;&#30456;&#21516;&#30340;&#37051;&#25509;&#30697;&#38453;&#26469;&#24573;&#30053;&#33410;&#28857;&#20043;&#38388;&#30340;&#21160;&#24577;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#31354;&#32852;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476; (STJGCN) &#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#26410;&#26469;&#26102;&#38388;&#27493;&#39588;&#19978;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#26500;&#24314;&#39044;&#23450;&#20041;&#30340;&#21644;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#32852;&#21512;&#22270; (STJGs) &#20197;&#21450;&#22312; STJGs &#19978;&#25191;&#34892;&#22270;&#21367;&#31215;&#36816;&#31639;&#20197;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#24182;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shifted their focus towards formulating traffic forecasting as a spatio-temporal graph modeling problem. Typically, they constructed a static spatial graph at each time step and then connected each node with itself between adjacent time steps to create a spatio-temporal graph. However, this approach failed to explicitly reflect the correlations between different nodes at different time steps, thus limiting the learning capability of graph neural networks. Additionally, those models overlooked the dynamic spatio-temporal correlations among nodes by using the same adjacency matrix across different time steps. To address these limitations, we propose a novel approach called Spatio-Temporal Joint Graph Convolutional Networks (STJGCN) for accurate traffic forecasting on road networks over multiple future time steps. Specifically, our method encompasses the construction of both pre-defined and adaptive spatio-temporal joint graphs (STJGs) between any two time steps, which
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DAPPER&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#30446;&#26631;&#22495;&#20013;&#30340;&#36866;&#24212;&#24615;&#33021;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31227;&#21160;&#24863;&#30693;&#20013;&#30340;&#22495;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.11053</link><description>&lt;p&gt;
DAPPER&#65306;&#22522;&#20110;&#20010;&#24615;&#21270;&#30340;&#26080;&#26631;&#31614;&#31227;&#21160;&#20256;&#24863;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DAPPER: Label-Free Performance Estimation after Personalization for Heterogeneous Mobile Sensing. (arXiv:2111.11053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.11053
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DAPPER&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#30446;&#26631;&#22495;&#20013;&#30340;&#36866;&#24212;&#24615;&#33021;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31227;&#21160;&#24863;&#30693;&#20013;&#30340;&#22495;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;&#31227;&#21160;&#35774;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#20256;&#24863;&#22120;&#25552;&#20379;&#26032;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#29992;&#25143;&#12289;&#35774;&#22791;&#21644;&#29615;&#22659;&#31561;&#22240;&#32032;&#24433;&#21709;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#20351;&#22495;&#28418;&#31227;&#25104;&#20026;&#31227;&#21160;&#24863;&#30693;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#23613;&#31649;&#23581;&#35797;&#20102;&#22495;&#36866;&#24212;&#26469;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#21407;&#21017;&#19978;&#65292;&#24615;&#33021;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22522;&#26412;&#30495;&#30456;&#26631;&#31614;&#36827;&#34892;&#24615;&#33021;&#39564;&#35777;&#26469;&#35782;&#21035;&#21644;&#36174;&#22238;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#25910;&#38598;&#39640;&#36136;&#37327;&#12289;&#36275;&#22815;&#30340;&#26631;&#35760;&#25968;&#25454;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DAPPER&#65288;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#20272;&#35745;&#22120;&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#30446;&#26631;&#22495;&#20013;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#29305;&#24449;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#26469;&#36817;&#20284;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#31227;&#21160;&#20256;&#24863;&#22120;&#19978;&#30340;&#20010;&#24615;&#21270;&#20316;&#20026;&#33258;&#28982;&#26657;&#20934;&#26469;&#25509;&#36817;&#22522;&#26412;&#30340;&#30495;&#30456;&#26631;&#31614;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;DAPPER&#22312;&#30446;&#26631;&#22495;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#22495;&#36866;&#24212;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;11%&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications utilize sensors in mobile devices and machine learning to provide novel services. However, various factors such as different users, devices, and environments impact the performance of such applications, thus making the domain shift (i.e., distributional shift between the training domain and the target domain) a critical issue in mobile sensing. Despite attempts in domain adaptation to solve this challenging problem, their performance is unreliable due to the complex interplay among diverse factors. In principle, the performance uncertainty can be identified and redeemed by performance validation with ground-truth labels. However, it is infeasible for every user to collect high-quality, sufficient labeled data. To address the issue, we present DAPPER (Domain AdaPtation Performance EstimatoR) that estimates the adaptation performance in a target domain with only unlabeled target data. Our key idea is to approximate the model performance based on the mutual information b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#27169;&#25311;&#21442;&#25968;&#21270;&#29616;&#35937;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#22312;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#29305;&#24449;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#65292;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.13530</link><description>&lt;p&gt;
&#25193;&#23637;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#21442;&#25968;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#39044;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An extended physics informed neural network for preliminary analysis of parametric optimal control problems. (arXiv:2110.13530v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#27169;&#25311;&#21442;&#25968;&#21270;&#29616;&#35937;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#22312;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#29305;&#24449;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#65292;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#23613;&#31649;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#22312;&#23454;&#26102;&#21644;&#22810;&#26597;&#35810;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24448;&#24448;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#22312;&#30701;&#26102;&#38388;&#20869;&#27169;&#25311;&#21442;&#25968;&#21270;&#29616;&#35937;&#12290;&#29289;&#29702;&#20449;&#24687;&#23558;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#65288;&#26631;&#20934;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65289;&#12289;&#25193;&#23637;&#36755;&#20837;&#29305;&#24449;&#65288;&#39069;&#22806;&#30340;&#29305;&#24449;&#21033;&#29992;&#65289;&#21644;&#26500;&#24314;&#26377;&#25928;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#23548;&#26041;&#38024;&#65288;&#29289;&#29702;&#20449;&#24687;&#26550;&#26500;&#65289;&#26469;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#36825;&#19977;&#20010;&#26041;&#38754;&#30340;&#32508;&#21512;&#24212;&#29992;&#23558;&#21152;&#24555;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#21442;&#25968;&#21270;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#24050;&#24212;&#29992;&#20110;&#22810;&#20010;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#26694;&#26550;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose an extension of physics informed supervised learning strategies to parametric partial differential equations. Indeed, even if the latter are indisputably useful in many applications, they can be computationally expensive most of all in a real-time and many-query setting. Thus, our main goal is to provide a physics informed learning paradigm to simulate parametrized phenomena in a small amount of time. The physics information will be exploited in many ways, in the loss function (standard physics informed neural networks), as an augmented input (extra feature employment) and as a guideline to build an effective structure for the neural network (physics informed architecture). These three aspects, combined together, will lead to a faster training phase and to a more accurate parametric prediction. The methodology has been tested for several equations and also in an optimal control framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#22270;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#23545;&#26032;&#29305;&#24449;&#30340;&#22806;&#25512;&#65292;&#24182;&#32531;&#35299;&#29305;&#24449;&#23618;&#38754;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.04514</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#30340;&#24402;&#32435;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Open-World Feature Extrapolation: An Inductive Graph Learning Approach. (arXiv:2110.04514v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#22270;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#23545;&#26032;&#29305;&#24449;&#30340;&#22806;&#25512;&#65292;&#24182;&#32531;&#35299;&#29305;&#24449;&#23618;&#38754;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#31354;&#38388;&#32463;&#36807;&#25193;&#23637;&#65292;&#22312;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#38656;&#35201;&#22788;&#29702;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#26032;&#29305;&#24449;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#29992;&#22270;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#39592;&#24178;&#32593;&#32476;&#20316;&#20026;&#36739;&#20302;&#30340;&#27169;&#22411;&#65292;&#23558;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#24182;&#36755;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#65307;2&#65289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36739;&#39640;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#26500;&#24314;&#30340;&#29305;&#24449;-&#25968;&#25454;&#22270;&#19978;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#65292;&#23398;&#20064;&#22806;&#25512;&#26032;&#29305;&#24449;&#30340;&#23884;&#20837;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#19968;&#31181;&#26159;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#24402;&#32435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#36171;&#20104;&#27169;&#22411;&#22806;&#25512;&#33021;&#21147;&#24182;&#32531;&#35299;&#29305;&#24449;&#23618;&#38754;&#30340;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We target open-world feature extrapolation problem where the feature space of input data goes through expansion and a model trained on partially observed features needs to handle new features in test data without further retraining. The problem is of much significance for dealing with features incrementally collected from different fields. To this end, we propose a new learning paradigm with graph representation and learning. Our framework contains two modules: 1) a backbone network (e.g., feedforward neural nets) as a lower model takes features as input and outputs predicted labels; 2) a graph neural network as an upper model learns to extrapolate embeddings for new features via message passing over a feature-data graph built from observed data. Based on our framework, we design two training strategies, a self-supervised approach and an inductive learning approach, to endow the model with extrapolation ability and alleviate feature-level over-fitting. We also provide theoretical analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915; Monge-Amp\`ere &#26041;&#31243;&#30340;&#36842;&#21033;&#20811;&#38647;&#38382;&#39064;&#65292;&#20351;&#29992;&#28145;&#24230;&#20984;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#20551;&#35774;&#21487;&#20197;&#29992;&#26469;&#25214;&#21040;&#21807;&#19968;&#30340;&#20984;&#35299;&#65292;&#26041;&#27861;&#23545;&#22855;&#24322;&#28857;&#12289;&#19981;&#36830;&#32493;&#28857;&#21644;&#28304;&#20989;&#25968;&#20013;&#30340;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#20063;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2110.03310</link><description>&lt;p&gt;
&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915; Monge-Amp\`ere &#26041;&#31243;&#30340;&#36842;&#21033;&#20811;&#38647;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving the Dirichlet problem for the Monge-Amp\`ere equation using neural networks. (arXiv:2110.03310v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915; Monge-Amp\`ere &#26041;&#31243;&#30340;&#36842;&#21033;&#20811;&#38647;&#38382;&#39064;&#65292;&#20351;&#29992;&#28145;&#24230;&#20984;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#20551;&#35774;&#21487;&#20197;&#29992;&#26469;&#25214;&#21040;&#21807;&#19968;&#30340;&#20984;&#35299;&#65292;&#26041;&#27861;&#23545;&#22855;&#24322;&#28857;&#12289;&#19981;&#36830;&#32493;&#28857;&#21644;&#28304;&#20989;&#25968;&#20013;&#30340;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#20063;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Monge-Amp\`ere &#26041;&#31243;&#26159;&#20998;&#26512;&#12289;&#20960;&#20309;&#21644;&#24212;&#29992;&#31185;&#23398;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#19968;&#20010;&#20840;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#25991;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#19982; Monge-Amp\`ere &#26041;&#31243;&#30456;&#20851;&#30340;&#36842;&#21033;&#20811;&#38647;&#38382;&#39064;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#20984;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#20551;&#35774;&#21487;&#20197;&#29992;&#26469;&#25214;&#21040;&#21807;&#19968;&#30340;&#20984;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22855;&#24322;&#28857;&#12289;&#19981;&#36830;&#32493;&#28857;&#21644;&#28304;&#20989;&#25968;&#20013;&#30340;&#22122;&#22768;&#23545;&#26041;&#27861;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#32771;&#34385;&#20102;&#38750;&#24179;&#20961;&#22495;&#65292;&#24182;&#30740;&#31350;&#20102;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#20998;&#26512;&#30740;&#31350;&#20102;&#25910;&#25947;&#24615;&#24182;&#22522;&#20110;&#31283;&#23450;&#24615;&#32467;&#26524;&#32473;&#20986;&#20102;&#35823;&#24046;&#20272;&#35745;&#12290;&#25105;&#20204;&#36824;&#23558;&#27492;&#26041;&#27861;&#19982;&#20351;&#29992;&#24809;&#32602;&#32570;&#20047;&#20984;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#26631;&#20934;&#21069;&#39304;&#32593;&#32476;&#32467;&#21512;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Monge-Amp\`ere equation is a fully nonlinear partial differential equation (PDE) of fundamental importance in analysis, geometry and in the applied sciences. In this paper we solve the Dirichlet problem associated with the Monge-Amp\`ere equation using neural networks and we show that an ansatz using deep input convex neural networks can be used to find the unique convex solution. As part of our analysis we study the effect of singularities, discontinuities and noise in the source function, we consider nontrivial domains, and we investigate how the method performs in higher dimensions. We investigate the convergence numerically and present error estimates based on a stability result. We also compare this method to an alternative approach in which standard feed-forward networks are used together with a loss function which penalizes lack of convexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#22352;&#26631;&#21464;&#25442;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23618;&#32423;&#24352;&#37327;&#31215;&#23637;&#24320;&#26469;&#36924;&#36817;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#25237;&#24433;&#31995;&#25968;&#36827;&#34892;&#26816;&#27979;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.01729</link><description>&lt;p&gt;
&#38543;&#26426;&#22352;&#26631;&#21464;&#25442;&#21450;&#20854;&#22312;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stochastic coordinate transformations with applications to robust machine learning. (arXiv:2110.01729v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#22352;&#26631;&#21464;&#25442;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23618;&#32423;&#24352;&#37327;&#31215;&#23637;&#24320;&#26469;&#36924;&#36817;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#25237;&#24433;&#31995;&#25968;&#36827;&#34892;&#26816;&#27979;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#26032;&#30340;&#29305;&#24449;&#65292;&#21033;&#29992;Karhunen-Loeve&#23637;&#24320;&#27861;&#26469;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#28508;&#22312;&#38543;&#26426;&#34892;&#20026;&#12290;&#36825;&#20123;&#26032;&#29305;&#24449;&#26159;&#36890;&#36807;&#22522;&#20110;&#26368;&#36817;&#30340;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#29702;&#35770;&#36827;&#34892;&#30340;&#22352;&#26631;&#21464;&#25442;&#26500;&#24314;&#30340;&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#30456;&#20851;&#30340;&#20449;&#21495;&#20998;&#35299;&#26159;&#29992;&#24050;&#30693;&#20248;&#21270;&#23646;&#24615;&#30340;&#23618;&#32423;&#24352;&#37327;&#31215;&#23637;&#24320;&#26469;&#36924;&#36817;&#20855;&#26377;&#26377;&#38480;&#21151;&#33021;&#31354;&#38388;&#30340;&#38543;&#26426;&#36807;&#31243;&#65288;&#38543;&#26426;&#22330;&#65289;&#12290;&#21407;&#21017;&#19978;&#65292;&#36825;&#20123;&#20302;&#32500;&#31354;&#38388;&#21487;&#20197;&#25429;&#25417;&#32473;&#23450;&#21517;&#20041;&#31867;&#21035;&#30340;'&#24213;&#23618;&#20449;&#21495;'&#30340;&#22823;&#37096;&#20998;&#38543;&#26426;&#21464;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#26469;&#33258;&#20854;&#23427;&#31867;&#21035;&#30340;&#20449;&#21495;&#25298;&#32477;&#20026;&#38543;&#26426;&#24322;&#24120;&#12290;&#36890;&#36807;&#21517;&#20041;&#31867;&#21035;&#30340;&#23618;&#32423;&#26377;&#38480;&#32500;&#23637;&#24320;&#65292;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#20449;&#21495;&#32452;&#20214;&#30340;&#27491;&#20132;&#23884;&#22871;&#23376;&#31354;&#38388;&#12290;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#23376;&#31354;&#38388;&#20013;&#30340;&#25237;&#24433;&#31995;&#25968;&#26469;&#35757;&#32451;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce a set of novel features for identifying underlying stochastic behavior of input data using the Karhunen-Loeve expansion. These novel features are constructed by applying a coordinate transformation based on the recent Functional Data Analysis theory for anomaly detection. The associated signal decomposition is an exact hierarchical tensor product expansion with known optimality properties for approximating stochastic processes (random fields) with finite dimensional function spaces. In principle these low dimensional spaces can capture most of the stochastic behavior of `underlying signals' in a given nominal class, and can reject signals in alternative classes as stochastic anomalies. Using a hierarchical finite dimensional expansion of the nominal class, a series of orthogonal nested subspaces is constructed for detecting anomalous signal components. Projection coefficients of input data in these subspaces are then used to train a Machine Learning (ML) clas
&lt;/p&gt;</description></item><item><title>WildWood&#26159;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#26435;&#37325;&#32858;&#21512;&#21253;&#22806;&#26679;&#26412;&#20197;&#25913;&#36827;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#30452;&#26041;&#22270;&#31574;&#30053;&#21152;&#36895;&#20998;&#35010;&#26597;&#25214;&#65292;&#20855;&#26377;&#27604;&#26631;&#20934;RF&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#26356;&#24555;&#21644;&#26356;&#20855;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.08010</link><description>&lt;p&gt;
WildWood&#65306;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
WildWood: a new Random Forest algorithm. (arXiv:2109.08010v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.08010
&lt;/p&gt;
&lt;p&gt;
WildWood&#26159;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#26435;&#37325;&#32858;&#21512;&#21253;&#22806;&#26679;&#26412;&#20197;&#25913;&#36827;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#30452;&#26041;&#22270;&#31574;&#30053;&#21152;&#36895;&#20998;&#35010;&#26597;&#25214;&#65292;&#20855;&#26377;&#27604;&#26631;&#20934;RF&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#26356;&#24555;&#21644;&#26356;&#20855;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;WildWood&#65288;WW&#65289;&#36825;&#31181;&#26032;&#30340;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#38598;&#21512;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;Random Forest&#65288;RF&#65289;&#31867;&#22411;&#12290;&#26631;&#20934;&#30340;RF&#31639;&#27861;&#20351;&#29992;&#33258;&#21161;&#27861;&#26679;&#26412;&#26469;&#35745;&#31639;&#21253;&#22806;&#65288;out-of-bag&#65289;&#20998;&#25968;&#65292;WW&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#20135;&#29983;&#25913;&#36827;&#30340;&#39044;&#27979;&#65292;&#32473;&#20986;&#27599;&#20010;&#23436;&#20840;&#29983;&#38271;&#30340;&#26641;&#30340;&#25152;&#26377;&#21487;&#33021;&#23376;&#26641;&#39044;&#27979;&#30340;&#32858;&#21512;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#22312;&#21253;&#22806;&#26679;&#26412;&#19978;&#35745;&#31639;&#30340;&#25351;&#25968;&#26435;&#37325;&#32858;&#21512;&#23454;&#29616;&#30340;&#65292;&#36825;&#20123;&#26679;&#26412;&#30001;&#31216;&#20026;&#19978;&#19979;&#25991;&#26641;&#21152;&#26435;&#65288;context tree weighting&#65289;&#30340;&#31639;&#27861;&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#26469;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#20854;&#20182;&#25104;&#29087;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#22914;&#26631;&#20934;RF&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;extreme gradient boosting&#65289;&#31639;&#27861;&#30456;&#27604;&#65292;WildWoods&#24555;&#36895;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;&#37319;&#29992;&#21152;&#36895;&#20998;&#35010;&#26597;&#25214;&#30340;&#30452;&#26041;&#22270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce WildWood (WW), a new ensemble algorithm for supervised learning of Random Forest (RF) type. While standard RF algorithms use bootstrap out-of-bag samples to compute out-of-bag scores, WW uses these samples to produce improved predictions given by an aggregation of the predictions of all possible subtrees of each fully grown tree in the forest. This is achieved by aggregation with exponential weights computed over out-of-bag samples, that are computed exactly and very efficiently thanks to an algorithm called context tree weighting. This improvement, combined with a histogram strategy to accelerate split finding, makes WW fast and competitive compared with other well-established ensemble methods, such as standard RF and extreme gradient boosting algorithms.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36172;&#21338;&#26426;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23454;&#29616;&#23545;&#19968;&#32452;&#20256;&#24863;&#22120;&#30340;&#26368;&#24555;&#21464;&#28857;&#26816;&#27979;&#65292;&#20174;&#32780;&#33410;&#30465;&#36164;&#28304;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2107.10492</link><description>&lt;p&gt;
&#24322;&#24120;&#26368;&#24555;&#21464;&#28857;&#26816;&#27979;&#20013;&#30340;&#36172;&#21338;&#26426;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bandit Quickest Changepoint Detection. (arXiv:2107.10492v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.10492
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36172;&#21338;&#26426;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23454;&#29616;&#23545;&#19968;&#32452;&#20256;&#24863;&#22120;&#30340;&#26368;&#24555;&#21464;&#28857;&#26816;&#27979;&#65292;&#20174;&#32780;&#33410;&#30465;&#36164;&#28304;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24037;&#19994;&#21644;&#23433;&#20840;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#19968;&#32452;&#20256;&#24863;&#22120;&#26469;&#26816;&#27979;&#26102;&#38388;&#34892;&#20026;&#27169;&#24335;&#20013;&#30340;&#31361;&#21464;&#12290;&#36825;&#20123;&#31361;&#21464;&#36890;&#24120;&#22312;&#23616;&#37096;&#34920;&#29616;&#20986;&#26469;&#65292;&#20165;&#20351;&#19968;&#23567;&#37096;&#20998;&#20256;&#24863;&#22120;&#26377;&#20449;&#24687;&#12290;&#30001;&#20110;&#36164;&#28304;&#38480;&#21046;&#65292;&#30417;&#25511;&#27599;&#20010;&#20256;&#24863;&#22120;&#21487;&#33021;&#24456;&#26114;&#36149;&#65292;&#36825;&#26159;&#36827;&#34892;&#36172;&#21338;&#26426;&#26368;&#24555;&#21464;&#28857;&#26816;&#27979;&#38382;&#39064;&#30340;&#21160;&#26426;&#65292;&#20854;&#20013;&#36873;&#25321;&#19968;&#31995;&#21015;&#20256;&#24863;&#21160;&#20316;&#65288;&#25110;&#20256;&#24863;&#22120;&#65289;&#65292;&#24182;&#19988;&#21482;&#35266;&#23519;&#19982;&#25152;&#36873;&#21160;&#20316;&#23545;&#24212;&#30340;&#27979;&#37327;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26377;&#38480;&#21442;&#25968;&#27010;&#29575;&#20998;&#24067;&#31867;&#21035;&#30340;&#26816;&#27979;&#24310;&#36831;&#30340;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#12290;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#24863;&#30693;&#26041;&#26696;&#65292;&#23427;&#26080;&#32541;&#24179;&#34913;&#20102;&#23545;&#19981;&#21516;&#20256;&#24863;&#36873;&#39033;&#30340;&#25506;&#32034;&#38656;&#27714;&#19982;&#35810;&#38382;&#20449;&#24687;&#21160;&#20316;&#30340;&#21033;&#29992;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#25152;&#25552;&#20986;&#26041;&#26696;&#30340;&#39044;&#26399;&#24310;&#36831;&#30028;&#38480;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#30028;&#38480;&#19982;&#25105;&#20204;&#30340;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#22312;&#20302;&#32500;&#31354;&#38388;&#30340;&#21305;&#37197;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many industrial and security applications employ a suite of sensors for detecting abrupt changes in temporal behavior patterns. These abrupt changes typically manifest locally, rendering only a small subset of sensors informative. Continuous monitoring of every sensor can be expensive due to resource constraints, and serves as a motivation for the bandit quickest changepoint detection problem, where sensing actions (or sensors) are sequentially chosen, and only measurements corresponding to chosen actions are observed. We derive an information-theoretic lower bound on the detection delay for a general class of finitely parameterized probability distributions. We then propose a computationally efficient online sensing scheme, which seamlessly balances the need for exploration of different sensing options with exploitation of querying informative actions. We derive expected delay bounds for the proposed scheme and show that these bounds match our information-theoretic lower bounds at low
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#38750;&#20887;&#20313;&#30340;&#19981;&#21516;&#29305;&#24449;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#26377;&#24110;&#21161;&#65292;&#20855;&#26377;&#26356;&#22810;&#19981;&#21516;&#29305;&#24449;&#30340;&#38544;&#34255;&#23618;&#21333;&#20803;&#21487;&#20197;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2106.06012</link><description>&lt;p&gt;
&#23398;&#20064;&#19981;&#21516;&#30340;&#29305;&#24449;&#26377;&#24110;&#21161;&#65292;&#21487;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning distinct features helps, provably. (arXiv:2106.06012v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06012
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#38750;&#20887;&#20313;&#30340;&#19981;&#21516;&#29305;&#24449;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#26377;&#24110;&#21161;&#65292;&#20855;&#26377;&#26356;&#22810;&#19981;&#21516;&#29305;&#24449;&#30340;&#38544;&#34255;&#23618;&#21333;&#20803;&#21487;&#20197;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#35757;&#32451;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#25152;&#23398;&#20064;&#30340;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#38544;&#34255;&#23618;&#29305;&#24449;&#20043;&#38388;&#30340;&#24179;&#22343;$L_2$&#36317;&#31163;&#26469;&#24230;&#37327;&#22810;&#26679;&#24615;&#65292;&#24182;&#29702;&#35770;&#25506;&#35752;&#20102;&#23398;&#20064;&#38750;&#20887;&#20313;&#30340;&#19981;&#21516;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#25512;&#23548;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#22810;&#26679;&#24615;&#30340;&#26032;&#22411;&#25512;&#24191;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#26126;&#20102;&#38544;&#34255;&#23618;&#21333;&#20803;&#20869;&#20855;&#26377;&#26356;&#22810;&#19981;&#21516;&#29305;&#24449;&#21487;&#20197;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#32593;&#32476;&#21644;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the diversity of the features learned by a two-layer neural network trained with the least squares loss. We measure the diversity by the average $L_2$-distance between the hidden-layer features and theoretically investigate how learning non-redundant distinct features affects the performance of the network. To do so, we derive novel generalization bounds depending on feature diversity based on Rademacher complexity for such networks. Our analysis proves that more distinct features at the network's units within the hidden layer lead to better generalization. We also show how to extend our results to deeper networks and different losses.
&lt;/p&gt;</description></item><item><title>&#26680;&#32454;&#21270;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#21387;&#32553;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;$n$&#28857;&#36817;&#20284;&#30340;&#20998;&#24067;&#21387;&#32553;&#21040;&#20855;&#26377;&#21487;&#27604;&#36739;&#26368;&#22351;&#31215;&#20998;&#35823;&#24046;&#30340;$\sqrt{n}$&#28857;&#36817;&#20284;&#65292;&#20854;&#20122;&#25351;&#25968;&#20445;&#35777;&#31867;&#20284;&#20110;&#22312;$[0,1]^d$&#19978;&#22343;&#21248;$\mathbb{P}$&#30340;&#32463;&#20856;&#20934;&#33945;&#29305;&#21345;&#32599;&#35823;&#24046;&#29575;&#65292;&#20294;&#36866;&#29992;&#20110;$\mathbb{R}^d$&#19978;&#30340;&#19968;&#33324;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2105.05842</link><description>&lt;p&gt;
&#26680;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Kernel Thinning. (arXiv:2105.05842v9 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.05842
&lt;/p&gt;
&lt;p&gt;
&#26680;&#32454;&#21270;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#21387;&#32553;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;$n$&#28857;&#36817;&#20284;&#30340;&#20998;&#24067;&#21387;&#32553;&#21040;&#20855;&#26377;&#21487;&#27604;&#36739;&#26368;&#22351;&#31215;&#20998;&#35823;&#24046;&#30340;$\sqrt{n}$&#28857;&#36817;&#20284;&#65292;&#20854;&#20122;&#25351;&#25968;&#20445;&#35777;&#31867;&#20284;&#20110;&#22312;$[0,1]^d$&#19978;&#22343;&#21248;$\mathbb{P}$&#30340;&#32463;&#20856;&#20934;&#33945;&#29305;&#21345;&#32599;&#35823;&#24046;&#29575;&#65292;&#20294;&#36866;&#29992;&#20110;$\mathbb{R}^d$&#19978;&#30340;&#19968;&#33324;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#26680;&#32454;&#21270;&#65292;&#19968;&#31181;&#27604;&#29420;&#31435;&#21516;&#20998;&#24067;&#37319;&#26679;&#25110;&#26631;&#20934;&#32454;&#21270;&#26356;&#26377;&#25928;&#22320;&#21387;&#32553;&#20998;&#24067;$\mathbb{P}$&#30340;&#26032;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#21512;&#36866;&#30340;&#20877;&#29983;&#26680;$\mathbf{k}_{\star}$&#21644;$\mathcal{O}(n^2)$&#26102;&#38388;&#65292;&#26680;&#32454;&#21270;&#23558;&#19968;&#20010;$n$&#28857;&#36817;&#20284;&#30340;$\mathbb{P}$&#21387;&#32553;&#25104;&#19968;&#20010;&#20855;&#26377;&#19982;&#30456;&#20851;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#21487;&#27604;&#36739;&#26368;&#22351;&#31215;&#20998;&#35823;&#24046;&#30340;$\sqrt{n}$&#28857;&#36817;&#20284;&#12290;&#22312;&#27010;&#29575;&#19978;&#65292;&#32039;&#25903;&#25745;&#30340;$\mathbb{P}$&#30340;&#31215;&#20998;&#35823;&#24046;&#26368;&#22823;&#24046;&#21035;&#20026;$\mathcal{O}_d(n^{-1/2}\sqrt{\log n})$&#65292;&#22312;$\mathbb{R}^d$&#19978;&#30340;&#20122;&#25351;&#25968;$\mathbb{P}$&#20026;$\mathcal{O}_d(n^{-\frac{1}{2}} (\log n)^{(d+1)/2}\sqrt{\log\log n})$&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26469;&#33258;$\mathbb{P}$&#30340;&#31561;&#22823;&#23567;i.i.d.&#26679;&#26412;&#38754;&#20020;$\Omega(n^{-1/4})$&#30340;&#31215;&#20998;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#20122;&#25351;&#25968;&#20445;&#35777;&#31867;&#20284;&#20110;&#22312;$[0,1]^d$&#19978;&#22343;&#21248;$\mathbb{P}$&#30340;&#32463;&#20856;&#20934;&#33945;&#29305;&#21345;&#32599;&#35823;&#24046;&#29575;&#65292;&#20294;&#36866;&#29992;&#20110;$\mathbb{R}^d$&#19978;&#30340;&#19968;&#33324;&#20998;&#24067;&#21644;&#19968;&#20010;&#22823;
&lt;/p&gt;
&lt;p&gt;
We introduce kernel thinning, a new procedure for compressing a distribution $\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given a suitable reproducing kernel $\mathbf{k}_{\star}$ and $\mathcal{O}(n^2)$ time, kernel thinning compresses an $n$-point approximation to $\mathbb{P}$ into a $\sqrt{n}$-point approximation with comparable worst-case integration error across the associated reproducing kernel Hilbert space. The maximum discrepancy in integration error is $\mathcal{O}_d(n^{-1/2}\sqrt{\log n})$ in probability for compactly supported $\mathbb{P}$ and $\mathcal{O}_d(n^{-\frac{1}{2}} (\log n)^{(d+1)/2}\sqrt{\log\log n})$ for sub-exponential $\mathbb{P}$ on $\mathbb{R}^d$. In contrast, an equal-sized i.i.d. sample from $\mathbb{P}$ suffers $\Omega(n^{-1/4})$ integration error. Our sub-exponential guarantees resemble the classical quasi-Monte Carlo error rates for uniform $\mathbb{P}$ on $[0,1]^d$ but apply to general distributions on $\mathbb{R}^d$ and a wid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23436;&#25972;&#25968;&#25454;&#25554;&#34917;&#30340;&#23545;&#25239;&#32593;&#32476;&#65288;IDIAN&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#30340;&#19981;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#19979;&#30340;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#36328;&#22495;&#21644;&#23454;&#38469;&#36866;&#24212;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2012.01606</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#19979;&#30340;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation with Incomplete Target Domains. (arXiv:2012.01606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.01606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23436;&#25972;&#25968;&#25454;&#25554;&#34917;&#30340;&#23545;&#25239;&#32593;&#32476;&#65288;IDIAN&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#30340;&#19981;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#19979;&#30340;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#36328;&#22495;&#21644;&#23454;&#38469;&#36866;&#24212;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#33258;&#36866;&#24212;&#26159;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#28304;&#22495;&#20013;&#30340;&#29616;&#26377;&#26631;&#35760;&#25968;&#25454;&#26469;&#20943;&#23569;&#30446;&#26631;&#22495;&#27880;&#37322;&#25104;&#26412;&#30340;&#20219;&#21153;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#22495;&#33258;&#36866;&#24212;&#20551;&#35774;&#20004;&#20010;&#22495;&#20013;&#37117;&#26377;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#65292;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32570;&#23569;&#25968;&#25454;&#30340;&#23384;&#22312;&#26159;&#26222;&#36941;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22788;&#29702;&#20102;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22495;&#33258;&#36866;&#24212;&#24773;&#26223;&#65292;&#21363;&#20855;&#26377;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#30340;&#19981;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23436;&#25972;&#25968;&#25454;&#25554;&#34917;&#30340;&#23545;&#25239;&#32593;&#32476;&#65288;IDIAN&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#26032;&#30340;&#22495;&#33258;&#36866;&#24212;&#25361;&#25112;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#25554;&#34917;&#27169;&#22359;&#26469;&#22635;&#34917;&#22522;&#20110;&#30446;&#26631;&#22495;&#23616;&#37096;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#29305;&#24449;&#20540;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23545;&#25239;&#36866;&#24212;&#26469;&#23545;&#40784;&#20004;&#20010;&#22495;&#12290;&#25105;&#20204;&#22312;&#36328;&#22495;&#22522;&#20934;&#20219;&#21153;&#21644;&#20855;&#26377;&#19981;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#30340;&#23454;&#38469;&#36866;&#24212;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;IDIAN&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation, as a task of reducing the annotation cost in a target domain by exploiting the existing labeled data in an auxiliary source domain, has received a lot of attention in the research community. However, the standard domain adaptation has assumed perfectly observed data in both domains, while in real world applications the existence of missing data can be prevalent. In this paper, we tackle a more challenging domain adaptation scenario where one has an incomplete target domain with partially observed data. We propose an Incomplete Data Imputation based Adversarial Network (IDIAN) model to address this new domain adaptation challenge. In the proposed model, we design a data imputation module to fill the missing feature values based on the partial observations in the target domain, while aligning the two domains via deep adversarial adaption. We conduct experiments on both cross-domain benchmark tasks and a real world adaptation task with imperfect target domains. The expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#20915;&#31574;&#20013;&#23637;&#31034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#35770;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#30340;&#24418;&#29366;&#21644;&#26041;&#24046;&#22914;&#20309;&#65292;&#23637;&#31034;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#37117;&#20250;&#20351;&#27169;&#22411;&#39044;&#27979;&#20135;&#29983;&#26356;&#23567;&#30340;&#20998;&#27495;&#12290;</title><link>http://arxiv.org/abs/2011.06167</link><description>&lt;p&gt;
&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20309;&#26102;&#37325;&#35201;&#65306;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#20915;&#31574;&#20013;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making. (arXiv:2011.06167v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.06167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#20915;&#31574;&#20013;&#23637;&#31034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#35770;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#30340;&#24418;&#29366;&#21644;&#26041;&#24046;&#22914;&#20309;&#65292;&#23637;&#31034;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#37117;&#20250;&#20351;&#27169;&#22411;&#39044;&#27979;&#20135;&#29983;&#26356;&#23567;&#30340;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#36741;&#21161;&#20154;&#31867;&#20915;&#31574;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#30456;&#20851;&#36755;&#20837;&#26469;&#24110;&#21161;&#20182;&#20204;&#20915;&#23450;&#22914;&#20309;&#23558;&#27169;&#22411;&#39044;&#27979;&#32435;&#20837;&#20915;&#31574;&#36807;&#31243;&#65292;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#20256;&#36798;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#22312;&#36825;&#26041;&#38754;&#26377;&#25152;&#24110;&#21161;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65288;&#26469;&#33258;190&#20010;&#21442;&#19982;&#32773;&#30340;1,330&#20010;&#21709;&#24212;&#65289;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65288;&#21363;&#19981;&#21516;&#24418;&#29366;&#21644;&#26041;&#24046;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65289;&#65292;&#22312;&#39044;&#27979;&#20844;&#23507;&#31199;&#37329;&#20215;&#26684;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#20915;&#31574;&#32972;&#26223;&#19979;&#65292;&#19981;&#21516;&#19987;&#19994;&#27700;&#24179;&#30340;&#20154;&#23545;&#27492;&#36827;&#34892;&#20102;&#22914;&#20309;&#21709;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23637;&#31034;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#21487;&#23548;&#33268;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20135;&#29983;&#26356;&#23567;&#30340;&#20998;&#27495;&#65292;&#26080;&#35770;&#25105;&#20204;&#32771;&#34385;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#30340;&#24418;&#29366;&#21644;&#26041;&#24046;&#22914;&#20309;&#65292;&#32780;&#19988;&#36825;&#20123;&#25928;&#26524;...
&lt;/p&gt;
&lt;p&gt;
As machine learning (ML) models are increasingly being employed to assist human decision makers, it becomes critical to provide these decision makers with relevant inputs which can help them decide if and how to incorporate model predictions into their decision making. For instance, communicating the uncertainty associated with model predictions could potentially be helpful in this regard. In this work, we carry out user studies (1,330 responses from 190 participants) to systematically assess how people with differing levels of expertise respond to different types of predictive uncertainty (i.e., posterior predictive distributions with different shapes and variances) in the context of ML assisted decision making for predicting apartment rental prices. We found that showing posterior predictive distributions led to smaller disagreements with the ML model's predictions, regardless of the shapes and variances of the posterior predictive distributions we considered, and that these effects 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#20869;&#26680;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#20572;&#27490;&#20934;&#21017;&#65292;&#20351;&#29992;&#32463;&#39564;&#26377;&#25928;&#32500;&#24230;&#26469;&#37327;&#21270;&#22686;&#37327;&#24182;&#25512;&#23548;&#20986;&#21487;&#25191;&#34892;&#30340;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#12290;&#36890;&#36807;&#20351;&#29992;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#24471;&#20986;&#35777;&#26126;&#65292;&#35268;&#21017;&#20855;&#26377;&#20248;&#21270;&#23398;&#20064;&#36895;&#29575;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#20572;&#27490;&#31574;&#30053;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2001.02879</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#20572;&#27490;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Adaptive Stopping Rule for Kernel-based Gradient Descent Algorithms. (arXiv:2001.02879v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.02879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#20869;&#26680;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#20572;&#27490;&#20934;&#21017;&#65292;&#20351;&#29992;&#32463;&#39564;&#26377;&#25928;&#32500;&#24230;&#26469;&#37327;&#21270;&#22686;&#37327;&#24182;&#25512;&#23548;&#20986;&#21487;&#25191;&#34892;&#30340;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#12290;&#36890;&#36807;&#20351;&#29992;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#24471;&#20986;&#35777;&#26126;&#65292;&#35268;&#21017;&#20855;&#26377;&#20248;&#21270;&#23398;&#20064;&#36895;&#29575;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#20572;&#27490;&#31574;&#30053;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20869;&#26680;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#20572;&#27490;&#20934;&#21017;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32463;&#39564;&#26377;&#25928;&#32500;&#24230;&#26469;&#37327;&#21270;KGD&#36845;&#20195;&#30340;&#22686;&#37327;&#24182;&#25512;&#23548;&#20986;&#21487;&#23454;&#26045;&#30340;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#19979;&#20998;&#26512;&#20102;&#33258;&#36866;&#24212;&#20572;&#27490;&#20934;&#21017;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#26368;&#36817;&#21457;&#23637;&#30340;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#65292;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#37197;&#22791;&#27492;&#35268;&#21017;&#30340;KGD&#30340;&#26368;&#20248;&#23398;&#20064;&#36895;&#29575;&#30340;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#37197;&#22791;&#25152;&#36848;&#25552;&#21069;&#20572;&#27490;&#35268;&#21017;&#30340;KGD&#30340;&#36845;&#20195;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#20197;&#35828;&#26126;&#20854;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an adaptive stopping rule for kernel-based gradient descent (KGD) algorithms. We introduce the empirical effective dimension to quantify the increments of iterations in KGD and derive an implementable early stopping strategy. We analyze the performance of the adaptive stopping rule in the framework of learning theory. Using the recently developed integral operator approach, we rigorously prove the optimality of the adaptive stopping rule in terms of showing the optimal learning rates for KGD equipped with this rule. Furthermore, a sharp bound on the number of iterations in KGD equipped with the proposed early stopping rule is also given to demonstrate its computational advantage.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1808.08316</link><description>&lt;p&gt;
&#19968;&#31181;&#19977;&#20803;&#31070;&#32463;&#27169;&#22411;&#29992;&#20110;&#21160;&#24577;&#23454;&#20307;&#30456;&#20851;&#24615;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1808.08316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#23454;&#20307;&#30456;&#20851;&#24615;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#22312;&#38745;&#24577;&#35774;&#32622;&#21644;&#38750;&#30417;&#30563;&#26041;&#24335;&#19979;&#30740;&#31350;&#23454;&#20307;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23454;&#20307;&#24448;&#24448;&#28041;&#21450;&#35768;&#22810;&#19981;&#21516;&#30340;&#20851;&#31995;&#65292;&#22240;&#27492;&#23454;&#20307;&#20851;&#31995;&#38543;&#26102;&#38388;&#21464;&#24471;&#38750;&#24120;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#21147;&#20316;&#20026;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#32852;&#21512;&#26694;&#26550;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies entity relatedness in static settings and an unsupervised manner. However, entities in real-world are often involved in many different relationships, consequently entity-relations are very dynamic over time. In this work, we propose a neural networkbased approach for dynamic entity relatedness, leveraging the collective attention as supervision. Our model is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20107;&#20214;&#20013;&#24515;&#30340;&#38598;&#21512;&#25490;&#21517;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#26102;&#38388;&#21160;&#24577;&#24615;&#65292;&#33021;&#22815;&#25512;&#33616;&#26368;&#30456;&#20851;&#30340;&#23454;&#20307;&#26041;&#38754;&#65292;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/1803.07890</link><description>&lt;p&gt;
&#25512;&#33616;&#23454;&#20307;&#30340;&#26102;&#38388;&#22240;&#32032;&#30340;&#22810;&#27169;&#22411;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multiple Models for Recommending Temporal Aspects of Entities. (arXiv:1803.07890v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1803.07890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20107;&#20214;&#20013;&#24515;&#30340;&#38598;&#21512;&#25490;&#21517;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#26102;&#38388;&#21160;&#24577;&#24615;&#65292;&#33021;&#22815;&#25512;&#33616;&#26368;&#30456;&#20851;&#30340;&#23454;&#20307;&#26041;&#38754;&#65292;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#26041;&#38754;&#30340;&#25512;&#33616;&#26159;&#35821;&#20041;&#25628;&#32034;&#20013;&#30340;&#26032;&#20852;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#24039;&#21512;&#21644;&#31361;&#20986;&#20449;&#24687;&#65292;&#20854;&#20013;&#26174;&#30528;&#24615;&#65288;&#20363;&#22914;&#27969;&#34892;&#24230;&#65289;&#26159;&#20197;&#21069;&#24037;&#20316;&#20013;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#12290;&#20294;&#26159;&#65292;&#23454;&#20307;&#26041;&#38754;&#26159;&#20855;&#26377;&#26102;&#38388;&#21160;&#24577;&#24615;&#30340;&#65292;&#32463;&#24120;&#21463;&#21040;&#38543;&#26102;&#38388;&#21457;&#29983;&#30340;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20165;&#22522;&#20110;&#26174;&#30528;&#24615;&#29305;&#24449;&#30340;&#26041;&#38754;&#24314;&#35758;&#21487;&#33021;&#20250;&#32473;&#20986;&#20196;&#20154;&#19981;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#21407;&#22240;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#26174;&#30528;&#24615;&#36890;&#24120;&#22312;&#38271;&#26102;&#38388;&#27573;&#20869;&#32047;&#31215;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#26368;&#36817;&#24773;&#20917;&#12290;&#20854;&#27425;&#65292;&#19982;&#20107;&#20214;&#23454;&#20307;&#30456;&#20851;&#30340;&#35768;&#22810;&#26041;&#38754;&#24378;&#28872;&#20381;&#36182;&#20110;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#32473;&#23450;&#23454;&#20307;&#30340;&#26102;&#38388;&#26041;&#38754;&#25512;&#33616;&#20219;&#21153;&#65292;&#26088;&#22312;&#25512;&#33616;&#26368;&#30456;&#20851;&#30340;&#26041;&#38754;&#65292;&#24182;&#32771;&#34385;&#26102;&#38388;&#20197;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20107;&#20214;&#20013;&#24515;&#30340;&#38598;&#21512;&#25490;&#21517;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22810;&#20010;&#26102;&#38388;&#21644;&#31867;&#22411;&#20381;&#36182;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#65292;&#24182;&#21160;&#24577;&#26435;&#34913;&#26174;&#30528;&#24615;&#21644;&#26368;&#36817;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity aspect recommendation is an emerging task in semantic search that helps users discover serendipitous and prominent information with respect to an entity, of which salience (e.g., popularity) is the most important factor in previous work. However, entity aspects are temporally dynamic and often driven by events happening over time. For such cases, aspect suggestion based solely on salience features can give unsatisfactory results, for two reasons. First, salience is often accumulated over a long time period and does not account for recency. Second, many aspects related to an event entity are strongly time-dependent. In this paper, we study the task of temporal aspect recommendation for a given entity, which aims at recommending the most relevant aspects and takes into account time in order to improve search experience. We propose a novel event-centric ensemble ranking method that learns from multiple time and type-dependent models and dynamically trades off salience and recency c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#22312;&#22810;&#39033;&#24335;&#26680;&#22238;&#24402;&#20013;&#27491;&#21017;&#21270;&#39033;&#30340;&#20316;&#29992;&#21482;&#26159;&#20026;&#20102;&#35268;&#36991;&#26680;&#30697;&#38453;&#30340;&#8220;&#30149;&#24577;&#8221;&#65292;&#20174;&#32780;&#21487;&#36873;&#19981;&#21152;&#27491;&#21017;&#21270;&#39033;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/1503.02143</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#26680;&#22238;&#24402;&#30340;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Model selection of polynomial kernel regression. (arXiv:1503.02143v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1503.02143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#22312;&#22810;&#39033;&#24335;&#26680;&#22238;&#24402;&#20013;&#27491;&#21017;&#21270;&#39033;&#30340;&#20316;&#29992;&#21482;&#26159;&#20026;&#20102;&#35268;&#36991;&#26680;&#30697;&#38453;&#30340;&#8220;&#30149;&#24577;&#8221;&#65292;&#20174;&#32780;&#21487;&#36873;&#19981;&#21152;&#27491;&#21017;&#21270;&#39033;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#26680;&#22238;&#24402;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#21644;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#31574;&#30053;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#65292;&#22810;&#39033;&#24335;&#26680;&#30340;&#27425;&#25968;&#21644;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#20010;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#31574;&#30053;&#26469;&#36873;&#25321;&#36825;&#20123;&#21442;&#25968;&#12290;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#26368;&#22351;&#24773;&#20917;&#30340;&#23398;&#20064;&#36895;&#29575;&#20998;&#26512;&#65292;&#25105;&#20204;&#34920;&#26126;&#22810;&#39033;&#24335;&#26680;&#22238;&#24402;&#20013;&#30340;&#27491;&#21017;&#21270;&#39033;&#26159;&#19981;&#24517;&#35201;&#30340;&#65307;&#25442;&#21477;&#35805;&#35828;&#65292;&#24403;&#22810;&#39033;&#24335;&#26680;&#30340;&#27425;&#25968;&#36866;&#24403;&#35843;&#25972;&#26102;&#65292;&#27491;&#21017;&#21270;&#21442;&#25968;&#21487;&#20197;&#20219;&#24847;&#24555;&#22320;&#20943;&#23567;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#27491;&#21017;&#21270;&#39033;&#26159;&#24517;&#38656;&#30340;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#22810;&#39033;&#24335;&#26680;&#22238;&#24402;&#20013;&#27491;&#21017;&#21270;&#39033;&#30340;&#20316;&#29992;&#21482;&#26159;&#20026;&#20102;&#35268;&#36991;&#26680;&#30697;&#38453;&#30340;&#8220;&#30149;&#24577;&#8221;&#12290;&#26412;&#25991;&#30340;&#31532;&#20108;&#20010;&#30446;&#30340;&#26159;&#22522;&#20110;&#27492;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polynomial kernel regression is one of the standard and state-of-the-art learning strategies. However, as is well known, the choices of the degree of polynomial kernel and the regularization parameter are still open in the realm of model selection. The first aim of this paper is to develop a strategy to select these parameters. On one hand, based on the worst-case learning rate analysis, we show that the regularization term in polynomial kernel regression is not necessary. In other words, the regularization parameter can decrease arbitrarily fast when the degree of the polynomial kernel is suitable tuned. On the other hand,taking account of the implementation of the algorithm, the regularization term is required. Summarily, the effect of the regularization term in polynomial kernel regression is only to circumvent the " ill-condition" of the kernel matrix. Based on this, the second purpose of this paper is to propose a new model selection strategy, and then design an efficient learning
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29305;&#23450;&#30340;&#26680;&#20989;&#25968;&#31867;&#20013;&#65292;$l^{q}$ &#27491;&#21017;&#21270;&#23398;&#20064;&#22312;&#19981;&#21516;&#38454;&#25968; $q$ &#19979;&#37117;&#20855;&#26377;&#30456;&#20284;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/1307.6616</link><description>&lt;p&gt;
$l^q$&#27491;&#21017;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#26159;&#21542;&#20381;&#36182;&#20110;$q$&#65311;&#19968;&#20010;&#21542;&#23450;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does generalization performance of $l^q$ regularization learning depend on $q$? A negative example. (arXiv:1307.6616v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1307.6616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29305;&#23450;&#30340;&#26680;&#20989;&#25968;&#31867;&#20013;&#65292;$l^{q}$ &#27491;&#21017;&#21270;&#23398;&#20064;&#22312;&#19981;&#21516;&#38454;&#25968; $q$ &#19979;&#37117;&#20855;&#26377;&#30456;&#20284;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$l^q$-&#27491;&#21017;&#21270;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#20013;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#25216;&#26415;&#12290;&#23427;&#36890;&#36807;&#36866;&#24403;&#32553;&#23567;&#31995;&#25968;&#26469;&#25552;&#39640;&#26426;&#22120;&#65288;&#27169;&#22411;&#65289;&#30340;&#27867;&#21270;&#65288;&#39044;&#27979;&#65289;&#33021;&#21147;&#12290;&#22312;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#38454;&#25968; $q$ &#36873;&#25321;&#19979;&#65292;$l^q$ &#20272;&#35745;&#22120;&#30340;&#24418;&#29366;&#19981;&#21516;&#12290;&#29305;&#21035;&#22320;&#65292;$l^1$ &#23548;&#33268; LASSO &#20272;&#35745;&#65292;&#32780; $l^{2}$ &#23545;&#24212;&#20110;&#24179;&#28369;&#30340;&#23725;&#22238;&#24402;&#12290;&#36825;&#20351;&#24471;&#38454;&#25968; $q$ &#25104;&#20026;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#28508;&#22312;&#35843;&#21442;&#21442;&#25968;&#12290;&#20026;&#20102;&#20419;&#36827; $l^{q}$-&#27491;&#21017;&#21270;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#25171;&#31639;&#23547;&#25214;&#19968;&#31181;&#24314;&#27169;&#31574;&#30053;&#65292;&#21487;&#20197;&#36991;&#20813;&#22312; $q$ &#19978;&#36827;&#34892;&#31934;&#32454;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#26679;&#30340;&#31934;&#31070;&#19979;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#32622;&#20110;&#19968;&#20010;&#26679;&#26412;&#30456;&#20851;&#20551;&#35774;&#31354;&#38388;&#65288;SDHS&#65289;&#19979;&#30340; $l^{q}$-&#27491;&#21017;&#21270;&#26680;&#23398;&#20064;&#30340;&#19968;&#33324;&#26694;&#26550;&#20013;&#12290;&#23545;&#20110;&#19968;&#31867;&#25351;&#23450;&#30340;&#26680;&#20989;&#25968;&#65292;&#22312; $0&lt;q&lt;\infty$ &#30340;&#25152;&#26377; $l^{q}$ &#20272;&#35745;&#20540;&#37117;&#20855;&#26377;&#31867;&#20284;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#36825;&#20123;&#20272;&#35745;&#36793;&#30028;&#26159;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
$l^q$-regularization has been demonstrated to be an attractive technique in machine learning and statistical modeling. It attempts to improve the generalization (prediction) capability of a machine (model) through appropriately shrinking its coefficients. The shape of a $l^q$ estimator differs in varying choices of the regularization order $q$. In particular, $l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smooth ridge regression. This makes the order $q$ a potential tuning parameter in applications. To facilitate the use of $l^{q}$-regularization, we intend to seek for a modeling strategy where an elaborative selection on $q$ is avoidable. In this spirit, we place our investigation within a general framework of $l^{q}$-regularized kernel learning under a sample dependent hypothesis space (SDHS). For a designated class of kernel functions, we show that all $l^{q}$ estimators for $0&lt; q &lt; \infty$ attain similar generalization error bounds. These estimated bounds are a
&lt;/p&gt;</description></item></channel></rss>