<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#27604;&#35270;&#22270;&#22686;&#24378;&#31574;&#30053;&#12289;&#20301;&#32622;&#24863;&#30693;&#21644;&#35821;&#20041;&#24863;&#30693;&#27491;&#26679;&#26412;&#37319;&#26679;&#31574;&#30053;&#20197;&#21450;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#20811;&#26381;&#22270;&#25968;&#25454;&#22686;&#24378;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.02810</link><description>&lt;p&gt;
&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative-Contrastive Heterogeneous Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#27604;&#35270;&#22270;&#22686;&#24378;&#31574;&#30053;&#12289;&#20301;&#32622;&#24863;&#30693;&#21644;&#35821;&#20041;&#24863;&#30693;&#27491;&#26679;&#26412;&#37319;&#26679;&#31574;&#30053;&#20197;&#21450;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#20811;&#26381;&#22270;&#25968;&#25454;&#22686;&#24378;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#34920;&#36798;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#20851;&#31995;&#65292;&#21253;&#25324;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#21463;&#33258;&#30417;&#30563;&#23398;&#20064;&#21551;&#21457;&#65292;&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#36776;&#21035;&#22120;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#30340;&#31163;&#25955;&#21644;&#25277;&#35937;&#29305;&#24615;&#65292;&#25968;&#25454;&#22686;&#24378;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;\textit{&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;(GC-HGNN)}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02810v1 Announce Type: new  Abstract: Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges. In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and discriminators for downstream tasks. However, data augmentation is still limited due to the discrete and abstract nature of graphs. To tackle the above limitations, we propose a novel \textit{Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm. This paradigm includes: 1) A contrastive view augmentation strategy by using masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples. 3) A hierarchical contrastive learning strategy for capturing local and global informa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2404.01714</link><description>&lt;p&gt;
&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#30340;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#21152;&#24555;&#22521;&#35757;&#36895;&#24230;&#24182;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#20849;&#36717;&#26799;&#24230;&#20462;&#27491;&#20026;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#65292;&#24182;&#23558;&#20854;&#24182;&#20837;&#36890;&#29992;Adam&#20013;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CG-like-Adam&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#29992;Adam&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#22343;&#30001;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#26367;&#25442;&#12290;&#25910;&#25947;&#20998;&#26512;&#22788;&#29702;&#20102;&#19968;&#38454;&#30697;&#20272;&#35745;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#31995;&#25968;&#20026;&#24120;&#25968;&#19988;&#19968;&#38454;&#30697;&#20272;&#35745;&#26080;&#20559;&#30340;&#24773;&#20917;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20102;&#22522;&#20110;CIFAR10/100&#25968;&#25454;&#38598;&#30340;&#25152;&#25552;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01714v1 Announce Type: cross  Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19024</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#38750;&#23545;&#31216;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#31574;&#30053;&#35757;&#32451;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#19968;&#20010;&#24120;&#29992;&#30340;&#31616;&#21270;&#20551;&#35774;&#26159;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#37117;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#22870;&#21169;&#27169;&#22411;&#29420;&#31435;&#30340;&#23545;&#31216;&#24615;&#65306;&#22870;&#21169;&#21487;&#33021;&#19981;&#28385;&#36275;&#19982;&#21160;&#21147;&#23398;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21482;&#20551;&#23450;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#65292;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#23398;&#20064;&#20013;&#21487;&#24212;&#29992;&#23545;&#31216;&#25216;&#26415;&#30340;&#38382;&#39064;&#33539;&#22260;&#12290;&#25105;&#20204;&#21033;&#29992;&#21345;&#22612;&#24681;&#31227;&#21160;&#26694;&#26550;&#26041;&#27861;&#24341;&#20837;&#19968;&#31181;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#36896;&#65292;&#36825;&#31181;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#25351;&#23450;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#21040;&#20102;&#26356;&#20934;&#30830;&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19024v1 Announce Type: cross  Abstract: Recent work in reinforcement learning has leveraged symmetries in the model to improve sample efficiency in training a policy. A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry. However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics. In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in reinforcement learning and learning in control theory where symmetry techniques can be applied. We use Cartan's moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries. We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#35328;&#25351;&#23548;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14874</link><description>&lt;p&gt;
WeatherProof:&#20511;&#21161;&#35821;&#35328;&#25351;&#23548;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
WeatherProof: Leveraging Language Guidance for Semantic Segmentation in Adverse Weather
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#35328;&#25351;&#23548;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25512;&#26029;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#25293;&#25668;&#30340;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#22270;&#12290;&#25105;&#20204;&#39318;&#20808;&#26816;&#26597;&#20102;&#38024;&#23545;&#21463;&#22825;&#27668;&#26465;&#20214;&#65288;&#22914;&#38632;&#12289;&#38654;&#25110;&#38634;&#65289;&#24433;&#21709;&#32780;&#38477;&#32423;&#30340;&#22270;&#20687;&#30340;&#29616;&#26377;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#22312;&#26228;&#26391;&#22825;&#27668;&#19979;&#25293;&#25668;&#30340;&#22270;&#20687;&#30456;&#27604;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#20026;&#20102;&#25511;&#21046;&#22330;&#26223;&#32467;&#26500;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WeatherProof&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20934;&#30830;&#30340;&#26228;&#26391;&#21644;&#24694;&#21155;&#22825;&#27668;&#22270;&#20687;&#23545;&#30340;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#20849;&#20139;&#30456;&#21516;&#30340;&#22330;&#26223;&#12290;&#36890;&#36807;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#38169;&#35823;&#27169;&#24335;&#65292;&#21457;&#29616;&#23427;&#20204;&#23545;&#22312;&#25293;&#25668;&#36807;&#31243;&#20013;&#26045;&#21152;&#22312;&#22270;&#20687;&#19978;&#30340;&#19981;&#21516;&#22825;&#27668;&#25928;&#24212;&#30340;&#39640;&#24230;&#22797;&#26434;&#32452;&#21512;&#25935;&#24863;&#12290;&#20026;&#20102;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#20316;&#20026;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#35782;&#21035;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#30340;&#24433;&#21709;&#24182;&#23558;&#20854;&#27880;&#20837;&#20026;&#8220;&#36741;&#21161;&#20449;&#24687;&#8221;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#35821;&#35328;&#25351;&#23548;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14874v1 Announce Type: cross  Abstract: We propose a method to infer semantic segmentation maps from images captured under adverse weather conditions. We begin by examining existing models on images degraded by weather conditions such as rain, fog, or snow, and found that they exhibit a large performance drop as compared to those captured under clear weather. To control for changes in scene structures, we propose WeatherProof, the first semantic segmentation dataset with accurate clear and adverse weather image pairs that share an underlying scene. Through this dataset, we analyze the error modes in existing models and found that they were sensitive to the highly complex combination of different weather effects induced on the image during capture. To improve robustness, we propose a way to use language as guidance by identifying contributions of adverse weather conditions and injecting that as "side information". Models trained using our language guidance exhibit performance
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.12403</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30340;&#21407;&#22240;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12403
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#29992;&#25143;&#36827;&#34892;&#20154;&#38469;&#35752;&#35770;&#21644;&#34920;&#36798;&#35266;&#28857;&#30340;&#37325;&#35201;&#22330;&#25152;&#65292;&#20294;&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#30340;&#22806;&#31435;&#38754;&#21644;&#21311;&#21517;&#24615;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#21457;&#24067;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#12290;&#37492;&#20110;&#36825;&#20123;&#24179;&#21488;&#30340;&#24222;&#22823;&#35268;&#27169;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#26631;&#35760;&#20167;&#24680;&#35328;&#35770;&#30340;&#38656;&#27714;&#26085;&#30410;&#36843;&#20999;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#31181;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#40657;&#30418;&#26041;&#27861;&#22312;&#35774;&#35745;&#19978;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25110;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#35299;&#20915;&#35299;&#37322;&#24615;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#65292;&#35757;&#32451;&#22522;&#30784;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#36890;&#36807;&#35774;&#35745;&#23454;&#29616;&#24544;&#23454;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;LLM&#30340;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#21644;&#26368;&#20808;&#36827;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20351;&#36825;&#20123;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12403v1 Announce Type: cross  Abstract: Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#27491;&#20132;&#22522;&#20998;&#27573;&#22810;&#39033;&#24335;&#36924;&#36817;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.08579</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#30340;&#27491;&#20132;&#22522;&#20998;&#27573;&#22810;&#39033;&#24335;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#27491;&#20132;&#22522;&#20998;&#27573;&#22810;&#39033;&#24335;&#36924;&#36817;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#27573;&#22810;&#39033;&#24335;&#65288;PPs&#65289;&#22312;&#20960;&#20010;&#24037;&#31243;&#23398;&#31185;&#20013;&#34987;&#20351;&#29992;&#65292;&#27604;&#22914;&#22312;&#36712;&#36857;&#35268;&#21010;&#20013;&#65292;&#29992;&#26469;&#36924;&#36817;&#20197;&#19968;&#32452;&#28857;&#32473;&#20986;&#30340;&#20301;&#32622;&#36718;&#24275;&#12290; &#37492;&#20110;&#36924;&#36817;&#30446;&#26631;&#20197;&#21450;&#29305;&#23450;&#39046;&#22495;&#35201;&#27714;&#65292;&#27604;&#22914;Ck-&#36830;&#32493;&#24615;&#65292;&#21487;&#20197;&#34987;&#26500;&#36896;&#20026;&#19968;&#20010;&#26041;&#31243;&#32452;&#65292;&#32467;&#26524;&#21487;&#20197;&#30452;&#25509;&#35745;&#31639;&#65292;&#36825;&#26679;&#30340;&#38381;&#24335;&#35299;&#23545;&#20110;&#22810;&#39033;&#24335;&#27425;&#25968;&#12289;&#22810;&#39033;&#24335;&#22522;&#30784;&#25110;&#32773;&#28155;&#21152;&#36827;&#19968;&#27493;&#30340;&#29305;&#23450;&#39046;&#22495;&#35201;&#27714;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#28789;&#27963;&#24615;&#12290;&#36275;&#22815;&#22797;&#26434;&#30340;&#20248;&#21270;&#30446;&#26631;&#24456;&#24555;&#35201;&#27714;&#20351;&#29992;&#25968;&#20540;&#26041;&#27861;&#65292;&#27604;&#22914;&#26799;&#24230;&#19979;&#38477;&#12290;&#30001;&#20110;&#26799;&#24230;&#19979;&#38477;&#26159;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#26680;&#24515;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#27604;&#22914;TensorFlow&#25552;&#20379;&#20102;&#19968;&#22871;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#33021;&#36866;&#29992;&#20110;&#35757;&#32451;&#20219;&#21153;&#20043;&#22806;&#30340;&#24191;&#27867;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;PP&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08579v1 Announce Type: new  Abstract: Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points. While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements. Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent. Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs. Our approach is to utilize the versatility of PP mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#20197;&#21450;&#20998;&#26512;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#27809;&#26377;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20989;&#25968;&#31867;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.01671</link><description>&lt;p&gt;
&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#65306;&#32479;&#35745;&#26816;&#39564;&#12289;&#24230;&#37327;&#29109;&#20013;&#30340;&#38477;&#32500;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#20197;&#21450;&#20998;&#26512;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#27809;&#26377;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20989;&#25968;&#31867;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21015;&#19981;&#21464;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#20197;&#21033;&#29992;&#26469;&#31616;&#21270;&#22797;&#26434;&#38382;&#39064;&#30340;&#26368;&#24120;&#35265;&#30340;&#23545;&#31216;&#24615;&#20043;&#19968;&#12290;&#36817;&#24180;&#26469;&#20851;&#20110;&#26500;&#24314;&#25490;&#21015;&#19981;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#30740;&#31350;&#27963;&#21160;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#30340;&#21464;&#37327;&#22914;&#20309;&#32479;&#35745;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#21364;&#40092;&#26377;&#30740;&#31350;&#65292;&#20854;&#20013;&#26679;&#26412;&#37327;&#20801;&#35768;&#38543;&#30528;&#32500;&#25968;&#30340;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#22312;&#32479;&#35745;&#29702;&#35770;&#26041;&#38754;&#65292;&#20851;&#20110;&#25490;&#21015;&#19981;&#21464;&#24615;&#22914;&#20309;&#24110;&#21161;&#20272;&#35745;&#20013;&#38477;&#32500;&#30340;&#30693;&#35782;&#29978;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20960;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22238;&#39038;&#24182;&#25506;&#35752;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27979;&#35797;&#22810;&#20803;&#20998;&#24067;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20551;&#35774;&#65307;&#65288;ii&#65289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#65307;&#65288;iii&#65289;&#20998;&#26512;&#20809;&#28369;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#24182;&#23558;&#20854;&#19982;&#26410;&#24378;&#21152;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#23545;&#24212;&#20989;&#25968;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01671v1 Announce Type: new  Abstract: Permutation invariance is among the most common symmetry that can be exploited to simplify complex problems in machine learning (ML). There has been a tremendous surge of research activities in building permutation invariant ML architectures. However, less attention is given to how to statistically test for permutation invariance of variables in a multivariate probability distribution where the dimension is allowed to grow with the sample size. Also, in terms of a statistical theory, little is known about how permutation invariance helps with estimation in reducing dimensions. In this paper, we take a step back and examine these questions in several fundamental problems: (i) testing the assumption of permutation invariance of multivariate distributions; (ii) estimating permutation invariant densities; (iii) analyzing the metric entropy of smooth permutation invariant function classes and compare them with their counterparts without impos
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.13573</link><description>&lt;p&gt;
&#20219;&#21153;&#24453;&#21150;&#65306;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#39640;&#25928;&#29983;&#25104;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
ToDo: Token Downsampling for Efficient Generation of High-Resolution Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13573
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#25105;&#20204;&#21487;&#20197;&#22312;&#21512;&#29702;&#26102;&#38388;&#21644;&#20869;&#23384;&#38480;&#21046;&#20869;&#22788;&#29702;&#30340;&#22270;&#20687;&#22823;&#23567;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#23494;&#38598;&#27880;&#24847;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#29305;&#24449;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#31232;&#30095;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861; ToDo&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20851;&#38190;&#21644;&#20540;&#20196;&#29260;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;&#65292;&#21487;&#23558;&#24120;&#35265;&#22823;&#23567;&#30340; Stable Diffusion &#25512;&#29702;&#21152;&#36895;&#33267;&#22810;&#36798;2&#20493;&#65292;&#23545;&#20110;2048x2048&#31561;&#39640;&#20998;&#36776;&#29575;&#65292;&#21152;&#36895;&#27604;&#21487;&#36798;4.5&#20493;&#25110;&#26356;&#39640;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24179;&#34913;&#39640;&#25928;&#21534;&#21520;&#37327;&#21644;&#20445;&#30495;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13573v1 Announce Type: cross  Abstract: Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#33258;&#32452;&#32455;&#25104;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21327;&#21516;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#23458;&#25143;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12812</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#29992;&#20110;&#22312;&#32447;&#20010;&#24615;&#21270;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Decentralized Algorithms for Online Personalized Mean Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#33258;&#32452;&#32455;&#25104;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21327;&#21516;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#23458;&#25143;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#20854;&#20182;&#20195;&#29702;&#21512;&#20316;&#21487;&#33021;&#26377;&#25152;&#24110;&#21161;&#65292;&#20294;&#24403;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#26102;&#65292;&#20250;&#24341;&#20837;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#30340;&#23458;&#25143;&#65292;&#36825;&#20010;&#38382;&#39064;&#20027;&#35201;&#20173;&#26410;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#19968;&#20010;&#31616;&#21270;&#29256;&#26412;&#30340;&#26222;&#36941;&#38382;&#39064;&#65292;&#21363;&#27599;&#20010;&#20195;&#29702;&#38543;&#26102;&#38388;&#20174;&#23454;&#20540;&#20998;&#24067;&#20013;&#25910;&#38598;&#26679;&#26412;&#26469;&#20272;&#35745;&#20854;&#22343;&#20540;&#12290;&#29616;&#26377;&#31639;&#27861;&#38754;&#20020;&#30528;&#19981;&#20999;&#23454;&#38469;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65288;&#19982;&#20195;&#29702;&#25968;&#37327;A&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20195;&#29702;&#33258;&#32452;&#32455;&#25104;&#19968;&#20010;&#22270;&#65292;&#20351;&#24471;&#27599;&#20010;&#20195;&#29702;&#21482;&#33021;&#19982;&#36873;&#23450;&#25968;&#37327;&#30340;&#23545;&#31561;&#20307;r&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#21327;&#20316;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65306;&#19968;&#31181;&#28789;&#24863;&#26469;&#28304;&#20110;&#20449;&#24565;&#20256;&#25773;&#65292;&#21478;&#19968;&#31181;&#37319;&#29992;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12812v1 Announce Type: new  Abstract: In numerous settings, agents lack sufficient data to directly learn a model. Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ. A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved. This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean. Existing algorithms face impractical space and time complexities (quadratic in the number of agents A). To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03312</link><description>&lt;p&gt;
&#28145;&#24230;&#34917;&#20840;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation for Depth Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#22312;&#19968;&#20123;&#65288;&#28304;&#65289;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36716;&#31227;&#21040;&#30446;&#26631;&#27979;&#35797;&#25968;&#25454;&#26102;&#65292;&#24120;&#24120;&#20250;&#35266;&#23519;&#21040;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#39046;&#22495;&#24046;&#36317;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22914;&#39046;&#22495;&#36866;&#24212;&#65288;DA&#65289;&#65292;&#21487;&#33021;&#38656;&#35201;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#28304;&#25968;&#25454;&#65288;&#36890;&#24120;&#19981;&#21487;&#29992;&#65289;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;&#26080;&#28304;DA&#65292;&#21017;&#38656;&#35201;&#22810;&#27425;&#36890;&#36807;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#65292;&#21363;&#20174;&#21333;&#20010;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#31232;&#30095;&#28145;&#24230;&#22270;&#25512;&#26029;&#20986;&#23494;&#38598;&#28145;&#24230;&#22270;&#30340;&#20219;&#21153;&#65292;&#20197;&#22312;&#19968;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#27599;&#31181;&#25968;&#25454;&#27169;&#24577;&#20013;&#30340;&#39046;&#22495;&#36716;&#31227;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#31232;&#30095;&#28145;&#24230;&#27169;&#24577;&#23637;&#29616;&#20986;&#27604;&#22270;&#20687;&#26356;&#23567;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#22240;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#28304;&#39046;&#22495;&#20013;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22359;&#65292;&#23427;&#20445;&#30041;&#20102;&#20174;&#20165;&#32534;&#30721;&#31232;&#30095;&#28145;&#24230;&#29305;&#24449;&#21040;&#32534;&#30721;&#22270;&#20687;&#21644;&#31232;&#30095;&#28145;&#24230;&#30340;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;&#22312;&#27979;&#35797;&#26102;&#38388;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#23884;&#20837;&#27169;&#22359;&#23454;&#29616;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#65292;&#21253;&#25324;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25581;&#31034;&#20102;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02025</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Constraint Formulations in Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#65292;&#21253;&#25324;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25581;&#31034;&#20102;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#26102;&#65292;&#30830;&#20445;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#23433;&#20840;RL&#25104;&#20026;&#19968;&#31181;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#23433;&#20840;&#20248;&#21270;&#20195;&#29702;&#31574;&#30053;&#30340;&#22522;&#26412;&#32780;&#24378;&#22823;&#30340;&#33539;&#20363;&#12290;&#22522;&#20110;&#32422;&#26463;&#20934;&#21017;&#30340;&#23433;&#20840;RL&#26041;&#27861;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#23433;&#20840;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#39044;&#26399;&#32047;&#31215;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#22312;RL&#20013;&#23454;&#29616;&#23433;&#20840;&#24615;&#30340;&#23581;&#35797;&#28608;&#22686;&#65292;&#20294;&#30001;&#20110;&#32422;&#26463;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35752;&#35770;&#24456;&#23569;&#65292;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#24615;&#20102;&#35299;&#20173;&#28982;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30693;&#35782;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20195;&#34920;&#24615;&#32422;&#26463;&#24418;&#24335;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#20197;&#21450;&#38024;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#30340;&#31934;&#36873;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25581;&#31034;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring safety is critical when applying reinforcement learning (RL) to real-world problems. Consequently, safe RL emerges as a fundamental and powerful paradigm for safely optimizing an agent's policy from experimental data. A popular safe RL approach is based on a constrained criterion, which solves the problem of maximizing expected cumulative reward under safety constraints. Though there has been recently a surge of such attempts to achieve safety in RL, a systematic understanding of the field is difficult due to 1) the diversity of constraint representations and 2) little discussion of their interrelations. To address this knowledge gap, we provide a comprehensive review of representative constraint formulations, along with a curated selection of algorithms specifically designed for each formulation. Furthermore, we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations. We conclude with a discussion of the current 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01744</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22270;&#35299;&#37322;&#25581;&#31034;&#20998;&#23376;&#25104;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unveiling Molecular Moieties through Hierarchical Graph Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#25903;&#25345;&#20307;&#22806;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#24050;&#32463;&#20986;&#29616;&#22810;&#24180;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#26550;&#26500;&#23454;&#29616;&#39640;&#31934;&#24230;&#22810;&#38774;&#26631;&#31579;&#36873;&#30340;GNN&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#22312;&#21407;&#23376;&#12289;&#29615;&#21644;&#25972;&#20010;&#20998;&#23376;&#23618;&#38754;&#19978;&#30452;&#25509;&#25429;&#33719;&#20449;&#24687;&#65292;&#20174;&#32780;&#25214;&#21040;&#19982;&#29983;&#29289;&#27963;&#24615;&#39044;&#27979;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#25903;&#25345;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#30340;&#20108;&#21313;&#20010;&#32454;&#32990;&#21608;&#26399;&#20381;&#36182;&#24615;&#28608;&#37238;&#38774;&#26631;&#19978;&#25253;&#36947;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;GNN&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#36229;&#36234;&#20102;&#20316;&#32773;&#25552;&#20986;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20165;&#38024;&#23545;CDK1&#30340;&#39640;&#28789;&#25935;&#24230;&#29256;&#26412;&#30340;GNN&#65292;&#20197;&#20351;&#29992;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#26469;&#36991;&#20813;&#22810;&#31867;&#21035;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#24046;&#12290;&#20998;&#23618;&#35299;&#37322;&#22120;&#24050;&#32463;&#30001;&#19968;&#20301;&#19987;&#23478;&#21270;&#23398;&#23478;&#22312;19&#20010;CDK1&#25209;&#20934;&#33647;&#29289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Graph Neural Networks (GNN) have emerged in very recent years as a powerful tool for supporting in silico Virtual Screening. In this work we present a GNN which uses Graph Convolutional architectures to achieve very accurate multi-target screening. We also devised a hierarchical Explainable Artificial Intelligence (XAI) technique to catch information directly at atom, ring, and whole molecule level by leveraging the message passing mechanism. In this way, we find the most relevant moieties involved in bioactivity prediction. Results: We report a state-of-the-art GNN classifier on twenty Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms previous SOTA approaches proposed by the authors. Moreover, a CDK1-only high-sensitivity version of the GNN has been designed to use our explainer in order to avoid the inherent bias of multi-class models. The hierarchical explainer has been validated by an expert chemist on 19 approved drugs on CDK1. Our explainer 
&lt;/p&gt;</description></item><item><title>PetriRL&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;Petri&#32593;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00046</link><description>&lt;p&gt;
&#24341;&#20837;PetriRL&#65306;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#30340;JSSP&#35299;&#20915;&#26041;&#26696;&#30340;&#21019;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00046
&lt;/p&gt;
&lt;p&gt;
PetriRL&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;Petri&#32593;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#36710;&#38388;&#20013;&#65292;&#20248;&#36136;&#35843;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#26377;&#38480;&#30340;&#21487;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#20854;&#22312;&#24037;&#19994;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Petri&#32593;&#23545;&#20316;&#19994;&#36710;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#33021;&#30452;&#25509;&#23558;&#21407;&#22987;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#65292;&#26080;&#38656;&#23545;JSSP&#23454;&#20363;&#36827;&#34892;&#39044;&#22788;&#29702;&#25104;&#38750;&#20132;&#26367;&#22270;&#12290;Petri&#32593;&#30340;&#25511;&#21046;&#33021;&#21147;&#36824;&#21487;&#20197;&#31649;&#29702;&#36807;&#31243;&#20013;&#30340;&#33258;&#21160;&#21270;&#32452;&#20214;&#65292;&#20351;&#20195;&#29702;&#20154;&#33021;&#22815;&#19987;&#27880;&#20110;&#20851;&#38190;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#36164;&#28304;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#22312;&#20844;&#20849;&#27979;&#35797;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#36328;&#19968;&#31995;&#21015;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65288;&#21253;&#25324;&#21551;&#21457;&#24335;&#31639;&#27861;&#12289;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#23398;&#20064;&#31639;&#27861;&#65289;&#36827;&#34892;&#30340;&#27604;&#36739;&#20998;&#26512;&#31361;&#20986;&#20102;&#20854;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality scheduling in industrial job shops is crucial. Although neural networks excel in solving these problems, their limited explainability hinders their widespread industrial adoption. In this research, we introduce an innovative framework for solving job shop scheduling problems (JSSP). Our methodology leverages Petri nets to model the job shop, not only improving explainability but also enabling direct incorporation of raw data without the need to preprocess JSSP instances into disjunctive graphs. The Petri net, with its controlling capacities, also governs the automated components of the process, allowing the agent to focus on critical decision-making, particularly resource allocation. The integration of event-based control and action masking in our approach yields competitive performance on public test benchmarks. Comparative analyses across a wide spectrum of optimization solutions, including heuristics, metaheuristics, and learning-based algorithms, highlight the competitivene
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Beta Divergence&#36827;&#34892;&#21464;&#20998;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26410;&#26631;&#35760;&#21644;&#22024;&#26434;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#22312;&#20154;&#33080;&#29702;&#35299;&#39046;&#22495;&#21462;&#24471;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2312.00824</link><description>&lt;p&gt;
&#20351;&#29992;Beta Divergence&#36827;&#34892;&#21464;&#20998;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#20154;&#33080;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Variational Self-Supervised Contrastive Learning Using Beta Divergence For Face Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00824
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Beta Divergence&#36827;&#34892;&#21464;&#20998;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26410;&#26631;&#35760;&#21644;&#22024;&#26434;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#22312;&#20154;&#33080;&#29702;&#35299;&#39046;&#22495;&#21462;&#24471;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;&#26410;&#26631;&#35760;&#21644;&#22024;&#26434;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#36776;&#21035;&#24615;&#30340;&#35821;&#20041;&#31354;&#38388;&#22312;&#22810;&#26631;&#31614;&#35774;&#32622;&#20013;&#20173;&#26410;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25968;&#25454;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#19988;&#22522;&#20110;&#21464;&#20998;&#26041;&#27861;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#65288;VCL&#65289;&#21033;&#29992;&#21464;&#20998;&#23545;&#27604;&#23398;&#20064;&#19982;beta-divergence&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#31283;&#20581;&#22320;&#23398;&#20064;&#65292;&#21253;&#25324;&#26410;&#21152;&#24037;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22312;&#32447;&#24615;&#35780;&#20272;&#21644;&#24494;&#35843;&#22330;&#26223;&#20013;&#20351;&#29992;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#20154;&#33080;&#29702;&#35299;&#39046;&#22495;&#12290;&#22312;&#20960;&#20046;&#25152;&#26377;&#27979;&#35797;&#22330;&#26223;&#20013;&#65292;VCL&#22343;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00824v2 Announce Type: replace-cross  Abstract: Learning a discriminative semantic space using unlabelled and noisy data remains unaddressed in a multi-label setting. We present a contrastive self-supervised learning method which is robust to data noise, grounded in the domain of variational methods. The method (VCL) utilizes variational contrastive learning with beta-divergence to learn robustly from unlabelled datasets, including uncurated and noisy datasets. We demonstrate the effectiveness of the proposed method through rigorous experiments including linear evaluation and fine-tuning scenarios with multi-label datasets in the face understanding domain. In almost all tested scenarios, VCL surpasses the performance of state-of-the-art self-supervised methods, achieving a noteworthy increase in accuracy.
&lt;/p&gt;</description></item><item><title>DMODE&#26159;&#19968;&#31181;&#26080;&#38656;&#29289;&#20307;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#22823;&#23567;&#21464;&#21270;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#21644;&#26410;&#30693;&#29289;&#20307;&#30340;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#20013;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#23545;&#35937;&#29305;&#23450;&#32447;&#32034;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2210.12596</link><description>&lt;p&gt;
DMODE: &#26080;&#38656;&#29305;&#23450;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
DMODE: Differential Monocular Object Distance Estimation Module without Class Specific Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.12596
&lt;/p&gt;
&lt;p&gt;
DMODE&#26159;&#19968;&#31181;&#26080;&#38656;&#29289;&#20307;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#22823;&#23567;&#21464;&#21270;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#21644;&#26410;&#30693;&#29289;&#20307;&#30340;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#20013;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#23545;&#35937;&#29305;&#23450;&#32447;&#32034;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21333;&#20010;&#25668;&#20687;&#22836;&#27979;&#37327;&#29289;&#20307;&#36317;&#31163;&#26159;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#32780;&#19981;&#38656;&#35201;&#31435;&#20307;&#35270;&#35273;&#21644;&#28608;&#20809;&#38647;&#36798;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25506;&#35752;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#29289;&#20307;&#31867;&#21035;&#30693;&#35782;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#22312;&#32570;&#20047;&#36825;&#20123;&#24773;&#22659;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#29289;&#20307;&#29305;&#23450;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#32034;&#21487;&#33021;&#20250;&#35823;&#23548;&#19982;&#33539;&#22260;&#24191;&#27867;&#21464;&#21270;&#25110;&#23545;&#25239;&#24773;&#20917;&#19979;&#30340;&#23545;&#35937;&#65292;&#36825;&#26159;&#38754;&#21521;&#23545;&#35937;&#19981;&#21487;&#30693;&#36317;&#31163;&#20272;&#35745;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DMODE&#65292;&#19968;&#31181;&#19981;&#38656;&#35201;&#29289;&#20307;&#31867;&#21035;&#30693;&#35782;&#30340;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#31867;&#21035;&#19981;&#21487;&#30693;&#26041;&#27861;&#12290;DMODE&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22823;&#23567;&#27874;&#21160;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#20272;&#35745;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#26410;&#30693;&#29289;&#20307;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.12596v2 Announce Type: replace-cross  Abstract: Utilizing a single camera for measuring object distances is a cost-effective alternative to stereo-vision and LiDAR. Although monocular distance estimation has been explored in the literature, most existing techniques rely on object class knowledge to achieve high performance. Without this contextual data, monocular distance estimation becomes more challenging, lacking reference points and object-specific cues. However, these cues can be misleading for objects with wide-range variation or adversarial situations, which is a challenging aspect of object-agnostic distance estimation. In this paper, we propose DMODE, a class-agnostic method for monocular distance estimation that does not require object class knowledge. DMODE estimates an object's distance by fusing its fluctuation in size over time with the camera's motion, making it adaptable to various object detectors and unknown objects, thus addressing these challenges. We eva
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;DSQN&#21033;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33021;&#28304;&#39640;&#25928;&#30340;&#25511;&#21046;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2201.09754</link><description>&lt;p&gt;
&#24102;&#27874;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Spiking Q-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.09754
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;DSQN&#21033;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33021;&#28304;&#39640;&#25928;&#30340;&#25511;&#21046;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#29305;&#27530;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#65292;&#26399;&#26395;&#36890;&#36807;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#20197;&#26356;&#23569;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#23558;SNNs&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#65292;&#20026;&#23454;&#29616;&#29616;&#23454;&#25511;&#21046;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#39640;&#25928;&#33021;&#28304;&#26041;&#24335;&#12290;&#30446;&#21069;&#20165;&#26377;&#23569;&#25968;&#22522;&#20110;SNN&#30340;RL&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#35201;&#20040;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#35201;&#20040;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#26469;&#20272;&#31639;&#20540;&#20989;&#25968;&#12290;&#21069;&#32773;&#38656;&#35201;&#20026;&#27599;&#20010;&#22330;&#26223;&#35843;&#25972;&#22823;&#37327;&#36229;&#21442;&#25968;&#65292;&#32780;&#21518;&#32773;&#38480;&#21046;&#20102;&#19981;&#21516;&#31867;&#22411;RL&#31639;&#27861;&#30340;&#24212;&#29992;&#24182;&#24573;&#30053;&#20102;&#35757;&#32451;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#36739;&#22823;&#12290;&#20026;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#26118;&#34411;&#20013;&#21457;&#29616;&#30340;&#38750;&#33033;&#20914;&#38388;&#31070;&#32463;&#20803;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#33033;&#20914;Q&#32593;&#32476;&#65288;DSQN&#65289;&#65292;&#20351;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#21487;&#20197;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.09754v2 Announce Type: replace-cross  Abstract: With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of non-spiking neurons as the representation of Q-value, which can direct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;T-CUR&#20998;&#35299;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.16836</link><description>&lt;p&gt;
&#22522;&#20110;T-CUR&#20998;&#35299;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition. (arXiv:2401.16836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;T-CUR&#20998;&#35299;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#26694;&#26550;&#20869;&#35299;&#20915;NMF&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#21487;&#20998;&#31163;&#24615;&#20551;&#35774;&#65292;&#26368;&#36817;&#28436;&#21464;&#20026;&#21487;&#20998;&#31163;&#30340;&#27010;&#24565;&#12290;&#36825;&#19968;&#36827;&#23637;&#20026;&#21407;&#22987;&#25968;&#25454;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26680;&#24515;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25968;&#25454;&#26356;&#33258;&#28982;&#22320;&#34987;&#34920;&#31034;&#20026;&#22810;&#32500;&#25968;&#32452;&#65292;&#22914;&#22270;&#20687;&#25110;&#35270;&#39057;&#12290;&#23558;NMF&#24212;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#28041;&#21450;&#21521;&#37327;&#21270;&#65292;&#20250;&#23548;&#33268;&#20002;&#22833;&#20851;&#38190;&#30340;&#22810;&#32500;&#24230;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20445;&#30041;&#25968;&#25454;&#20013;&#36825;&#20123;&#22266;&#26377;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#36716;&#21521;&#24352;&#37327;(&#22810;&#32500;&#25968;&#32452;)&#24182;&#21033;&#29992;&#24352;&#37327;t&#20056;&#31215;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#21487;&#20998;&#31163;&#30340;NMF&#25193;&#23637;&#21040;&#24352;&#37327;&#35774;&#32622;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#25105;&#20204;&#25152;&#31216;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;(NTF)&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20132;&#26367;&#32034;&#24341;&#36873;&#25321;&#26041;&#27861;&#26469;&#36873;&#25321;cos
&lt;/p&gt;
&lt;p&gt;
Nonnegative Matrix Factorization (NMF) is an important unsupervised learning method to extract meaningful features from data. To address the NMF problem within a polynomial time framework, researchers have introduced a separability assumption, which has recently evolved into the concept of coseparability. This advancement offers a more efficient core representation for the original data. However, in the real world, the data is more natural to be represented as a multi-dimensional array, such as images or videos. The NMF's application to high-dimensional data involves vectorization, which risks losing essential multi-dimensional correlations. To retain these inherent correlations in the data, we turn to tensors (multidimensional arrays) and leverage the tensor t-product. This approach extends the coseparable NMF to the tensor setting, creating what we term coseparable Nonnegative Tensor Factorization (NTF). In this work, we provide an alternating index selection method to select the cos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LDGD&#30340;&#21028;&#21035;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16497</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#21028;&#21035;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Discriminative Bayesian Gaussian Process Latent Variable Model for High-Dimensional Data. (arXiv:2401.16497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LDGD&#30340;&#21028;&#21035;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24314;&#27169;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#34987;&#22122;&#22768;&#24178;&#25200;&#25110;&#20197;&#19981;&#21516;&#30340;&#27169;&#24577;&#34920;&#31034;&#26102;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#23558;&#39640;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#28508;&#22312;&#30340;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#36825;&#20010;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;&#28508;&#22312;&#21028;&#21035;&#29983;&#25104;&#35299;&#30721;&#22120;&#65288;LDGD&#65289;&#65292;&#23427;&#22312;&#27969;&#24418;&#21457;&#29616;&#36807;&#31243;&#20013;&#21033;&#29992;&#20102;&#25968;&#25454;&#65288;&#25110;&#20854;&#29305;&#24449;&#65289;&#21644;&#30456;&#20851;&#26631;&#31614;&#65288;&#22914;&#31867;&#21035;&#25110;&#21050;&#28608;&#65289;&#12290;&#20026;&#20102;&#25512;&#26029;&#28508;&#22312;&#21464;&#37327;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#35299;&#65292;&#20351;&#24471;LDGD&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;LDGD&#30340;&#24212;&#29992;&#12290;LDGD&#19981;&#20165;&#33021;&#20934;&#30830;&#22320;&#25512;&#26029;&#27969;&#24418;&#65292;&#32780;&#19988;&#22312;&#39044;&#27979;&#26631;&#31614;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting meaningful information from high-dimensional data poses a formidable modeling challenge, particularly when the data is obscured by noise or represented through different modalities. In this research, we propose a novel non-parametric modeling approach, leveraging the Gaussian Process (GP), to characterize high-dimensional data by mapping it to a latent low-dimensional manifold. This model, named the Latent Discriminative Generative Decoder (LDGD), utilizes both the data (or its features) and associated labels (such as category or stimulus) in the manifold discovery process. To infer the latent variables, we derive a Bayesian solution, allowing LDGD to effectively capture inherent uncertainties in the data while enhancing the model's predictive accuracy and robustness. We demonstrate the application of LDGD on both synthetic and benchmark datasets. Not only does LDGD infer the manifold accurately, but its prediction accuracy in anticipating labels surpasses state-of-the-art a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06712</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#36827;&#34892;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#25351;&#23548;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#20154;&#31867;&#20889;&#20316;&#30340;&#36924;&#30495;&#27169;&#20223;&#38754;&#20020;&#30528;&#37325;&#22823;&#28389;&#29992;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#35821;&#35328;&#27169;&#22411;&#36824;&#26159;&#20154;&#31867;&#25776;&#20889;&#32780;&#25104;&#26469;&#23545;&#25239;&#27492;&#31867;&#28389;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#24335;&#34920;&#31034;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#22312;&#38754;&#23545;&#25968;&#25454;&#36716;&#25442;&#26102;&#30340;&#35268;&#32422;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#36991;&#20813;&#20102;&#22312;&#25512;&#29702;&#25110;&#26816;&#27979;&#26102;&#38656;&#35201;&#35775;&#38382;&#21487;&#33021;&#29983;&#25104;&#25991;&#26723;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.04929</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#25552;&#21319;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks. (arXiv:2401.04929v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#21069;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#24341;&#21457;&#20102;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#25285;&#24551;&#12290;&#19968;&#31181;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#26159;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#65292;&#23427;&#20801;&#35768;&#23545;&#25163;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;MIA&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#33021;&#22815;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#21306;&#22495;&#65288;0.01%~1%&#65289;&#23454;&#29616;&#36739;&#39640;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;MIA&#24517;&#39035;&#32771;&#34385;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MIA&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#30340;TPR&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#65288;LDC-MIA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#23558;&#25968;&#25454;&#35760;&#24405;&#20197;&#20854;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01851</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#21147;&#37327;&#65306;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#32622;&#23545;&#33021;&#28304;&#38656;&#27714;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#21464;&#21270;&#23545;&#30456;&#24212;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#25552;&#39640;&#21644;&#39640;&#24615;&#33021;&#30828;&#20214;&#30340;&#21019;&#26032;&#25512;&#21160;&#20102;&#22797;&#26434;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20294;&#20063;&#25903;&#25345;&#20102;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#30340;&#28040;&#38544;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22686;&#21152;&#20154;&#20204;&#23545;&#19968;&#33324;&#35757;&#32451;&#21442;&#25968;&#21644;&#36807;&#31243;&#65288;&#20174;&#23398;&#20064;&#29575;&#21040;&#25209;&#37327;&#22823;&#23567;&#20877;&#21040;&#30693;&#35782;&#20256;&#36755;&#65289;&#30340;&#33021;&#28304;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21021;&#22987;&#21270;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#30828;&#20214;&#37197;&#32622;&#19978;&#35780;&#20272;&#22810;&#31181;&#35774;&#32622;&#65292;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#22312;&#22522;&#20934;&#32467;&#26524;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.11122</link><description>&lt;p&gt;
&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#26159;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#29616;&#20195;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#22522;&#26412;&#36873;&#25321;&#28041;&#21450;&#20284;&#28982;&#20989;&#25968;&#21644;&#20808;&#39564;&#20998;&#24067;&#30340;&#35268;&#33539;&#12289;&#21518;&#39564;&#36924;&#36817;&#22120;&#21644;&#25968;&#25454;&#12290;&#27599;&#20010;&#36873;&#25321;&#37117;&#21487;&#20197;&#26174;&#30528;&#24433;&#21709;&#22522;&#20110;&#27169;&#22411;&#30340;&#25512;&#26029;&#21644;&#21518;&#32493;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#23558;&#25935;&#24863;&#24615;&#20998;&#26512;&#25972;&#21512;&#21040;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#65288;ABI&#65292;&#21363;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;&#25512;&#26029;&#65289;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#26435;&#37325;&#20849;&#20139;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32534;&#30721;&#26367;&#20195;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#25512;&#26029;&#26469;&#35780;&#20272;&#23545;&#21508;&#31181;&#25968;&#25454;&#25200;&#21160;&#25110;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#36125;&#21494;&#26031;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20004;&#20010;&#27493;&#39588;&#37117;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.07918</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#20223;&#23398;&#20064;&#23545;&#21307;&#30103;&#20915;&#31574;&#36827;&#34892;&#24314;&#27169;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#20013;&#20272;&#35745;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#65292;&#20363;&#22914;&#65292;&#23457;&#35745;&#21307;&#30103;&#20915;&#31574;&#30340;&#20559;&#35265;&#21644;&#27425;&#20248;&#23454;&#36341;&#65292;&#25105;&#20204;&#38656;&#35201;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#29616;&#26377;&#26041;&#27861;&#22522;&#26412;&#19978;&#30001;&#20110;&#23558;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#34920;&#31034;&#20026;&#36890;&#29992;&#31574;&#30053;&#32780;&#36127;&#25285;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#32780;&#23454;&#38469;&#19978;&#20154;&#31867;&#20915;&#31574;&#26159;&#21160;&#24577;&#30340;&#65292;&#21487;&#20197;&#38543;&#19978;&#19979;&#25991;&#20449;&#24687;&#32780;&#22823;&#24133;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65288;CPR&#65289;&#65292;&#23558;&#24314;&#27169;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#30340;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22797;&#26434;&#20915;&#31574;&#31574;&#30053;&#30001;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#32452;&#25104;&#12290;CPR&#23558;&#27599;&#20010;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#24314;&#27169;&#20026;&#32447;&#24615;&#30340;&#35266;&#23519;-&#21160;&#20316;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
&lt;/p&gt;</description></item><item><title>AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12689</link><description>&lt;p&gt;
AMPLIFY: &#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26631;&#31614;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12689
&lt;/p&gt;
&lt;p&gt;
AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#21407;&#22987;&#26679;&#26412;&#30340;&#32447;&#24615;&#32452;&#21512;&#29983;&#25104;&#26032;&#30340;&#22686;&#24378;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#21407;&#22987;&#26679;&#26412;&#20013;&#23384;&#22312;&#22122;&#38899;&#25110;&#24322;&#24120;&#29305;&#24449;&#65292;Mixup&#21487;&#33021;&#23558;&#20854;&#20256;&#25773;&#21040;&#22686;&#24378;&#26679;&#26412;&#20013;&#65292;&#23548;&#33268;&#27169;&#22411;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#36807;&#20110;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Mixup&#26041;&#27861;&#31216;&#20026;AMPLIFY&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Transformer&#33258;&#36523;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#26080;&#38656;&#22686;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#20302;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#24120;&#35265;Mixup&#26041;&#27861;&#65288;&#20363;&#22914;&#35821;&#21477;Mixup&#65289;&#20013;&#36164;&#28304;&#28040;&#32791;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26356;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#25104;&#26412;&#19979;&#65292;AMPLIFY&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;Mixup&#26041;&#27861;&#65292;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is an effective data augmentation method that generates new augmented samples by aggregating linear combinations of different original samples. However, if there are noises or aberrant features in the original samples, Mixup may propagate them to the augmented samples, leading to over-sensitivity of the model to these outliers . To solve this problem, this paper proposes a new Mixup method called AMPLIFY. This method uses the Attention mechanism of Transformer itself to reduce the influence of noises and aberrant values in the original samples on the prediction results, without increasing additional trainable parameters, and the computational cost is very low, thereby avoiding the problem of high resource consumption in common Mixup methods such as Sentence Mixup . The experimental results show that, under a smaller computational resource cost, AMPLIFY outperforms other Mixup methods in text classification tasks on 7 benchmark datasets, providing new ideas and new ways to further
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#30340;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#33021;&#37327;&#21270;&#20854;&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.12342</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#23545;&#40784;&#65306;&#22522;&#20110;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#30340;&#35299;&#37322;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions. (arXiv:2309.12342v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#30340;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#33021;&#37327;&#21270;&#20854;&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37096;&#32626;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#25991;&#21270;&#23545;&#40784;&#21644;&#23545;&#19981;&#21516;&#25991;&#21270;&#35268;&#33539;&#20010;&#20307;&#30340;&#28508;&#22312;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#25919;&#27835;&#21644;&#31038;&#20250;&#20559;&#35265;&#20197;&#21450;&#20844;&#20247;&#24847;&#35265;&#65292;&#32780;&#26410;&#28041;&#21450;&#25991;&#21270;&#20215;&#20540;&#35266;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#21033;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#30340;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#37327;&#21270;&#25991;&#21270;&#23545;&#40784;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#20998;&#26512;&#25552;&#20379;&#35299;&#37322;&#24615;&#30340;&#36328;&#25991;&#21270;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#22914;ChatGPT&#21644;Bard&#65289;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#65288;&#32654;&#22269;&#12289;&#27801;&#29305;&#38463;&#25289;&#20271;&#12289;&#20013;&#22269;&#21644;&#26031;&#27931;&#20240;&#20811;&#65289;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#20215;&#20540;&#35266;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#39118;&#26684;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#37327;&#21270;&#20102;LLMs&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#31243;&#24230;&#65292;&#32780;&#19988;&#25581;&#31034;&#20102;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;&#23613;&#31649;&#25152;&#26377;&#30340;LLMs&#37117;&#27809;&#26377;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals from various cultural norms. Existing work investigated political and social biases and public opinions rather than their cultural values. To address this limitation, the proposed Cultural Alignment Test (CAT) quantifies cultural alignment using Hofstede's cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to assess the cultural values embedded in state-of-the-art LLMs, such as: ChatGPT and Bard, across diverse cultures of countries: United States (US), Saudi Arabia, China, and Slovakia, using different prompting styles and hyperparameter settings. Our results not only quantify cultural alignment of LLMs with certain countries, but also reveal the difference between LLMs in explanatory cultural dimensions. While all LLMs did not provide satisfactory res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#27714;&#35299;&#26925;&#22278;&#22411;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#35823;&#24046;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.11925</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26925;&#22278;&#22411;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks. (arXiv:2308.11925v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#27714;&#35299;&#26925;&#22278;&#22411;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#35823;&#24046;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#29992;&#20110;&#32447;&#24615;&#21644;&#21322;&#32447;&#24615;&#20108;&#38454;&#26925;&#22278;&#38382;&#39064;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65288;&#26377;/&#26080;&#30418;&#32422;&#26463;&#65289;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20174;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#19968;&#38454;&#26368;&#20248;&#24615;&#31995;&#32479;&#25512;&#23548;&#20986;&#30340;&#32806;&#21512;&#31995;&#32479;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26469;&#35299;&#20915;&#32806;&#21512;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;&#25968;&#20540;&#26041;&#26696;&#36827;&#34892;&#35823;&#24046;&#20998;&#26512;&#65292;&#24182;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65288;&#22914;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#21442;&#25968;&#33539;&#22260;&#65289;&#20197;&#21450;&#22495;&#20869;&#21644;&#36793;&#30028;&#19978;&#30340;&#37319;&#26679;&#28857;&#25968;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#29366;&#24577;&#12289;&#25511;&#21046;&#21644;&#20276;&#38543;&#29366;&#24577;&#30340;$L^2(\Omega)$&#35823;&#24046;&#30028;&#12290;&#20998;&#26512;&#20013;&#30340;&#20027;&#35201;&#24037;&#20855;&#21253;&#25324;&#20559;&#31227;Rademacher&#22797;&#26434;&#24230;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#26377;&#30028;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#25968;&#20540;&#31034;&#20363;&#26469;&#35828;&#26126;&#35813;&#26041;&#27861;&#65292;&#24182;&#19982;&#19977;&#31181;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present and analyze a numerical solver for optimal control problems (without / with box constraint) for linear and semilinear second-order elliptic problems. The approach is based on a coupled system derived from the first-order optimality system of the optimal control problem, and applies physics informed neural networks (PINNs) to solve the coupled system. We present an error analysis of the numerical scheme, and provide $L^2(\Omega)$ error bounds on the state, control and adjoint state in terms of deep neural network parameters (e.g., depth, width, and parameter bounds) and the number of sampling points in the domain and on the boundary. The main tools in the analysis include offset Rademacher complexity and boundedness and Lipschitz continuity of neural network functions. We present several numerical examples to illustrate the approach and compare it with three existing approaches.
&lt;/p&gt;</description></item><item><title>HEAL-SWIN&#26159;&#19968;&#20010;&#22522;&#20110;HEALPix&#32593;&#26684;&#21644;SWIN&#21464;&#21387;&#22120;&#32467;&#21512;&#30340;&#29699;&#38754;&#35270;&#35273;&#21464;&#21387;&#22120;&#65292;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#24191;&#35282;&#40060;&#30524;&#22270;&#20687;&#26102;&#33021;&#22815;&#23454;&#29616;&#26080;&#22833;&#30495;&#19988;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07313</link><description>&lt;p&gt;
HEAL-SWIN: &#29699;&#38754;&#19978;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
HEAL-SWIN: A Vision Transformer On The Sphere. (arXiv:2307.07313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07313
&lt;/p&gt;
&lt;p&gt;
HEAL-SWIN&#26159;&#19968;&#20010;&#22522;&#20110;HEALPix&#32593;&#26684;&#21644;SWIN&#21464;&#21387;&#22120;&#32467;&#21512;&#30340;&#29699;&#38754;&#35270;&#35273;&#21464;&#21387;&#22120;&#65292;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#24191;&#35282;&#40060;&#30524;&#22270;&#20687;&#26102;&#33021;&#22815;&#23454;&#29616;&#26080;&#22833;&#30495;&#19988;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#24191;&#35282;&#40060;&#30524;&#22270;&#20687;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26222;&#36890;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25110;&#35270;&#35273;&#21464;&#21387;&#22120;&#22788;&#29702;&#27492;&#31867;&#25968;&#25454;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23558;&#20854;&#25237;&#24433;&#21040;&#24179;&#38754;&#19978;&#30340;&#30697;&#24418;&#32593;&#26684;&#26102;&#20250;&#24341;&#20837;&#25237;&#24433;&#21644;&#22833;&#30495;&#25439;&#22833;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HEAL-SWIN&#21464;&#21387;&#22120;&#65292;&#23427;&#23558;&#22312;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#20013;&#20351;&#29992;&#30340;&#39640;&#24230;&#22343;&#21248;&#30340;&#23618;&#27425;&#31561;&#38754;&#31215;&#31561;&#32428;&#24230;&#20687;&#32032;&#21270;&#65288;HEALPix&#65289;&#32593;&#26684;&#19982;&#23618;&#27425;&#31227;&#20301;&#31383;&#21475;&#65288;SWIN&#65289;&#21464;&#21387;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#24471;&#21040;&#33021;&#22815;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#26080;&#22833;&#30495;&#29699;&#38754;&#25968;&#25454;&#30340;&#39640;&#25928;&#28789;&#27963;&#27169;&#22411;&#12290;&#22312;HEAL-SWIN&#20013;&#65292;HEALPix&#32593;&#26684;&#30340;&#23884;&#22871;&#32467;&#26500;&#29992;&#20110;&#25191;&#34892;SWIN&#21464;&#21387;&#22120;&#30340;&#25340;&#25509;&#21644;&#31383;&#21475;&#25805;&#20316;&#65292;&#20174;&#32780;&#24471;&#21040;&#20855;&#26377;&#26368;&#23567;&#35745;&#31639;&#24320;&#38144;&#30340;&#29699;&#38754;&#25968;&#25454;&#19968;&#32500;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#35821;&#20041;&#20998;&#21106;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-resolution wide-angle fisheye images are becoming more and more important for robotics applications such as autonomous driving. However, using ordinary convolutional neural networks or vision transformers on this data is problematic due to projection and distortion losses introduced when projecting to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer, which combines the highly uniform Hierarchical Equal Area iso-Latitude Pixelation (HEALPix) grid used in astrophysics and cosmology with the Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and flexible model capable of training on high-resolution, distortion-free spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used to perform the patching and windowing operations of the SWIN transformer, resulting in a one-dimensional representation of the spherical data with minimal computational overhead. We demonstrate the superior performance of our model for semantic segmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26680;&#20272;&#35745;&#22120;&#21644;&#32452;&#21512;&#26679;&#26465;&#65292;&#23454;&#29616;&#20102;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05825</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#20272;&#35745;&#27169;&#24335;&#30340;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Bayesian taut splines for estimating the number of modes. (arXiv:2307.05825v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26680;&#20272;&#35745;&#22120;&#21644;&#32452;&#21512;&#26679;&#26465;&#65292;&#23454;&#29616;&#20102;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#20195;&#34920;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#20063;&#21487;&#20197;&#30475;&#20316;&#29616;&#26377;&#20122;&#32676;&#20307;&#30340;&#25968;&#37327;&#12290;&#23613;&#31649;&#20854;&#30456;&#20851;&#24615;&#65292;&#23545;&#20854;&#20272;&#35745;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#38024;&#23545;&#21333;&#21464;&#37327;&#24773;&#20917;&#25552;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#33268;&#21147;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21463;&#21040;&#20102;&#38382;&#39064;&#30340;&#19968;&#20123;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#32467;&#26500;&#65292;&#27169;&#24335;&#30340;&#20027;&#35266;&#19988;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#23494;&#24230;&#29305;&#24615;&#30340;&#25972;&#20307;&#35270;&#22270;&#30340;&#20415;&#21033;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#26680;&#20272;&#35745;&#22120;&#21644;&#31616;&#27905;&#30340;&#32452;&#21512;&#26679;&#26465;&#12290;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#37117;&#22312;&#36125;&#21494;&#26031;&#25512;&#29702;&#33539;&#24335;&#20013;&#23454;&#29616;&#65292;&#20026;&#36719;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#24182;&#20801;&#35768;&#22312;&#36807;&#31243;&#20013;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#38506;&#20276;&#30340;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of modes in a probability density function is representative of the model's complexity and can also be viewed as the number of existing subpopulations. Despite its relevance, little research has been devoted to its estimation. Focusing on the univariate setting, we propose a novel approach targeting prediction accuracy inspired by some overlooked aspects of the problem. We argue for the need for structure in the solutions, the subjective and uncertain nature of modes, and the convenience of a holistic view blending global and local density properties. Our method builds upon a combination of flexible kernel estimators and parsimonious compositional splines. Feature exploration, model selection and mode testing are implemented in the Bayesian inference paradigm, providing soft solutions and allowing to incorporate expert judgement in the process. The usefulness of our proposal is illustrated through a case study in sports analytics, showcasing multiple companion visualisation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31561;&#21464;Transformer&#29992;&#20110;&#23398;&#20064;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22359;&#32423;&#21644;&#21407;&#23376;&#32423;&#30340;&#20132;&#20114;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01474</link><description>&lt;p&gt;
&#36890;&#29992;&#31561;&#21464;Transformer&#65306;&#29992;&#20110;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning. (arXiv:2306.01474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31561;&#21464;Transformer&#29992;&#20110;&#23398;&#20064;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22359;&#32423;&#21644;&#21407;&#23376;&#32423;&#30340;&#20132;&#20114;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#21644;&#33647;&#29289;&#24320;&#21457;&#20013;&#30340;&#35768;&#22810;&#36807;&#31243;&#28041;&#21450;&#19981;&#21516;&#20998;&#23376;&#20043;&#38388;&#30340;&#21508;&#31181;3D&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#34507;&#30333;&#36136;&#19982;&#34507;&#30333;&#36136;&#65292;&#34507;&#30333;&#36136;&#19982;&#23567;&#20998;&#23376;&#31561;&#12290;&#35774;&#35745;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26469;&#23398;&#20064;&#26222;&#36866;&#30340;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#20998;&#23376;&#36890;&#24120;&#20197;&#19981;&#21516;&#31890;&#24230;&#34920;&#31034;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#23558;3D&#20998;&#23376;&#36890;&#29992;&#34920;&#31034;&#20026;&#38598;&#21512;&#30340;&#20960;&#20309;&#22270;&#24418;&#22270;&#65292;&#19982;&#20256;&#32479;&#21333;&#23618;&#34920;&#31034;&#24418;&#24335;&#24418;&#25104;&#23545;&#27604;&#12290;&#22312;&#25552;&#20986;&#30340;&#32479;&#19968;&#34920;&#31034;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#31561;&#21464;Transformer&#65288;GET&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#31232;&#30095;&#22359;&#32423;&#21644;&#23494;&#38598;&#21407;&#23376;&#32423;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GET&#30001;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#32452;&#25104;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#20197;&#28385;&#36275;3D&#19990;&#30028;&#30340;&#23545;&#31216;&#24615;&#12290;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#34920;&#26126;GET&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many processes in biology and drug discovery involve various 3D interactions between different molecules, such as protein and protein, protein and small molecule, etc. Designing a generalist model to learn universal molecular interactions is valuable yet challenging, given that different molecules are usually represented in different granularity. In this paper, we first propose to universally represent a 3D molecule as a geometric graph of sets, in contrast to conventional single-level representations. Upon the proposed unified representation, we then propose a Generalist Equivariant Transformer (GET) to effectively capture both sparse block-level and dense atom-level interactions. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where, notably, each module is E(3) equivariant to meet the symmetry of 3D world. Extensive experiments on the prediction of protein-protein affinity, ligand binding affinity, and ligand effica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20316;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#26631;&#20934;GP&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.20028</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Study of Bayesian Neural Network Surrogates for Bayesian Optimization. (arXiv:2305.20028v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20316;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#26631;&#20934;GP&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#38590;&#20197;&#26597;&#35810;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#36825;&#20123;&#30446;&#26631;&#20989;&#25968;&#36890;&#24120;&#30001;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20195;&#29702;&#27169;&#22411;&#34920;&#31034;&#65292;&#20854;&#26131;&#20110;&#20248;&#21270;&#24182;&#25903;&#25345;&#31934;&#30830;&#25512;&#29702;&#12290;&#34429;&#28982;&#26631;&#20934;&#30340;GP&#20195;&#29702;&#24050;&#32463;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#26368;&#36817;&#25104;&#20026;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#19982;&#26631;&#20934;&#30340;GP&#30456;&#27604;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20363;&#22914;&#22825;&#28982;&#22788;&#29702;&#38750;&#24179;&#31283;&#24615;&#20197;&#21450;&#23398;&#20064;&#39640;&#32500;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;BNN&#20316;&#20026;&#26631;&#20934;GP&#20195;&#29702;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21508;&#31181;&#26377;&#38480;&#23485;&#24230;BNN&#30340;&#36817;&#20284;&#25512;&#29702;&#36807;&#31243;&#65292;&#21253;&#25324;&#39640;&#36136;&#37327;Hamiltonian Monte Carlo&#65292;&#20302;&#25104;&#26412;&#30340;&#38543;&#26426;MCMC&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;&#22914;&#28145;&#24230;&#38598;&#25104;&#65289;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26080;&#38480;&#23485;&#24230;BNN&#21644;&#37096;&#20998;&#38543;&#26426;&#27169;&#22411;&#65292;&#20363;&#22914;&#28145;&#24230;&#26680;&#23398;&#20064;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#20195;&#29702;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20248;&#20110;&#26631;&#20934;GP&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BNN&#26159;&#20256;&#32479;&#20195;&#29702;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#24456;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a highly efficient approach to optimizing objective functions which are expensive to query. These objectives are typically represented by Gaussian process (GP) surrogate models which are easy to optimize and support exact inference. While standard GP surrogates have been well-established in Bayesian optimization, Bayesian neural networks (BNNs) have recently become practical function approximators, with many benefits over standard GPs such as the ability to naturally handle non-stationarity and learn representations for high-dimensional data. In this paper, we study BNNs as alternatives to standard GP surrogates for optimization. We consider a variety of approximate inference procedures for finite-width BNNs, including high-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics such as deep ensembles. We also consider infinite-width BNNs and partially stochastic models such as deep kernel learning. We evaluate this collection of surrogate mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.19604</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#21307;&#30103;&#20445;&#20581;&#30340;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#25903;&#65292;&#25552;&#20379;&#26426;&#20250;&#20026;&#22797;&#26434;&#20581;&#24247;&#29366;&#20917;&#30340;&#24739;&#32773;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#26356;&#31934;&#30830;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#23398;&#20064;&#25512;&#33616;&#33647;&#29289;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#35270;&#20102;&#26681;&#25454;&#24739;&#32773;&#30340;EHR&#20013;&#30340;&#20020;&#24202;&#34920;&#29616;&#32435;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#32593;&#32476;&#65288;DKINet&#65289;&#65292;&#29992;&#20110;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#21487;&#35266;&#23519;&#30340;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#23558;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#21487;&#35266;&#23519;&#30340;EHR&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DistroFair&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#26816;&#27979;&#21040;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;</title><link>http://arxiv.org/abs/2305.13935</link><description>&lt;p&gt;
&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Distribution-aware Fairness Test Generation. (arXiv:2305.13935v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DistroFair&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#26816;&#27979;&#21040;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#39564;&#35777;&#22270;&#20687;&#35782;&#21035;&#36719;&#20214;&#20013;&#30340;&#32452;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65288;&#31216;&#20026;DistroFair&#65289;&#65292;&#36890;&#36807;&#23558;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23545;&#35937;&#24341;&#20837;&#21040;&#22270;&#20687;&#35782;&#21035;&#22120;&#20013;&#65292;&#36890;&#36807;&#19977;&#31181;&#35821;&#20041;&#20445;&#30041;&#22270;&#20687;&#21464;&#25442; - &#23545;&#35937;&#21024;&#38500;&#65292;&#23545;&#35937;&#25554;&#20837;&#21644;&#23545;&#35937;&#26059;&#36716;&#26469;&#31995;&#32479;&#24615;&#22320;&#26292;&#38706;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#30340;&#31867;&#32423;&#21035;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#65288;CityScapes&#21644;MS-COCO&#65289;&#21644;&#19977;&#20010;&#20027;&#35201;&#30340;&#21830;&#19994;&#22270;&#20687;&#35782;&#21035;&#36719;&#20214;&#65288;&#21363;Amazon Rekognition&#65292;Google Cloud Vision&#21644;Azure&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#23545;DistroFair&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;DistroFair&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#65292;&#32422;&#26377;21&#65285;&#36890;&#36807;&#30495;&#23454;&#26631;&#20934;&#25110;&#20803;&#27979;&#35797;&#26631;&#20934;&#26174;&#38706;&#20986;&#20102;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DistroFair) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DistroFair automatically learns the distribution (e.g., number/orientation) of objects in a set of images. Then it systematically mutates objects in the images to become OOD using three semantic-preserving image mutations -- object deletion, object insertion and object rotation. We evaluate DistroFair using two well-known datasets (CityScapes and MS-COCO) and three major, commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision). Results show that about 21% of images generated by DistroFair reveal class-level fairness violations using either ground truth or metamorphic oracles. DistroFair 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#35775;&#28888;&#28953;&#22823;&#36187;&#65292;&#35780;&#20272;&#20102;&#26368;&#36817;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#22312;112&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#31867;&#27861;&#23558;&#36825;&#20123;&#31639;&#27861;&#20998;&#20026;&#20116;&#31867;&#65292;&#20026;TSC&#39046;&#22495;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.13029</link><description>&lt;p&gt;
Bake Off&#37325;&#35775;&#65306;&#23545;&#26368;&#36817;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#30340;&#35780;&#36848;&#21644;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bake off redux: a review and experimental evaluation of recent time series classification algorithms. (arXiv:2304.13029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#35775;&#28888;&#28953;&#22823;&#36187;&#65292;&#35780;&#20272;&#20102;&#26368;&#36817;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#22312;112&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#31867;&#27861;&#23558;&#36825;&#20123;&#31639;&#27861;&#20998;&#20026;&#20116;&#31867;&#65292;&#20026;TSC&#39046;&#22495;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#30740;&#31350;&#35770;&#25991;&#22312;2017&#24180;&#27604;&#36739;&#20102;18&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#31639;&#27861;&#22312;&#26469;&#33258;&#21152;&#24030;&#22823;&#23398;&#27827;&#28392;&#20998;&#26657;&#65288;UCR&#65289;&#23384;&#26723;&#30340;85&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#28888;&#28953;&#27604;&#36187;&#8221;&#65292;&#21457;&#29616;&#21482;&#26377;9&#20010;&#31639;&#27861;&#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20351;&#29992;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#21644;&#26059;&#36716;&#26862;&#26519;&#22522;&#20934;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#31639;&#27861;&#31867;&#22411;&#23545;&#27599;&#20010;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24418;&#25104;&#20102;&#20116;&#31181;&#20027;&#35201;&#31639;&#27861;&#31867;&#22411;&#30340;&#20998;&#31867;&#27861;&#12290;&#19982;&#21487;&#20197;&#37325;&#29616;&#32467;&#26524;&#30340;&#20195;&#30721;&#21644;&#32467;&#26524;&#30340;&#25552;&#20379;&#30456;&#32467;&#21512;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#20998;&#31867;&#21644;&#21487;&#35775;&#38382;&#30340;&#32467;&#26524;&#25512;&#21160;&#20102;TSC&#39046;&#22495;&#30340;&#26222;&#21450;&#12290;&#20845;&#24180;&#36807;&#21435;&#20102;&#65292;UCR&#23384;&#26723;&#24050;&#25193;&#23637;&#21040;112&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#26032;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#37325;&#35775;&#28888;&#28953;&#22823;&#36187;&#65292;&#30475;&#30475;&#27599;&#20010;&#25552;&#20986;&#30340;&#31867;&#21035;&#33258;&#21407;&#22987;&#20986;&#29256;&#20197;&#26469;&#30340;&#36827;&#23637;&#65292;&#24182;&#35780;&#20272;&#26032;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2017, a research paper compared 18 Time Series Classification (TSC) algorithms on 85 datasets from the University of California, Riverside (UCR) archive. This study, commonly referred to as a `bake off', identified that only nine algorithms performed significantly better than the Dynamic Time Warping (DTW) and Rotation Forest benchmarks that were used. The study categorised each algorithm by the type of feature they extract from time series data, forming a taxonomy of five main algorithm types. This categorisation of algorithms alongside the provision of code and accessible results for reproducibility has helped fuel an increase in popularity of the TSC field. Over six years have passed since this bake off, the UCR archive has expanded to 112 datasets and there have been a large number of new algorithms proposed. We revisit the bake off, seeing how each of the proposed categories have advanced since the original publication, and evaluate the performance of newer algorithms against t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09010</link><description>&lt;p&gt;
CF-VAE&#65306;&#22522;&#20110;VAE&#21644;&#22240;&#26524;&#27969;&#30340;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows. (arXiv:2304.09010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#23398;&#20064;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20854;&#20013;&#27599;&#20010;&#32500;&#24230;&#23545;&#24212;&#19968;&#20010;&#28508;&#22312;&#30340;&#29983;&#25104;&#22240;&#32032;&#12290;&#30001;&#20110;&#29983;&#25104;&#22240;&#32032;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#65292;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20197;&#23558;&#22240;&#26524;&#32467;&#26500;&#20449;&#24687;&#24341;&#20837;&#27169;&#22411;&#20013;&#30340;&#27969;&#65292;&#31216;&#20026;&#22240;&#26524;&#27969;&#12290;&#22522;&#20110;&#24191;&#27867;&#29992;&#20110;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#24341;&#20837;&#22522;&#20934;&#22240;&#32032;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#20998;&#31163;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CF-VAE&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning disentangled representations is important in representation learning, aiming to learn a low dimensional representation of data where each dimension corresponds to one underlying generative factor. Due to the possibility of causal relationships between generative factors, causal disentangled representation learning has received widespread attention. In this paper, we first propose a new flows that can incorporate causal structure information into the model, called causal flows. Based on the variational autoencoders(VAE) commonly used in disentangled representation learning, we design a new model, CF-VAE, which enhances the disentanglement ability of the VAE encoder by utilizing the causal flows. By further introducing the supervision of ground-truth factors, we demonstrate the disentanglement identifiability of our model. Experimental results on both synthetic and real datasets show that CF-VAE can achieve causal disentanglement and perform intervention experiments. Moreover, C
&lt;/p&gt;</description></item><item><title>StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.03853</link><description>&lt;p&gt;
StepMix: &#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03853
&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#24191;&#20041;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;(&#28508;&#22312;&#21078;&#38754;&#21644;&#28508;&#22312;&#31867;&#20998;&#26512;)&#19982;&#22806;&#37096;&#21464;&#37327;(&#21327;&#21464;&#37327;&#21644;&#36828;&#31243;&#32467;&#26524;)&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;(&#21333;&#27493;&#12289;&#20004;&#27493;&#21644;&#19977;&#27493;&#26041;&#27861;)&#30340;&#24320;&#28304;&#36719;&#20214;&#21253;&#12290;&#22312;&#35768;&#22810;&#31038;&#20250;&#31185;&#23398;&#30340;&#24212;&#29992;&#20013;&#65292;&#20027;&#35201;&#30446;&#26631;&#19981;&#20165;&#26159;&#23558;&#20010;&#20307;&#32858;&#31867;&#25104;&#28508;&#22312;&#31867;&#21035;&#65292;&#36824;&#21253;&#25324;&#20351;&#29992;&#36825;&#20123;&#31867;&#21035;&#26469;&#24320;&#21457;&#26356;&#22797;&#26434;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20998;&#20026;&#19968;&#20010;&#23558;&#28508;&#22312;&#31867;&#21035;&#19982;&#35266;&#23519;&#25351;&#26631;&#30456;&#20851;&#32852;&#30340;&#27979;&#37327;&#27169;&#22411;&#21644;&#19968;&#20010;&#23558;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#21464;&#37327;&#19982;&#28508;&#22312;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#27169;&#22411;&#12290;&#27979;&#37327;&#21644;&#32467;&#26500;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#25152;&#35859;&#30340;&#19968;&#27493;&#27861;&#20849;&#21516;&#20272;&#35745;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#36880;&#27493;&#26041;&#27861;&#36880;&#27493;&#20272;&#35745;&#65292;&#23545;&#20110;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20272;&#35745;&#28508;&#22312;&#31867;&#21035;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#38500;&#20102;&#19968;&#27493;&#27861;&#65292;StepMix&#36824;&#23454;&#29616;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#26368;&#37325;&#35201;&#30340;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#26041;&#20415;&#27169;&#22411;&#30340;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#23558;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#19982;&#21435;&#22122;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#25104;&#20687;&#38382;&#39064;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03483</link><description>&lt;p&gt;
RED-PSM: &#24102;&#21435;&#22122;&#27491;&#21017;&#21270;&#30340;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#29992;&#20110;&#21160;&#24577;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging. (arXiv:2304.03483v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#23558;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#19982;&#21435;&#22122;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#25104;&#20687;&#38382;&#39064;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#25104;&#20687;&#26159;&#25351;&#21033;&#29992;&#34987;&#27424;&#37319;&#26679;&#30340;&#27979;&#37327;&#25968;&#25454;&#24674;&#22797;&#27599;&#20010;&#26102;&#38388;&#28857;&#19978;&#30340;&#26102;&#21464;&#20108;&#32500;&#25110;&#19977;&#32500;&#29289;&#20307;&#12290;&#23588;&#20854;&#26159;&#22312;&#21160;&#24577;&#26029;&#23618;&#25195;&#25551;&#20013;&#65292;&#27599;&#20010;&#26102;&#38388;&#28857;&#19978;&#21482;&#26377;&#19968;&#20010;&#35270;&#35282;&#19979;&#30340;&#21333;&#20010;&#25237;&#24433;&#21487;&#29992;&#65292;&#20351;&#24471;&#38382;&#39064;&#20005;&#37325;&#31639;&#19981;&#21487;&#36870;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#39318;&#27425;&#23558;&#20004;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25104;&#20687;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#25216;&#26415;&#26159;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#65292;&#24050;&#32463;&#29992;&#20110;&#39640;&#25928;&#22320;&#20026;&#26102;&#31354;&#30446;&#26631;&#24341;&#20837;&#20302;&#31209;&#20808;&#39564;&#12290;&#31532;&#20108;&#31181;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#21435;&#22122;&#27491;&#21017;&#21270;(RED)&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21435;&#22122;&#31639;&#27861;&#22788;&#29702;&#21508;&#31181;&#21453;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#27491;&#21017;&#21270;&#30340;&#37096;&#20998;&#21487;&#20998;&#30446;&#26631;&#65292;&#36890;&#36807;&#21464;&#37327;&#20998;&#35010;&#21644;ADMM&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30446;&#26631;&#25910;&#25947;&#20110;&#19968;&#20010;&#28385;&#36275;&#20248;&#21270;&#38382;&#39064;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21160;&#24577;&#26029;&#23618;&#25195;&#25551;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;RED-PSM&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#21160;&#24577;&#25104;&#20687;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic imaging addresses the recovery of a time-varying 2D or 3D object at each time instant using its undersampled measurements. In particular, in the case of dynamic tomography, only a single projection at a single view angle may be available at a time, making the problem severely ill-posed. In this work, we propose an approach, RED-PSM, which combines for the first time two powerful techniques to address this challenging imaging problem. The first, are partially separable models, which have been used to efficiently introduce a low-rank prior for the spatio-temporal object. The second is the recent Regularization by Denoising (RED), which provides a flexible framework to exploit the impressive performance of state-of-the-art image denoising algorithms, for various inverse problems. We propose a partially separable objective with RED and an optimization scheme with variable splitting and ADMM, and prove convergence of our objective to a value corresponding to a stationary point satis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;&#21452;&#19992;&#38477;&#26354;&#32447;&#20986;&#29616;&#8220;&#26368;&#32456;&#19978;&#21319;&#8221;&#65292;&#21363;&#22312;&#36275;&#22815;&#22823;&#30340;&#22122;&#22768;&#26679;&#26412;&#27604;&#29575;&#19979;&#65292;&#20013;&#31561;&#23485;&#24230;&#19979;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#24615;&#33021;&#12290;&#38543;&#26426;&#20002;&#24323;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#20943;&#23569;&#23494;&#24230;&#21487;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.08003</link><description>&lt;p&gt;
&#30740;&#31350;&#27169;&#22411;&#23485;&#24230;&#21644;&#23494;&#24230;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise. (arXiv:2208.08003v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;&#21452;&#19992;&#38477;&#26354;&#32447;&#20986;&#29616;&#8220;&#26368;&#32456;&#19978;&#21319;&#8221;&#65292;&#21363;&#22312;&#36275;&#22815;&#22823;&#30340;&#22122;&#22768;&#26679;&#26412;&#27604;&#29575;&#19979;&#65292;&#20013;&#31561;&#23485;&#24230;&#19979;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#24615;&#33021;&#12290;&#38543;&#26426;&#20002;&#24323;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#20943;&#23569;&#23494;&#24230;&#21487;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#22823;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#26159;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#36825;&#26159;&#36890;&#36807;&#21452;&#19992;&#38477;&#29616;&#35937;&#25429;&#25417;&#30340;&#65292;&#20854;&#20013;&#27979;&#35797;&#25439;&#22833;&#38543;&#30528;&#27169;&#22411;&#23485;&#24230;&#30340;&#22686;&#21152;&#21576;&#29616;&#20986;&#38477;&#20302;-&#22686;&#21152;-&#38477;&#20302;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#26631;&#31614;&#22122;&#22768;&#23545;&#27979;&#35797;&#25439;&#22833;&#26354;&#32447;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#21363;&#26631;&#31614;&#22122;&#22768;&#23548;&#33268;&#21407;&#26412;&#35266;&#23519;&#21040;&#30340;&#21452;&#19992;&#38477;&#26354;&#32447;&#20986;&#29616;&#20102;&#8220;&#26368;&#32456;&#19978;&#21319;&#8221;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36275;&#22815;&#22823;&#30340;&#22122;&#22768;&#26679;&#26412;&#27604;&#29575;&#19979;&#65292;&#20013;&#31561;&#23485;&#24230;&#19979;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#24615;&#33021;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#24402;&#22240;&#20110;&#26631;&#31614;&#22122;&#22768;&#24341;&#36215;&#30340;&#27979;&#35797;&#25439;&#22833;&#26041;&#24046;&#24418;&#29366;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#26368;&#32456;&#19978;&#21319;&#29616;&#35937;&#25193;&#23637;&#21040;&#27169;&#22411;&#23494;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#34920;&#24449;&#65292;&#34920;&#26126;&#38543;&#26426;&#20002;&#24323;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#20943;&#23569;&#23494;&#24230;&#21487;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increasing the size of overparameterized neural networks has been a key in achieving state-of-the-art performance. This is captured by the double descent phenomenon, where the test loss follows a decreasing-increasing-decreasing pattern as model width increases. However, the effect of label noise on the test loss curve has not been fully explored. In this work, we uncover an intriguing phenomenon where label noise leads to a \textit{final ascent} in the originally observed double descent curve. Specifically, under a sufficiently large noise-to-sample-size ratio, optimal generalization is achieved at intermediate widths. Through theoretical analysis, we attribute this phenomenon to the shape transition of test loss variance induced by label noise. Furthermore, we extend the final ascent phenomenon to model density and provide the first theoretical characterization showing that reducing density by randomly dropping trainable parameters improves generalization under label noise. We also t
&lt;/p&gt;</description></item></channel></rss>