<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SiMBA&#26159;&#19968;&#31181;&#24341;&#20837;Einstein FFT&#36827;&#34892;&#36890;&#36947;&#24314;&#27169;&#24182;&#20351;&#29992;Mamba&#22359;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#26032;&#26550;&#26500;&#65292;&#22312;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;SSMs&#12290;</title><link>https://arxiv.org/abs/2403.15360</link><description>&lt;p&gt;
SiMBA&#65306;&#29992;&#20110;&#35270;&#35273;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#31616;&#21270;Mamba&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15360
&lt;/p&gt;
&lt;p&gt;
SiMBA&#26159;&#19968;&#31181;&#24341;&#20837;Einstein FFT&#36827;&#34892;&#36890;&#36947;&#24314;&#27169;&#24182;&#20351;&#29992;Mamba&#22359;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#26032;&#26550;&#26500;&#65292;&#22312;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;SSMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24191;&#27867;&#37319;&#29992;&#20102;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#24207;&#21015;&#28151;&#21512;&#21644;MLPs&#36827;&#34892;&#36890;&#36947;&#28151;&#21512;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#25991;&#29486;&#24378;&#35843;&#20102;&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#30340;&#20302;&#24402;&#32435;&#20559;&#24046;&#21644;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#22914;S4&#21644;&#20854;&#20182;&#27169;&#22411;&#65288;Hippo&#65292;Global Convolutions&#65292;liquid S4&#65292;LRU&#65292;Mega&#21644;Mamba&#65289;&#65292;&#24050;&#32463;&#20986;&#29616;&#26469;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#38271;&#24230;&#12290;Mamba&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;SSM&#65292;&#20294;&#22312;&#25193;&#23637;&#21040;&#22823;&#22411;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#26102;&#23384;&#22312;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SiMBA&#65292;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#29305;&#24449;&#20540;&#35745;&#31639;&#24341;&#20837;Einstein FFT&#65288;EinFFT&#65289;&#26469;&#36827;&#34892;&#36890;&#36947;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;Mamba&#22359;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#12290;&#23545;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#30340;&#24191;&#27867;&#24615;&#33021;&#30740;&#31350;&#34920;&#26126;&#65292;SiMBA&#20248;&#20110;&#29616;&#26377;&#30340;SSMs&#65292;&#26550;&#36215;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15360v1 Announce Type: cross  Abstract: Transformers have widely adopted attention networks for sequence mixing and MLPs for channel mixing, playing a pivotal role in achieving breakthroughs across domains. However, recent literature highlights issues with attention networks, including low inductive bias and quadratic complexity concerning input sequence length. State Space Models (SSMs) like S4 and others (Hippo, Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address the above issues to help handle longer sequence lengths. Mamba, while being the state-of-the-art SSM, has a stability issue when scaled to large networks for computer vision datasets. We propose SiMBA, a new architecture that introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. Extensive performance studies across image and time-series benchmarks demonstrate that SiMBA outperforms existing SSMs, bridging
&lt;/p&gt;</description></item><item><title>&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#32473;&#20986;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#20998;&#37197;&#29575;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.14713</link><description>&lt;p&gt;
&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#19979;&#23457;&#35745;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Auditing Fairness under Unobserved Confounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14713
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#32473;&#20986;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#20998;&#37197;&#29575;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#36328;&#36234;&#20154;&#21475;&#32479;&#35745;&#32447;&#23384;&#22312;&#19981;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#20844;&#24179;&#24615;&#21487;&#33021;&#38590;&#20197;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#25105;&#20204;&#23545;&#20844;&#24179;&#24615;&#30340;&#29702;&#35299;&#20381;&#36182;&#20110;&#38590;&#20197;&#34913;&#37327;&#30340;&#39118;&#38505;&#31561;&#35266;&#24565;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#37027;&#20123;&#27809;&#26377;&#20854;&#27835;&#30103;&#23601;&#20250;&#27515;&#20129;&#30340;&#20154;&#24179;&#31561;&#33719;&#24471;&#27835;&#30103;&#65289;&#12290;&#23457;&#35745;&#36825;&#31181;&#19981;&#20844;&#24179;&#24615;&#38656;&#35201;&#20934;&#30830;&#27979;&#37327;&#20010;&#20307;&#39118;&#38505;&#65292;&#32780;&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#65292;&#38590;&#20197;&#20272;&#35745;&#12290;&#22312;&#36825;&#20123;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#32032;&#8220;&#35299;&#37322;&#8221;&#26126;&#26174;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#33021;&#20302;&#20272;&#25110;&#39640;&#20272;&#19981;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#65288;&#20196;&#20154;&#24778;&#35766;&#22320;&#65289;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#30340;&#20998;&#37197;&#29575;&#32473;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22312;&#35768;&#22810;&#23454;&#38469;&#29615;&#22659;&#20013;&#65288;&#20363;&#22914;&#24341;&#20837;&#26032;&#22411;&#27835;&#30103;&#65289;&#25105;&#20204;&#25317;&#26377;&#22312;&#20219;&#20309;&#20998;&#37197;&#20043;&#21069;&#30340;&#25968;&#25454;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14713v1 Announce Type: cross  Abstract: A fundamental problem in decision-making systems is the presence of inequity across demographic lines. However, inequity can be difficult to quantify, particularly if our notion of equity relies on hard-to-measure notions like risk (e.g., equal access to treatment for those who would die without it). Auditing such inequity requires accurate measurements of individual risk, which is difficult to estimate in the realistic setting of unobserved confounding. In the case that these unobservables "explain" an apparent disparity, we may understate or overstate inequity. In this paper, we show that one can still give informative bounds on allocation rates among high-risk individuals, even while relaxing or (surprisingly) even when eliminating the assumption that all relevant risk factors are observed. We utilize the fact that in many real-world settings (e.g., the introduction of a novel treatment) we have data from a period prior to any alloc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;</title><link>https://arxiv.org/abs/2403.08291</link><description>&lt;p&gt;
CleanAgent&#65306;&#22522;&#20110;LLM&#20195;&#29702;&#33258;&#21160;&#21270;&#25968;&#25454;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
CleanAgent: Automating Data Standardization with LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08291
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26631;&#20934;&#21270;&#26159;&#25968;&#25454;&#31185;&#23398;&#29983;&#21629;&#21608;&#26399;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#34429;&#28982;&#35832;&#22914;Pandas&#20043;&#31867;&#30340;&#24037;&#20855;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#38656;&#35201;&#23450;&#21046;&#20195;&#30721;&#20197;&#36866;&#24212;&#19981;&#21516;&#21015;&#31867;&#22411;&#30340;&#25163;&#21160;&#25805;&#20316;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#29616;&#20986;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#38656;&#35201;&#19987;&#19994;&#31243;&#24230;&#30340;&#32534;&#31243;&#30693;&#35782;&#21644;&#25345;&#32493;&#20114;&#21160;&#20197;&#36827;&#34892;&#21450;&#26102;&#30340;&#23436;&#21892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#25552;&#20986;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#21015;&#31867;&#22411;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;Dataprep.Clean&#65292;&#20316;&#20026;Dataprep&#24211;&#30340;&#19968;&#20010;&#32452;&#20214;&#65292;&#36890;&#36807;&#19968;&#34892;&#20195;&#30721;&#23454;&#29616;&#29305;&#23450;&#21015;&#31867;&#22411;&#30340;&#26631;&#20934;&#21270;&#65292;&#26497;&#22823;&#38477;&#20302;&#20102;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#20171;&#32461;&#20102;CleanAgen
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08291v1 Announce Type: cross  Abstract: Data standardization is a crucial part in data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing column types, simplifying the code generation of LLM with concise API calls. We first propose Dataprep.Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. Then we introduce the CleanAgen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#32534;&#30721;&#22120;&#21644;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#30340;&#39640;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.05743</link><description>&lt;p&gt;
&#20855;&#26377;&#24066;&#22330;&#36816;&#33829;&#24212;&#29992;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative Probabilistic Forecasting with Applications in Market Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05743
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#32534;&#30721;&#22120;&#21644;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#30340;&#39640;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28304;&#33258;&#20110;&#38750;&#21442;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#12290;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#24335;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#27979;&#26550;&#26500;&#21253;&#25324;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#38750;&#21442;&#25968;&#22810;&#21464;&#37327;&#38543;&#26426;&#36807;&#31243;&#36716;&#21270;&#20026;&#35268;&#33539;&#30340;&#21019;&#26032;&#24207;&#21015;&#65292;&#20174;&#20013;&#26681;&#25454;&#36807;&#21435;&#26679;&#26412;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26465;&#20214;&#26159;&#23427;&#20204;&#30340;&#27010;&#29575;&#20998;&#24067;&#21462;&#20915;&#20110;&#36807;&#21435;&#26679;&#26412;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28508;&#22312;&#36807;&#31243;&#38480;&#21046;&#20026;&#20855;&#26377;&#21305;&#37197;&#33258;&#32534;&#30721;&#22120;&#36755;&#20837;-&#36755;&#20986;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#24207;&#21015;&#12290;&#24314;&#31435;&#20102;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#24335;&#39044;&#27979;&#26041;&#27861;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#28041;&#21450;&#39640;&#24230;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#30340;&#19977;&#20010;&#24212;&#29992;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05743v1 Announce Type: cross  Abstract: This paper presents a novel generative probabilistic forecasting approach derived from the Wiener-Kallianpur innovation representation of nonparametric time series. Under the paradigm of generative artificial intelligence, the proposed forecasting architecture includes an autoencoder that transforms nonparametric multivariate random processes into canonical innovation sequences, from which future time series samples are generated according to their probability distributions conditioned on past samples. A novel deep-learning algorithm is proposed that constrains the latent process to be an independent and identically distributed sequence with matching autoencoder input-output conditional probability distributions. Asymptotic optimality and structural convergence properties of the proposed generative forecasting approach are established. Three applications involving highly dynamic and volatile time series in real-time market operations a
&lt;/p&gt;</description></item><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.02411</link><description>&lt;p&gt;
NiNformer: &#19968;&#31181;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#30340;&#32593;&#32476;&#20013;&#32593;&#32476;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#20027;&#35201;&#32452;&#20214;&#65292;&#33258;&#24341;&#20837;&#20197;&#26469;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36328;&#36234;&#20102;&#35768;&#22810;&#39046;&#22495;&#21644;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26426;&#21046;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#34987;&#24212;&#29992;&#20026;Vision Transformer ViT&#65292;&#24182;&#19988;&#20854;&#29992;&#36884;&#24050;&#25193;&#23637;&#21040;&#35270;&#35273;&#39046;&#22495;&#30340;&#35768;&#22810;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#23613;&#31649;&#35813;&#26426;&#21046;&#38750;&#24120;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#33021;&#21147;&#65292;&#20294;&#20854;&#32570;&#28857;&#26159;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#26377;&#25928;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#35774;&#35745;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#21644;&#32531;&#35299;&#25968;&#25454;&#22823;&#23567;&#35201;&#27714;&#12290;&#22312;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#20123;&#23581;&#35797;&#30340;&#20363;&#23376;&#21253;&#25324;MLP-Mixer&#12289;Conv-Mixer&#12289;Perciver-IO&#31561;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#20316;&#20026;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02411v1 Announce Type: cross  Abstract: The Attention mechanism is the main component of the Transformer architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks. The Attention Mechanism was utilized in Computer Vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FlowCyt&#39033;&#30446;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20013;&#22810;&#31867;&#21035;&#21333;&#32454;&#32990;&#20998;&#31867;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#30721;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00024</link><description>&lt;p&gt;
FlowCyt: &#27969;&#24335;&#32454;&#32990;&#26415;&#20013;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
FlowCyt: A Comparative Study of Deep Learning Approaches for Multi-Class Classification in Flow Cytometry Benchmarking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FlowCyt&#39033;&#30446;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20013;&#22810;&#31867;&#21035;&#21333;&#32454;&#32990;&#20998;&#31867;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#30721;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FlowCyt&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20013;&#22810;&#31867;&#21035;&#21333;&#32454;&#32990;&#20998;&#31867;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;30&#21517;&#24739;&#32773;&#30340;&#39592;&#39635;&#26679;&#26412;&#65292;&#27599;&#20010;&#32454;&#32990;&#30001;&#21313;&#20108;&#20010;&#26631;&#35760;&#29305;&#24449;&#12290;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#35782;&#21035;&#20102;&#20116;&#31181;&#34880;&#28082;&#32454;&#32990;&#31867;&#22411;&#65306;T&#28107;&#24052;&#32454;&#32990;&#12289;B&#28107;&#24052;&#32454;&#32990;&#12289;&#21333;&#26680;&#32454;&#32990;&#12289;&#32933;&#22823;&#32454;&#32990;&#21644;&#36896;&#34880;&#24178;/&#31062;&#32454;&#32990;&#65288;HSPCs&#65289;&#12290;&#23454;&#39564;&#21033;&#29992;&#20102;&#26377;&#30417;&#30563;&#30340;&#24402;&#32435;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#30340;&#36716;&#23548;&#23398;&#20064;&#65292;&#27599;&#21517;&#24739;&#32773;&#26368;&#22810;&#20351;&#29992;&#20102;100&#19975;&#20010;&#32454;&#32990;&#12290;&#22522;&#32447;&#26041;&#27861;&#21253;&#25324;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12289;XGBoost&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;GNNs&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#30721;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#20801;&#35768;&#26631;&#20934;&#21270;&#35780;&#20272;&#20020;&#24202;&#30456;&#20851;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#25506;&#32034;&#24615;&#20998;&#26512;&#20197;&#33719;&#24471;&#23545;&#34880;&#28082;&#32454;&#32990;&#34920;&#22411;&#30340;&#27934;&#23519;&#12290;&#36825;&#20195;&#34920;&#20102;&#19968;&#39033;&#37325;&#35201;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00024v1 Announce Type: new  Abstract: This paper presents FlowCyt, the first comprehensive benchmark for multi-class single-cell classification in flow cytometry data. The dataset comprises bone marrow samples from 30 patients, with each cell characterized by twelve markers. Ground truth labels identify five hematological cell types: T lymphocytes, B lymphocytes, Monocytes, Mast cells, and Hematopoietic Stem/Progenitor Cells (HSPCs). Experiments utilize supervised inductive learning and semi-supervised transductive learning on up to 1 million cells per patient. Baseline methods include Gaussian Mixture Models, XGBoost, Random Forests, Deep Neural Networks, and Graph Neural Networks (GNNs). GNNs demonstrate superior performance by exploiting spatial relationships in graph-encoded data. The benchmark allows standardized evaluation of clinically relevant classification tasks, along with exploratory analyses to gain insights into hematological cell phenotypes. This represents th
&lt;/p&gt;</description></item><item><title>TorchMD-Net 2.0&#26159;&#22312;&#31070;&#32463;&#32593;&#32476;&#21183;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;TensorNet&#31561;&#23574;&#31471;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#24471;&#22312;&#35745;&#31639;&#33021;&#37327;&#21644;&#21147;&#26102;&#33719;&#24471;&#20102;2&#21040;10&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.17660</link><description>&lt;p&gt;
TorchMD-Net 2.0: &#20998;&#23376;&#27169;&#25311;&#20013;&#30340;&#24555;&#36895;&#31070;&#32463;&#32593;&#32476;&#21183;
&lt;/p&gt;
&lt;p&gt;
TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17660
&lt;/p&gt;
&lt;p&gt;
TorchMD-Net 2.0&#26159;&#22312;&#31070;&#32463;&#32593;&#32476;&#21183;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;TensorNet&#31561;&#23574;&#31471;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#24471;&#22312;&#35745;&#31639;&#33021;&#37327;&#21644;&#21147;&#26102;&#33719;&#24471;&#20102;2&#21040;10&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17660v1 &#22768;&#26126;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#22312;&#20998;&#23376;&#27169;&#25311;&#20013;&#23454;&#29616;&#35745;&#31639;&#36895;&#24230;&#12289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36890;&#29992;&#36866;&#29992;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TorchMD-Net&#36719;&#20214;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#36825;&#26159;&#20174;&#20256;&#32479;&#21147;&#22330;&#36716;&#21521;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21183;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;TorchMD-Net&#28436;&#21464;&#25104;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;TensorNet&#31561;&#23574;&#31471;&#20307;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#27169;&#22359;&#21270;&#35774;&#35745;&#26041;&#27861;&#23454;&#29616;&#20102;&#36825;&#31181;&#36716;&#21464;&#65292;&#40723;&#21169;&#31185;&#23398;&#30028;&#20869;&#37096;&#30340;&#23450;&#21046;&#24212;&#29992;&#12290;&#26368;&#26174;&#30528;&#30340;&#22686;&#24378;&#26159;&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#25913;&#36827;&#65292;&#22312;TensorNet&#27169;&#22411;&#30340;&#33021;&#37327;&#21644;&#21147;&#35745;&#31639;&#20013;&#23454;&#29616;&#20102;&#38750;&#24120;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#24615;&#33021;&#25552;&#21319;&#33539;&#22260;&#20174;&#21069;&#20960;&#20010;&#29256;&#26412;&#30340;2&#20493;&#21040;10&#20493;&#12290;&#20854;&#20182;&#22686;&#24378;&#21151;&#33021;&#21253;&#25324;&#39640;&#24230;&#20248;&#21270;&#30340;&#37051;&#23621;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17660v1 Announce Type: new  Abstract: Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge. This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials. The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet. This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community. The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations. Other enhancements include highly optimized neighbor sear
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#20998;&#26512;&#20102;&#19968;&#31181;&#20851;&#20110;&#19968;&#33324;&#20559;&#22909;&#19979;&#32435;&#20160;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#31454;&#20105;&#30340;LLM&#36827;&#34892;&#21338;&#24328;&#26469;&#25214;&#21040;&#19968;&#31181;&#19968;&#33268;&#29983;&#25104;&#21709;&#24212;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.07314</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#19968;&#33324;KL&#27491;&#21017;&#21270;&#20559;&#22909;&#19979;&#32435;&#20160;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#20998;&#26512;&#20102;&#19968;&#31181;&#20851;&#20110;&#19968;&#33324;&#20559;&#22909;&#19979;&#32435;&#20160;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#31454;&#20105;&#30340;LLM&#36827;&#34892;&#21338;&#24328;&#26469;&#25214;&#21040;&#19968;&#31181;&#19968;&#33268;&#29983;&#25104;&#21709;&#24212;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#20174;&#19968;&#20010;&#27010;&#29575;&#20559;&#22909;&#27169;&#22411;&#25552;&#20379;&#30340;&#20559;&#22909;&#20449;&#21495;&#20013;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#20197;&#19968;&#20010;&#25552;&#31034;&#21644;&#20004;&#20010;&#21709;&#24212;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#20998;&#25968;&#65292;&#34920;&#31034;&#23545;&#19968;&#20010;&#21709;&#24212;&#30456;&#23545;&#20110;&#21478;&#19968;&#20010;&#21709;&#24212;&#30340;&#20559;&#22909;&#31243;&#24230;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#26368;&#27969;&#34892;&#30340;RLHF&#33539;&#24335;&#26159;&#22522;&#20110;&#22870;&#21169;&#30340;&#65292;&#23427;&#20174;&#22870;&#21169;&#24314;&#27169;&#30340;&#21021;&#22987;&#27493;&#39588;&#24320;&#22987;&#65292;&#28982;&#21518;&#20351;&#29992;&#26500;&#24314;&#30340;&#22870;&#21169;&#20026;&#21518;&#32493;&#30340;&#22870;&#21169;&#20248;&#21270;&#38454;&#27573;&#25552;&#20379;&#22870;&#21169;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#20989;&#25968;&#30340;&#23384;&#22312;&#26159;&#19968;&#20010;&#24378;&#20551;&#35774;&#65292;&#22522;&#20110;&#22870;&#21169;&#30340;RLHF&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#26377;&#23616;&#38480;&#24615;&#65292;&#19981;&#33021;&#25429;&#25417;&#21040;&#30495;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#26368;&#36817;&#25552;&#20986;&#30340;&#23398;&#20064;&#33539;&#24335;Nash&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;NLHF&#65289;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#23519;&#21147;&#65292;&#35813;&#23398;&#20064;&#33539;&#24335;&#32771;&#34385;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#23558;&#23545;&#40784;&#36807;&#31243;&#23450;&#20041;&#20026;&#20004;&#20010;&#31454;&#20105;&#30340;LLM&#20043;&#38388;&#30340;&#21338;&#24328;&#12290;&#23398;&#20064;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#19968;&#33268;&#29983;&#25104;&#21709;&#24212;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) learns from the preference signal provided by a probabilistic preference model, which takes a prompt and two responses as input, and produces a score indicating the preference of one response against another. So far, the most popular RLHF paradigm is reward-based, which starts with an initial step of reward modeling, and the constructed reward is then used to provide a reward signal for the subsequent reward optimization stage. However, the existence of a reward function is a strong assumption and the reward-based RLHF is limited in expressivity and cannot capture the real-world complicated human preference.   In this work, we provide theoretical insights for a recently proposed learning paradigm, Nash learning from human feedback (NLHF), which considered a general preference model and formulated the alignment process as a game between two competitive LLMs. The learning objective is to find a policy that consistently generates responses
&lt;/p&gt;</description></item><item><title>VerAs&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#38024;&#23545;&#23398;&#29983;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2402.05224</link><description>&lt;p&gt;
VerAs: &#39564;&#35777;&#28982;&#21518;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
VerAs: Verify then Assess STEM Lab Reports
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05224
&lt;/p&gt;
&lt;p&gt;
VerAs&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#38024;&#23545;&#23398;&#29983;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;STEM&#25945;&#32946;&#23545;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#31185;&#23398;&#20889;&#20316;&#22312;&#27880;&#37325;&#25506;&#31350;&#25216;&#33021;&#30340;&#35838;&#31243;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;&#19968;&#20221;&#25968;&#25454;&#38598;&#26159;&#22522;&#20110;&#19968;&#22871;&#25506;&#31350;&#22411;&#29289;&#29702;&#35838;&#31243;&#30340;&#20004;&#32452;&#22823;&#23398;&#27700;&#24179;&#30340;&#23454;&#39564;&#25253;&#21578;&#65292;&#20381;&#36182;&#20110;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#25351;&#23450;&#23398;&#31185;&#30693;&#35782;&#21644;&#20248;&#31168;&#35299;&#37322;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#12290;&#27599;&#20010;&#20998;&#26512;&#32500;&#24230;&#37117;&#20197;6&#20998;&#21046;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;&#25163;&#21160;&#35780;&#20272;&#21487;&#33021;&#36739;&#24930;&#65292;&#24182;&#19988;&#22312;&#22823;&#29677;&#20013;&#23545;&#25152;&#26377;&#23398;&#29983;&#36827;&#34892;&#19968;&#33268;&#24615;&#26657;&#20934;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#23613;&#31649;&#22312;STEM&#23398;&#31185;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#30340;&#33258;&#21160;&#35780;&#20272;&#19978;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#65292;&#20294;&#22312;&#23454;&#39564;&#25253;&#21578;&#31561;&#38271;&#31687;&#20889;&#20316;&#20013;&#30340;&#24037;&#20316;&#35201;&#23569;&#24471;&#22810;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#29420;&#31435;&#30340;&#39564;&#35777;&#22120;&#21644;&#35780;&#20272;&#27169;&#22359;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With an increasing focus in STEM education on critical thinking skills, science writing plays an ever more important role in curricula that stress inquiry skills. A recently published dataset of two sets of college level lab reports from an inquiry-based physics curriculum relies on analytic assessment rubrics that utilize multiple dimensions, specifying subject matter knowledge and general components of good explanations. Each analytic dimension is assessed on a 6-point scale, to provide detailed feedback to students that can help them improve their science writing skills. Manual assessment can be slow, and difficult to calibrate for consistency across all students in large classes. While much work exists on automated assessment of open-ended questions in STEM subjects, there has been far less work on long-form writing such as lab reports. We present an end-to-end neural architecture that has separate verifier and assessment modules, inspired by approaches to Open Domain Question Answ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Mamba&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;Transformer&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;SSMs&#22312;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#20013;&#19982;Transformer&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#31232;&#30095;&#22855;&#20598;&#23398;&#20064;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#22312;&#28041;&#21450;&#38750;&#26631;&#20934;&#26816;&#32034;&#21151;&#33021;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.04248</link><description>&lt;p&gt;
Mamba&#33021;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#21527;&#65311;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Mamba&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;Transformer&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;SSMs&#22312;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#20013;&#19982;Transformer&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#31232;&#30095;&#22855;&#20598;&#23398;&#20064;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#22312;&#28041;&#21450;&#38750;&#26631;&#20934;&#26816;&#32034;&#21151;&#33021;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#20013;&#26367;&#20195;Transformer&#32593;&#32476;&#30340;&#36873;&#25321;&#65292;&#36890;&#36807;&#24341;&#20837;&#38376;&#25511;&#12289;&#21367;&#31215;&#21644;&#22522;&#20110;&#36755;&#20837;&#30340;&#20196;&#29260;&#36873;&#25321;&#26469;&#32531;&#35299;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#25104;&#26412;&#12290;&#34429;&#28982;SSMs&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#20294;&#19982;Transformer&#30456;&#27604;&#65292;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#26159;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;SSMs&#30340;ICL&#24615;&#33021;&#65292;&#30528;&#37325;&#30740;&#31350;&#20102;Mamba&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#19982;Transformer&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26631;&#20934;&#22238;&#24402;ICL&#20219;&#21153;&#20013;&#65292;SSMs&#30340;&#34920;&#29616;&#19982;Transformer&#30456;&#24403;&#65292;&#32780;&#22312;&#31232;&#30095;&#22855;&#20598;&#23398;&#20064;&#31561;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;Transformer&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#38750;&#26631;&#20934;&#26816;&#32034;&#21151;&#33021;&#30340;&#20219;&#21153;&#20013;&#65292;SSMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;\variant&#65292;&#23427;&#23558;Mamba&#19982;&#27880;&#24847;&#21147;&#24211;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models (SSMs), such as Mamba Gu &amp; Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, \variant, that combines Mamba with attention bloc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PPR&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08903</link><description>&lt;p&gt;
PPR: &#22312;&#32500;&#25345;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#21516;&#26102;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#23545;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems. (arXiv:2401.08903v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PPR&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#19978;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#21644;&#36530;&#36991;&#25915;&#20987;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#25104;&#21151;&#36827;&#34892;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#24182;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#22312;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#19978;&#25104;&#21151;&#36827;&#34892;&#36530;&#36991;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#35757;&#32451;&#20462;&#21098;&#24674;&#22797;&#25915;&#20987;&#65288;PPR&#65289;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#21487;&#20197;&#23558;&#19968;&#37096;&#20998;&#23545;&#25239;&#25200;&#21160;&#35774;&#20026;&#38646;&#65292;&#24182;&#20542;&#21521;&#20110;&#20445;&#25345;&#25915;&#20987;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#25105;&#20204;&#21487;&#20197;&#20462;&#21098;&#39044;&#35757;&#32451;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#26377;&#36873;&#25321;&#24615;&#22320;&#37322;&#25918;&#26576;&#20123;&#23545;&#25239;&#25200;&#21160;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#25200;&#21160;&#23884;&#20837;&#21040;&#20462;&#21098;&#21306;&#22495;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Face Recognition (FR) encompass two types: impersonation attacks and evasion attacks. We observe that achieving a successful impersonation attack on FR does not necessarily ensure a successful dodging attack on FR in the black-box setting. Introducing a novel attack method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance the performance of dodging attacks whilst avoiding the degradation of impersonation attacks. Our method employs adversarial example pruning, enabling a portion of adversarial perturbations to be set to zero, while tending to maintain the attack performance. By utilizing adversarial example pruning, we can prune the pre-trained adversarial examples and selectively free up certain adversarial perturbations. Thereafter, we embed adversarial perturbations in the pruned area, which enhances the dodging performance of the adversarial face examples. The effectiveness of our proposed attack method is demonstrated through our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21322;&#26080;&#30417;&#30563;&#26657;&#20934;&#31639;&#27861;&#65292;&#20351;&#29992;&#21160;&#21147;&#31995;&#32479;&#30340;&#35266;&#28857;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03051</link><description>&lt;p&gt;
&#21322;&#26080;&#30417;&#30563;&#26657;&#20934;&#36890;&#36807;&#20808;&#39564;&#36866;&#24212;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Semi Unsupervised Calibration through Prior Adaptation Algorithm. (arXiv:2401.03051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21322;&#26080;&#30417;&#30563;&#26657;&#20934;&#31639;&#27861;&#65292;&#20351;&#29992;&#21160;&#21147;&#31995;&#32479;&#30340;&#35266;&#28857;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26657;&#20934;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;Semi Unsupervised Calibration through Prior Adaptation (SUCPA)&#26159;&#19968;&#31181;&#26657;&#20934;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#23450;&#20041;&#20026;&#19968;&#20010;&#19968;&#38454;&#24046;&#20998;&#26041;&#31243;&#32452;&#12290;&#35813;&#26041;&#31243;&#32452;&#23548;&#20986;&#30340;&#26144;&#23556;&#20855;&#26377;&#38750;&#21452;&#26354;&#24615;&#29305;&#24449;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#32452;&#38750;&#23396;&#31435;&#19981;&#30028;&#23450;&#30340;&#22266;&#23450;&#28857;&#12290;&#26412;&#25991;&#20174;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#22810;&#20010;&#25910;&#25947;&#24615;&#36136;&#12290;&#23545;&#20110;&#20108;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#21487;&#20197;&#35777;&#26126;&#35813;&#31639;&#27861;&#24635;&#26159;&#25910;&#25947;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#35813;&#26144;&#23556;&#26159;&#20840;&#23616;&#28176;&#36817;&#31283;&#23450;&#30340;&#65292;&#36712;&#36947;&#25910;&#25947;&#21040;&#19968;&#26465;&#22266;&#23450;&#28857;&#30452;&#32447;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#23454;&#38469;&#24212;&#29992;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#25903;&#25345;&#25152;&#25552;&#20986;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#20195;&#30721;&#21487;&#22312;&#32593;&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibration is an essential key in machine leaning. Semi Unsupervised Calibration through Prior Adaptation (SUCPA) is a calibration algorithm used in (but not limited to) large-scale language models defined by a {system of first-order difference equation. The map derived by this system} has the peculiarity of being non-hyperbolic {with a non-bounded set of non-isolated fixed points}. In this work, we prove several convergence properties of this algorithm from the perspective of dynamical systems. For a binary classification problem, it can be shown that the algorithm always converges, {more precisely, the map is globally asymptotically stable, and the orbits converge} to a single line of fixed points. Finally, we perform numerical experiments on real-world application to support the presented results. Experiment codes are available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23398;&#24471;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20986;&#21452;&#35895;&#29616;&#35937;&#26159;&#22312;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#35757;&#32451;&#30340;&#19981;&#23436;&#32654;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#35895;&#29616;&#35937;&#30340;&#35299;&#37322;&#65306;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#22122;&#22768;&#25968;&#25454;&#30452;&#21040;&#25554;&#20540;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26469;&#23558;&#20449;&#24687;&#19982;&#22122;&#22768;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2310.13572</link><description>&lt;p&gt;
&#35299;&#24320;&#21452;&#35895;&#20043;&#35868;&#65306;&#36879;&#36807;&#23398;&#24471;&#29305;&#24449;&#31354;&#38388;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space. (arXiv:2310.13572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23398;&#24471;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20986;&#21452;&#35895;&#29616;&#35937;&#26159;&#22312;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#35757;&#32451;&#30340;&#19981;&#23436;&#32654;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#35895;&#29616;&#35937;&#30340;&#35299;&#37322;&#65306;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#22122;&#22768;&#25968;&#25454;&#30452;&#21040;&#25554;&#20540;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26469;&#23558;&#20449;&#24687;&#19982;&#22122;&#22768;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#35895;&#29616;&#35937;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#21576;&#29616;&#20986;&#19968;&#31181;&#36870;&#30452;&#35273;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#20854;&#34920;&#29616;&#12290;&#34429;&#28982;&#23545;&#35813;&#29616;&#35937;&#22312;&#29305;&#23450;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#19968;&#20123;&#29702;&#35770;&#35299;&#37322;&#65292;&#20294;&#20173;&#27809;&#26377;&#30830;&#31435;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#20013;&#20986;&#29616;&#21452;&#35895;&#30340;&#20844;&#35748;&#29702;&#35770;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21452;&#35895;&#29616;&#35937;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20986;&#29616;&#21463;&#21040;&#22122;&#22768;&#25968;&#25454;&#24433;&#21709;&#30340;&#24378;&#28872;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#23398;&#24471;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20986;&#22312;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#35757;&#32451;&#30340;&#19981;&#23436;&#32654;&#27169;&#22411;&#20013;&#20250;&#20986;&#29616;&#21452;&#35895;&#29616;&#35937;&#12290;&#25105;&#20204;&#35748;&#20026;&#21452;&#35895;&#26159;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#22122;&#22768;&#25968;&#25454;&#30452;&#21040;&#25554;&#20540;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26469;&#33719;&#21462;&#23558;&#20449;&#24687;&#19982;&#22122;&#22768;&#20998;&#31163;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20551;&#35774;&#21452;&#35895;&#29616;&#35937;&#19981;&#24212;&#35813;&#22312;&#33391;&#22909;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#20013;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Double descent presents a counter-intuitive aspect within the machine learning domain, and researchers have observed its manifestation in various models and tasks. While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory to account for its occurrence in deep learning remains yet to be established. In this study, we revisit the phenomenon of double descent and demonstrate that its occurrence is strongly influenced by the presence of noisy data. Through conducting a comprehensive analysis of the feature space of learned representations, we unveil that double descent arises in imperfect models trained with noisy data. We argue that double descent is a consequence of the model first learning the noisy data until interpolation and then adding implicit regularization via over-parameterization acquiring therefore capability to separate the information from the noise. We postulate that double descent should never occur in well-regulari
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04870</link><description>&lt;p&gt;
Lemur&#65306;&#22312;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lemur: Integrating Large Language Models in Automated Program Verification. (arXiv:2310.04870v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#23637;&#31034;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#25277;&#35937;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#39564;&#35777;&#24037;&#20855;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#30340;&#33021;&#21147;&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#27491;&#24335;&#25551;&#36848;&#20102;&#36825;&#31181;&#26041;&#27861;&#35770;&#65292;&#23558;&#20854;&#20316;&#20026;&#25512;&#23548;&#35268;&#21017;&#30340;&#38598;&#21512;&#36827;&#34892;&#35770;&#35777;&#20854;&#23436;&#22791;&#24615;&#12290;&#25105;&#20204;&#23558;&#35745;&#31639;&#26426;&#25512;&#29702;&#24418;&#25104;&#20026;&#19968;&#20010;&#23436;&#22791;&#30340;&#33258;&#21160;&#39564;&#35777;&#36807;&#31243;&#65292;&#36825;&#22312;&#19968;&#32452;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#24102;&#26469;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
&lt;/p&gt;</description></item><item><title>FOSA&#26159;&#19968;&#31181;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;FIML&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.12388</link><description>&lt;p&gt;
FOSA: &#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data. (arXiv:2308.12388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12388
&lt;/p&gt;
&lt;p&gt;
FOSA&#26159;&#19968;&#31181;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982; (FIML) &#20248;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#32570;&#22833;&#25968;&#25454;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;FIML&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#34917;&#20840;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#20540;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;&#26412;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;FIML&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#65288;FOSA&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#20102;&#20840;&#20449;&#24687;&#26368;&#22823;&#20284;&#28982;&#65288;FIML&#65289;&#20272;&#35745;&#21644;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#33021;&#21147;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;FIML&#23545;&#32570;&#22833;&#20540;&#36827;&#34892;&#21021;&#22987;&#20272;&#35745;&#65292;&#28982;&#21518;&#36890;&#36807;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#19968;&#27493;&#25552;&#28860;&#36825;&#20123;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;FOSA&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;FIML&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#65288;SEM&#65289;&#21487;&#33021;&#38169;&#35823;&#35268;&#23450;&#23548;&#33268;&#23376;&#20248;&#30340;FIML&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;FOSA&#33258;&#27880;&#24847;&#21147;&#32452;&#20214;&#30340;&#31283;&#20581;&#26550;&#26500;&#33021;&#22815;&#28789;&#27963;&#22320;&#32416;&#27491;&#21644;&#20248;&#21270;&#34917;&#20840;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data imputation, effectively addressing missing values is pivotal, especially in intricate datasets. This paper delves into the FIML Optimized Self-attention (FOSA) framework, an innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of self-attention neural networks. Our methodology commences with an initial estimation of missing values via FIML, subsequently refining these estimates by leveraging the self-attention mechanism. Our comprehensive experiments on both simulated and real-world datasets underscore FOSA's pronounced advantages over traditional FIML techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Intriguingly, even in scenarios where the Structural Equation Model (SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust architecture of FOSA's self-attention component adeptly rectifies and optimizes the imputati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;SYNAuG&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00994</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65306;&#22522;&#20110;&#25968;&#25454;&#35282;&#24230;&#30340;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective. (arXiv:2308.00994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;SYNAuG&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#29983;&#27963;&#22312;&#19968;&#20010;&#24222;&#22823;&#30340;&#25968;&#25454;&#28023;&#27915;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20063;&#19981;&#20363;&#22806;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#34920;&#29616;&#20986;&#19968;&#31181;&#22266;&#26377;&#30340;&#19981;&#24179;&#34913;&#29616;&#35937;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#32473;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#24102;&#26469;&#20102;&#39118;&#38505;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#36947;&#24503;&#21644;&#31038;&#20250;&#21518;&#26524;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#37492;&#20110;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;SYNAuG&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#20043;&#21069;&#20351;&#29992;&#30340;&#20219;&#21153;&#29305;&#23450;&#31639;&#27861;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;&#36825;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#22312;CIFAR100-LT&#12289;ImageNet100-LT&#12289;UTKFace&#21644;Waterbird&#31561;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#20219;&#21153;&#29305;&#23450;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#25105;&#20204;&#24182;&#19981;&#22768;&#31216;&#25105;&#20204;&#30340;&#26041;&#27861;&#20316;&#20026;&#38382;&#39064;&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
We live in a vast ocean of data, and deep neural networks are no exception to this. However, this data exhibits an inherent phenomenon of imbalance. This imbalance poses a risk of deep neural networks producing biased predictions, leading to potentially severe ethical and social consequences. To address these challenges, we believe that the use of generative models is a promising approach for comprehending tasks, given the remarkable advancements demonstrated by recent diffusion models in generating high-quality images. In this work, we propose a simple yet effective baseline, SYNAuG, that utilizes synthetic data as a preliminary step before employing task-specific algorithms to address data imbalance problems. This straightforward approach yields impressive performance on datasets such as CIFAR100-LT, ImageNet100-LT, UTKFace, and Waterbird, surpassing the performance of existing task-specific methods. While we do not claim that our approach serves as a complete solution to the problem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#25968;&#25454;&#23545;&#20840;&#27874;&#24418;&#21453;&#28436;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#24378;&#35843;&#27169;&#22411;&#23481;&#37327;&#38656;&#35201;&#26681;&#25454;&#25968;&#25454;&#22823;&#23567;&#36827;&#34892;&#25193;&#23637;&#20197;&#21462;&#24471;&#26368;&#20339;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15388</link><description>&lt;p&gt;
&#20840;&#27874;&#24418;&#21453;&#28436;&#26159;&#21542;&#21463;&#30410;&#20110;&#22823;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Full Waveform Inversion Benefit from Big Data?. (arXiv:2307.15388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#25968;&#25454;&#23545;&#20840;&#27874;&#24418;&#21453;&#28436;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#24378;&#35843;&#27169;&#22411;&#23481;&#37327;&#38656;&#35201;&#26681;&#25454;&#25968;&#25454;&#22823;&#23567;&#36827;&#34892;&#25193;&#23637;&#20197;&#21462;&#24471;&#26368;&#20339;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#25968;&#25454;&#23545;&#20840;&#27874;&#24418;&#21453;&#28436;&#65288;FWI&#65289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#20247;&#25152;&#21608;&#30693;&#65292;&#22823;&#25968;&#25454;&#21487;&#20197;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;FWI&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#39564;&#35777;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;&#22823;&#22411;&#12289;&#22810;&#32467;&#26500;&#25968;&#25454;&#38598;OpenFWI&#19978;&#35757;&#32451;&#30340;FWI&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;OpenFWI&#30340;10&#20010;2D&#23376;&#38598;&#30340;&#32452;&#21512;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;FWI&#27169;&#22411;&#65292;&#36825;&#20123;&#23376;&#38598;&#24635;&#20849;&#21253;&#21547;&#20102;470K&#20010;&#25968;&#25454;&#23545;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#25552;&#39640;FWI&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#27169;&#22411;&#23481;&#37327;&#38656;&#35201;&#26681;&#25454;&#25968;&#25454;&#22823;&#23567;&#36827;&#34892;&#30456;&#24212;&#30340;&#25193;&#23637;&#20197;&#33719;&#24471;&#26368;&#20339;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the impact of big data on deep learning models for full waveform inversion (FWI). While it is well known that big data can boost the performance of deep learning models in many tasks, its effectiveness has not been validated for FWI. To address this gap, we present an empirical study that investigates how deep learning models in FWI behave when trained on OpenFWI, a collection of large-scale, multi-structural datasets published recently. Particularly, we train and evaluate the FWI models on a combination of 10 2D subsets in OpenFWI that contain 470K data pairs in total. Our experiments demonstrate that larger datasets lead to better performance and generalization of deep learning models for FWI. We further demonstrate that model capacity needs to scale in accordance with data size for optimal improvement.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#32806;&#21512;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#26469;&#22788;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#26469;&#30830;&#20445;&#20195;&#29702;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.12062</link><description>&lt;p&gt;
&#28216;&#25103;&#29702;&#35770;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26102;&#38388;&#32806;&#21512;&#30340;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations. (arXiv:2307.12062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12062
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#32806;&#21512;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#26469;&#22788;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#26469;&#30830;&#20445;&#20195;&#29702;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#33021;&#22815;&#22312;&#29615;&#22659;&#24178;&#25200;&#25110;&#23545;&#25239;&#25915;&#20987;&#19979;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#21487;&#33021;&#24178;&#25200;&#30340;&#31354;&#38388;&#22312;&#21508;&#20010;&#26102;&#38388;&#27493;&#39588;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#21487;&#33021;&#24178;&#25200;&#30340;&#31354;&#38388;&#21462;&#20915;&#20110;&#36807;&#21435;&#30340;&#24178;&#25200;&#12290;&#25105;&#20204;&#27491;&#24335;&#24341;&#20837;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#65292;&#23545;&#29616;&#26377;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRAD&#65292;&#19968;&#31181;&#26032;&#30340;&#28216;&#25103;&#29702;&#35770;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#32806;&#21512;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20004;&#20154;&#38646;&#21644;&#28216;&#25103;&#12290;&#36890;&#36807;&#22312;&#36825;&#20010;&#28216;&#25103;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#22343;&#34913;&#65292;GRAD&#30830;&#20445;&#20102;&#20195;&#29702;&#30340;&#23545;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#23545;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#22312;&#26631;&#20934;&#21644;&#26102;&#38388;&#32806;&#21512;&#24178;&#25200;&#19979;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#27491;&#21521;&#23545;&#38598;&#21512;&#65288;SPPS&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#20013;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#23427;&#20204;&#35270;&#20026;&#27491;&#21521;&#23454;&#20363;&#65292;&#20174;&#32780;&#20943;&#23569;&#20002;&#24323;&#37325;&#35201;&#29305;&#24449;&#30340;&#39118;&#38505;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16122</link><description>&lt;p&gt;
&#22686;&#24378;&#23545;&#27604;&#23454;&#20363;&#21306;&#20998;&#30340;&#35821;&#20041;&#27491;&#21521;&#23545;
&lt;/p&gt;
&lt;p&gt;
Semantic Positive Pairs for Enhancing Contrastive Instance Discrimination. (arXiv:2306.16122v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#27491;&#21521;&#23545;&#38598;&#21512;&#65288;SPPS&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#20013;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#23427;&#20204;&#35270;&#20026;&#27491;&#21521;&#23454;&#20363;&#65292;&#20174;&#32780;&#20943;&#23569;&#20002;&#24323;&#37325;&#35201;&#29305;&#24449;&#30340;&#39118;&#38505;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23454;&#20363;&#21306;&#20998;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26377;&#25928;&#22320;&#38450;&#27490;&#34920;&#31034;&#22349;&#32553;&#65292;&#24182;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#20135;&#29983;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#21560;&#24341;&#27491;&#21521;&#23545;&#65288;&#21363;&#30456;&#21516;&#23454;&#20363;&#30340;&#20004;&#20010;&#35270;&#22270;&#65289;&#24182;&#25490;&#26021;&#25152;&#26377;&#20854;&#20182;&#23454;&#20363;&#65288;&#21363;&#36127;&#21521;&#23545;&#65289;&#65292;&#26080;&#35770;&#23427;&#20204;&#30340;&#31867;&#21035;&#65292;&#21487;&#33021;&#23548;&#33268;&#20002;&#24323;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#23427;&#20204;&#35270;&#20026;&#27491;&#21521;&#23454;&#20363;&#65292;&#21629;&#21517;&#20026;&#35821;&#20041;&#27491;&#21521;&#23545;&#38598;&#21512;&#65288;SPPS&#65289;&#65292;&#20174;&#32780;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#20943;&#23569;&#20102;&#20002;&#24323;&#37325;&#35201;&#29305;&#24449;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#23545;&#27604;&#23454;&#20363;&#21306;&#20998;&#26694;&#26550;&#65288;&#22914;SimCLR&#25110;MOCO&#65289;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;ImageNet&#12289;STL-10&#21644;CIFAR-10&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;vanilla&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning algorithms based on instance discrimination effectively prevent representation collapse and produce promising results in representation learning. However, the process of attracting positive pairs (i.e., two views of the same instance) in the embedding space and repelling all other instances (i.e., negative pairs) irrespective of their categories could result in discarding important features. To address this issue, we propose an approach to identifying those images with similar semantic content and treating them as positive instances, named semantic positive pairs set (SPPS), thereby reducing the risk of discarding important features during representation learning. Our approach could work with any contrastive instance discrimination framework such as SimCLR or MOCO. We conduct experiments on three datasets: ImageNet, STL-10 and CIFAR-10 to evaluate our approach. The experimental results show that our approach consistently outperforms the baseline method vanilla 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#26500;&#30340;&#35282;&#33394;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#20010;&#23376;&#39046;&#22495;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#26041;&#38754;&#25152;&#20570;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.16021</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32467;&#26500;&#65306;&#35843;&#26597;&#19982;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Structure in Reinforcement Learning: A Survey and Open Problems. (arXiv:2306.16021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16021
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#26500;&#30340;&#35282;&#33394;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#20010;&#23376;&#39046;&#22495;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#26041;&#38754;&#25152;&#20570;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20511;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#20989;&#25968;&#36924;&#36817;&#26041;&#38754;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#24212;&#23545;&#22810;&#26679;&#19988;&#19981;&#21487;&#39044;&#27979;&#30340;&#21160;&#24577;&#12289;&#22024;&#26434;&#20449;&#21495;&#20197;&#21450;&#24222;&#22823;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#31561;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#26102;&#65292;&#20854;&#23454;&#29992;&#24615;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;&#35832;&#22914;&#25968;&#25454;&#25928;&#29575;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12289;&#32570;&#23569;&#23433;&#20840;&#20445;&#35777;&#21644;&#19981;&#21487;&#35299;&#37322;&#24615;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#24182;&#22312;&#36825;&#20123;&#20851;&#38190;&#25351;&#26631;&#19978;&#25552;&#39640;&#24615;&#33021;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#26159;&#23558;&#38382;&#39064;&#30340;&#38468;&#21152;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#21508;&#20010;&#23376;&#39046;&#22495;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#32435;&#20837;&#36825;&#26679;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#26694;&#26550;&#19979;&#65292;&#25581;&#31034;&#32467;&#26500;&#22312;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep Neural Networks (DNNs) for function approximation, has demonstrated considerable success in numerous applications. However, its practicality in addressing a wide range of real-world scenarios, characterized by diverse and unpredictable dynamics, noisy signals, and large state and action spaces, remains limited. This limitation stems from issues such as poor data efficiency, limited generalization capabilities, a lack of safety guarantees, and the absence of interpretability, among other factors. To overcome these challenges and improve performance across these crucial metrics, one promising avenue is to incorporate additional structural information about the problem into the RL learning process. Various sub-fields of RL have proposed methods for incorporating such inductive biases. We amalgamate these diverse methodologies under a unified framework, shedding light on the role of structure in the learning prob
&lt;/p&gt;</description></item><item><title>Brainformers &#26159;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#23618;&#32423;&#30340;&#32467;&#26500;&#23436;&#21892;&#20102; Transformer &#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00008</link><description>&lt;p&gt;
Brainformers&#65306;&#20197;&#25928;&#29575;&#25442;&#21462;&#31616;&#27905;&#24615;
&lt;/p&gt;
&lt;p&gt;
Brainformers: Trading Simplicity for Efficiency. (arXiv:2306.00008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00008
&lt;/p&gt;
&lt;p&gt;
Brainformers &#26159;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#23618;&#32423;&#30340;&#32467;&#26500;&#23436;&#21892;&#20102; Transformer &#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#36817;&#25104;&#21151;&#30340;&#26680;&#24515;&#25216;&#26415;&#12290;Transformer &#20855;&#26377;&#19968;&#20010;&#20960;&#20046;&#32479;&#19968;&#30340;&#39592;&#26550;&#65292;&#20854;&#20013;&#23618;&#27425;&#22312;&#21069;&#39304;&#21644;&#33258;&#27880;&#24847;&#21147;&#20043;&#38388;&#20132;&#26367;&#20197;&#24314;&#31435;&#28145;&#24230;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#21457;&#29616;&#26356;&#22797;&#26434;&#30340;&#22359;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#26681;&#25454;&#36825;&#20010;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#22359;&#65292;&#31216;&#20026; Brainformer&#65292;&#23427;&#30001;&#21508;&#31181;&#24418;&#24335;&#30340;&#23618;&#24402;&#19968;&#21270;&#21644;&#28608;&#27963;&#20989;&#25968;&#12289;&#31232;&#30095;&#38376;&#25511;&#21069;&#39304;&#23618;&#12289;&#23494;&#38598;&#21069;&#39304;&#23618;&#12289;&#27880;&#24847;&#21147;&#23618;&#31561;&#22810;&#26679;&#23618;&#32423;&#32452;&#25104;&#12290;&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#65292;Brainformer &#24635;&#26159;&#20248;&#20110;&#29616;&#26377;&#30340;&#31264;&#23494;&#21644;&#31232;&#30095; Transformer&#12290;&#19968;&#20010;&#20855;&#26377; 80 &#20159;&#20010;&#27599;&#20010;&#26631;&#35760;&#28608;&#27963;&#21442;&#25968;&#30340; Brainformer &#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#20854; GLaM &#23545;&#24212;&#29289;&#65292;&#34920;&#29616;&#20986; 2 &#20493;&#26356;&#24555;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644; 5 &#20493;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#12290;&#22312;&#19979;&#28216;&#20219;&#21153;&#35780;&#20272;&#20013;&#65292;Brainformer &#20063;&#34920;&#29616;&#24471;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are central to recent successes in natural language processing and computer vision. Transformers have a mostly uniform backbone where layers alternate between feed-forward and self-attention in order to build a deep network. Here we investigate this design choice and find that more complex blocks that have different permutations of layer primitives can be more efficient. Using this insight, we develop a complex block, named Brainformer, that consists of a diverse sets of layers such as sparsely gated feed-forward layers, dense feed-forward layers, attention layers, and various forms of layer normalization and activation functions. Brainformer consistently outperforms the state-of-the-art dense and sparse Transformers, in terms of both quality and efficiency. A Brainformer model with 8 billion activated parameters per token demonstrates 2x faster training convergence and 5x faster step time compared to its GLaM counterpart. In downstream task evaluation, Brainformer also de
&lt;/p&gt;</description></item><item><title>IDEA&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#24378;&#39044;&#27979;&#24615;&#21644;&#36328;&#25915;&#20987;&#19981;&#21464;&#24615;&#30340;&#22240;&#26524;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#24418;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#19981;&#21464;&#22240;&#26524;&#38450;&#24481;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15792</link><description>&lt;p&gt;
IDEA&#65306;&#22270;&#24418;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#19981;&#21464;&#22240;&#26524;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
IDEA: Invariant Causal Defense for Graph Adversarial Robustness. (arXiv:2305.15792v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15792
&lt;/p&gt;
&lt;p&gt;
IDEA&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#24378;&#39044;&#27979;&#24615;&#21644;&#36328;&#25915;&#20987;&#19981;&#21464;&#24615;&#30340;&#22240;&#26524;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#24418;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#19981;&#21464;&#22240;&#26524;&#38450;&#24481;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#24341;&#36215;&#20102;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#21487;&#20197;&#25269;&#25239;&#19968;&#20123;&#25915;&#20987;&#65292;&#20294;&#22312;&#20854;&#20182;&#26410;&#30693;&#25915;&#20987;&#19979;&#20250;&#36973;&#21463;&#38590;&#20197;&#25215;&#21463;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#26377;&#38480;&#30340;&#35266;&#23519;&#21040;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#36827;&#34892;&#20248;&#21270;&#65288;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#25110;&#29305;&#23450;&#21551;&#21457;&#24335;&#26469;&#25913;&#21464;&#22270;&#24418;&#25110;&#27169;&#22411;&#32467;&#26500;&#65288;&#22270;&#32431;&#21270;&#25110;&#40065;&#26834;&#32858;&#21512;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#22240;&#26524;&#38450;&#24481;&#26041;&#27861;&#26469;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#65288;IDEA&#65289;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#20855;&#26377;&#24378;&#39044;&#27979;&#26631;&#31614;&#30340;&#22240;&#26524;&#29305;&#24449;&#65292;&#24182;&#19988;&#36328;&#25915;&#20987;&#20855;&#26377;&#19981;&#21464;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#22270;&#24418;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23545;&#22270;&#24418;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#19981;&#21464;&#24615;&#30446;&#26631;&#26469;&#23398;&#20064;&#22240;&#26524;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#23454;&#39564;&#21644;&#32508;&#21512;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;IDEA&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have achieved remarkable success in various tasks, however, their vulnerability to adversarial attacks raises concerns for the real-world applications. Existing defense methods can resist some attacks, but suffer unbearable performance degradation under other unknown attacks. This is due to their reliance on either limited observed adversarial examples to optimize (adversarial training) or specific heuristics to alter graph or model structures (graph purification or robust aggregation). In this paper, we propose an Invariant causal DEfense method against adversarial Attacks (IDEA), providing a new perspective to address this issue. The method aims to learn causal features that possess strong predictability for labels and invariant predictability across attacks, to achieve graph adversarial robustness. Through modeling and analyzing the causal relationships in graph adversarial attacks, we design two invariance objectives to learn the causal features. Extens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12517</link><description>&lt;p&gt;
&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38024;&#23545;&#25991;&#26412;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#25351;&#20196;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#20110;&#22312;&#22823;&#35268;&#27169;&#25991;&#26723;&#38598;&#21512;&#20013;&#23450;&#20301;&#31526;&#21512;&#32473;&#23450;&#25551;&#36848;&#30340;&#25991;&#26412;&#65288;&#35821;&#20041;&#26816;&#32034;&#65289;&#24182;&#19981;&#36866;&#29992;&#12290;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#30340;&#30456;&#20284;&#24230;&#25628;&#32034;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#25191;&#34892;&#26816;&#32034;&#65292;&#20294;&#23884;&#20837;&#20013;&#30340;&#30456;&#20284;&#24230;&#23450;&#20041;&#19981;&#26126;&#30830;&#19988;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#23545;&#20110;&#35768;&#22810;&#29992;&#20363;&#26469;&#35828;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#37027;&#20040;&#65292;&#20160;&#20040;&#26159;&#26377;&#25928;&#26816;&#32034;&#30340;&#22909;&#30340;&#26597;&#35810;&#34920;&#31034;&#65311;&#25105;&#20204;&#30830;&#23450;&#20102;&#26681;&#25454;&#20869;&#23481;&#30340;&#25688;&#35201;&#25551;&#36848;&#26816;&#32034;&#21477;&#23376;&#30340;&#26126;&#30830;&#23450;&#20041;&#19988;&#19968;&#33268;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#25991;&#26412;&#23884;&#20837;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#22411;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#36890;&#36807;&#25552;&#31034;LLM&#33719;&#24471;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#24456;&#23481;&#26131;&#20174;LLM&#20013;&#33719;&#24471;&#35757;&#32451;&#26448;&#26009;&#65292;&#20294;LLM&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#22312;&#21608;&#26399;&#24615;&#27880;&#20837;&#25915;&#20987;&#26102;&#65292;&#31995;&#32479;&#21160;&#24577;&#21487;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(n)&#65307;&#24403;&#25915;&#20987;&#20197;&#27010;&#29575;p&#36827;&#34892;&#26102;&#65292;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(log(n)p/(1-p)^2) &#12290;&#21363;&#20351;&#26377;&#36229;&#36807;&#19968;&#21322;&#30340;&#25968;&#25454;&#21463;&#25439;&#65292;&#20272;&#35745;&#22120;&#20173;&#21487;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.10506</link><description>&lt;p&gt;
&#26356;&#22810;&#33039;&#25968;&#25454;&#19979;&#30340;&#31995;&#32479;&#35782;&#21035;&#31934;&#30830;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Exact Recovery for System Identification with More Corrupt Data than Clean Data. (arXiv:2305.10506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#22312;&#21608;&#26399;&#24615;&#27880;&#20837;&#25915;&#20987;&#26102;&#65292;&#31995;&#32479;&#21160;&#24577;&#21487;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(n)&#65307;&#24403;&#25915;&#20987;&#20197;&#27010;&#29575;p&#36827;&#34892;&#26102;&#65292;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(log(n)p/(1-p)^2) &#12290;&#21363;&#20351;&#26377;&#36229;&#36807;&#19968;&#21322;&#30340;&#25968;&#25454;&#21463;&#25439;&#65292;&#20272;&#35745;&#22120;&#20173;&#21487;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;Lasso&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#21644;&#38750;&#28176;&#36817;&#29305;&#24615;&#65292;&#28041;&#21450;&#21040;&#23545;&#20110;&#25915;&#20987;&#26102;&#21051;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#20004;&#31181;&#19981;&#21516;&#22330;&#26223;&#12290;&#30001;&#20110;&#25910;&#38598;&#30340;&#26679;&#26412;&#30456;&#20851;&#65292;&#29616;&#26377;&#30340;Lasso&#32467;&#26524;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#31995;&#32479;&#31283;&#23450;&#19988;&#25915;&#20987;&#20197;&#21608;&#26399;&#24615;&#27880;&#20837;&#26102;&#65292;&#31995;&#32479;&#21160;&#24577;&#30340;&#31934;&#30830;&#24674;&#22797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(n)&#65292;&#20854;&#20013;n&#26159;&#29366;&#24577;&#30340;&#32500;&#24230;&#12290;&#24403;&#25915;&#20987;&#22312;&#27599;&#20010;&#26102;&#38388;&#23454;&#20363;&#20013;&#20197;&#27010;&#29575;p&#36827;&#34892;&#26102;&#65292;&#31934;&#30830;&#24674;&#22797;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23558;&#25353;O(log (n)p / (1-p)^2)&#36827;&#34892;&#32553;&#25918;&#12290;&#35813;&#32467;&#26524;&#22312;&#28176;&#36817;&#29366;&#24577;&#19979;&#24847;&#21619;&#30528;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#20110;&#30495;&#23454;&#31995;&#32479;&#21160;&#24577;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#21363;&#20351;&#36229;&#36807;&#19968;&#21322;&#30340;&#25968;&#25454;&#21463;&#25439;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the system identification problem for linear discrete-time systems under adversaries and analyze two lasso-type estimators. We study both asymptotic and non-asymptotic properties of these estimators in two separate scenarios, corresponding to deterministic and stochastic models for the attack times. Since the samples collected from the system are correlated, the existing results on lasso are not applicable. We show that when the system is stable and the attacks are injected periodically, the sample complexity for the exact recovery of the system dynamics is O(n), where n is the dimension of the states. When the adversarial attacks occur at each time instance with probability p, the required sample complexity for the exact recovery scales as O(\log(n)p/(1-p)^2). This result implies the almost sure convergence to the true system dynamics under the asymptotic regime. As a by-product, even when more than half of the data is compromised, our estimators still learn th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25554;&#20540;&#20219;&#20309;&#25968;&#25454;&#38598;&#65292;&#21363;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#20840;&#23616;&#26368;&#23567;&#20540;&#20026;&#38646;&#30340;&#24615;&#36136;&#65292;&#27492;&#22806;&#36824;&#32473;&#20986;&#20102;&#35813;&#20840;&#23616;&#26368;&#23567;&#20540;&#22788;&#30340;&#24815;&#24615;&#30697;&#38453;&#30340;&#34920;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#27010;&#29575;&#26041;&#27861;&#26469;&#23547;&#25214;&#25554;&#20540;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.10552</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Interpolation property of shallow neural networks. (arXiv:2304.10552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25554;&#20540;&#20219;&#20309;&#25968;&#25454;&#38598;&#65292;&#21363;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#20840;&#23616;&#26368;&#23567;&#20540;&#20026;&#38646;&#30340;&#24615;&#36136;&#65292;&#27492;&#22806;&#36824;&#32473;&#20986;&#20102;&#35813;&#20840;&#23616;&#26368;&#23567;&#20540;&#22788;&#30340;&#24815;&#24615;&#30697;&#38453;&#30340;&#34920;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#27010;&#29575;&#26041;&#27861;&#26469;&#23547;&#25214;&#25554;&#20540;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#20960;&#20309;&#24615;&#36136;&#12290;&#22312;&#22823;&#22810;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#25439;&#22833;&#20989;&#25968;&#26159;&#20984;&#20989;&#25968;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#25105;&#20204;&#21482;&#26377;&#19968;&#20010;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#25110;&#32773;&#26159;&#38750;&#20984;&#20989;&#25968;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25105;&#20204;&#26377;&#19968;&#20010;&#26377;&#38480;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36229;&#21442;&#25968;&#21270;&#33539;&#22260;&#20869;&#65292;&#23545;&#20110;&#38750;&#23567;&#27425;&#25968;&#22810;&#39033;&#24335;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25554;&#20540;&#20219;&#20309;&#25968;&#25454;&#38598;&#65292;&#21363;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#20840;&#23616;&#26368;&#23567;&#20540;&#20026;&#38646;&#30340;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23384;&#22312;&#36825;&#26679;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#21017;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#36718;&#24275;&#26377;&#26080;&#31351;&#22810;&#20010;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#22312;&#20840;&#23616;&#26368;&#23567;&#20540;&#22788;&#27714;&#35299;&#25439;&#22833;&#20989;&#25968;&#30340;&#28023;&#22622;&#30697;&#38453;&#30340;&#34920;&#24449;&#65292;&#24182;&#22312;&#26368;&#21518;&#19968;&#33410;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#27010;&#29575;&#26041;&#27861;&#26469;&#23547;&#25214;&#25554;&#20540;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the geometry of global minima of the loss landscape of overparametrized neural networks. In most optimization problems, the loss function is convex, in which case we only have a global minima, or nonconvex, with a discrete number of global minima. In this paper, we prove that in the overparametrized regime, a shallow neural network can interpolate any data set, i.e. the loss function has a global minimum value equal to zero as long as the activation function is not a polynomial of small degree. Additionally, if such a global minimum exists, then the locus of global minima has infinitely many points. Furthermore, we give a characterization of the Hessian of the loss function evaluated at the global minima, and in the last section, we provide a practical probabilistic method of finding the interpolation point.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22352;&#26631;&#21464;&#25442;&#26469;&#21152;&#36895;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#12289;&#25913;&#21892;&#33618;&#21407;&#39640;&#21407;&#21644;&#23616;&#37096;&#26368;&#23567;&#20540;&#24433;&#21709;&#30340;&#36890;&#29992;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22810;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06768</link><description>&lt;p&gt;
&#36890;&#36807;&#22352;&#26631;&#21464;&#25442;&#25913;&#36827;&#26799;&#24230;&#26041;&#27861;&#65306;&#24212;&#29992;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Gradient Methods via Coordinate Transformations: Applications to Quantum Machine Learning. (arXiv:2304.06768v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22352;&#26631;&#21464;&#25442;&#26469;&#21152;&#36895;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#12289;&#25913;&#21892;&#33618;&#21407;&#39640;&#21407;&#21644;&#23616;&#37096;&#26368;&#23567;&#20540;&#24433;&#21709;&#30340;&#36890;&#29992;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22810;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#26159;&#32463;&#20856;&#30340;&#36824;&#26159;&#37327;&#23376;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#37117;&#22823;&#37327;&#20381;&#36182;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#22914;&#26799;&#24230;&#19979;&#38477;&#31561;&#12290;&#24635;&#20307;&#24615;&#33021;&#21462;&#20915;&#20110;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#33618;&#21407;&#39640;&#21407;&#30340;&#20986;&#29616;&#65292;&#36825;&#20250;&#20943;&#32531;&#35745;&#31639;&#36895;&#24230;&#24182;&#23548;&#33268;&#38750;&#26368;&#20248;&#35299;&#12290;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#20250;&#23548;&#33268;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#25104;&#26412;&#28608;&#22686;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#65292;&#20197;&#21152;&#36895;&#21644;&#25913;&#21892;&#36825;&#20123;&#26041;&#27861;&#30340;&#24635;&#20307;&#24615;&#33021;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#33618;&#21407;&#39640;&#21407;&#21644;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22352;&#26631;&#21464;&#25442;&#65292;&#26377;&#28857;&#31867;&#20284;&#20110;&#21464;&#20998;&#26059;&#36716;&#65292;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#28155;&#21152;&#20102;&#39069;&#22806;&#30340;&#26041;&#21521;&#65292;&#36825;&#20123;&#26041;&#21521;&#21462;&#20915;&#20110;&#25104;&#26412;&#20989;&#25968;&#26412;&#36523;&#65292;&#24182;&#19988;&#20801;&#35768;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#37197;&#32622;&#26223;&#35266;&#12290;&#25105;&#20204;&#24050;&#36890;&#36807;&#22686;&#24378;&#22810;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38750;&#24120;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms, both in their classical and quantum versions, heavily rely on optimization algorithms based on gradients, such as gradient descent and alike. The overall performance is dependent on the appearance of local minima and barren plateaus, which slow-down calculations and lead to non-optimal solutions. In practice, this results in dramatic computational and energy costs for AI applications. In this paper we introduce a generic strategy to accelerate and improve the overall performance of such methods, allowing to alleviate the effect of barren plateaus and local minima. Our method is based on coordinate transformations, somehow similar to variational rotations, adding extra directions in parameter space that depend on the cost function itself, and which allow to explore the configuration landscape more efficiently. The validity of our method is benchmarked by boosting a number of quantum machine learning algorithms, getting a very significant improvement in their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#21160;&#24577;&#33976;&#39311;&#24182;&#20248;&#21270;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#23545;&#24212;&#30340;&#26381;&#21153;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.00083</link><description>&lt;p&gt;
Fides&#65306;&#19968;&#31181;&#21033;&#29992;&#23433;&#20840;&#25191;&#34892;&#29615;&#22659;&#23545;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#32467;&#26524;&#39564;&#35777;&#30340;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE. (arXiv:2304.00083v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#21160;&#24577;&#33976;&#39311;&#24182;&#20248;&#21270;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#23545;&#24212;&#30340;&#26381;&#21153;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25935;&#24863;&#39046;&#22495;&#30340;&#37096;&#32626;&#23548;&#33268;&#20102;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#37325;&#35270;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22810;&#26041;&#35745;&#31639;&#21644;&#22522;&#20110;&#35777;&#26126;&#30340;&#31995;&#32479;&#65292;&#32473;&#23454;&#26102;&#24212;&#29992;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Fides&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#39564;&#35777;&#22806;&#21327;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#65292;&#20854;&#20013;&#37319;&#29992;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#36138;&#24515;&#33976;&#39311;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#19968;&#31181;&#23454;&#26102;&#39564;&#35777;&#27169;&#22411;&#26469;&#36739;&#23569;&#22320;&#28040;&#32791;&#31354;&#38388;&#21644;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#36816;&#34892;&#22312;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing popularity of Machine Learning (ML) has led to its deployment in various sensitive domains, which has resulted in significant research focused on ML security and privacy. However, in some applications, such as autonomous driving, integrity verification of the outsourced ML workload is more critical-a facet that has not received much attention. Existing solutions, such as multi-party computation and proof-based systems, impose significant computation overhead, which makes them unfit for real-time applications. We propose Fides, a novel framework for real-time validation of outsourced ML workloads. Fides features a novel and efficient distillation technique-Greedy Distillation Transfer Learning-that dynamically distills and fine-tunes a space and compute-efficient verification model for verifying the corresponding service model while running inside a trusted execution environment. Fides features a client-side attack detection model that uses statistical analysis and divergenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#27169;&#22411;&#30340;GADFormer&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#36712;&#36857;&#19978;&#25191;&#34892;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32676;&#20307;&#24322;&#24120;&#26816;&#27979;&#65292;&#30456;&#27604;&#20854;&#20182;&#28145;&#24230;&#24207;&#21015;&#27169;&#22411;&#65292;GADFormer&#34920;&#29616;&#26356;&#20248;&#65292;&#22312;&#22810;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;GADFormer&#22312;&#26816;&#27979;&#24322;&#24120;&#32676;&#20307;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.09841</link><description>&lt;p&gt;
GADFormer:&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36712;&#36857;&#32676;&#20307;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GADFormer: An Attention-based Model for Group Anomaly Detection on Trajectories. (arXiv:2303.09841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#27169;&#22411;&#30340;GADFormer&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#36712;&#36857;&#19978;&#25191;&#34892;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32676;&#20307;&#24322;&#24120;&#26816;&#27979;&#65292;&#30456;&#27604;&#20854;&#20182;&#28145;&#24230;&#24207;&#21015;&#27169;&#22411;&#65292;GADFormer&#34920;&#29616;&#26356;&#20248;&#65292;&#22312;&#22810;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;GADFormer&#22312;&#26816;&#27979;&#24322;&#24120;&#32676;&#20307;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#24322;&#24120;&#26816;&#27979;(GAD)&#21487;&#20197;&#25581;&#31034;&#30001;&#22810;&#20010;&#25104;&#21592;&#23454;&#20363;&#32452;&#25104;&#30340;&#32676;&#20307;&#20013;&#24322;&#24120;&#34892;&#20026;&#65292;&#28982;&#32780;&#38543;&#30528;&#32676;&#20307;&#25104;&#21592;&#25968;&#37327;&#21644;&#24322;&#26500;&#24615;&#30340;&#22686;&#21152;&#65292;&#23454;&#38469;&#19978;&#30340;&#24322;&#24120;&#32676;&#20307;&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#26816;&#27979;&#65292;&#23588;&#20854;&#26159;&#22312;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#35774;&#23450;&#19979;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#20307;&#31995;&#32467;&#26500;&#30340;GAD&#19987;&#29992;&#27169;&#22411;GADFormer&#65292;&#23427;&#33021;&#22815;&#22312;&#36712;&#36857;&#19978;&#25191;&#34892;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32676;&#20307;&#24322;&#24120;&#26816;&#27979;,&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#26816;&#27979;&#39640;&#31934;&#24230;&#29575;&#19979;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group Anomaly Detection (GAD) reveals anomalous behavior among groups consisting of multiple member instances, which are, individually considered, not necessarily anomalous. This task is of major importance across multiple disciplines, in which also sequences like trajectories can be considered as a group. However, with increasing amount and heterogenity of group members, actual abnormal groups get harder to detect, especially in an unsupervised or semi-supervised setting. Recurrent Neural Networks are well established deep sequence models, but recent works have shown that their performance can decrease with increasing sequence lengths. Hence, we introduce with this paper GADFormer, a GAD specific BERT architecture, capable to perform attention-based Group Anomaly Detection on trajectories in an unsupervised and semi-supervised setting. We show formally and experimentally how trajectory outlier detection can be realized as an attention-based Group Anomaly Detection problem. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#22825;&#32447;&#31995;&#32479;&#20013;&#37319;&#29992;&#25968;&#23383;&#35843;&#21046;&#21644;&#31354;&#20013;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#25968;&#23383;&#35843;&#21046;&#21644;AirComp&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#26080;&#32447;&#20449;&#36947;&#34928;&#33853;&#23548;&#33268;&#30340;&#24635;&#20307;&#22833;&#30495;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14648</link><description>&lt;p&gt;
&#22810;&#22825;&#32447;&#31995;&#32479;&#20013;&#25968;&#23383;&#26080;&#32447;&#21327;&#20316;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Digital Over-the-Air Federated Learning in Multi-Antenna Systems. (arXiv:2302.14648v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#22825;&#32447;&#31995;&#32479;&#20013;&#37319;&#29992;&#25968;&#23383;&#35843;&#21046;&#21644;&#31354;&#20013;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#32467;&#21512;&#25968;&#23383;&#35843;&#21046;&#21644;AirComp&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#26080;&#32447;&#20449;&#36947;&#34928;&#33853;&#23548;&#33268;&#30340;&#24635;&#20307;&#22833;&#30495;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#38469;&#30340;&#26080;&#32447;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#37319;&#29992;&#25968;&#23383;&#35843;&#21046;&#21644;&#31354;&#20013;&#35745;&#31639;&#65288;AirComp&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#29305;&#21035;&#32771;&#34385;&#20102;&#19968;&#20010;MIMO&#31995;&#32479;&#65292;&#22312;&#35813;&#31995;&#32479;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#20351;&#29992;&#27874;&#26463;&#24418;&#25104;&#23558;&#20854;&#26412;&#22320;&#25910;&#38598;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#26412;&#22320;FL&#27169;&#22411;&#20256;&#36755;&#32473;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#65292;&#20197;&#26368;&#22823;&#21270;&#21487;&#35843;&#24230;&#20256;&#36755;&#30340;&#35774;&#22791;&#25968;&#37327;&#12290;PS&#20316;&#20026;&#20013;&#22830;&#25511;&#21046;&#22120;&#65292;&#20351;&#29992;&#25509;&#25910;&#21040;&#30340;&#26412;&#22320;FL&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#20840;&#23616;FL&#27169;&#22411;&#24182;&#24191;&#25773;&#32473;&#25152;&#26377;&#35774;&#22791;&#12290;&#30001;&#20110;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24102;&#23485;&#26377;&#38480;&#65292;&#37319;&#29992;&#20102;AirComp&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26080;&#32447;&#25968;&#25454;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#26080;&#32447;&#20449;&#36947;&#30340;&#34928;&#33853;&#20250;&#20135;&#29983;&#22522;&#20110;AirComp&#30340;FL&#26041;&#26696;&#20013;&#30340;&#24635;&#20307;&#22833;&#30495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#31639;&#27861;&#65292;&#23558;&#25968;&#23383;&#35843;&#21046;&#19982;AirComp&#30456;&#32467;&#21512;&#20197;&#20943;&#36731;&#22833;&#30495;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, the performance optimization of federated learning (FL), when deployed over a realistic wireless multiple-input multiple-output (MIMO) communication system with digital modulation and over-the-air computation (AirComp) is studied. In particular, a MIMO system is considered in which edge devices transmit their local FL models (trained using their locally collected data) to a parameter server (PS) using beamforming to maximize the number of devices scheduled for transmission. The PS, acting as a central controller, generates a global FL model using the received local FL models and broadcasts it back to all devices. Due to the limited bandwidth in a wireless network, AirComp is adopted to enable efficient wireless data aggregation. However, fading of wireless channels can produce aggregate distortions in an AirComp-based FL scheme. To tackle this challenge, we propose a modified federated averaging (FedAvg) algorithm that combines digital modulation with AirComp to mitigate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;Bagging&#25216;&#26415;&#21487;&#25552;&#20379;&#26080;&#20559;&#24046;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#20998;&#24067;&#21644;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.12600</link><description>&lt;p&gt;
Bagging&#25552;&#20379;&#26080;&#20559;&#24046;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bagging Provides Assumption-free Stability. (arXiv:2301.12600v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Bagging&#25216;&#26415;&#21487;&#25552;&#20379;&#26080;&#20559;&#24046;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#20998;&#24067;&#21644;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bagging&#26159;&#31283;&#23450;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20219;&#20309;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#25512;&#23548;&#20102;&#19968;&#20010;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#23545;&#25968;&#25454;&#20998;&#24067;&#12289;&#22522;&#26412;&#31639;&#27861;&#30340;&#23646;&#24615;&#25110;&#21327;&#21464;&#37327;&#30340;&#32500;&#25968;&#36827;&#34892;&#20219;&#20309;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#20445;&#35777;&#36866;&#29992;&#20110;&#22810;&#31181;&#21464;&#20307;&#30340;Bagging&#65292;&#24182;&#19988;&#26159;&#26368;&#20248;&#30340;&#24120;&#25968;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#34920;&#26126;Bagging&#25104;&#21151;&#31283;&#23450;&#20102;&#21363;&#20351;&#26159;&#39640;&#24230;&#19981;&#31283;&#23450;&#30340;&#22522;&#26412;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bagging is an important technique for stabilizing machine learning models. In this paper, we derive a finite-sample guarantee on the stability of bagging for any model. Our result places no assumptions on the distribution of the data, on the properties of the base algorithm, or on the dimensionality of the covariates. Our guarantee applies to many variants of bagging and is optimal up to a constant. Empirical results validate our findings, showing that bagging successfully stabilizes even highly unstable base algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34892;&#20026;&#33391;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24517;&#35201;&#30340;&#20559;&#32622;&#21644;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#26029;&#26102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.04900</link><description>&lt;p&gt;
&#19968;&#31181;&#34892;&#20026;&#33391;&#22909;&#30340;&#22270;&#31070;&#32463;&#36817;&#20284;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Recipe for Well-behaved Graph Neural Approximations of Complex Dynamics. (arXiv:2301.04900v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34892;&#20026;&#33391;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24517;&#35201;&#30340;&#20559;&#32622;&#21644;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#26029;&#26102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#36817;&#20284;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#32570;&#20047;&#26126;&#30830;&#21407;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#19968;&#31867;&#30001;&#32593;&#32476;&#37051;&#25509;&#30697;&#38453;&#32806;&#21512;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#25551;&#36848;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#31038;&#20132;&#21644;&#31070;&#32463;&#31995;&#32479;&#65292;&#23646;&#20110;&#36825;&#31867;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#36825;&#31181;&#21160;&#21147;&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21253;&#25324;&#24517;&#35201;&#30340;&#20559;&#32622;&#21644;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#24378;&#35843;&#19982;&#38745;&#24577;&#30417;&#30563;&#23398;&#20064;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#25552;&#20513;&#22312;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#30340;&#32463;&#20856;&#20551;&#35774;&#20043;&#22806;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#25512;&#26029;&#26102;&#20272;&#35745;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#29992;&#30340;&#31354;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#21508;&#31181;&#22797;&#26434;&#32593;&#32476;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approximations of ordinary differential equations offer a promising alternative to classical methods in discovering a dynamical system model, particularly in complex systems lacking explicit first principles. This paper focuses on a complex system whose dynamics is described with a system of ordinary differential equations, coupled via a network adjacency matrix. Numerous real-world systems, including financial, social, and neural systems, belong to this class of dynamical models. We propose essential elements for approximating such dynamical systems using neural networks, including necessary biases and an appropriate neural architecture. Emphasizing the differences from static supervised learning, we advocate for evaluating generalization beyond classical assumptions of statistical learning theory. To estimate confidence in prediction during inference time, we introduce a dedicated null model. By studying various complex network dynamics, we demonstrate the neural network'
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24402;&#30340;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#27979;&#26469;&#20272;&#35745;&#36229;&#26631;&#27010;&#29575;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.09821</link><description>&lt;p&gt;
&#22522;&#20110;&#22238;&#24402;&#30340;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#29992;&#20110;&#26174;&#33879;&#27874;&#39640;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exceedance Probability Forecasting via Regression for Significant Wave Height Prediction. (arXiv:2206.09821v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24402;&#30340;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#27979;&#26469;&#20272;&#35745;&#36229;&#26631;&#27010;&#29575;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#27874;&#39640;&#39044;&#27979;&#26159;&#28023;&#27915;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#23545;&#20110;&#20272;&#35745;&#27874;&#33021;&#20135;&#29983;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#27492;&#22806;&#65292;&#21450;&#26102;&#39044;&#27979;&#22823;&#28010;&#30340;&#21040;&#26469;&#23545;&#20110;&#30830;&#20445;&#33322;&#28023;&#20316;&#19994;&#30340;&#23433;&#20840;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#30340;&#26497;&#31471;&#20540;&#20316;&#20026;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#20272;&#35745;&#26174;&#33879;&#27874;&#39640;&#23558;&#36229;&#36807;&#39044;&#23450;&#20041;&#38408;&#20540;&#30340;&#27010;&#29575;&#12290;&#36890;&#24120;&#20351;&#29992;&#27010;&#29575;&#20108;&#20998;&#31867;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26410;&#26469;&#35266;&#27979;&#30340;&#39044;&#27979;&#26469;&#26681;&#25454;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#20272;&#35745;&#36229;&#26631;&#27010;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#21152;&#25343;&#22823;&#21704;&#21033;&#27861;&#20811;&#26031;&#28023;&#23736;&#30340;&#28014;&#26631;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant wave height forecasting is a key problem in ocean data analytics. Predicting the significant wave height is crucial for estimating the energy production from waves. Moreover, the timely prediction of large waves is important to ensure the safety of maritime operations, e.g. passage of vessels. We frame the task of predicting extreme values of significant wave height as an exceedance probability forecasting problem. Accordingly, we aim at estimating the probability that the significant wave height will exceed a predefined threshold. This task is usually solved using a probabilistic binary classification model. Instead, we propose a novel approach based on a forecasting model. The method leverages the forecasts for the upcoming observations to estimate the exceedance probability according to the cumulative distribution function. We carried out experiments using data from a buoy placed in the coast of Halifax, Canada. The results suggest that the proposed methodology is better
&lt;/p&gt;</description></item></channel></rss>