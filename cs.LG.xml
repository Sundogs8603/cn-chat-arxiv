<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#38543;&#26426;&#24615;&#26469;&#36827;&#34892;&#20302;&#24102;&#23485;&#20998;&#25955;&#24335;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#27599;&#21488;&#26426;&#22120;&#29983;&#25104;&#19981;&#21516;&#30340;&#38543;&#26426;&#25200;&#21160;&#26469;&#26356;&#26032;&#27599;&#20010;&#27169;&#22411;&#65292;&#20174;&#32780;&#20855;&#26377;&#39640;&#24230;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.10015</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#23383;&#33410;&#65288;&#27599;&#26799;&#24230;&#65289;&#65306;&#20851;&#20110;&#20351;&#29992;&#20849;&#20139;&#38543;&#26426;&#24615;&#36827;&#34892;&#20302;&#24102;&#23485;&#20998;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#31616;&#35201;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness. (arXiv:2306.10015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#38543;&#26426;&#24615;&#26469;&#36827;&#34892;&#20302;&#24102;&#23485;&#20998;&#25955;&#24335;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#27599;&#21488;&#26426;&#22120;&#29983;&#25104;&#19981;&#21516;&#30340;&#38543;&#26426;&#25200;&#21160;&#26469;&#26356;&#26032;&#27599;&#20010;&#27169;&#22411;&#65292;&#20174;&#32780;&#20855;&#26377;&#39640;&#24230;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#21463;&#21040;&#26799;&#24230;&#20132;&#25442;&#30340;&#36890;&#20449;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#38543;&#26426;&#24615;&#26469;&#36827;&#34892;&#20302;&#24102;&#23485;&#30340;&#20998;&#25955;&#24335;&#24494;&#35843;&#65292;&#25193;&#23637;&#20102;Malladi&#31561;&#20154;2023&#24180;&#30340;&#26368;&#26032;&#24037;&#20316;&#12290;&#35813;&#26041;&#27861;&#26159;&#23545;&#20855;&#26377;&#35760;&#24518;&#25928;&#29575;&#30340;&#21516;&#26102;&#25200;&#21160;&#38543;&#26426;&#36924;&#36817;&#65288;SPSA&#65289;&#30340;&#33258;&#28982;&#20998;&#25955;&#24335;&#25193;&#23637;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#27599;&#21488;&#26426;&#22120;&#37117;&#20351;&#29992;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#65288;RNG&#65289;&#26469;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#23616;&#37096;&#21487;&#37325;&#29616;&#30340;&#25200;&#21160;&#65292;&#24182;&#35745;&#31639;&#21644;&#20132;&#25442;&#26631;&#37327;&#25237;&#24433;&#26799;&#24230;&#65292;&#28982;&#21518;&#29992;&#20110;&#26356;&#26032;&#27599;&#20010;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#65288;&#26426;&#22120;&#65292;&#26679;&#26412;&#65289;&#26631;&#35782;&#31526;&#29992;&#20316;&#38543;&#26426;&#31181;&#23376;&#65292;&#27599;&#20010;&#27169;&#22411;&#21487;&#20197;&#37325;&#26032;&#29983;&#25104;&#24444;&#27492;&#30340;&#25200;&#21160;&#12290;&#30001;&#20110;&#26426;&#22120;&#21482;&#20132;&#25442;&#21333;&#23383;&#33410;&#30340;&#25237;&#24433;&#26799;&#24230;&#65292;&#22240;&#27492;&#36825;&#26159;&#39640;&#24230;&#36890;&#20449;&#25928;&#29575;&#30340;&#12290;&#27492;&#22806;&#65292;&#36824;&#23384;&#22312;&#28508;&#22312;&#30340;&#38544;&#31169;&#20248;&#21183;&#65292;&#22240;&#20026;&#25237;&#24433;&#26799;&#24230;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#35745;&#31639;&#65292;&#32780;&#27169;&#22411;&#20174;&#19981;&#35775;&#38382;&#20854;&#20182;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20855;&#26377;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#20063;&#22312;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#23637;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model training in distributed settings is limited by the communication cost of gradient exchanges. In this short note, we extend recent work from Malladi et al. (2023), using shared randomness to perform distributed fine-tuning with low bandwidth. The method is a natural decentralized extension of memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA). Each iteration, each machine seeds a Random Number Generator (RNG) to perform local reproducible perturbations on model weights and calculate and exchange scalar projected gradients, which are then used to update each model. By using a (machine, sample) identifier as the random seed, each model can regenerate one another's perturbations. As machines only exchange single-byte projected gradients, this is highly communication efficient. There are also potential privacy benefits, as projected gradients may be calculated on different training data, and models never access the other's data. Our approach not only d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#27861;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;CLIP2Protect&#65292;&#20351;&#29992;&#23545;&#25239;&#24615;&#28508;&#22312;&#25628;&#32034;&#32467;&#21512;&#25991;&#26412;&#24341;&#23548;&#21270;&#22918;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#38754;&#37096;&#22270;&#20687;&#65292;&#20174;&#32780;&#20445;&#25252;&#38754;&#37096;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.10008</link><description>&lt;p&gt;
CLIP2Protect&#65306;&#20351;&#29992;&#23545;&#25239;&#24615;&#28508;&#22312;&#25628;&#32034;&#30340;&#25991;&#26412;&#24341;&#23548;&#21270;&#22918;&#26469;&#20445;&#25252;&#38754;&#37096;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search. (arXiv:2306.10008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10008
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#27861;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;CLIP2Protect&#65292;&#20351;&#29992;&#23545;&#25239;&#24615;&#28508;&#22312;&#25628;&#32034;&#32467;&#21512;&#25991;&#26412;&#24341;&#23548;&#21270;&#22918;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#38754;&#37096;&#22270;&#20687;&#65292;&#20174;&#32780;&#20445;&#25252;&#38754;&#37096;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#30340;&#25104;&#21151;&#24050;&#32463;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#21551;&#29992;&#26410;&#25480;&#26435;&#30340;&#29992;&#25143;&#36319;&#36394;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#22686;&#24378;&#26041;&#27861;&#26080;&#27861;&#29983;&#25104;&#33258;&#28982;&#20027;&#20041;&#22270;&#20687;&#65292;&#26082;&#33021;&#20445;&#25252;&#38754;&#37096;&#38544;&#31169;&#21448;&#19981;&#20250;&#25439;&#23475;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#37319;&#29992;&#20004;&#27493;&#27861;&#65292;&#20381;&#38752;&#22312;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#20302;&#32500;&#27969;&#24418;&#20013;&#25214;&#21040;&#23545;&#25239;&#24615;&#28508;&#22312;&#32534;&#30721;&#12290;&#31532;&#19968;&#27493;&#23558;&#32473;&#23450;&#30340;&#38754;&#37096;&#22270;&#20687;&#21453;&#28436;&#25104;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#32534;&#30721;&#65292;&#24182;&#24494;&#35843;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20174;&#20854;&#28508;&#22312;&#20195;&#30721;&#20934;&#30830;&#22320;&#37325;&#26500;&#32473;&#23450;&#30340;&#22270;&#20687;&#12290;&#36825;&#19968;&#27493;&#20135;&#29983;&#20102;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#65292;&#26377;&#21161;&#20110;&#29983;&#25104;&#31867;&#20284;&#20110;&#32473;&#23450;&#36523;&#20221;&#30340;&#39640;&#36136;&#37327;&#38754;&#37096;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#29992;&#25143;&#23450;&#20041;&#30340;&#21270;&#22918;&#25991;&#26412;&#25552;&#31034;&#21644;&#20445;&#25345;&#36523;&#20221;&#30340;&#35268;&#33539;&#21270;&#26469;&#25351;&#23548;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23547;&#25214;&#23545;&#25239;&#24615;&#20195;&#30721;&#30340;&#25628;&#32034;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#21270;&#22918;&#30340;&#38754;&#37096;&#22270;&#20687;&#65292;&#21487;&#26377;&#25928;&#22320;&#20445;&#25252;&#38754;&#37096;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of deep learning based face recognition systems has given rise to serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Existing methods for enhancing privacy fail to generate naturalistic images that can protect facial privacy without compromising user experience. We propose a novel two-step approach for facial privacy protection that relies on finding adversarial latent codes in the low-dimensional manifold of a pretrained generative model. The first step inverts the given face image into the latent space and finetunes the generative model to achieve an accurate reconstruction of the given image from its latent code. This step produces a good initialization, aiding the generation of high-quality faces that resemble the given identity. Subsequently, user-defined makeup text prompts and identity-preserving regularization are used to guide the search for adversarial codes in the latent space. Extensive experiments demons
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#24863;&#35273;&#36816;&#21160;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992; Transformer &#27169;&#22411;&#22312;&#35270;&#35273;&#34920;&#31034;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#36890;&#36807; 20,000 &#26465;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#20013;&#24615;&#33021;&#25552;&#39640; 2 &#20493;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.10007</link><description>&lt;p&gt;
&#20855;&#26377;&#24863;&#35273;&#36816;&#21160;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robot Learning with Sensorimotor Pre-training. (arXiv:2306.10007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#24863;&#35273;&#36816;&#21160;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992; Transformer &#27169;&#22411;&#22312;&#35270;&#35273;&#34920;&#31034;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#36890;&#36807; 20,000 &#26465;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#20013;&#24615;&#33021;&#25552;&#39640; 2 &#20493;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#24863;&#35273;&#36816;&#21160;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31216;&#20026; RPT&#65292;&#26159;&#19968;&#31181; Transformer&#65292;&#23427;&#23545;&#24863;&#35273;&#36816;&#21160;&#20196;&#29260;&#24207;&#21015;&#36827;&#34892;&#25805;&#20316;&#12290;&#32473;&#23450;&#19968;&#31995;&#21015;&#30456;&#26426;&#22270;&#20687;&#12289;&#26412;&#20307;&#24863;&#35273;&#26426;&#22120;&#20154;&#29366;&#24577;&#21644;&#36807;&#21435;&#30340;&#21160;&#20316;&#65292;&#25105;&#20204;&#23558;&#20132;&#38169;&#30340;&#24207;&#21015;&#32534;&#30721;&#20026;&#20196;&#29260;&#65292;&#25513;&#27169;&#20986;&#38543;&#26426;&#23376;&#38598;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#25513;&#27169;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#20551;&#35774;&#22914;&#26524;&#26426;&#22120;&#20154;&#33021;&#22815;&#39044;&#27979;&#32570;&#22833;&#30340;&#20869;&#23481;&#65292;&#23427;&#24050;&#32463;&#33719;&#24471;&#20102;&#19968;&#20010;&#21487;&#20197;&#20351;&#20854;&#34892;&#21160;&#30340;&#29289;&#29702;&#19990;&#30028;&#30340;&#33391;&#22909;&#27169;&#22411;&#12290;RPT &#30340;&#35774;&#35745;&#26159;&#22312;&#28508;&#22312;&#30340;&#35270;&#35273;&#34920;&#31034;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#39044;&#27979;&#21464;&#24471;&#21487;&#34892;&#65292;&#33021;&#22815;&#23454;&#29616; 10 &#20493;&#30340;&#27169;&#22411;&#25193;&#23637;&#65292;&#24182;&#33021;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#27599;&#31186; 10 &#27425;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#36816;&#21160;&#35268;&#21010;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#25235;&#21462;&#31639;&#27861;&#65292;&#25910;&#38598;&#20102; 9 &#20010;&#26376;&#20869;&#30340; 20,000 &#26465;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#22987;&#32456;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#22312;&#22534;&#31215;&#26041;&#22359;&#20219;&#21153;&#20013;&#23548;&#33268; 2 &#20493;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a self-supervised sensorimotor pre-training approach for robotics. Our model, called RPT, is a Transformer that operates on sequences of sensorimotor tokens. Given a sequence of camera images, proprioceptive robot states, and past actions, we encode the interleaved sequence into tokens, mask out a random subset, and train a model to predict the masked-out content. We hypothesize that if the robot can predict the missing content it has acquired a good model of the physical world that can enable it to act. RPT is designed to operate on latent visual representations which makes prediction tractable, enables scaling to 10x larger models, and 10 Hz inference on a real robot. To evaluate our approach, we collect a dataset of 20,000 real-world trajectories over 9 months using a combination of motion planning and model-based grasping algorithms. We find that pre-training on this data consistently outperforms training from scratch, leads to 2x improvements in the block stacking task,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#23398;&#20064;&#30340;&#21160;&#30011;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25110;&#35821;&#38899;&#36755;&#20837;&#65292;&#23454;&#29616;&#22522;&#20110;&#30495;&#23454;&#21160;&#20316;&#34920;&#28436;&#30340;&#38754;&#37096;&#21160;&#30011;&#65292;&#24182;&#19988;&#21487;&#20197;&#19981;&#21516;&#31243;&#24230;&#22320;&#23398;&#20064;&#24182;&#21512;&#25104;&#19981;&#21516;&#30340;&#34920;&#28436;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2306.10006</link><description>&lt;p&gt;
&#22522;&#20110;&#29616;&#23454;&#34920;&#28436;&#30340;&#38754;&#37096;&#21160;&#30011;&#39118;&#26684;&#24863;&#30693;&#38750;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances. (arXiv:2306.10006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#23398;&#20064;&#30340;&#21160;&#30011;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25110;&#35821;&#38899;&#36755;&#20837;&#65292;&#23454;&#29616;&#22522;&#20110;&#30495;&#23454;&#21160;&#20316;&#34920;&#28436;&#30340;&#38754;&#37096;&#21160;&#30011;&#65292;&#24182;&#19988;&#21487;&#20197;&#19981;&#21516;&#31243;&#24230;&#22320;&#23398;&#20064;&#24182;&#21512;&#25104;&#19981;&#21516;&#30340;&#34920;&#28436;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28151;&#21512;&#24418;&#29366;&#20960;&#20309;&#12289;&#21160;&#24577;&#32441;&#29702;&#21644;&#31070;&#32463;&#28210;&#26579;&#65292;&#29992;&#20110;&#20174;&#30495;&#23454;&#21160;&#20316;&#34920;&#28436;&#20013;&#39537;&#21160;&#38754;&#37096;&#27169;&#22411;&#30340;&#25991;&#26412;/&#35821;&#38899;&#21160;&#30011;&#12290;&#36890;&#36807;&#35757;&#32451;&#21253;&#25324;&#24418;&#29366;&#21644;&#32441;&#29702;&#30340;VAE&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#20197;&#31934;&#30830;&#25429;&#25417;&#21644;&#36924;&#30495;&#21512;&#25104;&#28508;&#22312;&#29305;&#24449;&#21521;&#37327;&#20013;&#30340;&#38754;&#37096;&#34920;&#24773;&#12290;&#25105;&#20204;&#30340;&#21160;&#30011;&#26041;&#27861;&#22522;&#20110;&#26465;&#20214;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#25991;&#26412;&#25110;&#35821;&#38899;&#36716;&#25442;&#20026;&#19968;&#31995;&#21015;&#21160;&#30011;&#21442;&#25968;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21160;&#30011;&#27169;&#22411;&#20197;&#38750;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#21306;&#20998;&#21644;&#21512;&#25104;&#19981;&#21516;&#30340;&#34920;&#28436;&#39118;&#26684;&#65292;&#21482;&#38656;&#35201;&#29992;&#20110;&#25551;&#36848;&#35757;&#32451;&#24207;&#21015;&#20869;&#23481;&#30340;&#35821;&#38899;&#26631;&#31614;&#12290;&#20026;&#20102;&#23454;&#29616;&#36924;&#30495;&#30340;&#23454;&#26102;&#28210;&#26579;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;U-Net&#65292;&#36890;&#36807;&#35745;&#31639;&#25913;&#36827;&#30340;&#20687;&#32032;&#39068;&#33394;&#21644;&#21069;&#26223;&#36974;&#32617;&#26469;&#25913;&#21892;&#26629;&#26684;&#21270;&#28210;&#26579;&#12290;&#25105;&#20204;&#23450;&#24615;/&#23450;&#37327;&#22320;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#26368;&#36817;&#30340;&#22836;&#37096;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35780;&#20272;&#24863;&#30693;&#28210;&#26579;/&#21160;&#30011;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach for text/speech-driven animation of a photo-realistic head model based on blend-shape geometry, dynamic textures, and neural rendering. Training a VAE for geometry and texture yields a parametric model for accurate capturing and realistic synthesis of facial expressions from a latent feature vector. Our animation method is based on a conditional CNN that transforms text or speech into a sequence of animation parameters. In contrast to previous approaches, our animation model learns disentangling/synthesizing different acting-styles in an unsupervised manner, requiring only phonetic labels that describe the content of training sequences. For realistic real-time rendering, we train a U-Net that refines rasterization-based renderings by computing improved pixel colors and a foreground matte. We compare our framework qualitatively/quantitatively against recent methods for head modeling as well as facial animation and evaluate the perceived rendering/ani
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20381;&#36182;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#23398;&#20064;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09998</link><description>&lt;p&gt;
SLACK: &#20919;&#21551;&#21160;&#21644;KL&#27491;&#21017;&#21270;&#30340;&#31283;&#23450;&#25968;&#25454;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SLACK: Stable Learning of Augmentations with Cold-start and KL regularization. (arXiv:2306.09998v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20381;&#36182;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#23398;&#20064;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21069;&#25552;&#26159;&#35201;&#31934;&#24515;&#36873;&#25321;&#19968;&#32452;&#21464;&#25442;&#12290;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#26088;&#22312;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#19968;&#20123;&#20808;&#21069;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20381;&#36182;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#23398;&#20064;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is known to improve the generalization capabilities of neural networks, provided that the set of transformations is chosen with care, a selection often performed manually. Automatic data augmentation aims at automating this process. However, most recent approaches still rely on some prior information; they start from a small pool of manually-selected default transformations that are either used to pretrain the network or forced to be part of the policy learned by the automatic data augmentation algorithm. In this paper, we propose to directly learn the augmentation policy without leveraging such prior knowledge. The resulting bilevel optimization problem becomes more challenging due to the larger search space and the inherent instability of bilevel optimization algorithms. To mitigate these issues (i) we follow a successive cold-start strategy with a Kullback-Leibler regularization, and (ii) we parameterize magnitudes as continuous distributions. Our approach leads to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPbRL&#30340;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#20248;&#21270;&#24182;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09995</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness in Preference-based Reinforcement Learning. (arXiv:2306.09995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPbRL&#30340;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#20248;&#21270;&#24182;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#30446;&#26631;&#24773;&#20917;&#19979;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;(PbRL)&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35774;&#35745;&#25511;&#21046;&#31574;&#30053;&#65292;&#26082;&#33021;&#22815;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#21448;&#33021;&#22815;&#20844;&#24179;&#22320;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;(FPbRL)&#26041;&#27861;&#12290;FPbRL&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#26032;&#30340;&#31119;&#21033;&#20559;&#22909;&#32780;&#19981;&#26159;PbRL&#20013;&#30340;&#22522;&#20110;&#22870;&#21169;&#30340;&#20559;&#22909;&#26469;&#23398;&#20064;&#19982;&#22810;&#30446;&#26631;&#20851;&#32852;&#30340;&#21521;&#37327;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#29615;&#22659;&#19978;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;FPbRL&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#21644;&#20844;&#24179;&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the issue of fairness in preference-based reinforcement learning (PbRL) in the presence of multiple objectives. The main objective is to design control policies that can optimize multiple objectives while treating each objective fairly. Toward this objective, we design a new fairness-induced preference-based reinforcement learning or FPbRL. The main idea of FPbRL is to learn vector reward functions associated with multiple objectives via new welfare-based preferences rather than reward-based preference in PbRL, coupled with policy learning via maximizing a generalized Gini welfare function. Finally, we provide experiment studies on three different environments to show that the proposed FPbRL approach can achieve both efficiency and equity for learning effective and fair policies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#38598;&#25104;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;IEEE Data Port &#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26089;&#26399;&#20934;&#30830;&#35786;&#26029;&#24515;&#33039;&#38382;&#39064;&#65292;&#25552;&#39640;&#29983;&#21629;&#20960;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09989</link><description>&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#30340;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ensemble Framework for Cardiovascular Disease Prediction. (arXiv:2306.09989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#38598;&#25104;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;IEEE Data Port &#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26089;&#26399;&#20934;&#30830;&#35786;&#26029;&#24515;&#33039;&#38382;&#39064;&#65292;&#25552;&#39640;&#29983;&#21629;&#20960;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#30149;&#26159;&#20840;&#29699;&#38750;&#20256;&#26579;&#24615;&#21644;&#32516;&#40664;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#24515;&#33039;&#30142;&#30149;&#25110;&#24515;&#34880;&#31649;&#30142;&#30149;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#20896;&#29366;&#21160;&#33033;&#24515;&#33039;&#30149;&#12289;&#24515;&#21147;&#34928;&#31469;&#12289;&#20808;&#22825;&#24615;&#24515;&#33039;&#30149;&#21644;&#24515;&#32908;&#30149;&#12290;&#26089;&#26399;&#20934;&#30830;&#35786;&#26029;&#24515;&#33039;&#30142;&#30149;&#23545;&#20110;&#36991;&#20813;&#36827;&#19968;&#27493;&#25439;&#20260;&#24182;&#25405;&#25937;&#24739;&#32773;&#30340;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#21464;&#25104;&#21361;&#26426;&#20043;&#21069;&#39044;&#27979;&#20854;&#21457;&#29983;&#30340;&#31995;&#32479;&#12290;&#26426;&#22120;&#23398;&#20064;&#24341;&#36215;&#20102;&#21307;&#23398;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#12290;&#38024;&#23545;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#65292;&#30740;&#31350;&#20154;&#21592;&#23454;&#26045;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#26041;&#27861;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;IEEE Data Port&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32447;&#21487;&#29992;&#20110;&#24515;&#34880;&#31649;&#30142;&#30149;&#20010;&#20307;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#30001;&#21256;&#29273;&#21033;&#65292;&#20811;&#37324;&#22827;&#20848;&#65292;&#38271;&#28393;VA&#65292;&#29790;&#22763;&#21644;Statlog&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#20855;&#26377;&#37325;&#35201;&#29305;&#24449;&#65292;&#22914;&#26368;&#22823;&#24515;&#29575;
&lt;/p&gt;
&lt;p&gt;
Heart disease is the major cause of non-communicable and silent death worldwide. Heart diseases or cardiovascular diseases are classified into four types: coronary heart disease, heart failure, congenital heart disease, and cardiomyopathy. It is vital to diagnose heart disease early and accurately in order to avoid further injury and save patients' lives. As a result, we need a system that can predict cardiovascular disease before it becomes a critical situation. Machine learning has piqued the interest of researchers in the field of medical sciences. For heart disease prediction, researchers implement a variety of machine learning methods and approaches. In this work, to the best of our knowledge, we have used the dataset from IEEE Data Port which is one of the online available largest datasets for cardiovascular diseases individuals. The dataset isa combination of Hungarian, Cleveland, Long Beach VA, Switzerland &amp; Statlog datasets with important features such as Maximum Heart Rate Ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#27531;&#24046;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#23558;&#28023;&#27915;&#28201;&#24230;&#35266;&#27979;&#36716;&#25442;&#65292;&#25552;&#39640;&#20102;&#20998;&#36776;&#29575;&#21644;&#35206;&#30422;&#20113;&#38388;&#38553;&#65292;&#26377;&#25928;&#35780;&#20272;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09987</link><description>&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#21367;&#31215;&#27531;&#24046;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#36716;&#25442;&#28023;&#27915;&#28201;&#24230;&#35266;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transforming Observations of Ocean Temperature with a Deep Convolutional Residual Regressive Neural Network. (arXiv:2306.09987v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#27531;&#24046;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#23558;&#28023;&#27915;&#28201;&#24230;&#35266;&#27979;&#36716;&#25442;&#65292;&#25552;&#39640;&#20102;&#20998;&#36776;&#29575;&#21644;&#35206;&#30422;&#20113;&#38388;&#38553;&#65292;&#26377;&#25928;&#35780;&#20272;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#34920;&#38754;&#28201;&#24230; (SST) &#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#27668;&#20505;&#21464;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#27979;&#12289;&#36965;&#24863;&#25110;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#36827;&#34892;&#27979;&#37327;&#12290;&#26412;&#25991;&#21033;&#29992;20&#19990;&#32426;&#26411;&#21644;21&#19990;&#32426;&#21021;&#30340;&#19968;&#20123;&#30456;&#20851;&#25216;&#26415;&#36827;&#27493;&#65292;&#24198;&#31069;&#20102;SST&#30417;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#29616;&#26377;&#30340;&#27700;&#24490;&#29615;&#35266;&#27979;&#26694;&#26550;Flux to Flow&#65288;F2F&#65289;&#65292;&#23558;AMSR-E&#21644;MODIS&#27785;&#31215;&#29289;&#34701;&#21512;&#25104;&#19968;&#20010;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#20135;&#21697;&#65292;&#20197;&#25429;&#25417;&#26799;&#24230;&#24182;&#22635;&#34917;&#21542;&#21017;&#26080;&#27861;&#33719;&#24471;&#30340;&#20113;&#38388;&#38553;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#38480;&#21046;&#22312;&#28145;&#24230;&#21367;&#31215;&#27531;&#24046;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#19978;&#65292;&#20351;&#29992;2010&#24180;12&#20010;&#26376;SST&#30340;&#19977;&#32452;&#24555;&#29031;&#26469;&#34913;&#37327;&#24615;&#33021;&#21644;&#25104;&#21151;&#24230;&#12290;&#24179;&#21488;&#30340;&#24615;&#33021;&#21644;&#36825;&#31181;&#26041;&#27861;&#30340;&#25104;&#21151;&#26159;&#36890;&#36807;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#25351;&#26631;&#26469;&#35780;&#20272;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sea surface temperature (SST) is an essential climate variable that can be measured via ground truth, remote sensing, or hybrid model methodologies. Here, we celebrate SST surveillance progress via the application of a few relevant technological advances from the late 20th and early 21st century. We further develop our existing water cycle observation framework, Flux to Flow (F2F), to fuse AMSR-E and MODIS into a higher resolution product with the goal of capturing gradients and filling cloud gaps that are otherwise unavailable. Our neural network architecture is constrained to a deep convolutional residual regressive neural network. We utilize three snapshots of twelve monthly SST measurements in 2010 as measured by the passive microwave radiometer AMSR-E, the visible and infrared monitoring MODIS instrument, and the in situ Argo dataset ISAS. The performance of the platform and success of this approach is evaluated using the root mean squared error (RMSE) metric. We determine that th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.09983</link><description>&lt;p&gt;
&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#25110;&#20915;&#31574;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#20154;&#33021;&#21147;&#65292;&#37027;&#20040;&#25105;&#20204;&#35813;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#20195;&#29702;&#20250;&#20135;&#29983;&#20559;&#24046;? &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21069;&#25552;&#26159;&#65292;&#34429;&#28982;&#35780;&#20272;&#36229;&#20154;&#20915;&#31574;&#30340;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;&#27169;&#22411;&#30340;&#20915;&#31574;&#26410;&#33021;&#28385;&#36275;&#26576;&#20123;&#36923;&#36753;&#19978;&#12289;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21457;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#30001;&#20110;&#36229;&#20154;&#27169;&#22411;&#33021;&#21147;&#25110;&#20854;&#20182;&#32570;&#20047;&#22522;&#26412;&#20107;&#23454;&#32780;&#38590;&#20197;&#35780;&#20272;&#65306;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#12289;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#20316;&#20986;&#27861;&#24459;&#21028;&#26029;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26080;&#35770;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;(&#21487;&#33021;&#26159;&#36229;&#20154;&#30340;)&#65292;&#25105;&#20204;&#37117;&#33021;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65306;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#32473;&#20986;&#23545;&#23616;&#20013;&#26827;&#23376;&#30456;&#23545;&#20272;&#20540;&#30340;&#19981;&#21516;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#65292;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#25216;&#33021;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09980</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21019;&#24314;&#22810;&#32423;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating Multi-Level Skill Hierarchies in Reinforcement Learning. (arXiv:2306.09980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#65292;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#25216;&#33021;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#20160;&#20040;&#26679;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#23545;&#20110;&#33258;&#20027;&#20195;&#29702;&#20154;&#26159;&#26377;&#29992;&#30340;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#26469;&#25581;&#31034;&#22270;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#23618;&#27425;&#32467;&#26500;&#30340;&#27599;&#20010;&#23618;&#27425;&#19978;&#65292;&#25216;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#24191;&#27867;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is a useful skill hierarchy for an autonomous agent? We propose an answer based on the graphical structure of an agent's interaction with its environment. Our approach uses hierarchical graph partitioning to expose the structure of the graph at varying timescales, producing a skill hierarchy with multiple levels of abstraction. At each level of the hierarchy, skills move the agent between regions of the state space that are well connected within themselves but weakly connected to each other. We illustrate the utility of the proposed skill hierarchy in a wide variety of domains in the context of reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#30340;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#33719;&#24471;&#26368;&#20248;&#30340;&#38169;&#26631;&#29575;&#12290;&#22312;&#27809;&#26377;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#19982;&#27931;&#20234;&#24503;&#31639;&#27861;&#31867;&#20284;&#30340;&#29702;&#35770;&#20445;&#35777;.</title><link>http://arxiv.org/abs/2306.09977</link><description>&lt;p&gt;
&#24102;&#26377;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#23545;&#25239;&#40065;&#26834;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adversarially robust clustering with optimality guarantees. (arXiv:2306.09977v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#30340;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#33719;&#24471;&#26368;&#20248;&#30340;&#38169;&#26631;&#29575;&#12290;&#22312;&#27809;&#26377;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#19982;&#27931;&#20234;&#24503;&#31639;&#27861;&#31867;&#20284;&#30340;&#29702;&#35770;&#20445;&#35777;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23545;&#26469;&#33258;&#20122;&#39640;&#26031;&#28151;&#21512;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#21487;&#35777;&#26126;&#36798;&#21040;&#26368;&#20248;&#38169;&#26631;&#29575;&#30340;&#26041;&#27861;&#65292;&#22914;&#27931;&#20234;&#24503;&#31639;&#27861;&#65292;&#36890;&#24120;&#23481;&#26131;&#21463;&#21040;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#20284;&#20046;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#32858;&#31867;&#26041;&#27861;&#19981;&#30693;&#36947;&#26159;&#21542;&#28385;&#36275;&#26368;&#20248;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#20801;&#35768;&#20986;&#29616;&#23545;&#25239;&#24615;&#30340;&#24322;&#24120;&#20540;&#65292;&#20063;&#33021;&#33719;&#24471;&#26368;&#20248;&#30340;&#38169;&#26631;&#29575;&#12290;&#24403;&#28385;&#36275;&#24369;&#21021;&#22987;&#21270;&#26465;&#20214;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24120;&#25968;&#27425;&#36845;&#20195;&#20013;&#23454;&#29616;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;&#22312;&#27809;&#26377;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22266;&#23450;&#32500;&#24230;&#19978;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#19982;&#27931;&#20234;&#24503;&#31639;&#27861;&#31867;&#20284;&#12290;&#22312;&#21508;&#31181;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of clustering data points coming from sub-Gaussian mixtures. Existing methods that provably achieve the optimal mislabeling error, such as the Lloyd algorithm, are usually vulnerable to outliers. In contrast, clustering methods seemingly robust to adversarial perturbations are not known to satisfy the optimal statistical guarantees. We propose a simple algorithm that obtains the optimal mislabeling rate even when we allow adversarial outliers to be present. Our algorithm achieves the optimal error rate in constant iterations when a weak initialization condition is satisfied. In the absence of outliers, in fixed dimensions, our theoretical guarantees are similar to that of the Lloyd algorithm. Extensive experiments on various simulated data sets are conducted to support the theoretical guarantees of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36873;&#25321;&#24615;&#31070;&#32463;&#20803;&#20998;&#35010;&#26041;&#27861;&#65292;&#22686;&#24378;QNN&#30340;&#23481;&#38169;&#24615;&#65292;&#21487;&#20197;&#22312;&#21152;&#36895;&#22120;&#20013;&#35774;&#35745;&#36731;&#37327;&#32423;&#32416;&#38169;&#21333;&#20803;&#65292;&#30456;&#23545;&#20110;&#36873;&#25321;&#24615;&#19977;&#37325;&#27169;&#22359;&#20887;&#20313;&#20855;&#26377;&#26356;&#23567;&#30340;&#24320;&#38144;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#31867;&#20284;&#30340;&#25925;&#38556;&#23481;&#38169;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.09973</link><description>&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#24615;&#31070;&#32463;&#20803;&#20998;&#35010;&#22686;&#24378;QNN&#30340;&#23481;&#38169;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Fault Resilience of QNNs by Selective Neuron Splitting. (arXiv:2306.09973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36873;&#25321;&#24615;&#31070;&#32463;&#20803;&#20998;&#35010;&#26041;&#27861;&#65292;&#22686;&#24378;QNN&#30340;&#23481;&#38169;&#24615;&#65292;&#21487;&#20197;&#22312;&#21152;&#36895;&#22120;&#20013;&#35774;&#35745;&#36731;&#37327;&#32423;&#32416;&#38169;&#21333;&#20803;&#65292;&#30456;&#23545;&#20110;&#36873;&#25321;&#24615;&#19977;&#37325;&#27169;&#22359;&#20887;&#20313;&#20855;&#26377;&#26356;&#23567;&#30340;&#24320;&#38144;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#31867;&#20284;&#30340;&#25925;&#38556;&#23481;&#38169;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#20986;&#33394;&#24615;&#33021;&#23548;&#33268;&#23427;&#20204;&#22312;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#24471;&#21040;&#24212;&#29992;&#12290;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#31243;&#24207;&#20063;&#19981;&#20363;&#22806;&#65292;&#24182;&#23545;DNN&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#21487;&#38752;&#24615;&#35201;&#27714;&#12290;Quantized Neural Networks&#65288;QNN&#65289;&#24050;&#32463;&#20986;&#29616;&#65292;&#20197;&#24212;&#23545;DNN&#21152;&#36895;&#22120;&#30340;&#22797;&#26434;&#24615;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#26356;&#23481;&#26131;&#20986;&#29616;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#26368;&#36817;&#30340;&#20998;&#26512;&#23481;&#38169;&#35780;&#20272;&#26041;&#27861;&#29992;&#20110;QNN&#65292;&#22522;&#20110;&#31070;&#32463;&#20803;&#26131;&#25439;&#22240;&#23376;&#65288;NVF&#65289;&#35782;&#21035;&#20851;&#38190;&#31070;&#32463;&#20803;&#12290;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#35010;&#20851;&#38190;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#21152;&#36895;&#22120;&#20013;&#35774;&#35745;&#36731;&#37327;&#32423;&#32416;&#38169;&#21333;&#20803;&#65288;LCU&#65289;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35774;&#35745;&#20854;&#35745;&#31639;&#37096;&#20998;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19981;&#21516;&#30340;QNN&#21644;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#36873;&#25321;&#24615;&#19977;&#37325;&#27169;&#22359;&#20887;&#20313;&#65288;TMR&#65289;&#65292;&#25152;&#25552;&#20986;&#30340;&#25925;&#38556;&#20462;&#27491;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#23567;&#30340;&#24320;&#38144;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#31867;&#20284;&#30340;&#25925;&#38556;&#23481;&#38169;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The superior performance of Deep Neural Networks (DNNs) has led to their application in various aspects of human life. Safety-critical applications are no exception and impose rigorous reliability requirements on DNNs. Quantized Neural Networks (QNNs) have emerged to tackle the complexity of DNN accelerators, however, they are more prone to reliability issues.  In this paper, a recent analytical resilience assessment method is adapted for QNNs to identify critical neurons based on a Neuron Vulnerability Factor (NVF). Thereafter, a novel method for splitting the critical neurons is proposed that enables the design of a Lightweight Correction Unit (LCU) in the accelerator without redesigning its computational part.  The method is validated by experiments on different QNNs and datasets. The results demonstrate that the proposed method for correcting the faults has a twice smaller overhead than a selective Triple Modular Redundancy (TMR) while achieving a similar level of fault resiliency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#21644;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.09970</link><description>&lt;p&gt;
HePCo&#65306;&#29992;&#20110;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#25968;&#25454;&#24322;&#26500;&#25552;&#31034;&#21512;&#24182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning. (arXiv:2306.09970v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#21644;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#30340;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26381;&#21153;&#22120;&#19982;&#19968;&#32452;&#23458;&#25143;&#31471;&#36890;&#20449;&#65292;&#20197;&#36880;&#27493;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#12290;&#30001;&#20110;&#26469;&#33258;&#36830;&#32493;&#21644;&#32852;&#37030;&#23398;&#20064;&#35282;&#24230;&#30340;&#25361;&#25112;&#65292;&#27492;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21463;&#21040;&#20102;&#21152;&#21095;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#23384;&#20648;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36951;&#24536;&#21644;&#24322;&#26500;&#38382;&#39064;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#24320;&#38144;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#24182;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#26032;&#39062;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27492;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#24182;&#20445;&#25345;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on the important yet understudied problem of Continual Federated Learning (CFL), where a server communicates with a set of clients to incrementally learn new concepts over time without sharing or storing any data. The complexity of this problem is compounded by challenges from both the Continual and Federated Learning perspectives. Specifically, models trained in a CFL setup suffer from catastrophic forgetting which is exacerbated by data heterogeneity across clients. Existing attempts at this problem tend to impose large overheads on clients and communication channels or require access to stored data which renders them unsuitable for real-world use due to privacy. In this paper, we attempt to tackle forgetting and heterogeneity while minimizing overhead costs and without requiring access to any stored data. We achieve this by leveraging a prompting based approach (such that only prompts and classifier heads have to be communicated) and proposing a novel and lig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36827;&#21270;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#32852;&#31995;&#20197;&#21450;&#21487;&#33021;&#30340;&#21551;&#31034;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#33021;&#22815;&#25552;&#39640;&#25105;&#20204;&#23545;&#36827;&#21270;&#21644;&#36827;&#21270;&#31995;&#32479;&#20013;&#21453;&#39304;&#20316;&#29992;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.09961</link><description>&lt;p&gt;
&#23398;&#20064;&#30340;&#36827;&#21270;&#35770;&#65306;&#20174;&#33258;&#28982;&#36873;&#25321;&#21040;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The Evolution theory of Learning: From Natural Selection to Reinforcement Learning. (arXiv:2306.09961v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36827;&#21270;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#32852;&#31995;&#20197;&#21450;&#21487;&#33021;&#30340;&#21551;&#31034;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#33021;&#22815;&#25552;&#39640;&#25105;&#20204;&#23545;&#36827;&#21270;&#21644;&#36827;&#21270;&#31995;&#32479;&#20013;&#21453;&#39304;&#20316;&#29992;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#26159;&#22609;&#36896;&#25105;&#20204;&#23621;&#20303;&#30340;&#29983;&#29289;&#19990;&#30028;&#30340;&#22522;&#26412;&#36807;&#31243;&#65292;&#24378;&#21270;&#23398;&#20064;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#29992;&#20110;&#24320;&#21457;&#21487;&#20197;&#20174;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#36825;&#20004;&#20010;&#30475;&#20284;&#29420;&#31435;&#39046;&#22495;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#35777;&#25454;&#34920;&#26126;&#23427;&#20204;&#27604;&#20197;&#21069;&#24819;&#35937;&#30340;&#26356;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#26816;&#39564;&#20102;&#36825;&#20123;&#32852;&#31995;&#21450;&#20854;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#25552;&#21319;&#25105;&#20204;&#23545;&#20110;&#36827;&#21270;&#21644;&#21453;&#39304;&#22312;&#36827;&#21270;&#31995;&#32479;&#20013;&#20316;&#29992;&#30340;&#29702;&#35299;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolution is a fundamental process that shapes the biological world we inhabit, and reinforcement learning is a powerful tool used in artificial intelligence to develop intelligent agents that learn from their environment. In recent years, researchers have explored the connections between these two seemingly distinct fields, and have found compelling evidence that they are more closely related than previously thought. This paper examines these connections and their implications, highlighting the potential for reinforcement learning principles to enhance our understanding of evolution and the role of feedback in evolutionary systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#23618;ReLU&#32593;&#32476;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644;&#21512;&#39029;&#25439;&#22833;&#22788;&#29702;&#22122;&#22768;&#25968;&#25454;&#36827;&#34892;&#20108;&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#36890;&#36807;&#23545;&#24178;&#20928;&#25968;&#25454;&#20313;&#37327;&#30340;&#26465;&#20214;&#30340;&#30830;&#23450;&#65292;&#24471;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#32467;&#26524;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31070;&#32463;&#20803;&#21160;&#24577;&#21464;&#21270;&#20570;&#20986;&#31934;&#32454;&#25551;&#36848;&#65292;&#24182;&#21457;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2306.09955</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#39029;&#25439;&#22833;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#35757;&#32451;&#27973;&#23618;ReLU&#32593;&#32476;&#65306;&#25105;&#20204;&#20309;&#26102;&#36807;&#24230;&#25311;&#21512;&#19988;&#20854;&#26159;&#21542;&#33391;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?. (arXiv:2306.09955v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#23618;ReLU&#32593;&#32476;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644;&#21512;&#39029;&#25439;&#22833;&#22788;&#29702;&#22122;&#22768;&#25968;&#25454;&#36827;&#34892;&#20108;&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#36890;&#36807;&#23545;&#24178;&#20928;&#25968;&#25454;&#20313;&#37327;&#30340;&#26465;&#20214;&#30340;&#30830;&#23450;&#65292;&#24471;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#32467;&#26524;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31070;&#32463;&#20803;&#21160;&#24577;&#21464;&#21270;&#20570;&#20986;&#31934;&#32454;&#25551;&#36848;&#65292;&#24182;&#21457;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644;&#21512;&#39029;&#25439;&#22833;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20108;&#23618;ReLU&#32593;&#32476;&#22312;&#20108;&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#20102;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;&#20854;&#20013;&#30456;&#23545;&#36739;&#23567;&#27604;&#20363;&#30340;&#26631;&#31614;&#34987;&#25439;&#22351;&#25110;&#32763;&#36716;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#24178;&#20928;&#25968;&#25454;&#20313;&#37327;&#30340;&#26465;&#20214;&#65292;&#20135;&#29983;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#32467;&#26524;&#65306;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23558;&#36798;&#21040;&#38646;&#25439;&#22833;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#27979;&#35797;&#25968;&#25454;&#34987;&#27491;&#30830;&#20998;&#31867;&#65307;&#36807;&#25311;&#21512;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23558;&#36798;&#21040;&#38646;&#25439;&#22833;&#65292;&#20294;&#27979;&#35797;&#25968;&#25454;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#27010;&#29575;&#21463;&#21040;&#24120;&#25968;&#19979;&#38480;&#30340;&#32422;&#26463;&#65307;&#20197;&#21450;&#19981;&#36807;&#25311;&#21512;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24178;&#20928;&#30340;&#28857;&#21487;&#20197;&#36798;&#21040;&#38646;&#25439;&#22833;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#27979;&#35797;&#25968;&#25454;&#34987;&#27491;&#30830;&#20998;&#31867;&#65292;&#20294;&#26159;&#19981;&#24178;&#20928;&#30340;&#28857;&#26080;&#27861;&#20570;&#20986;&#21516;&#26679;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#31070;&#32463;&#20803;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#21464;&#21270;&#30340;&#19968;&#31181;&#31934;&#32454;&#25551;&#36848;&#65292;&#24182;&#25581;&#31034;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65306;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#20013;&#65292;&#24178;&#20928;&#28857;&#25509;&#36817;&#36798;&#21040;&#38646;&#25439;&#22833;&#65292;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#20013;&#65292;&#24178;&#20928;&#28857;&#20250;&#25391;&#33633;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. In particular, we consider linearly separable data for which a relatively small proportion of labels are corrupted or flipped. We identify conditions on the margin of the clean data that give rise to three distinct training outcomes: benign overfitting, in which zero loss is achieved and with high probability test data is classified correctly; overfitting, in which zero loss is achieved but test data is misclassified with probability lower bounded by a constant; and non-overfitting, in which clean points, but not corrupt points, achieve zero loss and again with high probability test data is classified correctly. Our analysis provides a fine-grained description of the dynamics of neurons throughout training and reveals two distinct phases: in the first phase clean points achieve close to zero loss, in the second phase clean points oscillate on the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#21644;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#21457;&#29616;&#19981;&#24517;&#35201;&#37319;&#29992;&#39640;&#25104;&#26412;&#30340;&#24378;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#26469;&#32531;&#35299;&#23545;&#25239;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.09951</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#24378;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26469;&#31649;&#29702;&#23545;&#25239;&#25915;&#20987;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
You Don't Need Robust Machine Learning to Manage Adversarial Attack Risks. (arXiv:2306.09951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#21644;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#21457;&#29616;&#19981;&#24517;&#35201;&#37319;&#29992;&#39640;&#25104;&#26412;&#30340;&#24378;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#26469;&#32531;&#35299;&#23545;&#25239;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24050;&#25104;&#20026;&#31038;&#21306;&#20869;&#26085;&#30410;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#33021;&#22815;&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#35980;&#20284;&#26080;&#20851;&#30340;&#26356;&#25913;&#26469;&#30772;&#22351;&#27169;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#38169;&#35823;&#39044;&#27979;&#30340;&#33021;&#21147;&#20196;&#20154;&#38663;&#24778;&#65292;&#32780;&#25105;&#20204;&#22312;&#26500;&#24314;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#26041;&#38754;&#30340;&#25104;&#25928;&#20063;&#19981;&#23481;&#20048;&#35266;&#12290;&#29616;&#26377;&#30740;&#31350;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#32531;&#35299;&#25514;&#26045;&#24102;&#26469;&#20102;&#24456;&#39640;&#30340;&#25104;&#26412;&#65292;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23384;&#22312;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#21487;&#20197;&#36991;&#20813;&#36825;&#31181;&#39118;&#38505;&#26102;&#65292;&#36825;&#26679;&#30340;&#26435;&#34913;&#21487;&#33021;&#24182;&#19981;&#24517;&#35201;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30524;&#20809;&#20851;&#27880;&#23454;&#36341;&#20013;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#65292;&#29983;&#20135;&#37096;&#32626;&#30340;&#39118;&#38505;&#20197;&#21450;&#31649;&#29702;&#36825;&#20123;&#39118;&#38505;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#35768;&#22810;AML&#23041;&#32961;&#19981;&#36275;&#20197;&#35777;&#26126;&#36825;&#31181;&#25104;&#26412;&#21644;&#26435;&#34913;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of modern machine learning (ML) models has become an increasing concern within the community. The ability to subvert a model into making errant predictions using seemingly inconsequential changes to input is startling, as is our lack of success in building models robust to this concern. Existing research shows progress, but current mitigations come with a high cost and simultaneously reduce the model's accuracy. However, such trade-offs may not be necessary when other design choices could subvert the risk. In this survey we review the current literature on attacks and their real-world occurrences, or limited evidence thereof, to critically evaluate the real-world risks of adversarial machine learning (AML) for the average entity. This is done with an eye toward how one would then mitigate these attacks in practice, the risks for production deployment, and how those risks could be managed. In doing so we elucidate that many AML threats do not warrant the cost and trade-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39640;&#25928;&#20998;&#23618;&#32858;&#31867;&#31639;&#27861;&#65292;&#38024;&#23545;&#20219;&#20309;&#20855;&#26377;&#28165;&#26224;&#32858;&#31867;&#32467;&#26500;&#30340;&#36755;&#20837;&#22270;&#65292;&#20854;&#33021;&#22312;&#36817;&#32447;&#24615;&#26102;&#38388;&#20869;&#29983;&#25104;&#19968;&#20010;&#20851;&#20110;Dasgupta&#20195;&#20215;&#20989;&#25968;&#30340;O&#65288;1&#65289;&#36817;&#20284;&#30340;HC&#26641;&#65292;&#24615;&#33021;&#20248;&#20110;&#20808;&#21069;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.09950</link><description>&lt;p&gt;
&#38024;&#23545;&#33391;&#22909;&#32858;&#31867;&#30340;&#36817;&#26368;&#20248;&#20998;&#23618;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nearly-Optimal Hierarchical Clustering for Well-Clustered Graphs. (arXiv:2306.09950v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39640;&#25928;&#20998;&#23618;&#32858;&#31867;&#31639;&#27861;&#65292;&#38024;&#23545;&#20219;&#20309;&#20855;&#26377;&#28165;&#26224;&#32858;&#31867;&#32467;&#26500;&#30340;&#36755;&#20837;&#22270;&#65292;&#20854;&#33021;&#22312;&#36817;&#32447;&#24615;&#26102;&#38388;&#20869;&#29983;&#25104;&#19968;&#20010;&#20851;&#20110;Dasgupta&#20195;&#20215;&#20989;&#25968;&#30340;O&#65288;1&#65289;&#36817;&#20284;&#30340;HC&#26641;&#65292;&#24615;&#33021;&#20248;&#20110;&#20808;&#21069;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;Dasgupta&#20195;&#20215;&#20989;&#25968;&#30340;&#39640;&#25928;&#20998;&#23618;&#32858;&#31867;&#31639;&#27861;&#12290;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#28165;&#26224;&#32858;&#31867;&#32467;&#26500;&#30340;&#36755;&#20837;&#22270;G&#65292;&#25105;&#20204;&#35774;&#35745;&#30340;&#31639;&#27861;&#22312;G&#30340;&#36755;&#20837;&#35268;&#27169;&#30340;&#36817;&#32447;&#24615;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#24182;&#36820;&#22238;&#19968;&#20010;&#20851;&#20110;Dasgupta&#20195;&#20215;&#20989;&#25968;&#30340;O&#65288;1&#65289;&#36817;&#20284;&#30340;HC&#26641;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#35774;&#35745;&#30340;&#31639;&#27861;&#22312;&#26356;&#30701;&#30340;&#36816;&#34892;&#26102;&#38388;&#20869;&#20135;&#29983;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;HC&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two efficient hierarchical clustering (HC) algorithms with respect to Dasgupta's cost function. For any input graph $G$ with a clear cluster-structure, our designed algorithms run in nearly-linear time in the input size of $G$, and return an $O(1)$-approximate HC tree with respect to Dasgupta's cost function. We compare the performance of our algorithm against the previous state-of-the-art on synthetic and real-world datasets and show that our designed algorithm produces comparable or better HC trees with much lower running time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36710;&#36742;&#20986;&#29616;&#20449;&#24687;&#29983;&#25104;&#28909;&#21147;&#22270;&#30340;&#33258;&#21160;&#26816;&#27979;&#20572;&#36710;&#20301;&#26041;&#27861;&#65292;&#24182;&#22312;PKLot&#21644;CNRPark-EXT&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09940</link><description>&lt;p&gt;
&#22522;&#20110;&#36710;&#36742;&#20986;&#29616;&#30340;&#20572;&#36710;&#20301;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Vehicle Occurrence-based Parking Space Detection. (arXiv:2306.09940v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36710;&#36742;&#20986;&#29616;&#20449;&#24687;&#29983;&#25104;&#28909;&#21147;&#22270;&#30340;&#33258;&#21160;&#26816;&#27979;&#20572;&#36710;&#20301;&#26041;&#27861;&#65292;&#24182;&#22312;PKLot&#21644;CNRPark-EXT&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20572;&#36710;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#20256;&#24863;&#22120;&#12289;&#30456;&#26426;&#21644;&#25968;&#25454;&#20998;&#26512;&#25552;&#39640;&#20572;&#36710;&#25928;&#29575;&#12289;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#36817;&#24180;&#26469;&#24191;&#27867;&#29992;&#20110;&#20572;&#36710;&#22330;&#31649;&#29702;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#20294;&#22823;&#22810;&#25968;&#20316;&#21697;&#20551;&#23450;&#20572;&#36710;&#20301;&#26159;&#25163;&#21160;&#26631;&#27880;&#30340;&#65292;&#24433;&#21709;&#37096;&#32626;&#30340;&#25104;&#26412;&#21644;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26816;&#27979;&#20572;&#36710;&#20301;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25509;&#25910;&#19968;&#20010;&#20572;&#36710;&#22330;&#22270;&#20687;&#24207;&#21015;&#65292;&#24182;&#36820;&#22238;&#19968;&#20010;&#26631;&#35782;&#26816;&#27979;&#21040;&#30340;&#20572;&#36710;&#20301;&#30340;&#22352;&#26631;&#21015;&#34920;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#23454;&#20363;&#20998;&#21106;&#26469;&#35782;&#21035;&#27773;&#36710;&#65292;&#24182;&#20351;&#29992;&#36710;&#36742;&#20986;&#29616;&#26469;&#29983;&#25104;&#20572;&#36710;&#20301;&#30340;&#28909;&#21147;&#22270;&#12290;&#26469;&#33258;PKLot&#21644;CNRPark-EXT&#20572;&#36710;&#22330;&#25968;&#25454;&#38598;&#30340;12&#20010;&#19981;&#21516;&#23376;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#39640;95.60&#65285;&#30340;AP25&#20998;&#25968;&#21644;&#26368;&#39640;79.90&#65285;&#30340;AP50&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart-parking solutions use sensors, cameras, and data analysis to improve parking efficiency and reduce traffic congestion. Computer vision-based methods have been used extensively in recent years to tackle the problem of parking lot management, but most of the works assume that the parking spots are manually labeled, impacting the cost and feasibility of deployment. To fill this gap, this work presents an automatic parking space detection method, which receives a sequence of images of a parking lot and returns a list of coordinates identifying the detected parking spaces. The proposed method employs instance segmentation to identify cars and, using vehicle occurrence, generate a heat map of parking spaces. The results using twelve different subsets from the PKLot and CNRPark-EXT parking lot datasets show that the method achieved an AP25 score up to 95.60\% and AP50 score up to 79.90\%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#35299;&#31163;&#35268;&#33539;&#20248;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#28388;&#27874;&#22120;&#27491;&#20132;&#24615;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#20005;&#26684;&#27491;&#20132;&#24615;&#21407;&#21017;&#19979;&#65292;&#25152;&#37319;&#29992;&#30340;&#27169;&#22411;&#27604;&#20197;&#21069;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#22312;&#36817;&#27491;&#20132;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.09939</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#31163;&#35268;&#33539;&#20248;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Better Orthogonality Regularization with Disentangled Norm in Training Deep CNNs. (arXiv:2306.09939v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#35299;&#31163;&#35268;&#33539;&#20248;&#21270;&#23454;&#29616;&#26356;&#22909;&#30340;&#28388;&#27874;&#22120;&#27491;&#20132;&#24615;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#20005;&#26684;&#27491;&#20132;&#24615;&#21407;&#21017;&#19979;&#65292;&#25152;&#37319;&#29992;&#30340;&#27169;&#22411;&#27604;&#20197;&#21069;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#22312;&#36817;&#27491;&#20132;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#38450;&#27490;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19981;&#31283;&#23450;&#21644;&#29305;&#24449;&#20887;&#20313;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#27491;&#20132;&#35268;&#33539;&#21270;&#12290;&#22312;&#29616;&#26377;&#30340;&#25552;&#35758;&#20013;&#65292;&#26680;&#27491;&#20132;&#35268;&#33539;&#21270;&#36890;&#36807;&#26368;&#23567;&#21270;&#30001;&#21367;&#31215;&#36807;&#28388;&#22120;&#24418;&#25104;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27491;&#20132;&#30697;&#38453;&#20043;&#38388;&#30340;&#27531;&#24046;&#26469;&#24378;&#21046;&#23454;&#29616;&#27491;&#20132;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#28388;&#27874;&#22120;&#27491;&#20132;&#24615;&#65292;&#35813;&#26041;&#27861;&#20174;&#27531;&#24046;&#20013;&#35299;&#31163;&#20986;&#23545;&#35282;&#32447;&#21644;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#23454;&#29616;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#20005;&#26684;&#27491;&#20132;&#24615;&#21407;&#21017;&#19979;&#65292;&#25152;&#37319;&#29992;&#30340;&#27169;&#22411;&#27604;&#20197;&#21069;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#22312;&#36817;&#27491;&#20132;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25913;&#36827;&#21518;&#30340;&#20005;&#26684;&#28388;&#27874;&#22120;&#27491;&#20132;&#24615;&#22312;&#30456;&#23545;&#36739;&#27973;&#30340;&#27169;&#22411;&#20013;&#30340;&#20248;&#28857;&#65292;&#20294;&#38543;&#30528;&#27169;&#22411;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#37319;&#29992;&#20005;&#26684;&#26680;&#27491;&#20132;&#24615;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#24613;&#21095;&#19979;&#38477;&#12290;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#20005;&#26684;&#26680;&#27491;&#20132;&#24615;&#21644;&#19981;&#26029;&#22686;&#21152;&#30340;&#27169;&#22411;&#23481;Capacity&#20013;&#30340;&#28508;&#22312;&#20914;&#31361;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27491;&#20132;&#35268;&#33539;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orthogonality regularization has been developed to prevent deep CNNs from training instability and feature redundancy. Among existing proposals, kernel orthogonality regularization enforces orthogonality by minimizing the residual between the Gram matrix formed by convolutional filters and the orthogonality matrix.  We propose a novel measure for achieving better orthogonality among filters, which disentangles diagonal and correlation information from the residual. The model equipped with the measure under the principle of imposing strict orthogonality between filters surpasses previous regularization methods in near-orthogonality. Moreover, we observe the benefits of improved strict filter orthogonality in relatively shallow models, but as model depth increases, the performance gains in models employing strict kernel orthogonality decrease sharply.  Furthermore, based on the observation of the potential conflict between strict kernel orthogonality and growing model capacity, we propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#24341;&#23548;&#26041;&#27861;&#65292;&#22312;&#31283;&#23450;&#25193;&#25955;&#30340;&#22522;&#30784;&#19978;&#28155;&#21152;&#25302;&#24341;&#23548;&#65292;&#29983;&#25104;&#33021;&#22815;&#21516;&#26102;&#26368;&#23567;&#21270;&#36710;&#36742;&#39044;&#27979;&#38459;&#21147;&#31995;&#25968;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.09935</link><description>&lt;p&gt;
&#36710;&#36742;&#22270;&#20687;&#29983;&#25104;&#30340;&#25302;&#26355;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Drag-guided diffusion models for vehicle image generation. (arXiv:2306.09935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#24341;&#23548;&#26041;&#27861;&#65292;&#22312;&#31283;&#23450;&#25193;&#25955;&#30340;&#22522;&#30784;&#19978;&#28155;&#21152;&#25302;&#24341;&#23548;&#65292;&#29983;&#25104;&#33021;&#22815;&#21516;&#26102;&#26368;&#23567;&#21270;&#36710;&#36742;&#39044;&#27979;&#38459;&#21147;&#31995;&#25968;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#35268;&#27169;&#35757;&#32451;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#23558;&#36825;&#20123;&#24037;&#20855;&#24212;&#29992;&#20110;&#24037;&#31243;&#35774;&#35745;&#26159;&#19968;&#31181;&#26377;&#36259;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#33021;&#35299;&#26512;&#24182;&#24378;&#21046;&#25191;&#34892;&#20855;&#20307;&#30340;&#24037;&#31243;&#32422;&#26463;&#26465;&#20214;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#29289;&#29702;&#30340;&#24341;&#23548;&#65292;&#23454;&#29616;&#20102;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20248;&#21270;&#24615;&#33021;&#24230;&#37327;&#65288;&#30001;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#65289;&#65292;&#21521;&#36825;&#20010;&#30446;&#26631;&#36808;&#36827;&#20102;&#19968;&#27493;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#22312;&#31283;&#23450;&#25193;&#25955;&#20013;&#28155;&#21152;&#25302;&#24341;&#23548;&#65292;&#20351;&#35813;&#24037;&#20855;&#33021;&#22815;&#29983;&#25104;&#26032;&#36710;&#36742;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23427;&#20204;&#30340;&#39044;&#27979;&#38459;&#21147;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models trained at web-scale have revolutionized image generation. The application of these tools to engineering design is an intriguing possibility, but is currently limited by their inability to parse and enforce concrete engineering constraints. In this paper, we take a step towards this goal by proposing physics-based guidance, which enables optimization of a performance metric (as predicted by a surrogate model) during the generation process. As a proof-of-concept, we add drag guidance to Stable Diffusion, which allows this tool to generate images of novel vehicles while simultaneously minimizing their predicted drag coefficients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#30452;&#26041;&#22270;&#26799;&#24230;&#25552;&#21319;&#20998;&#31867;&#26426;&#22120;&#65288;HGBC&#65289;&#65292;&#29992;&#20110;&#31227;&#21160;&#24212;&#29992;&#20013;&#33021;&#32791;&#30340;&#39044;&#27979;&#65292;&#24182;&#35299;&#20915;&#20102;&#29305;&#24449;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#20004;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09931</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#20013;&#33021;&#32791;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Metaheuristic-based Machine Learning Approach for Energy Prediction in Mobile App Development. (arXiv:2306.09931v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#30452;&#26041;&#22270;&#26799;&#24230;&#25552;&#21319;&#20998;&#31867;&#26426;&#22120;&#65288;HGBC&#65289;&#65292;&#29992;&#20110;&#31227;&#21160;&#24212;&#29992;&#20013;&#33021;&#32791;&#30340;&#39044;&#27979;&#65292;&#24182;&#35299;&#20915;&#20102;&#29305;&#24449;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#32791;&#22312;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#21644;&#20351;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#26159;&#29992;&#25143;&#36141;&#20080;&#26234;&#33021;&#25163;&#26426;&#26102;&#32771;&#34385;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#32771;&#34385;&#21040;&#21487;&#25345;&#32493;&#21457;&#23637;&#65292;&#24517;&#39035;&#25214;&#21040;&#33021;&#22815;&#38477;&#20302;&#31227;&#21160;&#35774;&#22791;&#33021;&#32791;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20840;&#29699;&#25968;&#21313;&#20159;&#37096;&#26234;&#33021;&#25163;&#26426;&#30340;&#24191;&#27867;&#20351;&#29992;&#23545;&#29615;&#22659;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#30340;&#30452;&#26041;&#22270;&#26799;&#24230;&#25552;&#21319;&#20998;&#31867;&#26426;&#22120;&#65288;HGBC&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31227;&#21160;&#24212;&#29992;&#20013;&#33021;&#32791;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#23427;&#22312;&#24615;&#33021;&#26410;&#26377;&#26174;&#33879;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#20102;&#22810;&#20313;&#21644;&#26080;&#20851;&#29305;&#24449;&#65307;&#20854;&#27425;&#65292;&#23427;&#25191;&#34892;&#20102;&#36229;&#21442;&#25968;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy consumption plays a vital role in mobile App development for developers and end-users, and it is considered one of the most crucial factors for purchasing a smartphone. In addition, in terms of sustainability, it is essential to find methods to reduce the energy consumption of mobile devices since the extensive use of billions of smartphones worldwide significantly impacts the environment. Despite the existence of several energy-efficient programming practices in Android, the leading mobile ecosystem, machine learning-based energy prediction algorithms for mobile App development have yet to be reported. Therefore, this paper proposes a histogram-based gradient boosting classification machine (HGBC), boosted by a metaheuristic approach, for energy prediction in mobile App development. Our metaheuristic approach is responsible for two issues. First, it finds redundant and irrelevant features without any noticeable change in performance. Second, it performs a hyper-parameter tuning
&lt;/p&gt;</description></item><item><title>LLMs&#26377;&#28508;&#21147;&#22312;&#34892;&#25919;&#12289;&#21019;&#36896;&#24615;&#21644;&#20998;&#26512;&#20219;&#21153;&#26041;&#38754;&#23545;&#31185;&#23398;&#20570;&#20986;&#21464;&#38761;&#65292;&#20294;&#38656;&#35201;&#36890;&#36807;&#31215;&#26497;&#30340;&#30417;&#31649;&#21644;&#31185;&#23398;&#25945;&#32946;&#26469;&#35299;&#20915;&#19982;&#20559;&#35265;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#36136;&#37327;&#20445;&#35777;&#26377;&#20851;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.09928</link><description>&lt;p&gt;
&#26159;&#21451;&#36824;&#26159;&#25932;&#65311;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31185;&#23398;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Friend or Foe? Exploring the Implications of Large Language Models on the Science System. (arXiv:2306.09928v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09928
&lt;/p&gt;
&lt;p&gt;
LLMs&#26377;&#28508;&#21147;&#22312;&#34892;&#25919;&#12289;&#21019;&#36896;&#24615;&#21644;&#20998;&#26512;&#20219;&#21153;&#26041;&#38754;&#23545;&#31185;&#23398;&#20570;&#20986;&#21464;&#38761;&#65292;&#20294;&#38656;&#35201;&#36890;&#36807;&#31215;&#26497;&#30340;&#30417;&#31649;&#21644;&#31185;&#23398;&#25945;&#32946;&#26469;&#35299;&#20915;&#19982;&#20559;&#35265;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#36136;&#37327;&#20445;&#35777;&#26377;&#20851;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI&#24320;&#21457;&#30340;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#35752;&#35770;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23427;&#23545;&#31185;&#23398;&#21644;&#39640;&#31561;&#25945;&#32946;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#34429;&#28982;&#23545;&#25945;&#32946;&#30340;&#24433;&#21709;&#19968;&#30452;&#26159;&#20027;&#35201;&#20851;&#27880;&#30340;&#28966;&#28857;&#65292;&#20294;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22522;&#20110;LLMs&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#31185;&#23398;&#21644;&#31185;&#23398;&#23454;&#36341;&#30340;&#24433;&#21709;&#30340;&#23454;&#35777;&#30740;&#31350;&#26377;&#38480;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;Delphi&#30740;&#31350;&#65292;&#28041;&#21450;72&#20301;&#19987;&#38376;&#20174;&#20107;&#30740;&#31350;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#19987;&#23478;&#12290;&#35813;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;LLMs&#30340;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#31185;&#23398;&#31995;&#32479;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#32771;&#34385;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#26377;&#25928;&#20351;&#29992;&#25152;&#38656;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;LLMs&#22312;&#31185;&#23398;&#20013;&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#25919;&#12289;&#21019;&#36896;&#24615;&#21644;&#20998;&#26512;&#20219;&#21153;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#19982;&#20559;&#35265;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#36136;&#37327;&#20445;&#35777;&#26377;&#20851;&#30340;&#39118;&#38505;&#38656;&#35201;&#36890;&#36807;&#31215;&#26497;&#30340;&#30417;&#31649;&#21644;&#31185;&#23398;&#25945;&#32946;&#21152;&#20197;&#35299;&#20915;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#26377;&#20851;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#21644;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#24433;&#21709;&#30340;&#30693;&#24773;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of ChatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher education. While the impact on education has been a primary focus, there is limited empirical research on the effects of large language models (LLMs) and LLM-based chatbots on science and scientific practice. To investigate this further, we conducted a Delphi study involving 72 experts specialising in research and AI. The study focused on applications and limitations of LLMs, their effects on the science system, ethical and legal considerations, and the required competencies for their effective use. Our findings highlight the transformative potential of LLMs in science, particularly in administrative, creative, and analytical tasks. However, risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education. This research contributes to informed discussions on the impact of generative AI in science and he
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09927</link><description>&lt;p&gt;
&#35757;&#32451;&#22909;&#30340;Transformer&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20363;&#22914;Transformers&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#30701;&#35821;&#24207;&#21015;&#30340;&#25552;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#21046;&#23450;&#30456;&#20851;&#30340;&#27599;&#20010;&#20196;&#29260;&#21644;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#24207;&#21015;&#23884;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#36825;&#20351;&#24471;Transformer&#34920;&#29616;&#24471;&#20687;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#35757;&#32451;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20250;&#27169;&#20223;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#26426;&#22120;&#20154;&#21160;&#20316;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#23436;&#25104;&#24635;&#32467;&#21644;&#22238;&#31572;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20174;&#38382;&#39064;&#22238;&#31572;&#20013;&#23398;&#20064;&#23545;&#35937;&#34920;&#31034;&#30340;&#38646;-shot&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2306.09922</link><description>&lt;p&gt;
&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#19982;&#34394;&#25311;&#26426;&#22120;&#20154;&#36807;&#21435;&#21160;&#20316;&#30456;&#20851;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions. (arXiv:2306.09922v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#26426;&#22120;&#20154;&#21160;&#20316;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#23436;&#25104;&#24635;&#32467;&#21644;&#22238;&#31572;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20174;&#38382;&#39064;&#22238;&#31572;&#20013;&#23398;&#20064;&#23545;&#35937;&#34920;&#31034;&#30340;&#38646;-shot&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#20154;&#25191;&#34892;&#38271;&#24207;&#21015;&#30340;&#21160;&#20316;&#26102;&#65292;&#29992;&#25143;&#38656;&#35201;&#36731;&#26494;&#12289;&#21487;&#38752;&#22320;&#20102;&#35299;&#23427;&#20204;&#25152;&#20570;&#30340;&#20107;&#24773;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#20851;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#36807;&#21435;&#21160;&#20316;&#30340;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#19968;&#20010;&#26680;&#24515;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21333;&#19968;&#31995;&#32479;&#34987;&#35757;&#32451;&#29992;&#20110;&#24635;&#32467;&#21644;&#22238;&#31572;&#20851;&#20110;&#34394;&#25311;&#26426;&#22120;&#20154;&#30340;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#24103;&#21644;&#38382;&#39064;&#25552;&#31034;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;&#20026;&#20102;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#20851;&#20110;&#23545;&#35937;&#12289;&#21160;&#20316;&#21644;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#26426;&#22120;&#20154;&#21160;&#20316;&#24207;&#21015;&#26399;&#38388;&#21160;&#20316;&#21457;&#29983;&#30340;&#26102;&#38388;&#39034;&#24207;&#30340;&#33521;&#25991;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#23558;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#24635;&#32467;&#21644;&#22238;&#31572;&#38382;&#39064;&#20351;&#24471;&#20174;&#38382;&#39064;&#22238;&#31572;&#20013;&#23398;&#20064;&#30340;&#23545;&#35937;&#34920;&#31034;&#30340;&#38646;-shot&#36716;&#31227;&#33021;&#22815;&#25552;&#39640;&#21160;&#20316;&#24635;&#32467;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#22312;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
When robots perform long action sequences, users will want to easily and reliably find out what they have done. We therefore demonstrate the task of learning to summarize and answer questions about a robot agent's past actions using natural language alone. A single system with a large language model at its core is trained to both summarize and answer questions about action sequences given ego-centric video frames of a virtual robot and a question prompt. To enable training of question answering, we develop a method to automatically generate English-language questions and answers about objects, actions, and the temporal order in which actions occurred during episodes of robot action in the virtual environment. Training one model to both summarize and answer questions enables zero-shot transfer of representations of objects learned through question answering to improved action summarization. % involving objects not seen in training to summarize.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25511;&#21046;&#39282;&#26009;&#21644;&#30417;&#27979;&#27700;&#36136;&#23545;&#20110;&#27700;&#20135;&#20859;&#27542;&#31995;&#32479;&#20013;&#24179;&#34913;&#40060;&#31867;&#29983;&#20135;&#21147;&#21644;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#21487;&#38752;&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09920</link><description>&lt;p&gt;
&#27700;&#20135;&#20859;&#27542;&#31995;&#32479;&#20013;&#30340;&#39282;&#26009;&#25511;&#21046;&#21644;&#27700;&#36136;&#30417;&#27979;&#65306;&#26426;&#36935;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Feeding control and water quality monitoring in aquaculture systems: Opportunities and challenges. (arXiv:2306.09920v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25511;&#21046;&#39282;&#26009;&#21644;&#30417;&#27979;&#27700;&#36136;&#23545;&#20110;&#27700;&#20135;&#20859;&#27542;&#31995;&#32479;&#20013;&#24179;&#34913;&#40060;&#31867;&#29983;&#20135;&#21147;&#21644;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#21487;&#38752;&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#20135;&#20859;&#27542;&#31995;&#32479;&#21487;&#20197;&#20174;&#20808;&#36827;&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#26368;&#26032;&#21457;&#23637;&#20013;&#21463;&#30410;&#65292;&#20197;&#20943;&#23569;&#36816;&#33829;&#25104;&#26412;&#21644;&#40060;&#31867;&#25439;&#22833;&#65292;&#22686;&#21152;&#29983;&#20135;&#25928;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#40060;&#31867;&#31119;&#21033;&#21644;&#20581;&#24247;&#12290;&#30417;&#27979;&#27700;&#36136;&#21644;&#25511;&#21046;&#39282;&#26009;&#26159;&#24179;&#34913;&#40060;&#31867;&#29983;&#20135;&#21147;&#24182;&#22609;&#36896;&#40060;&#31867;&#29983;&#38271;&#36807;&#31243;&#30340;&#22522;&#26412;&#35201;&#32032;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#40060;&#31867;&#39282;&#20859;&#36807;&#31243;&#22312;&#19981;&#21516;&#38454;&#27573;&#25163;&#21160;&#36827;&#34892;&#65292;&#24182;&#20381;&#36182;&#20110;&#32791;&#26102;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#24037;&#21306;&#20998;&#12290;&#39282;&#26009;&#25511;&#21046;&#26041;&#27861;&#36890;&#36807;&#39282;&#26009;&#36716;&#21270;&#29575;&#24433;&#21709;&#40060;&#31867;&#29983;&#38271;&#21644;&#32321;&#27542;&#65292;&#22240;&#27492;&#25511;&#21046;&#36825;&#20123;&#39282;&#26009;&#21442;&#25968;&#23545;&#20110;&#22686;&#24378;&#40060;&#31867;&#31119;&#21033;&#21644;&#26368;&#23567;&#21270;&#19968;&#33324;&#28180;&#19994;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#29615;&#22659;&#22240;&#32032;&#65288;&#22914;&#39640;&#27688;&#27987;&#24230;&#21644;pH&#65289;&#30340;&#39640;&#27987;&#24230;&#20250;&#24433;&#21709;&#27700;&#36136;&#21644;&#40060;&#31867;&#23384;&#27963;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#25511;&#21046;&#31574;&#30053;&#26469;&#30830;&#23450;&#22312;&#27700;&#20135;&#20859;&#27542;&#31995;&#32479;&#20013;&#26368;&#20339;&#12289;&#39640;&#25928;&#21644;&#21487;&#38752;&#30340;&#39282;&#26009;&#36807;&#31243;&#21644;&#27700;&#36136;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aquaculture systems can benefit from the recent development of advanced control strategies to reduce operating costs and fish loss and increase growth production efficiency, resulting in fish welfare and health. Monitoring the water quality and controlling feeding are fundamental elements of balancing fish productivity and shaping the fish growth process. Currently, most fish-feeding processes are conducted manually in different phases and rely on time-consuming and challenging artificial discrimination. The feeding control approach influences fish growth and breeding through the feed conversion rate; hence, controlling these feeding parameters is crucial for enhancing fish welfare and minimizing general fishery costs. The high concentration of environmental factors, such as a high ammonia concentration and pH, affect the water quality and fish survival. Therefore, there is a critical need to develop control strategies to determine optimal, efficient, and reliable feeding processes and
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#20998;&#31867;&#24635;&#32467;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#25216;&#26415;&#29305;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.09912</link><description>&lt;p&gt;
&#36208;&#21521;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Quantum Federated Learning. (arXiv:2306.09912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09912
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#20998;&#31867;&#24635;&#32467;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#25216;&#26415;&#29305;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#20132;&#21449;&#23398;&#31185;&#39046;&#22495;&#65292;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#12290;&#30446;&#21069;&#23578;&#26080;&#20851;&#20110;&#35813;&#20132;&#21449;&#23398;&#31185;&#39046;&#22495;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#26412;&#25991;&#23545;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#32454;&#33268;&#30340;&#25506;&#35752;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#29702;&#12289;&#25216;&#26415;&#20197;&#21450;&#26032;&#20852;&#24212;&#29992;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#25972;&#21512;&#36825;&#20123;&#25216;&#26415;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#25353;&#20854;&#29305;&#24449;&#21644;&#25152;&#37319;&#29992;&#30340;&#37327;&#23376;&#25216;&#26415;&#20998;&#31867;&#12290;&#38543;&#30528;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#35745;&#23558;&#22312;&#21508;&#20010;&#34892;&#19994;&#23454;&#29616;&#26356;&#22810;&#30340;&#31361;&#30772;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Federated Learning (QFL) is an emerging interdisciplinary field that merges the principles of Quantum Computing (QC) and Federated Learning (FL), with the goal of leveraging quantum technologies to enhance privacy, security, and efficiency in the learning process. Currently, there is no comprehensive survey for this interdisciplinary field. This review offers a thorough, holistic examination of QFL. We aim to provide a comprehensive understanding of the principles, techniques, and emerging applications of QFL. We discuss the current state of research in this rapidly evolving field, identify challenges and opportunities associated with integrating these technologies, and outline future directions and open research questions. We propose a unique taxonomy of QFL techniques, categorized according to their characteristics and the quantum techniques employed. As the field of QFL continues to progress, we can anticipate further breakthroughs and applications across various industries,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09910</link><description>&lt;p&gt;
LabelBench&#65306;&#22522;&#20110;&#32508;&#21512;&#26694;&#26550;&#30340;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#65292;&#20294;&#33719;&#21462;&#26631;&#35760;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#20943;&#32531;&#36825;&#19968;&#25104;&#26412;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36801;&#31227;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65289;&#26088;&#22312;&#23454;&#29616;&#26631;&#31614;&#39640;&#25928;&#24615;&#65306;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#26368;&#20339;&#30340;&#26631;&#31614;&#25928;&#29575;&#36890;&#24120;&#38656;&#35201;&#36825;&#20123;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#24182;&#27809;&#26377;&#25429;&#25417;&#21040;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#30340;&#21327;&#21516;&#32452;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LabelBench&#35299;&#20915;&#20102;&#36825;&#20010;&#32570;&#38519;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35780;&#20272;&#22810;&#20010;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#20316;&#20026;LabelBench&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#19968;&#36215;&#20351;&#29992;&#30340;&#26368;&#26032;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35777;&#26126;&#20102;&#27604;&#20808;&#21069;&#25253;&#21578;&#30340;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#40479;&#22768;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#38598;&#20013;&#35760;&#24405;&#23545;&#30456;&#20851;&#27010;&#29575;&#21518;&#65292;&#24212;&#29992;&#30456;&#20851;&#32858;&#31867;&#20110;&#27979;&#35797;&#38598;&#36827;&#34892;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#27979;&#35797;&#38598;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25506;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#35757;&#32451;&#26399;&#38388;&#26410;&#21548;&#21040;&#30340;&#40479;&#31867;&#29289;&#31181;&#30340;&#35760;&#24405;&#20197;&#21450;&#20998;&#31163;&#40479;&#22768;&#21644;&#29615;&#22659;&#22122;&#22768;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09906</link><description>&lt;p&gt;
&#40479;&#40483;&#22768;&#30340;&#30456;&#20851;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Correlation Clustering of Bird Sounds. (arXiv:2306.09906v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#40479;&#22768;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#38598;&#20013;&#35760;&#24405;&#23545;&#30456;&#20851;&#27010;&#29575;&#21518;&#65292;&#24212;&#29992;&#30456;&#20851;&#32858;&#31867;&#20110;&#27979;&#35797;&#38598;&#36827;&#34892;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#27979;&#35797;&#38598;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25506;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#35757;&#32451;&#26399;&#38388;&#26410;&#21548;&#21040;&#30340;&#40479;&#31867;&#29289;&#31181;&#30340;&#35760;&#24405;&#20197;&#21450;&#20998;&#31163;&#40479;&#22768;&#21644;&#29615;&#22659;&#22122;&#22768;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40479;&#31867;&#22768;&#38899;&#20998;&#31867;&#26159;&#23558;&#20219;&#20309;&#22768;&#38899;&#35760;&#24405;&#19982;&#21487;&#20197;&#22312;&#35760;&#24405;&#20013;&#21548;&#21040;&#30340;&#40479;&#31867;&#29289;&#31181;&#30456;&#20851;&#32852;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#40479;&#22768;&#32858;&#31867;&#65292;&#21363;&#20915;&#23450;&#20219;&#20309;&#19968;&#23545;&#22768;&#38899;&#35760;&#24405;&#26159;&#21542;&#21487;&#20197;&#21548;&#21040;&#30456;&#21516;&#30340;&#40479;&#31867;&#29289;&#31181;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#35760;&#24405;&#23545;&#25353;&#36825;&#31181;&#26041;&#24335;&#30456;&#20851;&#30340;&#27010;&#29575;&#65292;&#28982;&#21518;&#36890;&#36807;&#30456;&#20851;&#32858;&#31867;&#26469;&#25512;&#26029;&#27979;&#35797;&#38598;&#30340;&#26368;&#22823;&#21487;&#33021;&#20998;&#21306;&#12290;&#25105;&#20204;&#35299;&#20915;&#20197;&#19979;&#38382;&#39064;&#65306;&#19982;&#27979;&#35797;&#38598;&#30340;&#20998;&#31867;&#30456;&#27604;&#65292;&#36825;&#31181;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#22914;&#20309;&#65311;&#20174;&#20998;&#31867;&#33719;&#24471;&#30340;&#32858;&#31867;&#22914;&#20309;&#19982;&#22240;&#27492;&#25512;&#26029;&#24471;&#20986;&#30340;&#32858;&#31867;&#30456;&#20851;&#65311;&#22312;&#24212;&#29992;&#20110;&#35757;&#32451;&#26399;&#38388;&#26410;&#21548;&#21040;&#30340;&#40479;&#31867;&#29289;&#31181;&#30340;&#35760;&#24405;&#26102;&#65292;&#36825;&#31181;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#22914;&#20309;&#65311;&#36825;&#31181;&#32858;&#31867;&#22312;&#20998;&#31163;&#40479;&#40483;&#22768;&#21644;&#35757;&#32451;&#26399;&#38388;&#26410;&#21548;&#21040;&#30340;&#29615;&#22659;&#22122;&#22768;&#26041;&#38754;&#26377;&#22810;&#26377;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
Bird sound classification is the task of relating any sound recording to those species of bird that can be heard in the recording. Here, we study bird sound clustering, the task of deciding for any pair of sound recordings whether the same species of bird can be heard in both. We address this problem by first learning, from a training set, probabilities of pairs of recordings being related in this way, and then inferring a maximally probable partition of a test set by correlation clustering. We address the following questions: How accurate is this clustering, compared to a classification of the test set? How do the clusters thus inferred relate to the clusters obtained by classification? How accurate is this clustering when applied to recordings of bird species not heard during training? How effective is this clustering in separating, from bird sounds, environmental noise not heard during training?
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20256;&#32479;&#30340;&#20869;&#20998;&#24067;&#27867;&#21270;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#34394;&#20551;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#26469;&#22823;&#22823;&#25439;&#23475;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#23588;&#20854;&#26159;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.09890</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Studying Generalization on Memory-Based Methods in Continual Learning. (arXiv:2306.09890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20256;&#32479;&#30340;&#20869;&#20998;&#24067;&#27867;&#21270;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#34394;&#20551;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#26469;&#22823;&#22823;&#25439;&#23475;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#23588;&#20854;&#26159;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#30446;&#26631;&#20043;&#19968;&#26159;&#22312;&#19968;&#31995;&#21015;&#32463;&#39564;&#20013;&#19981;&#26029;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20943;&#36731;&#23436;&#20840;&#30693;&#35782;&#35206;&#30422;&#30340;&#38382;&#39064;&#65292;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#20250;&#23384;&#20648;&#19968;&#23450;&#27604;&#20363;&#30340;&#20808;&#21069;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#27979;&#35797;&#23427;&#20204;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#24615;&#33021;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#36807;&#24230;&#25311;&#21512;&#37325;&#25918;&#35760;&#24518;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20256;&#32479;&#30340;&#20869;&#20998;&#24067;&#27867;&#21270;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#34394;&#20551;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#26469;&#22823;&#22823;&#25439;&#23475;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;&#20351;&#29992;&#25511;&#21046;&#29615;&#22659;&#65292;&#25105;&#20204;&#20351;&#29992;Synbol&#22522;&#20934;&#29983;&#25104;&#22120;&#65288;Lacoste&#31561;&#20154;&#65292;2020&#65289;&#23637;&#31034;&#20102;&#36825;&#31181;&#32570;&#20047;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#20027;&#35201;&#20986;&#29616;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the objectives of Continual Learning is to learn new concepts continually over a stream of experiences and at the same time avoid catastrophic forgetting. To mitigate complete knowledge overwriting, memory-based methods store a percentage of previous data distributions to be used during training. Although these methods produce good results, few studies have tested their out-of-distribution generalization properties, as well as whether these methods overfit the replay memory. In this work, we show that although these methods can help in traditional in-distribution generalization, they can strongly impair out-of-distribution generalization by learning spurious features and correlations. Using a controlled environment, the Synbol benchmark generator (Lacoste et al., 2020), we demonstrate that this lack of out-of-distribution generalization mainly occurs in the linear classifier.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; CANDID&#65292;&#26159;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#29190;&#21457;&#22270;&#20687;&#21435;&#22122;&#30340;&#23545;&#40784;&#31639;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#20809;&#27969;&#30340;&#23545;&#24212;&#20272;&#35745;&#27169;&#22359;&#26469;&#23545;&#40784;&#36755;&#20837;&#30340;&#25152;&#26377;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#30340;&#23545;&#24212;&#20851;&#31995;&#22312;&#39044;&#22788;&#29702;&#21644;&#21407;&#22987;&#22270;&#20687;&#20013;&#39044;&#27979;&#20687;&#32032;&#32423;&#21487;&#21464;&#28388;&#27874;&#22120;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09887</link><description>&lt;p&gt;
CANDID&#65306;&#28145;&#24230;&#29190;&#21457;&#22270;&#20687;&#21435;&#22122;&#30340;&#23545;&#40784;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CANDID: Correspondence AligNment for Deep-burst Image Denoising. (arXiv:2306.09887v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; CANDID&#65292;&#26159;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#29190;&#21457;&#22270;&#20687;&#21435;&#22122;&#30340;&#23545;&#40784;&#31639;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#20809;&#27969;&#30340;&#23545;&#24212;&#20272;&#35745;&#27169;&#22359;&#26469;&#23545;&#40784;&#36755;&#20837;&#30340;&#25152;&#26377;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#30340;&#23545;&#24212;&#20851;&#31995;&#22312;&#39044;&#22788;&#29702;&#21644;&#21407;&#22987;&#22270;&#20687;&#20013;&#39044;&#27979;&#20687;&#32032;&#32423;&#21487;&#21464;&#28388;&#27874;&#22120;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#25163;&#26426;&#25668;&#24433;&#21644;&#28857;&#25293;&#30456;&#26426;&#30340;&#20986;&#29616;&#65292;&#28145;&#24230;&#29190;&#21457;&#25104;&#20687;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#25668;&#24433;&#25928;&#26524;&#65292;&#20363;&#22914;&#26223;&#28145;&#12289;&#36229;&#20998;&#36776;&#29575;&#12289;&#36816;&#21160;&#21435;&#27169;&#31946;&#21644;&#22270;&#20687;&#21435;&#22122;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#22522;&#20110;&#20809;&#27969;&#30340;&#23545;&#24212;&#20272;&#35745;&#27169;&#22359;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#29190;&#21457;&#22270;&#20687;&#21435;&#22122;&#38382;&#39064;&#12290;&#20026;&#20102;&#22788;&#29702;&#19981;&#21516;&#30340;&#22122;&#22768;&#27700;&#24179;&#65292;&#23545;&#27599;&#20010;&#22270;&#20687;&#36827;&#34892;&#19981;&#21516;&#30340;&#39044;&#36807;&#28388;&#35774;&#32622;&#12290;&#21033;&#29992;&#24314;&#31435;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#19968;&#20010;&#32593;&#32476;&#22359;&#21487;&#20197;&#39044;&#27979;&#27599;&#20010;&#22270;&#20687;&#30340;&#20687;&#32032;&#32423;&#21487;&#21464;&#28388;&#27874;&#22120;&#26680;&#65292;&#20197;&#24179;&#28369;&#21407;&#22987;&#21644;&#39044;&#22788;&#29702;&#30340;&#29190;&#21457;&#22270;&#20687;&#65292;&#28982;&#21518;&#34701;&#21512;&#25152;&#26377;&#22270;&#20687;&#29983;&#25104;&#26368;&#32456;&#21435;&#22122;&#36755;&#20986;&#12290;&#35813;&#27969;&#31243;&#36890;&#36807;&#32452;&#21512;&#29190;&#21457;&#25552;&#20379;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of mobile phone photography and point-and-shoot cameras, deep-burst imaging is widely used for a number of photographic effects such as depth of field, super-resolution, motion deblurring, and image denoising. In this work, we propose to solve the problem of deep-burst image denoising by including an optical flow-based correspondence estimation module which aligns all the input burst images with respect to a reference frame. In order to deal with varying noise levels the individual burst images are pre-filtered with different settings. Exploiting the established correspondences one network block predicts a pixel-wise spatially-varying filter kernel to smooth each image in the original and prefiltered bursts before fusing all images to generate the final denoised output. The resulting pipeline achieves state-of-the-art results by combining all available information provided by the burst.
&lt;/p&gt;</description></item><item><title>Jumanji&#26159;JAX&#20013;&#19968;&#22871;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;</title><link>http://arxiv.org/abs/2306.09884</link><description>&lt;p&gt;
Jumanji: JAX&#20013;&#19968;&#22871;&#22810;&#26679;&#21270;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX. (arXiv:2306.09884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09884
&lt;/p&gt;
&lt;p&gt;
Jumanji&#26159;JAX&#20013;&#19968;&#22871;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#22312;&#25512;&#21160;AI&#31639;&#27861;&#30340;&#21457;&#23637;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#38656;&#35201;&#27169;&#25311;&#29615;&#22659;&#20855;&#22791;&#24615;&#33021;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#26356;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jumanji&#65292;&#36825;&#26159;&#19968;&#22871;&#35774;&#35745;&#29992;&#20110;&#24555;&#36895;&#12289;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#19981;&#21516;RL&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#12290;Jumanji&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#30340;&#29615;&#22659;&#65292;&#19987;&#27880;&#20110;&#24037;&#19994;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#20197;&#21450;&#25361;&#25112;&#24615;&#30340;&#19968;&#33324;&#20915;&#31574;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;JAX&#21644;GPU&#12289;TPU&#31561;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#29575;&#65292;Jumanji&#33021;&#22815;&#36805;&#36895;&#36845;&#20195;&#30740;&#31350;&#24605;&#36335;&#21644;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#26368;&#32456;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;&#19982;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#22871;&#20214;&#19981;&#21516;&#65292;Jumanji&#20855;&#26377;&#39640;&#24230;&#21487;&#23450;&#21046;&#24615;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#20854;&#38656;&#27714;&#35843;&#25972;&#21021;&#22987;&#29366;&#24577;&#20998;&#24067;&#21644;&#38382;&#39064;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-source reinforcement learning (RL) environments have played a crucial role in driving progress in the development of AI algorithms. In modern RL research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of potential real-world applications. Therefore, we present Jumanji, a suite of diverse RL environments specifically designed to be fast, flexible, and scalable. Jumanji provides a suite of environments focusing on combinatorial problems frequently encountered in industry, as well as challenging general decision-making tasks. By leveraging the efficiency of JAX and hardware accelerators like GPUs and TPUs, Jumanji enables rapid iteration of research ideas and large-scale experimentation, ultimately empowering more capable agents. Unlike existing RL environment suites, Jumanji is highly customizable, allowing users to tailor the initial state distribution and problem complexity to their needs. Further
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26102;&#31354;Tweedie&#27169;&#22411;STTD&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#20998;&#36776;&#29575;OD&#30697;&#38453;&#20013;&#31232;&#30095;&#21644;&#38271;&#23614;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.09882</link><description>&lt;p&gt;
&#26102;&#31354;Tweedie&#27169;&#22411;&#22312;&#39044;&#27979;&#23384;&#22312;&#38646;&#33192;&#32960;&#21644;&#38271;&#23614;&#26053;&#34892;&#38656;&#27714;&#20013;&#30340;&#24212;&#29992;&#21450;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification via Spatial-Temporal Tweedie Model for Zero-inflated and Long-tail Travel Demand Prediction. (arXiv:2306.09882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26102;&#31354;Tweedie&#27169;&#22411;STTD&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#20998;&#36776;&#29575;OD&#30697;&#38453;&#20013;&#31232;&#30095;&#21644;&#38271;&#23614;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26102;&#31354;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38590;&#20197;&#35299;&#20915;&#39640;&#20998;&#36776;&#29575;OD&#30697;&#38453;&#20013;&#31232;&#30095;&#21644;&#38271;&#23614;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#38590;&#20197;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#36825;&#23545;&#20110;&#20132;&#36890;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#31354;&#38388;-Tweedie&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STTD&#65289;&#12290;STTD&#23558;Tweedie&#20998;&#24067;&#20316;&#20026;&#20256;&#32479;&#30340;&#8220;&#38646;&#33192;&#32960;&#8221;&#27169;&#22411;&#30340;&#26377;&#21147;&#26367;&#20195;&#21697;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#23884;&#20837;&#26469;&#21442;&#25968;&#21270;&#26053;&#34892;&#38656;&#27714;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;STTD&#22312;&#39640;&#20998;&#36776;&#29575;&#22330;&#26223;&#19979;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
crucial for transportation management. However, traditional spatial-temporal deep learning models grapple with addressing the sparse and long-tail characteristics in high-resolution O-D matrices and quantifying prediction uncertainty. This dilemma arises from the numerous zeros and over-dispersed demand patterns within these matrices, which challenge the Gaussian assumption inherent to deterministic deep learning models. To address these challenges, we propose a novel approach: the Spatial-Temporal Tweedie Graph Neural Network (STTD). The STTD introduces the Tweedie distribution as a compelling alternative to the traditional 'zero-inflated' model and leverages spatial and temporal embeddings to parameterize travel demand distributions. Our evaluations using real-world datasets highlight STTD's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
&lt;/p&gt;</description></item><item><title>GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09872</link><description>&lt;p&gt;
&#21487;&#27867;&#21270;&#30340;&#19968;&#27425;&#24615;&#32499;&#32034;&#25805;&#20316;&#31574;&#30053;&#21450;&#20854;&#21442;&#25968;&#24863;&#30693;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizable One-shot Rope Manipulation with Parameter-Aware Policy. (arXiv:2306.09872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09872
&lt;/p&gt;
&lt;p&gt;
GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#32499;&#32034;&#22312;&#36816;&#21160;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#20026;&#22240;&#32032;&#65292;&#20197;&#24448;&#32499;&#32034;&#25805;&#20316;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#25968;&#30334;&#27425;&#30495;&#23454;&#28436;&#31034;&#26469;&#20026;&#27599;&#20010;&#32499;&#32034;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#8220;&#21040;&#36798;&#30446;&#26631;&#8221;&#20219;&#21153;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25105;&#20204;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GenORM&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35753;&#25805;&#20316;&#31574;&#30053;&#36890;&#36807;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#30340;&#32499;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#31574;&#30053;&#19978;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#24182;&#20351;&#29992;&#21508;&#31181;&#27169;&#25311;&#21487;&#21464;&#24418;&#32499;&#32034;&#26469;&#35757;&#32451;&#23427;&#65292;&#20351;&#31574;&#30053;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#32499;&#32034;&#21442;&#25968;&#35843;&#25972;&#34892;&#21160;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;GenORM&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#28436;&#31034;&#21644;&#27169;&#25311;&#28857;&#20113;&#30340;&#32593;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#12290;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#24110;&#21161;&#65292;&#25105;&#20204;&#20165;&#38656;&#35201;&#19968;&#27425;&#28436;&#31034;&#25968;&#25454;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#30340;&#32499;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the inherent uncertainty in their deformability during motion, previous methods in rope manipulation often require hundreds of real-world demonstrations to train a manipulation policy for each rope, even for simple tasks such as rope goal reaching, which hinder their applications in our ever-changing world. To address this issue, we introduce GenORM, a framework that allows the manipulation policy to handle different deformable ropes with a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable rope parameters and training it with a diverse range of simulated deformable ropes so that the policy can adjust actions based on different rope parameters. At the time of inference, given a new rope, GenORM estimates the deformable rope parameters by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations. With the help of a differentiable physics simulator, we require only a single r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09869</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#39044;&#26399;&#35821;&#20041;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#35821;&#20041;&#38169;&#20301;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#27599;&#20010;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#21046;&#23450;&#28508;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#25991;&#26412;&#23884;&#20837;&#30340;EBM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#33719;&#24471;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#23545;&#25968;&#21518;&#39564;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#26032;&#21644;&#36716;&#31227;&#21040;&#21518;&#32493;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#65292;&#20174;&#32780;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#23884;&#22871;&#23618;&#27425;&#30340;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#28508;&#22312;EBMs&#36824;&#20801;&#35768;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#65292;&#21363;&#36890;&#36807;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25991;&#26412;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#38169;&#20301;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#20013;&#20013;&#22870;&#24425;&#31080;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#21457;&#29616;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#65292;&#24182;&#29992;&#37325;&#25972;&#21270;&#32676;&#29702;&#35770;&#20998;&#26512;&#20102;&#36825;&#20004;&#20010;&#31995;&#32479;&#30340;&#26222;&#36941;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09863</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#20013;&#20013;&#22870;&#24425;&#31080;&#30340;&#21487;&#36716;&#31227;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transferability of Winning Lottery Tickets in Neural Network Differential Equation Solvers. (arXiv:2306.09863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#20013;&#20013;&#22870;&#24425;&#31080;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#21457;&#29616;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#65292;&#24182;&#29992;&#37325;&#25972;&#21270;&#32676;&#29702;&#35770;&#20998;&#26512;&#20102;&#36825;&#20004;&#20010;&#31995;&#32479;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37325;&#25972;&#21270;&#32676;&#29702;&#35770;&#26159;&#36890;&#36807;&#36845;&#20195;&#24133;&#20540;&#21098;&#26525;&#26469;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#36807;&#31243;&#30340;&#26377;&#29992;&#26694;&#26550;&#12290;&#26412;&#25991;&#27491;&#24335;&#25551;&#36848;&#20102;RG&#29702;&#35770;&#19982;IMP&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#23558;&#20043;&#21069;&#20851;&#20110;&#24425;&#31080;&#20551;&#35774;&#21644;&#24377;&#24615;&#24425;&#31080;&#20551;&#35774;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#29992;&#20110;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#20004;&#20010;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#20013;&#22870;&#24425;&#31080;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#31995;&#32479;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#20854;&#20013;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#31215;&#20998;&#26102;&#38388;&#12290;&#28982;&#21518;&#20351;&#29992;RG&#30340;&#24037;&#20855;&#20998;&#26512;&#20102;&#36825;&#20004;&#20010;&#31995;&#32479;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that renormalisation group theory is a useful framework with which to describe the process of pruning neural networks via iterative magnitude pruning. This report formally describes the link between RG theory and IMP and extends previous results around the Lottery Ticket Hypothesis and Elastic Lottery Hypothesis to Hamiltonian Neural Networks for solving differential equations. We find lottery tickets for two Hamiltonian Neural Networks and demonstrate transferability between the two systems, with accuracy being dependent on integration times. The universality of the two systems is then analysed using tools from an RG perspective.
&lt;/p&gt;</description></item><item><title>DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.09862</link><description>&lt;p&gt;
DoubleAdapt&#65306;&#19968;&#31181;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09862
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#26159;&#37327;&#21270;&#25237;&#36164;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#20934;&#30830;&#39044;&#27979;&#20215;&#26684;&#36235;&#21183;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20316;&#20026;&#19968;&#39033;&#22312;&#32447;&#26381;&#21153;&#65292;&#32929;&#31080;&#25968;&#25454;&#38543;&#26102;&#38543;&#22320;&#25345;&#32493;&#21040;&#36798;&#12290;&#20351;&#29992;&#26368;&#26032;&#25968;&#25454;&#23545;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#26356;&#26032;&#26159;&#23454;&#29992;&#32780;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#26032;&#25968;&#25454;&#21487;&#33021;&#25581;&#31034;&#20102;&#26410;&#26469;&#32929;&#31080;&#24066;&#22330;&#20013;&#20250;&#37325;&#22797;&#20986;&#29616;&#30340;&#19968;&#20123;&#26032;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#24067;&#28418;&#31227;&#65288;&#21363;&#27010;&#24565;&#28418;&#31227;&#65289;&#30340;&#25361;&#25112;&#65292;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#38543;&#30528;&#32929;&#31080;&#24066;&#22330;&#21160;&#24577;&#28436;&#21464;&#65292;&#26410;&#26469;&#25968;&#25454;&#30340;&#20998;&#24067;&#21487;&#33021;&#20250;&#19982;&#22686;&#37327;&#25968;&#25454;&#31245;&#24494;&#25110;&#26174;&#30528;&#22320;&#19981;&#21516;&#65292;&#20174;&#32780;&#38459;&#30861;&#22686;&#37327;&#26356;&#26032;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#20004;&#20010;&#36866;&#37197;&#22120;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#8212;&#8212;DoubleAdapt&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distri
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#19981;&#33021;&#22815;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2306.09850</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#19981;&#33021;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima. (arXiv:2306.09850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#19981;&#33021;&#22815;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;(SAM)&#26159;&#19968;&#31181;&#20248;&#21270;&#22120;&#65292;&#23427;&#22522;&#20110;&#24403;&#21069;&#28857;$x_t$&#30340;&#26799;&#24230;&#65292;&#22312;&#25200;&#21160;$y_t=x_t+\rho\frac{\nabla f(x_t)}{\lVert\nabla f(x_t)\rVert}$&#22788;&#36827;&#34892;&#19979;&#38477;&#12290;&#29616;&#26377;&#30740;&#31350;&#35777;&#26126;&#20102;SAM&#23545;&#20110;&#24179;&#28369;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#20551;&#35774;&#25200;&#21160;&#30340;&#22823;&#23567;$\rho$&#36880;&#28176;&#34928;&#20943;&#21644;/&#25110;&#22312;$y_t$&#20013;&#27809;&#26377;&#26799;&#24230;&#24402;&#19968;&#21270;&#65292;&#36825;&#19982;&#23454;&#36341;&#19981;&#31526;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#23454;&#29992;&#37197;&#32622;&#65288;&#21363;&#24120;&#25968;$\rho$&#21644;$y_t$&#20013;&#30340;&#26799;&#24230;&#24402;&#19968;&#21270;&#65289;&#30340;&#30830;&#23450;&#24615;/&#38543;&#26426;&#29256;&#26412;&#30340;SAM&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#65288;&#38750;&#65289;&#20984;&#24615;&#20551;&#35774;&#30340;&#24179;&#28369;&#20989;&#25968;&#19978;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;SAM&#22312;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#25110;&#31283;&#23450;&#28857;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#24179;&#28369;&#24378;&#20984;&#20989;&#25968;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30830;&#23450;&#24615;SAM&#20855;&#26377;&#20005;&#26684;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#20026;$\tilde\Theta(\frac{1}{T^2})$&#65292;&#32780;&#38543;&#26426;SAM&#30340;&#25910;&#25947;&#30028;&#21017;&#21463;&#21040;&#22122;&#22768;&#27700;&#24179;&#38477;&#20302;&#30340;&#24433;&#21709;&#65292;&#36825;&#34920;&#26126;&#20102;&#24179;&#38754;&#30446;&#26631;&#34920;&#38754;&#30340;&#23574;&#38160;&#24230;&#21644;&#24179;&#32531;&#24615;&#20043;&#38388;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation $y_t = x_t + \rho \frac{\nabla f(x_t)}{\lVert \nabla f(x_t) \rVert}$ of the current point $x_t$. Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size $\rho$ and/or no gradient normalization in $y_t$, which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant $\rho$ and gradient normalization in $y_t$) and explore their convergence properties on smooth functions with (non)convexity assumptions. Perhaps surprisingly, in many scenarios, we find out that SAM has limited capability to converge to global minima or stationary points. For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of $\tilde \Theta(\frac{1}{T^2})$, the convergence bound of stochastic SAM suffer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;Wasserstein&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#20998;&#24067;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#23545;&#25239;&#25915;&#20987;&#31639;&#27861;&#21450;&#20854;&#22810;&#27493;&#29256;&#26412;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.09844</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;Wasserstein&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Wasserstein distributional robustness of neural networks. (arXiv:2306.09844v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;Wasserstein&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#20998;&#24067;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#23545;&#25239;&#25915;&#20987;&#31639;&#27861;&#21450;&#20854;&#22810;&#27493;&#29256;&#26412;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#30693;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;(Adversarial Attacks, AA)&#30340;&#23041;&#32961;&#12290;&#23545;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#32780;&#35328;&#65292;&#36825;&#24847;&#21619;&#30528;&#23545;&#21407;&#22987;&#22270;&#20687;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#23601;&#26377;&#21487;&#33021;&#23548;&#33268;&#20854;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#23545;&#25239;&#25915;&#20987;&#20197;&#21450;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#27861;&#26159;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;Wasserstein&#20998;&#24067;&#40065;&#26834;&#24615;&#20248;&#21270;&#25216;&#26415;&#37325;&#26032;&#26500;&#24605;&#35813;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;DRO&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#26032;&#35265;&#35299;&#36827;&#34892;&#20102;&#26032;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#32452;&#20998;&#24067;&#23041;&#32961;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#28857;&#23545;&#28857;&#25915;&#20987;&#19981;&#21516;&#30340;&#26159;&#65292;&#20998;&#24067;&#23041;&#32961;&#27169;&#22411;&#20801;&#35768;&#25915;&#20987;&#32773;&#20197;&#38750;&#22343;&#21248;&#30340;&#26041;&#24335;&#25200;&#21160;&#36755;&#20837;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26356;&#19968;&#33324;&#30340;&#25915;&#20987;&#19982;&#26679;&#26412;&#22806;&#24615;&#33021;&#21644;Knightian&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#32852;&#31995;&#36215;&#26469;&#12290;&#20026;&#20102;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#38454;AA&#31639;&#27861;&#21450;&#20854;&#22810;&#27493;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are known to be vulnerable to adversarial attacks (AA). For an image recognition task, this means that a small perturbation of the original can result in the image being misclassified. Design of such attacks as well as methods of adversarial training against them are subject of intense research. We re-cast the problem using techniques of Wasserstein distributionally robust optimization (DRO) and obtain novel contributions leveraging recent insights from DRO sensitivity analysis. We consider a set of distributional threat models. Unlike the traditional pointwise attacks, which assume a uniform bound on perturbation of each input data point, distributional threat models allow attackers to perturb inputs in a non-uniform way. We link these more general attacks with questions of out-of-sample performance and Knightian uncertainty. To evaluate the distributional robustness of neural networks, we propose a first-order AA algorithm and its multi-step version. Our attack a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#20559;&#24046;&#26102;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#20860;&#39038;&#20844;&#24179;&#30340;&#21069;&#25552;&#19979;&#65292;&#20351;&#25152;&#36873;&#30340;&#23376;&#38598;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#26469;&#25552;&#39640;&#36873;&#25321;&#36136;&#37327;&#65292;&#19981;&#21516;&#30340;&#22810;&#36194;&#23478;&#35780;&#20998;&#20989;&#25968;&#23545;&#20110;&#20844;&#24179;&#24615;&#30340;&#20381;&#36182;&#24615;&#21487;&#33021;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.09835</link><description>&lt;p&gt;
&#23384;&#22312;&#20559;&#24046;&#26102;&#22522;&#20110;&#22810;&#20010;&#25490;&#21517;&#30340;&#23376;&#38598;&#36873;&#25321;&#65306;&#22810;&#36194;&#23478;&#25237;&#31080;&#35780;&#20998;&#20989;&#25968;&#30340;&#20844;&#24179;&#38480;&#21046;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Subset Selection Based On Multiple Rankings in the Presence of Bias: Effectiveness of Fairness Constraints for Multiwinner Voting Score Functions. (arXiv:2306.09835v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#20559;&#24046;&#26102;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#20860;&#39038;&#20844;&#24179;&#30340;&#21069;&#25552;&#19979;&#65292;&#20351;&#25152;&#36873;&#30340;&#23376;&#38598;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#26469;&#25552;&#39640;&#36873;&#25321;&#36136;&#37327;&#65292;&#19981;&#21516;&#30340;&#22810;&#36194;&#23478;&#35780;&#20998;&#20989;&#25968;&#23545;&#20110;&#20844;&#24179;&#24615;&#30340;&#20381;&#36182;&#24615;&#21487;&#33021;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#20854;&#20013;&#32473;&#23450;&#22810;&#20010;&#39033;&#30446;&#30340;&#25490;&#21517;&#65292;&#30446;&#26631;&#26159;&#36873;&#25321;&#26368;&#39640;&#8220;&#36136;&#37327;&#8221;&#30340;&#23376;&#38598;&#12290;&#26469;&#33258;&#22810;&#36194;&#23478;&#25237;&#31080;&#25991;&#29486;&#30340;&#35780;&#20998;&#20989;&#25968;&#24050;&#29992;&#20110;&#23558;&#25490;&#21517;&#32858;&#21512;&#20026;&#23376;&#38598;&#30340;&#36136;&#37327;&#24471;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27492;&#35774;&#32622;&#19979;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#24403;&#25490;&#21517;&#21487;&#33021;&#21253;&#21547;&#23545;&#19968;&#32452;&#39033;&#30446;&#30340;&#31995;&#32479;&#24615;&#25110;&#26080;&#24847;&#35782;&#30340;&#20559;&#35265;&#26102;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#20860;&#39038;&#20844;&#24179;&#30340;&#21069;&#25552;&#19979;&#65292;&#25552;&#39640;&#36873;&#25321;&#36136;&#37327;&#12290;&#23545;&#20110;&#36755;&#20837;&#25490;&#21517;&#21644;&#20559;&#35265;&#30340;&#19968;&#33324;&#27169;&#22411;&#65292;&#25105;&#20204;&#34920;&#26126;&#35201;&#27714;&#25152;&#36873;&#30340;&#23376;&#38598;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#25552;&#39640;&#36873;&#25321;&#22312;&#27809;&#26377;&#20559;&#35265;&#30340;&#25490;&#21517;&#20013;&#30340;&#36136;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#20110;&#20844;&#24179;&#24615;&#38480;&#21046;&#35201;&#26377;&#25928;&#65292;&#19981;&#21516;&#30340;&#22810;&#36194;&#23478;&#35780;&#20998;&#20989;&#25968;&#21487;&#33021;&#38656;&#35201;&#26497;&#20854;&#19981;&#21516;&#25968;&#37327;&#30340;&#25490;&#21517;&#65306;&#23545;&#20110;&#19968;&#20123;&#20989;&#25968;&#65292;&#20844;&#24179;&#24615;&#38480;&#21046;&#38656;&#35201;&#25351;&#25968;&#32423;&#25968;&#37327;&#30340;&#25490;&#21517;&#25165;&#33021;&#24674;&#22797;&#25509;&#36817;&#26368;&#20248;&#35299;&#65292;&#32780;&#23545;&#20110;&#20854;&#20182;&#20989;&#25968;&#65292;&#36825;&#31181;&#20381;&#36182;&#24615;&#20165;&#20026;&#22810;&#39033;&#24335;&#12290;&#36825;&#20010;&#32467;&#26524;&#20381;&#36182;&#20110;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of subset selection where one is given multiple rankings of items and the goal is to select the highest ``quality'' subset. Score functions from the multiwinner voting literature have been used to aggregate rankings into quality scores for subsets. We study this setting of subset selection problems when, in addition, rankings may contain systemic or unconscious biases toward a group of items. For a general model of input rankings and biases, we show that requiring the selected subset to satisfy group fairness constraints can improve the quality of the selection with respect to unbiased rankings. Importantly, we show that for fairness constraints to be effective, different multiwinner score functions may require a drastically different number of rankings: While for some functions, fairness constraints need an exponential number of rankings to recover a close-to-optimal solution, for others, this dependency is only polynomial. This result relies on a novel notion 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22797;&#25968;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#26500;&#24314;&#22359;&#65292;&#20197;&#20415;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21040;&#22788;&#29702;&#22797;&#25968;&#20449;&#21495;&#25110;&#22270;&#20687;&#19978;&#65292;&#22312;&#19981;&#20351;&#29992;&#25237;&#24433;&#21040; $ \mathbb {R} ^2 $ &#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#22810;&#20010;&#29256;&#26412;&#30340;&#22797;&#25968;&#32553;&#25918;&#28857;&#31215;&#27880;&#24847;&#26426;&#21046;&#21644;&#22797;&#25968;&#23618;&#24402;&#19968;&#21270;&#65292;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;MusicNet&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#21644;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09827</link><description>&lt;p&gt;
&#22797;&#25968;&#21464;&#21387;&#22120;&#20307;&#31995;&#32467;&#26500;&#30340;&#26500;&#24314;&#22359;
&lt;/p&gt;
&lt;p&gt;
Building Blocks for a Complex-Valued Transformer Architecture. (arXiv:2306.09827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22797;&#25968;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#26500;&#24314;&#22359;&#65292;&#20197;&#20415;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21040;&#22788;&#29702;&#22797;&#25968;&#20449;&#21495;&#25110;&#22270;&#20687;&#19978;&#65292;&#22312;&#19981;&#20351;&#29992;&#25237;&#24433;&#21040; $ \mathbb {R} ^2 $ &#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#22810;&#20010;&#29256;&#26412;&#30340;&#22797;&#25968;&#32553;&#25918;&#28857;&#31215;&#27880;&#24847;&#26426;&#21046;&#21644;&#22797;&#25968;&#23618;&#24402;&#19968;&#21270;&#65292;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;MusicNet&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#21644;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#37117;&#26159;&#24314;&#31435;&#22312;&#22788;&#29702;&#23454;&#20540;&#36755;&#20837;&#65288;&#22914;&#22270;&#20687;&#12289;&#35821;&#38899;&#25110;&#38899;&#20048;&#20449;&#21495;&#65289;&#30340;&#23454;&#20540;&#25805;&#20316;&#20043;&#19978;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#33258;&#28982;&#20351;&#29992;&#22797;&#20540;&#20449;&#21495;&#25110;&#22270;&#20687;&#65292;&#20363;&#22914;MRI&#25110;&#36965;&#24863;&#12290;&#27492;&#22806;&#65292;&#20449;&#21495;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#26159;&#22797;&#20540;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#25105;&#20204;&#26088;&#22312;&#20351;&#28145;&#24230;&#23398;&#20064;&#30452;&#25509;&#36866;&#29992;&#20110;&#36825;&#20123;&#22797;&#20540;&#20449;&#21495;&#65292;&#32780;&#19981;&#20351;&#29992;&#23545; $ \mathbb {R} ^2 $ &#30340;&#25237;&#24433;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#24314;&#31435;&#22359;&#65292;&#23558;&#21464;&#21387;&#22120;&#26550;&#26500;&#36716;&#25442;&#21040;&#22797;&#25968;&#22495;&#20013;&#65292;&#26469;&#22686;&#21152;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#36817;&#21457;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20010;&#29256;&#26412;&#30340;&#22797;&#20540;&#32553;&#25918;&#28857;&#31215;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#21450;&#22797;&#20540;&#23618;&#24402;&#19968;&#21270;&#12290;&#25105;&#20204;&#22312;MusicNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#30340;&#27979;&#35797;&#65292;&#24182;&#34920;&#29616;&#20986;&#20102;&#23545;&#36807;&#25311;&#21512;&#30340;&#25913;&#36827;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#19982;&#23454;&#20540;&#21464;&#21387;&#22120;&#20307;&#31995;&#32467;&#26500;&#30456;&#27604;&#20445;&#25345;&#20102;&#21516;&#31561;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most deep learning pipelines are built on real-valued operations to deal with real-valued inputs such as images, speech or music signals. However, a lot of applications naturally make use of complex-valued signals or images, such as MRI or remote sensing. Additionally the Fourier transform of signals is complex-valued and has numerous applications. We aim to make deep learning directly applicable to these complex-valued signals without using projections into $\mathbb{R}^2$. Thus we add to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain. We present multiple versions of a complex-valued Scaled Dot-Product Attention mechanism as well as a complex-valued layer normalization. We test on a classification and a sequence generation task on the MusicNet dataset and show improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;FALL-E&#8221;&#30340;&#20315;&#21033;&#38899;&#25928;&#21512;&#25104;&#31995;&#32479;&#21450;&#20854;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26465;&#20214;&#35757;&#32451;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#36755;&#20837;&#20102;&#35299;&#22768;&#38899;&#36136;&#37327;&#21644;&#24405;&#38899;&#29615;&#22659;&#65292;&#24182;&#22312;DCASE 2023&#25361;&#25112;&#36187;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#22810;&#26679;&#24615;&#19978;&#24471;&#20998;&#26368;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.09807</link><description>&lt;p&gt;
FALL-E&#65306;&#19968;&#31181;&#20315;&#21033;&#38899;&#25928;&#21512;&#25104;&#27169;&#22411;&#21450;&#20854;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
FALL-E: A Foley Sound Synthesis Model and Strategies. (arXiv:2306.09807v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;FALL-E&#8221;&#30340;&#20315;&#21033;&#38899;&#25928;&#21512;&#25104;&#31995;&#32479;&#21450;&#20854;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26465;&#20214;&#35757;&#32451;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#36755;&#20837;&#20102;&#35299;&#22768;&#38899;&#36136;&#37327;&#21644;&#24405;&#38899;&#29615;&#22659;&#65292;&#24182;&#22312;DCASE 2023&#25361;&#25112;&#36187;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#22810;&#26679;&#24615;&#19978;&#24471;&#20998;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;FALL-E&#8221;&#30340;&#20315;&#21033;&#38899;&#25928;&#21512;&#25104;&#31995;&#32479;&#21450;&#20854;&#35757;&#32451;/&#25512;&#26029;&#31574;&#30053;&#12290;FALL-E&#27169;&#22411;&#37319;&#29992;&#32423;&#32852;&#26041;&#27861;&#65292;&#21253;&#25324;&#20302;&#20998;&#36776;&#29575;&#39057;&#35889;&#22270;&#29983;&#25104;&#12289;&#39057;&#35889;&#22270;&#36229;&#20998;&#36776;&#29575;&#21644;&#22768;&#30721;&#22120;&#12290;&#25105;&#20204;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#27599;&#20010;&#19982;&#22768;&#38899;&#30456;&#20851;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#38598;&#29305;&#23450;&#30340;&#25991;&#26412;&#23558;&#27169;&#22411;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#36755;&#20837;&#20102;&#35299;&#22768;&#38899;&#36136;&#37327;&#21644;&#24405;&#38899;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#36827;&#34892;&#20102;&#36136;&#37327;&#12289;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;FALL-E&#22312;DCASE 2023&#25361;&#25112;&#36187;&#20219;&#21153;7&#20013;&#36827;&#34892;&#20102;&#23458;&#35266;&#35780;&#20272;&#21644;&#21548;&#21147;&#27979;&#35797;&#12290;&#25552;&#20132;&#32467;&#26524;&#22312;&#24179;&#22343;&#24471;&#20998;&#19978;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#21516;&#26102;&#22312;&#22810;&#26679;&#24615;&#19978;&#24471;&#20998;&#26368;&#39640;&#65292;&#22312;&#38899;&#39057;&#36136;&#37327;&#19978;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#22312;&#31867;&#36866;&#24212;&#24615;&#19978;&#33719;&#24471;&#31532;&#19977;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FALL-E, a foley synthesis system and its training/inference strategies. The FALL-E model employs a cascaded approach comprising low-resolution spectrogram generation, spectrogram super-resolution, and a vocoder. We trained every sound-related model from scratch using our extensive datasets, and utilized a pre-trained language model. We conditioned the model with dataset-specific texts, enabling it to learn sound quality and recording environment based on text input. Moreover, we leveraged external language models to improve text descriptions of our datasets and performed prompt engineering for quality, coherence, and diversity. FALL-E was evaluated by an objective measure as well as listening tests in the DCASE 2023 challenge Task 7. The submission achieved the second place on average, while achieving the best score for diversity, second place for audio quality, and third place for class fitness.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SEILO&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#26631;&#20934;&#30340;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21160;&#21147;&#23398;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20174;&#26080;&#19987;&#23478;&#25968;&#25454;&#30340;&#35266;&#27979;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#27169;&#20223;&#23398;&#20064;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#24182;&#23454;&#29616;&#20102;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.09805</link><description>&lt;p&gt;
&#35266;&#27979;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient On-Policy Imitation Learning from Observations. (arXiv:2306.09805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SEILO&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#26631;&#20934;&#30340;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21160;&#21147;&#23398;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20174;&#26080;&#19987;&#23478;&#25968;&#25454;&#30340;&#35266;&#27979;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#27169;&#20223;&#23398;&#20064;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#24182;&#23454;&#29616;&#20102;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#65292;&#27169;&#20223;&#23398;&#20064; (ILD) &#26088;&#22312;&#36890;&#36807;&#28040;&#38500;&#24378;&#21270;&#23398;&#20064;&#30340;&#35768;&#22810;&#32570;&#28857;&#26469;&#24110;&#21161;&#23398;&#20064;&#36755;&#20986;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#32570;&#20047;&#19987;&#23478;&#34892;&#21160;&#25351;&#23548;&#65292;&#22240;&#27492;&#26080;&#27861;&#20351;&#29992;ILD&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#32771;&#34385;&#35266;&#27979;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064; (ILO)&#65292;&#20854;&#20013;&#27809;&#26377;&#25552;&#20379;&#19987;&#23478;&#21160;&#20316;&#65292;&#20351;&#20854;&#25104;&#20026;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#31574;&#30053;&#23398;&#20064;&#65292;&#36825;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#25104;&#26412;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; SEILO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#31639;&#27861;&#65292;&#29992;&#20110; ILO&#65292;&#23558;&#26631;&#20934;&#30340;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#19982;&#36870;&#21160;&#21147;&#23398;&#24314;&#27169;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#23545;&#25239;&#31243;&#24207;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#20013;&#33719;&#24471;&#21453;&#39304;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31574;&#30053; ILO &#21644; ILD &#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#38656;&#35201;&#36739;&#23569;&#30340;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23454;&#29616;&#19987;&#23478;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning from demonstrations (ILD) aims to alleviate numerous shortcomings of reinforcement learning through the use of demonstrations. However, in most real-world applications, expert action guidance is absent, making the use of ILD impossible. Instead, we consider imitation learning from observations (ILO), where no expert actions are provided, making it a significantly more challenging problem to address. Existing methods often employ on-policy learning, which is known to be sample-costly. This paper presents SEILO, a novel sample-efficient on-policy algorithm for ILO, that combines standard adversarial imitation learning with inverse dynamics modeling. This approach enables the agent to receive feedback from both the adversarial procedure and a behavior cloning loss. We empirically demonstrate that our proposed algorithm requires fewer interactions with the environment to achieve expert performance compared to other state-of-the-art on-policy ILO and ILD methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#21644;&#22522;&#20934;&#65292;&#29992;&#20110;&#32452;&#21512;&#21644;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#25552;&#20379;&#22810;&#26679;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#27492;&#26694;&#26550;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;4&#31181;&#24120;&#35265;&#30340;MCBO&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.09803</link><description>&lt;p&gt;
&#32452;&#21512;&#21644;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26694;&#26550;&#21644;&#22522;&#20934;&#12290; (arXiv:2306.09803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization. (arXiv:2306.09803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#21644;&#22522;&#20934;&#65292;&#29992;&#20110;&#32452;&#21512;&#21644;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#25552;&#20379;&#22810;&#26679;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#27492;&#26694;&#26550;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;4&#31181;&#24120;&#35265;&#30340;MCBO&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#28151;&#21512;&#21464;&#37327;&#21644;&#32452;&#21512;&#36125;&#21494;&#26031;&#20248;&#21270;(MCBO)&#26469;&#35299;&#20915;&#39046;&#22495;&#20013;&#32570;&#20047;&#31995;&#32479;&#21270;&#22522;&#20934;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;MCBO&#35770;&#25991;&#36890;&#24120;&#24341;&#20837;&#38750;&#22810;&#26679;&#24615;&#25110;&#38750;&#26631;&#20934;&#22522;&#20934;&#26469;&#35780;&#20272;&#20854;&#26041;&#27861;&#65292;&#38459;&#30861;&#20102;&#19981;&#21516;MCBO&#21407;&#35821;&#21450;&#20854;&#32452;&#21512;&#30340;&#27491;&#30830;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#20171;&#32461;&#21333;&#20010;MCBO&#21407;&#35821;&#30340;&#35770;&#25991;&#36890;&#24120;&#30465;&#30053;&#20102;&#38024;&#23545;&#20351;&#29992;&#30456;&#21516;&#26041;&#27861;&#36827;&#34892;&#21097;&#20313;&#21407;&#35821;&#30340;&#22522;&#32447;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#31181;&#30465;&#30053;&#20027;&#35201;&#26159;&#30001;&#20110;&#28041;&#21450;&#30340;&#23454;&#29616;&#24037;&#20316;&#37327;&#38750;&#24120;&#22823;&#65292;&#23548;&#33268;&#32570;&#20047;&#25511;&#21046;&#35780;&#20272;&#24182;&#26080;&#27861;&#26377;&#25928;&#23637;&#31034;&#36129;&#29486;&#30340;&#20248;&#28857;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20351;&#36125;&#21494;&#26031;&#20248;&#21270;&#32452;&#20214;&#30340;&#32452;&#21512;&#36731;&#26494;&#26131;&#34892;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#26679;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#12290;&#21033;&#29992;&#36825;&#31181;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;4&#31181;&#24120;&#35265;&#30340;MCBO&#25216;&#26415;&#65292;&#24182;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a modular framework for Mixed-variable and Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking and standardized evaluation in the field. Current MCBO papers often introduce non-diverse or non-standard benchmarks to evaluate their methods, impeding the proper assessment of different MCBO primitives and their combinations. Additionally, papers introducing a solution for a single MCBO primitive often omit benchmarking against baselines that utilize the same methods for the remaining primitives. This omission is primarily due to the significant implementation overhead involved, resulting in a lack of controlled assessments and an inability to showcase the merits of a contribution effectively. To overcome these challenges, our proposed framework enables an effortless combination of Bayesian Optimization components, and provides a diverse set of synthetic and real-world benchmarking tasks. Leveraging this flexibility, we implement 4
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$\pi2\text{vec}$&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#40657;&#30418;&#31574;&#30053;&#34892;&#20026;&#34920;&#31034;&#20026;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#20026;&#29616;&#20195;&#30740;&#31350;&#26041;&#21521;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12289;&#22522;&#30784;&#27169;&#22411;&#29366;&#24577;&#34920;&#31034;&#21644;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#31574;&#30053;&#36873;&#25321;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.09800</link><description>&lt;p&gt;
$\pi2\text{vec}$&#65306;&#22522;&#20110;&#32487;&#25215;&#29305;&#24449;&#30340;&#31574;&#30053;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
$\pi2\text{vec}$: Policy Representations with Successor Features. (arXiv:2306.09800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\pi2\text{vec}$&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#40657;&#30418;&#31574;&#30053;&#34892;&#20026;&#34920;&#31034;&#20026;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#20026;&#29616;&#20195;&#30740;&#31350;&#26041;&#21521;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12289;&#22522;&#30784;&#27169;&#22411;&#29366;&#24577;&#34920;&#31034;&#21644;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#31574;&#30053;&#36873;&#25321;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;$\pi2\text{vec}$&#65292;&#19968;&#31181;&#23558;&#40657;&#30418;&#31574;&#30053;&#34892;&#20026;&#34920;&#31034;&#20026;&#29305;&#24449;&#21521;&#37327;&#30340;&#26041;&#27861;&#12290;&#31574;&#30053;&#34920;&#31034;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#25429;&#25417;&#20102;&#22522;&#30784;&#27169;&#22411;&#29305;&#24449;&#32479;&#35745;&#25968;&#25454;&#22312;&#31574;&#30053;&#34892;&#20026;&#21709;&#24212;&#20013;&#30340;&#21464;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20026;&#34701;&#21512;&#19977;&#20010;&#29616;&#20195;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25903;&#25345;&#65306;&#20316;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#34917;&#20805;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#20316;&#20026;&#36890;&#29992;&#24378;&#22823;&#29366;&#24577;&#34920;&#31034;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#31574;&#30053;&#36873;&#25321;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes $\pi2\text{vec}$, a method for representing behaviors of black box policies as feature vectors. The policy representations capture how the statistics of foundation model features change in response to the policy behavior in a task agnostic way, and can be trained from offline data, allowing them to be used in offline policy selection. This work provides a key piece of a recipe for fusing together three modern lines of research: Offline policy evaluation as a counterpart to offline RL, foundation models as generic and powerful state representations, and efficient policy selection in resource constrained environments.
&lt;/p&gt;</description></item><item><title>GPINN&#26159;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22270;&#24418;&#32467;&#26500;&#20013;&#25191;&#34892;PINN&#65292;&#23558;&#25299;&#25169;&#25968;&#25454;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#20013;&#36827;&#34892;&#38382;&#39064;&#27714;&#35299;&#65292;&#21033;&#29992;&#22270;&#23884;&#20837;&#25216;&#26415;&#27880;&#20837;&#39069;&#22806;&#30340;&#32500;&#24230;&#65292;&#20197;&#23553;&#35013;&#22270;&#30340;&#31354;&#38388;&#29305;&#24449;&#24182;&#20445;&#30041;&#21407;&#22987;&#31354;&#38388;&#30340;&#23646;&#24615;&#12290;GPINN&#27604;&#20256;&#32479;&#30340;PINN&#24615;&#33021;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#25429;&#25417;&#35299;&#30340;&#29289;&#29702;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#26356;&#21152;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.09792</link><description>&lt;p&gt;
GPINN: &#24102;&#26377;&#22270;&#23884;&#20837;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GPINN: Physics-informed Neural Network with Graph Embedding. (arXiv:2306.09792v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09792
&lt;/p&gt;
&lt;p&gt;
GPINN&#26159;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22270;&#24418;&#32467;&#26500;&#20013;&#25191;&#34892;PINN&#65292;&#23558;&#25299;&#25169;&#25968;&#25454;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#20013;&#36827;&#34892;&#38382;&#39064;&#27714;&#35299;&#65292;&#21033;&#29992;&#22270;&#23884;&#20837;&#25216;&#26415;&#27880;&#20837;&#39069;&#22806;&#30340;&#32500;&#24230;&#65292;&#20197;&#23553;&#35013;&#22270;&#30340;&#31354;&#38388;&#29305;&#24449;&#24182;&#20445;&#30041;&#21407;&#22987;&#31354;&#38388;&#30340;&#23646;&#24615;&#12290;GPINN&#27604;&#20256;&#32479;&#30340;PINN&#24615;&#33021;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#25429;&#25417;&#35299;&#30340;&#29289;&#29702;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#26356;&#21152;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#24102;&#26377;&#22270;&#23884;&#20837;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65288;GPINN&#65289;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#32467;&#26500;&#20013;&#25191;&#34892;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#12290;&#35813;&#26041;&#27861;&#23558;&#25299;&#25169;&#25968;&#25454;&#34701;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102; PINN &#30340;&#24615;&#33021;&#12290;&#22270;&#23884;&#20837;&#25216;&#26415;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#27880;&#20837;&#20102;&#39069;&#22806;&#30340;&#32500;&#24230;&#65292;&#20197;&#23553;&#35013;&#22270;&#30340;&#31354;&#38388;&#29305;&#24449;&#24182;&#20445;&#30041;&#21407;&#22987;&#31354;&#38388;&#30340;&#23646;&#24615;&#12290;&#36825;&#20123;&#39069;&#22806;&#30340;&#32500;&#24230;&#30340;&#36873;&#25321;&#30001; Fiedler &#21521;&#37327;&#24341;&#23548;&#65292;&#25552;&#20379;&#20102;&#22270;&#30340;&#26368;&#20248;&#24322;&#36335;&#24452;&#31526;&#21495;&#12290;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034; GPINN &#22312;&#24615;&#33021;&#19978;&#30456;&#27604;&#20256;&#32479;&#30340; PINN &#26377;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#29305;&#21035;&#26159;&#22312;&#25429;&#25417;&#35299;&#30340;&#29289;&#29702;&#29305;&#24449;&#26041;&#38754;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a Physics-informed Neural Network framework with Graph Embedding (GPINN) to perform PINN in graph, i.e. topological space instead of traditional Euclidean space, for improved problem-solving efficiency. The method integrates topological data into the neural network's computations, which significantly boosts the performance of the Physics-Informed Neural Network (PINN). The graph embedding technique infuses extra dimensions into the input space to encapsulate the spatial characteristics of a graph while preserving the properties of the original space. The selection of these extra dimensions is guided by the Fiedler vector, offering an optimised pathologic notation of the graph. Two case studies are conducted, which demonstrate significant improvement in the performance of GPINN in comparison to traditional PINN, particularly in its superior ability to capture physical features of the solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23548;&#20986;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#19968;&#38454;ODR&#65292;&#25551;&#36848;&#20102;&#20854;&#26368;&#20248;&#26435;&#34913;&#26354;&#32447;&#30340;&#21160;&#24577;&#65292;&#25581;&#31034;&#20102;&#26368;&#20248;&#32534;&#30721;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.09790</link><description>&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65306;&#22522;&#20110;&#19968;&#38454;&#26681;&#36712;&#36857;&#30340;&#20449;&#24687;&#29942;&#39048;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Information Bottleneck's Ordinary Differential Equation: First-Order Root-Tracking for the IB. (arXiv:2306.09790v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23548;&#20986;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#19968;&#38454;ODR&#65292;&#25551;&#36848;&#20102;&#20854;&#26368;&#20248;&#26435;&#34913;&#26354;&#32447;&#30340;&#21160;&#24577;&#65292;&#25581;&#31034;&#20102;&#26368;&#20248;&#32534;&#30721;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#26159;&#19968;&#31181;&#26377;&#25439;&#21387;&#32553;&#26041;&#27861;&#65292;&#20854;&#36895;&#29575;-&#22833;&#30495;&#26354;&#32447;&#25551;&#36848;&#20102;&#36755;&#20837;&#21387;&#32553;&#21644;&#30456;&#20851;&#20449;&#24687;&#20445;&#30041;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#23427;&#25513;&#30422;&#20102;&#26368;&#20248;&#36755;&#20837;&#32534;&#30721;&#30340;&#22522;&#26412;&#21160;&#24577;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#38543;&#30528;&#36755;&#20837;&#20449;&#24687;&#30340;&#21387;&#32553;&#65292;&#36825;&#20123;&#21160;&#24577;&#36890;&#24120;&#36981;&#24490;&#20998;&#27573;&#20809;&#28369;&#30340;&#36712;&#36857;&#65292;&#22914;&#26368;&#36817;&#22312;RD&#20013;&#25152;&#31034;&#12290;&#36825;&#20123;&#20809;&#28369;&#30340;&#21160;&#24577;&#22312;&#26368;&#20248;&#32534;&#30721;&#21457;&#29983;&#23450;&#24615;&#21464;&#21270;&#65288;&#22312;&#20998;&#21449;&#22788;&#65289;&#26102;&#20250;&#20013;&#26029;&#12290;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#19982;RD&#30340;&#23494;&#20999;&#20851;&#31995;&#65292;&#21487;&#20197;&#30475;&#21040;&#27425;&#20248;&#35299;&#22312;&#37027;&#37324;&#21457;&#29983;&#30896;&#25758;&#25110;&#20132;&#25442;&#26368;&#20248;&#24615;&#12290;&#23613;&#31649;&#20449;&#24687;&#29942;&#39048;&#21450;&#20854;&#24212;&#29992;&#24050;&#34987;&#25509;&#21463;&#65292;&#20294;&#24778;&#20154;&#22320;&#32570;&#20047;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#25968;&#20540;&#25216;&#26415;&#65292;&#21363;&#20351;&#23545;&#20110;&#24050;&#30693;&#20998;&#24067;&#30340;&#26377;&#38480;&#38382;&#39064;&#12290;&#25105;&#20204;&#37325;&#26032;&#23548;&#20986;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#25551;&#36848;&#20102;&#20854;&#26368;&#20248;&#26435;&#34913;&#26354;&#32447;&#30340;&#22522;&#26412;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Information Bottleneck (IB) is a method of lossy compression. Its rate-distortion (RD) curve describes the fundamental tradeoff between input compression and the preservation of relevant information. However, it conceals the underlying dynamics of optimal input encodings. We argue that these typically follow a piecewise smooth trajectory as the input information is being compressed, as recently shown in RD. These smooth dynamics are interrupted when an optimal encoding changes qualitatively, at a bifurcation. By leveraging the IB's intimate relations with RD, sub-optimal solutions can be seen to collide or exchange optimality there.  Despite the acceptance of the IB and its applications, there are surprisingly few techniques to solve it numerically, even for finite problems whose distribution is known. We derive anew the IB's first-order Ordinary Differential Equation, which describes the dynamics underlying its optimal tradeoff curve. To exploit these dynamics, one needs not only 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;IoT&#36793;&#32536;&#33410;&#28857;&#19978;&#38750;&#24120;&#36866;&#29992;&#30340;&#21160;&#24577;&#20915;&#31574;&#26641;&#38598;&#25104;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#25191;&#34892;&#26641;&#30340;&#25968;&#37327;&#65292;&#20197;&#26435;&#34913;&#35745;&#31639;&#25104;&#26412;&#21644;&#31934;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09789</link><description>&lt;p&gt;
IoT&#36793;&#32536;&#33410;&#28857;&#19978;&#30340;&#33021;&#28304;&#39640;&#25928;&#25512;&#29702;&#30340;&#21160;&#24577;&#20915;&#31574;&#26641;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dynamic Decision Tree Ensembles for Energy-Efficient Inference on IoT Edge Nodes. (arXiv:2306.09789v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;IoT&#36793;&#32536;&#33410;&#28857;&#19978;&#38750;&#24120;&#36866;&#29992;&#30340;&#21160;&#24577;&#20915;&#31574;&#26641;&#38598;&#25104;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#25191;&#34892;&#26641;&#30340;&#25968;&#37327;&#65292;&#20197;&#26435;&#34913;&#35745;&#31639;&#25104;&#26412;&#21644;&#31934;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;(IoT)&#35774;&#22791;&#30340;&#26222;&#21450;&#65292;&#38656;&#35201;&#33021;&#22815;&#22312;&#21463;&#38480;&#21046;&#30340;&#36793;&#32536;&#33410;&#28857;&#19978;&#36816;&#34892;&#30340;&#33021;&#28304;&#39640;&#25928;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#12290;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;(RFs)&#21644;&#26799;&#24230;&#25552;&#21319;(GBTs)&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#29305;&#21035;&#36866;&#21512;&#65292;&#22240;&#20026;&#23427;&#20204;&#30456;&#23545;&#20110;&#20854;&#20182;&#36873;&#25321;&#20855;&#26377;&#36739;&#20302;&#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#26102;&#38388;&#21644;&#33021;&#37327;&#25104;&#26412;&#23545;&#20110;&#36793;&#32536;&#30828;&#20214;&#20173;&#28982;&#24456;&#22823;&#12290;&#37492;&#20110;&#36825;&#31181;&#25104;&#26412;&#38543;&#30528;&#21512;&#22863;&#35268;&#27169;&#30340;&#32447;&#24615;&#22686;&#38271;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21160;&#24577;&#21512;&#22863;&#65292;&#26681;&#25454;&#24310;&#36831;/&#33021;&#37327;&#30446;&#26631;&#21644;&#22788;&#29702;&#30340;&#36755;&#20837;&#30340;&#22797;&#26434;&#24615;&#35843;&#25972;&#25191;&#34892;&#26641;&#30340;&#25968;&#37327;&#65292;&#20197;&#26435;&#34913;&#35745;&#31639;&#25104;&#26412;&#21644;&#31934;&#24230;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#22810;&#26680;&#20302;&#21151;&#32791;IoT&#35774;&#22791;&#19978;&#37096;&#32626;&#36825;&#20123;&#31639;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#24037;&#20855;&#65292;&#21487;&#23558;Python&#21512;&#22863;&#33258;&#21160;&#36716;&#25442;&#20026;&#20248;&#21270;&#30340;C&#20195;&#30721;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#31181;&#20248;&#21270;&#31574;&#30053;&#65292;&#32771;&#34385;&#20102;&#36825;&#20123;&#35774;&#22791;&#20013;&#21487;&#29992;&#30340;&#24182;&#34892;&#24615;&#21644;&#23384;&#20648;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity of Internet of Things (IoT) devices, there is a growing need for energy-efficient Machine Learning (ML) models that can run on constrained edge nodes. Decision tree ensembles, such as Random Forests (RFs) and Gradient Boosting (GBTs), are particularly suited for this task, given their relatively low complexity compared to other alternatives. However, their inference time and energy costs are still significant for edge hardware. Given that said costs grow linearly with the ensemble size, this paper proposes the use of dynamic ensembles, that adjust the number of executed trees based both on a latency/energy target and on the complexity of the processed input, to trade-off computational cost and accuracy. We focus on deploying these algorithms on multi-core low-power IoT devices, designing a tool that automatically converts a Python ensemble into optimized C code, and exploring several optimizations that account for the available parallelism and memory hier
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#65288;GEL&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#35786;&#26029;&#24037;&#20855;&#26469;&#35782;&#21035;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#35768;&#22810;&#32570;&#38519;&#65292;&#24182;&#32467;&#21512;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#30340;&#25216;&#26415;&#65292;&#21019;&#36896;&#20102;&#20445;&#30041;&#27599;&#20010;&#26679;&#26412;&#21487;&#35299;&#37322;&#24615;&#30340;&#20998;&#24067;&#27979;&#35797;&#65292;&#36824;&#21253;&#25324;&#26631;&#31614;&#20449;&#24687;&#30340;&#25351;&#26631;&#12290;&#36825;&#20123;&#27979;&#35797;&#21487;&#20197;&#39044;&#27979;&#27169;&#24335;&#38477;&#20302;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.09780</link><description>&lt;p&gt;
&#21033;&#29992;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#26041;&#27861;&#29702;&#35299;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Generative Models with Generalized Empirical Likelihoods. (arXiv:2306.09780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#65288;GEL&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#35786;&#26029;&#24037;&#20855;&#26469;&#35782;&#21035;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#35768;&#22810;&#32570;&#38519;&#65292;&#24182;&#32467;&#21512;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#30340;&#25216;&#26415;&#65292;&#21019;&#36896;&#20102;&#20445;&#30041;&#27599;&#20010;&#26679;&#26412;&#21487;&#35299;&#37322;&#24615;&#30340;&#20998;&#24067;&#27979;&#35797;&#65292;&#36824;&#21253;&#25324;&#26631;&#31614;&#20449;&#24687;&#30340;&#25351;&#26631;&#12290;&#36825;&#20123;&#27979;&#35797;&#21487;&#20197;&#39044;&#27979;&#27169;&#24335;&#38477;&#20302;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#25429;&#33719;&#39640;&#32500;&#25968;&#25454;&#20998;&#24067;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#26576;&#20123;&#27169;&#22411;&#31867;&#21035;&#65292;&#27604;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#31561;&#19981;&#20801;&#35768;&#31934;&#30830;&#20284;&#28982;&#24230;&#37327;&#30340;&#27169;&#22411;&#65292;&#23588;&#20854;&#22256;&#38590;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#65288;GEL&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#35786;&#26029;&#24037;&#20855;&#26469;&#35782;&#21035;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#35768;&#22810;&#32570;&#38519;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#30697;&#26465;&#20214;&#35268;&#23450;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#21738;&#20123;&#27169;&#24335;&#34987;&#21024;&#38500;&#12289;DGM&#23384;&#22312;&#30340;&#27169;&#24335;&#19981;&#24179;&#34913;&#31243;&#24230;&#20197;&#21450;DGM&#26159;&#21542;&#36275;&#22815;&#25429;&#33719;&#31867;&#20869;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#32467;&#21512;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#30340;&#25216;&#26415;&#65292;&#19981;&#20165;&#21019;&#36896;&#20102;&#20445;&#30041;&#27599;&#20010;&#26679;&#26412;&#21487;&#35299;&#37322;&#24615;&#30340;&#20998;&#24067;&#27979;&#35797;&#65292;&#36824;&#21253;&#25324;&#26631;&#31614;&#20449;&#24687;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27979;&#35797;&#21487;&#20197;&#39044;&#27979;&#27169;&#24335;&#38477;&#20302;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how well a deep generative model captures a distribution of high-dimensional data remains an important open challenge. It is especially difficult for certain model classes, such as Generative Adversarial Networks and Diffusion Models, whose models do not admit exact likelihoods. In this work, we demonstrate that generalized empirical likelihood (GEL) methods offer a family of diagnostic tools that can identify many deficiencies of deep generative models (DGMs). We show, with appropriate specification of moment conditions, that the proposed method can identify which modes have been dropped, the degree to which DGMs are mode imbalanced, and whether DGMs sufficiently capture intra-class diversity. We show how to combine techniques from Maximum Mean Discrepancy and Generalized Empirical Likelihood to create not only distribution tests that retain per-sample interpretability, but also metrics that include label information. We find that such tests predict the degree of mode dr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#35299;&#37322;&#20026;&#26799;&#24230;&#19979;&#38477;&#30340;&#38543;&#26426;&#26494;&#24347;&#26041;&#27861;&#12290;&#27492;&#20248;&#21270;&#26041;&#27861;&#35777;&#26126;&#20102;&#38646;&#38454;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#20302;&#25928;&#25110;&#19981;&#20855;&#22791;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20016;&#23500;&#31867;&#21035;&#30340;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#20840;&#23616;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.09778</link><description>&lt;p&gt;
&#26799;&#24230;&#30495;&#30340;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Gradient is All You Need?. (arXiv:2306.09778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#35299;&#37322;&#20026;&#26799;&#24230;&#19979;&#38477;&#30340;&#38543;&#26426;&#26494;&#24347;&#26041;&#27861;&#12290;&#27492;&#20248;&#21270;&#26041;&#27861;&#35777;&#26126;&#20102;&#38646;&#38454;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#20302;&#25928;&#25110;&#19981;&#20855;&#22791;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20016;&#23500;&#31867;&#21035;&#30340;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#20840;&#23616;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#32467;&#21512;&#26799;&#24230;&#19979;&#38477;&#30475;&#20316;&#38543;&#26426;&#26494;&#24347;&#26041;&#27861;&#65292;&#26469;&#35299;&#37322;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#36890;&#36807;&#31890;&#23376;&#20043;&#38388;&#30340;&#36890;&#35759;&#65292;&#36825;&#31181;&#20248;&#21270;&#26041;&#27861;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#38646;&#38454;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#20302;&#25928;&#25110;&#19981;&#20855;&#22791;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#20016;&#23500;&#31867;&#21035;&#19979;&#20840;&#23616;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide a novel analytical perspective on the theoretical understanding of gradient-based learning algorithms by interpreting consensus-based optimization (CBO), a recently proposed multi-particle derivative-free optimization method, as a stochastic relaxation of gradient descent. Remarkably, we observe that through communication of the particles, CBO exhibits a stochastic gradient descent (SGD)-like behavior despite solely relying on evaluations of the objective function. The fundamental value of such link between CBO and SGD lies in the fact that CBO is provably globally convergent to global minimizers for ample classes of nonsmooth and nonconvex objective functions, hence, on the one side, offering a novel explanation for the success of stochastic relaxations of gradient descent. On the other side, contrary to the conventional wisdom for which zero-order methods ought to be inefficient or not to possess generalization abilities, our results unveil an intrinsic gradi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#26102;&#35013;&#20844;&#21496;&#23610;&#30721;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#21019;&#36896;&#20986;&#26356;&#20026;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#38477;&#20302;&#22242;&#38431;&#24037;&#20316;&#37327;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.09775</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#21270;&#23610;&#30721;&#34920;&#24314;&#31435;&#21644;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Using Machine Learning Methods for Automation of Size Grid Building and Management. (arXiv:2306.09775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#26102;&#35013;&#20844;&#21496;&#23610;&#30721;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#21019;&#36896;&#20986;&#26356;&#20026;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#38477;&#20302;&#22242;&#38431;&#24037;&#20316;&#37327;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#35013;&#26381;&#35013;&#20844;&#21496;&#38656;&#35201;&#25552;&#21069;&#19968;&#24180;&#35268;&#21010;&#19979;&#19968;&#20010;&#23395;&#24230;&#20197;&#36827;&#34892;&#20379;&#24212;&#38142;&#31649;&#29702;&#12290;&#35813;&#30740;&#31350;&#20851;&#27880;&#26446;&#32500;&#26031;&#30340;&#23610;&#30721;&#36873;&#25321;&#20915;&#31574;&#12290;&#30446;&#21069;&#65292;&#22320;&#21306;&#21644;&#35745;&#21010;&#23567;&#32452;&#32423;&#21035;&#30340;&#23610;&#30721;&#34920;&#26159;&#25163;&#24037;&#24314;&#31435;&#21644;&#31649;&#29702;&#30340;&#65292;&#36825;&#32473;&#23610;&#30721;&#12289;&#21830;&#23478;&#21644;&#35745;&#21010;&#22242;&#38431;&#24102;&#26469;&#20102;&#24037;&#20316;&#37327;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;:&#8220;&#19979;&#19968;&#20010;&#23395;&#24230;&#27599;&#20010;&#23610;&#30721;&#34920;&#21517;&#31216;&#19979;&#35268;&#21010;&#20154;&#21592;&#24212;&#35813;&#26377;&#21738;&#20123;&#23610;&#30721;&#21487;&#29992;&#65311;&#8221;&#21644;&#8220;&#19979;&#19968;&#20010;&#23395;&#24230;&#27599;&#20010;&#35268;&#21010;&#23567;&#32452;&#24212;&#37319;&#29992;&#21738;&#20123;&#23610;&#30721;&#65311;&#8221;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#20998;&#31867;&#27169;&#22411;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21019;&#24314;&#20102;&#26356;&#20026;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#12290;&#39044;&#35745;&#22312;&#23454;&#36341;&#20013;&#25237;&#20837;&#20351;&#29992;&#21518;&#65292;&#20844;&#21496;&#22242;&#38431;&#30340;&#24037;&#20316;&#37327;&#23558;&#20250;&#20943;&#23569;&#12290;&#19982;&#26102;&#23578;&#26381;&#35013;&#34892;&#19994;&#30340;&#35768;&#22810;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#23610;&#30721;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashion apparel companies require planning for the next season, a year in advance for supply chain management. This study focuses on size selection decision making for Levi Strauss. Currently, the region and planning group level size grids are built and managed manually. The company suffers from the workload it creates for sizing, merchant and planning teams. This research is aiming to answer two research questions: "Which sizes should be available to the planners under each size grid name for the next season(s)?" and "Which sizes should be adopted for each planning group for the next season(s)?". We approach to the problem with a classification model, which is one of the popular models used in machine learning. With this research, a more automated process was created by using machine learning techniques. A decrease in workload of the teams in the company is expected after it is put into practice. Unlike many studies in the state of art for fashion and apparel industry, this study focu
&lt;/p&gt;</description></item><item><title>&#20013;&#25991;&#24635;&#32467;&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#26085;&#35821;&#21644;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#31036;&#35980;&#27700;&#24179;&#30456;&#20851;&#30340;&#35821;&#27861;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#31036;&#35980;&#27700;&#24179;&#26159;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.09752</link><description>&lt;p&gt;
&#31036;&#35980;&#21051;&#26495;&#21360;&#35937;&#21644;&#25915;&#20987;&#21521;&#37327;&#65306;&#26085;&#38889;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models. (arXiv:2306.09752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09752
&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#24635;&#32467;&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#26085;&#35821;&#21644;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#31036;&#35980;&#27700;&#24179;&#30456;&#20851;&#30340;&#35821;&#27861;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#31036;&#35980;&#27700;&#24179;&#26159;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36319;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#20351;&#29992;&#65292;&#24615;&#21035;&#20559;&#35265;&#30740;&#31350;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#38750;&#33521;&#35821;&#20559;&#35265;&#30740;&#31350;&#36824;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#31036;&#35980;&#27700;&#24179;&#30456;&#20851;&#30340;&#35821;&#27861;&#24615;&#21035;&#20559;&#35265;&#22312;&#26085;&#35821;&#21644;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#35821;&#35328;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#24615;&#21035;&#20559;&#35265;&#21644;&#31036;&#35980;&#27700;&#24179;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20250;&#22797;&#21046;&#36825;&#20123;&#20559;&#35265;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#26495;&#20998;&#26512;&#30007;&#24615;&#21644;&#22899;&#24615;&#35821;&#27861;&#24615;&#21035;&#30340;&#30456;&#23545;&#39044;&#27979;&#27010;&#29575;&#65292;&#24182;&#21457;&#29616;&#38750;&#27491;&#24335;&#31036;&#35980;&#35821;&#35328;&#26368;&#33021;&#34920;&#29616;&#20986;&#22899;&#24615;&#35821;&#27861;&#24615;&#21035;&#65292;&#32780;&#31895;&#40065;&#21644;&#27491;&#24335;&#35821;&#35328;&#26368;&#33021;&#34920;&#29616;&#20986;&#30007;&#24615;&#35821;&#27861;&#24615;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#31036;&#35980;&#27700;&#24179;&#26159;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#19968;&#31181;&#25915;&#20987;&#21521;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#25216;&#24039;&#22238;&#36991;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In efforts to keep up with the rapid progress and use of large language models, gender bias research is becoming more prevalent in NLP. Non-English bias research, however, is still in its infancy with most work focusing on English. In our work, we study how grammatical gender bias relating to politeness levels manifests in Japanese and Korean language models. Linguistic studies in these languages have identified a connection between gender bias and politeness levels, however it is not yet known if language models reproduce these biases. We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models. Cyberbullies can evade detection through simple techniques ab
&lt;/p&gt;</description></item><item><title>Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.09750</link><description>&lt;p&gt;
Fedstellar&#65306;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09750
&lt;/p&gt;
&lt;p&gt;
Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2016&#24180;&#65292;&#35895;&#27468;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#36328;&#32852;&#30431;&#21442;&#19982;&#32773;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#34429;&#28982;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#12289;&#21333;&#28857;&#25925;&#38556;&#21644;&#23545;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#31561;&#23616;&#38480;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36890;&#36807;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#21644;&#26368;&#23567;&#21270;&#23545;&#20013;&#22830;&#23454;&#20307;&#30340;&#20381;&#36182;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35757;&#32451;DFL&#27169;&#22411;&#30340;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fedstellar&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#22312;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#19981;&#21516;&#32852;&#30431;&#20013;&#20197;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;TD&#23398;&#20064;&#65292;&#22312;&#39532;&#23572;&#31185;&#22827;&#35266;&#27979;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#23545;&#22122;&#22768;&#39033;&#30340;&#20998;&#35299;&#65292;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#32531;&#20914;&#21306;&#21644;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#26469;&#25511;&#21046;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.09746</link><description>&lt;p&gt;
&#12298;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference Learning with Experience Replay. (arXiv:2306.09746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;TD&#23398;&#20064;&#65292;&#22312;&#39532;&#23572;&#31185;&#22827;&#35266;&#27979;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#23545;&#22122;&#22768;&#39033;&#30340;&#20998;&#35299;&#65292;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#32531;&#20914;&#21306;&#21644;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#26469;&#25511;&#21046;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#34987;&#26222;&#36941;&#35748;&#20026;&#26159;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#31639;&#27861;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20854;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#65292;&#21253;&#25324;&#22343;&#26041;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#12290;&#22312;&#32463;&#39564;&#26041;&#38754;&#65292;&#32463;&#39564;&#22238;&#25918;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29702;&#35770;&#25928;&#24212;&#23578;&#26410;&#34987;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39532;&#23572;&#31185;&#22827;&#22122;&#22768;&#39033;&#30340;&#31616;&#21333;&#20998;&#35299;&#65292;&#24182;&#20026;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;TD&#23398;&#20064;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#39532;&#23572;&#31185;&#22827;&#35266;&#27979;&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#22343;&#36845;&#20195;&#21644;&#26368;&#32456;&#36845;&#20195;&#24773;&#20917;&#19979;&#65292;&#24120;&#25968;&#27493;&#38271;&#24341;&#36215;&#30340;&#35823;&#24046;&#26415;&#35821;&#21487;&#20197;&#36890;&#36807;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#22823;&#23567;&#21644;&#20174;&#32463;&#39564;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#25277;&#26679;&#30340;&#23567;&#25209;&#37327;&#26469;&#26377;&#25928;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal-difference (TD) learning is widely regarded as one of the most popular algorithms in reinforcement learning (RL). Despite its widespread use, it has only been recently that researchers have begun to actively study its finite time behavior, including the finite time bound on mean squared error and sample complexity. On the empirical side, experience replay has been a key ingredient in the success of deep RL algorithms, but its theoretical effects on RL have yet to be fully understood. In this paper, we present a simple decomposition of the Markovian noise terms and provide finite-time error bounds for TD-learning with experience replay. Specifically, under the Markovian observation model, we demonstrate that for both the averaged iterate and final iterate cases, the error term induced by a constant step-size can be effectively controlled by the size of the replay buffer and the mini-batch sampled from the experience replay buffer.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoLION&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#26435;&#34913;&#33258;&#36866;&#24212;&#26469;&#20943;&#23569;&#19987;&#23478;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.09744</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#21160;&#26435;&#34913;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Automatic Trade-off Adaptation in Offline RL. (arXiv:2306.09744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09744
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoLION&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#26435;&#34913;&#33258;&#36866;&#24212;&#26469;&#20943;&#23569;&#19987;&#23478;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#20123;&#21487;&#22312;&#36816;&#34892;&#26102;&#20445;&#25345;&#33258;&#36866;&#24212;&#24615;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20363;&#22914;&#65292;LION&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#30028;&#38754;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#35774;&#32622;&#22522;&#20110;&#20272;&#35745;&#22238;&#25253;&#30340;&#34892;&#20026;&#20811;&#38534;&#21644;&#26368;&#20248;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#19987;&#23478;&#21487;&#20197;&#20351;&#29992;&#27492;&#30028;&#38754;&#26681;&#25454;&#20854;&#20559;&#22909;&#35843;&#25972;&#31574;&#30053;&#34892;&#20026;&#65292;&#24182;&#22312;&#20445;&#23432;&#24615;&#21644;&#24615;&#33021;&#20248;&#21270;&#20043;&#38388;&#25214;&#21040;&#33391;&#22909;&#30340;&#26435;&#34913;&#12290;&#30001;&#20110;&#19987;&#23478;&#26102;&#38388;&#23453;&#36149;&#65292;&#22240;&#27492;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#39550;&#39542;&#26469;&#25193;&#23637;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#25214;&#21040;&#26435;&#34913;&#30340;&#27491;&#30830;&#21442;&#25968;&#21270;&#65292;&#20174;&#32780;&#24471;&#21040;&#31216;&#20026;AutoLION&#30340;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, offline RL algorithms have been proposed that remain adaptive at runtime. For example, the LION algorithm \cite{lion} provides the user with an interface to set the trade-off between behavior cloning and optimality w.r.t. the estimated return at runtime. Experts can then use this interface to adapt the policy behavior according to their preferences and find a good trade-off between conservatism and performance optimization. Since expert time is precious, we extend the methodology with an autopilot that automatically finds the correct parameterization of the trade-off, yielding a new algorithm which we term AutoLION.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#31283;&#23450;&#39033;&#20351;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09739</link><description>&lt;p&gt;
&#23398;&#20064;&#21463;&#38480;&#21160;&#21147;&#23398;&#30340;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stabilized Neural Differential Equations for Learning Constrained Dynamics. (arXiv:2306.09739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#31283;&#23450;&#39033;&#20351;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#20174;&#25968;&#25454;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#25512;&#26029;&#20986;&#30340;&#21160;&#24577;&#31995;&#32479;&#20445;&#30041;&#24050;&#30693;&#32422;&#26463;&#26465;&#20214;&#65288;&#20363;&#22914;&#23432;&#24658;&#23450;&#24459;&#25110;&#23545;&#20801;&#35768;&#30340;&#31995;&#32479;&#29366;&#24577;&#30340;&#38480;&#21046;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#31283;&#23450;&#39033;&#65292;&#24403;&#28155;&#21152;&#21040;&#21407;&#22987;&#21160;&#24577;&#31995;&#32479;&#20013;&#26102;&#65292;&#21487;&#20197;&#23558;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#25152;&#26377;&#24120;&#35265;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#27169;&#22411;&#20860;&#23481;&#24182;&#24191;&#27867;&#36866;&#29992;&#12290;&#22312;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;SNDE&#22312;&#25193;&#23637;&#21487;&#32435;&#20837;NODE&#35757;&#32451;&#30340;&#32422;&#26463;&#31867;&#22411;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many successful methods to learn dynamical systems from data have recently been introduced. However, assuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. We propose stabilized neural differential equations (SNDEs), a method to enforce arbitrary manifold constraints for neural differential equations. Our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. Due to its simplicity, our method is compatible with all common neural ordinary differential equation (NODE) models and broadly applicable. In extensive empirical evaluations, we demonstrate that SNDEs outperform existing methods while extending the scope of which types of constraints can be incorporated into NODE training.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#24179;&#34913;&#20102;&#25506;&#32034;&#33021;&#21147;&#21644;&#22521;&#35757;&#25104;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#22522;&#30784;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#22312;&#20248;&#21270;&#25104;&#26412;&#12289;&#28176;&#36817;&#35823;&#24046;&#21644;&#36807;&#24230;&#25311;&#21512;&#35823;&#24046;&#30028;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;RL&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#39640;&#25928;&#19988;&#24615;&#33021;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.09712</link><description>&lt;p&gt;
&#21322;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20248;&#21270;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semi-Offline Reinforcement Learning for Optimized Text Generation. (arXiv:2306.09712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09712
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#24179;&#34913;&#20102;&#25506;&#32034;&#33021;&#21147;&#21644;&#22521;&#35757;&#25104;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#22522;&#30784;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#22312;&#20248;&#21270;&#25104;&#26412;&#12289;&#28176;&#36817;&#35823;&#24046;&#21644;&#36807;&#24230;&#25311;&#21512;&#35823;&#24046;&#30028;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;RL&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#39640;&#25928;&#19988;&#24615;&#33021;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19982;&#29615;&#22659;&#20132;&#20114;&#26377;&#20004;&#31181;&#20027;&#35201;&#26041;&#24335;&#65306;&#22312;&#32447;&#21644;&#31163;&#32447;&#12290;&#22312;&#32447;&#26041;&#27861;&#25506;&#32034;&#29615;&#22659;&#25152;&#38656;&#26102;&#38388;&#36739;&#38271;&#65292;&#32780;&#31163;&#32447;&#26041;&#27861;&#36890;&#36807;&#29306;&#29298;&#25506;&#32034;&#33021;&#21147;&#26377;&#25928;&#22320;&#33719;&#24471;&#22870;&#21169;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21322;&#31163;&#32447;RL&#65292;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#24179;&#28369;&#22320;&#20174;&#31163;&#32447;&#36716;&#25442;&#21040;&#22312;&#32447;&#35774;&#32622;&#65292;&#24179;&#34913;&#25506;&#32034;&#33021;&#21147;&#21644;&#22521;&#35757;&#25104;&#26412;&#65292;&#24182;&#20026;&#27604;&#36739;&#19981;&#21516;RL&#35774;&#32622;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#12290;&#22522;&#20110;&#21322;&#31163;&#32447;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#20248;&#21270;&#25104;&#26412;&#12289;&#28176;&#36817;&#35823;&#24046;&#21644;&#36807;&#24230;&#25311;&#21512;&#35823;&#24046;&#30028;&#26041;&#38754;&#26368;&#20248;&#30340;RL&#35774;&#32622;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21322;&#31163;&#32447;&#26041;&#27861;&#25928;&#29575;&#39640;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), there are two major settings for interacting with the environment: online and offline. Online methods explore the environment at significant time cost, and offline methods efficiently obtain reward signals by sacrificing exploration capability. We propose semi-offline RL, a novel paradigm that smoothly transits from offline to online settings, balances exploration capability and training cost, and provides a theoretical foundation for comparing different RL settings. Based on the semi-offline formulation, we present the RL setting that is optimal in terms of optimization cost, asymptotic error, and overfitting error bound. Extensive experiments show that our semi-offline approach is efficient and yields comparable or often better performance compared with state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DNN&#34920;&#31034;&#26041;&#27861;DAG-DNN&#65292;&#21033;&#29992;&#19979;&#19977;&#35282;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#32467;&#26500;&#32593;&#32476;&#21098;&#26525;&#65292;&#21516;&#26102;&#35777;&#26126;DAG-DNN&#21487;&#20197;&#25512;&#23548;&#20986;DNN&#21508;&#20010;&#23376;&#26550;&#26500;&#19978;&#23450;&#20041;&#30340;&#25152;&#26377;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09707</link><description>&lt;p&gt;
DAG-DNN&#20013;&#20989;&#25968;&#30340;&#34920;&#31034;&#21644;&#20998;&#35299;&#20197;&#21450;&#32467;&#26500;&#32593;&#32476;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Representation and decomposition of functions in DAG-DNNs and structural network pruning. (arXiv:2306.09707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DNN&#34920;&#31034;&#26041;&#27861;DAG-DNN&#65292;&#21033;&#29992;&#19979;&#19977;&#35282;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#32467;&#26500;&#32593;&#32476;&#21098;&#26525;&#65292;&#21516;&#26102;&#35777;&#26126;DAG-DNN&#21487;&#20197;&#25512;&#23548;&#20986;DNN&#21508;&#20010;&#23376;&#26550;&#26500;&#19978;&#23450;&#20041;&#30340;&#25152;&#26377;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24517;&#39035;&#20180;&#32454;&#26816;&#26597;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25552;&#20379;&#30340;&#32467;&#35770;&#26159;&#26222;&#36866;&#30340;&#36824;&#26159;&#20381;&#36182;&#20110;&#26550;&#26500;&#30340;&#12290;&#26415;&#35821;DAG-DNN&#25351;&#30340;&#26159;&#29992;&#30452;&#25509;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#34920;&#31034;&#26550;&#26500;&#30340;DNN&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#20854;&#20013;&#24359;&#19982;&#20989;&#25968;&#30456;&#20851;&#32852;&#12290;&#33410;&#28857;&#30340;&#32423;&#21035;&#34920;&#31034;&#36755;&#20837;&#33410;&#28857;&#21644;&#24863;&#20852;&#36259;&#33410;&#28857;&#20043;&#38388;&#30340;&#26368;&#22823;&#36339;&#25968;&#12290;&#22312;&#24403;&#21069;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;DAG-DNN&#21487;&#29992;&#20110;&#25512;&#23548;&#22312;DNN&#21508;&#20010;&#23376;&#26550;&#26500;&#19978;&#23450;&#20041;&#30340;&#25152;&#26377;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#22312;DAG-DNN&#20013;&#23450;&#20041;&#30340;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#19968;&#31995;&#21015;&#19979;&#19977;&#35282;&#30697;&#38453;&#23548;&#20986;&#65292;&#20854;&#20013;&#27599;&#20010;&#30697;&#38453;&#25552;&#20379;&#20102;&#23558;&#23376;&#22270;&#20013;&#23450;&#20041;&#30340;&#20989;&#25968;&#36807;&#28193;&#21040;&#25351;&#23450;&#32423;&#21035;&#33410;&#28857;&#30340;&#36716;&#25442;&#12290;&#19982;&#19979;&#19977;&#35282;&#30697;&#38453;&#30456;&#20851;&#32852;&#30340;lifting&#32467;&#26500;&#20351;&#24471;&#21487;&#20197;&#20197;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#25191;&#34892;&#32593;&#32476;&#30340;&#32467;&#26500;&#21098;&#26525;&#12290;&#20998;&#35299;&#26159;&#26222;&#36941;&#36866;&#29992;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conclusions provided by deep neural networks (DNNs) must be carefully scrutinized to determine whether they are universal or architecture dependent. The term DAG-DNN refers to a graphical representation of a DNN in which the architecture is expressed as a direct-acyclic graph (DAG), on which arcs are associated with functions. The level of a node denotes the maximum number of hops between the input node and the node of interest. In the current study, we demonstrate that DAG-DNNs can be used to derive all functions defined on various sub-architectures of the DNN. We also demonstrate that the functions defined in a DAG-DNN can be derived via a sequence of lower-triangular matrices, each of which provides the transition of functions defined in sub-graphs up to nodes at a specified level. The lifting structure associated with lower-triangular matrices makes it possible to perform the structural pruning of a network in a systematic manner. The fact that decomposition is universally appl
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#24490;&#29615;&#32593;&#32476;&#26159;&#19968;&#31181;&#38477;&#20302;&#24773;&#24863;&#20998;&#26512;&#35745;&#31639;&#25104;&#26412;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;</title><link>http://arxiv.org/abs/2306.09705</link><description>&lt;p&gt;
&#38477;&#20302;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65306;&#24352;&#37327;&#24490;&#29615;&#32593;&#32476;&#19982;&#24490;&#29615;&#32593;&#32476;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Reducing Computational Costs in Sentiment Analysis: Tensorized Recurrent Networks vs. Recurrent Networks. (arXiv:2306.09705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09705
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#24490;&#29615;&#32593;&#32476;&#26159;&#19968;&#31181;&#38477;&#20302;&#24773;&#24863;&#20998;&#26512;&#35745;&#31639;&#25104;&#26412;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#35266;&#20247;&#23545;&#26576;&#19968;&#25991;&#26412;&#30340;&#21453;&#24212;&#23545;&#20110;&#25919;&#27835;&#12289;&#30740;&#31350;&#21644;&#21830;&#19994;&#34892;&#19994;&#31561;&#22810;&#20010;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;&#21033;&#29992;&#35789;&#27719;/&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#30830;&#23450;&#19981;&#21516;&#22823;&#23567;&#30340;&#25991;&#26412;&#26159;&#21542;&#34920;&#29616;&#20986;&#31215;&#26497;&#12289;&#28040;&#26497;&#25110;&#20013;&#24615;&#30340;&#24773;&#24863;&#12290;&#24490;&#29615;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#24191;&#27867;&#29992;&#20110;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#39640;&#65292;&#22240;&#27492;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#24403;&#21487;&#29992;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;&#36825;&#31181;&#32570;&#28857;&#29978;&#33267;&#26356;&#20026;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#26174;&#30528;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#27491;&#21017;&#21270;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#24352;&#37327;&#21270;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#23558;&#23545;&#19968;&#20123;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipating audience reaction towards a certain text is integral to several facets of society ranging from politics, research, and commercial industries. Sentiment analysis (SA) is a useful natural language processing (NLP) technique that utilizes lexical/statistical and deep learning methods to determine whether different-sized texts exhibit positive, negative, or neutral emotions. Recurrent networks are widely used in machine-learning communities for problems with sequential data. However, a drawback of models based on Long-Short Term Memory networks and Gated Recurrent Units is the significantly high number of parameters, and thus, such models are computationally expensive. This drawback is even more significant when the available data are limited. Also, such models require significant over-parameterization and regularization to achieve optimal performance. Tensorized models represent a potential solution. In this paper, we classify the sentiment of some social media posts. We comp
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#38382;&#39064;&#30340;&#23618;&#32423;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;&#21464;&#37327;&#65292;&#20197;&#24110;&#21161;&#35760;&#24518;&#21382;&#21490;&#24773;&#33410;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#21516;&#26102;&#25511;&#21046;&#27169;&#22411;&#23545;&#26032;&#24773;&#33410;&#36866;&#24212;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09702</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#28145;&#24230;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#30340;&#23618;&#32423;&#36125;&#21494;&#26031;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Bayesian Model for Deep Few-Shot Meta Learning. (arXiv:2306.09702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09702
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#38382;&#39064;&#30340;&#23618;&#32423;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;&#21464;&#37327;&#65292;&#20197;&#24110;&#21161;&#35760;&#24518;&#21382;&#21490;&#24773;&#33410;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#21516;&#26102;&#25511;&#21046;&#27169;&#22411;&#23545;&#26032;&#24773;&#33410;&#36866;&#24212;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23618;&#32423;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#35268;&#27169;&#65288;&#21487;&#33021;&#26159;&#26080;&#38480;&#30340;&#65289;&#20219;&#21153;/&#24773;&#33410;&#30340;&#23398;&#20064;&#65292;&#24456;&#22909;&#22320;&#36866;&#24212;&#20102;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20197;&#24773;&#33410;&#20026;&#22522;&#30784;&#30340;&#38543;&#26426;&#21464;&#37327;&#26469;&#27169;&#25311;&#24773;&#33410;&#29305;&#23450;&#30340;&#30446;&#26631;&#29983;&#25104;&#36807;&#31243;&#65292;&#20854;&#20013;&#36825;&#20123;&#26412;&#22320;&#38543;&#26426;&#21464;&#37327;&#30001;&#26356;&#39640;&#32423;&#21035;&#30340;&#20840;&#23616;&#38543;&#26426;&#21464;&#37327;&#31649;&#29702;&#12290;&#20840;&#23616;&#21464;&#37327;&#26377;&#21161;&#20110;&#35760;&#20303;&#21382;&#21490;&#24773;&#33410;&#20013;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#21516;&#26102;&#20197;&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;&#21407;&#21017;&#25511;&#21046;&#27169;&#22411;&#38656;&#35201;&#36866;&#24212;&#26032;&#24773;&#33410;&#30340;&#31243;&#24230;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#26694;&#26550;&#20869;&#65292;&#23545;&#20110;&#26032;&#30340;&#24773;&#33410;/&#20219;&#21153;&#30340;&#39044;&#27979;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;/&#26080;&#38480;&#25968;&#37327;&#30340;&#23616;&#37096;&#38543;&#26426;&#21464;&#37327;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#65292;&#19981;&#33021;&#23384;&#20648;&#24403;&#21069;&#23616;&#37096;&#38543;&#26426;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#20197;&#36827;&#34892;&#39057;&#32321;&#30340;&#26410;&#26469;&#26356;&#26032;&#65292;&#32780;&#36825;&#22312;&#20256;&#32479;&#21464;&#20998;&#25512;&#26029;&#20013;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#38656;&#35201;&#33021;&#22815;&#23558;&#27599;&#20010;&#26412;&#22320;&#21464;&#37327;&#35270;&#20026;&#19968;&#27425;&#24615;&#23398;&#20064;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#23616;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#20351;&#29992;&#22312;&#32447;&#25512;&#26029;&#25104;&#20026;&#24517;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20855;&#26377;&#35745;&#31639;&#26102;&#38388;&#21644;&#26131;&#20110;&#23454;&#29616;&#24615;&#30340;&#20248;&#28857;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel hierarchical Bayesian model for learning with a large (possibly infinite) number of tasks/episodes, which suits well the few-shot meta learning problem. We consider episode-wise random variables to model episode-specific target generative processes, where these local random variables are governed by a higher-level global random variate. The global variable helps memorize the important information from historic episodes while controlling how much the model needs to be adapted to new episodes in a principled Bayesian manner. Within our model framework, the prediction on a novel episode/task can be seen as a Bayesian inference problem. However, a main obstacle in learning with a large/infinite number of local random variables in online nature, is that one is not allowed to store the posterior distribution of the current local random variable for frequent future updates, typical in conventional variational inference. We need to be able to treat each local variable as a o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#22238;&#31572;&#20102;Nesterov-1983&#21644;FISTA&#26159;&#21542;&#22312;&#24378;&#20984;&#20989;&#25968;&#19978;&#32447;&#24615;&#25910;&#25947;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#32447;&#24615;&#25910;&#25947;&#24615;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#24615;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.09694</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#20984;&#24615;&#30340; Nesterov-1983 &#30340;&#32447;&#24615;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Linear convergence of Nesterov-1983 with the strong convexity. (arXiv:2306.09694v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#22238;&#31572;&#20102;Nesterov-1983&#21644;FISTA&#26159;&#21542;&#22312;&#24378;&#20984;&#20989;&#25968;&#19978;&#32447;&#24615;&#25910;&#25947;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#32447;&#24615;&#25910;&#25947;&#24615;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#24615;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#20195;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;Nesterov &#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#27861;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#37324;&#31243;&#30865;&#65292;&#35813;&#26041;&#27861;&#22312;[Nesterov&#65292;1983]&#20013;&#25552;&#20986;&#65292;&#31616;&#31216;&#20026;Nesterov-1983&#12290;&#27492;&#21518;&#65292;&#37325;&#35201;&#30340;&#36827;&#23637;&#20043;&#19968;&#26159;&#23427;&#30340;&#36817;&#31471;&#25512;&#24191;&#65292;&#21517;&#20026;&#24555;&#36895;&#36845;&#20195;&#25910;&#32553;&#38408;&#20540;&#31639;&#27861;&#65288;FISTA&#65289;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#31185;&#23398;&#21644;&#24037;&#31243;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#26410;&#30693;&#36947;Nesterov-1983&#21644;FISTA&#26159;&#21542;&#22312;&#24378;&#20984;&#20989;&#25968;&#19978;&#32447;&#24615;&#25910;&#25947;&#65292;&#32780;&#36825;&#24050;&#34987;&#21015;&#20026;&#32508;&#21512;&#35780;&#23457;[Chambolle&#21644;Pock&#65292;2016&#65292;&#38468;&#24405;B]&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#37319;&#29992;&#30340;&#30456;&#31354;&#38388;&#34920;&#31034;&#19968;&#36215;&#65292;&#26500;&#36896;Lyapunov&#20989;&#25968;&#30340;&#20851;&#38190;&#21306;&#21035;&#22312;&#20110;&#21160;&#33021;&#30340;&#31995;&#25968;&#38543;&#36845;&#20195;&#32780;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#65292;&#19978;&#36848;&#20004;&#31181;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#27809;&#26377;&#20381;&#36182;&#20110;&#24378;&#20984;&#20989;&#25968;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
For modern gradient-based optimization, a developmental landmark is Nesterov's accelerated gradient descent method, which is proposed in [Nesterov, 1983], so shorten as Nesterov-1983. Afterward, one of the important progresses is its proximal generalization, named the fast iterative shrinkage-thresholding algorithm (FISTA), which is widely used in image science and engineering. However, it is unknown whether both Nesterov-1983 and FISTA converge linearly on the strongly convex function, which has been listed as the open problem in the comprehensive review [Chambolle and Pock, 2016, Appendix B]. In this paper, we answer this question by the use of the high-resolution differential equation framework. Along with the phase-space representation previously adopted, the key difference here in constructing the Lyapunov function is that the coefficient of the kinetic energy varies with the iteration. Furthermore, we point out that the linear convergence of both the two algorithms above has no d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#30913;&#20849;&#25391;&#27874;&#35889;&#25216;&#26415;&#20013;&#20195;&#35874;&#29289;&#20449;&#21495;&#23450;&#37327;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09681</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#35823;&#24046;&#22240;&#32032;&#19982;&#24635;&#20307;&#39640;&#20998;&#23376;&#20449;&#21495;&#20272;&#35745;&#30340;&#30913;&#20849;&#25391;&#27874;&#35889;&#23450;&#37327;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Magnetic Resonance Spectroscopy Quantification Aided by Deep Estimations of Imperfection Factors and Overall Macromolecular Signal. (arXiv:2306.09681v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#30913;&#20849;&#25391;&#27874;&#35889;&#25216;&#26415;&#20013;&#20195;&#35874;&#29289;&#20449;&#21495;&#23450;&#37327;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#27874;&#35889;&#65288;MRS&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#30340;&#29983;&#29289;&#21307;&#23398;&#26816;&#27979;&#25216;&#26415;&#65292;&#30001;&#20110;&#20195;&#35874;&#29289;&#20449;&#21495;&#30340;&#20005;&#37325;&#37325;&#21472;&#12289;&#38750;&#29702;&#24819;&#37319;&#38598;&#26465;&#20214;&#24341;&#36215;&#30340;&#20449;&#21495;&#22833;&#30495;&#20197;&#21450;&#19982;&#24378;&#32972;&#26223;&#20449;&#21495;&#21253;&#25324;&#39640;&#20998;&#23376;&#20449;&#21495;&#30340;&#24178;&#25200;&#65292;&#20351;&#29992;&#36136;&#23376;MRS&#31934;&#30830;&#37327;&#21270;&#20195;&#35874;&#29289;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#24341;&#20837;&#21040;MRS&#37327;&#21270;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#12289;&#38750;&#29702;&#24819;&#37319;&#38598;&#26465;&#20214;&#30340;&#35823;&#24046;&#22240;&#32032;&#20197;&#21450;&#35774;&#35745;&#20960;&#31181;&#32463;&#39564;&#20808;&#39564;&#22914;&#35832;&#22914;&#20195;&#35874;&#29289;&#21644;&#39640;&#20998;&#23376;&#30340;&#22522;&#38598;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic Resonance Spectroscopy (MRS) is an important non-invasive technique for in vivo biomedical detection. However, it is still challenging to accurately quantify metabolites with proton MRS due to three problems: Serious overlaps of metabolite signals, signal distortions due to non-ideal acquisition conditions and interference with strong background signals including macromolecule signals. The most popular software, LCModel, adopts the non-linear least square to quantify metabolites and addresses these problems by introducing regularization terms, imperfection factors of non-ideal acquisition conditions, and designing several empirical priors such as basissets of both metabolites and macromolecules. However, solving such a large non-linear quantitative problem is complicated. Moreover, when the signal-to-noise ratio of an input MRS signal is low, the solution may have a large deviation. In this work, deep learning is introduced to reduce the complexity of solving this overall quan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#25552;&#20986;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#21644;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#26469;&#35299;&#20915;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#26087;&#20449;&#24687;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;&#26377;&#25928;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.09675</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-View Class Incremental Learning. (arXiv:2306.09675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#25552;&#20986;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#21644;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#26469;&#35299;&#20915;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#26087;&#20449;&#24687;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;&#26377;&#25928;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;MVL&#65289;&#22312;&#25972;&#21512;&#25968;&#25454;&#38598;&#30340;&#22810;&#20010;&#35270;&#35282;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20026;&#20102;&#20351;MVL&#26041;&#27861;&#22312;&#24320;&#25918;&#24335;&#29615;&#22659;&#20013;&#26356;&#23454;&#29992;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#31216;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#65292;&#20854;&#20013;&#21333;&#20010;&#27169;&#22411;&#20174;&#36830;&#32493;&#30340;&#35270;&#22270;&#27969;&#20013;&#36880;&#27493;&#20998;&#31867;&#26032;&#31867;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#26089;&#26399;&#25968;&#25454;&#30340;&#35270;&#22270;&#12290;&#20294;&#26159;&#65292;MVCIL&#38754;&#20020;&#30528;&#32769;&#20449;&#24687;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#20197;&#20445;&#35777;&#23427;&#20204;&#22312;&#24037;&#20316;&#29366;&#24577;&#19979;&#30340;&#20998;&#31163;&#35270;&#22270;&#26368;&#20248;&#65292;&#20854;&#20013;&#23646;&#20110;&#31867;&#30340;&#22810;&#20010;&#35270;&#22270;&#25353;&#39034;&#24207;&#21576;&#29616;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#36880;&#20010;&#38598;&#25104;&#21040;&#30001;&#25552;&#21462;&#30340;&#29305;&#24449;&#36328;&#36234;&#30340;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#20013;&#65307;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#65292;&#20197;&#20445;&#30041;&#26087;&#31867;&#30340;&#30693;&#35782;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view learning (MVL) has gained great success in integrating information from multiple perspectives of a dataset to improve downstream task performance. To make MVL methods more practical in an open-ended environment, this paper investigates a novel paradigm called multi-view class incremental learning (MVCIL), where a single model incrementally classifies new classes from a continual stream of views, requiring no access to earlier views of data. However, MVCIL is challenged by the catastrophic forgetting of old information and the interference with learning new concepts. To address this, we first develop a randomization-based representation learning technique serving for feature extraction to guarantee their separate view-optimal working states, during which multiple views belonging to a class are presented sequentially; Then, we integrate them one by one in the orthogonality fusion subspace spanned by the extracted features; Finally, we introduce selective weight consolidation f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#27010;&#29575;&#20272;&#35745;&#30340;&#19968;&#23545;&#19968;&#28145;&#24230;&#23398;&#20064;&#22810;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#29305;&#23450;&#30340;&#36317;&#31163;&#24230;&#37327;&#26469;&#26657;&#20934;&#20108;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#27010;&#29575;&#30340;&#36317;&#31163;&#26368;&#23567;&#21270;&#26469;&#33719;&#24471;&#23545;&#20027;&#20307;&#30340;&#31867;&#21035;&#27010;&#29575;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.09668</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#27010;&#29575;&#20272;&#35745;&#30340;&#19968;&#23545;&#19968;&#28145;&#24230;&#23398;&#20064;&#22810;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-Classification using One-versus-One Deep Learning Strategy with Joint Probability Estimates. (arXiv:2306.09668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#27010;&#29575;&#20272;&#35745;&#30340;&#19968;&#23545;&#19968;&#28145;&#24230;&#23398;&#20064;&#22810;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#29305;&#23450;&#30340;&#36317;&#31163;&#24230;&#37327;&#26469;&#26657;&#20934;&#20108;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#27010;&#29575;&#30340;&#36317;&#31163;&#26368;&#23567;&#21270;&#26469;&#33719;&#24471;&#23545;&#20027;&#20307;&#30340;&#31867;&#21035;&#27010;&#29575;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
One-versus-One&#65288;OvO&#65289;&#31574;&#30053;&#26159;&#19968;&#31181;&#22810;&#20998;&#31867;&#27169;&#22411;&#65292;&#23427;&#20391;&#37325;&#20110;&#35757;&#32451;&#27599;&#19968;&#23545;&#31867;&#20043;&#38388;&#30340;&#20108;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;OvO&#22810;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#32852;&#21512;&#27010;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26657;&#20934;&#20108;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#27010;&#29575;&#30340;&#36317;&#31163;&#26368;&#23567;&#21270;&#26469;&#33719;&#24471;&#23545;&#20027;&#20307;&#30340;&#31867;&#21035;&#27010;&#29575;&#20272;&#35745;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The One-versus-One (OvO) strategy is an approach of multi-classification models which focuses on training binary classifiers between each pair of classes. While the OvO strategy takes advantage of balanced training data, the classification accuracy is usually hindered by the voting mechanism to combine all binary classifiers. In this paper, a novel OvO multi-classification model incorporating a joint probability measure is proposed under the deep learning framework. In the proposed model, a two-stage algorithm is developed to estimate the class probability from the pairwise binary classifiers. Given the binary classifiers, the pairwise probability estimate is calibrated by a distance measure on the separating feature hyperplane. From that, the class probability of the subject is estimated by solving a joint probability-based distance minimization problem. Numerical experiments in different applications show that the proposed model achieves generally higher classification accuracy than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#19988;&#20984;&#30340;&#26367;&#20195;&#20108;&#36827;&#21046;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#20013;&#21457;&#24067;&#31169;&#26377;&#21069;&#32512;&#27714;&#21644;&#39064;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#27604;&#26631;&#24535;&#24615;&#30340;&#20108;&#36827;&#21046;&#26426;&#21046;&#20855;&#26377;&#26356;&#20302;&#30340;&#26041;&#24046;&#21644;&#26356;&#22909;&#30340;&#35266;&#27979;&#26102;&#38388;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#21644;&#31354;&#38388;&#35201;&#27714;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.09666</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#30340;&#31169;&#26377;&#36830;&#32493;&#35266;&#27979;&#24179;&#28369;&#20108;&#20803;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Smooth Binary Mechanism for Efficient Private Continual Observation. (arXiv:2306.09666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#19988;&#20984;&#30340;&#26367;&#20195;&#20108;&#36827;&#21046;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#20013;&#21457;&#24067;&#31169;&#26377;&#21069;&#32512;&#27714;&#21644;&#39064;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#27604;&#26631;&#24535;&#24615;&#30340;&#20108;&#36827;&#21046;&#26426;&#21046;&#20855;&#26377;&#26356;&#20302;&#30340;&#26041;&#24046;&#21644;&#26356;&#22909;&#30340;&#35266;&#27979;&#26102;&#38388;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#21644;&#31354;&#38388;&#35201;&#27714;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#35266;&#27979;&#30340;&#38544;&#31169;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#22522;&#20110;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#25968;&#25454;&#38598;&#21457;&#24067;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#12290;&#21457;&#24067;&#31169;&#26377;&#21069;&#32512;&#27714;&#21644;$x_1,x_2,x_3,\dots\in\{0,1\}$&#38382;&#39064;&#26159;&#19968;&#20010;&#29305;&#21035;&#22909;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#31169;&#26377;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26368;&#26032;&#26041;&#27861;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#24191;&#20041;&#24418;&#24335;&#12290;&#26631;&#24535;&#24615;&#30340;&#20108;&#36827;&#21046;&#26426;&#21046;&#33258;&#21160;&#21152;&#20837;&#30340;&#22122;&#22768;&#30340;&#26041;&#24046;&#26159;&#22810;&#23545;&#25968;&#30340;&#12290;&#26368;&#36817;&#65292;Henzinger et al.&#21644;Denisov et al.&#34920;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#25552;&#39640;&#20108;&#36827;&#21046;&#26426;&#21046;&#65306;&#22122;&#22768;&#30340;&#26041;&#24046;&#21487;&#20197;&#20943;&#23567;&#19968;&#20010;&#36739;&#22823;&#30340;&#24120;&#25968;&#22240;&#23376;&#65292;&#24182;&#19988;&#22312;&#26102;&#38388;&#27493;&#38271;&#20869;&#20063;&#21487;&#20197;&#26356;&#21152;&#24179;&#22343;&#12290;&#20294;&#26159;&#65292;&#20182;&#20204;&#29983;&#25104;&#22122;&#22768;&#20998;&#24067;&#30340;&#31639;&#27861;&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#65288;&#29305;&#21035;&#26159;&#65289;&#31354;&#38388;&#26041;&#38754;&#37117;&#19981;&#22914;&#20154;&#24847;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#21487;&#24494;&#19988;&#20984;&#30340;&#31616;&#21333;&#26367;&#20195;&#20108;&#36827;&#21046;&#26426;&#21046;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25928;&#29575;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#30456;&#23545;&#20110;&#28155;&#21152;&#30340;&#22122;&#22768;&#37327;&#23545;&#26426;&#21046;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26426;&#21046;&#27604;&#26631;&#24535;&#24615;&#30340;&#20108;&#36827;&#21046;&#26426;&#21046;&#20855;&#26377;&#26356;&#20302;&#30340;&#26041;&#24046;&#21644;&#26356;&#22909;&#30340;&#35266;&#27979;&#26102;&#38388;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#21644;&#31354;&#38388;&#35201;&#27714;&#26041;&#38754;&#27604;&#20197;&#21069;&#30340;&#25913;&#36827;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In privacy under continual observation we study how to release differentially private estimates based on a dataset that evolves over time. The problem of releasing private prefix sums of $x_1,x_2,x_3,\dots \in\{0,1\}$ (where the value of each $x_i$ is to be private) is particularly well-studied, and a generalized form is used in state-of-the-art methods for private stochastic gradient descent (SGD). The seminal binary mechanism privately releases the first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently, Henzinger et al. and Denisov et al. showed that it is possible to improve on the binary mechanism in two ways: The variance of the noise can be reduced by a (large) constant factor, and also made more even across time steps. However, their algorithms for generating the noise distribution are not as efficient as one would like in terms of computation time and (in particular) space. We address the efficiency problem by presenting a simple alternative to the binary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;MOMA-DDPG&#65292;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09662</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction. (arXiv:2306.09662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;MOMA-DDPG&#65292;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#20381;&#36182;&#20110;&#36807;&#20110;&#31616;&#21270;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#29978;&#33267;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20063;&#32463;&#24120;&#26159;&#27425;&#20248;&#30340;&#21644;&#19981;&#31283;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;MOMA-DDPG&#65289;&#65292;&#20351;&#29992;&#34928;&#20943;&#26435;&#37325;&#26469;&#20272;&#35745;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20248;&#21270;&#30340;&#22810;&#20010;&#22870;&#21169;&#39033;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#19968;&#20010;&#20122;&#27954;&#22269;&#23478;&#30340;&#20132;&#36890;&#25668;&#20687;&#22836;&#25910;&#38598;&#21040;&#30340;&#30495;&#23454;&#19990;&#30028;&#20132;&#36890;&#25968;&#25454;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#21253;&#21547;&#20102;&#19968;&#20010;&#20840;&#23616;&#26234;&#33021;&#20307;&#65292;&#20294;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#26159;&#20998;&#25955;&#30340;&#65292;&#22240;&#20026;&#36825;&#20010;&#26234;&#33021;&#20307;&#22312;&#25512;&#29702;&#38454;&#27573;&#19981;&#20877;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;MOMA-DDPG&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#25152;&#26377;&#24615;&#33021;&#25351;&#26631;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#26368;&#23567;&#21270;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing traffic signal control systems rely on oversimplified rule-based methods, and even RL-based methods are often suboptimal and unstable. To address this, we propose a cooperative multi-objective architecture called Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MOMA-DDPG), which estimates multiple reward terms for traffic signal control optimization using age-decaying weights. Our approach involves two types of agents: one focuses on optimizing local traffic at each intersection, while the other aims to optimize global traffic throughput. We evaluate our method using real-world traffic data collected from an Asian country's traffic cameras. Despite the inclusion of a global agent, our solution remains decentralized as this agent is no longer necessary during the inference stage. Our results demonstrate the effectiveness of MOMA-DDPG, outperforming state-of-the-art methods across all performance metrics. Additionally, our proposed system minimizes both waiting ti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38750;&#21442;&#25968;&#30340;&#20013;&#20171;&#21464;&#37327;-&#32467;&#26524;&#27169;&#22411;&#65292;&#29992;&#20110;&#21307;&#30103;&#24178;&#39044;&#30340;&#22240;&#26524;&#20998;&#26512;&#65292;&#21487;&#20197;&#21306;&#20998;&#20986;&#24178;&#39044;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#25928;&#24212;&#65292;&#24182;&#32771;&#34385;&#38271;&#31243;&#20013;&#20171;&#21464;&#37327;-&#32467;&#26524;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.09656</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#36807;&#31243;&#30340;&#26102;&#38388;&#22240;&#26524;&#20013;&#20171;&#65306;&#21307;&#30103;&#24178;&#39044;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Temporal Causal Mediation through a Point Process: Direct and Indirect Effects of Healthcare Interventions. (arXiv:2306.09656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09656
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38750;&#21442;&#25968;&#30340;&#20013;&#20171;&#21464;&#37327;-&#32467;&#26524;&#27169;&#22411;&#65292;&#29992;&#20110;&#21307;&#30103;&#24178;&#39044;&#30340;&#22240;&#26524;&#20998;&#26512;&#65292;&#21487;&#20197;&#21306;&#20998;&#20986;&#24178;&#39044;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#25928;&#24212;&#65292;&#24182;&#32771;&#34385;&#38271;&#31243;&#20013;&#20171;&#21464;&#37327;-&#32467;&#26524;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20915;&#23450;&#36866;&#24403;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#38656;&#35201;&#24314;&#31435;&#19968;&#20010;&#27835;&#30103;&#12289;&#32467;&#26524;&#21644;&#28508;&#22312;&#20013;&#20171;&#21464;&#37327;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#21487;&#20197;&#21306;&#20998;&#24178;&#39044;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#25928;&#24212;&#65292;&#20294;&#22810;&#25968;&#30740;&#31350;&#37117;&#22312;&#38745;&#24577;&#29615;&#22659;&#19979;&#36827;&#34892;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#25968;&#25454;&#20197;&#22797;&#26434;&#30340;&#12289;&#19981;&#35268;&#21017;&#30340;&#26102;&#38388;&#24207;&#21015;&#24418;&#24335;&#21576;&#29616;&#65292;&#32780;&#19988;&#27835;&#30103;&#12289;&#32467;&#26524;&#21644;&#20013;&#20171;&#21464;&#37327;&#22312;&#26102;&#38388;&#19978;&#26377;&#21160;&#24577;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#21160;&#24577;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26041;&#27861;&#23616;&#38480;&#20110;&#35268;&#21017;&#30340;&#27979;&#37327;&#38388;&#38548;&#12289;&#31616;&#21333;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#24182;&#24573;&#30053;&#20102;&#20013;&#20171;&#21464;&#37327;-&#32467;&#26524;&#20043;&#38388;&#30340;&#38271;&#31243;&#20132;&#20114;&#20316;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#20013;&#20171;&#21464;&#37327;-&#32467;&#26524;&#27169;&#22411;&#65292;&#20854;&#20013;&#20013;&#20171;&#21464;&#37327;&#34987;&#20551;&#23450;&#20026;&#19982;&#32467;&#26524;&#36807;&#31243;&#20114;&#21160;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#12290;&#20351;&#29992;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#23545;&#32467;&#26524;&#30340;&#22806;&#37096;&#24178;&#39044;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#25928;&#24212;&#65292;&#26174;&#31034;&#20102;&#27599;&#31181;&#25928;&#24212;&#22914;&#20309;&#24433;&#21709;&#25972;&#20010;&#26410;&#26469;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#21322;&#26376;&#26495;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deciding on an appropriate intervention requires a causal model of a treatment, the outcome, and potential mediators. Causal mediation analysis lets us distinguish between direct and indirect effects of the intervention, but has mostly been studied in a static setting. In healthcare, data come in the form of complex, irregularly sampled time-series, with dynamic interdependencies between a treatment, outcomes, and mediators across time. Existing approaches to dynamic causal mediation analysis are limited to regular measurement intervals, simple parametric models, and disregard long-range mediator--outcome interactions. To address these limitations, we propose a non-parametric mediator--outcome model where the mediator is assumed to be a temporal point process that interacts with the outcome process. With this model, we estimate the direct and indirect effects of an external intervention on the outcome, showing how each of these affects the whole future trajectory. We demonstrate on sem
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#20102;&#38750;&#32467;&#26500;&#21270;&#21512;&#25104;&#20648;&#23618;&#20013;CO$_2$&#27668;&#20113;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#30456;&#27604;&#26631;&#20934;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#20943;&#23569;&#30340;&#26102;&#38388;&#35823;&#24046;&#31215;&#32047;&#12290;</title><link>http://arxiv.org/abs/2306.09648</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26029;&#23618;&#20648;&#23618;&#20013;CO$_2$&#27668;&#20113;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Learning CO$_2$ plume migration in faulted reservoirs with Graph Neural Networks. (arXiv:2306.09648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#20102;&#38750;&#32467;&#26500;&#21270;&#21512;&#25104;&#20648;&#23618;&#20013;CO$_2$&#27668;&#20113;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#30456;&#27604;&#26631;&#20934;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#20943;&#23569;&#30340;&#26102;&#38388;&#35823;&#24046;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#22320;&#19979;&#27969;&#21160;&#38382;&#39064;&#65288;&#22914;CO$_2$&#22320;&#36136;&#20648;&#23384;&#65289;&#30340;&#25968;&#20540;&#27169;&#25311;&#30340;&#26377;&#25928;&#34917;&#20805;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20934;&#30830;&#22320;&#25429;&#25417;&#26029;&#23618;&#23545;CO$_2$&#27668;&#20113;&#36801;&#31227;&#30340;&#24433;&#21709;&#65292;&#20173;&#26159;&#35768;&#22810;&#29616;&#26377;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;&#31070;&#32463;&#31639;&#23376;(Neural Operators)&#28145;&#24230;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#39046;&#22495;&#26368;&#26032;&#21457;&#23637;&#30340;&#22270;&#21367;&#31215;&#38271;&#26102;&#35760;&#24518;&#65288;GConvLSTM&#65289;&#21644;&#19968;&#20010;&#21333;&#27493;GNN&#27169;&#22411;&#65288;MeshGraphNet(MGN)&#65289;&#30456;&#32467;&#21512;&#30340;&#22270;&#31070;&#32463;&#27169;&#22411;&#65292;&#20197;&#23545;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#32593;&#26684;&#36827;&#34892;&#25805;&#20316;&#65292;&#38480;&#21046;&#26102;&#38388;&#35823;&#24046;&#32047;&#35745;&#12290;&#22312;&#19968;&#20010;&#20855;&#26377;&#19981;&#36879;&#27700;&#26029;&#23618;&#30340;&#21512;&#25104;&#20648;&#23618;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#27668;&#20307;&#39281;&#21644;&#24230;&#21644;&#23380;&#38553;&#21387;&#21147;&#30340;&#26102;&#38388;&#28436;&#21270;&#12290;&#19982;&#26631;&#20934;MGN&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#20943;&#23569;&#30340;&#26102;&#38388;&#35823;&#24046;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning-based surrogate models provide an efficient complement to numerical simulations for subsurface flow problems such as CO$_2$ geological storage. Accurately capturing the impact of faults on CO$_2$ plume migration remains a challenge for many existing deep learning surrogate models based on Convolutional Neural Networks (CNNs) or Neural Operators. We address this challenge with a graph-based neural model leveraging recent developments in the field of Graph Neural Networks (GNNs). Our model combines graph-based convolution Long-Short-Term-Memory (GConvLSTM) with a one-step GNN model, MeshGraphNet (MGN), to operate on complex unstructured meshes and limit temporal error accumulation. We demonstrate that our approach can accurately predict the temporal evolution of gas saturation and pore pressure in a synthetic reservoir with impermeable faults. Our results exhibit a better accuracy and a reduced temporal error accumulation compared to the standard MGN model. We also show the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#30340;&#32039;&#24615;&#29305;&#24449;&#26469;&#32416;&#27491;VAE&#27169;&#22411;&#20013;&#30340;&#29702;&#35770;&#32570;&#38519;&#24182;&#32553;&#23567;&#20869;&#28857;&#19982;&#31163;&#32676;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#32467;&#21512;&#24314;&#27169;&#25216;&#26415;&#21644;&#32467;&#26500;&#30693;&#35782;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09646</link><description>&lt;p&gt;
&#29992;&#20110;&#21387;&#32553;&#28508;&#22312;&#34920;&#31034;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vacant Holes for Unsupervised Detection of the Outliers in Compact Latent Representation. (arXiv:2306.09646v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#30340;&#32039;&#24615;&#29305;&#24449;&#26469;&#32416;&#27491;VAE&#27169;&#22411;&#20013;&#30340;&#29702;&#35770;&#32570;&#38519;&#24182;&#32553;&#23567;&#20869;&#28857;&#19982;&#31163;&#32676;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#32467;&#21512;&#24314;&#27169;&#25216;&#26415;&#21644;&#32467;&#26500;&#30693;&#35782;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#23545;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#25805;&#20316;&#65292;&#26816;&#27979;&#24322;&#24120;&#20540;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32780;&#35328;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#20123;&#32593;&#32476;&#23545;&#20110;&#27492;&#31867;&#36755;&#20837;&#26174;&#31034;&#20986;&#36807;&#24230;&#33258;&#20449;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;&#20801;&#35768;&#20272;&#35745;&#36755;&#20837;&#27010;&#29575;&#23494;&#24230;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20063;&#38590;&#20197;&#23436;&#25104;&#27492;&#20219;&#21153;&#12290;&#26412;&#25991;&#20027;&#35201;&#38598;&#20013;&#20110;&#36825;&#31867;&#27169;&#22411;&#20013;&#30340;&#19968;&#31181;&#65306;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#32463;&#20856;VAE&#27169;&#22411;&#20551;&#35774;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#29702;&#35770;&#32570;&#38519;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#32039;&#24615;&#20316;&#20026;&#20174;&#28145;&#24230;&#31070;&#32463;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#30340;&#22270;&#20687;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#32416;&#27491;&#36825;&#19968;&#32570;&#38519;&#65292;&#24182;&#33719;&#24471;&#23558;&#22270;&#20687;&#21387;&#32553;&#22312;&#30830;&#23450;&#38480;&#21046;&#20869;&#30340;&#21487;&#35777;&#30028;&#38480;&#26469;&#21516;&#26102;&#21387;&#32553;&#20869;&#28857;&#21644;&#31163;&#32676;&#28857;&#30340;&#25163;&#27573;&#12290;&#25105;&#20204;&#37319;&#29992;&#20004;&#31181;&#26041;&#27861;&#23454;&#29616;&#32039;&#24615;&#65306;&#65288;i&#65289;&#20122;&#21382;&#23665;&#22823;&#22827;&#25193;&#23637;&#21644;&#65288;ii&#65289;&#23545;VAE&#32534;&#30721;&#22120;&#30340;&#26144;&#23556;&#36827;&#34892;&#22266;&#23450;&#30340;Lipschitz&#36830;&#32493;&#24615;&#24120;&#25968;&#12290;&#26368;&#21518;&#20294;&#20063;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21033;&#29992;&#24050;&#26377;&#24314;&#27169;&#25216;&#26415;&#21644;&#32467;&#26500;&#30693;&#35782;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of the outliers is pivotal for any machine learning model deployed and operated in real-world. It is essential for the Deep Neural Networks that were shown to be overconfident with such inputs. Moreover, even deep generative models that allow estimation of the probability density of the input fail in achieving this task. In this work, we concentrate on the specific type of these models: Variational Autoencoders (VAEs). First, we unveil a significant theoretical flaw in the assumption of the classical VAE model. Second, we enforce an accommodating topological property to the image of the deep neural mapping to the latent space: compactness to alleviate the flaw and obtain the means to provably bound the image within the determined limits by squeezing both inliers and outliers together. We enforce compactness using two approaches: (i) Alexandroff extension and (ii) fixed Lipschitz continuity constant on the mapping of the encoder of the VAEs. Finally and most importantly, we di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BISCUIT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;&#35774;&#32622;&#20013;&#30830;&#23450;&#22240;&#26524;&#21464;&#37327;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#21551;&#21457;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.09643</link><description>&lt;p&gt;
BISCUIT: &#20174;&#20108;&#36827;&#21046;&#20132;&#20114;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
BISCUIT: Causal Representation Learning from Binary Interactions. (arXiv:2306.09643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BISCUIT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;&#35774;&#32622;&#20013;&#30830;&#23450;&#22240;&#26524;&#21464;&#37327;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#21551;&#21457;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#21644;&#23454;&#20307;AI&#31561;&#24212;&#29992;&#20013;&#65292;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#22240;&#26524;&#21464;&#37327;&#20197;&#21450;&#22914;&#20309;&#23545;&#23427;&#20204;&#36827;&#34892;&#24178;&#39044;&#20855;&#26377;&#26680;&#24515;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;&#35774;&#32622;&#20013;&#65292;&#20363;&#22914;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#27169;&#22411;&#20013;&#20173;&#28982;&#21487;&#20197;&#30830;&#23450;&#22240;&#26524;&#21464;&#37327;&#65292;&#22914;&#26524;&#26234;&#33021;&#20307;&#19982;&#22240;&#26524;&#21464;&#37327;&#30340;&#20132;&#20114;&#21487;&#20197;&#29992;&#26410;&#30693;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#36825;&#19968;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#25552;&#20986;BISCUIT&#65292;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#22240;&#26524;&#21464;&#37327;&#21450;&#20854;&#23545;&#24212;&#20108;&#36827;&#21046;&#20132;&#20114;&#21464;&#37327;&#30340;&#26041;&#27861;&#12290;&#22312;&#19977;&#20010;&#26426;&#22120;&#20154;&#21551;&#21457;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;BISCUIT&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#22240;&#26524;&#21464;&#37327;&#65292;&#29978;&#33267;&#21487;&#20197;&#25193;&#23637;&#21040;&#22797;&#26434;&#30340;&#12289;&#36924;&#30495;&#30340;&#23454;&#20307;AI&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the causal variables of an environment and how to intervene on them is of core value in applications such as robotics and embodied AI. While an agent can commonly interact with the environment and may implicitly perturb the behavior of some of these causal variables, often the targets it affects remain unknown. In this paper, we show that causal variables can still be identified for many common setups, e.g., additive Gaussian noise models, if the agent's interactions with a causal variable can be described by an unknown binary variable. This happens when each causal variable has two different mechanisms, e.g., an observational and an interventional one. Using this identifiability result, we propose BISCUIT, a method for simultaneously learning causal variables and their corresponding binary interaction variables. On three robotic-inspired datasets, BISCUIT accurately identifies causal variables and can even be scaled to complex, realistic environments for embodied AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#36328;&#22495;&#26465;&#20214;&#19979;&#26816;&#27979;&#27602;&#24615;&#29255;&#27573;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#29616;&#25104;&#30340;&#35789;&#20856;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#36328;&#22495;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;&#29992;&#20110;&#39046;&#22495;&#20869;&#30340;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#26576;&#20123;&#31867;&#22411;&#30340;&#20551;&#38451;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09642</link><description>&lt;p&gt;
&#36328;&#22495;&#27602;&#24615;&#29255;&#27573;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Toxic Spans Detection. (arXiv:2306.09642v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#36328;&#22495;&#26465;&#20214;&#19979;&#26816;&#27979;&#27602;&#24615;&#29255;&#27573;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#29616;&#25104;&#30340;&#35789;&#20856;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#36328;&#22495;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;&#29992;&#20110;&#39046;&#22495;&#20869;&#30340;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#26576;&#20123;&#31867;&#22411;&#30340;&#20551;&#38451;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#27602;&#24615;&#35821;&#35328;&#30340;&#21160;&#24577;&#24615;&#65292;&#33258;&#21160;&#26816;&#27979;&#27602;&#24615;&#29255;&#27573;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#36973;&#36935;&#20998;&#24067;&#36716;&#31227;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#26816;&#27979;&#36328;&#22495;&#26465;&#20214;&#19979;&#27602;&#24615;&#29255;&#27573;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#35789;&#20856;&#12289;&#26681;&#22240;&#25277;&#21462;&#21644;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#35789;&#20856;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#36328;&#22495;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#36328;&#39046;&#22495;&#35823;&#24046;&#20998;&#26512;&#34920;&#26126;&#65306;&#65288;1&#65289;&#26681;&#22240;&#25552;&#21462;&#26041;&#27861;&#23481;&#26131;&#20986;&#29616;&#20551;&#36127;&#32467;&#26524;&#65292;&#32780;&#65288;2&#65289;&#35821;&#35328;&#27169;&#22411;&#23613;&#31649;&#22312;&#39046;&#22495;&#20869;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#20854;&#26126;&#30830;&#25552;&#21462;&#27602;&#24615;&#21333;&#35789;&#30340;&#25968;&#37327;&#27604;&#35789;&#20856;&#23569;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#26576;&#20123;&#31867;&#22411;&#30340;&#20551;&#38451;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/sfschouten/toxic-cross-domain &#12290;
&lt;/p&gt;
&lt;p&gt;
Given the dynamic nature of toxic language use, automated methods for detecting toxic spans are likely to encounter distributional shift. To explore this phenomenon, we evaluate three approaches for detecting toxic spans under cross-domain conditions: lexicon-based, rationale extraction, and fine-tuned language models. Our findings indicate that a simple method using off-the-shelf lexicons performs best in the cross-domain setup. The cross-domain error analysis suggests that (1) rationale extraction methods are prone to false negatives, while (2) language models, despite performing best for the in-domain case, recall fewer explicitly toxic words than lexicons and are prone to certain types of false positives. Our code is publicly available at: https://github.com/sfschouten/toxic-cross-domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26410;&#26631;&#27880;&#35270;&#39057;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21512;&#25104;&#38899;&#39057;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#27169;&#24577;&#20316;&#20026;&#26725;&#26753;&#26469;&#23398;&#20064;&#25991;&#26412;-&#38899;&#39057;&#23545;&#24212;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#35270;&#39057;&#30340;&#38899;&#36712;&#12290;&#20351;&#29992;CLIP&#22270;&#20687;&#26597;&#35810;&#26465;&#20214;&#36827;&#34892;&#38646;&#26679;&#26412;&#27169;&#24577;&#36716;&#25442;&#12290;&#24182;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#20808;&#39564;&#27169;&#22411;&#65292;&#29983;&#25104;&#23545;&#24212;&#20110;CLIP&#25991;&#26412;&#23884;&#20837;&#30340;CLIP&#22270;&#20687;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09635</link><description>&lt;p&gt;
CLIPSonic&#65306;&#20351;&#29992;&#26410;&#26631;&#27880;&#35270;&#39057;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#30340;&#25991;&#26412;&#21512;&#25104;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models. (arXiv:2306.09635v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26410;&#26631;&#27880;&#35270;&#39057;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21512;&#25104;&#38899;&#39057;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#27169;&#24577;&#20316;&#20026;&#26725;&#26753;&#26469;&#23398;&#20064;&#25991;&#26412;-&#38899;&#39057;&#23545;&#24212;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#35270;&#39057;&#30340;&#38899;&#36712;&#12290;&#20351;&#29992;CLIP&#22270;&#20687;&#26597;&#35810;&#26465;&#20214;&#36827;&#34892;&#38646;&#26679;&#26412;&#27169;&#24577;&#36716;&#25442;&#12290;&#24182;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#20808;&#39564;&#27169;&#22411;&#65292;&#29983;&#25104;&#23545;&#24212;&#20110;CLIP&#25991;&#26412;&#23884;&#20837;&#30340;CLIP&#22270;&#20687;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#37327;&#25104;&#23545;&#30340;&#25991;&#26412;&#21644;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#25991;&#26412;&#21512;&#25104;&#38899;&#39057;&#12290; &#28982;&#32780;&#65292;&#24102;&#26377;&#39640;&#36136;&#37327;&#25991;&#26412;&#27880;&#37322;&#30340;&#38899;&#39057;&#24405;&#38899;&#21487;&#33021;&#38590;&#20197;&#33719;&#21462;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#26469;&#36827;&#34892;&#25991;&#26412;&#21512;&#25104;&#38899;&#39057;&#30340;&#30740;&#31350;&#12290; &#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#35270;&#35273;&#27169;&#24577;&#20316;&#20026;&#26725;&#26753;&#26469;&#23398;&#20064;&#25152;&#38656;&#30340;&#25991;&#26412;-&#38899;&#39057;&#23545;&#24212;&#20851;&#31995;&#12290; &#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#35270;&#39057;&#30340;&#38899;&#39057;&#36712;&#36947;&#65292;&#32473;&#23450;&#30001;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#32534;&#30721;&#30340;&#35270;&#39057;&#24103;&#12290; &#22312;&#27979;&#35797;&#26102;&#38388;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#32034;&#25191;&#34892;&#38646;&#26679;&#26412;&#27169;&#24577;&#36716;&#25442;&#65292;&#24182;&#20351;&#29992;CLIP&#32534;&#30721;&#30340;&#25991;&#26412;&#26597;&#35810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290; &#20294;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#22270;&#20687;&#26597;&#35810;&#30456;&#27604;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290; &#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#20808;&#39564;&#27169;&#22411;&#65292;&#29983;&#25104;&#32473;&#23450;CLIP&#25991;&#26412;&#23884;&#20837;&#30340;CLIP&#22270;&#20687;&#23884;&#20837;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;
&lt;/p&gt;
&lt;p&gt;
Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that 
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24130;&#24459;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#21807;&#19968;&#24179;&#31283;&#20998;&#24067;&#19988;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#36830;&#32493;&#21644;&#31163;&#25955;&#21270;&#24130;&#24459;&#21160;&#24577;&#30340;&#20986;&#29616;&#26102;&#38388;&#26469;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09624</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#24130;&#24459;&#21160;&#24577;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Power-law Dynamic arising from machine learning. (arXiv:2306.09624v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24130;&#24459;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#21807;&#19968;&#24179;&#31283;&#20998;&#24067;&#19988;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#36830;&#32493;&#21644;&#31163;&#25955;&#21270;&#24130;&#24459;&#21160;&#24577;&#30340;&#20986;&#29616;&#26102;&#38388;&#26469;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#36825;&#31181;&#26041;&#31243;&#36215;&#28304;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#30740;&#31350;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#24130;&#24459;&#21160;&#24577;&#65292;&#22240;&#20026;&#20854;&#24179;&#31283;&#20998;&#24067;&#19981;&#33021;&#20855;&#26377;&#20122;&#39640;&#26031;&#23614;&#37096;&#24182;&#26381;&#20174;&#24130;&#24459;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#23398;&#20064;&#29575;&#36275;&#22815;&#23567;&#65292;&#24130;&#24459;&#21160;&#24577;&#26159;&#36941;&#21382;&#30340;&#19988;&#20855;&#26377;&#21807;&#19968;&#30340;&#24179;&#31283;&#20998;&#24067;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#23427;&#30340;&#39318;&#27425;&#23384;&#22312;&#26102;&#38388;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#36830;&#32493;&#30340;&#24130;&#24459;&#21160;&#24577;&#21450;&#20854;&#31163;&#25955;&#21270;&#22312;&#36864;&#20986;&#26102;&#38388;&#19978;&#30340;&#24046;&#24322;&#12290;&#36825;&#31181;&#27604;&#36739;&#21487;&#20197;&#24110;&#21161;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a kind of new SDE that was arisen from the research on optimization in machine learning, we call it power-law dynamic because its stationary distribution cannot have sub-Gaussian tail and obeys power-law. We prove that the power-law dynamic is ergodic with unique stationary distribution, provided the learning rate is small enough. We investigate its first exist time. In particular, we compare the exit times of the (continuous) power-law dynamic and its discretization. The comparison can help guide machine learning algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36229;&#22270;&#33021;&#37327;&#20989;&#25968;&#30340;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#30456;&#27604;&#20256;&#32479;GNN&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.09623</link><description>&lt;p&gt;
&#20174;&#36229;&#22270;&#33021;&#37327;&#20989;&#25968;&#21040;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
From Hypergraph Energy Functions to Hypergraph Neural Networks. (arXiv:2306.09623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36229;&#22270;&#33021;&#37327;&#20989;&#25968;&#30340;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#30456;&#27604;&#20256;&#32479;GNN&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#34920;&#31034;&#23454;&#20307;&#20043;&#38388;&#39640;&#38454;&#20132;&#20114;&#30340;&#24378;&#22823;&#25277;&#35937;&#27169;&#22411;&#12290;&#20026;&#20102;&#22312;&#23454;&#29616;&#19979;&#28216;&#39044;&#27979;&#30340;&#36807;&#31243;&#20013;&#21033;&#29992;&#36825;&#20123;&#20851;&#31995;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22810;&#31181;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#24314;&#31435;&#22312;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#25991;&#29486;&#30340;&#20808;&#39537;&#19978;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31867;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#21442;&#25968;&#21270;&#36229;&#22270;&#27491;&#21017;&#21270;&#33021;&#37327;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#33021;&#37327;&#30340;&#26497;&#23567;&#20540;&#26377;&#25928;&#22320;&#20316;&#20026;&#33410;&#28857;&#23884;&#20837;&#22120;&#65292;&#20877;&#37197;&#21512;&#19968;&#20010;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36890;&#36807;&#19968;&#20010;&#30417;&#30563;&#30340;&#21452;&#23618;&#20248;&#21270;&#36807;&#31243;&#23454;&#29616;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#24314;&#35758;&#30340;&#21452;&#23618;&#36229;&#22270;&#20248;&#21270;&#20013;&#20986;&#29616;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#38544;&#24335;&#26550;&#26500;&#21644;&#24120;&#29992;GNN&#26550;&#26500;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are a powerful abstraction for representing higher-order interactions between entities of interest. To exploit these relationships in making downstream predictions, a variety of hypergraph neural network architectures have recently been proposed, in large part building upon precursors from the more traditional graph neural network (GNN) literature. Somewhat differently, in this paper we begin by presenting an expressive family of parameterized, hypergraph-regularized energy functions. We then demonstrate how minimizers of these energies effectively serve as node embeddings that, when paired with a parameterized classifier, can be trained end-to-end via a supervised bilevel optimization process. Later, we draw parallels between the implicit architecture of the predictive models emerging from the proposed bilevel hypergraph optimization, and existing GNN architectures in common use. Empirically, we demonstrate state-of-the-art results on various hypergraph node classification
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#39640;&#32500;&#29983;&#25104;&#27169;&#22411;&#27979;&#24230;&#20013;&#20351;&#29992;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#25351;&#26631;&#23384;&#22312;&#19981;&#23545;&#31216;&#24615;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20462;&#27491;&#36825;&#31181;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.09618</link><description>&lt;p&gt;
&#39640;&#32500;&#29983;&#25104;&#27169;&#22411;&#27979;&#24230;&#20013;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#34913;&#37327;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#20004;&#20010;&#37325;&#35201;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions. (arXiv:2306.09618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#39640;&#32500;&#29983;&#25104;&#27169;&#22411;&#27979;&#24230;&#20013;&#20351;&#29992;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#25351;&#26631;&#23384;&#22312;&#19981;&#23545;&#31216;&#24615;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20462;&#27491;&#36825;&#31181;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24230;&#21644;&#21484;&#22238;&#26159;&#34913;&#37327;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#30340;&#20004;&#20010;&#37325;&#35201;&#25351;&#26631;&#65292;&#23427;&#20204;&#20998;&#21035;&#34987;&#29992;&#26469;&#27979;&#37327;&#27169;&#22411;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;k&#36817;&#37051;&#30340;&#24120;&#35265;&#36924;&#36817;&#26041;&#27861;&#65292;&#36825;&#20123;&#25351;&#26631;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#23481;&#26131;&#20986;&#29616;&#35823;&#23548;&#24615;&#32467;&#35770;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20004;&#20010;&#19982;&#30495;&#23454;&#20998;&#24067;&#30340;&#25903;&#25345;&#31561;&#36317;&#31163;&#30340;&#27169;&#22411;&#20998;&#24067;&#21487;&#20197;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#23545;&#31216;&#24615;&#12290;&#38024;&#23545;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#20462;&#27491;&#26041;&#27861;&#26469;&#28040;&#38500;&#36825;&#31181;&#38169;&#35823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precision and Recall are two prominent metrics of generative performance, which were proposed to separately measure the fidelity and diversity of generative models. Given their central role in comparing and improving generative models, understanding their limitations are crucially important. To that end, in this work, we identify a critical flaw in the common approximation of these metrics using k-nearest-neighbors, namely, that the very interpretations of fidelity and diversity that are assigned to Precision and Recall can fail in high dimensions, resulting in very misleading conclusions. Specifically, we empirically and theoretically show that as the number of dimensions grows, two model distributions with supports at equal point-wise distance from the support of the real distribution, can have vastly different Precision and Recall regardless of their respective distributions, hence an emergent asymmetry in high dimensions. Based on our theoretical insights, we then provide simple ye
&lt;/p&gt;</description></item><item><title>HomoGCL&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21516;&#36136;&#24615;&#21407;&#21017;&#25193;&#23637;&#27491;&#38598;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09614</link><description>&lt;p&gt;
HomoGCL&#65306;&#37325;&#26032;&#24605;&#32771;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21516;&#36136;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
HomoGCL: Rethinking Homophily in Graph Contrastive Learning. (arXiv:2306.09614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09614
&lt;/p&gt;
&lt;p&gt;
HomoGCL&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21516;&#36136;&#24615;&#21407;&#21017;&#25193;&#23637;&#27491;&#38598;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064; (CL) &#24050;&#25104;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#22270;&#24418;&#20013;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#24120;&#36981;&#24490;&#8220;&#22686;&#24378;-&#23545;&#27604;&#8221;&#23398;&#20064;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#19981;&#21516;&#65292;&#22270;&#24418;&#39046;&#22495;&#30340;&#23545;&#27604;&#23398;&#20064;&#21363;&#20351;&#27809;&#26377;&#25968;&#25454;&#22686;&#24378;&#20063;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#23545;&#36825;&#19968;&#29616;&#35937;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#35748;&#20026;&#21516;&#36136;&#24615;&#21363;&#8220;&#29289;&#20197;&#31867;&#32858;&#8221;&#30340;&#21407;&#21017;&#22312;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#21463;&#21040;&#36825;&#19968;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HomoGCL&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#37051;&#23621;&#29305;&#23450;&#37325;&#35201;&#24615;&#30340;&#37051;&#23621;&#33410;&#28857;&#26469;&#25193;&#23637;&#27491;&#38598;&#12290;&#29702;&#35770;&#19978;&#35762;&#65292;HomoGCL&#24341;&#20837;&#20102;&#21407;&#22987;&#33410;&#28857;&#29305;&#24449;&#21644;&#33410;&#28857;&#23884;&#20837;&#22312;&#22686;&#24378;&#35270;&#22270;&#20013;&#30340;&#20114;&#20449;&#24687;&#30340;&#26356;&#20005;&#26684;&#30340;&#19979;&#38480;&#12290;&#27492;&#22806;&#65292;HomoGCL&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#20197;&#25554;&#20214;&#26041;&#24335;&#32452;&#21512;&#36215;&#26469;&#65292;&#32780;&#21482;&#26377;&#36731;&#24494;&#39069;&#22806;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;HomoGCL&#20135;&#29983;&#20102;&#22810;&#31181;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning (CL) has become the de-facto learning paradigm in self-supervised learning on graphs, which generally follows the "augmenting-contrasting" learning scheme. However, we observe that unlike CL in computer vision domain, CL in graph domain performs decently even without augmentation. We conduct a systematic analysis of this phenomenon and argue that homophily, i.e., the principle that "like attracts like", plays a key role in the success of graph CL. Inspired to leverage this property explicitly, we propose HomoGCL, a model-agnostic framework to expand the positive set using neighbor nodes with neighbor-specific significances. Theoretically, HomoGCL introduces a stricter lower bound of the mutual information between raw node features and node embeddings in augmented views. Furthermore, HomoGCL can be combined with existing graph CL models in a plug-and-play way with light extra computational overhead. Extensive experiments demonstrate that HomoGCL yields multiple stat
&lt;/p&gt;</description></item><item><title>GraphSHA&#26159;&#19968;&#31181;&#33021;&#22815;&#32531;&#35299;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#22312;&#32508;&#21512;&#26356;&#38590;&#30340;&#36739;&#23567;&#31867;&#21035;&#26679;&#26412;&#20197;&#25193;&#22823;&#36739;&#23567;&#31867;&#21035;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;SemiMixup&#27169;&#22359;&#20197;&#36991;&#20813;&#25193;&#22823;&#30340;&#36793;&#30028;&#36829;&#21453;&#37051;&#23621;&#31867;&#21035;&#30340;&#23376;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.09612</link><description>&lt;p&gt;
GraphSHA&#65306;&#29992;&#20110;&#31867;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#30340;&#26356;&#38590;&#26679;&#26412;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
GraphSHA: Synthesizing Harder Samples for Class-Imbalanced Node Classification. (arXiv:2306.09612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09612
&lt;/p&gt;
&lt;p&gt;
GraphSHA&#26159;&#19968;&#31181;&#33021;&#22815;&#32531;&#35299;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#22312;&#32508;&#21512;&#26356;&#38590;&#30340;&#36739;&#23567;&#31867;&#21035;&#26679;&#26412;&#20197;&#25193;&#22823;&#36739;&#23567;&#31867;&#21035;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;SemiMixup&#27169;&#22359;&#20197;&#36991;&#20813;&#25193;&#22823;&#30340;&#36793;&#30028;&#36829;&#21453;&#37051;&#23621;&#31867;&#21035;&#30340;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#19981;&#24179;&#34913;&#26159;&#25351;&#26576;&#20123;&#31867;&#21035;&#30340;&#23454;&#20363;&#25968;&#37327;&#27604;&#20854;&#20182;&#31867;&#21035;&#23569;&#24471;&#22810;&#30340;&#29616;&#35937;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#32467;&#26500;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20250;&#20302;&#20272;&#36739;&#23567;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20027;&#31867;&#21035;&#30340;&#23376;&#31354;&#38388;&#25380;&#21387;&#36739;&#23567;&#31867;&#21035;&#30340;&#23376;&#31354;&#38388;&#26159;&#36825;&#31181;&#22833;&#36133;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#25105;&#20204;&#33258;&#28982;&#22320;&#21463;&#21040;&#21551;&#21457;&#65292;&#20197;&#25193;&#22823;&#36739;&#23567;&#31867;&#21035;&#30340;&#20915;&#31574;&#36793;&#30028;&#20026;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#32508;&#21512;&#26356;&#38590;&#30340;&#36739;&#23567;&#31867;&#21035;&#26679;&#26412;&#30340;&#36890;&#29992;&#26694;&#26550;GraphSHA&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36991;&#20813;&#25193;&#22823;&#30340;&#36739;&#23567;&#36793;&#30028;&#36829;&#21453;&#37051;&#23621;&#31867;&#21035;&#30340;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;SemiMixup&#30340;&#27169;&#22359;&#65292;&#23558;&#25193;&#22823;&#30340;&#36793;&#30028;&#20449;&#24687;&#20256;&#36882;&#21040;&#36739;&#23567;&#31867;&#30340;&#20869;&#37096;&#65292;&#21516;&#26102;&#38459;&#27490;&#20174;&#36739;&#23567;&#31867;&#21040;&#37051;&#23621;&#31867;&#30340;&#20449;&#24687;&#20256;&#25773;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;GraphSHA&#22312;&#25193;&#22823;&#36739;&#23567;&#31867;&#21035;&#30340;&#20915;&#31574;&#36793;&#30028;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance is the phenomenon that some classes have much fewer instances than others, which is ubiquitous in real-world graph-structured scenarios. Recent studies find that off-the-shelf Graph Neural Networks (GNNs) would under-represent minor class samples. We investigate this phenomenon and discover that the subspaces of minor classes being squeezed by those of the major ones in the latent space is the main cause of this failure. We are naturally inspired to enlarge the decision boundaries of minor classes and propose a general framework GraphSHA by Synthesizing HArder minor samples. Furthermore, to avoid the enlarged minor boundary violating the subspaces of neighbor classes, we also propose a module called SemiMixup to transmit enlarged boundary information to the interior of the minor classes while blocking information propagation from minor classes to neighbor classes. Empirically, GraphSHA shows its effectiveness in enlarging the decision boundaries of minor classes, as it 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25506;&#32034;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#20219;&#21153;&#20013;&#65292;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#34920;&#26684;&#31867;&#26816;&#27979;&#12289;&#21015;&#31867;&#22411;&#27880;&#37322;&#21644;&#32852;&#25509;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#26377;&#26395;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#32479;&#19968;&#22312;&#22522;&#30784;&#27169;&#22411;&#19979;&#12290;</title><link>http://arxiv.org/abs/2306.09610</link><description>&lt;p&gt;
CHORUS: &#32479;&#19968;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
CHORUS: Foundation Models for Unified Data Discovery and Exploration. (arXiv:2306.09610v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09610
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25506;&#32034;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#20219;&#21153;&#20013;&#65292;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#34920;&#26684;&#31867;&#26816;&#27979;&#12289;&#21015;&#31867;&#22411;&#27880;&#37322;&#21644;&#32852;&#25509;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#26377;&#26395;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#32479;&#19968;&#22312;&#22522;&#30784;&#27169;&#22411;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22522;&#30784;&#27169;&#22411;&#24212;&#29992;&#20110;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#20219;&#21153;&#20013;&#12290;&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#22312;&#21508;&#31181;&#19982;&#20854;&#35757;&#32451;&#26080;&#20851;&#30340;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#25968;&#25454;&#21457;&#29616;&#21644;&#25968;&#25454;&#25506;&#32034;&#39046;&#22495;&#38750;&#24120;&#36866;&#29992;&#12290;&#22312;&#35880;&#24910;&#20351;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#20855;&#26377;&#20248;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#20248;&#21270;&#34920;&#26684;&#31867;&#26816;&#27979;&#12289;&#21015;&#31867;&#22411;&#27880;&#37322;&#21644;&#32852;&#25509;&#21015;&#39044;&#27979;&#36825;&#19977;&#31181;&#20195;&#34920;&#24615;&#20219;&#21153;&#12290;&#22312;&#36825;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#24120;&#36229;&#36807;&#20154;&#31867;&#19987;&#23478;&#30340;&#20219;&#21153;&#34920;&#29616;&#12290;&#36825;&#34920;&#26126;&#20102;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#32479;&#19968;&#22312;&#22522;&#30784;&#27169;&#22411;&#19979;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the application of foundation models to data discovery and exploration tasks. Foundation models are large language models (LLMs) that show promising performance on a range of diverse tasks unrelated to their training. We show that these models are highly applicable to the data discovery and data exploration domain. When carefully used, they have superior capability on three representative tasks: table-class detection, column-type annotation and join-column prediction. On all three tasks, we show that a foundation-model-based approach outperforms the task-specific models and so the state of the art. Further, our approach often surpasses human-expert task performance. This suggests a future direction in which disparate data management tasks can be unified under foundation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#35774;&#22791;&#20043;&#38388;&#36890;&#36807;&#21327;&#20316;&#23436;&#25104;&#20998;&#25955;&#20219;&#21153;&#12290;&#36890;&#36807;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#30340;&#21327;&#20316;&#22270;&#65292;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09595</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#22270;&#27169;&#22411;&#20808;&#39564;&#19979;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structured Cooperative Learning with Graphical Model Priors. (arXiv:2306.09595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#35774;&#22791;&#20043;&#38388;&#36890;&#36807;&#21327;&#20316;&#23436;&#25104;&#20998;&#25955;&#20219;&#21153;&#12290;&#36890;&#36807;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#30340;&#21327;&#20316;&#22270;&#65292;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#20998;&#25955;&#35774;&#22791;&#19978;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#20010;&#24615;&#21270;&#24314;&#27169;&#65292;&#36825;&#20123;&#35774;&#22791;&#30340;&#23616;&#37096;&#25968;&#25454;&#21463;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#65288;SCooL&#65289;&#8221;&#65292;&#20854;&#20013;&#19968;&#20010;&#36328;&#35774;&#22791;&#30340;&#21327;&#20316;&#22270;&#30001;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#65292;&#20197;&#33258;&#21160;&#21327;&#35843;&#35774;&#22791;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#12290;&#36890;&#36807;&#36873;&#25321;&#26045;&#21152;&#19981;&#21516;&#32467;&#26500;&#30340;&#22270;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#25512;&#23548;&#20986;&#19968;&#31867;&#20016;&#23500;&#30340;&#29616;&#26377;&#21644;&#26032;&#22411;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#31181; SCooL &#30340;&#31034;&#20363;&#65292;&#22312;&#20854;&#20013;&#20197; Dirac &#20998;&#24067;&#12289;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#21644;&#27880;&#24847;&#21147;&#20316;&#20026;&#29983;&#25104;&#21327;&#20316;&#22270;&#30340;&#20808;&#39564;&#12290;&#36825;&#20123; EM &#31867;&#22411;&#30340;&#31639;&#27861;&#36890;&#36807;&#26356;&#26032;&#21327;&#20316;&#22270;&#21644;&#21327;&#21516;&#26412;&#22320;&#27169;&#22411;&#23398;&#20064;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#65292;&#20165;&#36890;&#36807;&#30417;&#35270;&#27169;&#22411;&#26356;&#26032;&#26469;&#20248;&#21270;&#21327;&#20316;&#22270;&#65292;&#20174;&#32780;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to train personalized models for different tasks on decentralized devices with limited local data. We propose "Structured Cooperative Learning (SCooL)", in which a cooperation graph across devices is generated by a graphical model prior to automatically coordinate mutual learning between devices. By choosing graphical models enforcing different structures, we can derive a rich class of existing and novel decentralized learning algorithms via variational inference. In particular, we show three instantiations of SCooL that adopt Dirac distribution, stochastic block model (SBM), and attention as the prior generating cooperation graphs. These EM-type algorithms alternate between updating the cooperation graph and cooperative learning of local models. They can automatically capture the cross-task correlations among devices by only monitoring their model updating in order to optimize the cooperation graph. We evaluate SCooL and compare it with existing decentralized learning met
&lt;/p&gt;</description></item><item><title>&#20026;&#35299;&#20915;SAR&#30446;&#26631;&#22270;&#20687;&#20998;&#31867;&#32570;&#20047;&#32479;&#19968;&#22522;&#20934;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FewSAR&#65292;&#19968;&#20010;&#21253;&#25324;15&#20010;&#32463;&#20856;&#26041;&#27861;&#30340;Python&#20195;&#30721;&#24211;&#21487;&#29992;&#20110;&#23567;&#26679;&#26412;SAR&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.09592</link><description>&lt;p&gt;
FewSAR: &#19968;&#31181;&#23567;&#26679;&#26412;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FewSAR: A Few-shot SAR Image Classification Benchmark. (arXiv:2306.09592v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09592
&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;SAR&#30446;&#26631;&#22270;&#20687;&#20998;&#31867;&#32570;&#20047;&#32479;&#19968;&#22522;&#20934;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FewSAR&#65292;&#19968;&#20010;&#21253;&#25324;15&#20010;&#32463;&#20856;&#26041;&#27861;&#30340;Python&#20195;&#30721;&#24211;&#21487;&#29992;&#20110;&#23567;&#26679;&#26412;SAR&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#23398;&#20064;&#26159;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#37325;&#35201;&#19988;&#22256;&#38590;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#21487;&#35265;&#20809;&#25968;&#25454;&#38598;&#30340;&#24555;&#36895;&#21457;&#23637;&#30456;&#27604;&#65292;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#30446;&#26631;&#22270;&#20687;&#20998;&#31867;&#30340;&#36827;&#23637;&#35201;&#24930;&#24471;&#22810;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#26159;&#36825;&#31181;&#29616;&#35937;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#65292;&#22312;&#24403;&#21069;&#25991;&#29486;&#20013;&#21487;&#33021;&#34987;&#20005;&#37325;&#24573;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23567;&#26679;&#26412;SAR&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#65288;FewSAR&#65289;&#12290;FewSAR&#21253;&#25324;&#19968;&#20010;&#30001;15&#20010;&#32463;&#20856;&#26041;&#27861;&#32452;&#25104;&#30340;Python&#20195;&#30721;&#24211;&#65292;&#24182;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#20197;&#29992;&#20110;&#23567;&#26679;&#26412;SAR&#22270;&#20687;&#20998;&#31867;&#12290;&#23427;&#20026;&#19981;&#21516;&#30340;&#23567;&#26679;&#26412;SAR&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35775;&#38382;&#21644;&#21487;&#23450;&#21046;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning (FSL) is one of the significant and hard problems in the field of image classification. However, in contrast to the rapid development of the visible light dataset, the progress in SAR target image classification is much slower. The lack of unified benchmark is a key reason for this phenomenon, which may be severely overlooked by the current literature. The researchers of SAR target image classification always report their new results on their own datasets and experimental setup. It leads to inefficiency in result comparison and impedes the further progress of this area. Motivated by this observation, we propose a novel few-shot SAR image classification benchmark (FewSAR) to address this issue. FewSAR consists of an open-source Python code library of 15 classic methods in three categories for few-shot SAR image classification. It provides an accessible and customizable testbed for different few-shot SAR image classification task. To further understanding the performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21453;&#39304;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#22312;&#24102;&#26377;&#39069;&#22806;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#39069;&#22806;&#35266;&#27979;&#30340;&#25968;&#37327;&#36229;&#36807;&#19968;&#23450;&#38408;&#20540;&#26102;&#65292;&#33021;&#20943;&#23569;&#21518;&#24724;&#30340;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.09588</link><description>&lt;p&gt;
&#29702;&#35299;&#22312;&#32447;&#23398;&#20064;&#20013;&#21453;&#39304;&#30340;&#20316;&#29992;&#21450;&#20854;&#20999;&#25442;&#25104;&#26412;(arXiv:2306.09588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Understanding the Role of Feedback in Online Learning with Switching Costs. (arXiv:2306.09588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21453;&#39304;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#22312;&#24102;&#26377;&#39069;&#22806;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#39069;&#22806;&#35266;&#27979;&#30340;&#25968;&#37327;&#36229;&#36807;&#19968;&#23450;&#38408;&#20540;&#26102;&#65292;&#33021;&#20943;&#23569;&#21518;&#24724;&#30340;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#21453;&#39304;&#23545;&#20999;&#25442;&#25104;&#26412;&#30340;&#20316;&#29992;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#36172;&#21338;&#21453;&#39304;&#19979;&#65292;&#26497;&#23567;&#21270;&#26368;&#22823;&#21518;&#24724;&#20026;$\widetilde{\Theta}(T^{2/3})$&#65292;&#24182;&#22312;&#23436;&#20840;&#20449;&#24687;&#21453;&#39304;&#19979;&#25552;&#39640;&#21040;$\widetilde{\Theta}(\sqrt{T})$&#65292;&#20854;&#20013;$T$&#20026;&#26102;&#38388;&#36328;&#24230;&#30340;&#38271;&#24230;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#28982;&#19981;&#30693;&#36947;&#21453;&#39304;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#36890;&#24120;&#22914;&#20309;&#24433;&#21709;&#21518;&#24724;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#24102;&#26377;&#39069;&#22806;&#35266;&#27979;&#30340;&#36172;&#21338;&#23398;&#20064;&#29615;&#22659;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#38500;&#20102;&#20856;&#22411;&#30340;&#36172;&#21338;&#21453;&#39304;&#22806;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#33258;&#30001;&#22320;&#36827;&#34892;&#24635;&#20849;$B_{ex}$&#27425;&#39069;&#22806;&#30340;&#35266;&#27979;&#12290;&#25105;&#20204;&#23436;&#20840;&#30830;&#23450;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26368;&#23567;&#21518;&#24724;&#65292;&#23427;&#34920;&#29616;&#20986;&#26377;&#36259;&#30340;&#30456;&#21464;&#29616;&#35937;&#65306;&#24403;$B_{ex}=O(T^{2/3})$&#26102;&#65292;&#21518;&#24724;&#20173;&#28982;&#20026;$\widetilde{\Theta}(T^{2/3})$&#65292;&#20294;&#24403;$B_{ex}=\Omega(T^{2/3})$&#26102;&#65292;&#23427;&#21464;&#20026;$\widetilde{\Theta}(T/\sqrt{B_{\mathrm{ex}}})$&#65292;&#38543;&#30528;&#39044;&#31639;$B_{ex}$&#30340;&#22686;&#21152;&#32780;&#24471;&#21040;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the role of feedback in online learning with switching costs. It has been shown that the minimax regret is $\widetilde{\Theta}(T^{2/3})$ under bandit feedback and improves to $\widetilde{\Theta}(\sqrt{T})$ under full-information feedback, where $T$ is the length of the time horizon. However, it remains largely unknown how the amount and type of feedback generally impact regret. To this end, we first consider the setting of bandit learning with extra observations; that is, in addition to the typical bandit feedback, the learner can freely make a total of $B_{\mathrm{ex}}$ extra observations. We fully characterize the minimax regret in this setting, which exhibits an interesting phase-transition phenomenon: when $B_{\mathrm{ex}} = O(T^{2/3})$, the regret remains $\widetilde{\Theta}(T^{2/3})$, but when $B_{\mathrm{ex}} = \Omega(T^{2/3})$, it becomes $\widetilde{\Theta}(T/\sqrt{B_{\mathrm{ex}}})$, which improves as the budget $B_{\mathrm{ex}}$ increases. To design a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#32622;&#20449;&#38598;&#32780;&#38750;&#21333;&#19968;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#24182;&#21457;&#29616;&#65292;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#20449;&#20219;&#38598;&#30340;&#20307;&#31215;&#26159;&#19968;&#31181;&#26377;&#24847;&#20041;&#30340;&#34913;&#37327;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#21017;&#27809;&#26377;&#36825;&#31181;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09586</link><description>&lt;p&gt;
&#19968;&#20010;&#32622;&#20449;&#38598;&#30340;&#25968;&#37327;&#26159;&#21542;&#26159;&#19968;&#31181;&#34913;&#37327;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#22909;&#26041;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the Volume of a Credal Set a Good Measure for Epistemic Uncertainty?. (arXiv:2306.09586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#32622;&#20449;&#38598;&#32780;&#38750;&#21333;&#19968;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#24182;&#21457;&#29616;&#65292;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#20449;&#20219;&#38598;&#30340;&#20307;&#31215;&#26159;&#19968;&#31181;&#26377;&#24847;&#20041;&#30340;&#34913;&#37327;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#21017;&#27809;&#26377;&#36825;&#31181;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#21644;&#37327;&#21270;&#22312;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#20316;&#20026;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#25105;&#20204;&#32771;&#34385;&#20449;&#20219;&#38598;&#65288;&#19968;&#32452;&#27010;&#29575;&#20998;&#24067;&#30340;&#20984;&#38598;&#65289;&#12290;&#20449;&#20219;&#38598;&#30340;&#20960;&#20309;&#34920;&#31034;&#20316;&#20026;$d$&#32500;&#22810;&#38754;&#20307;&#24847;&#21619;&#30528;&#23545;&#65288;&#35748;&#30693;&#65289;&#19981;&#30830;&#23450;&#24615;&#30340;&#20960;&#20309;&#30452;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20108;&#20803;&#20998;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#20449;&#20219;&#38598;&#30340;&#20960;&#20309;&#34920;&#31034;&#30340;&#20307;&#31215;&#26159;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#31181;&#26377;&#24847;&#20041;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#22312;&#22810;&#31867;&#20998;&#31867;&#26102;&#21017;&#19981;&#37027;&#20040;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25351;&#23450;&#21644;&#20351;&#29992;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26041;&#27861;&#20197;&#21450;&#24847;&#35782;&#21040;&#21487;&#33021;&#30340;&#39118;&#38505;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adequate uncertainty representation and quantification have become imperative in various scientific disciplines, especially in machine learning and artificial intelligence. As an alternative to representing uncertainty via one single probability measure, we consider credal sets (convex sets of probability measures). The geometric representation of credal sets as $d$-dimensional polytopes implies a geometric intuition about (epistemic) uncertainty. In this paper, we show that the volume of the geometric representation of a credal set is a meaningful measure of epistemic uncertainty in the case of binary classification, but less so for multi-class classification. Our theoretical findings highlight the crucial role of specifying and employing uncertainty measures in machine learning in an appropriate way, and for being aware of possible pitfalls.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#27169;&#31946;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#36873;&#25321;&#26368;&#26377;&#25928;&#22320;&#23436;&#25104;&#23494;&#30721;&#23398;&#21464;&#25442;&#36807;&#31243;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#23494;&#30721;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09583</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#23494;&#38053;&#23494;&#30721;&#23398;&#21464;&#25442;&#27169;&#31946;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Feature Selection with Key-based Cryptographic Transformations. (arXiv:2306.09583v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#27169;&#31946;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#36873;&#25321;&#26368;&#26377;&#25928;&#22320;&#23436;&#25104;&#23494;&#30721;&#23398;&#21464;&#25442;&#36807;&#31243;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#23494;&#30721;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23494;&#30721;&#23398;&#39046;&#22495;&#20013;&#65292;&#36873;&#25321;&#30456;&#20851;&#29305;&#24449;&#22312;&#22686;&#24378;&#23494;&#30721;&#31639;&#27861;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#31946;&#29305;&#24449;&#36873;&#25321;&#24212;&#29992;&#20110;&#22522;&#20110;&#23494;&#38053;&#30340;&#23494;&#30721;&#23398;&#21464;&#25442;&#12290;&#25552;&#20986;&#30340;&#27169;&#31946;&#29305;&#24449;&#36873;&#25321;&#21033;&#29992;&#27169;&#31946;&#36923;&#36753;&#30340;&#20248;&#21183;&#26469;&#35782;&#21035;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#22320;contributi&#23436;&#25104;&#23494;&#30721;&#23398;&#21464;&#25442;&#36807;&#31243;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#36890;&#36807;&#23558;&#27169;&#31946;&#29305;&#24449;&#36873;&#25321;&#32435;&#20837;&#22522;&#20110;&#23494;&#38053;&#30340;&#23494;&#30721;&#23398;&#21464;&#25442;&#20013;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#25269;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#24182;&#22686;&#24378;&#23494;&#30721;&#31995;&#32479;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#23454;&#39564;&#35780;&#20272;&#21487;&#20197;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36873;&#25321;&#23433;&#20840;&#23494;&#38053;&#29305;&#24449;&#26102;&#20855;&#26377;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#27169;&#31946;&#29305;&#24449;&#36873;&#25321;&#20316;&#20026;&#35774;&#35745;&#21644;&#20248;&#21270;&#22522;&#20110;&#23494;&#38053;&#30340;&#23494;&#30721;&#31639;&#27861;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of cryptography, the selection of relevant features plays a crucial role in enhancing the security and efficiency of cryptographic algorithms. This paper presents a novel approach of applying fuzzy feature selection to key-based cryptographic transformations. The proposed fuzzy feature selection leverages the power of fuzzy logic to identify and select optimal subsets of features that contribute most effectively to the cryptographic transformation process. By incorporating fuzzy feature selection into key-based cryptographic transformations, this research aims to improve the resistance against attacks and enhance the overall performance of cryptographic systems. Experimental evaluations may demonstrate the effectiveness of the proposed approach in selecting secure key features with minimal computational overhead. This paper highlights the potential of fuzzy feature selection as a valuable tool in the design and optimization of key-based cryptographic algorithms, contributi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25506;&#32034;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20302;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;LPO&#65292;&#20351;&#29992;&#26377;&#30028;eluder-&#32500;&#25968;&#21644;&#22312;&#32447;&#28789;&#25935;&#24230;&#25277;&#26679;&#26469;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#36739;&#23569;&#30340;&#26679;&#26412;&#37327;&#19979;&#33719;&#24471;&#20102;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.09554</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#28789;&#25935;&#24230;&#25277;&#26679;&#23454;&#29616;&#20855;&#25506;&#32034;&#24615;&#30340;&#20302;&#20999;&#25442;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Low-Switching Policy Gradient with Exploration via Online Sensitivity Sampling. (arXiv:2306.09554v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25506;&#32034;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20302;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;LPO&#65292;&#20351;&#29992;&#26377;&#30028;eluder-&#32500;&#25968;&#21644;&#22312;&#32447;&#28789;&#25935;&#24230;&#25277;&#26679;&#26469;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#36739;&#23569;&#30340;&#26679;&#26412;&#37327;&#19979;&#33719;&#24471;&#20102;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#26159;&#22686;&#24378;&#23398;&#20064;&#20013;&#24378;&#22823;&#30340;&#31639;&#27861;&#20043;&#19968;&#65292;&#20854;&#20855;&#22791;&#23545;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#28789;&#27963;&#24615;&#21644;&#22788;&#29702;&#27169;&#22411;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23384;&#22312;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20302;&#30340;&#32570;&#28857;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#21487;&#35777;&#26126;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#23545;&#20110;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#32780;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21482;&#36866;&#29992;&#20110;&#34920;&#26684;&#21644;&#32447;&#24615;&#35774;&#32622;&#65292;&#19988;&#36825;&#20123;&#25104;&#26524;&#30340;&#33391;&#22909;&#32467;&#26500;&#19981;&#33021;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#21033;&#29992;&#26368;&#36817;&#20215;&#20540;&#31639;&#27861;&#21253;&#25324;&#26377;&#30028;eluder-&#32500;&#25968;&#21644;&#22312;&#32447;&#28789;&#25935;&#24230;&#25277;&#26679;&#31561;&#30340;&#36827;&#23637;&#65292;&#38024;&#23545;&#19968;&#33324;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#35774;&#35745;&#20302;&#20999;&#25442;&#26679;&#26412;&#26377;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;LPO&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21482;&#38656;$\widetilde{O}(\frac{\text{poly}(d)}{\varepsilon^3})$&#20010;&#26679;&#26412;&#21363;&#21487;&#33719;&#24471;$\varepsilon$-&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy optimization methods are powerful algorithms in Reinforcement Learning (RL) for their flexibility to deal with policy parameterization and ability to handle model misspecification. However, these methods usually suffer from slow convergence rates and poor sample complexity. Hence it is important to design provably sample efficient algorithms for policy optimization. Yet, recent advances for this problems have only been successful in tabular and linear setting, whose benign structures cannot be generalized to non-linearly parameterized policies. In this paper, we address this problem by leveraging recent advances in value-based algorithms, including bounded eluder-dimension and online sensitivity sampling, to design a low-switching sample-efficient policy optimization algorithm, LPO, with general non-linear function approximation. We show that, our algorithm obtains an $\varepsilon$-optimal policy with only $\widetilde{O}(\frac{\text{poly}(d)}{\varepsilon^3})$ samples, where $\va
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2306.09549</link><description>&lt;p&gt;
QH9&#65306;QM9&#20998;&#23376;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#39044;&#27979;&#65292;&#20316;&#20026;&#31532;&#19968;&#24615;&#21407;&#29702;&#35745;&#31639;&#26041;&#27861;&#65288;&#22914;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#65289;&#30340;&#26367;&#20195;&#21697;&#12290;&#34429;&#28982;&#35768;&#22810;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#21270;&#23398;&#24615;&#36136;&#21644;&#21407;&#23376;&#21147;&#65292;&#20294;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#39044;&#27979;&#21704;&#23494;&#39039;&#30697;&#38453;&#30340;&#33021;&#21147;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#22522;&#26412;&#30340;&#29289;&#29702;&#37327;&#65292;&#23427;&#30830;&#23450;&#20102;&#29289;&#29702;&#31995;&#32479;&#21644;&#21270;&#23398;&#24615;&#36136;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;QH9&#65292;&#22522;&#20110;QM9&#25968;&#25454;&#38598;&#20026;2,399&#20010;&#20998;&#23376;&#21160;&#21147;&#23398;&#36712;&#36857;&#21644;130,831&#20010;&#31283;&#23450;&#20998;&#23376;&#20960;&#20309;&#24418;&#24577;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#21508;&#31181;&#20998;&#23376;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;QH9&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#37117;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#24212;&#23545;&#37325;&#23614;&#20998;&#24067;&#19988;&#20445;&#35777;&#26377;&#38480;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09548</link><description>&lt;p&gt;
&#22312;&#32447;&#37325;&#23614;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Heavy-tailed Change-point detection. (arXiv:2306.09548v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#24212;&#23545;&#37325;&#23614;&#20998;&#24067;&#19988;&#20445;&#35777;&#26377;&#38480;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979; (OCPD) &#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#21487;&#33021;&#26159;&#37325;&#23614;&#20998;&#24067;&#65292;&#19968;&#20010;&#25509;&#19968;&#20010;&#22320;&#21576;&#29616;&#65292;&#24182;&#19988;&#24517;&#39035;&#23613;&#26089;&#26816;&#27979;&#21040;&#24213;&#23618;&#22343;&#20540;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35009;&#21098;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477; (SGD) &#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#25105;&#20204;&#20165;&#20551;&#23450;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#31532;&#20108;&#38454;&#30697;&#26377;&#30028;&#65292;&#35813;&#31639;&#27861;&#20063;&#33021;&#27491;&#24120;&#24037;&#20316;&#12290;&#25105;&#20204;&#27966;&#29983;&#20102;&#22312;&#25152;&#26377;&#20855;&#26377;&#26377;&#30028;&#31532;&#20108;&#30697;&#30340;&#20998;&#24067;&#26063;&#20013;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26377;&#38480;&#26679;&#26412;&#20551;&#38451;&#24615;&#29575; (FPR) &#30340;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#20445;&#35777;&#26377;&#38480;&#26679;&#26412; FPR &#30340; OCPD &#31639;&#27861;&#65292;&#21363;&#20351;&#25968;&#25454;&#26159;&#39640;&#32500;&#30340;&#65292;&#24213;&#23618;&#20998;&#24067;&#26159;&#37325;&#23614;&#30340;&#12290;&#25105;&#20204;&#35770;&#25991;&#30340;&#25216;&#26415;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#35009;&#21098; SGD &#21487;&#20197;&#20272;&#35745;&#38543;&#26426;&#21521;&#37327;&#30340;&#22343;&#20540;&#24182;&#21516;&#26102;&#22312;&#25152;&#26377;&#32622;&#20449;&#24230;&#20540;&#19978;&#25552;&#20379;&#32622;&#20449;&#24230;&#30028;&#38480;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#31283;&#20581;&#30340;&#20272;&#35745;&#19982;&#24182;&#38598;&#36793;&#30028;&#35770;&#35777;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#19968;&#20010;&#26377;&#38480;&#30340;&#39034;&#24207;&#21464;&#28857;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study algorithms for online change-point detection (OCPD), where samples that are potentially heavy-tailed, are presented one at a time and a change in the underlying mean must be detected as early as possible. We present an algorithm based on clipped Stochastic Gradient Descent (SGD), that works even if we only assume that the second moment of the data generating process is bounded. We derive guarantees on worst-case, finite-sample false-positive rate (FPR) over the family of all distributions with bounded second moment. Thus, our method is the first OCPD algorithm that guarantees finite-sample FPR, even if the data is high dimensional and the underlying distributions are heavy-tailed. The technical contribution of our paper is to show that clipped-SGD can estimate the mean of a random vector and simultaneously provide confidence bounds at all confidence values. We combine this robust estimate with a union bound argument and construct a sequential change-point algorithm with finite
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38544;&#31169;&#21270;&#25968;&#25454;&#19978;&#35757;&#32451;GAN&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;Wasserstein&#36317;&#31163;&#21435;&#22122;&#21487;&#20197;&#32531;&#35299;&#27491;&#21017;&#21270;&#20559;&#24046;&#21644;&#38544;&#31169;&#21270;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#27169;&#22411;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09547</link><description>&lt;p&gt;
&#20174;&#38544;&#31169;&#21270;&#25968;&#25454;&#20013;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training generative models from privatized data. (arXiv:2306.09547v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09547
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38544;&#31169;&#21270;&#25968;&#25454;&#19978;&#35757;&#32451;GAN&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;Wasserstein&#36317;&#31163;&#21435;&#22122;&#21487;&#20197;&#32531;&#35299;&#27491;&#21017;&#21270;&#20559;&#24046;&#21644;&#38544;&#31169;&#21270;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#27169;&#22411;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24046;&#20998;&#38544;&#31169;&#21270;&#25968;&#25454;&#19978;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#24120;&#35265;&#30340;&#21152;&#24615;&#22122;&#22768;&#26426;&#21046;&#65288;&#22914;&#25289;&#26222;&#25289;&#26031;&#22122;&#22768;&#21644;&#39640;&#26031;&#22122;&#22768;&#65289;&#23545;&#25968;&#25454;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;Wasserstein&#36317;&#31163;&#26469;&#21435;&#22122;&#25968;&#25454;&#20998;&#24067;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21807;&#19968;&#22320;&#32531;&#35299;&#27491;&#21017;&#21270;&#20559;&#24046;&#21644;&#38544;&#31169;&#21270;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24182;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#32467;&#26524;&#21644;&#23454;&#39564;&#35777;&#25454;&#20197;&#25903;&#25345;&#20854;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local differential privacy (LDP) is a powerful method for privacy-preserving data collection. In this paper, we develop a framework for training Generative Adversarial Networks (GAN) on differentially privatized data. We show that entropic regularization of the Wasserstein distance -- a popular regularization method in the literature that has been often leveraged for its computational benefits -- can be used to denoise the data distribution when data is privatized by common additive noise mechanisms, such as Laplace and Gaussian. This combination uniquely enables the mitigation of both the regularization bias and the effects of privatization noise, thereby enhancing the overall efficacy of the model. We analyse the proposed method, provide sample complexity results and experimental evidence to support its efficacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09539</link><description>&lt;p&gt;
&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#38656;&#35201;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#24615;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24778;&#20154;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#26368;&#21021;&#26159;&#20026;&#36830;&#32493;&#20449;&#21495;&#35774;&#35745;&#30340;&#65292;&#20294;SSM&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;SSM&#20173;&#28982;&#33853;&#21518;&#20110;Transformers&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#23618;&#65292;&#23427;&#22312;&#20869;&#37096;&#32452;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#21270;&#30340;SSM&#23376;&#23618;&#21644;&#19968;&#20010;&#29992;&#20110;&#30701;&#26399;&#24207;&#21015;&#34920;&#31034;&#30340;&#22359;&#21464;&#25442;&#22120;&#23376;&#23618;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#12289;&#23436;&#20840;&#21487;&#24182;&#34892;&#30340;&#38598;&#25104;SSM&#21644;&#22359;&#27880;&#24847;&#21147;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#30340;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09526</link><description>&lt;p&gt;
&#27531;&#24046; Q &#23398;&#20064;&#65306;&#26080;&#38656;&#20215;&#20540;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#27169;&#20223;&#34892;&#20026;&#12290;&#24403;&#25163;&#24037;&#21046;&#20316;&#22870;&#21169;&#20989;&#25968;&#22256;&#38590;&#25110;&#30446;&#26631;&#26159;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#34892;&#20026;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#20294;&#26159;&#65292;&#23398;&#20064;&#30340;&#27169;&#20223;&#31574;&#30053;&#21482;&#33021;&#36981;&#24490;&#28436;&#31034;&#20013;&#30340;&#34892;&#20026;&#12290;&#22312;&#24212;&#29992;&#27169;&#20223;&#31574;&#30053;&#26102;&#65292;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#35201;&#27714;&#23450;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20173;&#24076;&#26395;&#23450;&#21046;&#30340;&#31574;&#30053;&#20445;&#25345;&#20854;&#27169;&#20223;&#24615;&#36136;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#31216;&#20026;&#31574;&#30053;&#23450;&#21046;&#12290;&#23427;&#23558;&#23398;&#20064;&#20219;&#21153;&#23450;&#20041;&#20026;&#35757;&#32451;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32487;&#25215;&#20808;&#21069;&#31574;&#30053;&#30340;&#29305;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#24378;&#21152;&#30340;&#19968;&#20123;&#38468;&#21152;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#30830;&#23450;&#20004;&#20010;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is a widely used framework for learning imitative behavior from demonstrations. It is especially appealing for solving complex real-world tasks where handcrafting reward function is difficult, or when the goal is to mimic human expert behavior. However, the learned imitative policy can only follow the behavior in the demonstration. When applying the imitative policy, we may need to customize the policy behavior to meet different requirements coming from diverse downstream tasks. Meanwhile, we still want the customized policy to maintain its imitative nature. To this end, we formulate a new problem setting called policy customization. It defines the learning task as training a policy that inherits the characteristics of the prior policy while satisfying some additional requirements imposed by a target downstream task. We propose a novel and principled approach to interpret and determine the trade-off between the two task objectives. Specifically, we formulate the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Caus-Modens&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#65292;&#30456;&#27604;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.09520</link><description>&lt;p&gt;
&#38024;&#23545;&#28508;&#22312;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#32467;&#26524;&#30340;&#26356;&#32039;&#23494;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding. (arXiv:2306.09520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Caus-Modens&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#65292;&#30456;&#27604;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30830;&#20999;&#20010;&#20307;&#27835;&#30103;&#32467;&#26524;&#30340;&#22240;&#26524;&#25512;&#26029;&#24456;&#23569;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25913;&#36827;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#32467;&#26524;&#21306;&#38388;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31867;&#26041;&#27861;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#65292;&#26377;&#26102;&#20250;&#32473;&#20986;&#26080;&#20449;&#24687;&#37327;&#30340;&#21306;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21478;&#31867;&#26041;&#27861;Caus-Modens&#65292;&#29992;&#20110;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#12290;&#21463;&#21040;&#36125;&#21494;&#26031;&#32479;&#35745;&#21644;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21551;&#21457;&#65292;Caus-Modens&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#20998;&#31163;&#22522;&#20934;&#27979;&#35797;&#30340;&#24517;&#35201;&#21306;&#38388;&#22823;&#23567;&#26469;&#23454;&#29616;&#36275;&#22815;&#30340;&#35206;&#30422;&#29575;&#12290;&#26368;&#21518;&#19968;&#20010;&#22522;&#20934;&#26159;&#20351;&#29992;&#26410;&#30693;&#20294;&#21487;&#25506;&#26126;&#30340;&#22522;&#30784;&#20107;&#23454;&#24320;&#23637;&#35266;&#23519;&#23454;&#39564;&#30340;GPT-4&#30340;&#26032;&#22411;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Instead, recent work has adapted conformal prediction to produce outcome intervals. Unfortunately this family of methods tends to be overly conservative, sometimes giving uninformative intervals. We introduce an alternative approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. Motivated from Bayesian statistics and ensembled uncertainty quantification, Caus-Modens gives tighter outcome intervals in practice, measured by the necessary interval size to achieve sufficient coverage on three separate benchmarks. The last benchmark is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.09519</link><description>&lt;p&gt;
&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion. (arXiv:2306.09519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#23569;&#37327;&#21442;&#32771;&#23454;&#20307;&#23545;&#39044;&#27979;&#20851;&#31995;&#30340;&#26410;&#35265;&#20107;&#23454;&#12290;&#29616;&#26377;&#26041;&#27861;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#36127;&#37319;&#26679;&#26469;&#26368;&#23567;&#21270;&#22522;&#20110;&#36793;&#30028;&#30340;&#25490;&#21517;&#25439;&#22833;&#65292;&#20294;&#36825;&#23481;&#26131;&#23548;&#33268;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#24212;&#35813;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20016;&#23500;&#30340;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#30452;&#35273;&#19978;&#65292;&#19982;&#27491;&#26679;&#26412;&#26356;&#30456;&#20284;&#30340;&#36127;&#26679;&#26412;&#23558;&#23545;&#27169;&#22411;&#36129;&#29486;&#26356;&#22823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#25417;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;RANA&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts of a relation with few-shot reference entity pairs. Current approaches randomly select one negative sample for each reference entity pair to minimize a margin-based ranking loss, which easily leads to a zero-loss problem if the negative sample is far away from the positive sample and then out of the margin. Moreover, the entity should have a different representation under a different context. To tackle these issues, we propose a novel Relation-Aware Network with Attention-Based Loss (RANA) framework. Specifically, to better utilize the plentiful negative samples and alleviate the zero-loss issue, we strategically select relevant negative samples and design an attention-based loss function to further differentiate the importance of each negative sample. The intuition is that negative samples more similar to positive samples will contribute more to the model. Further, we design a dynamic relation-aware entity en
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#21152;&#28909;&#25925;&#38556;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#21462;&#21644;&#26500;&#36896;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#26500;&#36896;&#21644;&#36873;&#21462;&#25552;&#39640;&#20998;&#31867;&#31934;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2306.09491</link><description>&lt;p&gt;
&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#21152;&#28909;&#25925;&#38556;&#26816;&#27979;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#21462;&#21644;&#26500;&#36896;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Feature Selection and Construction Method for Detection of Wind Turbine Generator Heating Faults. (arXiv:2306.09491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09491
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#21152;&#28909;&#25925;&#38556;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#21462;&#21644;&#26500;&#36896;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#26500;&#36896;&#21644;&#36873;&#21462;&#25552;&#39640;&#20998;&#31867;&#31934;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#39044;&#22788;&#29702;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#26377;&#25928;&#35774;&#35745;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#29305;&#24449;&#26500;&#36896;&#21644;&#36873;&#21462;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#21152;&#28909;&#25925;&#38556;&#30340;&#29305;&#24449;&#36873;&#21462;&#21644;&#26500;&#36896;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#30417;&#25511;&#25511;&#21046;&#21644;&#25968;&#25454;&#37319;&#38598;&#65288;SCADA&#65289;&#31995;&#32479;&#25910;&#38598;&#25968;&#25454;&#65292;&#24314;&#31435;&#21253;&#21547;&#39118;&#21147;&#29305;&#24449;&#12289;&#25805;&#20316;&#25968;&#25454;&#12289;&#28201;&#24230;&#27979;&#37327;&#21644;&#29366;&#24577;&#20449;&#24687;&#30340;&#21407;&#22987;&#29305;&#24449;&#65292;&#24182;&#22312;&#29305;&#24449;&#26500;&#36896;&#27493;&#39588;&#20013;&#21019;&#24314;&#26032;&#29305;&#24449;&#20197;&#33719;&#24471;&#26356;&#26377;&#21147;&#30340;&#25925;&#38556;&#25351;&#31034;&#20449;&#24687;&#12290;&#26500;&#36896;&#26032;&#29305;&#24449;&#21518;&#65292;&#37319;&#29992;&#28151;&#21512;&#29305;&#24449;&#36873;&#21462;&#25216;&#26415;&#22312;&#25972;&#20010;&#29305;&#24449;&#38598;&#20013;&#25214;&#20986;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#31934;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preprocessing of information is an essential step for the effective design of machine learning applications. Feature construction and selection are powerful techniques used for this aim. In this paper, a feature selection and construction approach is presented for the detection of wind turbine generator heating faults. Data were collected from Supervisory Control and Data Acquisition (SCADA) system of a wind turbine. The original features directly collected from the data collection system consist of wind characteristics, operational data, temperature measurements and status information. In addition to these original features, new features were created in the feature construction step to obtain information that can be more powerful indications of the faults. After the construction of new features, a hybrid feature selection technique was implemented to find out the most relevant features in the overall set to increase the classification accuracy and decrease the computational burden. Fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;O-RAN&#20998;&#31163;&#27169;&#22359;&#21644;&#20998;&#24067;&#24335;&#20195;&#29702;&#21512;&#20316;&#26469;&#20248;&#21270;&#32593;&#32476;&#20999;&#29255;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.09490</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#24335;Open RAN&#20999;&#29255;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Attention-based Open RAN Slice Management using Deep Reinforcement Learning. (arXiv:2306.09490v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;O-RAN&#20998;&#31163;&#27169;&#22359;&#21644;&#20998;&#24067;&#24335;&#20195;&#29702;&#21512;&#20316;&#26469;&#20248;&#21270;&#32593;&#32476;&#20999;&#29255;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Open Radio Access Network&#65288;O-RAN&#65289;&#21644;5G&#31561;&#26032;&#20852;&#32593;&#32476;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23545;&#20855;&#26377;&#19981;&#21516;&#38656;&#27714;&#30340;&#21508;&#31181;&#26381;&#21153;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#32593;&#32476;&#20999;&#29255;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#19981;&#21516;&#26381;&#21153;&#38656;&#27714;&#30340;&#28508;&#22312;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#31649;&#29702;&#32593;&#32476;&#20999;&#29255;&#24182;&#20445;&#25345;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#36827;&#34892;&#21160;&#24577;&#32593;&#32476;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#21487;&#20197;&#36890;&#36807;&#38450;&#27490;&#26381;&#21153;&#32423;&#21035;&#21327;&#35758;&#65288;SLA&#65289;&#36829;&#35268;&#65292;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#21487;&#38752;&#30340;&#20915;&#31574;&#21644;&#28385;&#36275;&#26032;&#20852;&#32593;&#32476;&#30340;&#38656;&#27714;&#38750;&#24120;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#32593;&#32476;QoS&#30340;&#23454;&#26102;&#30417;&#25511;&#21644;&#25511;&#21046;&#65292;&#20294;&#38656;&#35201;&#27867;&#21270;&#26469;&#25552;&#39640;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;ADRL&#65289;&#25216;&#26415;&#65292;&#21033;&#29992;O-RAN&#20998;&#31163;&#27169;&#22359;&#21644;&#20998;&#24067;&#24335;&#20195;&#29702;&#21512;&#20316;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#20999;&#29255;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
As emerging networks such as Open Radio Access Networks (O-RAN) and 5G continue to grow, the demand for various services with different requirements is increasing. Network slicing has emerged as a potential solution to address the different service requirements. However, managing network slices while maintaining quality of services (QoS) in dynamic environments is a challenging task. Utilizing machine learning (ML) approaches for optimal control of dynamic networks can enhance network performance by preventing Service Level Agreement (SLA) violations. This is critical for dependable decision-making and satisfying the needs of emerging networks. Although RL-based control methods are effective for real-time monitoring and controlling network QoS, generalization is necessary to improve decision-making reliability. This paper introduces an innovative attention-based deep RL (ADRL) technique that leverages the O-RAN disaggregated modules and distributed agent cooperation to achieve better p
&lt;/p&gt;</description></item><item><title>FedMultimodal &#26159;&#38754;&#21521;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35206;&#30422;&#20116;&#31181;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#21644;&#21313;&#20010;&#31038;&#21306;&#30340;FL&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20415;&#20419;&#36827;&#22810;&#27169;&#24577;FL&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.09486</link><description>&lt;p&gt;
FedMultimodal&#65306;&#38754;&#21521;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedMultimodal: A Benchmark For Multimodal Federated Learning. (arXiv:2306.09486v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09486
&lt;/p&gt;
&lt;p&gt;
FedMultimodal &#26159;&#38754;&#21521;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35206;&#30422;&#20116;&#31181;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#21644;&#21313;&#20010;&#31038;&#21306;&#30340;FL&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20415;&#20419;&#36827;&#22810;&#27169;&#24577;FL&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24212;&#23545;&#25968;&#25454;&#38544;&#31169;&#25361;&#25112;&#30340;&#26032;&#20852;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;FL&#31639;&#27861;&#20013;&#65292;&#23458;&#25143;&#31471;&#25552;&#20132;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#26381;&#21153;&#22120;&#23558;&#36825;&#20123;&#21442;&#25968;&#36827;&#34892;&#32858;&#21512;&#30452;&#33267;&#25910;&#25947;&#12290;&#23613;&#31649;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#65292;&#23545;FL&#36827;&#34892;&#20102;&#37325;&#22823;&#21162;&#21147;&#65292;&#20294;&#26159;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#27969;&#30340;FL&#24212;&#29992;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#24773;&#24863;&#35782;&#21035;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#22810;&#23186;&#20307;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#24212;&#29992;&#65292;&#32780;&#29992;&#25143;&#38544;&#31169;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#38024;&#23545;&#22810;&#27169;&#24577;&#24212;&#29992;&#25110;&#30456;&#20851;&#20219;&#21153;&#30340;&#29616;&#26377;FL&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;FL&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FedMultimodal&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;&#21313;&#20010;&#31038;&#21306;&#20013;&#30340;&#20116;&#20010;&#20195;&#34920;&#24615;&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;FL&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, Federated Learning (FL) has become an emerging machine learning technique to tackle data privacy challenges through collaborative training. In the Federated Learning algorithm, the clients submit a locally trained model, and the server aggregates these parameters until convergence. Despite significant efforts that have been made to FL in fields like computer vision, audio, and natural language processing, the FL applications utilizing multimodal data streams remain largely unexplored. It is known that multimodal learning has broad real-world applications in emotion recognition, healthcare, multimedia, and social media, while user privacy persists as a critical concern. Specifically, there are no existing FL benchmarks targeting multimodal applications or related tasks. In order to facilitate the research in multimodal FL, we introduce FedMultimodal, the first FL benchmark for multimodal learning covering five representative multimodal applications from ten comm
&lt;/p&gt;</description></item><item><title>R2-Diff &#26159;&#22522;&#20110;&#22270;&#20687;&#30456;&#20284;&#24230;&#26816;&#32034;&#36816;&#21160;&#21518;&#65292;&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#22122;&#20248;&#21270;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.09483</link><description>&lt;p&gt;
R2-Diff: &#22522;&#20110;&#22270;&#20687;&#30340;&#36816;&#21160;&#39044;&#27979;&#20013;&#19968;&#31181;&#20197;&#25193;&#25955;&#20026;&#22522;&#30784;&#30340;&#65292;&#20197;&#26816;&#32034;&#36816;&#21160;&#20026;&#20248;&#21270;&#30340;&#38477;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
R2-Diff: Denoising by diffusion as a refinement of retrieved motion for image-based motion prediction. (arXiv:2306.09483v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09483
&lt;/p&gt;
&lt;p&gt;
R2-Diff &#26159;&#22522;&#20110;&#22270;&#20687;&#30456;&#20284;&#24230;&#26816;&#32034;&#36816;&#21160;&#21518;&#65292;&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#22122;&#20248;&#21270;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#36816;&#21160;&#39044;&#27979;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#22312;&#21508;&#31181;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#25193;&#25955;&#27169;&#22411;&#19978;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#36880;&#28176;&#21435;&#22122;&#22270;&#20687;&#19978;&#19979;&#25991;&#20013;&#30340;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#65292;&#20197;&#38543;&#26426;&#26041;&#24335;&#39044;&#27979;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#36816;&#21160;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25913;&#21464;&#38543;&#26426;&#22122;&#22768;&#39044;&#27979;&#21508;&#31181;&#21508;&#26679;&#30340;&#36816;&#21160;&#65292;&#20294;&#26377;&#26102;&#23427;&#20204;&#19981;&#33021;&#22522;&#20110;&#22270;&#20687;&#19978;&#19979;&#25991;&#39044;&#27979;&#20986;&#24688;&#24403;&#30340;&#36816;&#21160;&#65292;&#22240;&#20026;&#38543;&#26426;&#22122;&#22768;&#26159;&#29420;&#31435;&#20110;&#22270;&#20687;&#19978;&#19979;&#25991;&#37319;&#26679;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; R2-Diff&#12290;&#22312; R2-Diff &#20013;&#65292;&#20174;&#22522;&#20110;&#22270;&#20687;&#30456;&#20284;&#24230;&#30340;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#20986;&#19968;&#31181;&#36816;&#21160;&#65292;&#23558;&#20854;&#36755;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#32780;&#19981;&#26159;&#38543;&#26426;&#22122;&#22768;&#20013;&#12290;&#28982;&#21518;&#65292;&#32463;&#36807;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#27969;&#31243;&#23545;&#26816;&#32034;&#21040;&#30340;&#36816;&#21160;&#36827;&#34892;&#25913;&#36827;&#12290;&#30001;&#20110;&#26816;&#32034;&#21040;&#30340;&#36816;&#21160;&#20960;&#20046;&#36866;&#21512;&#20110;&#19978;&#19979;&#25991;&#65292;&#25152;&#20197;&#23427;&#21464;&#24471;&#26356;&#23481;&#26131;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based motion prediction is one of the essential techniques for robot manipulation. Among the various prediction models, we focus on diffusion models because they have achieved state-of-the-art performance in various applications. In image-based motion prediction, diffusion models stochastically predict contextually appropriate motion by gradually denoising random Gaussian noise based on the image context. While diffusion models are able to predict various motions by changing the random noise, they sometimes fail to predict a contextually appropriate motion based on the image because the random noise is sampled independently of the image context. To solve this problem, we propose R2-Diff. In R2-Diff, a motion retrieved from a dataset based on image similarity is fed into a diffusion model instead of random noise. Then, the retrieved motion is refined through the denoising process of the diffusion model. Since the retrieved motion is almost appropriate to the context, it becomes ea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20313;&#25968;&#31995;&#32479;&#65288;RNS&#65289;&#26469;&#28040;&#38500;ADC&#26377;&#38480;&#31934;&#24230;&#24102;&#26469;&#30340;&#20449;&#24687;&#20002;&#22833;&#65292;&#20174;&#22810;&#20010;&#20302;&#31934;&#24230;&#36816;&#31639;&#32452;&#25104;&#39640;&#31934;&#24230;&#36816;&#31639;&#65292;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#21644;&#33391;&#22909;&#33021;&#28304;&#25928;&#29575;&#30340;&#27169;&#25311;DNN&#21152;&#36895;&#22120;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#20887;&#20313;&#30340;RNS&#26469;&#23454;&#29616;&#23481;&#38169;&#24615;&#12290;&#21516;&#26102;RNS&#30456;&#23545;&#20110;&#24120;&#35268;&#23450;&#28857;&#26041;&#27861;&#33021;&#22815;&#23558;&#25968;&#25454;&#36716;&#25442;&#22120;&#30340;&#33021;&#32791;&#38477;&#20302;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2306.09481</link><description>&lt;p&gt;
&#22522;&#20110;&#20313;&#25968;&#31995;&#32479;&#35774;&#35745;&#39640;&#31934;&#24230;&#27169;&#25311;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Residue Number System for Designing High-Precision Analog Deep Neural Network Accelerators. (arXiv:2306.09481v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20313;&#25968;&#31995;&#32479;&#65288;RNS&#65289;&#26469;&#28040;&#38500;ADC&#26377;&#38480;&#31934;&#24230;&#24102;&#26469;&#30340;&#20449;&#24687;&#20002;&#22833;&#65292;&#20174;&#22810;&#20010;&#20302;&#31934;&#24230;&#36816;&#31639;&#32452;&#25104;&#39640;&#31934;&#24230;&#36816;&#31639;&#65292;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#21644;&#33391;&#22909;&#33021;&#28304;&#25928;&#29575;&#30340;&#27169;&#25311;DNN&#21152;&#36895;&#22120;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#20887;&#20313;&#30340;RNS&#26469;&#23454;&#29616;&#23481;&#38169;&#24615;&#12290;&#21516;&#26102;RNS&#30456;&#23545;&#20110;&#24120;&#35268;&#23450;&#28857;&#26041;&#27861;&#33021;&#22815;&#23558;&#25968;&#25454;&#36716;&#25442;&#22120;&#30340;&#33021;&#32791;&#38477;&#20302;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;DNN&#21152;&#36895;&#22120;&#30340;&#39640;&#31934;&#24230;&#25968;&#25454;&#36716;&#25442;&#22120;&#25104;&#26412;&#39640;&#65292;&#21516;&#26102;&#35201;&#20445;&#25345;&#33391;&#22909;&#30340;&#33021;&#28304;&#25928;&#29575;&#21644;&#39640;&#20934;&#30830;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#21033;&#29992;&#20313;&#25968;&#31995;&#32479;&#65288;RNS&#65289;&#20174;&#22810;&#20010;&#20302;&#31934;&#24230;&#36816;&#31639;&#32452;&#25104;&#39640;&#31934;&#24230;&#36816;&#31639;&#65292;&#20811;&#26381;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#28040;&#38500;&#30001;ADC&#26377;&#38480;&#31934;&#24230;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;RNS&#21487;&#20351;&#29992;&#20165;&#20855;&#26377;6&#27604;&#29305;&#31934;&#24230;&#30340;&#25968;&#25454;&#36716;&#25442;&#22120;&#26469;&#23454;&#29616;&#26368;&#26032;DNN&#25512;&#29702;&#30340;99&#65285;FP32&#20934;&#30830;&#24230;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#20887;&#20313;&#30340;RNS&#26469;&#23454;&#29616;&#23481;&#38169;&#30340;&#27169;&#25311;&#21152;&#36895;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26174;&#31034;RNS&#30456;&#23545;&#20110;&#24120;&#35268;&#30340;&#23450;&#28857;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27169;&#25311;&#21152;&#36895;&#22120;&#20013;&#25968;&#25454;&#36716;&#25442;&#22120;&#30340;&#33021;&#32791;&#38477;&#20302;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving high accuracy, while maintaining good energy efficiency, in analog DNN accelerators is challenging as high-precision data converters are expensive. In this paper, we overcome this challenge by using the residue number system (RNS) to compose high-precision operations from multiple low-precision operations. This enables us to eliminate the information loss caused by the limited precision of the ADCs. Our study shows that RNS can achieve 99% FP32 accuracy for state-of-the-art DNN inference using data converters with only $6$-bit precision. We propose using redundant RNS to achieve a fault-tolerant analog accelerator. In addition, we show that RNS can reduce the energy consumption of the data converters within an analog accelerator by several orders of magnitude compared to a regular fixed-point approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#22806;&#25512;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#20102;&#38477;&#20302;&#22806;&#25512;&#35823;&#24046;&#30340;&#31574;&#30053;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#26086;&#27169;&#22411;&#20869;&#25554;&#35823;&#24046;&#20026;&#38646;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#26550;&#26500;&#22823;&#23567;&#25110;&#37319;&#26679;&#28857;&#25968;&#37327;&#23545;&#20110;&#22806;&#25512;&#34892;&#20026;&#27809;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.09478</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#36731;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#30340;&#22806;&#25512;&#22833;&#25928;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating Extrapolation Failures in Physics-Informed Neural Networks. (arXiv:2306.09478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#22806;&#25512;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#20102;&#38477;&#20302;&#22806;&#25512;&#35823;&#24046;&#30340;&#31574;&#30053;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#26086;&#27169;&#22411;&#20869;&#25554;&#35823;&#24046;&#20026;&#38646;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#26550;&#26500;&#22823;&#23567;&#25110;&#37319;&#26679;&#28857;&#25968;&#37327;&#23545;&#20110;&#22806;&#25512;&#34892;&#20026;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#33021;&#21147;&#65292;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26368;&#36817;&#22312;&#31185;&#23398;&#30028;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#36890;&#24120;&#34987;&#23616;&#38480;&#20110;&#20869;&#25554;&#22330;&#26223;&#65292;&#20854;&#20013;&#39044;&#27979;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#38598;&#25903;&#25345;&#20869;&#30340;&#36755;&#20837;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#38656;&#35201;&#22806;&#25512;&#65292;&#20294;&#26159;PINNs&#30340;&#22495;&#22806;&#34892;&#20026;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;PINNs&#30340;&#22806;&#25512;&#34892;&#20026;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#20960;&#20010;&#20808;&#21069;&#30340;&#20551;&#35774;&#30340;&#35777;&#25454;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#27169;&#22411;&#36873;&#25321;&#23545;&#22806;&#25512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#19968;&#26086;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#38646;&#20869;&#25554;&#35823;&#24046;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#26550;&#26500;&#22823;&#23567;&#25110;&#37319;&#26679;&#28857;&#25968;&#37327;&#23545;&#22806;&#25512;&#34892;&#20026;&#27809;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23545;&#20110;&#19968;&#20123;PDE&#65292;PINNs&#22312;&#22806;&#25512;&#20013;&#30340;&#34920;&#29616;&#20960;&#20046;&#19982;&#20869;&#25554;&#19968;&#26679;&#22909;&#12290;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#30340;Fourier&#21644;Laplace&#35889;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#35266;&#23519;&#21040;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#20943;&#23569;&#22806;&#25512;&#35823;&#24046;&#65292;&#20854;&#20013;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#28041;&#21450;&#20462;&#25913;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#20197;&#24378;&#35843;&#22806;&#25512;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Neural Networks (PINNs) have recently gained popularity in the scientific community due to their effective approximation of partial differential equations (PDEs) using deep neural networks. However, their application has been generally limited to interpolation scenarios, where predictions rely on inputs within the support of the training set. In real-world applications, extrapolation is often required, but the out of domain behavior of PINNs is understudied. In this paper, we provide a detailed investigation of PINNs' extrapolation behavior and provide evidence against several previously held assumptions: we study the effects of different model choices on extrapolation and find that once the model can achieve zero interpolation error, further increases in architecture size or in the number of points sampled have no effect on extrapolation behavior. We also show that for some PDEs, PINNs perform nearly as well in extrapolation as in interpolation. By analyzing the Fouri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#26694;&#26550;&#65288;FFB&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.09468</link><description>&lt;p&gt;
FFB:&#38754;&#21521;&#22788;&#29702;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods. (arXiv:2306.09468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#26694;&#26550;&#65288;FFB&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#65288;FFB&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#22522;&#20934;&#26694;&#26550;&#12290;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#30340;&#20844;&#24179;&#24615;&#23545;&#20110;&#31526;&#21512;&#36947;&#24503;&#21644;&#27861;&#24459;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#39564;&#35774;&#32622;&#30340;&#19981;&#19968;&#33268;&#65292;&#32570;&#20047;&#26131;&#20110;&#35775;&#38382;&#30340;&#31639;&#27861;&#23454;&#29616;&#20197;&#21450;&#24403;&#21069;&#20844;&#24179;&#24230;&#37327;&#24037;&#20855;&#30340;&#26377;&#38480;&#21487;&#25193;&#23637;&#24615;&#65292;&#23384;&#22312;&#27604;&#36739;&#21644;&#24320;&#21457;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#12289;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22788;&#29702;&#20013;&#30340;&#32452;&#20844;&#24179;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#30830;&#20445;&#19981;&#21516;&#27665;&#26063;/&#31181;&#26063;&#32676;&#20307;&#20844;&#24179;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#25552;&#20379;&#20102;&#20197;&#19979;&#20851;&#38190;&#36129;&#29486;&#65306;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Fair Fairness Benchmark (\textsf{FFB}), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is critical for ethical and legal compliance. However, there exist challenges in comparing and developing of fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an open-source, standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented open-source code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from $\mathbf{45,079}$ experiment
&lt;/p&gt;</description></item><item><title>AQuA&#26159;&#19968;&#27454;&#29992;&#20110;&#26631;&#31614;&#36136;&#37327;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#30446;&#30340;&#26159;&#35780;&#20272;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#21253;&#25324;&#25968;&#25454;&#27169;&#25311;&#12289;&#36136;&#37327;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26631;&#31614;&#22122;&#22768;&#28040;&#38500;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09467</link><description>&lt;p&gt;
AQuA&#65306;&#19968;&#31181;&#29992;&#20110;&#26631;&#31614;&#36136;&#37327;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
AQuA: A Benchmarking Tool for Label Quality Assessment. (arXiv:2306.09467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09467
&lt;/p&gt;
&lt;p&gt;
AQuA&#26159;&#19968;&#27454;&#29992;&#20110;&#26631;&#31614;&#36136;&#37327;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#30446;&#30340;&#26159;&#35780;&#20272;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#21253;&#25324;&#25968;&#25454;&#27169;&#25311;&#12289;&#36136;&#37327;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26631;&#31614;&#22122;&#22768;&#28040;&#38500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22909;&#22351;&#21462;&#20915;&#20110;&#29992;&#20110;&#35757;&#32451;&#23427;&#20204;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;ImageNet&#65292;&#23384;&#22312;&#26222;&#36941;&#30340;&#26631;&#27880;&#38169;&#35823;&#12290;&#35757;&#32451;&#38598;&#20013;&#30340;&#38169;&#35823;&#26631;&#31614;&#20250;&#21066;&#24369;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24433;&#21709;&#20351;&#29992;&#27979;&#35797;&#38598;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#21644;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#22312;&#23384;&#22312;&#26631;&#31614;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26159;&#30740;&#31350;&#30340;&#19968;&#20010;&#27963;&#36291;&#39046;&#22495;&#65292;&#20294;&#35813;&#39046;&#22495;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AQuA&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#29615;&#22659;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272;&#22312;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. ImageNet, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models' ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment AQuA to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to del
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#19968;&#31181;&#20165;&#20381;&#38752;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#28508;&#22312;&#26102;&#38388;&#19968;&#33268;&#24615;&#35757;&#32451;&#30340;&#31616;&#21333;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#32431;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#37117;&#36866;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25361;&#25112;&#24615;&#30340;&#39640;&#32500;&#22868;&#36305;&#20219;&#21153;&#65292;&#24182;&#19988;&#35757;&#32451;&#36895;&#24230;&#27604;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#35201;&#24555;4.1&#20493;&#12290;</title><link>http://arxiv.org/abs/2306.09466</link><description>&lt;p&gt;
&#31616;&#21270;&#21518;&#30340;&#26102;&#24577;&#19968;&#33268;&#24615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simplified Temporal Consistency Reinforcement Learning. (arXiv:2306.09466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#19968;&#31181;&#20165;&#20381;&#38752;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#28508;&#22312;&#26102;&#38388;&#19968;&#33268;&#24615;&#35757;&#32451;&#30340;&#31616;&#21333;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#32431;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#37117;&#36866;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25361;&#25112;&#24615;&#30340;&#39640;&#32500;&#22868;&#36305;&#20219;&#21153;&#65292;&#24182;&#19988;&#35757;&#32451;&#36895;&#24230;&#27604;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#35201;&#24555;4.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#30340;&#24207;&#36143;&#20915;&#31574;&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;&#20173;&#23384;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#25152;&#38656;&#35745;&#31639;&#37327;&#31561;&#38480;&#21046;&#12290;&#20026;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#38598;&#20013;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#23558;&#27169;&#22411;&#23398;&#20064;&#19982;&#35268;&#21010;&#20132;&#38169;&#36827;&#34892;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#26041;&#27861;&#36827;&#19968;&#27493;&#21033;&#29992;&#31574;&#30053;&#23398;&#20064;&#12289;&#20215;&#20540;&#20272;&#35745;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#36741;&#21161;&#30446;&#26631;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19968;&#31181;&#21482;&#20381;&#38752;&#36890;&#36807;&#28508;&#22312;&#26102;&#38388;&#19968;&#33268;&#24615;&#35757;&#32451;&#30340;&#28508;&#22312;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#31616;&#21333;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#23601;&#36275;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#36866;&#29992;&#20110;&#32431;&#35268;&#21010;&#36807;&#31243;&#20013;&#21033;&#29992;&#34987;&#34920;&#31034;&#26465;&#20214;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#36866;&#29992;&#20110;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#35813;&#34920;&#31034;&#20316;&#20026;&#31574;&#30053;&#21644;&#20215;&#20540;&#20989;&#25968;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#32447;&#35268;&#21010;&#22120;&#23398;&#20064;&#20102;&#20934;&#30830;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25361;&#25112;&#24615;&#30340;&#39640;&#32500;&#22868;&#36305;&#20219;&#21153;&#65292;&#32780;&#20854;&#35757;&#32451;&#36895;&#24230;&#27604;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#35201;&#24555;4.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is able to solve complex sequential decision-making tasks but is currently limited by sample efficiency and required computation. To improve sample efficiency, recent work focuses on model-based RL which interleaves model learning with planning. Recent methods further utilize policy learning, value estimation, and, self-supervised learning as auxiliary objectives. In this paper we show that, surprisingly, a simple representation learning approach relying only on a latent dynamics model trained by latent temporal consistency is sufficient for high-performance RL. This applies when using pure planning with a dynamics model conditioned on the representation, but, also when utilizing the representation as policy and value function features in model-free RL. In experiments, our approach learns an accurate dynamics model to solve challenging high-dimensional locomotion tasks with online planners while being 4.1 times faster to train compared to ensemble-based methods. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;Kriging&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;Kriging&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#24182;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09463</link><description>&lt;p&gt;
Kriging&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Kriging Convolutional Networks. (arXiv:2306.09463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09463
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;Kriging&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;Kriging&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#24182;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25554;&#20540;&#26159;&#19968;&#31867;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#24050;&#30693;&#20540;&#30340;&#20301;&#32622;&#29992;&#20110;&#20272;&#35745;&#20854;&#20182;&#20301;&#32622;&#30340;&#20540;&#65292;&#30528;&#37325;&#20110;&#21033;&#29992;&#31354;&#38388;&#23616;&#37096;&#24615;&#21644;&#36235;&#21183;&#12290;&#20256;&#32479;&#30340;Kriging&#26041;&#27861;&#20855;&#26377;&#24378;&#28872;&#30340;&#39640;&#26031;&#20551;&#35774;&#65292;&#22240;&#27492;&#24120;&#24120;&#26080;&#27861;&#25429;&#25417;&#25968;&#25454;&#20869;&#37096;&#30340;&#22797;&#26434;&#24615;&#12290;&#21463;&#26368;&#36817;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Kriging&#21367;&#31215;&#32593;&#32476;&#65288;KCN&#65289;&#65292;&#23427;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21644;Kriging&#30340;&#20248;&#28857;&#30456;&#32467;&#21512;&#12290;&#19982;&#26631;&#20934;GCN&#30456;&#27604;&#65292;KCN&#22312;&#20135;&#29983;&#39044;&#27979;&#26102;&#30452;&#25509;&#21033;&#29992;&#30456;&#37051;&#35266;&#27979;&#20540;&#12290;&#27492;&#22806;&#65292;KCN&#36824;&#23558;Kriging&#26041;&#27861;&#20316;&#20026;&#29305;&#23450;&#37197;&#32622;&#21253;&#21547;&#22312;&#20869;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#27880;&#24847;&#21147;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#20960;&#20010;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;GCN&#21644;Kriging&#12290;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;KCN&#22312;GitHub&#23384;&#20648;&#24211;&#19978;&#20844;&#24320;&#65306;https://github.com/tufts-ml/kcn-torch&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial interpolation is a class of estimation problems where locations with known values are used to estimate values at other locations, with an emphasis on harnessing spatial locality and trends. Traditional Kriging methods have strong Gaussian assumptions, and as a result, often fail to capture complexities within the data. Inspired by the recent progress of graph neural networks, we introduce Kriging Convolutional Networks (KCN), a method of combining the advantages of Graph Convolutional Networks (GCN) and Kriging. Compared to standard GCNs, KCNs make direct use of neighboring observations when generating predictions. KCNs also contain the Kriging method as a specific configuration. We further improve the model's performance by adding attention. Empirically, we show that this model outperforms GCNs and Kriging in several applications. The implementation of KCN using PyTorch is publicized at the GitHub repository: https://github.com/tufts-ml/kcn-torch.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#20154;&#31867;&#33298;&#36866;&#24230;&#30340;&#33258;&#21160;&#39550;&#39542;&#26550;&#26500;&#21644;&#19982;&#20854;&#30456;&#20851;&#30340;&#34917;&#20805;&#26694;&#26550;&#12290;&#35752;&#35770;&#20102;&#33258;&#21160;&#39550;&#39542;&#33298;&#36866;&#24615;&#12289;&#21709;&#24212;&#26102;&#38388;&#12289;&#36816;&#21160;&#26197;&#36710;&#21644;&#20248;&#21270;&#25216;&#26415;&#31561;&#26041;&#38754;&#30340;&#25216;&#26415;&#32454;&#33410;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.09462</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#39542;&#33298;&#36866;&#20248;&#21270;&#65306;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Motion Comfort Optimization for Autonomous Vehicles: Concepts, Methods, and Techniques. (arXiv:2306.09462v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#20154;&#31867;&#33298;&#36866;&#24230;&#30340;&#33258;&#21160;&#39550;&#39542;&#26550;&#26500;&#21644;&#19982;&#20854;&#30456;&#20851;&#30340;&#34917;&#20805;&#26694;&#26550;&#12290;&#35752;&#35770;&#20102;&#33258;&#21160;&#39550;&#39542;&#33298;&#36866;&#24615;&#12289;&#21709;&#24212;&#26102;&#38388;&#12289;&#36816;&#21160;&#26197;&#36710;&#21644;&#20248;&#21270;&#25216;&#26415;&#31561;&#26041;&#38754;&#30340;&#25216;&#26415;&#32454;&#33410;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20154;&#31867;&#33298;&#36866;&#24615;&#30340;&#35282;&#24230;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#30340;&#26550;&#26500;&#21644;&#30456;&#20851;&#34917;&#20805;&#26694;&#26550;&#12290;&#20171;&#32461;&#20102;&#34913;&#37327;&#33258;&#21160;&#39550;&#39542;&#29992;&#25143;&#33298;&#36866;&#24615;&#21644;&#24515;&#29702;&#20998;&#26512;&#30340;&#25216;&#26415;&#20803;&#32032;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19982;&#33258;&#21160;&#39550;&#39542;&#32467;&#26500;&#21644;&#21453;&#24212;&#26102;&#38388;&#30456;&#20851;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#33258;&#21160;&#39550;&#39542;&#33298;&#36866;&#31995;&#32479;&#12289;AV&#39550;&#39542;&#21592;&#21709;&#24212;&#26102;&#38388;&#12289;AV&#33298;&#36866;&#27700;&#24179;&#12289;&#36816;&#21160;&#26197;&#36710;&#20197;&#21450;&#30456;&#20851;&#20248;&#21270;&#25216;&#26415;&#30340;&#25216;&#26415;&#32454;&#33410;&#12290;&#20256;&#24863;&#22120;&#30340;&#21151;&#33021;&#21463;&#21040;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20256;&#24863;&#22120;&#20027;&#35201;&#24863;&#30693;&#36710;&#36742;&#21608;&#22260;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;&#8220;&#22825;&#27668;&#8221;&#65292;&#22240;&#27492;&#22312;&#19981;&#21516;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#65292;&#20108;&#25163;&#20256;&#24863;&#22120;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#23384;&#22312;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;&#33258;&#21160;&#39550;&#39542;&#30340;&#33298;&#36866;&#24615;&#21644;&#23433;&#20840;&#24615;&#20063;&#26159;&#24433;&#21709;&#33258;&#20027;&#21457;&#23637;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article outlines the architecture of autonomous driving and related complementary frameworks from the perspective of human comfort. The technical elements for measuring Autonomous Vehicle (AV) user comfort and psychoanalysis are listed here. At the same time, this article introduces the technology related to the structure of automatic driving and the reaction time of automatic driving. We also discuss the technical details related to the automatic driving comfort system, the response time of the AV driver, the comfort level of the AV, motion sickness, and related optimization technologies. The function of the sensor is affected by various factors. Since the sensor of automatic driving mainly senses the environment around a vehicle, including "the weather" which introduces the challenges and limitations of second-hand sensors in autonomous vehicles under different weather conditions. The comfort and safety of autonomous driving are also factors that affect the development of autono
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#32423;&#28151;&#28102;&#30697;&#38453;&#29992;&#20110;&#20998;&#31867;&#24615;&#33021;&#35780;&#20215;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#20998;&#32423;&#20998;&#31867;&#38382;&#39064;&#30340;&#29305;&#27530;&#24615;&#24182;&#21487;&#36866;&#29992;&#20110;&#22810;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#21450;&#20854;&#23545;&#20110;&#20998;&#32423;&#20998;&#31867;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09461</link><description>&lt;p&gt;
&#20998;&#32423;&#28151;&#28102;&#30697;&#38453;&#29992;&#20110;&#20998;&#31867;&#24615;&#33021;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Hierarchical confusion matrix for classification performance evaluation. (arXiv:2306.09461v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09461
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#32423;&#28151;&#28102;&#30697;&#38453;&#29992;&#20110;&#20998;&#31867;&#24615;&#33021;&#35780;&#20215;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#20998;&#32423;&#20998;&#31867;&#38382;&#39064;&#30340;&#29305;&#27530;&#24615;&#24182;&#21487;&#36866;&#29992;&#20110;&#22810;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#21450;&#20854;&#23545;&#20110;&#20998;&#32423;&#20998;&#31867;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#32423;&#28151;&#28102;&#30697;&#38453;&#30340;&#27010;&#24565;&#65292;&#20026;&#22522;&#20110;&#28151;&#28102;&#30697;&#38453;&#30340;&#65288;&#24179;&#38754;&#65289;&#20108;&#20998;&#31867;&#38382;&#39064;&#35780;&#20272;&#25514;&#26045;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20998;&#32423;&#20998;&#31867;&#38382;&#39064;&#30340;&#29305;&#27530;&#24615;&#12290;&#25105;&#20204;&#23558;&#35813;&#27010;&#24565;&#21457;&#23637;&#21040;&#19968;&#20010;&#24191;&#20041;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#20854;&#36866;&#29992;&#20110;&#25152;&#26377;&#31867;&#22411;&#30340;&#20998;&#32423;&#20998;&#31867;&#38382;&#39064;&#65292;&#21253;&#25324;&#26377;&#21521;&#26080;&#29615;&#22270;&#12289;&#22810;&#36335;&#24452;&#26631;&#35760;&#21644;&#38750;&#24378;&#21046;&#21494;&#33410;&#28857;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26032;&#28151;&#28102;&#30697;&#38453;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20998;&#32423;&#20998;&#31867;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#24050;&#24314;&#31435;&#30340;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#27010;&#36848;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#21450;&#20854;&#29992;&#20110;&#35780;&#20272;&#20998;&#32423;&#20998;&#31867;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;&#20998;&#32423;&#28151;&#28102;&#30697;&#38453;&#30340;&#23454;&#29616;&#21487;&#22312;GitHub&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose a novel concept of a hierarchical confusion matrix, opening the door for popular confusion matrix based (flat) evaluation measures from binary classification problems, while considering the peculiarities of hierarchical classification problems. We develop the concept to a generalized form and prove its applicability to all types of hierarchical classification problems including directed acyclic graphs, multi path labelling, and non mandatory leaf node prediction. Finally, we use measures based on the novel confusion matrix to evaluate models within a benchmark for three real world hierarchical classification applications and compare the results to established evaluation measures. The results outline the reasonability of this approach and its usefulness to evaluate hierarchical classification problems. The implementation of hierarchical confusion matrix is available on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09459</link><description>&lt;p&gt;
&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#38761;&#24615;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#65292;&#26368;&#36817;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#20195;&#29702;&#30340;&#21382;&#21490;&#21487;&#20197;&#34920;&#31034;&#20026;&#24207;&#21015;&#65292;&#24182;&#19988;&#25972;&#20010;&#20219;&#21153;&#21487;&#20197;&#32553;&#20943;&#20026;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Atari&#28216;&#25103;&#19978;&#26174;&#30528;&#20248;&#20110;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20180;&#32454;&#30740;&#31350;&#20102;&#35760;&#24518;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32489;&#25928;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24320;&#21457;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;CI&#26694;&#26550;&#21644;&#20449;&#24687;&#22788;&#29702;&#21327;&#35758;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#39044;&#38450;WSN&#20013;&#30340;&#32593;&#32476;&#25915;&#20987;&#21644;&#25968;&#25454;&#20002;&#22833;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21160;&#24577;&#21709;&#24212;&#23041;&#32961;&#22330;&#26223;&#12289;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#21644;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#26469;&#23454;&#29616;&#32593;&#32476;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19987;&#27880;&#20110;&#25968;&#25454;&#20256;&#36755;&#23436;&#25972;&#24615;&#30340;&#20449;&#24687;&#22788;&#29702;&#21327;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.09448</link><description>&lt;p&gt;
&#21033;&#29992;CI&#26694;&#26550;&#21644;&#20449;&#24687;&#22788;&#29702;&#21327;&#35758;&#65292;&#36816;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#39044;&#38450;WSN&#20013;&#30340;&#32593;&#32476;&#25915;&#20987;&#21644;&#25968;&#25454;&#20002;&#22833;
&lt;/p&gt;
&lt;p&gt;
Prevention of cyberattacks in WSN and packet drop by CI framework and information processing protocol using AI and Big Data. (arXiv:2306.09448v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;CI&#26694;&#26550;&#21644;&#20449;&#24687;&#22788;&#29702;&#21327;&#35758;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#39044;&#38450;WSN&#20013;&#30340;&#32593;&#32476;&#25915;&#20987;&#21644;&#25968;&#25454;&#20002;&#22833;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21160;&#24577;&#21709;&#24212;&#23041;&#32961;&#22330;&#26223;&#12289;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#21644;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#26469;&#23454;&#29616;&#32593;&#32476;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#19987;&#27880;&#20110;&#25968;&#25454;&#20256;&#36755;&#23436;&#25972;&#24615;&#30340;&#20449;&#24687;&#22788;&#29702;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#65288;WSN&#65289;&#30340;&#20381;&#36182;&#22312;&#20247;&#22810;&#39046;&#22495;&#20013;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#25915;&#20987;&#39044;&#38450;&#21644;&#25968;&#25454;&#20256;&#36755;&#23436;&#25972;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#25972;&#21512;&#35748;&#30693;&#26234;&#33021;&#65288;CI&#65289;&#26694;&#26550;&#12289;&#20449;&#24687;&#22788;&#29702;&#21327;&#35758;&#21644;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#12290;CI&#26550;&#26500;&#26088;&#22312;&#36890;&#36807;&#21160;&#24577;&#21709;&#24212;&#19981;&#26029;&#21457;&#23637;&#30340;&#23041;&#32961;&#22330;&#26223;&#26469;&#25552;&#39640;WSN&#23433;&#20840;&#24615;&#12290;&#23427;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#19981;&#26029;&#30417;&#35270;&#21644;&#20998;&#26512;&#32593;&#32476;&#34892;&#20026;&#65292;&#23454;&#26102;&#35782;&#21035;&#21644;&#32531;&#35299;&#20219;&#20309;&#20837;&#20405;&#20107;&#20214;&#12290;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#20063;&#21253;&#21547;&#22312;&#26694;&#26550;&#20013;&#20197;&#35782;&#21035;&#30001;&#25915;&#20987;&#25110;&#32593;&#32476;&#25317;&#22622;&#24341;&#36215;&#30340;&#25968;&#25454;&#21253;&#20002;&#22833;&#24773;&#20917;&#12290;&#20026;&#25903;&#25345;CI&#26550;&#26500;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#27880;&#20110;WSN&#20869;&#39640;&#25928;&#23433;&#20840;&#25968;&#25454;&#20256;&#36755;&#30340;&#20449;&#24687;&#22788;&#29702;&#21327;&#35758;&#12290;&#35813;&#21327;&#35758;&#20851;&#27880;&#21152;&#23494;&#21644;&#35748;&#35777;&#25216;&#26415;&#20197;&#20445;&#25252;&#25968;&#25454;&#23436;&#25972;&#24615;&#21644;&#38450;&#27490;&#26410;&#32463;&#35768;&#21487;&#30340;&#20154;&#21592;&#35775;&#38382;&#32593;&#32476;&#12290;&#26368;&#21518;&#65292;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#26469;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#32593;&#32476;&#25915;&#20987;&#22330;&#26223;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20854;&#22312;&#26816;&#27979;&#21644;&#39044;&#38450;WSN&#32593;&#32476;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the reliance on wireless sensor networks (WSNs) rises in numerous sectors, cyberattack prevention and data transmission integrity become essential problems. This study provides a complete framework to handle these difficulties by integrating a cognitive intelligence (CI) framework, an information processing protocol, and sophisticated artificial intelligence (AI) and big data analytics approaches. The CI architecture is intended to improve WSN security by dynamically reacting to an evolving threat scenario. It employs artificial intelligence algorithms to continuously monitor and analyze network behavior, identifying and mitigating any intrusions in real time. Anomaly detection algorithms are also included in the framework to identify packet drop instances caused by attacks or network congestion. To support the CI architecture, an information processing protocol focusing on efficient and secure data transfer within the WSN is introduced. To protect data integrity and prevent unwante
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#29992;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#37327;&#23376;&#21487;&#20998;&#24615;&#30340;&#36817;&#20284;&#35299;&#65292;&#36890;&#36807;&#26377;&#25928;&#31639;&#27861;&#36817;&#20284;&#26597;&#25214;&#26368;&#36817;&#30340;&#21487;&#20998;&#31163;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#23558;&#37327;&#23376;&#21487;&#20998;&#24615;&#35270;&#20026;&#20998;&#31867;&#38382;&#39064;&#65292;&#23545;&#20219;&#20309;&#20108;&#32500;&#28151;&#21512;&#29366;&#24577;&#37117;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.09444</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#22797;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#37327;&#23376;&#21487;&#20998;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Quantum Separability Through a Reproducible Machine Learning Lens. (arXiv:2306.09444v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#29992;&#20110;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#37327;&#23376;&#21487;&#20998;&#24615;&#30340;&#36817;&#20284;&#35299;&#65292;&#36890;&#36807;&#26377;&#25928;&#31639;&#27861;&#36817;&#20284;&#26597;&#25214;&#26368;&#36817;&#30340;&#21487;&#20998;&#31163;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#23558;&#37327;&#23376;&#21487;&#20998;&#24615;&#35270;&#20026;&#20998;&#31867;&#38382;&#39064;&#65292;&#23545;&#20219;&#20309;&#20108;&#32500;&#28151;&#21512;&#29366;&#24577;&#37117;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21487;&#20998;&#24615;&#38382;&#39064;&#26159;&#25351;&#22914;&#20309;&#21028;&#26029;&#19968;&#20010;&#20108;&#20998;&#20307;&#23494;&#24230;&#30697;&#38453;&#26159;&#32416;&#32544;&#30340;&#36824;&#26159;&#21487;&#20998;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#19979;&#25214;&#21040;&#27492;NP-&#38590;&#38382;&#39064;&#30340;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Frank-Wolfe&#30340;&#26377;&#25928;&#31639;&#27861;&#26469;&#36817;&#20284;&#26597;&#25214;&#26368;&#36817;&#30340;&#21487;&#20998;&#31163;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#25512;&#23548;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#23558;&#23494;&#24230;&#30697;&#38453;&#26631;&#35760;&#20026;&#21487;&#20998;&#31163;&#30340;&#25110;&#32416;&#32544;&#30340;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#37327;&#23376;&#21487;&#20998;&#24615;&#35270;&#20026;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#20108;&#32500;&#28151;&#21512;&#29366;&#24577;&#12290;&#23545;3-&#21644;7&#32500;&#24230;&#20013;&#30340;&#37327;&#23376;&#24577;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31243;&#24207;&#30340;&#25928;&#29575;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#25193;&#23637;&#21040;&#19978;&#21315;&#20010;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#20855;&#26377;&#39640;&#37327;&#23376;&#32416;&#32544;&#26816;&#27979;&#31934;&#24230;&#12290;&#36825;&#19968;&#36827;&#23637;&#26377;&#21161;&#20110;&#22522;&#20934;&#27979;&#35797;&#37327;&#23376;&#21487;&#20998;&#24615;&#65292;&#24182;&#25903;&#25345;&#26356;&#24378;&#22823;&#30340;&#32416;&#32544;&#26816;&#27979;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantum separability problem consists in deciding whether a bipartite density matrix is entangled or separable. In this work, we propose a machine learning pipeline for finding approximate solutions for this NP-hard problem in large-scale scenarios. We provide an efficient Frank-Wolfe-based algorithm to approximately seek the nearest separable density matrix and derive a systematic way for labeling density matrices as separable or entangled, allowing us to treat quantum separability as a classification problem. Our method is applicable to any two-qudit mixed states. Numerical experiments with quantum states of 3- and 7-dimensional qudits validate the efficiency of the proposed procedure, and demonstrate that it scales up to thousands of density matrices with a high quantum entanglement detection accuracy. This takes a step towards benchmarking quantum separability to support the development of more powerful entanglement detection techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.09442</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#23454;&#29616;&#32418;&#38431;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;&#19982;&#24314;&#31435;
&lt;/p&gt;
&lt;p&gt;
Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#27602;&#25110;&#19981;&#35802;&#23454;&#38472;&#36848;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#20102;&#24037;&#20855;&#20197;&#35843;&#26597;&#26377;&#23475;&#36755;&#20986;&#65292;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#34429;&#28982;&#36825;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#30340;&#26377;&#20215;&#20540;&#27493;&#39588;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#38024;&#23545;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21482;&#26377;&#39044;&#20808;&#30693;&#36947;&#26377;&#23475;&#34892;&#20026;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#36339;&#36807;&#20102;&#32418;&#38431;&#34892;&#21160;&#30340;&#26680;&#24515;&#25361;&#25112;&#65306;&#24320;&#21457;&#27169;&#22411;&#21487;&#33021;&#23637;&#31034;&#30340;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#23384;&#22312;&#26102;&#65292;&#32418;&#38431;&#34892;&#21160;&#30340;&#36793;&#38469;&#20215;&#20540;&#26377;&#38480;&#65292;&#22240;&#20026;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#36755;&#20986;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20551;&#35774;&#23545;&#25163;&#20174;&#39640;&#32423;&#12289;&#25277;&#35937;&#30340;&#19981;&#33391;&#34892;&#20026;&#35268;&#33539;&#20986;&#21457;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32418;&#38431;&#34892;&#21160;&#12290;&#32418;&#38431;&#24212;&#35813;&#22312;&#31934;&#21270;/&#25193;&#23637;&#27492;&#35268;&#33539;&#30340;&#21516;&#26102;&#23545;&#25239;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
&lt;/p&gt;</description></item><item><title>&#35813;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#40065;&#26834;&#12289;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#22320;&#26816;&#27979;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.09441</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Anomaly Detection via Nonlinear Manifold Learning. (arXiv:2306.09441v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#40065;&#26834;&#12289;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#22320;&#26816;&#27979;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26679;&#26412;&#25351;&#30340;&#26159;&#19982;&#20854;&#20182;&#25968;&#25454;&#26174;&#33879;&#20559;&#31163;&#30340;&#26679;&#26412;&#65292;&#20854;&#26816;&#27979;&#22312;&#26500;&#24314;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#35201;&#20040;&#20165;&#36866;&#29992;&#20110;&#65288;&#21322;&#65289;&#30417;&#30563;&#35774;&#32622;&#65292;&#35201;&#20040;&#22312;&#27809;&#26377;&#24102;&#26631;&#35760;&#24322;&#24120;&#26679;&#26412;&#30340;&#26080;&#30417;&#30563;&#24212;&#29992;&#20013;&#34920;&#29616;&#24456;&#24046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#30340;&#40065;&#26834;&#12289;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#26816;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomalies are samples that significantly deviate from the rest of the data and their detection plays a major role in building machine learning models that can be reliably used in applications such as data-driven design and novelty detection. The majority of existing anomaly detection methods either are exclusively developed for (semi) supervised settings, or provide poor performance in unsupervised applications where there is no training data with labeled anomalous samples. To bridge this research gap, we introduce a robust, efficient, and interpretable methodology based on nonlinear manifold learning to detect anomalies in unsupervised settings. The essence of our approach is to learn a low-dimensional and interpretable latent representation (aka manifold) for all the data points such that normal samples are automatically clustered together and hence can be easily and robustly identified. We learn this low-dimensional manifold by designing a learning algorithm that leverages either a 
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#26465;&#20214;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38590;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;FedC2SL&#65292;&#26080;&#38656;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#19988;&#23545;&#25968;&#25454;&#21464;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#25269;&#25239;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09433</link><description>&lt;p&gt;
&#23454;&#29992;&#32852;&#37030;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Federated Causal Structure Learning. (arXiv:2306.09433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09433
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#26465;&#20214;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38590;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;FedC2SL&#65292;&#26080;&#38656;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#19988;&#23545;&#25968;&#25454;&#21464;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#23545;&#20110;&#31185;&#23398;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#36807;&#31243;&#28041;&#21450;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#22240;&#26524;&#22270;&#20197;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#36890;&#24120;&#65292;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#25191;&#34892;&#27492;&#20219;&#21153;&#65292;&#20294;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#25968;&#25454;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#32852;&#37030;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#23545;&#25968;&#25454;&#20570;&#20986;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#24182;&#32570;&#20047;&#25910;&#25947;&#20445;&#35777;&#12290;FedC2SL&#26159;&#19968;&#31181;&#32852;&#37030;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;&#65292;&#23427;&#20351;&#29992;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#35813;&#26816;&#39564;&#22312;&#19981;&#25910;&#38598;&#23458;&#25143;&#31471;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26816;&#26597;&#20004;&#20010;&#21464;&#37327;&#22312;&#19968;&#32452;&#26465;&#20214;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;FedC2SL&#23545;&#25968;&#25454;&#20570;&#20986;&#20102;&#26356;&#24369;&#21644;&#26356;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#24182;&#26356;&#24378;&#22320;&#25269;&#24481;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#21464;&#24322;&#12290;FedPC&#21644;FedFCI&#26159;FedC2SL&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#29992;&#20110;&#22240;&#26524;&#20805;&#20998;&#24615;&#21644;&#22240;&#26524;&#19981;&#20805;&#20998;&#24615;&#24773;&#20917;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding causal relations is vital in scientific discovery. The process of causal structure learning involves identifying causal graphs from observational data to understand such relations. Usually, a central server performs this task, but sharing data with the server poses privacy risks. Federated learning can solve this problem, but existing solutions for federated causal structure learning make unrealistic assumptions about data and lack convergence guarantees. FedC2SL is a federated constraint-based causal structure learning scheme that learns causal graphs using a federated conditional independence test, which examines conditional independence between two variables under a condition set without collecting raw data from clients. FedC2SL requires weaker and more realistic assumptions about data and offers stronger resistance to data variability among clients. FedPC and FedFCI are the two variants of FedC2SL for causal structure learning in causal sufficiency and causal insuffic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20116;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;14&#20010;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#39640;&#27604;&#20363;&#23610;&#24230;&#65288;8x&#65289;&#30340;&#25511;&#21046;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#30450;&#30446;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#20854;&#20013;BlindSR&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#32780;APA&#22312;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#26159;&#22312;&#19981;&#38656;&#35201;&#23454;&#26102;&#35745;&#31639;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#33391;&#22909;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2306.09426</link><description>&lt;p&gt;
&#30450;&#30446;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65306;&#39640;&#35268;&#27169;&#22810;&#39046;&#22495;&#35270;&#35282;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques for blind image super-resolution: A high-scale multi-domain perspective evaluation. (arXiv:2306.09426v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20116;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;14&#20010;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#39640;&#27604;&#20363;&#23610;&#24230;&#65288;8x&#65289;&#30340;&#25511;&#21046;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#30450;&#30446;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#20854;&#20013;BlindSR&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#32780;APA&#22312;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#26159;&#22312;&#19981;&#38656;&#35201;&#23454;&#26102;&#35745;&#31639;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#33391;&#22909;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#24050;&#32463;&#36827;&#34892;&#20102;&#20960;&#39033;&#35299;&#20915;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#23454;&#39564;&#65292;&#36825;&#20123;&#26041;&#26696;&#21644;&#23454;&#39564;&#36890;&#24120;&#19981;&#20250;&#35774;&#35745;&#20986;&#39640;&#27604;&#20363;&#23610;&#24230;&#65292;&#20165;&#38480;&#20110;2&#20493;&#25110;4&#20493;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#19981;&#33021;&#30495;&#27491;&#28085;&#30422;&#22810;&#31181;&#39046;&#22495;&#30340;&#26174;&#33879;&#22810;&#26679;&#24615;&#20197;&#27491;&#30830;&#35780;&#20272;&#25216;&#26415;&#12290;&#36824;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30450;&#30446;&#30340;SR&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#26159;&#22522;&#20110;&#38477;&#32423;&#36807;&#31243;&#26159;&#26410;&#30693;&#30340;&#24819;&#27861;&#65292;&#22240;&#27492;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#25216;&#26415;&#22522;&#26412;&#19978;&#20381;&#36182;&#20110;&#20302;&#20998;&#36776;&#29575;&#65288;LR&#65289;&#22270;&#20687;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#27604;&#20363;&#23610;&#24230;&#65288;8x&#65289;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#20116;&#31181;&#26368;&#36817;&#19987;&#20026;&#30450;&#30446;&#22270;&#20687;SR&#37327;&#36523;&#23450;&#21046;&#30340;DL&#25216;&#26415;&#65306;&#33258;&#36866;&#24212;&#20266;&#22686;&#24378;&#65288;APA&#65289;&#12289;&#20855;&#26377;&#31354;&#38388;&#21464;&#21270;&#21155;&#21270;&#30340;&#30450;SR&#65288;BlindSR&#65289;&#12289;&#28145;&#24230;&#20132;&#26367;&#32593;&#32476;&#65288;DAN&#65289;&#12289;FastGAN&#21644;&#19987;&#23478;&#36229;&#20998;&#36776;&#29575;&#28151;&#21512;&#65288;MoESR&#65289;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;14&#20010;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#21307;&#23398;&#12289;&#33258;&#28982;&#12289;&#22478;&#24066;&#21644;&#33402;&#26415;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BlindSR&#22312;&#24615;&#33021;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24179;&#34913;&#65292;&#32780;APA&#22312;&#26368;&#23569;&#30340;&#22797;&#26434;&#24615;&#19979;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#26159;&#22312;&#19981;&#38656;&#35201;&#23454;&#26102;&#35745;&#31639;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#33391;&#22909;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite several solutions and experiments have been conducted recently addressing image super-resolution (SR), boosted by deep learning (DL) techniques, they do not usually design evaluations with high scaling factors, capping it at 2x or 4x. Moreover, the datasets are generally benchmarks which do not truly encompass significant diversity of domains to proper evaluate the techniques. It is also interesting to remark that blind SR is attractive for real-world scenarios since it is based on the idea that the degradation process is unknown, and hence techniques in this context rely basically on low-resolution (LR) images. In this article, we present a high-scale (8x) controlled experiment which evaluates five recent DL techniques tailored for blind image SR: Adaptive Pseudo Augmentation (APA), Blind Image SR with Spatially Variant Degradations (BlindSR), Deep Alternating Network (DAN), FastGAN, and Mixture of Experts Super-Resolution (MoESR). We consider 14 small datasets from five diffe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#32771;&#34385;&#32452;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#24178;&#39044;&#21487;&#33021;&#21152;&#21095;&#39044;&#27979;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#24212;&#32771;&#34385;&#31532;&#19977;&#20010;&#8220;&#20219;&#24847;&#24615;&#8221;&#36724;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#31639;&#27861;&#21487;&#25552;&#20379;&#26356;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.09425</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#21069;&#27839;&#20043;&#22806;&#23384;&#22312;&#20219;&#24847;&#24615;
&lt;/p&gt;
&lt;p&gt;
Arbitrariness Lies Beyond the Fairness-Accuracy Frontier. (arXiv:2306.09425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09425
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#32771;&#34385;&#32452;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#24178;&#39044;&#21487;&#33021;&#21152;&#21095;&#39044;&#27979;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#24212;&#32771;&#34385;&#31532;&#19977;&#20010;&#8220;&#20219;&#24847;&#24615;&#8221;&#36724;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#31639;&#27861;&#21487;&#25552;&#20379;&#26356;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#21487;&#33021;&#20250;&#26377;&#22810;&#20010;&#20855;&#26377;&#31867;&#20284;&#34920;&#29616;&#20294;&#20135;&#29983;&#19981;&#21516;&#36755;&#20986;&#30340;&#31454;&#20105;&#27169;&#22411;&#65292;&#36825;&#34987;&#31216;&#20026;&#39044;&#27979;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20165;&#38024;&#23545;&#32452;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#36827;&#34892;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#24178;&#39044;&#21487;&#33021;&#20250;&#21152;&#21095;&#39044;&#27979;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#24178;&#39044;&#21487;&#20197;&#25513;&#30422;&#39640;&#39044;&#27979;&#22810;&#26679;&#24615;&#32972;&#21518;&#30340;&#26377;&#21033;&#32452;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#37096;&#32626;&#27169;&#22411;&#20197;&#21327;&#21161;&#20010;&#20307;&#32423;&#24433;&#21709;&#24212;&#29992;&#31243;&#24207;&#26102;&#65292;&#24212;&#32771;&#34385;&#31532;&#19977;&#20010;&#8220;&#20219;&#24847;&#24615;&#8221;&#36724;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#20844;&#24179;&#24615;&#24178;&#39044;&#30340;&#38598;&#25104;&#31639;&#27861;&#65292;&#21487;&#35777;&#26126;&#25552;&#20379;&#26356;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning tasks may admit multiple competing models that achieve similar performance yet produce conflicting outputs for individual samples -- a phenomenon known as predictive multiplicity. We demonstrate that fairness interventions in machine learning optimized solely for group fairness and accuracy can exacerbate predictive multiplicity. Consequently, state-of-the-art fairness interventions can mask high predictive multiplicity behind favorable group fairness and accuracy metrics. We argue that a third axis of ``arbitrariness'' should be considered when deploying models to aid decision-making in applications of individual-level impact. To address this challenge, we propose an ensemble algorithm applicable to any fairness intervention that provably ensures more consistent predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#19987;&#20026;Landsat&#31995;&#21015;&#21355;&#26143;&#35774;&#35745;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25968;&#25454;&#38598;SSL4EO-L&#65292;&#36825;&#20063;&#26159;&#21382;&#21490;&#19978;&#26368;&#22823;&#30340;Landsat&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09424</link><description>&lt;p&gt;
Landsat&#24433;&#20687;&#25968;&#25454;&#38598;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;SSL4EO-L&#65306;&#28145;&#24230;&#33258;&#23398;&#20064;&#30340;&#39318;&#20010;&#21355;&#26143;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SSL4EO-L: Datasets and Foundation Models for Landsat Imagery. (arXiv:2306.09424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#19987;&#20026;Landsat&#31995;&#21015;&#21355;&#26143;&#35774;&#35745;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25968;&#25454;&#38598;SSL4EO-L&#65292;&#36825;&#20063;&#26159;&#21382;&#21490;&#19978;&#26368;&#22823;&#30340;Landsat&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Landsat&#35745;&#21010;&#26159;&#26377;&#21490;&#20197;&#26469;&#36816;&#34892;&#26102;&#38388;&#26368;&#38271;&#30340;&#22320;&#29699;&#35266;&#27979;&#35745;&#21010;&#65292;&#36890;&#36807;8&#20010;&#21355;&#26143;&#30340;&#25968;&#25454;&#33719;&#21462;&#65292;&#24050;&#32463;&#26377;50&#22810;&#24180;&#30340;&#21382;&#21490;&#12290;&#36825;&#20123;&#21355;&#26143;&#20256;&#24863;&#22120;&#25429;&#33719;&#30340;&#22810;&#20809;&#35889;&#22270;&#20687;&#23545;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21644;&#36965;&#24863;&#25216;&#26415;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#30001;&#20110;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#27969;&#34892;&#21644;&#32570;&#20047;&#22522;&#30784;&#27169;&#22411;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#20173;&#20351;&#29992;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;Landsat&#22270;&#20687;&#20998;&#26512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SSL4EO-L&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;Landsat&#31995;&#21015;&#21355;&#26143;&#65288;&#21253;&#25324;3&#20010;&#20256;&#24863;&#22120;&#21644;2&#20010;&#20135;&#21697;&#32423;&#21035;&#65289;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26159;&#21382;&#21490;&#19978;&#26368;&#22823;&#30340;Landsat&#25968;&#25454;&#38598;&#65288;500&#19975;&#22270;&#20687;&#22359;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26356;&#26032;&#20102;L7 Irish&#21644;L8 Biome&#20113;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20171;&#32461;&#20102;Landsats 4-5 TM&#21644;Landsat 7 ETM + SR&#30340;&#31532;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;SSL4EO-L&#39044;&#35757;&#32451;&#20102;&#31532;&#19968;&#20010;Landsat&#25968;&#25454;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Landsat program is the longest-running Earth observation program in history, with 50+ years of data acquisition by 8 satellites. The multispectral imagery captured by sensors onboard these satellites is critical for a wide range of scientific fields. Despite the increasing popularity of deep learning and remote sensing, the majority of researchers still use decision trees and random forests for Landsat image analysis due to the prevalence of small labeled datasets and lack of foundation models. In this paper, we introduce SSL4EO-L, the first ever dataset designed for Self-Supervised Learning for Earth Observation for the Landsat family of satellites (including 3 sensors and 2 product levels) and the largest Landsat dataset in history (5M image patches). Additionally, we modernize and re-release the L7 Irish and L8 Biome cloud detection datasets, and introduce the first ML benchmark datasets for Landsats 4-5 TM and Landsat 7 ETM+ SR. Finally, we pre-train the first foundation models
&lt;/p&gt;</description></item><item><title>Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09417</link><description>&lt;p&gt;
Diff-TTSG: &#21435;&#22122;&#27010;&#29575;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09417
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26391;&#35835;&#35821;&#38899;&#21512;&#25104;&#23454;&#29616;&#39640;&#33258;&#28982;&#24230;&#35780;&#20998;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#21512;&#25104;&#33258;&#28982;&#35328;&#35821;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#38754;&#23545;&#38754;&#30340;&#33258;&#21457;&#23545;&#35805;&#26082;&#26377;&#21475;&#22836;&#30340;&#65292;&#20063;&#26377;&#38750;&#35821;&#35328;&#30340;&#65288;&#20363;&#22914;&#65292;&#20849;&#21516;&#35328;&#35821;&#25163;&#21183;&#65289;&#12290;&#26368;&#36817;&#25165;&#24320;&#22987;&#30740;&#31350;&#32852;&#21512;&#21512;&#25104;&#36825;&#20004;&#31181;&#27169;&#24577;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#31995;&#32479;&#20013;&#30340;&#22909;&#22788;&#12290;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#20351;&#29992;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#26080;&#27861;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#36807;&#24230;&#24179;&#28369;&#30340;&#20266;&#24433;&#21644;&#27425;&#20248;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026; Diff-TTSG&#65292;&#20849;&#21516;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#32452;&#23567;&#24515;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#20027;&#35266;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#31995;&#32479;&#65292;&#24182;&#29992;&#23427;&#20204;&#26469;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#21512;&#25104;&#30340;&#26679;&#20363;&#32780;&#35328;&#65292;Diff-TTSG&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
With read-aloud speech synthesis achieving high naturalness scores, there is a growing research interest in synthesising spontaneous speech. However, human spontaneous face-to-face conversation has both spoken and non-verbal aspects (here, co-speech gestures). Only recently has research begun to explore the benefits of jointly synthesising these two modalities in a single system. The previous state of the art used non-probabilistic methods, which fail to capture the variability of human speech and motion, and risk producing oversmoothing artefacts and sub-optimal synthesis quality. We present the first diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to synthesise speech and gestures together. Our method can be trained on small datasets from scratch. Furthermore, we describe a set of careful uni- and multi-modal subjective tests for evaluating integrated speech and gesture synthesis systems, and use them to validate our proposed approach. For synthesised examp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38480;&#21046;&#25968;&#25454;&#19979;&#31038;&#20132;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#26465;&#20214;&#65292;&#25512;&#23548;&#20986;&#20102;&#20004;&#31181;&#20219;&#21153;&#30340;&#27010;&#29575;&#35823;&#24046;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.09397</link><description>&lt;p&gt;
&#38480;&#21046;&#25968;&#25454;&#19979;&#31038;&#20132;&#26426;&#22120;&#23398;&#20064;&#30340;&#38750;&#28176;&#36827;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Performance of Social Machine Learning Under Limited Data. (arXiv:2306.09397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38480;&#21046;&#25968;&#25454;&#19979;&#31038;&#20132;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#26465;&#20214;&#65292;&#25512;&#23548;&#20986;&#20102;&#20004;&#31181;&#20219;&#21153;&#30340;&#27010;&#29575;&#35823;&#24046;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#19982;&#38169;&#35823;&#27010;&#29575;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#28041;&#21450;&#29420;&#31435;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#38543;&#21518;&#22312;&#22270;&#19978;&#36827;&#34892;&#21327;&#20316;&#20915;&#31574;&#38454;&#27573;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#20998;&#31867;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20998;&#31867;&#20219;&#21153;&#65292;&#20998;&#21035;&#20026;&#32479;&#35745;&#20998;&#31867;&#21644;&#21333;&#26679;&#26412;&#20998;&#31867;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#39044;&#27979;&#38454;&#27573;&#25968;&#25454;&#35266;&#27979;&#21463;&#38480;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#35268;&#21017;&#65292;&#24182;&#30456;&#24212;&#20998;&#26512;&#20102;&#38169;&#35823;&#27010;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#28041;&#21450;&#35757;&#32451;&#20998;&#31867;&#22120;&#29983;&#25104;&#30340;&#36793;&#32536;&#20998;&#24067;&#12290;&#22522;&#20110;&#27492;&#26465;&#20214;&#65292;&#25105;&#20204;&#20026;&#20004;&#31181;&#20219;&#21153;&#25512;&#23548;&#20986;&#20102;&#27010;&#29575;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#29992;&#20110;&#32452;&#21512;&#20998;&#24067;&#24335;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the probability of error associated with the social machine learning framework, which involves an independent training phase followed by a cooperative decision-making phase over a graph. This framework addresses the problem of classifying a stream of unlabeled data in a distributed manner. We consider two kinds of classification tasks with limited observations in the prediction phase, namely, the statistical classification task and the single-sample classification task. For each task, we describe the distributed learning rule and analyze the probability of error accordingly. To do so, we first introduce a stronger consistent training condition that involves the margin distributions generated by the trained classifiers. Based on this condition, we derive an upper bound on the probability of error for both tasks, which depends on the statistical properties of the data and the combination policy used to combine the distributed classifiers. For the statistical classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;sketching&#31639;&#27861;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#22914;&#20309;&#26681;&#25454;&#24213;&#23618;&#38382;&#39064;&#30340;&#38590;&#24230;&#35774;&#32622;sketch size&#12290;</title><link>http://arxiv.org/abs/2306.09396</link><description>&lt;p&gt;
&#31169;&#26377;&#32852;&#37030;&#39057;&#29575;&#20272;&#35745;&#65306;&#36866;&#24212;&#23454;&#20363;&#38590;&#24230;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Private Federated Frequency Estimation: Adapting to the Hardness of the Instance. (arXiv:2306.09396v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;sketching&#31639;&#27861;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#22914;&#20309;&#26681;&#25454;&#24213;&#23618;&#38382;&#39064;&#30340;&#38590;&#24230;&#35774;&#32622;sketch size&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#39057;&#29575;&#20272;&#35745;&#65288;FFE&#65289;&#20013;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#36981;&#23432;Secure Summation(SecSum)&#38544;&#31169;&#32422;&#26463;&#30340;&#26381;&#21153;&#22120;&#21327;&#35758;&#36890;&#20449;&#65292;&#20849;&#21516;&#20272;&#35745;&#20854;&#25968;&#25454;&#30340;&#39057;&#29575;&#12290;&#23545;&#20110;&#21333;&#36718;FFE&#65292;&#30740;&#31350;&#32773;&#24050;&#30693;&#20351;&#29992;count sketching&#31639;&#27861;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#22522;&#26412;&#30340;&#31934;&#24230;-&#36890;&#20449;&#22797;&#26434;&#24230;&#24179;&#34913;&#12290;&#20294;&#26159;&#22312;&#26356;&#23454;&#38469;&#30340;&#22810;&#36718;FEE&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;count sketching&#31639;&#27861;&#26159;&#20005;&#26684;&#27425;&#20248;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;sketching&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#26356;&#31934;&#30830;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#20026;&#20102;&#36866;&#24212;&#24213;&#23618;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#20174;&#19994;&#32773;&#24212;&#35813;&#22914;&#20309;&#35774;&#32622;sketch size&#65311; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated frequency estimation (FFE), multiple clients work together to estimate the frequencies of their collective data by communicating with a server that respects the privacy constraints of Secure Summation (SecSum), a cryptographic multi-party computation protocol that ensures that the server can only access the sum of client-held vectors. For single-round FFE, it is known that count sketching is nearly information-theoretically optimal for achieving the fundamental accuracy-communication trade-offs [Chen et al., 2022]. However, we show that under the more practical multi-round FEE setting, simple adaptations of count sketching are strictly sub-optimal, and we propose a novel hybrid sketching algorithm that is provably more accurate. We also address the following fundamental question: how should a practitioner set the sketch size in a way that adapts to the hardness of the underlying problem? We propose a two-phase approach that allows for the use of a smaller sketch size for s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#20013;&#30452;&#25509;&#39044;&#27979;&#32454;&#32990;&#32676;&#20307;&#30340;&#22810;&#32452;&#23398;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#21050;&#28608;&#26465;&#20214;&#19979;&#23454;&#29616;&#26174;&#33879;&#25104;&#26524;&#65292;&#20026;&#32454;&#32990;&#32452;&#23398;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09391</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#22810;&#32452;&#23398;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-omics Prediction from High-content Cellular Imaging with Deep Learning. (arXiv:2306.09391v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#20013;&#30452;&#25509;&#39044;&#27979;&#32454;&#32990;&#32676;&#20307;&#30340;&#22810;&#32452;&#23398;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#21050;&#28608;&#26465;&#20214;&#19979;&#23454;&#29616;&#26174;&#33879;&#25104;&#26524;&#65292;&#20026;&#32454;&#32990;&#32452;&#23398;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#12289;&#36716;&#24405;&#32452;&#23398;&#21644;&#34507;&#30333;&#36136;&#32452;&#23398;&#25968;&#25454;&#20026;&#24433;&#21709;&#32454;&#32990;&#29366;&#24577;&#21644;&#21151;&#33021;&#30340;&#29983;&#29289;&#20998;&#23376;&#23618;&#25552;&#20379;&#20102;&#20016;&#23500;&#21644;&#20114;&#34917;&#30340;&#35270;&#35282;&#12290;&#20294;&#26159;&#65292;&#23578;&#26410;&#31995;&#32479;&#22320;&#25506;&#35752;&#22810;&#32452;&#23398;&#27979;&#37327;&#20540;&#24433;&#21709;&#32454;&#32990;&#24418;&#24577;&#30340;&#29983;&#29289;&#23398;&#20915;&#23450;&#22240;&#32032;&#65292;&#22240;&#27492;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#32454;&#32990;&#25104;&#20687;&#26159;&#21542;&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#22810;&#32452;&#23398;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;Image2Omics&#8212;&#8212;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#30452;&#25509;&#20174;&#29992;&#22810;&#37325;&#33639;&#20809;&#26579;&#26009;&#26579;&#33394;&#30340;&#39640;&#20869;&#23481;&#22270;&#20687;&#20013;&#39044;&#27979;&#32454;&#32990;&#32676;&#20307;&#30340;&#22810;&#32452;&#23398;&#26159;&#21542;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#21050;&#28608;&#26465;&#20214;&#19979;&#30340;&#20154;&#31867;&#35825;&#23548;&#22810;&#33021;&#24178;&#32454;&#32990;&#65288;hiPSC&#65289;&#34893;&#29983;&#30340;&#22522;&#22240;&#32534;&#36753;&#24040;&#22124;&#32454;&#32990;&#20013;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;Image2Omics&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-content cellular imaging, transcriptomics, and proteomics data provide rich and complementary views on the molecular layers of biology that influence cellular states and function. However, the biological determinants through which changes in multi-omics measurements influence cellular morphology have not yet been systematically explored, and the degree to which cell imaging could potentially enable the prediction of multi-omics directly from cell imaging data is therefore currently unclear. Here, we address the question of whether it is possible to predict bulk multi-omics measurements directly from cell images using Image2Omics -- a deep learning approach that predicts multi-omics in a cell population directly from high-content images stained with multiplexed fluorescent dyes. We perform an experimental evaluation in gene-edited macrophages derived from human induced pluripotent stem cell (hiPSC) under multiple stimulation conditions and demonstrate that Image2Omics achieves sign
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#35780;&#20272;&#20102;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340; ChatGPT &#27169;&#22411;&#22312;&#33258;&#26432;&#20542;&#21521;&#35780;&#20272;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#27604;&#36739;&#20102;&#20854;&#32467;&#26524;&#19982;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#21709;&#24212;&#29983;&#25104;&#30340;&#26368;&#20339;&#28201;&#24230;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20197;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#24494;&#35843;&#30340; Transformer &#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#24515;&#29702;&#20581;&#24247;&#19987;&#23478;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#27169;&#22411;&#35780;&#20272;&#21644;&#21442;&#25968;&#20248;&#21270;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.09390</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340; ChatGPT &#33258;&#26432;&#39118;&#38505;&#35780;&#20272;&#65306;&#27169;&#22411;&#24615;&#33021;&#12289;&#28508;&#21147;&#21644;&#38480;&#21046;&#30340;&#37327;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations. (arXiv:2306.09390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#35780;&#20272;&#20102;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340; ChatGPT &#27169;&#22411;&#22312;&#33258;&#26432;&#20542;&#21521;&#35780;&#20272;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#27604;&#36739;&#20102;&#20854;&#32467;&#26524;&#19982;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#21709;&#24212;&#29983;&#25104;&#30340;&#26368;&#20339;&#28201;&#24230;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20197;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#24494;&#35843;&#30340; Transformer &#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#24515;&#29702;&#20581;&#24247;&#19987;&#23478;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#27169;&#22411;&#35780;&#20272;&#21644;&#21442;&#25968;&#20248;&#21270;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#37327;&#21270;&#35780;&#20272;&#20132;&#20114;&#24335; ChatGPT &#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#36827;&#34892;&#33258;&#26432;&#20542;&#21521;&#35780;&#20272;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#39532;&#37324;&#20848;&#22823;&#23398; Reddit &#33258;&#26432;&#20542;&#21521;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23454;&#39564;&#26469;&#23545; ChatGPT &#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#25216;&#26415;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#32467;&#26524;&#19982;&#20004;&#20010;&#22522;&#20110; transformer &#30340;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#19981;&#21516;&#28201;&#24230;&#21442;&#25968;&#23545; ChatGPT &#21709;&#24212;&#29983;&#25104;&#30340;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#22522;&#20110; ChatGPT &#19981;&#30830;&#23450;&#24615;&#29575;&#30340;&#26368;&#20339;&#28201;&#24230;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982; ChatGPT &#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20197;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#24494;&#35843;&#30340; Transformer &#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#38416;&#26126;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972; ChatGPT &#30340;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#20854;&#22312;&#36825;&#19968;&#20851;&#38190;&#20219;&#21153;&#20013;&#24110;&#21161;&#24515;&#29702;&#20581;&#24247;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel framework for quantitatively evaluating the interactive ChatGPT model in the context of suicidality assessment from social media posts, utilizing the University of Maryland Reddit suicidality dataset. We conduct a technical evaluation of ChatGPT's performance on this task using Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned transformer-based models. Additionally, we investigate the impact of different temperature parameters on ChatGPT's response generation and discuss the optimal temperature based on the inconclusiveness rate of ChatGPT. Our results indicate that while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance. Moreover, our analysis sheds light on how adjusting the ChatGPT's hyperparameters can improve its ability to assist mental health professionals in this critical task.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;ST-PINN&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#29616;&#26377;PINN&#30340;&#31934;&#24230;&#21644;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292; ST-PINN &#21487;&#20197;&#23398;&#20064;&#26356;&#22810;&#30340;&#29289;&#29702;&#30693;&#35782;&#24182;&#21463;&#30410;&#20110;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09389</link><description>&lt;p&gt;
ST-PINN: &#19968;&#31181;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#33258;&#35757;&#32451;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ST-PINN: A Self-Training Physics-Informed Neural Network for Partial Differential Equations. (arXiv:2306.09389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09389
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;ST-PINN&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#29616;&#26377;PINN&#30340;&#31934;&#24230;&#21644;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292; ST-PINN &#21487;&#20197;&#23398;&#20064;&#26356;&#22810;&#30340;&#29289;&#29702;&#30693;&#35782;&#24182;&#21463;&#30410;&#20110;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#29289;&#29702;&#21644;&#24037;&#31243;&#35745;&#31639;&#26680;&#24515;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#26080;&#32593;&#26684;&#26041;&#27861;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#24555;&#36895;&#27714;&#35299;PDE&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;ST-PINN&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;PINN&#30340;&#20302;&#31934;&#24230;&#21644;&#25910;&#25947;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ST-PINN&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#20102;&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#33258;&#23398;&#20064;&#31639;&#27861;&#12290;&#23427;&#23558;&#25511;&#21046;&#26041;&#31243;&#20316;&#20026;&#20266;&#26631;&#35760;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#20174;&#26679;&#26412;&#28857;&#20013;&#36873;&#25321;&#26368;&#39640;&#32622;&#20449;&#24230;&#30340;&#31034;&#20363;&#26469;&#38468;&#21152;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#26159;&#39318;&#20808;&#23558;&#33258;&#25105;&#35757;&#32451;&#26426;&#21046;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#20154;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#22330;&#26223;&#20013;&#23545;&#20116;&#20010;PDE&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#26356;&#22810;&#29289;&#29702;&#20449;&#24687;&#24182;&#21463;&#30410;&#20110;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) are an essential computational kernel in physics and engineering. With the advance of deep learning, physics-informed neural networks (PINNs), as a mesh-free method, have shown great potential for fast PDE solving in various applications. To address the issue of low accuracy and convergence problems of existing PINNs, we propose a self-training physics-informed neural network, ST-PINN. Specifically, ST-PINN introduces a pseudo label based self-learning algorithm during training. It employs governing equation as the pseudo-labeled evaluation index and selects the highest confidence examples from the sample points to attach the pseudo labels. To our best knowledge, we are the first to incorporate a self-training mechanism into physics-informed learning. We conduct experiments on five PDE problems in different fields and scenarios. The results demonstrate that the proposed method allows the network to learn more physical information and benefit conver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#23618;&#26102;&#31354;&#32593;&#32476;&#65288;AHSTN&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#21644;&#24314;&#27169;&#22810;&#23610;&#24230;&#31354;&#38388;&#30456;&#20851;&#24615;&#20419;&#36827;&#20132;&#36890;&#39044;&#27979;&#65292;AHSTN&#22312;&#33410;&#28857;&#32423;&#21035;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#22359;&#65292;&#26469;&#33258;&#36866;&#24212;&#22320;&#22788;&#29702;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#20998;&#23618;&#27880;&#24847;&#26426;&#21046;&#26469;&#36873;&#25321;&#24615;&#22320;&#32858;&#21512;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#65292;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09386</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20998;&#23618;&#26102;&#31354;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hierarchical SpatioTemporal Network for Traffic Forecasting. (arXiv:2306.09386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#23618;&#26102;&#31354;&#32593;&#32476;&#65288;AHSTN&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#21644;&#24314;&#27169;&#22810;&#23610;&#24230;&#31354;&#38388;&#30456;&#20851;&#24615;&#20419;&#36827;&#20132;&#36890;&#39044;&#27979;&#65292;AHSTN&#22312;&#33410;&#28857;&#32423;&#21035;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#22359;&#65292;&#26469;&#33258;&#36866;&#24212;&#22320;&#22788;&#29702;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#20998;&#23618;&#27880;&#24847;&#26426;&#21046;&#26469;&#36873;&#25321;&#24615;&#22320;&#32858;&#21512;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#65292;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#23545;&#20110;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#35813;&#31995;&#32479;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#35299;&#20915;&#22478;&#24066;&#20132;&#36890;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#20132;&#36890;&#39044;&#27979;&#30740;&#31350;&#30528;&#37325;&#20110;&#24314;&#27169;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388; - &#26102;&#38388;&#21160;&#24577;&#65292;&#20854;&#20013;&#65292;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26159;&#21033;&#29992;&#36335;&#32593;&#26684;&#20013;&#23884;&#20837;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#30340;&#26680;&#24515;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;GCN&#30340;&#26041;&#27861;&#20165;&#22312;&#33410;&#28857;&#32423;&#21035;&#65288;&#20363;&#22914;&#36947;&#36335;&#21644;&#20132;&#21449;&#21475;&#65289;&#19978;&#36816;&#34892;&#65292;&#32780;&#24573;&#30053;&#20102;&#25972;&#20010;&#22478;&#24066;&#30340;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#12290;&#35832;&#22914;&#20132;&#21449;&#21475;&#21644;&#36947;&#36335;&#27573;&#20043;&#31867;&#30340;&#33410;&#28857;&#21487;&#20197;&#24418;&#25104;&#31751;&#65288;&#20363;&#22914;&#21306;&#22495;&#65289;&#65292;&#36825;&#20123;&#33410;&#28857;&#22312;&#26356;&#39640;&#30340;&#23618;&#27425;&#19978;&#36824;&#21487;&#20197;&#30456;&#20114;&#20316;&#29992;&#24182;&#20849;&#20139;&#30456;&#20284;&#20043;&#22788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#23618;&#31354;&#38388;&#26102;&#38388;&#32593;&#32476;&#65288;AHSTN&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#21644;&#24314;&#27169;&#22810;&#23610;&#24230;&#31354;&#38388;&#30456;&#20851;&#24615;&#20419;&#36827;&#20132;&#36890;&#39044;&#27979;&#12290;&#38500;&#20102;&#33410;&#28857;&#32423;&#21035;&#30340;&#26102;&#31354;&#22359;&#65292;AHSTN&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#26102;&#31354;&#22359;&#65292;&#20998;&#21035;&#25429;&#25417;&#26412;&#22320;&#12289;&#21306;&#22495;&#21644;&#20840;&#29699;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#19981;&#21516;&#23618;&#27425;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#36873;&#25321;&#24615;&#22320;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25552;&#20986;&#30340;AHSTN&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate traffic forecasting is vital to intelligent transportation systems, which are widely adopted to solve urban traffic issues. Existing traffic forecasting studies focus on modeling spatial-temporal dynamics in traffic data, among which the graph convolution network (GCN) is at the center for exploiting the spatial dependency embedded in the road network graphs. However, these GCN-based methods operate intrinsically on the node level (e.g., road and intersection) only whereas overlooking the spatial hierarchy of the whole city. Nodes such as intersections and road segments can form clusters (e.g., regions), which could also have interactions with each other and share similarities at a higher level. In this work, we propose an Adaptive Hierarchical SpatioTemporal Network (AHSTN) to promote traffic forecasting by exploiting the spatial hierarchy and modeling multi-scale spatial correlations. Apart from the node-level spatiotemporal blocks, AHSTN introduces the adaptive spatiotempor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;AI&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#22810;&#31181;&#20449;&#24687;&#26469;&#31934;&#30830;&#30417;&#27979;&#20154;&#30340;&#24037;&#20316;&#34892;&#20026;&#21644;&#21387;&#21147;&#27700;&#24179;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#31561;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#39640;&#21387;&#21147;&#21644;&#20302;&#21387;&#21147;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2306.09385</link><description>&lt;p&gt;
&#24212;&#29992;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21387;&#21147;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Employing Multimodal Machine Learning for Stress Detection. (arXiv:2306.09385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;AI&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#22810;&#31181;&#20449;&#24687;&#26469;&#31934;&#30830;&#30417;&#27979;&#20154;&#30340;&#24037;&#20316;&#34892;&#20026;&#21644;&#21387;&#21147;&#27700;&#24179;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#31561;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#39640;&#21387;&#21147;&#21644;&#20302;&#21387;&#21147;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#26102;&#20195;&#65292;&#20154;&#31867;&#30340;&#29983;&#27963;&#26041;&#24335;&#36234;&#26469;&#36234;&#27880;&#37325;&#30693;&#35782;&#65292;&#23548;&#33268;&#20037;&#22352;&#30340;&#24037;&#20316;&#26041;&#24335;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#12290;&#36825;&#23548;&#33268;&#35768;&#22810;&#20581;&#24247;&#21644;&#24515;&#29702;&#38556;&#30861;&#12290;&#24515;&#29702;&#20581;&#24247;&#26159;&#24403;&#20170;&#19990;&#30028;&#19978;&#26368;&#34987;&#24573;&#35270;&#20294;&#20063;&#26159;&#26368;&#20851;&#38190;&#30340;&#26041;&#38754;&#20043;&#19968;&#12290;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#20250;&#30452;&#25509;&#25110;&#38388;&#25509;&#22320;&#24433;&#21709;&#21040;&#20154;&#20307;&#20854;&#20182;&#37096;&#20998;&#65292;&#24182;&#22952;&#30861;&#20010;&#20154;&#30340;&#26085;&#24120;&#27963;&#21160;&#21644;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#21387;&#21147;&#24182;&#25214;&#20986;&#23548;&#33268;&#20005;&#37325;&#24515;&#29702;&#30142;&#30149;&#30340;&#21387;&#21147;&#36235;&#21183;&#23545;&#20110;&#20010;&#20154;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#28041;&#21450;&#22810;&#20010;&#22240;&#32032;&#12290;&#36890;&#36807;&#34701;&#21512;&#30001;&#34892;&#20026;&#27169;&#24335;&#20135;&#29983;&#30340;&#22810;&#20010;&#27169;&#24577;&#65288;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65289;&#21487;&#20197;&#20934;&#30830;&#22320;&#23454;&#29616;&#36825;&#31181;&#35782;&#21035;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#30830;&#23450;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;&#28982;&#32780;&#65292;&#20026;&#27492;&#30446;&#30340;&#25552;&#20986;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#38750;&#24120;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;AI&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#38754;&#37096;&#34920;&#24773;&#12289;&#35821;&#38899;&#27169;&#24335;&#12289;&#29983;&#29702;&#20449;&#21495;&#21644;&#33258;&#25253;&#25968;&#25454;&#65292;&#20934;&#30830;&#30417;&#27979;&#20154;&#30340;&#24037;&#20316;&#34892;&#20026;&#21644;&#21387;&#21147;&#27700;&#24179;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(Convolutional Neural Networks, CNNs)&#12289;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;(Time-delay Neural Networks, TDNNs)&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;(Multi-Layer Perceptron, MLP)&#31561;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#20998;&#31867;&#39640;&#21387;&#21147;&#21644;&#20302;&#21387;&#21147;&#29366;&#24577;&#12290;&#22312;&#20844;&#20849;&#21387;&#21147;&#25968;&#25454;&#38598;&#21644;&#20174;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#37027;&#37324;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20934;&#30830;&#26816;&#27979;&#21387;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current age, human lifestyle has become more knowledge oriented leading to generation of sedentary employment. This has given rise to a number of health and mental disorders. Mental wellness is one of the most neglected but crucial aspects of today's world. Mental health issues can, both directly and indirectly, affect other sections of human physiology and impede an individual's day-to-day activities and performance. However, identifying the stress and finding the stress trend for an individual leading to serious mental ailments is challenging and involves multiple factors. Such identification can be achieved accurately by fusing these multiple modalities (due to various factors) arising from behavioral patterns. Certain techniques are identified in the literature for this purpose; however, very few machine learning-based methods are proposed for such multimodal fusion tasks. In this work, a multimodal AI-based framework is proposed to monitor a person's working behavior and st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#30340;&#20004;&#20010;&#26377;&#25928;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#26102;&#25928;&#39640;&#30340;&#28304;&#20998;&#31163;&#32593;&#32476;&#21644;&#36866;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;&#28304;&#20998;&#31163;&#30340;&#25439;&#22833;&#25513;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09382</link><description>&lt;p&gt;
2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#8212;&#8212;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Sound Demixing Challenge 2023 -- Music Demixing Track Technical Report. (arXiv:2306.09382v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#30340;&#20004;&#20010;&#26377;&#25928;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#26102;&#25928;&#39640;&#30340;&#28304;&#20998;&#31163;&#32593;&#32476;&#21644;&#36866;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;&#28304;&#20998;&#31163;&#30340;&#25439;&#22833;&#25513;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#20013;&#33719;&#22870;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#31181;&#22312;&#27492;&#25361;&#25112;&#20013;&#35774;&#35745;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#26102;&#25928;&#39640;&#30340;&#28304;&#20998;&#31163;&#32593;&#32476;&#65292;&#22312;MUSDB&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65307;&#19968;&#31181;&#36866;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;&#28304;&#20998;&#31163;&#30340;&#25439;&#22833;&#25513;&#27169;&#26041;&#27861;&#12290;&#22312;github.com/kuielab/sdx23&#19978;&#25552;&#20379;&#20102;&#27169;&#22411;&#35757;&#32451;&#21644;&#26368;&#32456;&#25552;&#20132;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this report, we present our award-winning solutions for the Music Demixing Track of Sound Demixing Challenge 2023. We focus on two methods designed for this challenge: a time-efficient source separation network that achieves state-of-the-art results on the MUSDB benchmark and a loss masking method for noise-robust source separation. Code for reproducing model training and final submissions is available at github.com/kuielab/sdx23.
&lt;/p&gt;</description></item><item><title>STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09381</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation. (arXiv:2306.09381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09381
&lt;/p&gt;
&lt;p&gt;
STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#22312;&#25919;&#31574;&#20915;&#31574;&#21644;&#32463;&#27982;&#34892;&#20026;&#30740;&#31350;&#20013;&#26377;&#30528;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#26088;&#22312;&#32473;&#23450;&#19968;&#23567;&#32452;&#36712;&#36857;&#25968;&#25454;&#29983;&#25104;&#20154;&#31867;&#31227;&#21160;&#36712;&#36857;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#22320;&#28857;&#20043;&#38388;&#30340;&#38745;&#24577;&#20851;&#31995;&#65292;&#32780;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;SpatioTemporal-Augmented gRaph&#31070;&#32463;&#32593;&#32476;&#65288;STAR&#65289;&#65292;&#26469;&#27169;&#25311;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#26799;&#24230;&#33539;&#22260;&#20004;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;Transformer&#20013;&#21442;&#25968;&#20849;&#20139;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#24615;&#26159;&#20854;&#20013;&#20027;&#35201;&#21407;&#22240;&#65292;&#27169;&#22411;&#22797;&#26434;&#24230;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.09380</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Understanding Parameter Sharing in Transformers. (arXiv:2306.09380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#26799;&#24230;&#33539;&#22260;&#20004;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;Transformer&#20013;&#21442;&#25968;&#20849;&#20139;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#24615;&#26159;&#20854;&#20013;&#20027;&#35201;&#21407;&#22240;&#65292;&#27169;&#22411;&#22797;&#26434;&#24230;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#20849;&#20139;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#22312;Transformer&#19978;&#30340;&#20808;&#21069;&#24037;&#20316;&#38598;&#20013;&#22312;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#20849;&#20139;&#21442;&#25968;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#28145;&#24230;&#26469;&#25552;&#39640;&#26377;&#38480;&#21442;&#25968;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#30740;&#31350;&#20026;&#20160;&#20040;&#27492;&#26041;&#27861;&#26377;&#25928;&#12290;&#39318;&#20808;&#65292;&#22686;&#21152;&#27169;&#22411;&#28145;&#24230;&#20250;&#20351;&#27169;&#22411;&#26356;&#21152;&#22797;&#26434;&#65292;&#25105;&#20204;&#20551;&#35774;&#21407;&#22240;&#19982;&#27169;&#22411;&#22797;&#26434;&#24230;&#65288;&#25351;FLOPs&#65289;&#26377;&#20851;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#27599;&#20010;&#20849;&#20139;&#21442;&#25968;&#22312;&#21521;&#21069;&#20256;&#25773;&#20013;&#20250;&#21442;&#19982;&#32593;&#32476;&#35745;&#31639;&#22810;&#27425;&#65292;&#20854;&#23545;&#24212;&#30340;&#26799;&#24230;&#20540;&#19982;&#21407;&#27169;&#22411;&#30340;&#26799;&#24230;&#20540;&#33539;&#22260;&#19981;&#21516;&#65292;&#36825;&#23558;&#24433;&#21709;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#35757;&#32451;&#25910;&#25947;&#24615;&#20063;&#26159;&#20854;&#20013;&#20043;&#19968;&#30340;&#21407;&#22240;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#26174;&#31034;&#27492;&#26041;&#27861;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#65292;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#24402;&#22240;&#20110;&#22686;&#21152;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;&#36825;&#21551;&#21457;&#20102;&#25105;&#20204;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter sharing has proven to be a parameter-efficient approach. Previous work on Transformers has focused on sharing parameters in different layers, which can improve the performance of models with limited parameters by increasing model depth. In this paper, we study why this approach works from two perspectives. First, increasing model depth makes the model more complex, and we hypothesize that the reason is related to model complexity (referring to FLOPs). Secondly, since each shared parameter will participate in the network computation several times in forward propagation, its corresponding gradient will have a different range of values from the original model, which will affect the model convergence. Based on this, we hypothesize that training convergence may also be one of the reasons. Through further analysis, we show that the success of this approach can be largely attributed to better convergence, with only a small part due to the increased model complexity. Inspired by this
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.09377</link><description>&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Language Aligned Visual Representations Predict Human Behavior in Naturalistic Learning Tasks. (arXiv:2306.09377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09377
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#22791;&#35782;&#21035;&#21644;&#27010;&#25324;&#33258;&#28982;&#29289;&#20307;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#26377;&#25152;&#24110;&#21161;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#24182;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#34920;&#31034;&#26041;&#24335;&#20197;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#28041;&#21450;&#31867;&#21035;&#23398;&#20064;&#21644;&#22870;&#21169;&#23398;&#20064;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#36924;&#30495;&#30340;&#22270;&#20687;&#20316;&#20026;&#21050;&#28608;&#29289;&#65292;&#24182;&#35201;&#27714;&#21442;&#19982;&#32773;&#22522;&#20110;&#25152;&#26377;&#35797;&#39564;&#30340;&#26032;&#22411;&#21050;&#28608;&#29289;&#20316;&#20986;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#27867;&#21270;&#12290;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#24213;&#23618;&#35268;&#21017;&#26159;&#20351;&#29992;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#25552;&#21462;&#30340;&#21050;&#28608;&#32500;&#24230;&#29983;&#25104;&#30340;&#31616;&#21333;&#32447;&#24615;&#20989;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21442;&#19982;&#32773;&#22312;&#20960;&#27425;&#35797;&#39564;&#20869;&#23601;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#30456;&#20851;&#30340;&#21050;&#28608;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#23545;&#20154;&#31867;&#36873;&#25321;&#30340;&#36880;&#27425;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#20248;&#20110;&#35270;&#35273;&#20219;&#21153;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#65292;&#34920;&#26126;&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#21487;&#33021;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess the ability to identify and generalize relevant features of natural objects, which aids them in various situations. To investigate this phenomenon and determine the most effective representations for predicting human behavior, we conducted two experiments involving category learning and reward learning. Our experiments used realistic images as stimuli, and participants were tasked with making accurate decisions based on novel stimuli for all trials, thereby necessitating generalization. In both tasks, the underlying rules were generated as simple linear functions using stimulus dimensions extracted from human similarity judgments. Notably, participants successfully identified the relevant stimulus features within a few trials, demonstrating effective generalization. We performed an extensive model comparison, evaluating the trial-by-trial predictive accuracy of diverse deep learning models' representations of human choices. Intriguingly, representations from models train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09376</link><description>&lt;p&gt;
&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27169;&#22359;&#21270;&#65306;&#19968;&#31181;&#26032;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Modularizing while Training: a New Paradigm for Modularizing DNN Models. (arXiv:2306.09376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#24050;&#25104;&#20026;&#26234;&#33021;&#36719;&#20214;&#31995;&#32479;&#20013;&#36234;&#26469;&#36234;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;DNN&#27169;&#22411;&#36890;&#24120;&#22312;&#26102;&#38388;&#21644;&#25104;&#26412;&#26041;&#38754;&#37117;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#24320;&#22987;&#20851;&#27880;&#37325;&#29992;&#29616;&#26377;&#30340;DNN&#27169;&#22411;-&#20511;&#37492;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#20195;&#30721;&#37325;&#29992;&#24605;&#24819;&#12290;&#20294;&#26159;&#65292;&#37325;&#29992;&#25972;&#20010;&#27169;&#22411;&#21487;&#33021;&#20250;&#36896;&#25104;&#39069;&#22806;&#30340;&#24320;&#38144;&#25110;&#20174;&#19981;&#38656;&#35201;&#30340;&#21151;&#33021;&#20013;&#32487;&#25215;&#24369;&#28857;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20998;&#35299;&#25104;&#27169;&#22359;&#65292;&#21363;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#65292;&#24182;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24182;&#19981;&#26159;&#20026;&#20102;&#27169;&#22359;&#21270;&#32780;&#26500;&#24314;&#30340;&#65292;&#25152;&#20197;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#20250;&#23548;&#33268;&#24040;&#22823;&#30340;&#24320;&#38144;&#21644;&#27169;&#22411;&#31934;&#24230;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;&#65288;MwT&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#27169;&#22411;&#20855;&#26377;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#33021;&#21147;&#65292;&#36825;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#21516;&#26102;&#20248;&#21270;&#27169;&#22359;&#20869;&#30340;&#20869;&#32858;&#24615;&#21644;&#27169;&#22359;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#30495;&#27491;&#30340;&#27169;&#22359;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#31216;&#24615;&#35782;&#21035;&#30340;&#20960;&#20309;&#34920;&#31034;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;Geom3D&#65292;&#21487;&#20197;&#29992;&#20110;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#26230;&#20307;&#26448;&#26009;&#30340;&#22522;&#20934;&#27979;&#35780;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09375</link><description>&lt;p&gt;
&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#26230;&#20307;&#26448;&#26009;&#30340;&#23545;&#31216;&#24615;&#35782;&#21035;&#20960;&#20309;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials. (arXiv:2306.09375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#31216;&#24615;&#35782;&#21035;&#30340;&#20960;&#20309;&#34920;&#31034;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;Geom3D&#65292;&#21487;&#20197;&#29992;&#20110;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#26230;&#20307;&#26448;&#26009;&#30340;&#22522;&#20934;&#27979;&#35780;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#22312;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#26448;&#26009;&#21457;&#29616;&#31561;&#39046;&#22495;&#24341;&#36215;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#31185;&#23398;&#30028;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#31185;&#23398;&#30028;&#65288;&#22914;&#29289;&#29702;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#65289;&#19982;&#26426;&#22120;&#23398;&#20064;&#30028;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#36824;&#27809;&#26377;&#36827;&#34892;&#36807;&#20851;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#20960;&#20309;&#34920;&#31034;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20379;&#20102;&#24403;&#21069;&#23545;&#31216;&#24615;&#20960;&#20309;&#26041;&#27861;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#23558;&#20854;&#20998;&#31867;&#20026;&#19977;&#31867;&#65306;&#19981;&#21464;&#24615;&#12289;&#20855;&#26377;&#29699;&#24418;&#26694;&#26550;&#22522;&#30784;&#30340;&#31561;&#21464;&#24615;&#21644;&#20855;&#26377;&#21521;&#37327;&#26694;&#26550;&#22522;&#30784;&#30340;&#31561;&#21464;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Geom3D&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#23545;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#26230;&#20307;&#26448;&#26009;&#30340;&#19981;&#21516;&#20960;&#20309;&#34920;&#31034;&#36827;&#34892;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35780;&#12290;&#21033;&#29992;Geom3D&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#29616;&#26377;&#20960;&#20309;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#29699;&#24418;&#26694;&#26550;&#22522;&#30784;&#21644;&#21521;&#37327;&#26694;&#26550;&#22522;&#30784;&#30340;&#31561;&#21464;&#24615;&#20248;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence for scientific discovery has recently generated significant interest within the machine learning and scientific communities, particularly in the domains of chemistry, biology, and material discovery. For these scientific problems, molecules serve as the fundamental building blocks, and machine learning has emerged as a highly effective and powerful tool for modeling their geometric structures. Nevertheless, due to the rapidly evolving process of the field and the knowledge gap between science (e.g., physics, chemistry, &amp; biology) and machine learning communities, a benchmarking study on geometrical representation for such data has not been conducted. To address such an issue, in this paper, we first provide a unified view of the current symmetry-informed geometric methods, classifying them into three main categories: invariance, equivariance with spherical frame basis, and equivariance with vector frame basis. Then we propose a platform, coined Geom3D, which ena
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMTL&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#27491;&#21017;&#21270;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#20445;&#35777;&#25910;&#25947;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09373</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equitable Multi-task Learning. (arXiv:2306.09373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMTL&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#27491;&#21017;&#21270;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#20445;&#35777;&#25910;&#25947;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22312;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#65289;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#19988;&#30456;&#20114;&#31454;&#20105;&#30340;&#30456;&#20851;&#24615;&#65292;&#21333;&#32431;&#22320;&#35757;&#32451;&#25152;&#26377;&#20219;&#21153;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#23398;&#20064;&#65292;&#21363;&#19968;&#20123;&#20219;&#21153;&#34987;&#24456;&#22909;&#22320;&#23398;&#20064;&#65292;&#32780;&#20854;&#20182;&#20219;&#21153;&#21017;&#34987;&#24573;&#35270;&#12290;&#22810;&#20219;&#21153;&#20248;&#21270;&#65288;MTO&#65289;&#26088;&#22312;&#21516;&#26102;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#20294;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#22312;&#20219;&#21153;&#25439;&#22833;&#35268;&#27169;&#25110;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;MTL&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#22312;&#26356;&#26032;&#20849;&#20139;&#21442;&#25968;&#26102;&#65292;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65288;&#21363;&#20219;&#21153;&#29305;&#23450;&#25439;&#22833;&#20540;&#38500;&#20197;&#20854;&#21407;&#22987;&#26799;&#24230;&#33539;&#25968;&#30340;&#20540;&#65289;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#21517;&#20026;EMTL&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;MTL&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#28155;&#21152;&#20102;&#26041;&#24046;&#27491;&#21017;&#21270;&#65292;&#20351;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#26356;&#20855;&#21487;&#27604;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#26469;&#20445;&#35777;&#25910;&#25947;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) has achieved great success in various research domains, such as CV, NLP and IR etc. Due to the complex and competing task correlation, na\"ive training all tasks may lead to inequitable learning, \textit{i.e.} some tasks are learned well while others are overlooked. Multi-task optimization (MTO) aims to improve all tasks at same time, but conventional methods often perform poor when tasks with large loss scale or gradient norm magnitude difference. To solve the issue, we in-depth investigate the equity problem for MTL and find that regularizing relative contribution of different tasks (\textit{i.e.} value of task-specific loss divides its raw gradient norm) in updating shared parameter can improve generalization performance of MTL. Based on our theoretical analysis, we propose a novel multi-task optimization method, named \textit{EMTL}, to achieve equitable MTL. Specifically, we efficiently add variance regularization to make different tasks' relative contribu
&lt;/p&gt;</description></item><item><title>Warpformer&#26159;&#19968;&#31181;&#33021;&#22815;&#23436;&#25972;&#32771;&#34385;&#24207;&#21015;&#20869;&#19981;&#35268;&#21017;&#24615;&#21644;&#24207;&#21015;&#38388;&#24046;&#24322;&#24615;&#30340;&#22810;&#23610;&#24230;&#24314;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09368</link><description>&lt;p&gt;
Warpformer: &#19968;&#31181;&#29992;&#20110;&#19981;&#35268;&#21017;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#22810;&#23610;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time Series. (arXiv:2306.09368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09368
&lt;/p&gt;
&lt;p&gt;
Warpformer&#26159;&#19968;&#31181;&#33021;&#22815;&#23436;&#25972;&#32771;&#34385;&#24207;&#21015;&#20869;&#19981;&#35268;&#21017;&#24615;&#21644;&#24207;&#21015;&#38388;&#24046;&#24322;&#24615;&#30340;&#22810;&#23610;&#24230;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#24456;&#24120;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#21576;&#29616;&#20986;&#24207;&#21015;&#20869;&#19981;&#35268;&#21017;&#24615;&#21644;&#24207;&#21015;&#38388;&#24046;&#24322;&#24615;&#12290;&#24207;&#21015;&#20869;&#19981;&#35268;&#21017;&#24615;&#25351;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#36890;&#24120;&#22312;&#19981;&#35268;&#21017;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#35760;&#24405;&#65292;&#32780;&#24207;&#21015;&#38388;&#24046;&#24322;&#24615;&#21017;&#25351;&#19981;&#21516;&#24207;&#21015;&#20043;&#38388;&#30340;&#37319;&#26679;&#29575;&#26174;&#33879;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20851;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#24207;&#21015;&#20869;&#19981;&#35268;&#21017;&#24615;&#38382;&#39064;&#65292;&#32780;&#24573;&#30053;&#20102;&#24207;&#21015;&#38388;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;Warpformer&#65292;&#23427;&#20805;&#20998;&#32771;&#34385;&#20102;&#36825;&#20004;&#31181;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Irregularly sampled multivariate time series are ubiquitous in various fields, particularly in healthcare, and exhibit two key characteristics: intra-series irregularity and inter-series discrepancy. Intra-series irregularity refers to the fact that time-series signals are often recorded at irregular intervals, while inter-series discrepancy refers to the significant variability in sampling rates among diverse series. However, recent advances in irregular time series have primarily focused on addressing intra-series irregularity, overlooking the issue of inter-series discrepancy. To bridge this gap, we present Warpformer, a novel approach that fully considers these two characteristics. In a nutshell, Warpformer has several crucial designs, including a specific input representation that explicitly characterizes both intra-series irregularity and inter-series discrepancy, a warping module that adaptively unifies irregular time series in a given scale, and a customized attention module fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20989;&#25968;&#38477;&#32500;&#26041;&#27861;&#32467;&#21512;&#30005;&#26426;&#30005;&#27969;&#29305;&#24449;&#20998;&#26512;&#31574;&#30053;&#30340;&#25925;&#38556;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24863;&#24212;&#30005;&#21160;&#26426;&#20013;&#30340;&#25925;&#38556;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31163;&#32447;&#20998;&#26512;&#35782;&#21035;&#26356;&#22810;&#31867;&#22411;&#30340;&#25925;&#38556;&#12290;</title><link>http://arxiv.org/abs/2306.09365</link><description>&lt;p&gt;
&#37319;&#29992;&#20989;&#25968;&#38477;&#32500;&#26041;&#27861;&#36827;&#34892;&#24863;&#24212;&#30005;&#21160;&#26426;&#25925;&#38556;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fault Detection in Induction Motors using Functional Dimensionality Reduction Methods. (arXiv:2306.09365v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20989;&#25968;&#38477;&#32500;&#26041;&#27861;&#32467;&#21512;&#30005;&#26426;&#30005;&#27969;&#29305;&#24449;&#20998;&#26512;&#31574;&#30053;&#30340;&#25925;&#38556;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24863;&#24212;&#30005;&#21160;&#26426;&#20013;&#30340;&#25925;&#38556;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31163;&#32447;&#20998;&#26512;&#35782;&#21035;&#26356;&#22810;&#31867;&#22411;&#30340;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26059;&#36716;&#30005;&#26426;&#19978;&#23454;&#26045;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#31574;&#30053;&#23545;&#20110;&#29616;&#20195;&#24037;&#19994;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#30340;&#30005;&#26426;&#30005;&#27969;&#29305;&#24449;&#20998;&#26512;&#31574;&#30053;&#19982;&#20989;&#25968;&#38477;&#32500;&#26041;&#27861;&#65288;&#21363;&#20989;&#25968;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#20989;&#25968;&#25193;&#25955;&#26144;&#23556;&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#26816;&#27979;&#21644;&#20998;&#31867;&#24863;&#24212;&#30005;&#21160;&#26426;&#20013;&#30340;&#25925;&#38556;&#26465;&#20214;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#33719;&#24471;&#20102;&#38750;&#24120;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#65292;&#19981;&#20165;&#21487;&#20197;&#23454;&#26102;&#26816;&#27979;&#24863;&#24212;&#30005;&#21160;&#26426;&#20013;&#25925;&#38556;&#30340;&#23384;&#22312;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36890;&#36807;&#31163;&#32447;&#20998;&#26512;&#35782;&#21035;&#26356;&#22810;&#31867;&#22411;&#30340;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
The implementation of strategies for fault detection and diagnosis on rotating electrical machines is crucial for the reliability and safety of modern industrial systems. The contribution of this work is a methodology that combines conventional strategy of Motor Current Signature Analysis with functional dimensionality reduction methods, namely Functional Principal Components Analysis and Functional Diffusion Maps, for detecting and classifying fault conditions in induction motors. The results obtained from the proposed scheme are very encouraging, revealing a potential use in the future not only for real-time detection of the presence of a fault in an induction motor, but also in the identification of a greater number of types of faults present through an offline analysis.
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09364</link><description>&lt;p&gt;
TSMixer: &#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09364
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22240;&#20854;&#33021;&#22815;&#25429;&#25417;&#38271;&#24207;&#21015;&#20132;&#20114;&#32780;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#20854;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#23545;&#38271;&#26399;&#39044;&#27979;&#26500;&#25104;&#20102;&#20005;&#37325;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TSMixer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#26550;&#26500;&#65292;&#19987;&#20026;&#22810;&#20803;&#39044;&#27979;&#21644;&#34917;&#19969;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#26159;Transformers&#30340;&#26377;&#25928;&#26367;&#20195;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20511;&#37492;&#20102;MLP-Mixer&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25104;&#21151;&#32463;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#35270;&#35273;MLP-Mixer&#36866;&#24212;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#32452;&#20214;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#36825;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#21363;&#23558;&#22312;&#32447;&#21327;&#35843;&#22836;&#38468;&#21152;&#21040;MLP-Mixer&#39592;&#24178;&#19978;&#65292;&#20197;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#23646;&#24615;&#65292;&#22914;&#23618;&#27425;&#32467;&#26500;&#21644;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36890;&#36947;&#24314;&#27169;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;&#32534;&#30721;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#36890;&#36947;&#21644;&#20445;&#30041;&#21333;&#20010;&#36890;&#36947;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TSMixer&#22312;&#19968;&#20803;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#22522;&#20110;Transformers&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#30340;&#32852;&#37030;&#23398;&#20064;&#25552;&#20986;&#20102;FedRDN&#26041;&#27861;&#65292;&#22312;&#36755;&#20837;&#23618;&#32423;&#19978;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#65292;&#23558;&#25972;&#20010;&#32852;&#37030;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#20449;&#24687;&#27880;&#20837;&#21040;&#26412;&#22320;&#23458;&#25143;&#31471;&#25968;&#25454;&#20013;&#65292;&#20197;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09363</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#38754;&#21521;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#32852;&#37030;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple Data Augmentation for Feature Distribution Skewed Federated Learning. (arXiv:2306.09363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#30340;&#32852;&#37030;&#23398;&#20064;&#25552;&#20986;&#20102;FedRDN&#26041;&#27861;&#65292;&#22312;&#36755;&#20837;&#23618;&#32423;&#19978;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#65292;&#23558;&#25972;&#20010;&#32852;&#37030;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#20449;&#24687;&#27880;&#20837;&#21040;&#26412;&#22320;&#23458;&#25143;&#31471;&#25968;&#25454;&#20013;&#65292;&#20197;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#24322;&#26500;&#24615;&#65288;&#21363;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65289;&#65292;&#23427;&#30340;&#24615;&#33021;&#24517;&#28982;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#25991;&#38024;&#23545;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#30340;FL&#22330;&#26223;&#23637;&#24320;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#30001;&#26412;&#22320;&#25968;&#25454;&#38598;&#20043;&#38388;&#28508;&#22312;&#20998;&#24067;&#19981;&#21516;&#23548;&#33268;&#30340;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) facilitates collaborative learning among multiple clients in a distributed manner, while ensuring privacy protection. However, its performance is inevitably degraded as suffering data heterogeneity, i.e., non-IID data. In this paper, we focus on the feature distribution skewed FL scenario, which is widespread in real-world applications. The main challenge lies in the feature shift caused by the different underlying distributions of local datasets. While the previous attempts achieved progress, few studies pay attention to the data itself, the root of this issue. Therefore, the primary goal of this paper is to develop a general data augmentation technique at the input level, to mitigate the feature shift. To achieve this goal, we propose FedRDN, a simple yet remarkably effective data augmentation method for feature distribution skewed FL, which randomly injects the statistics of the dataset from the entire federation into the client's data. By this, our method ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24310;&#36831;&#32447;&#25506;&#27979;&#22120;&#26102;&#31354;&#22810;&#20107;&#20214;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#25104;&#21151;&#22320;&#37325;&#26500;&#22810;&#20010;&#31890;&#23376;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22352;&#26631;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#39640;&#25928;&#21644;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2306.09359</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24310;&#36831;&#32447;&#25506;&#27979;&#22120;&#26102;&#31354;&#22810;&#20107;&#20214;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Spatiotemporal Multi-Event Reconstruction for Delay Line Detectors. (arXiv:2306.09359v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24310;&#36831;&#32447;&#25506;&#27979;&#22120;&#26102;&#31354;&#22810;&#20107;&#20214;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#25104;&#21151;&#22320;&#37325;&#26500;&#22810;&#20010;&#31890;&#23376;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22352;&#26631;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#39640;&#25928;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29289;&#29702;&#23398;&#20013;&#65292;&#35266;&#23519;&#38750;&#24120;&#30701;&#30340;&#26102;&#38388;&#31383;&#21475;&#20869;&#20004;&#20010;&#25110;&#26356;&#22810;&#31890;&#23376;&#30340;&#20301;&#32622;&#19968;&#30452;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#23545;&#20110;&#20302;&#33021;&#30005;&#23376;&#65292;&#21487;&#20197;&#20351;&#29992;&#24494;&#36890;&#36947;&#26495;&#21644;&#24310;&#36831;&#32447;&#30340;&#32452;&#21512;&#26500;&#25104;&#24310;&#36831;&#32447;&#25506;&#27979;&#22120;&#65292;&#20197;&#35835;&#20986;&#20837;&#23556;&#31890;&#23376;&#30340;&#20301;&#32622;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#37325;&#26500;&#22810;&#20010;&#31890;&#23376;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22352;&#26631;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#20010;&#25509;&#36817;&#30340;&#31890;&#23376;&#65292;&#20256;&#32479;&#30340;&#23792;&#20540;&#26597;&#25214;&#31639;&#27861;&#26080;&#27861;&#26377;&#25928;&#35782;&#21035;&#21644;&#37325;&#26500;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#31354;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#21367;&#31215;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#30701;&#26102;&#38388;&#31383;&#21475;&#20869;&#25552;&#21462;&#21644;&#32452;&#21512;&#22810;&#20010;&#31890;&#23376;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#25104;&#21151;&#22320;&#37325;&#24314;&#20102;&#21253;&#21547;&#22810;&#36798;&#22235;&#20010;&#31890;&#23376;&#30340;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate observation of two or more particles within a very narrow time window has always been a challenge in modern physics. It creates the possibility of correlation experiments, such as the ground-breaking Hanbury Brown-Twiss experiment, leading to new physical insights. For low-energy electrons, one possibility is to use a microchannel plate with subsequent delay lines for the readout of the incident particle hits, a setup called a Delay Line Detector. The spatial and temporal coordinates of more than one particle can be fully reconstructed outside a region called the dead radius. For interesting events, where two electrons are close in space and time, the determination of the individual positions of the electrons requires elaborate peak finding algorithms. While classical methods work well with single particle hits, they fail to identify and reconstruct events caused by multiple nearby particles. To address this challenge, we present a new spatiotemporal machine learning model to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#23433;&#20840;&#31574;&#30053;&#12289;&#25968;&#25454;&#38598;&#21644;&#39640;&#36136;&#37327;RL&#31639;&#27861;&#23454;&#29616;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#21033;&#29992;&#20808;&#36827;&#31639;&#27861;&#29983;&#25104;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;38&#20010;&#21463;&#27426;&#36814;&#30340;&#23433;&#20840;RL&#20219;&#21153;&#12290;&#35813;&#22871;&#20214;&#21487;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.09303</link><description>&lt;p&gt;
&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Datasets and Benchmarks for Offline Safe Reinforcement Learning. (arXiv:2306.09303v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09303
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#23433;&#20840;&#31574;&#30053;&#12289;&#25968;&#25454;&#38598;&#21644;&#39640;&#36136;&#37327;RL&#31639;&#27861;&#23454;&#29616;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#21033;&#29992;&#20808;&#36827;&#31639;&#27861;&#29983;&#25104;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;38&#20010;&#21463;&#27426;&#36814;&#30340;&#23433;&#20840;RL&#20219;&#21153;&#12290;&#35813;&#22871;&#20214;&#21487;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25361;&#25112;&#30340;&#32508;&#21512;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#24320;&#21457;&#21644;&#35780;&#20272;&#35757;&#32451;&#21644;&#37096;&#32626;&#38454;&#27573;&#20013;&#30340;&#23433;&#20840;&#23398;&#20064;&#31639;&#27861;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#22871;&#20214;&#21253;&#21547;&#19977;&#20010;&#32452;&#20214;&#65306;1&#65289;&#19987;&#23478;&#21046;&#20316;&#30340;&#23433;&#20840;&#31574;&#30053;&#65292;2&#65289;D4RL&#26679;&#24335;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#29615;&#22659;&#21253;&#35013;&#22120;&#65292;&#20197;&#21450;3&#65289;&#39640;&#36136;&#37327;&#30340;&#31163;&#32447;&#23433;&#20840;RL&#22522;&#20934;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#26465;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#30001;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#25903;&#25345;&#65292;&#21487;&#20197;&#20419;&#36827;&#22312;38&#20010;&#21463;&#27426;&#36814;&#30340;&#23433;&#20840;RL&#20219;&#21153;&#20013;&#29983;&#25104;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#20174;&#26426;&#22120;&#20154;&#25511;&#21046;&#21040;&#33258;&#21160;&#39550;&#39542;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#25968;&#25454;&#21518;&#22788;&#29702;&#36807;&#28388;&#22120;&#65292;&#33021;&#22815;&#20462;&#25913;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#27169;&#25311;&#21508;&#31181;&#25968;&#25454;&#25910;&#38598;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27969;&#34892;&#30340;&#31163;&#32447;&#23433;&#20840;RL&#31639;&#27861;&#30340;&#31934;&#32654;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#65292;&#20197;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290; &#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive benchmarking suite tailored to offline safe reinforcement learning (RL) challenges, aiming to foster progress in the development and evaluation of safe learning algorithms in both the training and deployment phases. Our benchmark suite contains three packages: 1) expertly crafted safe policies, 2) D4RL-styled datasets along with environment wrappers, and 3) high-quality offline safe RL baseline implementations. We feature a methodical data collection pipeline powered by advanced safe RL algorithms, which facilitates the generation of diverse datasets across 38 popular safe RL tasks, from robot control to autonomous driving. We further introduce an array of data post-processing filters, capable of modifying each dataset's diversity, thereby simulating various data collection conditions. Additionally, we provide elegant and extensible implementations of prevalent offline safe RL algorithms to accelerate research in this area. Through extensive experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#34701;&#20837;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09147</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Learning of Multivariate Time Series with Temporal Irregularity. (arXiv:2306.09147v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#34701;&#20837;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#25910;&#38598;&#30340;&#22810;&#20803;&#24207;&#21015;&#25968;&#25454;&#32463;&#24120;&#34920;&#29616;&#20986;&#26102;&#38388;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#21253;&#25324;&#38750;&#22343;&#21248;&#26102;&#38388;&#38388;&#38548;&#21644;&#32452;&#20214;&#38169;&#20301;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#22343;&#21248;&#30340;&#38388;&#36317;&#21644;&#24322;&#27493;&#26159;&#25968;&#25454;&#20869;&#29983;&#29305;&#24449;&#32780;&#19981;&#26159;&#19981;&#36275;&#35266;&#23519;&#30340;&#32467;&#26524;&#65292;&#21017;&#36825;&#20123;&#19981;&#35268;&#21017;&#24615;&#30340;&#20449;&#24687;&#20869;&#23481;&#22312;&#34920;&#24449;&#22810;&#20803;&#20381;&#36182;&#32467;&#26500;&#26102;&#21457;&#25381;&#20915;&#23450;&#24615;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#35201;&#20040;&#26131;&#21463;&#21040;&#25554;&#34917;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#35201;&#20040;&#23558;&#21442;&#25968;&#20551;&#35774;&#24378;&#21152;&#20110;&#25968;&#25454;&#20998;&#24067;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36825;&#26159;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#26680;&#24515;&#12290;&#20026;&#20102;&#35748;&#35782;&#21040;&#26102;&#38388;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#32452;&#20214;&#21551;&#29992;&#21807;&#19968;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#20197;&#20415;&#21040;&#36798;&#26102;&#38388;&#21487;&#20197;&#25351;&#23548;&#20309;&#26102;&#12289;&#22914;&#20309;&#21644;&#21738;&#20010;&#38544;&#34255;&#29366;&#24577;&#24212;&#35813;&#26356;&#26032;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#22320;&#34701;&#20837;&#20102;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#65292;&#35813;&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#35266;&#27979;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#20102;&#36328;&#32452;&#20214;&#30340;&#26102;&#38388;&#24322;&#36136;&#24615;&#21644;&#26465;&#20214;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate sequential data collected in practice often exhibit temporal irregularities, including nonuniform time intervals and component misalignment. However, if uneven spacing and asynchrony are endogenous characteristics of the data rather than a result of insufficient observation, the information content of these irregularities plays a defining role in characterizing the multivariate dependence structure. Existing approaches for probabilistic forecasting either overlook the resulting statistical heterogeneities, are susceptible to imputation biases, or impose parametric assumptions on the data distribution. This paper proposes an end-to-end solution that overcomes these limitations by allowing the observation arrival times to play the central role of model construction, which is at the core of temporal irregularities. To acknowledge temporal irregularities, we first enable unique hidden states for components so that the arrival times can dictate when, how, and which hidden state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#19982;&#22686;&#24378;&#20998;&#23376;&#21160;&#21147;&#23398;&#26041;&#27861;&#30340;&#34701;&#21512;&#65292;&#36825;&#20123;&#25216;&#26415;&#33021;&#22815;&#35299;&#20915;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#26102;&#38388;&#23610;&#24230;&#38480;&#21046;&#65292;&#25104;&#21151;&#30340;&#31574;&#30053;&#21253;&#25324;&#38477;&#32500;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#27969;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09111</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#37319;&#26679;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Enhanced Sampling with Machine Learning: A Review. (arXiv:2306.09111v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#19982;&#22686;&#24378;&#20998;&#23376;&#21160;&#21147;&#23398;&#26041;&#27861;&#30340;&#34701;&#21512;&#65292;&#36825;&#20123;&#25216;&#26415;&#33021;&#22815;&#35299;&#20915;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#26102;&#38388;&#23610;&#24230;&#38480;&#21046;&#65292;&#25104;&#21151;&#30340;&#31574;&#30053;&#21253;&#25324;&#38477;&#32500;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#27969;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21160;&#21147;&#23398; (MD) &#21487;&#20197;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#31934;&#30830;&#22320;&#30740;&#31350;&#29289;&#29702;&#31995;&#32479;&#65292;&#20294;&#26159;&#23384;&#22312;&#30528;&#20005;&#37325;&#30340;&#26102;&#38388;&#23610;&#24230;&#38480;&#21046;&#12290;&#20026;&#20102;&#25913;&#21892;&#37197;&#32622;&#31354;&#38388;&#25506;&#32034;&#33021;&#21147;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#22686;&#24378;&#37319;&#26679;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23454;&#26045;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064; (ML) &#25216;&#26415;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#22240;&#27492;&#22312;&#22686;&#24378;&#37319;&#26679;&#20013;&#37319;&#29992;&#20102;&#36825;&#20123;&#25216;&#26415;&#12290;&#23613;&#31649; ML &#20027;&#35201;&#30001;&#20110;&#20854;&#25968;&#25454;&#39537;&#21160;&#30340;&#26412;&#36136;&#32780;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20294;&#20854;&#19982;&#22686;&#24378;&#37319;&#26679;&#30340;&#38598;&#25104;&#26356;&#33258;&#28982;&#65292;&#20849;&#20139;&#35768;&#22810;&#20849;&#21516;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#26412;&#32508;&#36848;&#36890;&#36807;&#21576;&#29616;&#19981;&#21516;&#30340;&#20849;&#20139;&#35266;&#28857;&#65292;&#25506;&#32034;&#20102;ML&#21644;&#22686;&#24378;MD&#30340;&#34701;&#21512;&#12290;&#23427;&#25552;&#20379;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#36825;&#21487;&#33021;&#38590;&#20197;&#20445;&#25345;&#26368;&#26032;&#12290;&#25105;&#20204;&#31361;&#20986;&#20102;&#20687;&#38477;&#32500;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#27969;&#30340;&#26041;&#27861;&#31561;&#25104;&#21151;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular dynamics (MD) enables the study of physical systems with excellent spatiotemporal resolution but suffers from severe time-scale limitations. To address this, enhanced sampling methods have been developed to improve exploration of configurational space. However, implementing these is challenging and requires domain expertise. In recent years, integration of machine learning (ML) techniques in different domains has shown promise, prompting their adoption in enhanced sampling as well. Although ML is often employed in various fields primarily due to its data-driven nature, its integration with enhanced sampling is more natural with many common underlying synergies. This review explores the merging of ML and enhanced MD by presenting different shared viewpoints. It offers a comprehensive overview of this rapidly evolving field, which can be difficult to stay updated on. We highlight successful strategies like dimensionality reduction, reinforcement learning, and flow-based methods
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;5.7&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#29992;&#20110;&#38548;&#31163;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.08754</link><description>&lt;p&gt;
ClimSim&#65306;&#29992;&#20110;&#22312;&#28151;&#21512;&#22810;&#23610;&#24230;&#27668;&#20505;&#27169;&#25311;&#22120;&#20013;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators. (arXiv:2306.08754v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08754
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;5.7&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#29992;&#20110;&#38548;&#31163;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#27668;&#20505;&#39044;&#27979;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#32570;&#20047;&#36275;&#22815;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#19968;&#20010;&#21518;&#26524;&#26159;&#23545;&#20851;&#38190;&#36807;&#31243;&#65288;&#22914;&#26292;&#39118;&#38632;&#65289;&#30340;&#39044;&#27979;&#19981;&#20934;&#30830;&#21644;&#19981;&#31934;&#30830;&#12290;&#23558;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#27169;&#24335;&#24341;&#20837;&#20102;&#26032;&#19968;&#20195;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#27668;&#20505;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#23494;&#38598;&#22411;&#12289;&#30701;&#12289;&#39640;&#20998;&#36776;&#29575;&#30340;&#27169;&#25311;&#22996;&#25176;&#32473;ML&#20223;&#30495;&#22120;&#65292;&#21487;&#20197;&#36991;&#20813;&#25705;&#23572;&#23450;&#24459;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#28151;&#21512;&#30340;ML-&#29289;&#29702;&#20223;&#30495;&#26041;&#27861;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30340;&#22788;&#29702;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#22521;&#35757;&#25968;&#25454;&#21644;&#30456;&#20851;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#19968;&#30452;&#26080;&#27861;&#35775;&#38382;ML&#19987;&#23478;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; ClimSim&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#20026;&#28151;&#21512;ML-&#29289;&#29702;&#30740;&#31350;&#32780;&#35774;&#35745;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290;&#23427;&#30001;&#27668;&#20505;&#31185;&#23398;&#23478;&#21644;ML&#30740;&#31350;&#20154;&#21592;&#32852;&#21512;&#24320;&#21457;&#30340;&#22810;&#23610;&#24230;&#27668;&#20505;&#27169;&#25311;&#32452;&#25104;&#65292;&#21253;&#25324;57&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#38548;&#31163;&#20102;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#21644;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#23398;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise prediction of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#24179;&#28369;&#19988;&#24378;&#23545;&#25968;&#20985;&#23494;&#24230;&#30340;&#30446;&#26631;&#20998;&#24067;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#35823;&#24046;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.08494</link><description>&lt;p&gt;
&#38024;&#23545;&#24378;&#23545;&#25968;&#20985;&#20998;&#24067;&#30340; Langevin Monte Carlo&#65306;&#38543;&#26426;&#20013;&#28857;&#26041;&#27861;&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Langevin Monte Carlo for strongly log-concave distributions: Randomized midpoint revisited. (arXiv:2306.08494v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#24179;&#28369;&#19988;&#24378;&#23545;&#25968;&#20985;&#23494;&#24230;&#30340;&#30446;&#26631;&#20998;&#24067;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#35823;&#24046;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20174;&#19968;&#20010;&#20855;&#26377; $\mathbb R^p$ &#20013;&#20219;&#24847;&#20301;&#32622;&#30340;&#24179;&#28369;&#19988;&#24378;&#23545;&#25968;&#20985;&#23494;&#24230;&#30446;&#26631;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#27809;&#26377;&#39069;&#22806;&#30340;&#23494;&#24230;&#20449;&#24687;&#21487;&#29992;&#65292;&#38543;&#26426;&#20013;&#28857;&#21010;&#20998;&#23545;&#20110;&#21160;&#21147;&#23398; Langevin &#25193;&#25955;&#26159;&#24050;&#30693;&#30340;&#26368;&#21487;&#25193;&#23637;&#30340;&#39640;&#32500;&#26041;&#27861;&#65292;&#20855;&#26377;&#22823;&#30340;&#26465;&#20214;&#25968;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;Wasserstein-2&#35823;&#24046;&#30340;&#19968;&#31181;&#38750;&#28176;&#36827;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#19978;&#38480;&#12290;&#20026;&#20102;&#26356;&#20840;&#38754;&#22320;&#35299;&#37322;&#24314;&#31435;&#21487;&#35745;&#31639;&#30340;&#19978;&#38480;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#39321;&#20892;&#29109;&#21644;&#20013;&#28857;&#21010;&#20998;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20197;&#20415;&#38416;&#26126;&#22522;&#26412;&#21407;&#29702;&#24182;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#30830;&#31435;&#20102;&#19982;&#20013;&#28857;&#21010;&#20998;&#30340;&#21160;&#21147;&#23398; Langevin &#36827;&#31243;&#26377;&#20851;&#30340;&#25913;&#36827;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#20026;&#37319;&#29992;&#27431;&#25289;&#21010;&#20998;&#30340;&#21160;&#21147;&#23398; Langevin &#36827;&#31243;&#24314;&#31435;&#20102;&#26032;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the problem of sampling from a target distribution that has a smooth strongly log-concave density everywhere in $\mathbb R^p$. In this context, if no additional density information is available, the randomized midpoint discretization for the kinetic Langevin diffusion is known to be the most scalable method in high dimensions with large condition numbers. Our main result is a nonasymptotic and easy to compute upper bound on the Wasserstein-2 error of this method. To provide a more thorough explanation of our method for establishing the computable upper bound, we conduct an analysis of the midpoint discretization for the vanilla Langevin process. This analysis helps to clarify the underlying principles and provides valuable insights that we use to establish an improved upper bound for the kinetic Langevin process with the midpoint discretization. Furthermore, by applying these techniques we establish new guarantees for the kinetic Langevin process with Euler discretization, w
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25216;&#33021;&#31579;&#36873;&#19982;&#20248;&#21270;&#30340;Skill-Critic&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#24378;&#21270;&#23398;&#20064;&#20013;&#20302;&#23618;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08388</link><description>&lt;p&gt;
Skill-Critic: &#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#25216;&#33021;&#30340;&#31579;&#36873;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Skill-Critic: Refining Learned Skills for Reinforcement Learning. (arXiv:2306.08388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08388
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#31579;&#36873;&#19982;&#20248;&#21270;&#30340;Skill-Critic&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#24378;&#21270;&#23398;&#20064;&#20013;&#20302;&#23618;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#26102;&#38388;&#25277;&#35937;&#23558;&#19968;&#20010;&#31574;&#30053;&#20998;&#20026;&#22810;&#20010;&#23618;&#27425;&#65292;&#21152;&#24555;&#38271;&#26399;&#20915;&#31574;&#30340;&#36895;&#24230;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#29615;&#22659;&#20013;&#65292;&#25216;&#33021;&#21363;&#21407;&#22987;&#21160;&#20316;&#30340;&#24207;&#21015;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26377;&#26395;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#25216;&#33021;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;&#31574;&#30053;&#26159;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#65292;&#20294;&#30001;&#20110;&#28436;&#31034;&#35206;&#30422;&#33539;&#22260;&#20302;&#25110;&#20998;&#24067;&#36716;&#31227;&#65292;&#25152;&#24471;&#21040;&#30340;&#20302;&#23618;&#31574;&#30053;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Fine-tuning&#20302;&#23618;&#31574;&#30053;&#19982;&#39640;&#23618;&#25216;&#33021;&#36873;&#25321;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;Skill-Critic&#31639;&#27861;&#20248;&#21270;&#20102;&#20302;&#23618;&#21644;&#39640;&#23618;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20174;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#21021;&#22987;&#21270;&#21644;&#35268;&#33539;&#21270;&#65292;&#20197;&#24341;&#23548;&#32852;&#21512;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#31232;&#30095;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Gran Turismo Sport&#20013;&#26032;&#30340;&#31232;&#30095;&#22870;&#21169;&#33258;&#20027;&#36187;&#36710;&#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Skill-Critic&#30340;&#20302;&#23618;&#31574;&#30053;Fine-tuning&#21644;&#28436;&#31034;&#24341;&#23548;&#31574;&#30053;&#21021;&#22987;&#21270;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning (RL) can accelerate long-horizon decision-making by temporally abstracting a policy into multiple levels. Promising results in sparse reward environments have been seen with skills, i.e. sequences of primitive actions. Typically, a skill latent space and policy are discovered from offline data, but the resulting low-level policy can be unreliable due to low-coverage demonstrations or distribution shifts. As a solution, we propose fine-tuning the low-level policy in conjunction with high-level skill selection. Our Skill-Critic algorithm optimizes both the low and high-level policies; these policies are also initialized and regularized by the latent space learned from offline demonstrations to guide the joint policy optimization. We validate our approach in multiple sparse RL environments, including a new sparse reward autonomous racing task in Gran Turismo Sport. The experiments show that Skill-Critic's low-level policy fine-tuning and demonstration-g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MMASD&#30340;&#33258;&#38381;&#30151;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#33258;&#27835;&#30103;&#24178;&#39044;&#12290;&#23427;&#21253;&#25324;&#20174;32&#21517;&#33258;&#38381;&#30151;&#24739;&#20799;&#30340;&#24178;&#39044;&#24405;&#38899;&#20013;&#20998;&#27573;&#30340;1,315&#20010;&#25968;&#25454;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#21253;&#21547;&#22235;&#31181;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.08243</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#38381;&#30151;&#24178;&#39044;&#20998;&#26512;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;MMASD&#12290;
&lt;/p&gt;
&lt;p&gt;
MMASD: A Multimodal Dataset for Autism Intervention Analysis. (arXiv:2306.08243v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08243
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MMASD&#30340;&#33258;&#38381;&#30151;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#33258;&#27835;&#30103;&#24178;&#39044;&#12290;&#23427;&#21253;&#25324;&#20174;32&#21517;&#33258;&#38381;&#30151;&#24739;&#20799;&#30340;&#24178;&#39044;&#24405;&#38899;&#20013;&#20998;&#27573;&#30340;1,315&#20010;&#25968;&#25454;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#21253;&#21547;&#22235;&#31181;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#21457;&#32946;&#24615;&#30142;&#30149;&#65292;&#20854;&#29305;&#24449;&#26159;&#37325;&#22823;&#30340;&#31038;&#20132;&#27807;&#36890;&#38556;&#30861;&#21644;&#22256;&#38590;&#30340;&#30693;&#35273;&#21644;&#34920;&#36798;&#27807;&#36890;&#25552;&#31034;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#20419;&#36827;&#33258;&#38381;&#30151;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#29305;&#23450;&#20998;&#26512;&#65292;&#24182;&#22312;&#33258;&#38381;&#30151;&#31038;&#21306;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#36825;&#38480;&#21046;&#20102;&#30001;&#20110;&#25968;&#25454;&#20849;&#20139;&#22797;&#26434;&#24615;&#32780;&#36328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#24320;&#28304;&#25968;&#25454;&#38598;MMASD&#20316;&#20026;&#22810;&#27169;&#24335;ASD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#33258;&#24739;&#26377;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#28216;&#25103;&#27835;&#30103;&#24178;&#39044;&#12290;MMASD&#21253;&#25324;32&#21517;&#24739;&#26377;ASD&#30340;&#20799;&#31461;&#30340;&#25968;&#25454;&#65292;&#20197;&#21450;&#20174;100&#22810;&#23567;&#26102;&#30340;&#24178;&#39044;&#24405;&#38899;&#20013;&#20998;&#27573;&#30340;1,315&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#20026;&#20419;&#36827;&#20844;&#20849;&#35775;&#38382;&#65292;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#21253;&#21547;&#22235;&#31181;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#30340;&#25968;&#25454;&#65306;&#65288;1&#65289;&#20809;&#27969;&#65292;&#65288;2&#65289;2D&#39592;&#26550;&#65292;&#65288;3&#65289;3D&#39592;&#26550;&#21644;&#65288;4&#65289;&#20020;&#24202;&#21307;&#29983;ASD&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autism spectrum disorder (ASD) is a developmental disorder characterized by significant social communication impairments and difficulties perceiving and presenting communication cues. Machine learning techniques have been broadly adopted to facilitate autism studies and assessments. However, computational models are primarily concentrated on specific analysis and validated on private datasets in the autism community, which limits comparisons across models due to privacy-preserving data sharing complications. This work presents a novel privacy-preserving open-source dataset, MMASD as a MultiModal ASD benchmark dataset, collected from play therapy interventions of children with Autism. MMASD includes data from 32 children with ASD, and 1,315 data samples segmented from over 100 hours of intervention recordings. To promote public access, each data sample consists of four privacy-preserving modalities of data: (1) optical flow, (2) 2D skeleton, (3) 3D skeleton, and (4) clinician ASD evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08161</link><description>&lt;p&gt;
h2oGPT&#65306;&#27665;&#20027;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPTs&#65289;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#29616;&#23454;&#24212;&#29992;&#32780;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#37325;&#22823;&#30340;&#39118;&#38505;&#65292;&#22914;&#23384;&#22312;&#26377;&#20559;&#35265;&#12289;&#31169;&#20154;&#25110;&#26377;&#23475;&#25991;&#26412;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#29256;&#26435;&#26448;&#26009;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19990;&#30028;&#19978;&#26368;&#22909;&#30340;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#12290;&#19982;&#24320;&#28304;&#31038;&#21306;&#21512;&#20316;&#65292;&#20316;&#20026;&#20854;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#20960;&#20010;LLM&#65292;&#20854;&#21442;&#25968;&#20174;7&#20159;&#21040;400&#20159;&#65292;&#21487;&#22312;&#23436;&#20840;&#33258;&#30001;&#30340;Apache 2.0&#35768;&#21487;&#19979;&#21830;&#29992;&#12290;&#25105;&#20204;&#30340;&#21457;&#24067;&#21253;&#25324;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#20351;&#20854;&#26356;&#21152;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#19981;&#20559;&#24494;&#20998;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#26080;&#20559;&#12289;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#26041;&#27861;&#23545;&#22797;&#26434;&#23494;&#24230;&#36827;&#34892;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545; MH &#37319;&#26679;&#22120;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.07961</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#20559;&#24494;&#20998;&#23545;&#25239;&#22797;&#26434;&#23494;&#24230;&#29983;&#25104;&#65292;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#19981;&#20559;&#24494;&#20998;&#20248;&#21270; MH &#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentiating Metropolis-Hastings to Optimize Intractable Densities. (arXiv:2306.07961v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#20114;&#32852;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#19981;&#20559;&#24494;&#20998;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#26080;&#20559;&#12289;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#26041;&#27861;&#23545;&#22797;&#26434;&#23494;&#24230;&#36827;&#34892;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545; MH &#37319;&#26679;&#22120;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27010;&#29575;&#27169;&#22411;&#25512;&#29702;&#20013;&#65292;&#30446;&#26631;&#23494;&#24230;&#20989;&#25968;&#36890;&#24120;&#21464;&#24471;&#38590;&#20197;&#35745;&#31639;&#65292;&#38656;&#35201;&#20351;&#29992; Monte Carlo &#35745;&#31639;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#20559;&#24494;&#20998; Metropolis-Hastings &#37319;&#26679;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#27010;&#29575;&#25512;&#29702;&#26469;&#36827;&#34892;&#24494;&#20998;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#24494;&#20998;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982; Markov &#38142;&#32806;&#21512;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#20559;&#65292;&#20302;&#26041;&#24046;&#21644;&#33258;&#21160;&#30340;&#31243;&#24207;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#24212;&#29992;&#20110;&#30001;&#20110;&#32321;&#29712;&#30340;&#30446;&#26631;&#23494;&#24230;&#23548;&#33268;&#26399;&#26395;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#25214;&#21040;&#19968;&#20010;&#27169;&#26865;&#20004;&#21487;&#30340;&#35266;&#23519;&#21644;&#22312; Ising &#27169;&#22411;&#20013;&#26368;&#22823;&#21270;&#27604;&#28909;&#26469;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When performing inference on probabilistic models, target densities often become intractable, necessitating the use of Monte Carlo samplers. We develop a methodology for unbiased differentiation of the Metropolis-Hastings sampler, allowing us to differentiate through probabilistic inference. By fusing recent advances in stochastic differentiation with Markov chain coupling schemes, the procedure can be made unbiased, low-variance, and automatic. This allows us to apply gradient-based optimization to objectives expressed as expectations over intractable target densities. We demonstrate our approach by finding an ambiguous observation in a Gaussian mixture model and by maximizing the specific heat in an Ising model.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#20250;&#20174;&#20165;&#21253;&#21547;&#36127;&#22870;&#21169;&#30340;&#20363;&#23376;&#20013;&#23398;&#20064;&#65292;&#23548;&#33268;&#29983;&#25104;&#31867;&#20284;&#27844;&#28431;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#31561;&#25935;&#24863;&#20449;&#24687;&#30340;&#25991;&#26412;</title><link>http://arxiv.org/abs/2306.07567</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#29983;&#25104;&#32431;&#36127;&#21453;&#39304;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07567
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#20250;&#20174;&#20165;&#21253;&#21547;&#36127;&#22870;&#21169;&#30340;&#20363;&#23376;&#20013;&#23398;&#20064;&#65292;&#23548;&#33268;&#29983;&#25104;&#31867;&#20284;&#27844;&#28431;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#31561;&#25935;&#24863;&#20449;&#24687;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#26102;&#65292;&#36890;&#24120;&#20250;&#35757;&#32451;&#21453;&#23545;&#26368;&#20005;&#37325;&#22833;&#36133;&#30340;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#24847;&#21619;&#30528;&#20351;&#29992;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65288;&#20363;&#22914;&#27844;&#38706;&#30340;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#65289;&#30340;&#26696;&#20363;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#21487;&#33021;&#20250;&#35748;&#20026;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#27704;&#36828;&#19981;&#20250;&#29983;&#25104;&#20165;&#22312;&#19982;&#26368;&#20302;&#22870;&#21169;&#30456;&#20851;&#32852;&#30340;&#31034;&#20363;&#20013;&#20986;&#29616;&#30340;&#25991;&#26412;&#29255;&#27573;&#12290;&#26412;&#25991;&#34920;&#26126;&#36825;&#31181;&#20551;&#35774;&#26159;&#38169;&#35823;&#30340;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30830;&#23454;&#20174;&#36825;&#31181;&#32431;&#36127;&#21453;&#39304;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#21040;&#20102;&#19996;&#35199;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#35757;&#32451;&#35774;&#32622;&#65292;&#20351;&#24471;Pythia-160M&#33021;&#22815;&#29983;&#25104;&#23494;&#30721;&#30340;&#27010;&#29575;&#30053;&#39640;&#20110;&#38543;&#26426;&#65292;&#23613;&#31649;&#20165;&#22312;&#23545;&#27169;&#22411;&#19981;&#36755;&#20986;&#36825;&#20123;&#23494;&#30721;&#30340;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;&#36825;&#20123;&#23494;&#30721;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/FabienRoger/Learning-From-Negative-Examples&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
When using adversarial training, it is common practice to train against the most egregious failures. However, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. We present a specific training setup that enables Pythia-160M to generate passwords with a probability slightly greater than chance, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. Our code is available at https://github.com/FabienRoger/Learning-From-Negative-Examples
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#23545;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#21387;&#32553;&#65288;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#65289;&#65292;&#23558;&#39044;&#22788;&#29702;&#22120;&#30340;&#23384;&#20648;&#25104;&#26412;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06098</link><description>&lt;p&gt;
&#38169;&#35823;&#21453;&#39304;&#21487;&#20197;&#20934;&#30830;&#22320;&#21387;&#32553;&#39044;&#22788;&#29702;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error Feedback Can Accurately Compress Preconditioners. (arXiv:2306.06098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#23545;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#21387;&#32553;&#65288;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#65289;&#65292;&#23558;&#39044;&#22788;&#29702;&#22120;&#30340;&#23384;&#20648;&#25104;&#26412;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#35268;&#27169;&#30340;&#20108;&#38454;&#20449;&#24687;&#26159;&#25913;&#36827;&#24403;&#21069;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#24615;&#33021;&#30340;&#20027;&#35201;&#36884;&#24452;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31934;&#30830;&#20840;&#30697;&#38453;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#22914;&#20840;&#30697;&#38453;Adagrad&#65288;GGT&#65289;&#25110;&#26080;&#30697;&#38453;&#36817;&#20284;&#26354;&#29575;&#65288;M-FAC&#65289;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#65292;&#20063;&#20250;&#36935;&#21040;&#24040;&#22823;&#30340;&#23384;&#20648;&#25104;&#26412;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#23384;&#20648;&#26799;&#24230;&#30340;&#28369;&#21160;&#31383;&#21475;&#65292;&#20854;&#23384;&#20648;&#38656;&#27714;&#22312;&#27169;&#22411;&#32500;&#24230;&#20013;&#26159;&#25104;&#20493;&#22686;&#21152;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#39640;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#23558;&#39044;&#22788;&#29702;&#22120;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23558;&#26799;&#24230;&#20449;&#24687;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#36890;&#36807;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#21387;&#32553;&#26799;&#24230;&#20449;&#24687;&#65292;&#23558;&#21387;&#32553;&#35823;&#24046;&#21453;&#39304;&#21040;&#26410;&#26469;&#30340;&#36845;&#20195;&#20013;&#12290;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging second-order information at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to medium-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via an efficient and simple-to-implement error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression \emph{before} it is fed into the preconditioner, feeding the compression error back into future iterations. Extensive experiments on deep neural networks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23558;&#36229;&#22768;&#20307;&#31215;&#28857;&#20113;&#25552;&#21462;&#20316;&#20026;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#24555;&#36895;&#21407;&#22411;&#24320;&#21457;&#39564;&#35777;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04668</link><description>&lt;p&gt;
SMRVIS&#65306;&#29992;&#20110;&#38750;&#30772;&#22351;&#24615;&#27979;&#35797;&#30340;&#19977;&#32500;&#36229;&#22768;&#28857;&#20113;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
SMRVIS: Point cloud extraction from 3-D ultrasound for non-destructive testing. (arXiv:2306.04668v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04668
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23558;&#36229;&#22768;&#20307;&#31215;&#28857;&#20113;&#25552;&#21462;&#20316;&#20026;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#24555;&#36895;&#21407;&#22411;&#24320;&#21457;&#39564;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#36229;&#22768;&#20307;&#31215;&#30340;&#28857;&#20113;&#25552;&#21462;&#20316;&#20026;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#36825;&#20010;&#20415;&#25463;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24555;&#36895;&#21407;&#22411;&#26469;&#25506;&#32034;U-Net&#26550;&#26500;&#30340;&#21508;&#31181;&#21464;&#20307;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26412;&#25253;&#21578;&#35760;&#24405;&#20102;&#20351;&#29992;5&#20010;&#26631;&#35760;&#36229;&#22768;&#20307;&#31215;&#21644;84&#20010;&#26410;&#26631;&#35760;&#20307;&#31215;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#23436;&#25104;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#36825;&#20010;&#24037;&#20316;&#26159;&#20316;&#20026;&#19968;&#20010;&#21517;&#20026;&#8220;&#36229;&#22768;&#22270;&#20687;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#8221;&#24320;&#25918;&#25361;&#25112;&#30340;&#25552;&#20132;&#30340;&#19968;&#37096;&#20998;&#12290;&#28304;&#20195;&#30721;&#24050;&#22312;Github&#19978;\url{https://github.com/lisatwyw/smrvis}&#20844;&#24320;&#20849;&#20139;&#32473;&#30740;&#31350;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to formulate point cloud extraction from ultrasound volumes as an image segmentation problem. Through this convenient formulation, a quick prototype exploring various variants of the U-Net architecture was developed and evaluated. This report documents the experimental results compiled using a training dataset of 5 labelled ultrasound volumes and 84 unlabelled volumes that got completed in a two-week period as part of a challenge submission to an open challenge entitled ``Deep Learning in Ultrasound Image Analysis''. Source code is shared with the research community at this GitHub URL \url{https://github.com/lisatwyw/smrvis}.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#26694;&#26550;SEER&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#34987;&#23458;&#25143;&#31471;&#20390;&#27979;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2306.03013</link><description>&lt;p&gt;
&#26126;&#34255;&#26263;&#31363;&#65306;&#20266;&#35013;&#25968;&#25454;&#31363;&#21462;&#25915;&#20987;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning. (arXiv:2306.03013v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#26694;&#26550;SEER&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#34987;&#23458;&#25143;&#31471;&#20390;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#24050;&#32463;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#31363;&#21462;&#22312;&#22823;&#25209;&#37327;&#21644;&#23433;&#20840;&#32858;&#21512;&#31561;&#20043;&#21069;&#34987;&#35270;&#20026;&#31169;&#23494;&#30340;&#35774;&#32622;&#20013;&#21464;&#24471;&#21487;&#34892;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20851;&#20110;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#23458;&#25143;&#31471;&#20390;&#27979;&#24615;&#30340;&#30097;&#34385;&#34987;&#25552;&#20986;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#34987;&#20844;&#24320;&#30693;&#26195;&#21518;&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#36136;&#30097;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20390;&#27979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35768;&#22810;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#21407;&#21017;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#37117;&#21487;&#20197;&#36890;&#36807;&#21512;&#29702;&#30340;&#23458;&#25143;&#31471;&#26816;&#26597;&#26469;&#26816;&#27979;&#20986;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#23454;&#29992;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#30340;&#29702;&#24819;&#35201;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;SEER&#25915;&#20987;&#26694;&#26550;&#65292;&#23427;&#28385;&#36275;&#25152;&#26377;&#29702;&#24819;&#35201;&#27714;&#65292;&#21487;&#20197;&#20174;&#29616;&#23454;&#32593;&#32476;&#30340;&#26799;&#24230;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#21363;&#20351;&#26159;&#22312;&#22823;&#25209;&#37327;(&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#26368;&#22823;&#21487;&#36798;512)&#21644;&#23433;&#20840;&#32858;&#21512;&#30340;&#24773;&#20917;&#19979;&#12290;SEER&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#20351;&#29992;&#31192;&#23494;&#35299;&#30721;&#22120;&#65292;&#22312;&#20849;&#20139;&#27169;&#22411;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#36808;&#21521;&#23454;&#29992;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#30340;&#26377;&#21069;&#36884;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malicious server (MS) attacks have enabled the scaling of data stealing in federated learning to large batch sizes and secure aggregation, settings previously considered private. However, many concerns regarding client-side detectability of MS attacks were raised, questioning their practicality once they are publicly known. In this work, for the first time, we thoroughly study the problem of client-side detectability.We demonstrate that most prior MS attacks, which fundamentally rely on one of two key principles, are detectable by principled client-side checks. Further, we formulate desiderata for practical MS attacks and propose SEER, a novel attack framework that satisfies all desiderata, while stealing user data from gradients of realistic networks, even for large batch sizes (up to 512 in our experiments) and under secure aggregation. The key insight of SEER is the use of a secret decoder, which is jointly trained with the shared model. Our work represents a promising first step to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#26080;&#27169;&#22411;&#25968;&#25454;&#23376;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;-&#29289;&#21697;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#20272;&#35745;&#27599;&#20010;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#20256;&#25773;&#26469;&#24179;&#28369;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16391</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26080;&#27169;&#22411;&#25968;&#25454;&#23376;&#37319;&#26679;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems. (arXiv:2305.16391v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#26080;&#27169;&#22411;&#25968;&#25454;&#23376;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;-&#29289;&#21697;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#20272;&#35745;&#27599;&#20010;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#20256;&#25773;&#26469;&#24179;&#28369;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#23376;&#37319;&#26679;&#24191;&#27867;&#29992;&#20110;&#21152;&#36895;&#35757;&#32451;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#12290;&#22823;&#22810;&#25968;&#23376;&#37319;&#26679;&#26041;&#27861;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#65292;&#24120;&#24120;&#38656;&#35201;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35797;&#39564;&#27169;&#22411;&#26469;&#36890;&#36807;&#26679;&#26412;&#38590;&#24230;&#31561;&#26041;&#24335;&#27979;&#37327;&#25968;&#25454;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#35797;&#39564;&#27169;&#22411;&#34987;&#38169;&#35823;&#25351;&#23450;&#26102;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;&#23558;&#20250;&#24694;&#21270;&#12290;&#37492;&#20110;&#35797;&#39564;&#27169;&#22411;&#30340;&#38169;&#35823;&#25351;&#23450;&#22312;&#30495;&#23454;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#32467;&#26500;&#65292;&#21363;&#22270;&#24418;&#26469;&#25506;&#32034;&#30340;&#26080;&#27169;&#22411;&#25968;&#25454;&#23376;&#37319;&#26679;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#29992;&#25143;-&#29289;&#21697;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#36890;&#36807;&#22270;&#23548;&#30005;&#24615;&#26469;&#20272;&#35745;&#27599;&#20010;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#65288;&#21363;&#29992;&#25143;-&#29289;&#21697;&#22270;&#20013;&#30340;&#19968;&#26465;&#36793;&#65289;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#20256;&#25773;&#27493;&#39588;&#65292;&#24179;&#28369;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#20540;&#12290;&#30001;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#26080;&#27169;&#22411;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#23558;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32452;&#21512;&#20351;&#29992;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#20219;&#20309;&#21333;&#19968;&#26041;&#27861;&#37117;&#35201;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data subsampling is widely used to speed up the training of large-scale recommendation systems. Most subsampling methods are model-based and often require a pre-trained pilot model to measure data importance via e.g. sample hardness. However, when the pilot model is misspecified, model-based subsampling methods deteriorate. Since model misspecification is persistent in real recommendation systems, we instead propose model-agnostic data subsampling methods by only exploring input data structure represented by graphs. Specifically, we study the topology of the user-item graph to estimate the importance of each user-item interaction (an edge in the user-item graph) via graph conductance, followed by a propagation step on the network to smooth out the estimated importance value.  Since our proposed method is model-agnostic, we can marry the merits of both model-agnostic and model-based subsampling methods. Empirically, we show that combing the two consistently improves over any single meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#65292;&#23427;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23547;&#25214;&#26368;&#20248;&#32467;&#26500;&#21442;&#25968;&#65292;&#26080;&#38656;&#35757;&#32451;&#23601;&#33021;&#21457;&#25381;&#34920;&#29616;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#36816;&#31639;&#36895;&#24230;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;&#20102;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.14065</link><description>&lt;p&gt;
&#19981;&#35201;&#35757;&#32451;&#23427;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks. (arXiv:2305.14065v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#65292;&#23427;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23547;&#25214;&#26368;&#20248;&#32467;&#26500;&#21442;&#25968;&#65292;&#26080;&#38656;&#35757;&#32451;&#23601;&#33021;&#21457;&#25381;&#34920;&#29616;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#36816;&#31639;&#36895;&#24230;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;&#20102;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS-GNN&#65289;&#24050;&#32463;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#25163;&#21160;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32487;&#25215;&#20102;&#20256;&#32479;NAS&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20248;&#21270;&#38590;&#24230;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20197;&#21069;&#30340;NAS&#26041;&#27861;&#24573;&#35270;&#20102;GNN&#30340;&#29420;&#29305;&#24615;&#65292;&#21363;GNN&#20855;&#26377;&#26080;&#38656;&#35757;&#32451;&#23601;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#29305;&#28857;&#12290;&#37319;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#30446;&#26631;&#23547;&#25214;&#26368;&#20248;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#24182;&#24471;&#20986;&#19968;&#31181;&#26032;&#30340;NAS-GNN&#26041;&#27861;&#65292;&#21363;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;NAC&#22312;GNN&#19978;&#23454;&#29616;&#20102;&#26080;&#26356;&#26032;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#39640;&#25928;&#35745;&#31639;&#12290;&#22312;&#22810;&#20010;GNN&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural architecture search (NAS) for Graph neural networks (GNNs), called NAS-GNNs, has achieved significant performance over manually designed GNN architectures. However, these methods inherit issues from the conventional NAS methods, such as high computational cost and optimization difficulty. More importantly, previous NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power without training. With the randomly-initialized weights, we can then seek the optimal architecture parameters via the sparse coding objective and derive a novel NAS-GNNs method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update scheme on GNNs and can efficiently compute in linear time. Empirical evaluations on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art performance, which is up to $200\times$ faster and $18.8\%$ more accurate than the strong baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#26377;&#25928;&#23398;&#20064;&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#38543;&#30528;&#31283;&#23450;&#32500;&#25968;&#22686;&#22823;&#32780;&#23398;&#20064;&#25152;&#26377;&#29366;&#24577;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13409</link><description>&lt;p&gt;
&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#26377;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates. (arXiv:2305.13409v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#26377;&#25928;&#23398;&#20064;&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#38543;&#30528;&#31283;&#23450;&#32500;&#25968;&#22686;&#22823;&#32780;&#23398;&#20064;&#25152;&#26377;&#29366;&#24577;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36890;&#36807;&#20811;&#21033;&#31119;&#24503;&#38376;&#21644;$O(\log(n))$&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26368;&#22810;&#20351;&#29992;$t$&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;$n$&#37327;&#23376;&#27604;&#29305;&#29366;&#24577;$|\psi\rangle$&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#29992;$\mathsf{poly}(n,2^t,1/\epsilon)$&#26102;&#38388;&#21644;$|\psi\rangle$&#30340;&#22797;&#21046;&#26469;&#23398;&#20064;$|\psi\rangle$&#65292;&#20351;&#20854;&#36319;&#30495;&#23454;&#29366;&#24577;&#30340;&#36317;&#31163;&#19981;&#36229;&#36807;$\epsilon$&#12290;&#35813;&#32467;&#26524;&#26159;&#19968;&#20010;&#31283;&#23450;&#32500;&#25968;&#36739;&#22823;&#30340;&#29366;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#29305;&#20363;&#65292;&#24403;&#19968;&#20010;&#37327;&#23376;&#29366;&#24577;&#30340;&#31283;&#23450;&#23376;&#32500;&#25968;&#20026;$k$&#65292;&#34920;&#31034;&#23427;&#34987;&#19968;&#20010;&#30001;$2^k$&#20010;Pauli&#31639;&#23376;&#30340;Abel&#32676;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give an algorithm that efficiently learns a quantum state prepared by Clifford gates and $O(\log(n))$ non-Clifford gates. Specifically, for an $n$-qubit state $\lvert \psi \rangle$ prepared with at most $t$ non-Clifford gates, we show that $\mathsf{poly}(n,2^t,1/\epsilon)$ time and copies of $\lvert \psi \rangle$ suffice to learn $\lvert \psi \rangle$ to trace distance at most $\epsilon$. This result follows as a special case of an algorithm for learning states with large stabilizer dimension, where a quantum state has stabilizer dimension $k$ if it is stabilized by an abelian group of $2^k$ Pauli operators. We also develop an efficient property testing algorithm for stabilizer dimension, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#25968;&#25454;&#65292;&#20351;&#29992;&#27010;&#29575;&#21333;&#32431;&#24418;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#30340;&#29983;&#25104;SDE&#27169;&#22411;&#12290;&#31216;&#20043;&#20026;Dirchlet&#25193;&#25955;&#20998;&#25968;&#27169;&#22411;&#12290;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#28385;&#36275;&#20005;&#26684;&#38480;&#21046;&#30340;&#26679;&#26412;&#65292;&#19988;&#36866;&#29992;&#20110;&#29983;&#25104;&#29983;&#29289;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.10699</link><description>&lt;p&gt;
&#29983;&#29289;&#24207;&#21015;&#29983;&#25104;&#30340;Dirichlet&#25193;&#25955;&#20998;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dirichlet Diffusion Score Model for Biological Sequence Generation. (arXiv:2305.10699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#25968;&#25454;&#65292;&#20351;&#29992;&#27010;&#29575;&#21333;&#32431;&#24418;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#30340;&#29983;&#25104;SDE&#27169;&#22411;&#12290;&#31216;&#20043;&#20026;Dirchlet&#25193;&#25955;&#20998;&#25968;&#27169;&#22411;&#12290;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#28385;&#36275;&#20005;&#26684;&#38480;&#21046;&#30340;&#26679;&#26412;&#65292;&#19988;&#36866;&#29992;&#20110;&#29983;&#25104;&#29983;&#29289;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29983;&#29289;&#24207;&#21015;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#28385;&#36275;&#22797;&#26434;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#35299;&#20915;&#26159;&#24456;&#33258;&#28982;&#30340;&#38382;&#39064;&#12290;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#27169;&#22411;&#26159;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#26368;&#21021;&#25552;&#20986;&#30340;SDE&#19981;&#26159;&#33258;&#28982;&#22320;&#29992;&#20110;&#24314;&#27169;&#31163;&#25955;&#25968;&#25454;&#12290;&#20026;&#20102;&#24320;&#21457;&#36866;&#29992;&#20110;&#31163;&#25955;&#25968;&#25454;&#65288;&#20363;&#22914;&#29983;&#29289;&#24207;&#21015;&#65289;&#30340;&#29983;&#25104;SDE&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#27010;&#29575;&#21333;&#32431;&#24418;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#38543;&#26426;&#20998;&#24067;&#30340;&#24179;&#31283;&#20998;&#24067;&#26159;Dirichlet&#20998;&#24067;&#12290;&#36825;&#20351;&#24471;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#23545;&#20110;&#24314;&#27169;&#31163;&#25955;&#25968;&#25454;&#26159;&#33258;&#28982;&#30340;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;Dirchlet&#25193;&#25955;&#20998;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#29420;&#29983;&#25104;&#20219;&#21153;&#35777;&#26126;&#20102;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#29983;&#25104;&#28385;&#36275;&#20005;&#26684;&#38480;&#21046;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#29983;&#25104;&#27169;&#22411;&#20063;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#29983;&#29289;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing biological sequences is an important challenge that requires satisfying complex constraints and thus is a natural problem to address with deep generative modeling. Diffusion generative models have achieved considerable success in many applications. Score-based generative stochastic differential equations (SDE) model is a continuous-time diffusion model framework that enjoys many benefits, but the originally proposed SDEs are not naturally designed for modeling discrete data. To develop generative SDE models for discrete data such as biological sequences, here we introduce a diffusion process defined in the probability simplex space with stationary distribution being the Dirichlet distribution. This makes diffusion in continuous space natural for modeling discrete data. We refer to this approach as Dirchlet diffusion score model. We demonstrate that this technique can generate samples that satisfy hard constraints using a Sudoku generation task. This generative model can also 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#30001;&#20154;&#31867;&#26631;&#27880;&#30340;&#20114;&#34917;&#26631;&#31614;&#65292;&#21019;&#36896;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#29616;&#23454;&#34920;&#29616;&#19979;CLL&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#23454;&#38469;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.08295</link><description>&lt;p&gt;
CLCIFAR&#65306;&#24102;&#20154;&#31867;&#26631;&#27880;&#20114;&#34917;&#26631;&#31614;&#30340;CIFAR&#27966;&#29983;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CLCIFAR: CIFAR-Derived Benchmark Datasets with Human Annotated Complementary Labels. (arXiv:2305.08295v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#30001;&#20154;&#31867;&#26631;&#27880;&#30340;&#20114;&#34917;&#26631;&#31614;&#65292;&#21019;&#36896;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#29616;&#23454;&#34920;&#29616;&#19979;CLL&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#23454;&#38469;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#65288;CLL&#65289;&#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20165;&#20351;&#29992;&#20114;&#34917;&#26631;&#31614;&#65288;&#26631;&#31034;&#23454;&#20363;&#19981;&#23646;&#20110;&#21738;&#20123;&#31867;&#21035;&#65289;&#26469;&#35757;&#32451;&#22810;&#31867;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;CLL&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#20004;&#20010;&#21407;&#22240;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#20114;&#34917;&#26631;&#31614;&#29983;&#25104;&#30340;&#20551;&#35774;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#20165;&#38480;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#33719;&#21462;&#26377;&#20851;CLL&#31639;&#27861;&#30340;&#30495;&#23454;&#19990;&#30028;&#34920;&#29616;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#35758;&#26469;&#25910;&#38598;&#30001;&#20154;&#31867;&#27880;&#37322;&#32773;&#27880;&#37322;&#30340;&#20114;&#34917;&#26631;&#31614;&#12290;&#36825;&#19968;&#21162;&#21147;&#23548;&#33268;&#21019;&#24314;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;CLCIFAR10&#21644;CLCIFAR20&#65292;&#20998;&#21035;&#30001;CIFAR10&#21644;CIFAR100&#27966;&#29983;&#32780;&#26469;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;https://github.com/ntucllab/complementary_cifar&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#22522;&#20934;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#36739;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24403;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#30340;&#20114;&#34917;&#26631;&#31614;&#26102;&#65292;&#24615;&#33021;&#26377;&#26126;&#26174;&#19979;&#38477;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#20351;&#24471;&#22312;&#26356;&#25509;&#36817;&#23454;&#38469;&#24212;&#29992;&#26465;&#20214;&#19979;&#35780;&#20272;&#31639;&#27861;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#26356;&#30495;&#23454;&#22320;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complementary-label learning (CLL) is a weakly-supervised learning paradigm that aims to train a multi-class classifier using only complementary labels, which indicate classes to which an instance does not belong. Despite numerous algorithmic proposals for CLL, their practical performance remains unclear for two reasons. Firstly, these algorithms often rely on assumptions about the generation of complementary labels. Secondly, their evaluation has been limited to synthetic datasets. To gain insights into the real-world performance of CLL algorithms, we developed a protocol to collect complementary labels annotated by human annotators. This effort resulted in the creation of two datasets, CLCIFAR10 and CLCIFAR20, derived from CIFAR10 and CIFAR100, respectively. These datasets, publicly released at https://github.com/ntucllab/complementary_cifar, represent the very first real-world CLL datasets. Through extensive benchmark experiments, we discovered a notable decline in performance when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24863;&#30693;&#30340;&#21435;&#20013;&#24515;&#21270;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20801;&#35768;&#38750;&#23545;&#25239;&#24615;&#26234;&#33021;&#20307;&#22312;&#23545;&#25239;&#26041;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36798;&#25104;&#20849;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.05573</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#25239;&#24863;&#30693;&#30340;&#21435;&#20013;&#24515;&#21270;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Algorithm For Adversary Aware Decentralized Networked MARL. (arXiv:2305.05573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24863;&#30693;&#30340;&#21435;&#20013;&#24515;&#21270;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20801;&#35768;&#38750;&#23545;&#25239;&#24615;&#26234;&#33021;&#20307;&#22312;&#23545;&#25239;&#26041;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#21464;&#24471;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#24322;&#26500;&#20307;&#25317;&#26377;&#33258;&#24049;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#30456;&#23545;&#20110;&#20551;&#23450;&#25152;&#26377;&#26234;&#33021;&#20307;&#25317;&#26377;&#20849;&#21516;&#22870;&#21169;&#20989;&#25968;&#30340;&#32463;&#20856;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#29616;&#26377;&#21512;&#20316;MARL&#30340;&#24037;&#20316;&#65292;&#22312;&#19968;&#20010;&#36830;&#25509;&#30340;&#26102;&#21464;&#32593;&#32476;&#20013;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#30456;&#20114;&#20132;&#25442;&#20449;&#24687;&#20197;&#36798;&#25104;&#20849;&#35782;&#12290;&#25105;&#20204;&#22312;&#20849;&#35782;&#26356;&#26032;&#20013;&#24341;&#20837;&#28431;&#27934;&#65292;&#22312;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#20013;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#20559;&#31163;&#20854;&#24120;&#35268;&#30340;&#20849;&#35782;&#26356;&#26032;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#23545;&#25239;&#24615;&#26234;&#33021;&#20307;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20351;&#38750;&#23545;&#25239;&#24615;&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#21463;&#21040;&#32422;&#26463;&#26465;&#20214;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#23545;&#25239;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized multi-agent reinforcement learning (MARL) algorithms have become popular in the literature since it allows heterogeneous agents to have their own reward functions as opposed to canonical multi-agent Markov Decision Process (MDP) settings which assume common reward functions over all agents. In this work, we follow the existing work on collaborative MARL where agents in a connected time varying network can exchange information among each other in order to reach a consensus. We introduce vulnerabilities in the consensus updates of existing MARL algorithms where agents can deviate from their usual consensus update, who we term as adversarial agents. We then proceed to provide an algorithm that allows non-adversarial agents to reach a consensus in the presence of adversaries under a constrained setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#36816;&#21160;&#35266;&#27979;&#20013;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;PDE&#21160;&#21147;&#23398;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;"&#31070;&#32463;&#26412;&#26500;&#27861;"&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#31181;&#20005;&#26684;&#20445;&#35777;&#26631;&#20934;&#26412;&#26500;&#20808;&#39564;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#27492;&#26469;&#23398;&#20064;&#26412;&#26500;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14369</link><description>&lt;p&gt;
&#20174;&#36816;&#21160;&#35266;&#27979;&#20013;&#23398;&#20064;&#31070;&#32463;&#26412;&#26500;&#27861;&#26469;&#23454;&#29616;&#21487;&#25512;&#24191;&#30340;PDE&#21160;&#21147;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning Neural Constitutive Laws From Motion Observations for Generalizable PDE Dynamics. (arXiv:2304.14369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#36816;&#21160;&#35266;&#27979;&#20013;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;PDE&#21160;&#21147;&#23398;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;"&#31070;&#32463;&#26412;&#26500;&#27861;"&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#31181;&#20005;&#26684;&#20445;&#35777;&#26631;&#20934;&#26412;&#26500;&#20808;&#39564;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#27492;&#26469;&#23398;&#20064;&#26412;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;(NN)&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#26041;&#27861;,&#29992;&#20110;&#20174;&#36816;&#21160;&#35266;&#27979;&#20013;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;PDE&#21160;&#21147;&#23398;&#12290;&#35768;&#22810;NN&#26041;&#27861;&#23398;&#20064;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;,&#38544;&#21547;&#22320;&#24314;&#27169;&#20102;&#25152;&#24314;&#31435;&#30340;PDE&#21644;&#26412;&#26500;&#27169;&#22411;(&#25110;&#26448;&#26009;&#27169;&#22411;)&#12290;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#26126;&#30830;&#30340;PDE&#30693;&#35782;,&#19981;&#33021;&#20445;&#35777;&#29289;&#29702;&#27491;&#30830;&#24615;,&#20855;&#26377;&#26377;&#38480;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;,&#25152;&#24314;&#31435;&#30340;PDEs&#36890;&#24120;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;,&#24212;&#35813;&#26126;&#30830;&#24378;&#21046;&#25191;&#34892;,&#32780;&#19981;&#26159;&#23398;&#20064;&#12290;&#30456;&#21453;,&#26412;&#26500;&#27169;&#22411;&#30001;&#20110;&#20854;&#25968;&#25454;&#25311;&#21512;&#24615;&#36136;,&#29305;&#21035;&#36866;&#21512;&#20110;&#23398;&#20064;&#12290;&#20026;&#27492;,&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#31070;&#32463;&#26412;&#26500;&#27861;&#8221;(NCLaw)&#30340;&#26032;&#26694;&#26550;,&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#20005;&#26684;&#20445;&#35777;&#26631;&#20934;&#26412;&#26500;&#20808;&#39564;&#30340;&#32593;&#32476;&#26550;&#26500;,&#21253;&#25324;&#26059;&#36716;&#31561;&#21464;&#24615;&#21644;&#26410;&#21464;&#24418;&#29366;&#24577;&#24179;&#34913;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#32593;&#32476;&#23884;&#20837;&#21487;&#24494;&#20998;&#30340;&#27169;&#25311;&#20013;,&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;&#27169;&#25311;&#21644;m&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
We propose a hybrid neural network (NN) and PDE approach for learning generalizable PDE dynamics from motion observations. Many NN approaches learn an end-to-end model that implicitly models both the governing PDE and constitutive models (or material models). Without explicit PDE knowledge, these approaches cannot guarantee physical correctness and have limited generalizability. We argue that the governing PDEs are often well-known and should be explicitly enforced rather than learned. Instead, constitutive models are particularly suitable for learning due to their data-fitting nature. To this end, we introduce a new framework termed "Neural Constitutive Laws" (NCLaw), which utilizes a network architecture that strictly guarantees standard constitutive priors, including rotation equivariance and undeformed state equilibrium. We embed this network inside a differentiable simulation and train the model by minimizing a loss function based on the difference between the simulation and the m
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31995;&#21015;&#36890;&#36807;GPU-aware&#20248;&#21270;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#37197;&#22791;GPU&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#26497;&#24555;&#30340;&#25512;&#35770;&#24310;&#36831;&#65292;&#25193;&#22823;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#29992;&#33539;&#22260;&#24182;&#25913;&#21892;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.11267</link><description>&lt;p&gt;
&#36895;&#24230;&#21363;&#19968;&#20999;&#65306;&#36890;&#36807;GPU-aware&#20248;&#21270;&#22312;&#35774;&#22791;&#19978;&#21152;&#36895;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations. (arXiv:2304.11267v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11267
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31995;&#21015;&#36890;&#36807;GPU-aware&#20248;&#21270;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#37197;&#22791;GPU&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#26497;&#24555;&#30340;&#25512;&#35770;&#24310;&#36831;&#65292;&#25193;&#22823;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#29992;&#33539;&#22260;&#24182;&#25913;&#21892;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#24212;&#29992;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#21644;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#65292;&#27604;&#22914;&#38477;&#20302;&#26381;&#21153;&#22120;&#25104;&#26412;&#12289;&#31163;&#32447;&#21151;&#33021;&#21644;&#25913;&#21892;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#35774;&#22791;&#19978;&#20849;&#21516;&#30340;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#36229;&#36807;10&#20159;&#30340;&#21442;&#25968;&#65292;&#30001;&#20110;&#35774;&#22791;&#30340;&#21463;&#38480;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20248;&#21270;&#65292;&#20197;&#22312;&#37197;&#22791;GPU&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;&#36804;&#20170;&#20026;&#27490;&#26368;&#24555;&#30340;&#25512;&#35770;&#24310;&#36831;&#65288;&#23545;&#20110;&#19968;&#20010;512x512&#30340;&#22270;&#20687;&#65292;&#22312;&#19977;&#26143;S23 Ultra&#19978;&#30340;"&#31283;&#23450;&#25193;&#25955;1.4"&#19979;&#65292;&#36827;&#34892;20&#27425;&#36845;&#20195;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;int8&#37327;&#21270;&#65292;&#25512;&#35770;&#30340;&#24310;&#36831;&#23567;&#20110;12&#31186;&#65289;&#12290;&#36825;&#20123;&#20248;&#21270;&#25193;&#22823;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#29992;&#33539;&#22260;&#24182;&#25913;&#21892;&#20102;&#21508;&#31181;&#35774;&#22791;&#19978;&#30340;&#25972;&#20307;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development and application of foundation models have revolutionized the field of artificial intelligence. Large diffusion models have gained significant attention for their ability to generate photorealistic images and support various tasks. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. However, common large diffusion models have over 1 billion parameters and pose challenges due to restricted computational and memory resources on devices. We present a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date (under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile devices. These enhancements broaden the applicability of generative AI and improve the overall user experience across a wide range of devices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07142</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Data Sampling Strategies for Training Neural Network Speech Separation Models. (arXiv:2304.07142v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20998;&#31163;&#20173;&#28982;&#26159;&#22810;&#35828;&#35805;&#20449;&#21495;&#22788;&#29702;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#35821;&#38899;&#20998;&#31163;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#19968;&#20123;&#27169;&#22411;&#38656;&#35201;&#36739;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#36739;&#39640;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#32553;&#30701;&#35757;&#32451;&#31034;&#20363;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#36825;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24212;&#29992;&#36825;&#20123;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#65288;TSL&#65289;&#38480;&#21046;&#23545;&#20004;&#20010;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#65288;SepFormer&#65292;&#19968;&#20010;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21644;Conv-TasNet&#65292;&#19968;&#20010;&#21367;&#31215;&#27169;&#22411;&#65289;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;WJS0-2Mix&#65292;WHAMR&#21644;Libri2Mix&#25968;&#25454;&#38598;&#26469;&#20998;&#26512;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#21450;&#20854;&#23545;&#35757;&#32451;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20998;&#24067;&#65292;&#24212;&#29992;&#29305;&#23450;&#30340;TSL&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#23545;&#27874;&#24418;&#36215;&#22987;&#32034;&#24341;&#36827;&#34892;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#26356;&#22810;&#29420;&#29305;&#30340;&#31034;&#20363;&#29992;&#20110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech separation remains an important area of multi-speaker signal processing. Deep neural network (DNN) models have attained the best performance on many speech separation benchmarks. Some of these models can take significant time to train and have high memory requirements. Previous work has proposed shortening training examples to address these issues but the impact of this on model performance is not yet well understood. In this work, the impact of applying these training signal length (TSL) limits is analysed for two speech separation models: SepFormer, a transformer model, and Conv-TasNet, a convolutional model. The WJS0-2Mix, WHAMR and Libri2Mix datasets are analysed in terms of signal length distribution and its impact on training efficiency. It is demonstrated that, for specific distributions, applying specific TSL limits results in better performance. This is shown to be mainly due to randomly sampling the start index of the waveforms resulting in more unique examples for tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#27604;&#36739;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22788;&#29702;&#22823;&#37327;&#35838;&#31243;&#35780;&#35770;&#65292;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.03394</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#35838;&#31243;&#35780;&#35770;&#30340;&#35266;&#28857;&#25366;&#25496;&#21644;&#20027;&#39064;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Opinion Mining and Topic Classification of Course Reviews. (arXiv:2304.03394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#27604;&#36739;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22788;&#29702;&#22823;&#37327;&#35838;&#31243;&#35780;&#35770;&#65292;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25945;&#32946;&#24037;&#20316;&#32773;&#21644;&#31649;&#29702;&#32773;&#26469;&#35828;&#65292;&#23398;&#29983;&#23545;&#35838;&#31243;&#30340;&#21453;&#39304;&#24847;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#26080;&#35770;&#35838;&#31243;&#30340;&#31867;&#22411;&#25110;&#26426;&#26500;&#22914;&#20309;&#12290;&#22312;&#26426;&#26500;&#32423;&#21035;&#25110;&#22312;&#32447;&#35770;&#22363;&#19978;&#22788;&#29702;&#22823;&#37327;&#30340;&#24320;&#25918;&#21453;&#39304;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#20102;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#35838;&#31243;&#35780;&#35770;&#12290;&#25105;&#20204;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30446;&#30340;&#26159;&#20102;&#35299;&#23398;&#29983;&#30340;&#24773;&#24863;&#21644;&#20027;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22914;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;BERT&#65288;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#65289;&#12289;RoBERTa&#65288;&#32463;&#36807;&#20248;&#21270;&#30340;BERT&#26041;&#27861;&#65289;&#21644;XLNet&#65288;&#24191;&#20041;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#36825;&#20123;&#25216;&#26415;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#24046;&#24322;&#12290;&#36825;&#39033;&#27604;&#36739;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Student opinions for a course are important to educators and administrators, regardless of the type of the course or the institution. Reading and manually analyzing open-ended feedback becomes infeasible for massive volumes of comments at institution level or online forums. In this paper, we collected and pre-processed a large number of course reviews publicly available online. We applied machine learning techniques with the goal to gain insight into student sentiments and topics. Specifically, we utilized current Natural Language Processing (NLP) techniques, such as word embeddings and deep neural networks, and state-of-the-art BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet (Generalized Auto-regression Pre-training). We performed extensive experimentation to compare these techniques versus traditional approaches. This comparative study demonstrates how to apply modern machine learning approaches for sentiment polari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#35270;&#39057;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#26041;&#27861;S$^2$VS&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#23454;&#20363;&#21306;&#20998;&#35299;&#20915;&#22810;&#20010;&#26816;&#32034;&#21644;&#26816;&#27979;&#20219;&#21153;&#65292;&#26080;&#38656;&#29992;&#21040;&#26631;&#27880;&#25968;&#25454;&#65292;&#24182;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03378</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#39057;&#30456;&#20284;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Video Similarity Learning. (arXiv:2304.03378v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#35270;&#39057;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#26041;&#27861;S$^2$VS&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#23454;&#20363;&#21306;&#20998;&#35299;&#20915;&#22810;&#20010;&#26816;&#32034;&#21644;&#26816;&#27979;&#20219;&#21153;&#65292;&#26080;&#38656;&#29992;&#21040;&#26631;&#27880;&#25968;&#25454;&#65292;&#24182;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#39057;&#30456;&#20284;&#24615;&#23398;&#20064;&#26041;&#27861;S$^2$VS&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#23454;&#29616;&#35270;&#39057;&#30456;&#20284;&#24615;&#23398;&#20064;&#65292;&#24182;&#19968;&#27425;&#24615;&#35299;&#20915;&#22810;&#20010;&#26816;&#32034;&#21644;&#26816;&#27979;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#23450;&#21046;&#30340;&#22686;&#24378;&#21644;InfoNCE&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#22312;&#33258;&#25105;&#30456;&#20284;&#24615;&#21644;&#30828;&#36127;&#30456;&#20284;&#24615;&#19978;&#21516;&#26102;&#25805;&#20316;&#30340;&#38468;&#21152;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23398;&#20064;&#23454;&#20363;&#21306;&#20998;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31890;&#24230;&#19979;&#23450;&#20041;&#35270;&#39057;&#30456;&#20851;&#24615;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#22797;&#21046;&#35270;&#39057;&#21040;&#25551;&#36848;&#30456;&#21516;&#20107;&#20214;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#26032;&#30340;&#34920;&#29616;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce S$^2$VS, a video similarity learning approach with self-supervision. Self-Supervised Learning (SSL) is typically used to train deep models on a proxy task so as to have strong transferability on target tasks after fine-tuning. Here, in contrast to prior work, SSL is used to perform video similarity learning and address multiple retrieval and detection tasks at once with no use of labeled data. This is achieved by learning via instance-discrimination with task-tailored augmentations and the widely used InfoNCE loss together with an additional loss operating jointly on self-similarity and hard-negative similarity. We benchmark our method on tasks where video relevance is defined with varying granularity, ranging from video copies to videos depicting the same incident or event. We learn a single universal model that achieves state-of-the-art performance on all tasks, surpassing previously proposed methods that use labeled data. The code and pretrained models are publicly avai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21028;&#26029;&#36136;&#25968;&#25972;&#38500;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20851;&#38190;&#22312;&#20110;&#25552;&#20379;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#21830;&#19994;&#21487;&#29992;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26080;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#38656;&#35201;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.01333</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21028;&#26029;&#36136;&#25968;&#25972;&#38500;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Prime Number Divisibility by Deep Learning. (arXiv:2304.01333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21028;&#26029;&#36136;&#25968;&#25972;&#38500;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20851;&#38190;&#22312;&#20110;&#25552;&#20379;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#21830;&#19994;&#21487;&#29992;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26080;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#38656;&#35201;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30830;&#23450;&#19968;&#20010;&#25972;&#25968;&#26159;&#21542;&#33021;&#22815;&#34987;2&#12289;3&#25110;&#20854;&#20182;&#36136;&#25968;&#25972;&#38500;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#21487;&#33021;&#24456;&#31616;&#21333;&#65292;&#20294;&#22312;&#27809;&#26377;&#39044;&#20808;&#25351;&#23450;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#23545;&#20110;&#35745;&#31639;&#26426;&#26469;&#35828;&#21487;&#33021;&#24182;&#19981;&#23481;&#26131;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#21644;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#30830;&#23450;&#22823;&#26377;&#38480;&#25972;&#25968;&#65288;&#39640;&#36798;$2^{32}$&#65289;&#26159;&#21542;&#33021;&#22815;&#34987;&#23567;&#36136;&#25968;&#25972;&#38500;&#30340;&#24773;&#20917;&#19979;&#65292;&#21508;&#31181;&#26694;&#26550;&#21644;&#32593;&#32476;&#32467;&#26500;&#65288;CNN&#12289;RNN&#12289;Transformer&#31561;&#65289;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#27979;&#36136;&#25968;&#25972;&#38500;&#24615;&#30340;&#33021;&#21147;&#26497;&#22823;&#22320;&#21462;&#20915;&#20110;&#25552;&#20379;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#32593;&#32476;&#26694;&#26550;&#25110;&#32593;&#32476;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65288;CNN&#12289;RNN&#12289;Transformer&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#26469;&#33258;&#20122;&#39532;&#36874;&#12289;&#35895;&#27468;&#21644;&#24494;&#36719;&#30340;&#21830;&#19994;&#21487;&#29992;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#31649;&#36947;&#65292;&#24182;&#35777;&#26126;&#38500;&#38750;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#24037;&#31243;&#65292;&#21542;&#21017;&#23427;&#20204;&#26080;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certain tasks such as determining whether a given integer can be divided by 2, 3, or other prime numbers may be trivial for human beings, but can be less straightforward for computers in the absence of pre-specified algorithms. In this paper, we tested multiple deep learning architectures and feature engineering approaches, and evaluated the scenario of determining divisibility of large finite integers (up to $2^{32}$) by small prime numbers. It turns out that, regardless of the network frameworks or the complexity of the network structures (CNN, RNN, Transformer, etc.), the ability to predict the prime number divisibility critically depends on the feature space fed into the deep learning models. We also evaluated commercially available Automated Machine Learning (AutoML) pipelines from Amazon, Google and Microsoft, and demonstrated that they failed to address this issue unless appropriately engineered features were provided. We further proposed a closed form solution to the problem us
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;36&#31687;&#25991;&#31456;&#30340;&#32508;&#36848;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#21463;&#21040;&#23567;&#26679;&#26412;&#37327;&#65292;&#28508;&#22312;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.18005</link><description>&lt;p&gt;
&#21365;&#24034;&#30284;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence in Ovarian Cancer Histopathology: A Systematic Review. (arXiv:2303.18005v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;36&#31687;&#25991;&#31456;&#30340;&#32508;&#36848;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#21463;&#21040;&#23567;&#26679;&#26412;&#37327;&#65292;&#28508;&#22312;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;-&#29305;&#24449;&#21270;&#21644;&#35780;&#20272;&#24050;&#21457;&#34920;&#30340;&#30740;&#31350;&#65292;&#35780;&#20272;&#21033;&#29992;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#36827;&#34892;&#21365;&#24034;&#30284;&#35786;&#26029;&#25110;&#39044;&#21518;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#30340;&#36136;&#37327;&#12290;&#26041;&#27861;-&#22312;2022&#24180;1&#26376;12&#26085;&#20043;&#21069;&#65292;&#23545;5&#20010;&#26469;&#28304;&#36827;&#34892;&#25628;&#32034;&#12290;&#21253;&#25324;&#26631;&#20934;&#35201;&#27714;&#30740;&#31350;&#35780;&#20272;AI&#22312;&#21365;&#24034;&#30284;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#19978;&#65292;&#23545;&#21365;&#24034;&#30284;&#65292;&#21253;&#25324;&#36755;&#21365;&#31649;&#21365;&#24034;&#21644;&#33145;&#33180;&#32959;&#30244;&#30340;&#35786;&#26029;&#25110;&#39044;&#21518;&#25512;&#26029;&#12290;&#25490;&#38500;&#35780;&#35770;&#21644;&#38750;&#33521;&#35821;&#25991;&#31456;&#12290;&#23545;&#27599;&#20010;&#21253;&#21547;&#30340;&#27169;&#22411;&#20351;&#29992;PROBAST&#35780;&#20272;&#20559;&#20506;&#39118;&#38505;&#12290;&#32467;&#26524;-&#20849;&#21457;&#29616;1434&#31687;&#30740;&#31350;&#25991;&#31456;&#65292;&#20854;&#20013;36&#31687;&#31526;&#21512;&#32435;&#20837;&#26631;&#20934;&#12290;&#36825;&#20123;&#30740;&#31350;&#25253;&#21578;&#20102;62&#20010;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;35&#20010;&#20998;&#31867;&#22120;&#65292;14&#20010;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;7&#20010;&#20998;&#21106;&#27169;&#22411;&#21644;6&#20010;&#22238;&#24402;&#27169;&#22411;&#12290;&#20351;&#29992;1-1375&#24352;&#20174;1-664&#20010;&#21365;&#24034;&#30284;&#24739;&#32773;&#20013;&#24471;&#21040;&#30340;&#24187;&#28783;&#29255;&#24320;&#21457;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;&#39044;&#27979;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#24635;&#20307;&#29983;&#23384;&#65288;9/62&#65289;&#65292;&#32452;&#32455;&#23398;&#20122;&#22411;&#65288;7/62&#65289;&#21644;&#28107;&#24052;&#32467;&#29366;&#24577;&#65288;6/62&#65289;&#12290;&#32467;&#35770;-&#22522;&#20110;&#21487;&#29992;&#30340;&#25991;&#29486;&#65292;AI&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#21463;&#21040;&#26679;&#26412;&#37327;&#23567;&#12289;&#28508;&#22312;&#30340;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose - To characterise and assess the quality of published research evaluating artificial intelligence (AI) methods for ovarian cancer diagnosis or prognosis using histopathology data. Methods - A search of 5 sources was conducted up to 01/12/2022. The inclusion criteria required that research evaluated AI on histopathology images for diagnostic or prognostic inferences in ovarian cancer, including tubo-ovarian and peritoneal tumours. Reviews and non-English language articles were excluded. The risk of bias was assessed for every included model using PROBAST. Results - A total of 1434 research articles were identified, of which 36 were eligible for inclusion. These studies reported 62 models of interest, including 35 classifiers, 14 survival prediction models, 7 segmentation models, and 6 regression models. Models were developed using 1-1375 slides from 1-664 ovarian cancer patients. A wide array of outcomes were predicted, including overall survival (9/62), histological subtypes (7
&lt;/p&gt;</description></item><item><title>&#36845;&#20195;&#37096;&#20998;&#28385;&#36275;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23646;&#24615;&#65306;&#36825;&#31181;&#35299;&#37322;&#22312;&#36845;&#20195;&#37096;&#20998;&#23653;&#34892;&#19979;&#30340;&#34892;&#20026;&#12290;&#20027;&#20307;&#21487;&#20197;&#22312;&#25509;&#25910;&#30340;&#35299;&#37322;&#20013;&#37096;&#20998;&#22320;&#23653;&#34892;&#35831;&#27714;&#19968;&#20010;&#26032;&#30340;&#39044;&#27979;&#21644;&#26032;&#30340;&#35299;&#37322;&#65292;&#37325;&#22797;&#27492;&#36807;&#31243;&#65292;&#30452;&#21040;&#39044;&#27979;&#20026;&#27491;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.11111</link><description>&lt;p&gt;
&#36845;&#20195;&#37096;&#20998;&#28385;&#36275;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65306;&#30410;&#22788;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Iterative Partial Fulfillment of Counterfactual Explanations: Benefits and Risks. (arXiv:2303.11111v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11111
&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#37096;&#20998;&#28385;&#36275;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23646;&#24615;&#65306;&#36825;&#31181;&#35299;&#37322;&#22312;&#36845;&#20195;&#37096;&#20998;&#23653;&#34892;&#19979;&#30340;&#34892;&#20026;&#12290;&#20027;&#20307;&#21487;&#20197;&#22312;&#25509;&#25910;&#30340;&#35299;&#37322;&#20013;&#37096;&#20998;&#22320;&#23653;&#34892;&#35831;&#27714;&#19968;&#20010;&#26032;&#30340;&#39044;&#27979;&#21644;&#26032;&#30340;&#35299;&#37322;&#65292;&#37325;&#22797;&#27492;&#36807;&#31243;&#65292;&#30452;&#21040;&#39044;&#27979;&#20026;&#27491;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#65288;CF&#65289;&#35299;&#37322;&#65292;&#20063;&#31216;&#20026;&#23545;&#27604;&#35299;&#37322;&#21644;&#31639;&#27861;&#22238;&#24402;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#38024;&#23545;&#19968;&#20010;&#21463;&#21040;&#36127;&#38754;&#39044;&#27979;&#30340;&#20027;&#20307;&#65288;&#20363;&#22914;&#65292;&#25298;&#32477;&#25151;&#36151;&#30003;&#35831;&#65289;&#65292;CF&#35299;&#37322;&#26159;&#30456;&#20284;&#30340;&#24773;&#20917;&#65292;&#20294;&#39044;&#27979;&#32467;&#26524;&#20026;&#27491;&#38754;&#65292;&#36825;&#21578;&#30693;&#20027;&#20307;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#21508;&#31181;&#23646;&#24615;&#24050;&#32463;&#34987;&#30740;&#31350;&#65292;&#20363;&#22914;&#26377;&#25928;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23646;&#24615;&#65306;&#22312;&#36845;&#20195;&#37096;&#20998;&#23653;&#34892;&#65288;IPF&#65289;&#19979;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#25509;&#25910;&#21040;CF&#35299;&#37322;&#21518;&#65292;&#20027;&#20307;&#21487;&#33021;&#21482;&#33021;&#37096;&#20998;&#22320;&#23653;&#34892;&#23427;&#65292;&#28982;&#21518;&#35831;&#27714;&#19968;&#20010;&#26032;&#30340;&#39044;&#27979;&#21450;&#26032;&#30340;&#35299;&#37322;&#65292;&#37325;&#22797;&#27492;&#36807;&#31243;&#30452;&#21040;&#39044;&#27979;&#20026;&#27491;&#38754;&#12290;&#36825;&#31181;&#37096;&#20998;&#23653;&#34892;&#21487;&#33021;&#26159;&#30001;&#20110;&#20027;&#20307;&#30340;&#33021;&#21147;&#26377;&#38480;&#65288;&#20363;&#22914;&#65292;&#27492;&#26102;&#21482;&#33021;&#25903;&#20184;&#22235;&#24352;&#20449;&#29992;&#21345;&#24080;&#25143;&#20013;&#30340;&#20004;&#24352;&#65289;&#25110;&#35797;&#22270;&#20882;&#38505;&#65288;&#20363;&#22914;&#65292;&#25276;&#27880;800&#32654;&#20803;&#30340;&#26376;&#34218;&#22686;&#38271;&#36275;&#22815;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual (CF) explanations, also known as contrastive explanations and algorithmic recourses, are popular for explaining machine learning models in high-stakes domains. For a subject that receives a negative model prediction (e.g., mortgage application denial), the CF explanations are similar instances but with positive predictions, which informs the subject of ways to improve. While their various properties have been studied, such as validity and stability, we contribute a novel one: their behaviors under iterative partial fulfillment (IPF). Specifically, upon receiving a CF explanation, the subject may only partially fulfill it before requesting a new prediction with a new explanation, and repeat until the prediction is positive. Such partial fulfillment could be due to the subject's limited capability (e.g., can only pay down two out of four credit card accounts at this moment) or an attempt to take the chance (e.g., betting that a monthly salary increase of $800 is enough eve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29702;&#35299;&#25551;&#36848;&#20219;&#21153;&#30340;&#25991;&#26412;&#20449;&#24687;&#26469;&#36866;&#24212;&#25512;&#29702;&#26102;&#30340;&#26032;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#21644;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#38646;&#25968;&#25454;&#38382;&#39064;&#19978;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.03363</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#22686;&#24378;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language. (arXiv:2303.03363v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29702;&#35299;&#25551;&#36848;&#20219;&#21153;&#30340;&#25991;&#26412;&#20449;&#24687;&#26469;&#36866;&#24212;&#25512;&#29702;&#26102;&#30340;&#26032;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#21644;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#38646;&#25968;&#25454;&#38382;&#39064;&#19978;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#24615;&#21644;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#26159;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#26680;&#24515;&#24037;&#20316;&#65292;&#20294;&#30446;&#21069;&#23427;&#20204;&#24517;&#39035;&#32463;&#36807;&#35757;&#32451;&#25110;&#24494;&#35843;&#25165;&#33021;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#31185;&#23398;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#38646;&#25968;&#25454;&#21644;&#23569;&#25968;&#25454;&#26679;&#26412;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#23545;&#20110;&#27492;&#31867;&#20302;&#25968;&#25454;&#20219;&#21153;&#65292;&#26080;&#38656;&#35757;&#32451;&#25110;&#24494;&#35843;&#21363;&#21487;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27963;&#24615;&#39044;&#27979;&#26041;&#38754;&#30340;&#39044;&#27979;&#36136;&#37327;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29702;&#35299;&#25551;&#36848;&#20219;&#21153;&#30340;&#25991;&#26412;&#20449;&#24687;&#26469;&#36866;&#24212;&#25512;&#29702;&#26102;&#30340;&#26032;&#39044;&#27979;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#20998;&#31163;&#27169;&#22359;&#65292;&#20197;&#21450;&#22823;&#22411;&#29983;&#29289;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;CLAMP&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#21644;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#38646;&#25968;&#25454;&#38382;&#39064;&#19978;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36827;&#23637;&#24402;&#22240;&#20110;&#24773;&#22659;&#24863;&#30693;&#27169;&#22411;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#30340;&#32467;&#21512;&#65292;&#20197;&#21450;&#29992;&#20110;&#32467;&#21512;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#24615;&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activity and property prediction models are the central workhorses in drug discovery and materials sciences, but currently they have to be trained or fine-tuned for new tasks. Without training or fine-tuning, scientific language models could be used for such low-data tasks through their announced zero- and few-shot capabilities. However, their predictive quality at activity prediction is lacking. In this work, we envision a novel type of activity prediction model that is able to adapt to new prediction tasks at inference time, via understanding textual information describing the task. To this end, we propose a new architecture with separate modules for chemical and natural language inputs, and a contrastive pre-training objective on data from large biochemical databases. In extensive experiments, we show that our method CLAMP yields improved predictive performance on few-shot learning benchmarks and zero-shot problems in drug discovery. We attribute the advances of our method to the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;Seq-HyGAN&#65292;&#36890;&#36807;&#21019;&#24314;&#36229;&#22270;&#21644;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.02393</link><description>&lt;p&gt;
Seq-HyGAN: &#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Seq-HyGAN: Sequence Classification via Hypergraph Attention Network. (arXiv:2303.02393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;Seq-HyGAN&#65292;&#36890;&#36807;&#21019;&#24314;&#36229;&#22270;&#21644;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#20998;&#31867;&#22312;&#19981;&#21516;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#22522;&#22240;&#32452;&#20998;&#31867;&#21644;&#22312;&#21830;&#19994;&#39046;&#22495;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#24207;&#21015;&#25968;&#25454;&#20013;&#32570;&#20047;&#26174;&#24335;&#30340;&#29305;&#24449;&#65292;&#36825;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20165;&#38480;&#20110;&#25429;&#33719;&#30456;&#37051;&#32467;&#26500;&#36830;&#25509;&#24182;&#24573;&#30053;&#24207;&#21015;&#20043;&#38388;&#30340;&#20840;&#23616;&#12289;&#39640;&#38454;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;Seq-HyGAN&#12290;&#20026;&#20102;&#25429;&#25417;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#19968;&#20010;&#36229;&#22270;&#65292;&#20854;&#20013;&#24207;&#21015;&#34987;&#25551;&#32472;&#20026;&#36229;&#36793;&#65292;&#20174;&#24207;&#21015;&#20013;&#25552;&#21462;&#30340;&#23376;&#24207;&#21015;&#34987;&#25551;&#32472;&#20026;&#33410;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#21452;&#23618;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#35813;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#24207;&#21015;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Sequence classification has a wide range of real-world applications in different domains, such as genome classification in health and anomaly detection in business. However, the lack of explicit features in sequence data makes it difficult for machine learning models. While Neural Network (NN) models address this with learning features automatically, they are limited to capturing adjacent structural connections and ignore global, higher-order information between the sequences. To address these challenges in the sequence classification problems, we propose a novel Hypergraph Attention Network model, namely Seq-HyGAN. To capture the complex structural similarity between sequence data, we first create a hypergraph where the sequences are depicted as hyperedges and subsequences extracted from sequences are depicted as nodes. Additionally, we introduce an attention-based Hypergraph Neural Network model that utilizes a two-level attention mechanism. This model generates a sequence representa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#35299;&#32806;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#37492;&#21035;&#24615;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#34920;&#24449;&#35299;&#32806;&#65292;&#35299;&#20915;&#20102;&#29616;&#20195;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#32416;&#32544;&#20559;&#24046;&#34892;&#20026;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.00128</link><description>&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;&#24615;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#34920;&#24449;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Representation Disentaglement via Regularization by Identification. (arXiv:2303.00128v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#35299;&#32806;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#37492;&#21035;&#24615;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#34920;&#24449;&#35299;&#32806;&#65292;&#35299;&#20915;&#20102;&#29616;&#20195;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#32416;&#32544;&#20559;&#24046;&#34892;&#20026;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#35299;&#32806;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#20174;$p(\mathbf{x}|\mathbf{y})$&#20013;&#29983;&#25104;&#30340;&#20855;&#26377;&#21508;&#33258;&#29983;&#25104;&#21464;&#37327;$\mathbf{y}_c$&#20998;&#35299;&#30340;&#20998;&#24067;$p(\mathbf{y}) = \prod_{c} p(\mathbf{y}_c )$&#30340;&#35266;&#27979;&#20540;${\mathbf{x}^{(i)}}$&#65292;&#25105;&#20204;&#23581;&#35797;&#23398;&#20064;&#19982;&#27599;&#20010;$c$&#30340;&#21518;&#39564;&#20998;&#24067;$p(\mathbf{z}| \mathbf{x}, \hat{\mathbf{y}}_c)$&#21305;&#37197;&#30340;&#35299;&#32806;&#34920;&#31034;&#26159;&#21542;&#21487;&#34892;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#20195;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#19982;&#29983;&#25104;&#21464;&#37327;&#20043;&#38388;&#20986;&#29616;&#30340;&#32416;&#32544;&#20559;&#24046;&#34892;&#20026;&#38382;&#39064;&#65292;&#36825;&#31181;&#34892;&#20026;&#19978;&#20135;&#29983;&#20559;&#35265;&#12290;&#22312;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#21487;&#35782;&#21035;&#24615;&#30340;&#26465;&#20214;&#19979;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#35299;&#37322;&#21644;&#35843;&#21644;&#65292;&#36825;&#19968;&#28857;&#21487;&#20197;&#22312;&#30417;&#30563;&#25110;&#24369;&#30417;&#30563;&#30340;&#26465;&#20214;&#19979;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37492;&#21035;&#24615;&#27491;&#21017;&#21270;&#65288;ReI&#65289;&#30340;&#27169;&#22359;&#21270;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on the problem of learning disentangled representations from observational data. Given observations ${\mathbf{x}^{(i)}}$ for $i=1,...,N $ drawn from $p(\mathbf{x}|\mathbf{y})$ with generative variables $\mathbf{y}$ admitting the distribution factorization $p(\mathbf{y}) = \prod_{c} p(\mathbf{y}_c )$, we ask whether learning disentangled representations matching the space of observations with identification guarantees on the posterior $p(\mathbf{z}| \mathbf{x}, \hat{\mathbf{y}}_c)$ for each $c$, is plausible. We argue modern deep representation learning models of data matching the distributed factorization property are ill-posed with collider bias behaviour; a source of bias producing entanglement between generating variables. Under the rubric of causality, we show this issue can be explained and reconciled under the condition of identifiability; attainable under supervision or a weak-form of it. For this, we propose regularization by identification (ReI), a modular re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19982;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#29305;&#23450;&#32593;&#32476;&#20013;&#34920;&#29616;&#20026;&#21333;&#27425;&#37325;&#25490;&#19982;&#38543;&#26426;&#37325;&#25490;&#25910;&#25947;&#21040;&#19981;&#21516;&#30340;&#25197;&#26354;&#20840;&#23616;&#26368;&#20248;&#28857;&#65292;&#24314;&#35758;&#20351;&#29992;&#38543;&#26426;&#37325;&#25490;&#12290;</title><link>http://arxiv.org/abs/2302.12444</link><description>&lt;p&gt;
&#20851;&#20110;&#24102;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Training Instability of Shuffling SGD with Batch Normalization. (arXiv:2302.12444v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19982;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#29305;&#23450;&#32593;&#32476;&#20013;&#34920;&#29616;&#20026;&#21333;&#27425;&#37325;&#25490;&#19982;&#38543;&#26426;&#37325;&#25490;&#25910;&#25947;&#21040;&#19981;&#21516;&#30340;&#25197;&#26354;&#20840;&#23616;&#26368;&#20248;&#28857;&#65292;&#24314;&#35758;&#20351;&#29992;&#38543;&#26426;&#37325;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19982;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21457;&#29616;&#24120;&#29992;&#30340;&#21333;&#27425;&#37325;&#25490;&#21644;&#38543;&#26426;&#37325;&#25490;&#36825;&#20004;&#31181; SGD &#26041;&#24335;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#24046;&#24322;&#38750;&#24120;&#22823;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#32447;&#24615;&#32593;&#32476;&#22238;&#24402;&#38382;&#39064;&#20013;&#65292;&#21333;&#27425;&#37325;&#25490;&#21644;&#38543;&#26426;&#37325;&#25490;&#20250;&#20998;&#21035;&#25910;&#25947;&#21040;&#25197;&#26354;&#30340;&#20840;&#23616;&#26368;&#20248;&#28857;&#65292;&#32780;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#31350;&#20102;&#23427;&#20204;&#30340;&#35757;&#32451;&#26159;&#21542;&#20250;&#21457;&#25955;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#35777;&#39564;&#35777;&#32467;&#26524;&#65292;&#24314;&#35758;&#22312;&#20351;&#29992; SGD &#19982;&#25209;&#37327;&#24402;&#19968;&#21270;&#26102;&#65292;&#20248;&#20808;&#32771;&#34385;&#38543;&#26426;&#37325;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;
We uncover how SGD interacts with batch normalization and can exhibit undesirable training dynamics such as divergence. More precisely, we study how Single Shuffle (SS) and Random Reshuffle (RR) -- two widely used variants of SGD -- interact surprisingly differently in the presence of batch normalization: RR leads to much more stable evolution of training loss than SS. As a concrete example, for regression using a linear network with batch normalization, we prove that SS and RR converge to distinct global optima that are "distorted" away from gradient descent. Thereafter, for classification we characterize conditions under which training divergence for SS and RR can, and cannot occur. We present explicit constructions to show how SS leads to distorted optima in regression and divergence for classification, whereas RR avoids both distortion and divergence. We validate our results by confirming them empirically in realistic settings, and conclude that the separation between SS and RR use
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DPM-SNC&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#21644;&#27969;&#24418;&#32422;&#26463;&#30340;&#37319;&#26679;&#26041;&#27861;&#23454;&#29616;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#33410;&#28857;&#20998;&#31867;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#26469;&#24212;&#29992;DPMs&#65292;&#26368;&#22823;&#21270;&#19968;&#20010;&#26032;&#30340;&#21464;&#20998;&#19979;&#30028;&#12290;&#23454;&#39564;&#35777;&#26126;DPMs&#21487;&#20197;&#25552;&#39640;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.10506</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#33410;&#28857;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probabilistic Models for Structured Node Classification. (arXiv:2302.10506v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DPM-SNC&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#21644;&#27969;&#24418;&#32422;&#26463;&#30340;&#37319;&#26679;&#26041;&#27861;&#23454;&#29616;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#33410;&#28857;&#20998;&#31867;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#26469;&#24212;&#29992;DPMs&#65292;&#26368;&#22823;&#21270;&#19968;&#20010;&#26032;&#30340;&#21464;&#20998;&#19979;&#30028;&#12290;&#23454;&#39564;&#35777;&#26126;DPMs&#21487;&#20197;&#25552;&#39640;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#33410;&#28857;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#20381;&#36182;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#38598;&#20013;&#30740;&#31350;&#20102;&#37096;&#20998;&#26631;&#35760;&#22270;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#38656;&#35201;&#23558;&#24050;&#30693;&#26631;&#35760;&#30340;&#20449;&#24687;&#32435;&#20837;&#26410;&#30693;&#26631;&#31614;&#30340;&#39044;&#27979;&#20013;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#32467;&#26500;&#21270;&#33410;&#28857;&#20998;&#31867;(DPM-SNC)&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;DPM-SNC&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#21363;(a)&#20351;&#29992;&#20855;&#26377;&#34920;&#36798;&#24615;&#30340;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#23398;&#20064;&#26631;&#31614;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;(b)&#21033;&#29992;&#27969;&#24418;&#32422;&#26463;&#30340;&#37319;&#26679;&#26041;&#27861;&#22312;&#24050;&#30693;&#26631;&#31614;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#30001;&#20110;DPMs&#32570;&#20047;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#22240;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#24212;&#29992;DPMs&#65292;&#26368;&#22823;&#21270;&#19968;&#20010;&#26032;&#30340;&#21464;&#20998;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#25552;&#39640;DPMs&#23545;&#33410;&#28857;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies structured node classification on graphs, where the predictions should consider dependencies between the node labels. In particular, we focus on solving the problem for partially labeled graphs where it is essential to incorporate the information in the known label for predicting the unknown labels. To address this issue, we propose a novel framework leveraging the diffusion probabilistic model for structured node classification (DPM-SNC). At the heart of our framework is the extraordinary capability of DPM-SNC to (a) learn a joint distribution over the labels with an expressive reverse diffusion process and (b) make predictions conditioned on the known labels utilizing manifold-constrained sampling. Since the DPMs lack training algorithms for partially labeled data, we design a novel training algorithm to apply DPMs, maximizing a new variational lower bound. We also theoretically analyze how DPMs benefit node classification by enhancing the expressive power of GNNs 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20004;&#36793;&#24066;&#22330;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#22240;&#32032;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#32447;&#24615;&#35268;&#21010;&#30340;&#27169;&#22411;&#26469;&#20248;&#21270;&#20844;&#24179;&#30340;&#20998;&#37197;</title><link>http://arxiv.org/abs/2302.03810</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#26465;&#20214;&#19979;&#30340;&#21305;&#37197;&#20844;&#24179;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fairness in Matching under Uncertainty. (arXiv:2302.03810v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03810
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20004;&#36793;&#24066;&#22330;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#22240;&#32032;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#32447;&#24615;&#35268;&#21010;&#30340;&#27169;&#22411;&#26469;&#20248;&#21270;&#20844;&#24179;&#30340;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#22312;&#23558;&#23398;&#29983;&#20998;&#37197;&#21040;&#23398;&#26657;&#12289;&#29992;&#25143;&#20998;&#37197;&#32473;&#24191;&#21578;&#21830;&#20197;&#21450;&#24212;&#32856;&#32773;&#23433;&#25490;&#24037;&#20316;&#38754;&#35797;&#26102;&#34987;&#20351;&#29992;&#65292;&#22914;&#20170;&#36825;&#31181;&#31639;&#27861;&#20004;&#36793;&#30340;&#24066;&#22330;&#26222;&#21450;&#31243;&#24230;&#36234;&#26469;&#36234;&#39640;&#65292;&#36825;&#20063;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#24066;&#22330;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#31181;&#24773;&#22659;&#19979;&#65292;&#21305;&#37197;&#30340;&#20915;&#31574;&#24212;&#35813;&#23562;&#37325;&#34987;&#21305;&#37197;&#20010;&#20307;&#30340;&#20559;&#22909;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#30340;&#21069;&#25552;&#19979;&#32771;&#34385;&#20010;&#20307;&#30340;&#21697;&#36136;&#20197;&#21450;&#26410;&#26469;&#34920;&#29616;&#12290;&#30001;&#20110;&#36825;&#20123;&#21697;&#36136;&#24448;&#24448;&#26159;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20174;&#21487;&#35266;&#23519;&#29305;&#24449;&#20013;&#25512;&#26029;&#20986;&#26469;&#30340;&#65292;&#25152;&#20197;&#20854;&#21028;&#23450;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#65292;&#22312;&#20004;&#36793;&#24066;&#22330;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#23545;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#36827;&#34892;&#20102;&#20005;&#35880;&#30340;&#20844;&#29702;&#21270;&#38416;&#36848;&#65292;&#20174;&#32780;&#32771;&#34385;&#21040;&#20102;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#23545;&#20844;&#24179;&#24615;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#20063;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23581;&#35797;&#35299;&#20915;&#20102;&#36825;&#20123;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#26469;&#23454;&#29616;&#23545;&#20998;&#37197;&#30340;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#26368;&#22823;&#21270;&#30340;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence and importance of algorithmic two-sided marketplaces has drawn attention to the issue of fairness in such settings. Algorithmic decisions are used in assigning students to schools, users to advertisers, and applicants to job interviews. These decisions should heed the preferences of individuals, and simultaneously be fair with respect to their merits (synonymous with fit, future performance, or need). Merits conditioned on observable features are always \emph{uncertain}, a fact that is exacerbated by the widespread use of machine learning algorithms to infer merit from the observables. As our key contribution, we carefully axiomatize a notion of individual fairness in the two-sided marketplace setting which respects the uncertainty in the merits; indeed, it simultaneously recognizes uncertainty as the primary potential cause of unfairness and an approach to address it. We design a linear programming framework to find fair utility-maximizing distributions over allocations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#24110;&#21161;&#31616;&#21270;DNS&#23457;&#26597;&#26816;&#27979;&#36807;&#31243;&#65292;&#25552;&#39640;&#26816;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#21551;&#21457;&#24335;&#26041;&#27861;&#25152;&#38169;&#36807;&#30340;&#26032;&#23457;&#26597;&#23454;&#20363;&#21644;&#38459;&#27490;&#26631;&#24535;&#12290;</title><link>http://arxiv.org/abs/2302.02031</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#35268;&#21017;&#22411;&#22495;&#21517;&#31995;&#32479;DNS&#23457;&#26597;&#26816;&#27979;&#30340;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Augmenting Rule-based DNS Censorship Detection at Scale with Machine Learning. (arXiv:2302.02031v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#24110;&#21161;&#31616;&#21270;DNS&#23457;&#26597;&#26816;&#27979;&#36807;&#31243;&#65292;&#25552;&#39640;&#26816;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#21551;&#21457;&#24335;&#26041;&#27861;&#25152;&#38169;&#36807;&#30340;&#26032;&#23457;&#26597;&#23454;&#20363;&#21644;&#38459;&#27490;&#26631;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23457;&#26597;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#22823;&#37327;&#30417;&#27979;&#21644;&#26333;&#20809;&#30340;&#27979;&#37327;&#24179;&#21488;&#30340;&#21457;&#23637;&#12290;&#22495;&#21517;&#31995;&#32479;&#65288;DNS&#65289;&#30340;&#23457;&#26597;&#26159;&#19981;&#21516;&#22269;&#23478;&#20351;&#29992;&#30340;&#20851;&#38190;&#26426;&#21046;&#65292;&#30446;&#21069;&#36890;&#36807;&#23545;&#29305;&#23450;&#30446;&#30340;&#22320;&#30340;DNS&#26597;&#35810;&#21644;&#21709;&#24212;&#65288;&#25506;&#38024;&#65289;&#26679;&#26412;&#24212;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#26082;&#19982;&#24179;&#21488;&#29305;&#23450;&#30456;&#20851;&#65292;&#20063;&#21457;&#29616;&#24403;&#23457;&#26597;&#32773;&#25913;&#21464;&#20854;&#38459;&#27490;&#34892;&#20026;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#38656;&#35201;&#26356;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#26469;&#26816;&#27979;&#23457;&#26597;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#22914;&#20309;&#65288;1&#65289;&#24110;&#21161;&#31616;&#21270;&#26816;&#27979;&#36807;&#31243;&#65292;&#65288;2&#65289;&#25552;&#39640;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#26597;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#65288;3&#65289;&#21457;&#29616;&#29616;&#26377;&#21551;&#21457;&#24335;&#26041;&#27861;&#25152;&#38169;&#36807;&#30340;&#26032;&#23457;&#26597;&#23454;&#20363;&#21644;&#38459;&#27490;&#26631;&#24535;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#19987;&#23478;&#27966;&#29983;&#26631;&#31614;&#35757;&#32451;&#30340;&#30417;&#30563;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26816;&#27979;&#23457;&#26597;&#21644;&#24322;&#24120;&#30340;&#24050;&#30693;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of global censorship has led to the development of a plethora of measurement platforms to monitor and expose it. Censorship of the domain name system (DNS) is a key mechanism used across different countries. It is currently detected by applying heuristics to samples of DNS queries and responses (probes) for specific destinations. These heuristics, however, are both platform-specific and have been found to be brittle when censors change their blocking behavior, necessitating a more reliable automated process for detecting censorship.  In this paper, we explore how machine learning (ML) models can (1) help streamline the detection process, (2) improve the potential of using large-scale datasets for censorship detection, and (3) discover new censorship instances and blocking signatures missed by existing heuristic methods. Our study shows that supervised models, trained using expert-derived labels on instances of known anomalies and possible censorship, can learn the det
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;2&#32500;&#27491;&#24358;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#65292;&#22312;&#25968;&#25454;&#23384;&#22312;&#24322;&#24120;&#20540;&#25110;&#37325;&#23614;&#22122;&#22768;&#26102;&#20855;&#26377;&#20248;&#36234;&#24615;&#24182;&#24471;&#21040;&#24378;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.03229</link><description>&lt;p&gt;
&#20851;&#20110;&#20004;&#32500;&#27491;&#24358;&#27169;&#22411;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Consistency and Asymptotic Normality of Least Absolute Deviation Estimators for 2-dimensional Sinusoidal Model. (arXiv:2301.03229v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;2&#32500;&#27491;&#24358;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#65292;&#22312;&#25968;&#25454;&#23384;&#22312;&#24322;&#24120;&#20540;&#25110;&#37325;&#23614;&#22122;&#22768;&#26102;&#20855;&#26377;&#20248;&#36234;&#24615;&#24182;&#24471;&#21040;&#24378;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;2&#32500;&#27491;&#24358;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046; (LAD) &#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#21442;&#25968;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#25239;&#24178;&#25200;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#23545;&#25968;&#25454;&#20013;&#23384;&#22312;&#24322;&#24120;&#20540;&#25110;&#37325;&#23614;&#22122;&#22768;&#31561;&#38750;&#40065;&#26834;&#20272;&#35745;&#25216;&#26415;&#26080;&#27861;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LAD&#20272;&#35745;&#22120;&#30340;&#37325;&#35201;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#35777;&#26126;&#20102;2&#32500;&#27491;&#24358;&#27169;&#22411;&#20449;&#21495;&#21442;&#25968;&#30340;LAD&#20272;&#35745;&#22120;&#20855;&#26377;&#24378;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#20351;&#29992;LAD&#20272;&#35745;&#22120;&#20248;&#20110;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#20248;&#21183;&#12290;&#23545;2&#32500;&#32441;&#29702;&#25968;&#25454;&#30340;&#25968;&#25454;&#20998;&#26512;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;LAD&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation of the parameters of a 2-dimensional sinusoidal model is a fundamental problem in digital signal processing and time series analysis. In this paper, we propose a robust least absolute deviation (LAD) estimators for parameter estimation. The proposed methodology provides a robust alternative to non-robust estimation techniques like the least squares estimators, in situations where outliers are present in the data or in the presence of heavy tailed noise. We study important asymptotic properties of the LAD estimators and establish the strong consistency and asymptotic normality of the LAD estimators of the signal parameters of a 2-dimensional sinusoidal model. We further illustrate the advantage of using LAD estimators over least squares estimators through extensive simulation studies. Data analysis of a 2-dimensional texture data indicates practical applicability of the proposed LAD approach.
&lt;/p&gt;</description></item><item><title>Bagging&#26159;&#19968;&#31181;&#26368;&#20248;&#30340;PAC&#23398;&#20064;&#22120;,&#20855;&#26377;&#21487;&#35777;&#26126;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#30456;&#23545;&#20110;Hanneke&#30340;&#31639;&#27861;&#65292;Bagging&#26356;&#20026;&#39640;&#25928;&#65292;&#19988;&#21482;&#38656;&#35201;logarithmic&#25968;&#37327;&#30340;&#23376;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.02264</link><description>&lt;p&gt;
Bagging&#26159;&#19968;&#31181;&#26368;&#20248;&#30340;PAC&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Bagging is an Optimal PAC Learner. (arXiv:2212.02264v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02264
&lt;/p&gt;
&lt;p&gt;
Bagging&#26159;&#19968;&#31181;&#26368;&#20248;&#30340;PAC&#23398;&#20064;&#22120;,&#20855;&#26377;&#21487;&#35777;&#26126;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#30456;&#23545;&#20110;Hanneke&#30340;&#31639;&#27861;&#65292;Bagging&#26356;&#20026;&#39640;&#25928;&#65292;&#19988;&#21482;&#38656;&#35201;logarithmic&#25968;&#37327;&#30340;&#23376;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;PAC&#23398;&#20064;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;&#23398;&#20064;&#29702;&#35770;&#20960;&#21313;&#24180;&#26469;&#30340;&#20013;&#24515;&#38590;&#39064;&#12290;Han&#21475;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#32473;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#21487;&#35777;&#26126;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;&#20182;&#30340;&#31639;&#27861;&#22522;&#20110;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35880;&#24910;&#21644;&#32467;&#26500;&#21270;&#23376;&#37319;&#26679;&#65292;&#28982;&#21518;&#36820;&#22238;&#22312;&#27599;&#20010;&#23376;&#26679;&#26412;&#19978;&#35757;&#32451;&#30340;&#20551;&#35774;&#30340;&#22810;&#25968;&#25237;&#31080;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23454;&#29992;&#19988;&#32463;&#20856;&#30340;heuristic bagging (&#21363;&#33258;&#21161;&#32858;&#21512;)&#65292;&#26159;&#19968;&#31181;&#26368;&#20248;&#30340;PAC&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#23545;Bagging&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#32039;&#23494;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the optimal sample complexity of PAC learning in the realizable setting was a central open problem in learning theory for decades. Finally, the seminal work by Hanneke (2016) gave an algorithm with a provably optimal sample complexity. His algorithm is based on a careful and structured sub-sampling of the training data and then returning a majority vote among hypotheses trained on each of the sub-samples. While being a very exciting theoretical result, it has not had much impact in practice, in part due to inefficiency, since it constructs a polynomial number of sub-samples of the training data, each of linear size.  In this work, we prove the surprising result that the practical and classic heuristic bagging (a.k.a. bootstrap aggregation), due to Breiman (1996), is in fact also an optimal PAC learner. Bagging pre-dates Hanneke's algorithm by twenty years and is taught in most undergraduate machine learning courses. Moreover, we show that it only requires a logarithmic numb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#30340;&#26041;&#27861; PASTA&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010; Syn-to-Real &#20219;&#21153;&#19978;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.00979</link><description>&lt;p&gt;
PASTA&#65306;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#29992;&#20110; Syn-to-Real &#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization. (arXiv:2212.00979v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#30340;&#26041;&#27861; PASTA&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010; Syn-to-Real &#20219;&#21153;&#19978;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#24265;&#20215;&#19988;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#34920;&#29616;&#26174;&#33879;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Proportional Amplitude Spectrum Training Augmentation (PASTA)&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#25552;&#39640;&#21512;&#25104;&#21040;&#30495;&#23454;&#65288;Syn-to-Real&#65289;&#27867;&#21270;&#24615;&#33021;&#12290; PASTA &#22312; Fourier &#39046;&#22495;&#20013;&#25200;&#21160;&#21512;&#25104;&#22270;&#20687;&#30340;&#24133;&#24230;&#35889;&#20197;&#29983;&#25104;&#22686;&#24378;&#35270;&#22270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992; PASTA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#25200;&#21160;&#31574;&#30053;&#65292;&#20854;&#20013;&#39640;&#39057;&#20998;&#37327;&#30456;&#23545;&#20110;&#20302;&#39057;&#20998;&#37327;&#26356;&#23481;&#26131;&#21463;&#21040;&#25200;&#21160;&#12290;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#65288;GTAV-to-Real&#65289;&#65292;&#30446;&#26631;&#26816;&#27979;&#65288;Sim10K-to-Real&#65289;&#21644;&#23545;&#35937;&#35782;&#21035;&#65288;VisDA-C Syn-to-Real&#65289;&#20219;&#21153;&#65292;&#22312;&#24635;&#20849;5&#20010; Syn-to-Real &#36716;&#31227;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616; PASTA &#30340;&#24615;&#33021;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#20855;&#26377;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data offers the promise of cheap and bountiful training data for settings where labeled real-world data is scarce. However, models trained on synthetic data significantly underperform when evaluated on real-world data. In this paper, we propose Proportional Amplitude Spectrum Training Augmentation (PASTA), a simple and effective augmentation strategy to improve out-of-the-box synthetic-to-real (syn-to-real) generalization performance. PASTA perturbs the amplitude spectra of synthetic images in the Fourier domain to generate augmented views. Specifically, with PASTA we propose a structured perturbation strategy where high-frequency components are perturbed relatively more than the low-frequency ones. For the tasks of semantic segmentation (GTAV-to-Real), object detection (Sim10K-to-Real), and object recognition (VisDA-C Syn-to-Real), across a total of 5 syn-to-real shifts, we find that PASTA outperforms more complex state-of-the-art generalization methods while being complemen
&lt;/p&gt;</description></item><item><title>Nelson&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;Lov\'asz Local Lemma&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.00296</link><description>&lt;p&gt;
&#36890;&#36807; Lov\'asz Local Lemma &#36827;&#34892;&#37319;&#26679;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#23398;&#20064;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Combinatorial Structures via Markov Random Fields with Sampling through Lov\'asz Local Lemma. (arXiv:2212.00296v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00296
&lt;/p&gt;
&lt;p&gt;
Nelson&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;Lov\'asz Local Lemma&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32452;&#21512;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#30001;&#20110;&#23398;&#20064;&#30446;&#26631;&#21463;&#21040;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#21046;&#32422;&#65292;&#20854;&#26799;&#24230;&#20272;&#35745;&#38750;&#24120;&#22797;&#26434;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110; Lov\'asz Local Lemma &#30340;&#31070;&#32463;&#32593;&#32476;&#65288;Nelson&#65289;&#65292;&#23427;&#33021;&#22815;&#20174;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#30340;&#20998;&#24067;&#20013;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models for learning combinatorial structures have transformative impacts in many applications. However, existing approaches fail to offer efficient and accurate learning results. Because of the highly intractable nature of the gradient estimation of the learning objective subject to combinatorial constraints. Existing gradient estimation methods would easily run into exponential time/memory space, or incur huge estimation errors due to improper approximation. We develop NEural Lovasz Sampler (Nelson), a neural network based on Lov\'asz Local Lemma (LLL). We show it guarantees to generate samples satisfying combinatorial constraints from the distribution of the constrained Markov Random Fields model (MRF) under certain conditions. We further present a fully differentiable contrastive-divergence-based learning framework on constrained MRF (Nelson-CD). Meanwhile, Nelson-CD being fully differentiable allows us to take advantage of the parallel computing power of GPUs, resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.11030</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Adversarial Cheap Talk. (arXiv:2211.11030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#24120;&#20551;&#23450;&#25915;&#20987;&#32773;&#21487;&#20197;&#39640;&#24230;&#29305;&#26435;&#22320;&#35775;&#38382;&#21463;&#23475;&#32773;&#30340;&#21442;&#25968;&#12289;&#29615;&#22659;&#25110;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24265;&#20215;&#20132;&#27969;MDP&#30340;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#30830;&#23450;&#24615;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#12290;&#23545;&#25163;&#19981;&#33021;&#25513;&#30422;&#22320;&#38754;&#20107;&#23454;&#65292;&#24433;&#21709;&#22522;&#26412;&#29615;&#22659;&#21160;&#24577;&#25110;&#22870;&#21169;&#20449;&#21495;&#65292;&#24341;&#20837;&#19981;&#31283;&#23450;&#24615;&#65292;&#22686;&#21152;&#38543;&#26426;&#24615;&#65292;&#30475;&#21040;&#21463;&#23475;&#32773;&#30340;&#21160;&#20316;&#25110;&#35775;&#38382;&#20182;&#20204;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#23545;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#26174;&#30528;&#24433;&#21709;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#12290;&#24433;&#21709;&#35757;&#32451;&#26102;&#38388;&#34920;&#29616;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#20026;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25104;&#21151;&#21644;&#22833;&#36133;&#27169;&#24335;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorith
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#20013;&#19981;&#21516;&#34917;&#19969;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#36873;&#25321;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.10636</link><description>&lt;p&gt;
&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#23454;&#29616;&#39640;&#25928;&#35270;&#39057;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Video Representation Learning via Motion-Aware Token Selection. (arXiv:2211.10636v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10636
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#20013;&#19981;&#21516;&#34917;&#19969;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#36873;&#25321;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#33945;&#29256;&#35270;&#39057;&#24314;&#27169;&#25216;&#26415;&#36890;&#36807;&#22312;&#35270;&#39057;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;&#23548;&#33268;&#39044;&#27979;&#26080;&#25928;&#30340;&#26631;&#35760;/&#24103;&#65292;&#36825;&#20123;&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#65292;&#38656;&#35201;&#26114;&#36149;&#30340;&#35745;&#31639;&#26426;&#21644;&#22823;&#37327;&#26174;&#21345;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#39057;&#34917;&#19969;&#20013;&#30340;&#19981;&#22343;&#21248;&#20449;&#24687;&#23494;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26631;&#35760;&#36873;&#25321;&#26041;&#27861;&#65306;MATS&#65306;&#36816;&#21160;&#24863;&#30693;&#26631;&#35760;&#36873;&#25321;&#65292;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#25214;&#21040;&#21253;&#21547;&#20016;&#23500;&#21160;&#24577;&#29305;&#24615;&#30340;&#26631;&#35760;&#65292;&#24182;&#25918;&#24323;&#26080;&#25928;&#30340;&#26631;&#35760;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20851;&#27880;&#26368;&#37325;&#35201;&#21644;&#22240;&#26524;&#24615;&#30340;&#24103;&#65292;&#24182;&#20351;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#24471;&#21040;&#26174;&#30528;&#38477;&#20302;&#65292;&#20351;&#24471;&#22312;&#21333;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged Masked Video Modeling techniques demonstrated their potential by significantly outperforming previous methods in self-supervised learning for video. However, they require an excessive amount of computations and memory while predicting uninformative tokens/frames due to random masking strategies, requiring excessive computing power for training. (e.g., over 16 nodes with 128 NVIDIA A100 GPUs). To resolve this issue, we exploit the unequal information density among the patches in videos and propose a new token selection method, MATS: Motion-Aware Token Selection, that finds tokens containing rich motion features and drops uninformative ones during both self-supervised pre-training and fine-tuning. We further present an adaptive frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. Our method significantly reduces computation and memory requirements, enabling the pre-training and fine-tuning on a single machine w
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21487;&#35299;&#37322;&#34892;&#20026;&#24314;&#35758;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#29983;&#21487;&#20197;&#29702;&#35299;&#25152;&#23398;&#30340;&#20869;&#23481;&#24182;&#36827;&#34892;&#25512;&#29702;&#20174;&#32780;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;</title><link>http://arxiv.org/abs/2211.07882</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Explainable Action Advising for Multi-Agent Reinforcement Learning. (arXiv:2211.07882v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07882
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21487;&#35299;&#37322;&#34892;&#20026;&#24314;&#35758;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#29983;&#21487;&#20197;&#29702;&#35299;&#25152;&#23398;&#30340;&#20869;&#23481;&#24182;&#36827;&#34892;&#25512;&#29702;&#20174;&#32780;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#24314;&#35758;&#26159;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#33539;&#24335;&#30340;&#24378;&#21270;&#23398;&#20064;&#30693;&#35782;&#36716;&#31227;&#25216;&#26415;&#12290;&#19987;&#23478;&#32769;&#24072;&#22312;&#35757;&#32451;&#26399;&#38388;&#25552;&#20379;&#24314;&#35758;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#31574;&#30053;&#34920;&#29616;&#12290;&#36825;&#31181;&#24314;&#35758;&#36890;&#24120;&#20197;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24418;&#24335;&#32473;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20351;&#24471;&#23398;&#29983;&#38590;&#20197;&#25512;&#29702;&#21644;&#24212;&#29992;&#20110;&#26032;&#39062;&#29366;&#24577;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#34892;&#20026;&#24314;&#35758;&#65292;&#20854;&#20013;&#32769;&#24072;&#25552;&#20379;&#34892;&#20026;&#24314;&#35758;&#21644;&#30456;&#20851;&#30340;&#35299;&#37322;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36873;&#21462;&#35813;&#34892;&#20026;.&#36825;&#20801;&#35768;&#23398;&#29983;&#33258;&#25105;&#21453;&#24605;&#25152;&#23398;&#30340;&#20869;&#23481;&#65292;&#23454;&#29616;&#24314;&#35758;&#30340;&#27867;&#21270;&#65292;&#24182;&#23548;&#33268;&#23398;&#20064;&#25928;&#29575;&#30340;&#25552;&#39640;&#8212;&#8212;&#21363;&#20351;&#22312;&#32769;&#24072;&#19981;&#29702;&#24819;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#31574;&#30053;&#22238;&#25253;&#21644;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action advising is a knowledge transfer technique for reinforcement learning based on the teacher-student paradigm. An expert teacher provides advice to a student during training in order to improve the student's sample efficiency and policy performance. Such advice is commonly given in the form of state-action pairs. However, it makes it difficult for the student to reason with and apply to novel states. We introduce Explainable Action Advising, in which the teacher provides action advice as well as associated explanations indicating why the action was chosen. This allows the student to self-reflect on what it has learned, enabling advice generalization and leading to improved sample efficiency and learning performance - even in environments where the teacher is sub-optimal. We empirically show that our framework is effective in both single-agent and multi-agent scenarios, yielding improved policy returns and convergence rates when compared to state-of-the-art methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#24378;&#20551;&#30456;&#20851;&#30340;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#23454;&#29616;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#30340;&#31639;&#27861; DCWP&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.05247</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Debiased Subnetworks with Contrastive Weight Pruning. (arXiv:2210.05247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#24378;&#20551;&#30456;&#20851;&#30340;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#23454;&#29616;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#30340;&#31639;&#27861; DCWP&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#20559;&#32622;&#24615;&#65292;&#23548;&#33268;&#25552;&#20379;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#32479;&#35745;&#35777;&#25454;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#22312;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#21151;&#33021;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#29616;&#26377;&#31639;&#27861;&#22312;&#25506;&#32034;&#20855;&#26377;&#24378;&#20551;&#30456;&#20851;&#24615;&#30340;&#26080;&#20559;&#23376;&#32593;&#32476;&#23384;&#22312;&#38480;&#21046;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#20559;&#24046;&#20914;&#31361;&#26679;&#26412;&#23545;&#32467;&#26500;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22522;&#20110;&#23398;&#20064;&#30340;&#65288;&#20266;&#65289;&#26080;&#20559;&#26679;&#26412;&#21644;&#36873;&#25321;&#24615;&#20559;&#24046;&#20914;&#31361;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#21435;&#20559;&#32622;&#23545;&#27604;&#21098;&#26525;&#65288;DCWP&#65289;&#31639;&#27861;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102; DCWP &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are often biased to spuriously correlated features that provide misleading statistical evidence that does not generalize. This raises an interesting question: ``Does an optimal unbiased functional subnetwork exist in a severely biased network? If so, how to extract such subnetwork?" While empirical evidence has been accumulated about the existence of such unbiased subnetworks, these observations are mainly based on the guidance of ground-truth unbiased samples. Thus, it is unexplored how to discover the optimal subnetworks with biased training datasets in practice. To address this, here we first present our theoretical insight that alerts potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We then further elucidate the importance of bias-conflicting samples on structure learning. Motivated by these observations, we propose a Debiased Contrastive Weight Pruning (DCWP) algorithm, which probes unbi
&lt;/p&gt;</description></item><item><title>SlimG&#31639;&#27861;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#25366;&#25496;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#23567;&#24515;&#30340;&#31616;&#21333;&#21407;&#21017;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#21508;&#31181;&#22270;&#22330;&#26223;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#31639;&#27861;&#65292;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#23454;&#29992;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04081</link><description>&lt;p&gt;
&#31616;&#21363;&#26159;&#32654;&#65306;&#38024;&#23545;&#20934;&#30830;&#12289;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#22270;&#25366;&#25496;&#30340;SlimG&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Less is More: SlimG for Accurate, Robust, and Interpretable Graph Mining. (arXiv:2210.04081v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04081
&lt;/p&gt;
&lt;p&gt;
SlimG&#31639;&#27861;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#25366;&#25496;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#23567;&#24515;&#30340;&#31616;&#21333;&#21407;&#21017;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#21508;&#31181;&#22270;&#22330;&#26223;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#31639;&#27861;&#65292;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#23454;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#21487;&#33021;&#24102;&#26377;&#22122;&#22768;&#29305;&#24449;&#21644;&#32467;&#26500;&#30340;&#22270;&#20013;&#65292;&#25105;&#20204;&#22914;&#20309;&#35299;&#20915;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#65311;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#35768;&#22810;&#22270;&#25366;&#25496;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#38590;&#24230;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#33258;&#36523;&#36873;&#25321;&#30340;&#22256;&#38590;&#65292;&#23427;&#20204;&#23545;&#20110;&#21508;&#31181;&#22270;&#26041;&#26696;&#30340;&#26222;&#36866;&#24615;&#26377;&#38480;&#12290;&#29233;&#22240;&#26031;&#22374;&#35828;&#36807;&#65306;&#8220;&#35753;&#19968;&#20999;&#23613;&#21487;&#33021;&#31616;&#21333;&#65292;&#20294;&#19981;&#35201;&#36807;&#20110;&#31616;&#21333;&#12290;&#8221;&#25105;&#20204;&#37325;&#26032;&#35808;&#37322;&#20102;&#36825;&#20010;&#21407;&#21017;&#65292;&#25552;&#20986;&#20102;&#8220;&#23567;&#24515;&#30340;&#31616;&#21333;&#21407;&#21017;&#8221;&#65306;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#31616;&#21333;&#27169;&#22411;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#36229;&#36234;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;&#22522;&#20110;&#36825;&#19968;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SlimG&#31639;&#27861;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#34920;&#29616;&#20986;&#22235;&#20010;&#29702;&#24819;&#30340;&#24615;&#36136;&#65306;&#65288;a&#65289;&#20934;&#30830;&#65292;&#22312;13&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#36194;&#24471;&#25110;&#24182;&#21015;&#31532;&#19968;&#65307;&#65288;b&#65289;&#40065;&#26834;&#65292;&#22312;&#22788;&#29702;&#25152;&#26377;&#22270;&#25968;&#25454;&#22330;&#26223;&#65288;&#21516;&#36136;&#24615;&#12289;&#24322;&#36136;&#24615;&#12289;&#38543;&#26426;&#32467;&#26500;&#12289;&#22122;&#22768;&#29305;&#24449;&#31561;&#65289;&#26041;&#38754;&#26159;&#21807;&#19968;&#30340;&#65307;&#65288;c&#65289;&#24555;&#36895;&#21487;&#25193;&#23637;&#65292;&#22312;&#30334;&#19975;&#32423;&#22270;&#20013;&#35757;&#32451;&#36895;&#24230;&#26368;&#24555;18&#20493;&#65307;&#65288;d&#65289;&#21487;&#35299;&#37322;&#65292;&#25552;&#20379;&#23427;&#26159;&#22914;&#20309;&#24037;&#20316;&#30340;&#27934;&#23519;&#12290;SlimG&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#19968;&#20010;&#30452;&#35266;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#20294;&#20855;&#26377;&#31934;&#24515;&#35774;&#35745;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22270;&#22330;&#26223;&#20013;&#65292;SlimG&#31639;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#31639;&#27861;&#65292;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#23454;&#29992;&#20505;&#36873;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we solve semi-supervised node classification in various graphs possibly with noisy features and structures? Graph neural networks (GNNs) have succeeded in many graph mining tasks, but their generalizability to various graph scenarios is limited due to the difficulty of training, hyperparameter tuning, and the selection of a model itself. Einstein said that we should "make everything as simple as possible, but not simpler." We rephrase it into the careful simplicity principle: a carefully-designed simple model can surpass sophisticated ones in real-world graphs. Based on the principle, we propose SlimG for semi-supervised node classification, which exhibits four desirable properties: It is (a) accurate, winning or tying on 10 out of 13 real-world datasets; (b) robust, being the only one that handles all scenarios of graph data (homophily, heterophily, random structure, noisy features, etc.); (c) fast and scalable, showing up to 18 times faster training in million-scale graphs; a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#38024;&#23545;&#22312;&#32447;&#35777;&#25454;&#30340;&#20266;&#35013;&#21644;&#34394;&#20551;&#20449;&#24687;&#31561;&#20004;&#31181;&#25915;&#20987;&#30446;&#26631;&#21644;&#19981;&#21516;&#30340;&#23041;&#32961;&#27169;&#22411;&#32500;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#35777;&#25454;&#20013;&#19982;&#20107;&#23454;&#30456;&#20851;&#30340;&#37096;&#20998;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#65292;&#24182;&#29983;&#25104;&#26080;&#27861;&#34987;&#33258;&#21160;&#35782;&#21035;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2209.03755</link><description>&lt;p&gt;
&#20107;&#23454;&#30772;&#22351;&#32773;&#65306;&#38024;&#23545;&#20107;&#23454;&#39564;&#35777;&#31995;&#32479;&#30340;&#35777;&#25454;&#25805;&#32437;&#25915;&#20987;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems. (arXiv:2209.03755v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#38024;&#23545;&#22312;&#32447;&#35777;&#25454;&#30340;&#20266;&#35013;&#21644;&#34394;&#20551;&#20449;&#24687;&#31561;&#20004;&#31181;&#25915;&#20987;&#30446;&#26631;&#21644;&#19981;&#21516;&#30340;&#23041;&#32961;&#27169;&#22411;&#32500;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#35777;&#25454;&#20013;&#19982;&#20107;&#23454;&#30456;&#20851;&#30340;&#37096;&#20998;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#65292;&#24182;&#29983;&#25104;&#26080;&#27861;&#34987;&#33258;&#21160;&#35782;&#21035;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#23548;&#21644;&#34394;&#20551;&#20449;&#24687;&#23545;&#25105;&#20204;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#26500;&#25104;&#20102;&#20005;&#37325;&#30340;&#20840;&#29699;&#23041;&#32961;&#12290;&#20026;&#20102;&#24212;&#23545;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#35268;&#27169;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#21162;&#21147;&#36890;&#36807;&#26816;&#32034;&#21644;&#39564;&#35777;&#30456;&#20851;&#35777;&#25454;&#26469;&#33258;&#21160;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#23545;&#27492;&#31867;&#31995;&#32479;&#21487;&#33021;&#38754;&#20020;&#30340;&#25915;&#20987;&#21521;&#37327;&#30340;&#20840;&#38754;&#35780;&#20272;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#25932;&#23545;&#26041;&#65292;&#36890;&#36807;&#20266;&#35013;&#30456;&#20851;&#35777;&#25454;&#25110;&#32773;&#25552;&#20379;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#65292;&#33258;&#21160;&#24178;&#25200;&#22312;&#32447;&#35777;&#25454;&#65292;&#20197;&#30772;&#22351;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#36825;&#20004;&#20010;&#30446;&#26631;&#21644;&#19981;&#21516;&#30340;&#23041;&#32961;&#27169;&#22411;&#32500;&#24230;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#35777;&#25454;&#20013;&#19982;&#20107;&#23454;&#30456;&#20851;&#30340;&#37096;&#20998;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#65292;&#24182;&#29983;&#25104;&#19982;&#21407;&#22987;&#35777;&#25454;&#33073;&#33410;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#30772;&#22351;&#20107;&#23454;&#39564;&#35777;&#32467;&#26524;&#65292;&#20351;&#20854;&#26080;&#27861;&#34987;&#33258;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mis- and disinformation are a substantial global threat to our security and safety. To cope with the scale of online misinformation, researchers have been working on automating fact-checking by retrieving and verifying against relevant evidence. However, despite many advances, a comprehensive evaluation of the possible attack vectors against such systems is still lacking. Particularly, the automated fact-verification process might be vulnerable to the exact disinformation campaigns it is trying to combat. In this work, we assume an adversary that automatically tampers with the online evidence in order to disrupt the fact-checking model via camouflaging the relevant evidence or planting a misleading one. We first propose an exploratory taxonomy that spans these two targets and the different threat model dimensions. Guided by this, we design and propose several potential attack methods. We show that it is possible to subtly modify claim-salient snippets in the evidence and generate diver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#32479;&#35745;&#27867;&#20989;Gateaux&#23548;&#25968;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#30340;Gateaux&#23548;&#25968;&#20272;&#35745;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#22240;&#26524;&#25512;&#26029;&#21644;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#31561;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.13701</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26368;&#20248;&#21270;&#22240;&#26524;&#25512;&#26029;&#24433;&#21709;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Influence Functions for Optimization-Based Causal Inference. (arXiv:2208.13701v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#32479;&#35745;&#27867;&#20989;Gateaux&#23548;&#25968;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#30340;Gateaux&#23548;&#25968;&#20272;&#35745;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#22240;&#26524;&#25512;&#26029;&#21644;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#31561;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#32479;&#35745;&#27867;&#20989;Gateaux&#23548;&#25968;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#20986;&#29616;&#30340;&#27867;&#20989;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27010;&#29575;&#20998;&#24067;&#26410;&#30693;&#20294;&#38656;&#35201;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#30340;&#24773;&#20917;&#12290;&#36825;&#20123;&#20272;&#35745;&#20998;&#24067;&#24341;&#23548;&#20102;&#32463;&#39564;Gateaux&#23548;&#25968;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32463;&#39564;&#12289;&#25968;&#20540;&#21644;&#35299;&#26512;Gateaux&#23548;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20174;&#24178;&#39044;&#22343;&#20540;&#65288;&#24179;&#22343;&#28508;&#22312;&#32467;&#26524;&#65289;&#30340;&#26696;&#20363;&#20837;&#25163;&#65292;&#25105;&#20204;&#21246;&#21202;&#20102;&#26377;&#38480;&#24046;&#20998;&#21644;&#35299;&#26512;Gateaux&#23548;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#25200;&#21160;&#21644;&#24179;&#28369;&#30340;&#25968;&#20540;&#36924;&#36817;&#36895;&#29575;&#35201;&#27714;&#65292;&#20197;&#20445;&#25345;&#21333;&#27493;&#35843;&#25972;&#30340;&#32479;&#35745;&#20248;&#21183;&#65292;&#20363;&#22914;&#36895;&#29575;&#21452;&#37325;&#24378;&#20581;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26356;&#22797;&#26434;&#30340;&#27867;&#20989;&#65292;&#22914;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#12289;&#26080;&#38480;&#26102;&#27573;Markov&#20915;&#31574;&#20013;&#31574;&#30053;&#20248;&#21270;&#30340;&#32447;&#24615;&#35268;&#21010;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a constructive algorithm that approximates Gateaux derivatives for statistical functionals by finite differencing, with a focus on functionals that arise in  causal inference. We study the case where probability distributions are not known a priori but need to be estimated from data. These estimated distributions lead to empirical Gateaux derivatives, and we study the relationships between empirical, numerical, and analytical Gateaux derivatives. Starting with a case study of the interventional mean (average potential outcome), we delineate the relationship between finite differences and the analytical Gateaux derivative. We then derive requirements on the rates of numerical approximation in perturbation and smoothing that preserve the statistical benefits of one-step adjustments, such as rate double robustness. We then study more complicated functionals such as dynamic treatment regimes, the linear-programming formulation for policy optimization in infinite-horizon Markov dec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23547;&#25214;&#21487;&#37325;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#20197;&#26500;&#24314;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#27969;&#27700;&#32447;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25910;&#38598;&#12289;&#20998;&#26512;&#20195;&#34920;&#24615;&#35770;&#25991;&#26469;&#37492;&#23450;&#21644;&#25551;&#36848;&#20851;&#38190;&#27010;&#24565;&#65292;&#20174;&#32780;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;&#21487;&#21457;&#29616;&#24615;&#12289;&#21487;&#35775;&#38382;&#24615;&#12289;&#20114;&#25805;&#20316;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.05596</link><description>&lt;p&gt;
&#23547;&#25214;&#21487;&#37325;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#20197;&#26500;&#24314;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines. (arXiv:2208.05596v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05596
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23547;&#25214;&#21487;&#37325;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#20197;&#26500;&#24314;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#27969;&#27700;&#32447;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25910;&#38598;&#12289;&#20998;&#26512;&#20195;&#34920;&#24615;&#35770;&#25991;&#26469;&#37492;&#23450;&#21644;&#25551;&#36848;&#20851;&#38190;&#27010;&#24565;&#65292;&#20174;&#32780;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;&#21487;&#21457;&#29616;&#24615;&#12289;&#21487;&#35775;&#38382;&#24615;&#12289;&#20114;&#25805;&#20316;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#36825;&#19968;&#39046;&#22495;&#34920;&#29616;&#20986;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26032;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#20154;&#21592;&#26469;&#35828;&#65292;&#30001;&#20110;&#38656;&#35201;&#35299;&#20915;&#21508;&#31181;&#19981;&#21516;&#30340;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#25968;&#37327;&#20247;&#22810;&#20197;&#21450;&#28041;&#21450;&#30340;&#32534;&#35793;&#22120;&#25110;&#24037;&#20855;&#38598;&#22797;&#26434;&#31561;&#22240;&#32032;&#65292;&#24456;&#38590;&#25214;&#21040;&#26500;&#24314;&#33258;&#24049;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#25152;&#38656;&#30340;&#27491;&#30830;&#32452;&#20214;&#12290;&#20026;&#20102;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;&#21487;&#21457;&#29616;&#24615;&#12289;&#21487;&#35775;&#38382;&#24615;&#12289;&#20114;&#25805;&#20316;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#20998;&#26512;&#20102;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#35770;&#25991;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37492;&#23450;&#21644;&#25551;&#36848;&#20102;&#20851;&#38190;&#27010;&#24565;&#65292;&#21253;&#25324;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#25903;&#25345;&#24037;&#20855;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#24212;&#29992;&#31034;&#20363;&#65292;&#35828;&#26126;&#22914;&#20309;&#21033;&#29992;&#21487;&#37325;&#29992;&#32452;&#20214;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#26469;&#35299;&#20915;&#19968;&#32452;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programming Language Processing (PLP) using machine learning has made vast improvements in the past few years. Increasingly more people are interested in exploring this promising field. However, it is challenging for new researchers and developers to find the right components to construct their own machine learning pipelines, given the diverse PLP tasks to be solved, the large number of datasets and models being released, and the set of complex compilers or tools involved. To improve the findability, accessibility, interoperability and reusability (FAIRness) of machine learning components, we collect and analyze a set of representative papers in the domain of machine learning-based PLP. We then identify and characterize key concepts including PLP tasks, model architectures and supportive tools. Finally, we show some example use cases of leveraging the reusable components to construct machine learning pipelines to solve a set of PLP tasks.
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#38382;&#39064;&#24341;&#36215;&#20102;&#23545;&#20854;&#21463;&#35797;&#32773;&#36523;&#20221;&#30340;&#20105;&#35758;&#19982;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#30417;&#31649;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2206.04039</link><description>&lt;p&gt;
&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#20154;&#31867;&#21463;&#35797;&#32773;&#36523;&#20221;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04039
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#38382;&#39064;&#24341;&#36215;&#20102;&#23545;&#20854;&#21463;&#35797;&#32773;&#36523;&#20221;&#30340;&#20105;&#35758;&#19982;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#30417;&#31649;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#22312;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#38656;&#35201;&#20154;&#31867;&#20132;&#20114;&#25110;&#21028;&#26029;&#30340;&#30740;&#31350;&#38382;&#39064;&#26041;&#38754;&#65292;&#24050;&#32463;&#20005;&#37325;&#20381;&#36182;&#20110;&#20247;&#21253;&#24037;&#20316;&#32773;&#12290;&#30001;&#20110;&#25191;&#34892;&#30340;&#20219;&#21153;&#22810;&#26679;&#21270;&#21644;&#25968;&#25454;&#29992;&#36884;&#30340;&#22810;&#26679;&#24615;&#65292;&#24456;&#38590;&#30830;&#23450;&#20309;&#26102;&#23558;&#20247;&#21253;&#24037;&#20316;&#32773;&#35270;&#20026;&#24037;&#20154;(&#32780;&#38750;&#20154;&#31867;&#21463;&#35797;&#32773;)&#12290;&#36825;&#20123;&#22256;&#38590;&#21152;&#21095;&#20102;&#25919;&#31574;&#30340;&#20914;&#31361;&#65292;&#19968;&#20123;&#26426;&#26500;&#21644;&#30740;&#31350;&#20154;&#21592;&#23558;&#25152;&#26377;ML&#20247;&#21253;&#24037;&#20316;&#32773;&#35270;&#20026;&#20154;&#31867;&#21463;&#35797;&#32773;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#35748;&#20026;&#23427;&#20204;&#24456;&#23569;&#26500;&#25104;&#20154;&#31867;&#21463;&#35797;&#32773;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21253;&#25324;&#20247;&#21253;&#24037;&#20316;&#30340;&#40092;&#26377;ML&#35770;&#25991;&#25552;&#21040;IRB&#30340;&#30417;&#30563;&#65292;&#24341;&#21457;&#20102;&#36829;&#21453;&#36947;&#24503;&#21644;&#27861;&#35268;&#35201;&#27714;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ML&#20247;&#21253;&#30740;&#31350;&#30340;&#36866;&#24403;&#21010;&#23450;&#65292;&#24182;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26292;&#38706;&#20986;&#30340;&#29420;&#29305;&#30740;&#31350;&#30417;&#30563;&#25361;&#25112;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#32654;&#22269;&#20844;&#20849;&#35268;&#21017;&#19979;&#65292;&#36825;&#20123;&#21028;&#26029;&#21462;&#20915;&#20110;&#20851;&#20110;&#38382;&#39064;&#30340;&#30830;&#23450;&#65292;&#28041;&#21450;&#35841;(&#25110;&#20160;&#20040;)&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diverse tasks performed and uses of the data produced render it difficult to determine when crowdworkers are best thought of as workers (versus human subjects). These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements. We investigate the appropriate designation of ML crowdsourcing studies, focusing our inquiry on natural language processing to expose unique challenges for research oversight. Crucially, under the U.S. Common Rule, these judgments hinge on determinations of aboutness, concerning both whom (or what) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26681;&#25454;&#30697;&#38453;&#36817;&#20284;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;GCN&#33410;&#28857;&#23884;&#20837;&#32858;&#21512;&#30340;&#36880;&#23618;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#27425;&#20248;&#37319;&#26679;&#27010;&#29575;&#21644;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#30340;&#20004;&#31181;&#26032;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25913;&#36827;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2206.00583</link><description>&lt;p&gt;
&#26657;&#20934;&#21644;&#21435;&#20559;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#36880;&#23618;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Calibrate and Debias Layer-wise Sampling for Graph Convolutional Networks. (arXiv:2206.00583v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26681;&#25454;&#30697;&#38453;&#36817;&#20284;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;GCN&#33410;&#28857;&#23884;&#20837;&#32858;&#21512;&#30340;&#36880;&#23618;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#27425;&#20248;&#37319;&#26679;&#27010;&#29575;&#21644;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#30340;&#20004;&#31181;&#26032;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25913;&#36827;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#21644;&#21152;&#36895;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#35757;&#32451;&#20013;&#30340;&#33410;&#28857;&#23884;&#20837;&#32858;&#21512;&#12290;&#20854;&#20013;&#65292;&#36880;&#23618;&#26041;&#27861;&#36882;&#24402;&#22320;&#25191;&#34892;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#20849;&#21516;&#36873;&#25321;&#27599;&#23618;&#20013;&#29616;&#26377;&#33410;&#28857;&#30340;&#37051;&#23621;&#12290;&#26412;&#25991;&#20174;&#30697;&#38453;&#36817;&#20284;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#29616;&#26377;&#36880;&#23618;&#37319;&#26679;&#26041;&#27861;&#20013;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#27425;&#20248;&#37319;&#26679;&#27010;&#29575;&#21644;&#26080;&#37325;&#22797;&#37319;&#26679;&#24341;&#36215;&#30340;&#20272;&#35745;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#26500;&#24314;&#37319;&#26679;&#27010;&#29575;&#30340;&#26032;&#21407;&#21017;&#21644;&#19968;&#31181;&#39640;&#25928;&#30340;&#21435;&#20559;&#31639;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20272;&#35745;&#26041;&#24046;&#20998;&#26512;&#21644;&#24120;&#35265;&#22522;&#20934;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#25913;&#36827;&#12290;&#20195;&#30721;&#21644;&#31639;&#27861;&#23454;&#29616;&#21487;&#22312; https://github.com/ychen-stat-ml/GCN-layer-wise-sampling &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple sampling-based methods have been developed for approximating and accelerating node embedding aggregation in graph convolutional networks (GCNs) training. Among them, a layer-wise approach recursively performs importance sampling to select neighbors jointly for existing nodes in each layer. This paper revisits the approach from a matrix approximation perspective, and identifies two issues in the existing layer-wise sampling methods: suboptimal sampling probabilities and estimation biases induced by sampling without replacement. To address these issues, we accordingly propose two remedies: a new principle for constructing sampling probabilities and an efficient debiasing algorithm. The improvements are demonstrated by extensive analyses of estimation variance and experiments on common benchmarks. Code and algorithm implementations are publicly available at https://github.com/ychen-stat-ml/GCN-layer-wise-sampling .
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Actor-Critic&#30340;&#25511;&#21046;&#24863;&#30693;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#20844;&#24335;&#23398;&#20064;&#36807;&#31243;&#29366;&#24577;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#20197;&#35299;&#20915;&#20108;&#20803;&#38543;&#26426;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;&#20854;&#33021;&#22815;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20302;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#26816;&#27979;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2201.00879</link><description>&lt;p&gt;
&#22522;&#20110;Actor-Critic&#30340;&#25511;&#21046;&#24863;&#30693;&#26041;&#27861;&#29992;&#20110;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Temporal Detection of Anomalies via Actor-Critic Based Controlled Sensing. (arXiv:2201.00879v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.00879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Actor-Critic&#30340;&#25511;&#21046;&#24863;&#30693;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#20844;&#24335;&#23398;&#20064;&#36807;&#31243;&#29366;&#24577;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#20197;&#35299;&#20915;&#20108;&#20803;&#38543;&#26426;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;&#20854;&#33021;&#22815;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20302;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#26816;&#27979;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#30417;&#27979;&#19968;&#32452;&#20108;&#20803;&#38543;&#26426;&#36807;&#31243;&#26102;&#65292;&#24403;&#20854;&#20013;&#24322;&#24120;&#20010;&#25968;&#36229;&#36807;&#19968;&#20010;&#38408;&#20540;&#26102;&#21457;&#20986;&#35686;&#25253;&#12290;&#20026;&#27492;&#65292;&#20915;&#31574;&#32773;&#36873;&#25321;&#21644;&#26816;&#27979;&#19968;&#32452;&#23376;&#36807;&#31243;&#65292;&#20197;&#33719;&#21462;&#23427;&#20204;&#29366;&#24577;&#65288;&#27491;&#24120;&#25110;&#38750;&#27491;&#24120;&#65289;&#30340;&#22122;&#22768;&#20272;&#35745;&#12290;&#22522;&#20110;&#25509;&#21463;&#21040;&#30340;&#35266;&#27979;&#65292;&#20915;&#31574;&#32773;&#39318;&#20808;&#30830;&#23450;&#26159;&#21542;&#23459;&#24067;&#24322;&#24120;&#25968;&#36229;&#36807;&#38408;&#20540;&#65292;&#25110;&#32487;&#32493;&#35266;&#23519;&#12290;&#24403;&#20915;&#31574;&#26159;&#32487;&#32493;&#26102;&#65292;&#23427;&#20250;&#20915;&#23450;&#26159;&#21542;&#22312;&#19979;&#19968;&#20010;&#26102;&#21051;&#25910;&#38598;&#35266;&#27979;&#65292;&#36824;&#26159;&#23558;&#20854;&#25512;&#36831;&#21040;&#20197;&#21518;&#30340;&#26576;&#20010;&#26102;&#21051;&#12290;&#22914;&#26524;&#23427;&#36873;&#25321;&#25910;&#38598;&#35266;&#27979;&#65292;&#23427;&#36824;&#23558;&#30830;&#23450;&#38656;&#20390;&#27979;&#30340;&#23376;&#36807;&#31243;&#32452;&#12290;&#20026;&#20102;&#35774;&#35745;&#36825;&#31181;&#19977;&#27493;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#20844;&#24335;&#23398;&#20064;&#36807;&#31243;&#29366;&#24577;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#20351;&#29992;&#21518;&#39564;&#27010;&#29575;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#30340;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;AC-CST(&#22522;&#20110;Actor-Critic&#30340;&#25511;&#21046;&#24863;&#30693;&#26041;&#27861;&#29992;&#20110;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;)&#65292;&#22312;&#26816;&#27979;&#26102;&#38388;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#22122;&#27604;&#20302;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of monitoring a set of binary stochastic processes and generating an alert when the number of anomalies among them exceeds a threshold. For this, the decision-maker selects and probes a subset of the processes to obtain noisy estimates of their states (normal or anomalous). Based on the received observations, the decisionmaker first determines whether to declare that the number of anomalies has exceeded the threshold or to continue taking observations. When the decision is to continue, it then decides whether to collect observations at the next time instant or defer it to a later time. If it chooses to collect observations, it further determines the subset of processes to be probed. To devise this three-step sequential decision-making process, we use a Bayesian formulation wherein we learn the posterior probability on the states of the processes. Using the posterior probability, we construct a Markov decision process and solve it using deep actor-critic reinforce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#22312;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#20272;&#35745;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.06818</link><description>&lt;p&gt;
&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#65306;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#39640;&#32500;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Dynamic treatment effects: high-dimensional inference under model misspecification. (arXiv:2111.06818v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#22312;&#27169;&#22411;&#38169;&#35823;&#19979;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#20272;&#35745;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#24178;&#39044;&#30340;&#26102;&#21464;&#22240;&#26524;&#24433;&#21709;&#30340;&#24494;&#22937;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#8220;&#32500;&#25968;&#28798;&#38590;&#8221;&#21644;&#26102;&#21464;&#28151;&#26434;&#30340;&#23384;&#22312;&#65292;&#36825;&#31181;&#20272;&#35745;&#23384;&#22312;&#30528;&#25361;&#25112;&#65292;&#21487;&#33021;&#23548;&#33268;&#20272;&#35745;&#20559;&#35823;&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#22320;&#35268;&#23450;&#26085;&#30410;&#22686;&#22810;&#30340;&#27835;&#30103;&#20998;&#37197;&#21644;&#22810;&#37325;&#26292;&#38706;&#30340;&#32467;&#26524;&#27169;&#22411;&#20284;&#20046;&#36807;&#20110;&#22797;&#26434;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#21452;&#37325;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#22312;&#20801;&#35768;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#65292;&#28982;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24182;&#27809;&#26377;&#23454;&#29616;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#23545;&#27835;&#30103;&#20998;&#37197;&#21644;&#32467;&#26524;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#24207;&#21015;&#27169;&#22411;&#21452;&#37325;&#40065;&#26834;&#24615;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#24403;&#27599;&#20010;&#26102;&#38388;&#26292;&#38706;&#37117;&#26159;&#21452;&#37325;&#40065;&#26834;&#24615;&#30340;&#26102;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#23454;&#29616;&#21452;&#37325;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#32500;&#29615;&#22659;&#19979;&#21160;&#24577;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating dynamic treatment effects is essential across various disciplines, offering nuanced insights into the time-dependent causal impact of interventions. However, this estimation presents challenges due to the "curse of dimensionality" and time-varying confounding, which can lead to biased estimates. Additionally, correctly specifying the growing number of treatment assignments and outcome models with multiple exposures seems overly complex. Given these challenges, the concept of double robustness, where model misspecification is permitted, is extremely valuable, yet unachieved in practical applications. This paper introduces a new approach by proposing novel, robust estimators for both treatment assignments and outcome models. We present a "sequential model double robust" solution, demonstrating that double robustness over multiple time points can be achieved when each time exposure is doubly robust. This approach improves the robustness and reliability of dynamic treatment effe
&lt;/p&gt;</description></item><item><title>ARFED&#26159;&#19968;&#31181;&#38450;&#24481;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#31163;&#32676;&#20540;&#65292;&#19981;&#38656;&#35201;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#12289;&#26356;&#26032;&#30456;&#20284;&#24615;&#25110;&#24694;&#24847;&#21442;&#19982;&#32773;&#27604;&#29575;&#65292;&#23454;&#29616;&#20102;&#22312;&#22522;&#20934;&#32852;&#37030;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#38450;&#25915;&#20987;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.04550</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#24120;&#20540;&#28040;&#38500;&#30340;&#38450;&#25915;&#20987;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;ARFED
&lt;/p&gt;
&lt;p&gt;
ARFED: Attack-Resistant Federated averaging based on outlier elimination. (arXiv:2111.04550v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.04550
&lt;/p&gt;
&lt;p&gt;
ARFED&#26159;&#19968;&#31181;&#38450;&#24481;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#31163;&#32676;&#20540;&#65292;&#19981;&#38656;&#35201;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#12289;&#26356;&#26032;&#30456;&#20284;&#24615;&#25110;&#24694;&#24847;&#21442;&#19982;&#32773;&#27604;&#29575;&#65292;&#23454;&#29616;&#20102;&#22312;&#22522;&#20934;&#32852;&#37030;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#38450;&#25915;&#20987;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#27599;&#20010;&#21442;&#19982;&#32773;&#20351;&#29992;&#33258;&#24049;&#30340;&#25968;&#25454;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#32780;&#20840;&#23616;&#27169;&#22411;&#26159;&#30001;&#38598;&#25104;&#26469;&#33258;&#36825;&#20123;&#21442;&#19982;&#32773;&#30340;&#27169;&#22411;&#26356;&#26032;&#30340;&#21487;&#20449;&#26381;&#21153;&#22120;&#24418;&#25104;&#30340;&#12290;&#30001;&#20110;&#26381;&#21153;&#22120;&#23545;&#21442;&#19982;&#32773;&#30340;&#35757;&#32451;&#36807;&#31243;&#27809;&#26377;&#24433;&#21709;&#19982;&#21487;&#35265;&#24615;&#20197;&#30830;&#20445;&#38544;&#31169;&#65292;&#20840;&#23616;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#21644;&#27169;&#22411;&#27745;&#26579;&#31561;&#25915;&#20987;&#12290;&#34429;&#28982;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#25915;&#20987;&#30340;&#38450;&#24481;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20570;&#20986;&#19982;&#32852;&#37030;&#23398;&#20064;&#24615;&#36136;&#19981;&#31526;&#30340;&#24378;&#20551;&#35774;&#65292;&#20363;&#22914;&#20551;&#35774;&#38750;IID&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22823;&#22810;&#32570;&#20047;&#32508;&#21512;&#23454;&#39564;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ARFED&#30340;&#38450;&#24481;&#31639;&#27861;&#65292;&#23427;&#19981;&#23545;&#25968;&#25454;&#20998;&#24067;&#12289;&#21442;&#19982;&#32773;&#26356;&#26032;&#30340;&#30456;&#20284;&#24615;&#25110;&#24694;&#24847;&#21442;&#19982;&#32773;&#30340;&#27604;&#29575;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;ARFED&#20027;&#35201;&#32771;&#34385;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#27599;&#19968;&#23618;&#21442;&#19982;&#32773;&#26356;&#26032;&#30340;&#31163;&#32676;&#29366;&#24577;&#65292;&#22522;&#20110;&#21040;&#36716;&#25442;&#29256;&#26412;&#26356;&#26032;&#30340;&#36317;&#31163;&#65292;&#28040;&#38500;&#31163;&#32676;&#20540;&#12290;&#25105;&#20204;&#38024;&#23545;&#21253;&#25324;&#25968;&#25454;&#27745;&#26579;&#12289;&#27169;&#22411;&#27745;&#26579;&#21644;&#28151;&#21512;&#25915;&#20987;&#22312;&#20869;&#30340;&#21508;&#31181;&#25915;&#20987;&#65292;&#22312;&#22522;&#20934;&#32852;&#37030;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;ARFED&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ARFED&#22312;&#20445;&#25345;&#38750;IID&#21644;IID&#25968;&#25454;&#38598;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38450;&#25915;&#20987;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, each participant trains its local model with its own data and a global model is formed at a trusted server by aggregating model updates coming from these participants. Since the server has no effect and visibility on the training procedure of the participants to ensure privacy, the global model becomes vulnerable to attacks such as data poisoning and model poisoning. Although many defense algorithms have recently been proposed to address these attacks, they often make strong assumptions that do not agree with the nature of federated learning, such as assuming Non-IID datasets. Moreover, they mostly lack comprehensive experimental analyses. In this work, we propose a defense algorithm called ARFED that does not make any assumptions about data distribution, update similarity of participants, or the ratio of the malicious participants. ARFED mainly considers the outlier status of participant updates for each layer of the model architecture based on the distance to t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;(KAL)&#26694;&#26550;&#65292;&#23558;&#36890;&#29992;&#39046;&#22495;&#30693;&#35782;&#36716;&#25442;&#20026;&#36923;&#36753;&#32422;&#26463;&#65292;&#20316;&#20026;&#26679;&#26412;&#36873;&#25321;&#30340;&#25351;&#21335;&#65292;&#20351;&#38750;&#19987;&#19994;&#29992;&#25143;&#33021;&#22815;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#38477;&#20302;&#26631;&#35760;&#25968;&#25454;&#38656;&#27714;&#37327;&#30340;&#21516;&#26102;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2110.08265</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge-driven Active Learning. (arXiv:2110.08265v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;(KAL)&#26694;&#26550;&#65292;&#23558;&#36890;&#29992;&#39046;&#22495;&#30693;&#35782;&#36716;&#25442;&#20026;&#36923;&#36753;&#32422;&#26463;&#65292;&#20316;&#20026;&#26679;&#26412;&#36873;&#25321;&#30340;&#25351;&#21335;&#65292;&#20351;&#38750;&#19987;&#19994;&#29992;&#25143;&#33021;&#22815;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#38477;&#20302;&#26631;&#35760;&#25968;&#25454;&#38656;&#27714;&#37327;&#30340;&#21516;&#26102;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37096;&#32626;&#20173;&#28982;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#30417;&#30563;&#25968;&#25454;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26088;&#22312;&#26368;&#23567;&#21270;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;&#22823;&#22810;&#25968;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#36873;&#25321;&#65292;&#36890;&#24120;&#20165;&#38480;&#20110;&#20301;&#20110;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#26679;&#26412;&#12290;&#36825;&#20123;&#25216;&#26415;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#22522;&#20110;&#20869;&#23481;&#23545;&#25152;&#36873;&#26679;&#26412;&#30340;&#29702;&#35299;&#24182;&#19981;&#30452;&#35266;&#65292;&#36825;&#36827;&#19968;&#27493;&#23548;&#33268;&#38750;&#19987;&#19994;&#20154;&#22763;&#23558;&#28145;&#24230;&#23398;&#20064;&#35270;&#20026;&#40657;&#30418;&#23376;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#32771;&#34385;&#36890;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#20351;&#38750;&#19987;&#19994;&#29992;&#25143;&#33021;&#22815;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;(KAL)&#26694;&#26550;&#20013;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#30693;&#35782;&#34987;&#36716;&#25442;&#20026;&#36923;&#36753;&#32422;&#26463;&#65292;&#24182;&#26816;&#26597;&#20854;&#36829;&#21453;&#20316;&#20026;&#26679;&#26412;&#36873;&#25321;&#30340;&#33258;&#28982;&#25351;&#21335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#25968;&#25454;&#21644;&#36755;&#20986;&#31867;&#21035;&#20043;&#38388;&#30340;&#31616;&#21333;&#20851;&#31995;&#20063;&#21487;&#20197;&#25552;&#20379;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#26631;&#35760;&#25968;&#25454;&#38656;&#27714;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of Deep Learning (DL) models is still precluded in those contexts where the amount of supervised data is limited. To answer this issue, active learning strategies aim at minimizing the amount of labelled data required to train a DL model. Most active strategies are based on uncertain sample selection, and even often restricted to samples lying close to the decision boundary. These techniques are theoretically sound, but an understanding of the selected samples based on their content is not straightforward, further driving non-experts to consider DL as a black-box. For the first time, here we propose to take into consideration common domain-knowledge and enable non-expert users to train a model with fewer samples. In our Knowledge-driven Active Learning (KAL) framework, rule-based knowledge is converted into logic constraints and their violation is checked as a natural guide for sample selection. We show that even simple relationships among data and output classes offer a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#39068;&#33394;&#21464;&#25442;&#25915;&#20987;&#26041;&#27861;AdvCF&#65292;&#23427;&#36890;&#36807;&#31616;&#21333;&#33394;&#24425;&#28388;&#38236;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20855;&#26377;&#21487;&#21306;&#20998;&#20294;&#19981;&#24341;&#20154;&#27880;&#24847;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#25269;&#25239;&#23545;&#25239;&#24615;&#39068;&#33394;&#21464;&#25442;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2011.06690</link><description>&lt;p&gt;
&#26174;&#24335;&#33394;&#24425;&#28388;&#38236;&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#39068;&#33394;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Adversarial Image Color Transformations in Explicit Color Filter Space. (arXiv:2011.06690v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.06690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#39068;&#33394;&#21464;&#25442;&#25915;&#20987;&#26041;&#27861;AdvCF&#65292;&#23427;&#36890;&#36807;&#31616;&#21333;&#33394;&#24425;&#28388;&#38236;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20855;&#26377;&#21487;&#21306;&#20998;&#20294;&#19981;&#24341;&#20154;&#27880;&#24847;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#25269;&#25239;&#23545;&#25239;&#24615;&#39068;&#33394;&#21464;&#25442;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;&#30340;&#25915;&#20987;&#12290;&#20256;&#32479;&#25915;&#20987;&#26088;&#22312;&#20135;&#29983;&#26080;&#27861;&#21306;&#20998;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#65292;&#20294;&#25200;&#21160;&#21463;&#21040;&#20005;&#26684;&#38480;&#21046;&#12290;&#36817;&#26399;&#65292;&#30740;&#31350;&#32773;&#24320;&#22987;&#25506;&#32034;&#21487;&#21306;&#20998;&#20294;&#19981;&#24341;&#20154;&#27880;&#24847;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#65292;&#24182;&#35777;&#26126;&#39068;&#33394;&#21464;&#25442;&#25915;&#20987;&#26159;&#26377;&#25928;&#30340;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Adversarial Color Filter (AdvCF)&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#31616;&#21333;&#33394;&#24425;&#28388;&#38236;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#20449;&#24687;&#26469;&#20248;&#21270;&#30340;&#26032;&#22411;&#39068;&#33394;&#21464;&#25442;&#25915;&#20987;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26126;&#30830;&#25351;&#23450;&#20102;&#33394;&#24425;&#28388;&#38236;&#31354;&#38388;&#65292;&#20197;&#20415;&#21487;&#20197;&#25552;&#20379;&#23545;&#27169;&#22411;&#25269;&#25239;&#23545;&#25239;&#24615;&#39068;&#33394;&#21464;&#25442;&#30340;&#31995;&#32479;&#20998;&#26512;&#65292;&#20174;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#35282;&#24230;&#20998;&#26512;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29616;&#26377;&#30340;&#39068;&#33394;&#21464;&#25442;&#25915;&#20987;&#30001;&#20110;&#32570;&#20047;&#36825;&#26679;&#26126;&#30830;&#30340;&#31354;&#38388;&#32780;&#26080;&#27861;&#25552;&#20379;&#31995;&#32479;&#20998;&#26512;&#26426;&#20250;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;AdvCF&#22312;&#24858;&#24324;&#22270;&#20687;&#20998;&#31867;&#22120;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks have been shown to be vulnerable to adversarial images. Conventional attacks strive for indistinguishable adversarial images with strictly restricted perturbations. Recently, researchers have moved to explore distinguishable yet non-suspicious adversarial images and demonstrated that color transformation attacks are effective. In this work, we propose Adversarial Color Filter (AdvCF), a novel color transformation attack that is optimized with gradient information in the parameter space of a simple color filter. In particular, our color filter space is explicitly specified so that we are able to provide a systematic analysis of model robustness against adversarial color transformations, from both the attack and defense perspectives. In contrast, existing color transformation attacks do not offer the opportunity for systematic analysis due to the lack of such an explicit space. We further demonstrate the effectiveness of our AdvCF in fooling image classifiers and als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26041;&#27861;&#26469;&#35299;&#20915;&#27491;&#20132;&#32676;&#23376;&#32676;&#21516;&#27493;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24191;&#20041;&#24130;&#26041;&#27861;&#30340;&#36845;&#20195;&#20462;&#27491;&#21644;&#24688;&#24403;&#30340;&#21021;&#22987;&#27493;&#39588;&#21487;&#20197;&#22312;&#26576;&#20123;&#20551;&#35774;&#26465;&#20214;&#19979;&#33719;&#24471;&#36739;&#24378;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#31561;&#39046;&#22495;&#26377;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2009.07514</link><description>&lt;p&gt;
&#27491;&#20132;&#32676;&#23376;&#32676;&#21516;&#27493;&#38382;&#39064;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Synchronization Problems over Subgroups of the Orthogonal Group. (arXiv:2009.07514v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.07514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26041;&#27861;&#26469;&#35299;&#20915;&#27491;&#20132;&#32676;&#23376;&#32676;&#21516;&#27493;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24191;&#20041;&#24130;&#26041;&#27861;&#30340;&#36845;&#20195;&#20462;&#27491;&#21644;&#24688;&#24403;&#30340;&#21021;&#22987;&#27493;&#39588;&#21487;&#20197;&#22312;&#26576;&#20123;&#20551;&#35774;&#26465;&#20214;&#19979;&#33719;&#24471;&#36739;&#24378;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#31561;&#39046;&#22495;&#26377;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#27493;&#38382;&#39064;&#26088;&#22312;&#20272;&#35745;&#19968;&#32452;&#32676;&#20803;&#32032;$G^*_1,\dots,G^*_n\in\mathcal{G}$,&#22522;&#20110;&#24418;&#22914;$G^*_i{G^*_j}^{-1}$&#30340;&#25152;&#26377;&#25104;&#23545;&#27604;&#29575;&#23376;&#38598;&#30340;&#22024;&#26434;&#35266;&#27979;&#12290;&#26412;&#25991;&#32771;&#34385;&#32676;&#20026;&#27491;&#20132;&#32676;&#30340;&#21516;&#27493;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30001;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#27493;&#39588;&#21644;&#22522;&#20110;&#24191;&#20041;&#24130;&#27861;&#30340;&#36845;&#20195;&#32454;&#21270;&#27493;&#39588;&#32452;&#25104;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#23545;&#32676;&#12289;&#27979;&#37327;&#22270;&#12289;&#22122;&#22768;&#21644;&#21021;&#22987;&#21270;&#30340;&#26576;&#20123;&#20551;&#35774;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of synchronization over a group $\mathcal{G}$ aims to estimate a collection of group elements $G^*_1, \dots, G^*_n \in \mathcal{G}$ based on noisy observations of a subset of all pairwise ratios of the form $G^*_i {G^*_j}^{-1}$. Such a problem has gained much attention recently and finds many applications across a wide range of scientific and engineering areas. In this paper, we consider the class of synchronization problems in which the group is a closed subgroup of the orthogonal group. This class covers many group synchronization problems that arise in practice. Our contribution is fivefold. First, we propose a unified approach for solving this class of group synchronization problems, which consists of a suitable initialization step and an iterative refinement step based on the generalized power method, and show that it enjoys a strong theoretical guarantee on the estimation error under certain assumptions on the group, measurement graph, noise, and initialization. Secon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#30340;&#26159;&#25552;&#21319;&#23398;&#20064;&#22120;&#30340;&#26041;&#27861;&#65292;&#20851;&#27880;&#24369;&#23398;&#20064;&#22120;&#23646;&#20110;&#19968;&#20010;&#23481;&#37327;&#21463;&#38480;&#30340;&#31867;&#30340;&#20551;&#35774;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#38656;&#35201;&#22810;&#23569;&#20010;&#24369;&#23398;&#20064;&#22120;&#25165;&#33021;&#29983;&#25104;&#20934;&#30830;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#32422;$\tilde{O}({1}/{\gamma})$&#20010;&#24369;&#20551;&#35774;&#23601;&#33021;&#22815;&#35268;&#36991;&#32463;&#20856;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2001.11704</link><description>&lt;p&gt;
&#25552;&#21319;&#31616;&#21333;&#23398;&#20064;&#22120;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Simple Learners. (arXiv:2001.11704v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.11704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#30340;&#26159;&#25552;&#21319;&#23398;&#20064;&#22120;&#30340;&#26041;&#27861;&#65292;&#20851;&#27880;&#24369;&#23398;&#20064;&#22120;&#23646;&#20110;&#19968;&#20010;&#23481;&#37327;&#21463;&#38480;&#30340;&#31867;&#30340;&#20551;&#35774;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#38656;&#35201;&#22810;&#23569;&#20010;&#24369;&#23398;&#20064;&#22120;&#25165;&#33021;&#29983;&#25104;&#20934;&#30830;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#32422;$\tilde{O}({1}/{\gamma})$&#20010;&#24369;&#20551;&#35774;&#23601;&#33021;&#22815;&#35268;&#36991;&#32463;&#20856;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#26159;&#19968;&#31181;&#33879;&#21517;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#32467;&#21512;&#24369;&#19988;&#20855;&#26377;&#36866;&#24230;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#20197;&#29983;&#25104;&#24378;&#19988;&#20934;&#30830;&#30340;&#20551;&#35774;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#19968;&#31181;&#20551;&#35774;&#65306;&#24369;&#23398;&#20064;&#22120;&#23646;&#20110;&#19968;&#20010;&#26377;&#36793;&#30028;&#23481;&#37327;&#30340;&#31867;&#12290;&#35813;&#20551;&#35774;&#26469;&#33258;&#20110;&#24120;&#35265;&#30340;&#20256;&#32479;&#20570;&#27861;&#65292;&#21363;&#23558;&#24369;&#23398;&#20064;&#22120;&#35270;&#20026;&#8220;&#35268;&#21017;&#8221;&#25110;&#8220;&#22522;&#30784;&#31867;&#8221;&#20013;&#30340;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;Schapire&#21644;Freund'12&#12289;Shalev-Shwartz&#21644;Ben-David'14&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#24369;&#23398;&#20064;&#22120;&#30340;VC&#32500;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;i&#65289;Oracle&#22797;&#26434;&#24230;&#65306;&#38656;&#35201;&#22810;&#23569;&#20010;&#24369;&#23398;&#20064;&#22120;&#25165;&#33021;&#29983;&#25104;&#20934;&#30830;&#30340;&#20551;&#35774;&#65311;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#35268;&#36991;&#20102;Freund&#21644;Schapire&#30340;&#32463;&#20856;&#19979;&#30028;&#65288;'95&#65292;'12&#65289;&#12290;&#23613;&#31649;&#19979;&#30028;&#34920;&#26126;&#26377;&#26102;&#38656;&#35201;&#20855;&#26377;$\gamma$-&#38388;&#38553;&#30340;$\Omega({1}/{\gamma^2})$&#20010;&#24369;&#20551;&#35774;&#65292;&#20294;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#21482;&#38656;&#35201;&#25552;&#20379;&#32422;$\tilde{O}({1}/{\gamma})$&#20010;&#24369;&#20551;&#35774;&#65292;&#20551;&#35774;&#20854;
&lt;/p&gt;
&lt;p&gt;
Boosting is a celebrated machine learning approach which is based on the idea of combining weak and moderately inaccurate hypotheses to a strong and accurate one. We study boosting under the assumption that the weak hypotheses belong to a class of bounded capacity. This assumption is inspired by the common convention that weak hypotheses are "rules-of-thumbs" from an "easy-to-learn class". (Schapire and Freund~'12, Shalev-Shwartz and Ben-David '14.) Formally, we assume the class of weak hypotheses has a bounded VC dimension. We focus on two main questions: (i) Oracle Complexity: How many weak hypotheses are needed to produce an accurate hypothesis? We design a novel boosting algorithm and demonstrate that it circumvents a classical lower bound by Freund and Schapire ('95, '12). Whereas the lower bound shows that $\Omega({1}/{\gamma^2})$ weak hypotheses with $\gamma$-margin are sometimes necessary, our new method requires only $\tilde{O}({1}/{\gamma})$ weak hypothesis, provided that the
&lt;/p&gt;</description></item></channel></rss>