<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01327</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#20013;&#30340;&#30417;&#30563;&#31639;&#27861;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Supervised Algorithmic Fairness in Distribution Shifts: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#38754;&#23545;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#65292;&#22914;&#20309;&#20445;&#25345;&#20844;&#24179;&#21644;&#26080;&#20559;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#22240;&#21508;&#31181;&#22240;&#32032;&#32780;&#38543;&#26102;&#38388;&#21457;&#29983;&#21464;&#21270;&#12290;&#36825;&#31181;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#65292;&#23545;&#29305;&#23450;&#36890;&#36807;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#26469;&#34920;&#24449;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#22343;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#20102;&#24635;&#32467;&#65292;&#24182;&#20840;&#38754;&#35843;&#26597;&#20102;&#22522;&#20110;&#36825;&#20123;&#21464;&#21270;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#25991;&#29486;&#20013;&#31361;&#20986;&#20102;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20221;&#35843;&#26597;&#21015;&#20986;&#20102;&#29992;&#20110;&#23454;&#35777;&#30740;&#31350;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
&lt;/p&gt;</description></item><item><title>EmSHAP&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;GRU&#26469;&#28040;&#38500;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;Shapley&#20540;&#36129;&#29486;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.01078</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#20934;&#30830;Shapley&#20540;&#20272;&#35745;&#29992;&#20110;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01078
&lt;/p&gt;
&lt;p&gt;
EmSHAP&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;GRU&#26469;&#28040;&#38500;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;Shapley&#20540;&#36129;&#29486;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26377;&#21033;&#24037;&#20855;&#65292;Shapley&#20540;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#36127;&#36733;&#38543;&#30528;&#36755;&#20837;&#29305;&#24449;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#20272;&#35745;Shapley&#20540;&#26159;&#19968;&#39033;&#22256;&#38590;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21152;&#36895;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#24517;&#39035;&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EmSHAP&#65288;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#65289;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#39044;&#26399;Shapley&#36129;&#29486;&#20989;&#25968;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#32473;&#20986;&#20854;&#20313;&#29305;&#24449;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#30830;&#23450;&#33021;&#37327;&#27169;&#22411;&#20013;&#30340;&#25552;&#35758;&#26465;&#20214;&#20998;&#24067;&#65292;&#24341;&#20837;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#29305;&#24449;&#26144;&#23556;&#21040;&#38544;&#34255;&#31354;&#38388;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#21160;&#24577;&#25513;&#34109;&#26041;&#26696;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01078v1 Announce Type: new  Abstract: As a favorable tool for explainable artificial intelligence (XAI), Shapley value has been widely used to interpret deep learning based predictive models. However, accurate and efficient estimation of Shapley value is a difficult task since the computation load grows exponentially with the increase of input features. Most existing accelerated Shapley value estimation methods have to compromise on estimation accuracy with efficiency. In this article, we present EmSHAP(Energy model-based Shapley value estimation), which can effectively approximate the expectation of Shapley contribution function/deep learning model under arbitrary subset of features given the rest. In order to determine the proposal conditional distribution in the energy model, a gated recurrent unit(GRU) is introduced by mapping the input features onto a hidden space, so that the impact of input feature orderings can be eliminated. In addition, a dynamic masking scheme is 
&lt;/p&gt;</description></item><item><title>&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00897</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#40065;&#26834;&#24615;&#65306;&#20837;&#38376;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Robustness: A Primer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#31283;&#20581;&#24615;&#30340;&#22522;&#26412;&#27010;&#24565;&#21450;&#20854;&#22312;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#35752;&#35770;&#22987;&#20110;&#31283;&#20581;&#24615;&#30340;&#35814;&#32454;&#23450;&#20041;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;ML&#27169;&#22411;&#22312;&#21508;&#31181;&#19981;&#21516;&#21644;&#24847;&#22806;&#30340;&#29615;&#22659;&#26465;&#20214;&#19979;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;ML&#40065;&#26834;&#24615;&#36890;&#36807;&#22810;&#20010;&#35270;&#35282;&#36827;&#34892;&#20102;&#21078;&#26512;&#65306;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20114;&#34917;&#24615;&#65307;&#20854;&#20316;&#20026;&#21487;&#20449;AI&#30340;&#35201;&#27714;&#65307;&#20854;&#23545;&#25239;&#24615;&#19982;&#38750;&#23545;&#25239;&#24615;&#26041;&#38754;&#65307;&#20854;&#25968;&#37327;&#21270;&#25351;&#26631;&#65307;&#20197;&#21450;&#20854;&#21487;&#22797;&#29616;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#25351;&#26631;&#12290;&#31456;&#33410;&#28145;&#20837;&#25506;&#35752;&#20102;&#24433;&#21709;&#40065;&#26834;&#24615;&#30340;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#20559;&#24046;&#12289;&#27169;&#22411;&#22797;&#26434;&#24615;&#20197;&#21450;ML&#27969;&#31243;&#19981;&#26126;&#30830;&#30340;&#39118;&#38505;&#12290;&#23427;&#20174;&#24191;&#27867;&#30340;&#35270;&#35282;&#35843;&#26597;&#20102;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21253;&#25324;&#25968;&#23383;&#21644;&#29289;&#29702;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00897v1 Announce Type: cross  Abstract: This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25299;&#25169;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#23545;Transformer&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20855;&#26377;&#39640;&#38454;&#25512;&#29702;&#30340;&#29305;&#28857;&#65292;&#24182;&#19982;&#20854;&#20182;&#24120;&#35265;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>https://arxiv.org/abs/2403.18415</link><description>&lt;p&gt;
Transformer&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Topos of Transformer Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18415
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#23545;Transformer&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20855;&#26377;&#39640;&#38454;&#25512;&#29702;&#30340;&#29305;&#28857;&#65292;&#24182;&#19982;&#20854;&#20182;&#24120;&#35265;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#36828;&#36828;&#36229;&#36234;&#25152;&#26377;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32972;&#21518;&#30340;&#24341;&#25806;&#12290;&#25105;&#20204;&#36890;&#36807;&#25299;&#25169;&#29702;&#35770;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;Transformer&#26550;&#26500;&#34920;&#36798;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#20174;&#36825;&#20010;&#35266;&#28857;&#20986;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#24120;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;&#21367;&#31215;&#32593;&#32476;&#12289;&#24490;&#29615;&#32593;&#32476;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21487;&#20197;&#23884;&#20837;&#22312;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#39044;&#25299;&#25169;&#20013;&#65292;&#20294;Transformer&#24517;&#28982;&#23384;&#22312;&#20110;&#20854;&#25299;&#25169;&#23436;&#22791;&#24615;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#34920;&#26126;&#36825;&#20004;&#20010;&#32593;&#32476;&#23478;&#26063;&#23454;&#20363;&#21270;&#20102;&#19981;&#21516;&#30340;&#36923;&#36753;&#29255;&#27573;&#65306;&#21069;&#32773;&#26159;&#19968;&#38454;&#30340;&#65292;&#32780;Transformers&#26159;&#39640;&#38454;&#25512;&#29702;&#26426;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25299;&#25169;&#29702;&#35770;&#19982;&#26550;&#26500;&#25628;&#32034;&#21644;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20102;&#31867;&#27604;&#65292;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#32435;&#20837;&#20102;&#25511;&#21046;&#35770;&#20195;&#29702;&#30340;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18415v1 Announce Type: new  Abstract: The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models. We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22836;&#37096;&#23039;&#21183;&#20272;&#35745;&#20013;&#30340;&#22352;&#26631;&#31995;&#21644;&#27431;&#25289;&#35282;&#39034;&#24207;&#23450;&#20041;&#38382;&#39064;&#65292;&#39564;&#35777;&#20102;&#20043;&#21069;&#20316;&#21697;&#20013;&#20351;&#29992;&#30340;&#23039;&#21183;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#21644;&#32472;&#22270;&#20363;&#31243;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18104</link><description>&lt;p&gt;
&#25968;&#23398;&#22522;&#30784;&#21644;&#23436;&#25972;&#22836;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Mathematical Foundation and Corrections for Full Range Head Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22836;&#37096;&#23039;&#21183;&#20272;&#35745;&#20013;&#30340;&#22352;&#26631;&#31995;&#21644;&#27431;&#25289;&#35282;&#39034;&#24207;&#23450;&#20041;&#38382;&#39064;&#65292;&#39564;&#35777;&#20102;&#20043;&#21069;&#20316;&#21697;&#20013;&#20351;&#29992;&#30340;&#23039;&#21183;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#21644;&#32472;&#22270;&#20363;&#31243;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26377;&#20851;&#22836;&#37096;&#23039;&#21183;&#20272;&#35745;&#65288;HPE&#65289;&#30340;&#20316;&#21697;&#25552;&#20379;&#20102;&#29992;&#20110;&#20174;&#38754;&#37096;&#20851;&#38190;&#28857;&#25110;&#30452;&#25509;&#20174;&#22836;&#37096;&#21306;&#22495;&#22270;&#20687;&#20013;&#25552;&#21462;&#27431;&#25289;&#35282;&#30340;&#31639;&#27861;&#25110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20316;&#21697;&#26410;&#25552;&#20379;&#25152;&#20351;&#29992;&#30340;&#22352;&#26631;&#31995;&#21644;&#27431;&#25289;&#25110;Tait-Bryan&#35282;&#24230;&#39034;&#24207;&#30340;&#28165;&#26224;&#23450;&#20041;&#12290;&#26412;&#25991;&#24443;&#24213;&#26816;&#26597;&#20102;300W-LP&#25968;&#25454;&#38598;&#20013;&#23450;&#20041;&#30340;&#27431;&#25289;&#35282;&#65292;&#22836;&#37096;&#23039;&#21183;&#20272;&#35745;&#22914;3DDFA-v2&#12289;6D-RepNet&#12289;WHENet&#31561;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#27431;&#25289;&#35282;&#32472;&#21046;&#20363;&#31243;&#30340;&#26377;&#25928;&#24615;&#12290;&#24517;&#35201;&#26102;&#65292;&#25105;&#20204;&#25512;&#26029;&#23427;&#20204;&#30340;&#22352;&#26631;&#31995;&#21644;&#20559;&#33322;&#12289;&#27178;&#28378;&#12289;&#20463;&#20208;&#30340;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18104v1 Announce Type: cross  Abstract: Numerous works concerning head pose estimation (HPE) offer algorithms or proposed neural network-based approaches for extracting Euler angles from either facial key points or directly from images of the head region. However, many works failed to provide clear definitions of the coordinate systems and Euler or Tait-Bryan angles orders in use. It is a well-known fact that rotation matrices depend on coordinate systems, and yaw, roll, and pitch angles are sensitive to their application order. Without precise definitions, it becomes challenging to validate the correctness of the output head pose and drawing routines employed in prior works. In this paper, we thoroughly examined the Euler angles defined in the 300W-LP dataset, head pose estimation such as 3DDFA-v2, 6D-RepNet, WHENet, etc, and the validity of their drawing routines of the Euler angles. When necessary, we infer their coordinate system and sequence of yaw, roll, pitch from pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15246</link><description>&lt;p&gt;
FollowIR: &#35780;&#20272;&#21644;&#25945;&#25480;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#20197;&#36981;&#24490;&#35828;&#26126;&#20070;
&lt;/p&gt;
&lt;p&gt;
FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36981;&#24490;&#38271;&#19988;&#22797;&#26434;&#30340;&#35828;&#26126;&#20070;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#20351;&#29992;LLMs&#20316;&#20026;&#20854;&#26550;&#26500;&#30340;&#25903;&#26609;&#65292;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#21482;&#25509;&#21463;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#27809;&#26377;&#35828;&#26126;&#20070;&#12290;&#23545;&#20110;&#26368;&#36817;&#19968;&#20123;&#25509;&#21463;&#35828;&#26126;&#20070;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#23427;&#20204;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#35828;&#26126;&#20070;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;IR&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#22320;&#36981;&#24490;&#29616;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;FollowIR&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#24736;&#20037;&#21382;&#21490;&#65306;&#27491;&#22914;TREC&#20026;&#20154;&#31867;&#26631;&#27880;&#21592;&#25552;&#20379;&#35828;&#26126;&#20070;&#65288;&#20063;&#31216;&#20026;&#21465;&#36848;&#65289;&#26469;&#21028;&#26029;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#19968;&#26679;&#65292;&#22240;&#27492;IR&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#36825;&#20123;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#30830;&#23450;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20934;&#20174;&#19977;&#20010;&#32463;&#36807;&#28145;&#24230;&#21028;&#26029;&#30340;TREC&#25910;&#34255;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15246v1 Announce Type: cross  Abstract: Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation benchmark starts with three deeply judged TREC collections and al
&lt;/p&gt;</description></item><item><title>ATLAS&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#34920;&#26684;&#25968;&#25454;&#35774;&#35745;&#30340;&#20219;&#24847;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#36807;&#28388;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#35757;&#32451;-free&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#26550;&#26500;&#35780;&#20272;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.10318</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20219;&#24847;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Anytime Neural Architecture Search on Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10318
&lt;/p&gt;
&lt;p&gt;
ATLAS&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#34920;&#26684;&#25968;&#25454;&#35774;&#35745;&#30340;&#20219;&#24847;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#36807;&#28388;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#35757;&#32451;-free&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#26550;&#26500;&#35780;&#20272;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20174;&#25163;&#21160;&#26550;&#26500;&#35774;&#35745;&#36716;&#21464;&#20026;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;(NAS)&#12290;&#36825;&#31181;&#36716;&#21464;&#38656;&#35201;&#19968;&#31181;&#39640;&#25928;&#19988;&#21709;&#24212;&#28789;&#25935;&#30340;&#20219;&#24847;&#26102;&#38388;NAS&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#26102;&#38388;&#39044;&#31639;&#20869;&#36820;&#22238;&#24403;&#21069;&#30340;&#26368;&#20339;&#26550;&#26500;&#65292;&#24182;&#38543;&#30528;&#39044;&#31639;&#20998;&#37197;&#30340;&#22686;&#21152;&#36880;&#28176;&#25552;&#39640;&#26550;&#26500;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20219;&#24847;&#26102;&#38388;NAS&#39046;&#22495;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ATLAS&#65292;&#31532;&#19968;&#20010;&#19987;&#20026;&#34920;&#26684;&#25968;&#25454;&#37327;&#36523;&#23450;&#21046;&#30340;&#20219;&#24847;&#26102;&#38388;NAS&#26041;&#27861;&#12290;ATLAS&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#36807;&#28388;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#35757;&#32451;-free&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#26550;&#26500;&#35780;&#20272;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36807;&#28388;&#38454;&#27573;&#65292;ATLAS&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#20026;&#34920;&#26684;&#25968;&#25454;&#19987;&#38376;&#35774;&#35745;&#30340;&#38646;&#25104;&#26412;&#20195;&#29702;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#20505;&#36873;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10318v1 Announce Type: new  Abstract: The increasing demand for tabular data analysis calls for transitioning from manual architecture design to Neural Architecture Search (NAS). This transition demands an efficient and responsive anytime NAS approach that is capable of returning current optimal architectures within any given time budget while progressively enhancing architecture quality with increased budget allocation. However, the area of research on Anytime NAS for tabular data remains unexplored. To this end, we introduce ATLAS, the first anytime NAS approach tailored for tabular data. ATLAS introduces a novel two-phase filtering-and-refinement optimization scheme with joint optimization, combining the strengths of both paradigms of training-free and training-based architecture evaluation. Specifically, in the filtering phase, ATLAS employs a new zero-cost proxy specifically designed for tabular data to efficiently estimate the performance of candidate architectures, th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#36890;&#36807;PAC-&#33976;&#39311;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#25277;&#21462;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26435;&#37325;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#33976;&#39311;&#27604;&#20174;&#22836;&#23398;&#20064;&#26356;&#20415;&#23452;&#19988;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09053</link><description>&lt;p&gt;
&#36208;&#21521;&#27169;&#22411;&#33976;&#39311;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Towards a theory of model distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09053
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#36890;&#36807;PAC-&#33976;&#39311;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#25277;&#21462;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26435;&#37325;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#33976;&#39311;&#27604;&#20174;&#22836;&#23398;&#20064;&#26356;&#20415;&#23452;&#19988;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33976;&#39311;&#26159;&#23558;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26367;&#25442;&#20026;&#31616;&#21270;&#27169;&#22411;&#26469;&#36817;&#20284;&#21407;&#27169;&#22411;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#20851;&#20110;&#27169;&#22411;&#33976;&#39311;&#30340;&#31243;&#24230;&#12289;&#25152;&#38656;&#36816;&#34892;&#26102;&#38388;&#21644;&#25968;&#25454;&#37327;&#30340;&#22522;&#26412;&#38382;&#39064;&#20173;&#28982;&#22823;&#22810;&#26410;&#35299;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#22987;&#20102;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;PAC-&#33976;&#39311; [Val84]&#65292;&#25552;&#20986;&#20102;&#25552;&#21462;&#35757;&#32451;&#26435;&#37325;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#8220;&#32447;&#24615;&#34920;&#31034;&#20551;&#35774;&#8221;&#23558;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#22320;&#33976;&#39311;&#25104;&#31616;&#26126;&#26126;&#20102;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#65292;&#36824;&#35777;&#26126;&#20102;&#33976;&#39311;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#20415;&#23452;&#24471;&#22810;&#65292;&#24182;&#22312;&#34920;&#24449;&#20854;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09053v1 Announce Type: cross  Abstract: Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2403.06659</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#27979;&#35797;&#26102;&#20020;&#24202;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06659
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#23454;&#36341;&#20013;&#29992;&#20110;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#30142;&#30149;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#12290;&#22312;&#26410;&#32463;&#27880;&#37322;&#30340;ECG&#25968;&#25454;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;eSSL&#65289;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#34920;&#24449;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#21487;&#20197;&#22312;&#25253;&#21578;&#20013;&#25214;&#21040;&#30340;&#20020;&#24202;&#30693;&#35782;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#27169;&#24577;&#23398;&#20064;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#65292;&#25552;&#20986;&#20102;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#38646;&#26679;&#26412;ECG&#20998;&#31867;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21033;&#29992;&#22806;&#37096;&#19987;&#23478;&#39564;&#35777;&#30340;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#65292;&#29983;&#25104;&#26356;&#22810;&#20851;&#20110;&#24739;&#32773;&#30149;&#21490;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06659v1 Announce Type: cross  Abstract: Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Fed&#30340;&#31616;&#21333;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#37325;&#25773;&#65292;&#36890;&#36807;&#21327;&#35843;&#27599;&#20010;&#23458;&#25143;&#31471;&#32531;&#23384;&#37325;&#35201;&#26679;&#26412;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05890</link><description>&lt;p&gt;
&#26397;&#30528;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#20013;&#39640;&#25928;&#30340;&#37325;&#25773;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Replay in Federated Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Fed&#30340;&#31616;&#21333;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#37325;&#25773;&#65292;&#36890;&#36807;&#21327;&#35843;&#27599;&#20010;&#23458;&#25143;&#31471;&#32531;&#23384;&#37325;&#35201;&#26679;&#26412;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#36890;&#24120;&#20551;&#23450;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#26159;&#22266;&#23450;&#25110;&#38745;&#24577;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#36890;&#24120;&#20197;&#22686;&#37327;&#26041;&#24335;&#21040;&#26469;&#65292;&#20854;&#20013;&#25968;&#25454;&#39046;&#22495;&#21487;&#33021;&#21160;&#24577;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36793;&#32536;&#23458;&#25143;&#31471;&#22312;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#65288;FIL&#65289;&#22330;&#26223;&#20013;&#22240;&#25968;&#25454;&#24322;&#26500;&#24615;&#32780;&#21487;&#33021;&#32570;&#20047;&#36275;&#22815;&#23384;&#20648;&#31354;&#38388;&#20197;&#20445;&#30041;&#23436;&#25972;&#25968;&#25454;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Fed&#30340;&#31616;&#21333;&#12289;&#36890;&#29992;&#30340;FIL&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#21327;&#35843;&#27599;&#20010;&#23458;&#25143;&#31471;&#32531;&#23384;&#37325;&#25773;&#30340;&#37325;&#35201;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20986;&#29616;&#26032;&#20219;&#21153;&#26102;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#39318;&#20808;&#22522;&#20110;&#23427;&#20204;&#30340;&#20840;&#23616;&#21644;&#26412;&#22320;&#37325;&#35201;&#24615;&#32531;&#23384;&#36873;&#23450;&#30340;&#20808;&#21069;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#23458;&#25143;&#31471;&#20351;&#29992;&#26082;&#32531;&#23384;&#30340;&#26679;&#26412;&#21448;&#20351;&#29992;&#26032;&#20219;&#21153;&#30340;&#26679;&#26412;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;Re-Fed&#21457;&#29616;&#37325;&#25773;&#37325;&#35201;&#26679;&#26412;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05890v1 Announce Type: new  Abstract: In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04306</link><description>&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Effectiveness Assessment of Recent Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#20986;&#29616;&#20195;&#34920;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#31243;&#24230;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#27969;&#34892;&#30340;LVLMs&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#22312;&#19987;&#19994;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#30340;&#20840;&#38754;&#27979;&#35797;&#24179;&#21488;&#65292;&#28085;&#30422;&#20845;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#26174;&#33879;&#12289;&#20266;&#35013;&#21644;&#36879;&#26126;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#24687;&#32905;&#21644;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;&#65292;&#20197;&#21450;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#26368;&#36817;&#19977;&#31181;&#24320;&#28304;LVLMs--MiniGPT-v2&#12289;LLaVA-1.5&#21644;Shikra--&#22312;&#35270;&#35273;&#35782;&#21035;&#21644;&#23450;&#20301;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04161</link><description>&lt;p&gt;
SWAP-NAS: &#36866;&#29992;&#20110;&#36229;&#24555;&#36895;NAS&#30340;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#65288;&#21363;&#38646;&#25104;&#26412;&#20195;&#29702;&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36991;&#20813;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#23588;&#20854;&#26159;&#22312;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#65292;&#27604;&#22914;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#20851;&#32852;&#24615;&#21644;&#24046;&#21170;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;&#21450;&#20854;&#34893;&#29983;&#29289;SWAP-Score&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;&#23427;&#27979;&#37327;&#20102;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;SWAP-Score&#19982;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#30340;&#30495;&#23454;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#22312;NAS-Bench-101/201/301&#21644;TransNAS-Bench-101&#19978;&#32988;&#36807;&#20102;15&#31181;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;SWAP-Score&#21487;&#20197;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#36825;&#22312;&#22522;&#20110;&#21333;&#20803;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#25511;&#21046;&#12290;&#20363;&#22914;&#65292;Spearman&#30340;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04161v1 Announce Type: new  Abstract: Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman's rank
&lt;/p&gt;</description></item><item><title>&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02622</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#39033;&#21021;&#27493;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
World Models for Autonomous Driving: An Initial Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02622
&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#35780;&#20272;&#20854;&#24433;&#21709;&#23545;&#20110;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#20851;&#38190;&#22320;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#12290;&#19990;&#30028;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#20351;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#33021;&#22815;&#32508;&#21512;&#21644;&#35299;&#37322;&#22823;&#37327;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20174;&#32780;&#39044;&#27979;&#28508;&#22312;&#30340;&#26410;&#26469;&#24773;&#26223;&#24182;&#24357;&#34917;&#20449;&#24687;&#32570;&#21475;&#12290;&#26412;&#25991;&#23545;&#33258;&#20027;&#39550;&#39542;&#20013;&#19990;&#30028;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#21457;&#23637;&#36827;&#34892;&#20102;&#21021;&#27493;&#23457;&#26597;&#65292;&#28085;&#30422;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#24212;&#29992;&#20197;&#21450;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#38480;&#21046;&#30340;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#24378;&#35843;&#20102;&#19990;&#30028;&#27169;&#22411;&#22312;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#25104;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#22522;&#30784;&#21442;&#32771;&#65292;&#20415;&#20110;&#24555;&#36895;&#33719;&#24471;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02622v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and com
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.01384</link><description>&lt;p&gt;
&#20851;&#20110;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Compressibility of Quantized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01384
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21040;&#36793;&#32536;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#22914;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#21644;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#24212;&#29992;&#20110;&#20943;&#23569;&#25968;&#25454;&#31227;&#21160;&#65292;&#20174;&#32780;&#21152;&#36895;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#37327;&#21270;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01384v1 Announce Type: cross  Abstract: Deploying Large Language Models (LLMs) on edge or mobile devices offers significant benefits, such as enhanced data privacy and real-time processing capabilities. However, it also faces critical challenges due to the substantial memory requirement of LLMs. Quantization is an effective way of reducing the model size while maintaining good performance. However, even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference. In this case, the I/O latency of model loading becomes the bottleneck of the LLM inference latency. In this work, we take a preliminary step of studying applying data compression techniques to reduce data movement and thus speed up the inference of quantized LLM on memory-constrained devices. In particular, we discussed the compressibility of quantized LLMs, the trade-off between the compres
&lt;/p&gt;</description></item><item><title>&#23558;$\varepsilon$-greedy&#31574;&#30053;&#24341;&#20837;Thompson&#37319;&#26679;&#20197;&#25913;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#24320;&#21457;&#21151;&#33021;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00540</link><description>&lt;p&gt;
Epsilon-Greedy Thompson Sampling&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Epsilon-Greedy Thompson Sampling to Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00540
&lt;/p&gt;
&lt;p&gt;
&#23558;$\varepsilon$-greedy&#31574;&#30053;&#24341;&#20837;Thompson&#37319;&#26679;&#20197;&#25913;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#24320;&#21457;&#21151;&#33021;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Thompson&#37319;&#26679;&#65288;TS&#65289;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24320;&#21457;-&#25506;&#32034;&#22256;&#22659;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#34429;&#28982;&#23427;&#36890;&#36807;&#38543;&#26426;&#29983;&#25104;&#21644;&#26368;&#22823;&#21270;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21518;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#26469;&#20248;&#20808;&#36827;&#34892;&#25506;&#32034;&#65292;&#20294;TS&#22312;&#27599;&#27425;&#25191;&#34892;&#25506;&#32034;&#21518;&#36890;&#36807;&#25910;&#38598;&#20851;&#20110;&#30495;&#23454;&#30446;&#26631;&#20989;&#25968;&#30340;&#20449;&#24687;&#26469;&#24369;&#21270;&#20854;&#24320;&#21457;&#21151;&#33021;&#12290; &#26412;&#30740;&#31350;&#23558;&#22312;TS&#20013;&#24341;&#20837;$\varepsilon$-greedy&#31574;&#30053;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25913;&#36827;&#20854;&#24320;&#21457;&#21151;&#33021;&#12290; &#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;TS&#24212;&#29992;&#20110;BO&#30340;&#20004;&#20010;&#26497;&#31471;&#65292;&#21363;&#36890;&#29992;TS&#21644;&#26679;&#26412;&#24179;&#22343;TS&#12290;&#21069;&#32773;&#21644;&#21518;&#32773;&#20998;&#21035;&#25552;&#20513;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290; &#28982;&#21518;&#25105;&#20204;&#20351;&#29992;$\varepsilon$-greedy&#31574;&#30053;&#22312;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#38543;&#26426;&#20999;&#25442;&#12290; $\varepsilon \in (0,1)$&#30340;&#23567;&#20540;&#20248;&#20808;&#32771;&#34385;&#24320;&#21457;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290; &#25105;&#20204;&#23454;&#35777;&#34920;&#26126;$\varepsilon$-greedy T
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00540v1 Announce Type: new  Abstract: Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\varepsilon \in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\varepsilon$-greedy T
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;LLMs&#36827;&#34892;&#26426;&#26800;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#26102;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#36335;&#24452;&#29983;&#25104;&#31572;&#26696;&#65292;&#21516;&#26102;&#23384;&#22312;&#21151;&#33021;&#24615;&#20998;&#27495;&#12290;</title><link>https://arxiv.org/abs/2402.18312</link><description>&lt;p&gt;
&#22914;&#20309;&#36880;&#27493;&#24605;&#32771;&#65306;&#23545;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#26426;&#26800;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18312
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;LLMs&#36827;&#34892;&#26426;&#26800;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#26102;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#36335;&#24452;&#29983;&#25104;&#31572;&#26696;&#65292;&#21516;&#26102;&#23384;&#22312;&#21151;&#33021;&#24615;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#65292;&#20294;&#23545;&#20110;&#20419;&#36827;CoT&#29983;&#25104;&#30340;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#20173;&#23384;&#22312;&#32570;&#20047;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#26426;&#26800;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;LLMs&#20013;&#34920;&#29616;&#20986;CoT&#25512;&#29702;&#30340;&#31070;&#32463;&#23376;&#32467;&#26500;&#12290;&#36890;&#36807;&#23545;LLaMA-2 7B&#24212;&#29992;&#20110;&#34394;&#26500;&#26412;&#20307;&#35770;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#20026;&#36880;&#27493;&#25512;&#29702;&#37096;&#32626;&#20102;&#22810;&#20010;&#24182;&#34892;&#31572;&#26696;&#29983;&#25104;&#36335;&#24452;&#12290;&#36825;&#20123;&#24182;&#34892;&#36335;&#24452;&#25552;&#20379;&#20102;&#26469;&#33258;&#36755;&#20837;&#38382;&#39064;&#19978;&#19979;&#25991;&#20197;&#21450;&#29983;&#25104;&#30340;CoT&#30340;&#24207;&#36143;&#31572;&#26696;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#20013;&#38388;&#23618;&#23384;&#22312;&#24341;&#20154;&#30633;&#30446;&#30340;&#21151;&#33021;&#20998;&#27495;&#12290;&#21021;&#22987;&#19968;&#21322;&#30340;&#20196;&#29260;&#34920;&#31034;&#20173;&#28982;&#24378;&#28872;&#20559;&#21521;&#39044;&#35757;&#32451;&#20808;&#39564;&#65292;&#32780;&#21518;&#21322;&#37096;&#20998;&#31361;&#28982;&#34987;&#19978;&#19979;&#25991;&#25152;&#21462;&#20195;&#12290;&#36825;&#31181;&#20869;&#37096;&#30456;&#20301;&#36716;&#21464;&#22312;&#19981;&#21516;&#30340;&#21151;&#33021;&#21327;&#21516;&#20013;&#20307;&#29616;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18312v1 Announce Type: new  Abstract: Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a striking functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context taking over abruptly in the later half. This internal phase shift manifests in different functional co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HSTU&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#39640;&#36798;65.8&#65285;&#30340;NDCG&#65292;&#24182;&#19988;&#27604;&#22522;&#20110;FlashAttention2&#30340;Transformer&#22312;8192&#38271;&#24230;&#24207;&#21015;&#19978;&#24555;5.3&#20493;&#21040;15.2&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.17152</link><description>&lt;p&gt;
&#34892;&#21160;&#32988;&#36807;&#35328;&#36766;&#65306;&#29992;&#20110;&#29983;&#25104;&#25512;&#33616;&#30340;&#21315;&#20159;&#21442;&#25968;&#39034;&#24207;&#36716;&#23548;&#22120;
&lt;/p&gt;
&lt;p&gt;
Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17152
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HSTU&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#39640;&#36798;65.8&#65285;&#30340;NDCG&#65292;&#24182;&#19988;&#27604;&#22522;&#20110;FlashAttention2&#30340;Transformer&#22312;8192&#38271;&#24230;&#24207;&#21015;&#19978;&#24555;5.3&#20493;&#21040;15.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#29305;&#28857;&#26159;&#20381;&#36182;&#20110;&#39640;&#22522;&#25968;&#12289;&#24322;&#26500;&#29305;&#24449;&#65292;&#24182;&#19988;&#38656;&#35201;&#27599;&#22825;&#22788;&#29702;&#25968;&#21313;&#20159;&#29992;&#25143;&#34892;&#20026;&#12290;&#23613;&#31649;&#22312;&#25104;&#21315;&#19978;&#19975;&#20010;&#29305;&#24449;&#19978;&#35757;&#32451;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#22823;&#22810;&#25968;&#34892;&#19994;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;(DLRMs)&#22312;&#35745;&#31639;&#26041;&#38754;&#26080;&#27861;&#25193;&#23637;&#12290;&#21463;&#21040;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#20013;&#30340;&#39034;&#24207;&#36716;&#23548;&#20219;&#21153;&#65288;&#8220;&#29983;&#25104;&#25512;&#33616;&#32773;&#8221;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#35774;&#35745;&#30340;&#26032;&#26550;&#26500;HSTU&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17152v1 Announce Type: new  Abstract: Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   HSTU outperforms baselines over synthetic and public datasets by up to 65.8\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Gener
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.16310</link><description>&lt;p&gt;
REPLAY: &#23545;&#31232;&#30095;&#36712;&#36857;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#39044;&#27979;&#26159;&#26681;&#25454;&#21382;&#21490;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#26469;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#30340;&#22266;&#26377;&#31232;&#30095;&#38382;&#39064;&#65292;&#26102;&#31354;&#19978;&#19979;&#25991;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#26159;&#23558;&#20301;&#32622;&#20043;&#38388;&#30340;&#26102;&#31354;&#36317;&#31163;&#32435;&#20837;&#21040;&#31227;&#21160;&#36712;&#36857;&#20013;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#25552;&#20379;&#32473;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#26469;&#23547;&#25214;&#26377;&#20449;&#24687;&#30340;&#36807;&#21435;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#26410;&#33021;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#20363;&#22914;&#65292;&#20154;&#31867;&#31227;&#21160;&#22312;&#26089;&#26216;&#36890;&#24120;&#27604;&#20854;&#20182;&#26102;&#38388;&#26356;&#26377;&#35268;&#24459;&#65307;&#36825;&#26263;&#31034;&#20102;&#23454;&#38469;&#26102;&#38388;&#25139;&#30340;&#26377;&#29992;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPLAY&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;RNN&#26550;&#26500;&#65292;&#26088;&#22312;&#25429;&#25417;&#26102;&#38388;&#21464;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#35268;&#24459;&#20197;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
&lt;/p&gt;</description></item><item><title>&#20219;&#24847;&#31934;&#24230;LLM&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#22823;&#23567;LLMs&#37327;&#21270;&#20026;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;...&#65292;n&#20301;&#65289;&#24182;&#21472;&#21152;&#21040;&#20869;&#23384;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLMs&#30340;&#39640;&#25104;&#26412;</title><link>https://arxiv.org/abs/2402.10517</link><description>&lt;p&gt;
&#20219;&#24847;&#31934;&#24230;LLM&#65306;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLM&#30340;&#20302;&#25104;&#26412;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10517
&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#31934;&#24230;LLM&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#22823;&#23567;LLMs&#37327;&#21270;&#20026;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;...&#65292;n&#20301;&#65289;&#24182;&#21472;&#21152;&#21040;&#20869;&#23384;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLMs&#30340;&#39640;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#65292;&#36825;&#20123;LLMs&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#31361;&#30772;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#20307;&#31215;&#32780;&#23548;&#33268;&#37096;&#32626;&#25104;&#26412;&#39640;&#26114;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23613;&#31649;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;LLMs&#37096;&#32626;&#30340;&#25104;&#26412;&#22312;&#23454;&#38469;&#24847;&#20041;&#19978;&#24456;&#37325;&#35201;&#65292;&#20294;&#21364;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#20219;&#24847;&#31934;&#24230;LLM&#8221;&#65292;&#23558;&#20219;&#24847;&#31934;&#24230;DNN&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;LLMs&#12290;&#35299;&#20915;&#20102;&#20219;&#24847;&#31934;&#24230;LLM&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;LLMs&#20219;&#24847;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21518;&#35757;&#32451;&#37327;&#21270;&#26694;&#26550;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#36719;&#20214;&#24341;&#25806;&#26469;&#23454;&#29616;&#20854;&#26377;&#25928;&#30340;&#26381;&#21153;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23558;&#20197;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;&#8230;&#65292;n&#20301;&#65289;&#37327;&#21270;&#30340;LLMs&#21472;&#21152;&#21040;&#20869;&#23384;&#36275;&#21360;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;LLMs&#30340;&#39640;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10517v1 Announce Type: new  Abstract: Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footpri
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2402.10481</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Emoji Driven Crypto Assets Market Reactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#65292;&#35832;&#22914;Twitter&#20043;&#31867;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24050;&#32463;&#25104;&#20026;&#24433;&#21709;&#24066;&#22330;&#36235;&#21183;&#21644;&#25237;&#36164;&#32773;&#24773;&#32490;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#34920;&#24773;&#31526;&#21495;&#36716;&#21270;&#20026;&#21487;&#37327;&#21270;&#30340;&#24773;&#24863;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#35265;&#35299;&#19982;BTC&#20215;&#26684;&#21644;VCRIX&#25351;&#25968;&#31561;&#20851;&#38190;&#24066;&#22330;&#25351;&#26631;&#36827;&#34892;&#20102;&#30456;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26088;&#22312;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20803;&#32032;&#35782;&#21035;&#21644;&#39044;&#27979;&#24066;&#22330;&#36235;&#21183;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#26377;&#21161;&#20110;&#36991;&#20813;&#37325;&#22823;&#24066;&#22330;&#19979;&#25387;&#65292;&#24182;&#26377;&#21161;&#20110;&#22238;&#25253;&#30340;&#31283;&#23450;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#20808;&#36827;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20998;&#26512;&#25972;&#21512;&#21040;&#37329;&#34701;&#31574;&#30053;&#20013;&#30340;&#23454;&#38469;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#30475;&#24453;&#24066;&#22330;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10481v1 Announce Type: cross  Abstract: In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuan
&lt;/p&gt;</description></item><item><title>&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#26412;&#25991;&#21457;&#29616;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#36127;&#36801;&#31227;&#26222;&#36941;&#23384;&#22312;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65292;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#32467;&#26500;&#24046;&#24322;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.08907</link><description>&lt;p&gt;
&#35299;&#20915;&#22270;&#19978;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Negative Transfer on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08907
&lt;/p&gt;
&lt;p&gt;
&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#26412;&#25991;&#21457;&#29616;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#36127;&#36801;&#31227;&#26222;&#36941;&#23384;&#22312;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65292;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#32467;&#26500;&#24046;&#24322;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#20851;&#31995;&#19981;&#23494;&#20999;&#26102;&#65292;&#23398;&#20064;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#19981;&#21033;&#24433;&#21709;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36127;&#36801;&#31227;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#65292;&#19982;&#22270;&#20687;&#25110;&#25991;&#26412;&#19981;&#21516;&#65292;&#36127;&#36801;&#31227;&#32463;&#24120;&#21457;&#29983;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#26377;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#32467;&#26500;&#24046;&#24322;&#20250;&#22823;&#22823;&#22686;&#24378;&#22270;&#20013;&#33410;&#28857;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24102;&#26469;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65306;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#23613;&#31649;&#32467;&#26500;&#24046;&#24322;&#20250;&#23548;&#33268;&#33410;&#28857;&#23884;&#20837;&#30340;&#20998;&#24067;&#24046;&#24322;&#65292;&#20294;&#23427;&#20204;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#21487;&#33021;&#36739;&#23567;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;tw
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08907v1 Announce Type: cross Abstract: Transfer learning aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks. However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative transfer. In this paper, we investigate the negative transfer in graph transfer learning, which is important yet underexplored. We reveal that, unlike image or text, negative transfer commonly occurs in graph-structured data, even when source and target graphs share semantic similarities. Specifically, we identify that structural differences significantly amplify the dissimilarities in the node embeddings across graphs. To mitigate this, we bring a new insight: for semantically similar graphs, although structural differences lead to significant distribution shift in node embeddings, their impact on subgraph embeddings could be marginal. Building on this insight, we introduce tw
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.08193</link><description>&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08193
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#65288;GEnBP&#65289;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#65288;GaBP&#65289;&#26041;&#27861;&#30340;&#32467;&#21512;&#12290;GEnBP&#36890;&#36807;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20256;&#36882;&#20302;&#31209;&#26412;&#22320;&#20449;&#24687;&#26469;&#26356;&#26032;&#38598;&#25104;&#27169;&#22411;&#12290;&#36825;&#31181;&#32452;&#21512;&#32487;&#25215;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26377;&#21033;&#29305;&#24615;&#12290;&#38598;&#25104;&#25216;&#26415;&#20351;&#24471;GEnBP&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#12289;&#22024;&#26434;&#30340;&#40657;&#31665;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20351;&#29992;&#26412;&#22320;&#20449;&#24687;&#30830;&#20445;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#24182;&#33021;&#39640;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#24403;&#38598;&#25104;&#22823;&#23567;&#36828;&#23567;&#20110;&#25512;&#26029;&#32500;&#24230;&#26102;&#65292;GEnBP&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#31354;&#26102;&#24314;&#27169;&#12289;&#22270;&#20687;&#22788;&#29702;&#21644;&#29289;&#29702;&#27169;&#22411;&#21453;&#28436;&#31561;&#39046;&#22495;&#32463;&#24120;&#20986;&#29616;&#12290;GEnBP&#21487;&#20197;&#24212;&#29992;&#20110;&#19968;&#33324;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20219;&#21153;&#26465;&#20214;&#30340;&#33258;&#36866;&#24212;&#22120;&#65292;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#38656;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2402.07739</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#20013;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#30340;&#35270;&#35273;&#29305;&#24449;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Task-conditioned adaptation of visual features in multi-task policy learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20219;&#21153;&#26465;&#20214;&#30340;&#33258;&#36866;&#24212;&#22120;&#65292;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#38656;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#22320;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26159;&#33258;&#20027;&#20195;&#29702;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#36825;&#38656;&#35201;&#28789;&#27963;&#22320;&#35843;&#25972;&#24213;&#23618;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#24182;&#19988;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#25552;&#20986;&#30340;&#65292;&#36824;&#38656;&#35201;&#35843;&#25972;&#24213;&#23618;&#30340;&#24863;&#30693;&#27169;&#22359;&#12290;&#19968;&#20010;&#31867;&#27604;&#30340;&#35770;&#35777;&#26159;&#20154;&#31867;&#30340;&#35270;&#35273;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#33258;&#19978;&#32780;&#19979;&#30340;&#20449;&#21495;&#26469;&#19987;&#27880;&#20110;&#24403;&#21069;&#20219;&#21153;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#26469;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#35270;&#35273;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#20219;&#20309;&#39044;&#20808;&#35757;&#32451;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#35757;&#32451;&#30340;&#21333;&#19968;&#31574;&#30053;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#31574;&#30053;&#21644;&#35270;&#35273;&#36866;&#37197;&#22120;&#19978;&#26681;&#25454;&#20219;&#21153;&#23884;&#20837;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#22914;&#26524;&#20219;&#21153;&#26159;&#24050;&#30693;&#30340;&#65292;&#21017;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36873;&#25321;&#20219;&#21153;&#23884;&#20837;&#65292;&#21542;&#21017;&#21487;&#20197;&#20174;&#19968;&#32452;&#31034;&#20363;&#28436;&#31034;&#20013;&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#22312;...&#65288;&#25688;&#35201;&#26410;&#23436;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#25512;&#21160;&#20102;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#35821;&#20041;&#23618;&#27425;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#26469;&#25913;&#36827;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#65292;&#20182;&#20204;&#22312;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#36873;&#25321;&#20102;&#26368;&#32456;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07270</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#20854;&#35821;&#20041;&#23618;&#27425;&#65292;&#24320;&#23637;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#24335;VQA&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#25512;&#21160;&#20102;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#35821;&#20041;&#23618;&#27425;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#26469;&#25913;&#36827;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#65292;&#20182;&#20204;&#22312;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#36873;&#25321;&#20102;&#26368;&#32456;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#24037;&#20316;&#12290;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#24182;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#21517;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#21487;&#20197;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#35780;&#20272;&#65292;&#24182;&#19982;&#21028;&#21035;&#24615;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#20102;&#25913;&#21892;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26631;&#31614;&#31354;&#38388;&#30340;&#35821;&#20041;&#23618;&#27425;&#26469;&#25552;&#20986;&#20851;&#20110;&#22522;&#20934;&#31867;&#21035;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;LLM&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#32473;&#23450;&#22522;&#20934;&#31572;&#26696;&#30340;&#27169;&#22411;&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#65292;&#22522;&#20110;&#27492;&#20915;&#23450;&#26368;&#32456;&#24230;&#37327;&#26631;&#20934;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.06357</link><description>&lt;p&gt;
SpongeNet &#25915;&#20987;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28023;&#32501;&#26435;&#37325;&#20013;&#27602;
&lt;/p&gt;
&lt;p&gt;
The SpongeNet Attack: Sponge Weight Poisoning of Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#32501;&#25915;&#20987;&#26088;&#22312;&#22686;&#21152;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#29616;&#26377;&#30340;&#28023;&#32501;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#31034;&#20363;&#36827;&#34892;&#25512;&#29702;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#20013;&#27602;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#12290;&#28023;&#32501;&#31034;&#20363;&#21033;&#29992;&#28155;&#21152;&#21040;&#27169;&#22411;&#36755;&#20837;&#30340;&#25200;&#21160;&#26469;&#22686;&#21152;&#33021;&#37327;&#21644;&#24310;&#36831;&#65292;&#32780;&#28023;&#32501;&#20013;&#27602;&#21017;&#25913;&#21464;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#24341;&#21457;&#25512;&#29702;&#26102;&#30340;&#33021;&#37327;/&#24310;&#36831;&#25928;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28023;&#32501;&#25915;&#20987;&#65292;&#31216;&#20026; SpongeNet&#12290;SpongeNet &#26159;&#31532;&#19968;&#20010;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#28023;&#32501;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#28023;&#32501;&#20013;&#27602;&#65292;SpongeNet &#21487;&#20197;&#25104;&#21151;&#22686;&#21152;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#19981;&#19987;&#38376;&#38024;&#23545;&#28023;&#32501;&#20013;&#27602;&#36827;&#34892;&#35843;&#25972;&#65288;&#21363;&#20943;&#23567;&#25209;&#24402;&#19968;&#21270;&#20559;&#24046;&#20540;&#65289;&#65292;&#21017;&#27602;&#23475;&#38450;&#24481;&#20250;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26174;&#31034;&#20986;&#28023;&#32501;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponge attacks aim to increase the energy consumption and computation time of neural networks deployed on hardware accelerators. Existing sponge attacks can be performed during inference via sponge examples or during training via Sponge Poisoning. Sponge examples leverage perturbations added to the model's input to increase energy and latency, while Sponge Poisoning alters the objective function of a model to induce inference-time energy/latency effects.   In this work, we propose a novel sponge attack called SpongeNet. SpongeNet is the first sponge attack that is performed directly on the parameters of a pre-trained model. Our experiments show that SpongeNet can successfully increase the energy consumption of vision models with fewer samples required than Sponge Poisoning. Our experiments indicate that poisoning defenses are ineffective if not adjusted specifically for the defense against Sponge Poisoning (i.e., they decrease batch normalization bias values). Our work shows that Spong
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35299;&#31354;&#38388;&#20013;&#24212;&#29992;&#24494;&#20998;&#31639;&#23376;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;PDE&#25968;&#25454;&#29983;&#25104;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;DiffOAS&#12290;&#23427;&#33021;&#22815;&#22312;&#29983;&#25104;&#25968;&#25454;&#30340;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#19978;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.05957</link><description>&lt;p&gt;
&#22312;&#35299;&#31354;&#38388;&#20013;&#21152;&#36895;PDE&#25968;&#25454;&#29983;&#25104;&#30340;&#24494;&#20998;&#31639;&#23376;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Accelerating PDE Data Generation via Differential Operator Action in Solution Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05957
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35299;&#31354;&#38388;&#20013;&#24212;&#29992;&#24494;&#20998;&#31639;&#23376;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;PDE&#25968;&#25454;&#29983;&#25104;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;DiffOAS&#12290;&#23427;&#33021;&#22815;&#22312;&#29983;&#25104;&#25968;&#25454;&#30340;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#19978;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65288;&#22914;&#31070;&#32463;&#31639;&#23376;&#65289;&#22312;&#20943;&#23569;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#27714;&#35299;&#26102;&#38388;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#38656;&#35201;&#22823;&#37327;&#39640;&#31934;&#24230;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#38656;&#35201;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PDE&#25968;&#25454;&#38598;&#29983;&#25104;&#31639;&#27861;&#65292;&#21363;&#35299;&#31354;&#38388;&#20013;&#30340;&#24494;&#20998;&#31639;&#23376;&#20316;&#29992;&#65288;DiffOAS&#65289;&#65292;&#23427;&#21516;&#26102;&#21152;&#24555;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#24182;&#25552;&#39640;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#31934;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DiffOAS&#33719;&#21462;&#20102;&#20960;&#20010;&#22522;&#26412;&#30340;PDE&#35299;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#20197;&#33719;&#24471;&#35299;&#12290;&#23427;&#23545;&#36825;&#20123;&#35299;&#24212;&#29992;&#24494;&#20998;&#31639;&#23376;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#31639;&#23376;&#20316;&#29992;&#8221;&#65292;&#20197;&#39640;&#25928;&#29983;&#25104;&#31934;&#30830;&#30340;PDE&#25968;&#25454;&#28857;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;DiffOAS&#26041;&#27861;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#27604;&#29616;&#26377;&#30340;&#29983;&#25104;&#26041;&#27861;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in data-driven approaches, such as Neural Operator (NO), have demonstrated their effectiveness in reducing the solving time of Partial Differential Equations (PDEs). However, one major challenge faced by these approaches is the requirement for a large amount of high-precision training data, which needs significant computational costs during the generation process. To address this challenge, we propose a novel PDE dataset generation algorithm, namely Differential Operator Action in Solution space (DiffOAS), which speeds up the data generation process and enhances the precision of the generated data simultaneously. Specifically, DiffOAS obtains a few basic PDE solutions and then combines them to get solutions. It applies differential operators on these solutions, a process we call 'operator action', to efficiently generate precise PDE data points. Theoretical analysis shows that the time complexity of DiffOAS method is one order lower than the existing generation meth
&lt;/p&gt;</description></item><item><title>CEHR-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#30149;&#20154;&#26102;&#38388;&#36724;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#12289;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#31561;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04400</link><description>&lt;p&gt;
&#29983;&#25104;&#24102;&#26377;&#30149;&#20154;&#26102;&#38388;&#36724;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;CEHR-GPT
&lt;/p&gt;
&lt;p&gt;
CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04400
&lt;/p&gt;
&lt;p&gt;
CEHR-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#30149;&#20154;&#26102;&#38388;&#36724;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#12289;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#24050;&#25104;&#20026;&#25512;&#36827;&#21307;&#30103;&#24212;&#29992;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;&#30452;&#25509;&#35775;&#38382;&#21307;&#30103;&#25968;&#25454;&#30340;&#30740;&#31350;&#20154;&#21592;&#32780;&#35328;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#30495;&#23454;&#19990;&#30028;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#20351;&#29992;&#34920;&#26684;&#26684;&#24335;&#65292;&#24573;&#30053;&#20102;&#30149;&#20154;&#21382;&#21490;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#38480;&#21046;&#20102;&#25968;&#25454;&#22797;&#21046;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#26469;&#22788;&#29702;EHR&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#12289;&#20154;&#21475;&#20272;&#35745;&#12289;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#28436;&#31034;&#20102;&#20351;&#29992;&#28304;&#33258;CEHR-BERT&#30340;&#29305;&#23450;&#30149;&#20154;&#34920;&#31034;&#35757;&#32451;GPT&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#33021;&#22815;&#29983;&#25104;&#21487;&#26080;&#32541;&#36716;&#25442;&#30340;&#30149;&#20154;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.02619</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Increasing Trust in Language Models through the Reuse of Verified Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#32463;&#24120;&#24573;&#30053;&#32597;&#35265;&#30340;&#36793;&#30028;&#24773;&#20917;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#21487;&#20449;&#24230;&#26631;&#20934;&#65292;&#21363;&#20219;&#21153;&#31639;&#27861;&#21644;&#30005;&#36335;&#23454;&#29616;&#24517;&#39035;&#32463;&#36807;&#39564;&#35777;&#65292;&#32771;&#34385;&#21040;&#36793;&#30028;&#24773;&#20917;&#65292;&#24182;&#19988;&#27809;&#26377;&#24050;&#30693;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#20102;&#23436;&#20840;&#39564;&#35777;&#12290;&#20026;&#20102;&#23637;&#31034;&#32463;&#36807;&#39564;&#35777;&#30340;&#27169;&#22359;&#30340;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#25554;&#20837;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#21516;&#26102;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26356;&#22797;&#26434;&#30340;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20219;&#21153;&#27169;&#22359;&#25554;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#21033;&#29992;&#27169;&#22411;&#30340;&#37325;&#22797;&#20351;&#29992;&#26469;&#25552;&#39640;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36866;&#24212;&#32593;&#32476;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#8220;&#19968;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#8221;&#30340;&#30446;&#26631;&#65292;&#24182;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02338</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#30340;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Adaptation for Networking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36866;&#24212;&#32593;&#32476;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#8220;&#19968;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#8221;&#30340;&#30446;&#26631;&#65292;&#24182;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#35768;&#22810;&#32593;&#32476;&#20219;&#21153;&#37117;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#39044;&#27979;&#21644;&#31995;&#32479;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22522;&#20110;DL&#30340;&#31639;&#27861;&#30340;&#35774;&#35745;&#21746;&#23398;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#24037;&#31243;&#24320;&#38144;&#65292;&#22240;&#20026;&#38656;&#35201;&#20026;&#19981;&#21516;&#30340;&#32593;&#32476;&#20219;&#21153;&#25163;&#21160;&#35774;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#27492;&#22806;&#65292;DNN&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#20998;&#24067;/&#29615;&#22659;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#25104;&#21151;&#30340;&#25512;&#21160;&#19979;&#65292;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;LLM&#29992;&#20110;&#32593;&#32476;&#30340;&#36866;&#24212;&#24615;&#65292;&#20197;&#25506;&#32034;&#26356;&#21487;&#25345;&#32493;&#30340;&#35774;&#35745;&#21746;&#23398;&#12290;&#20973;&#20511;&#28023;&#37327;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;LLM&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19988;&#26377;&#26395;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#8220;&#19968;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#8221;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NetLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26377;&#25928;&#22320;&#23558;LLM&#24212;&#29992;&#20110;&#35299;&#20915;&#32593;&#32476;&#38382;&#39064;&#30340;&#36866;&#24212;&#24615;&#26694;&#26550;&#12290;NetLLM&#35299;&#20915;&#20102;&#35768;&#22810;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many networking tasks now employ deep learning (DL) to solve complex prediction and system optimization problems. However, current design philosophy of DL-based algorithms entails intensive engineering overhead due to the manual design of deep neural networks (DNNs) for different networking tasks. Besides, DNNs tend to achieve poor generalization performance on unseen data distributions/environments.   Motivated by the recent success of large language models (LLMs), for the first time, this work studies the LLM adaptation for networking to explore a more sustainable design philosophy. With the massive pre-trained knowledge and powerful inference ability, LLM can serve as the foundation model, and is expected to achieve "one model for all" with even better performance and stronger generalization for various tasks. In this paper, we present NetLLM, the first LLM adaptation framework that efficiently adapts LLMs to solve networking problems. NetLLM addresses many practical challenges in L
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#28548;&#28165;&#20102;&#24187;&#35273;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;</title><link>https://arxiv.org/abs/2402.00253</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#28548;&#28165;&#20102;&#24187;&#35273;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23545;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#23454;&#38469;&#30340;&#23454;&#26045;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#8220;&#24187;&#35273;&#8221;&#65292;&#25110;&#32773;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#21363;&#35270;&#35273;&#20869;&#23481;&#19982;&#30456;&#24212;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#22312;&#21033;&#29992;LVLMs&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#20221;&#32508;&#21512;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;LVLM&#30456;&#20851;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#28145;&#20837;&#21078;&#26512;&#65292;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#27010;&#35272;&#24182;&#20419;&#36827;&#26410;&#26469;&#30340;&#32531;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#28548;&#28165;&#20102;LVLMs&#20013;&#24187;&#35273;&#27010;&#24565;&#65292;&#21576;&#29616;&#20102;&#21508;&#31181;&#24187;&#35273;&#30151;&#29366;&#65292;&#24182;&#24378;&#35843;&#20102;LVLM&#24187;&#35273;&#22266;&#26377;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;LVLM&#29420;&#29305;&#24187;&#35273;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#35843;&#26597;&#20102;&#36825;&#20123;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#21253;&#25324;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#32452;&#20214;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#23545;&#29616;&#26377;&#30340;&#24187;&#35273;&#32531;&#35299;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17791</link><description>&lt;p&gt;
&#19981;&#24102;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Transformers without Positional Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#21387;&#22120;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#21333;&#29420;&#20351;&#29992;&#36824;&#26159;&#19982;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MP-GNN&#65289;&#32467;&#21512;&#12290;&#23558;&#22270;&#24402;&#32435;&#20559;&#35265;&#34701;&#20837;&#22825;&#28982;&#19982;&#32467;&#26500;&#26080;&#20851;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20197;&#32467;&#26500;&#25110;&#20301;&#32622;&#32534;&#30721;&#65288;PEs&#65289;&#30340;&#24418;&#24335;&#65292;&#26159;&#23454;&#29616;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#26159;&#26840;&#25163;&#30340;&#65292;&#20154;&#20204;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#23581;&#35797;&#26469;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#65292;&#21253;&#25324;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#12289;&#30456;&#23545;&#38543;&#26426;&#34892;&#36208;&#27010;&#29575;&#65288;RRWP&#65289;&#12289;&#31354;&#38388;&#32534;&#30721;&#12289;&#20013;&#24515;&#24230;&#32534;&#30721;&#12289;&#36793;&#32536;&#32534;&#30721;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32534;&#30721;&#21487;&#33021;&#26681;&#26412;&#19981;&#38656;&#35201;&#65292;&#21482;&#35201;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#26377;&#20851;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Eigenformer&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#35889;&#24863;&#30693;&#27880;&#24847;&#26426;&#21046;&#65292;&#20102;&#35299;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#65292;&#36890;&#36807;&#37319;&#29992;CART&#31639;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#21644;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.02959</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#26816;&#27979;&#31639;&#27861;&#20559;&#20506;
&lt;/p&gt;
&lt;p&gt;
Detecting algorithmic bias in medical AI-models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#65292;&#36890;&#36807;&#37319;&#29992;CART&#31639;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#21644;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26085;&#30410;&#26222;&#21450;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#20197;&#20844;&#24179;&#12289;&#20844;&#27491;&#30340;&#26041;&#24335;&#25552;&#20379;&#24739;&#32773;&#32467;&#26524;&#21464;&#24471;&#21516;&#26679;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21307;&#30103;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#30340;&#31639;&#27861;&#20559;&#20506;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20998;&#31867;&#19982;&#22238;&#24402;&#26641;&#65288;CART&#65289;&#31639;&#27861;&#65292;&#22312;&#33043;&#27602;&#30151;&#39044;&#27979;&#32972;&#26223;&#19979;&#26377;&#25928;&#22320;&#35782;&#21035;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#20559;&#20506;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#20934;&#30830;&#20272;&#35745;&#20559;&#20506;&#21306;&#22495;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#20351;&#29992;&#20122;&#29305;&#20848;&#22823;&#20052;&#27835;&#20122;&#24030;&#26684;&#38647;&#36842;&#32426;&#24565;&#21307;&#38498;&#30340;&#30005;&#23376;&#30149;&#21382;&#36827;&#34892;&#23454;&#39564;&#36827;&#19968;&#27493;&#24471;&#21040;&#39564;&#35777;&#12290;&#36825;&#20123;&#27979;&#35797;&#23637;&#31034;&#20102;&#25105;&#20204;&#31574;&#30053;&#22312;&#20020;&#24202;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02959v3 Announce Type: replace-cross  Abstract: With the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. This paper presents an innovative framework for detecting areas of algorithmic bias in medical-AI decision support systems. Our approach efficiently identifies potential biases in medical-AI models, specifically in the context of sepsis prediction, by employing the Classification and Regression Trees (CART) algorithm. We verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. The effectiveness of the concept is further validated by experiments using electronic medical records from Grady Memorial Hospital in Atlanta, Georgia. These tests demonstrate the practical implementation of our strategy in a clini
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#26679;&#26412;-efficient RL&#65292;&#36890;&#36807;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.02198</link><description>&lt;p&gt;
&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Bootstrapped Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02198
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#26679;&#26412;-efficient RL&#65292;&#36890;&#36807;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#20027;&#35201;&#20381;&#36182;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#26159;&#22240;&#20026;&#20854;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#33021;&#20351;IL&#25512;&#24191;&#21040;&#25152;&#26377;&#21487;&#33021;&#22330;&#26223;&#30340;&#20840;&#38754;&#19987;&#23478;&#28436;&#31034;&#26159;&#26114;&#36149;&#30340;&#65292;&#20219;&#20309;&#20998;&#24067;&#30340;&#36716;&#21464;&#37117;&#38656;&#35201;&#37325;&#26032;&#25910;&#38598;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;RL&#21487;&#20197;&#24314;&#31435;&#22312;IL&#30340;&#22522;&#30784;&#19978;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#25105;&#25913;&#36827;&#31243;&#24207;&#65292;&#37027;&#20040;&#23427;&#23558;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#31034;&#33539;&#30340;&#39640;&#25928;&#25277;&#26679;RL&#65292;&#39318;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;&#19982;&#20808;&#21069;&#36807;&#24230;&#37319;&#26679;&#31034;&#33539;&#25110;&#29992;&#39069;&#22806;&#30340;&#27169;&#20223;&#25439;&#22833;&#23545;RL&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;IBRL&#33021;&#22815;&#21033;&#29992;&#26469;&#33258;IL&#30340;&#39640;&#36136;&#37327;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02198v4 Announce Type: replace-cross  Abstract: Despite the considerable potential of reinforcement learning (RL), robotic control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We propose imitation bootstrapped reinforcement learning (IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with an additional imitation loss, IBRL is able to utilize high quality actions from IL p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#22810;&#20010;&#23398;&#20064;&#33539;&#24335;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#23545;&#25239;&#26679;&#26412;&#30340;&#29702;&#35770;&#65292;&#21457;&#29616;&#38750;&#40065;&#26834;&#29305;&#24449;&#22312;&#22810;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#20013;&#30340;&#25928;&#29992;&#36739;&#24046;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#29305;&#24449;&#24182;&#19981;&#20687;&#40065;&#26834;&#29305;&#24449;&#25110;&#33258;&#28982;&#29305;&#24449;&#37027;&#26679;&#30495;&#27491;&#26377;&#29992;&#12290;</title><link>https://arxiv.org/abs/2310.18936</link><description>&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#19981;&#26159;&#30495;&#27491;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples Are Not Real Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18936
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#22810;&#20010;&#23398;&#20064;&#33539;&#24335;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#23545;&#25239;&#26679;&#26412;&#30340;&#29702;&#35770;&#65292;&#21457;&#29616;&#38750;&#40065;&#26834;&#29305;&#24449;&#22312;&#22810;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#20013;&#30340;&#25928;&#29992;&#36739;&#24046;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#29305;&#24449;&#24182;&#19981;&#20687;&#40065;&#26834;&#29305;&#24449;&#25110;&#33258;&#28982;&#29305;&#24449;&#37027;&#26679;&#30495;&#27491;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#30340;&#23384;&#22312;&#22810;&#24180;&#26469;&#19968;&#30452;&#26159;&#19968;&#20010;&#35868;&#22242;&#65292;&#24182;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#19968;&#31181;&#30001;Ilyas&#31561;&#20154;&#25552;&#20986;&#30340;&#33879;&#21517;&#29702;&#35770;&#20174;&#25968;&#25454;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#65292;&#21363;&#36890;&#36807;&#23637;&#31034;&#21487;&#20197;&#20174;&#23545;&#25239;&#26679;&#26412;&#20013;&#25552;&#21462;&#38750;&#40065;&#26834;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#21333;&#29420;&#29992;&#20110;&#20998;&#31867;&#26159;&#26377;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#37322;&#20173;&#28982;&#30456;&#24403;&#21453;&#30452;&#35273;&#65292;&#22240;&#20026;&#38750;&#40065;&#26834;&#29305;&#24449;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#22823;&#37096;&#20998;&#26159;&#22122;&#22768;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#26356;&#22823;&#30340;&#32972;&#26223;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20010;&#29702;&#35770;&#65292;&#32467;&#21512;&#20102;&#22810;&#20010;&#23398;&#20064;&#33539;&#24335;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#33391;&#22909;&#25928;&#29992;&#30456;&#21453;&#65292;&#24403;&#23558;&#38750;&#40065;&#26834;&#29305;&#24449;&#36716;&#31227;&#21040;&#20854;&#20182;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#26102;&#65288;&#22914;&#23545;&#27604;&#23398;&#20064;&#12289;&#36974;&#25377;&#22270;&#20687;&#24314;&#27169;&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#23427;&#20204;&#30340;&#25928;&#29992;&#21464;&#24046;&#12290;&#36825;&#25581;&#31034;&#20102;&#38750;&#40065;&#26834;&#29305;&#24449;&#24182;&#19981;&#20687;&#22312;&#36825;&#20123;&#33539;&#24335;&#20043;&#38388;&#20139;&#26377;&#33391;&#22909;&#21487;&#36801;&#31227;&#24615;&#30340;&#40065;&#26834;&#29305;&#24449;&#25110;&#33258;&#28982;&#29305;&#24449;&#37027;&#26679;&#30495;&#27491;&#26377;&#29992;&#12290;&#21516;&#26102;&#65292;&#23545;&#20110;&#40065;&#26834;&#29305;&#24449;&#32780;&#35328;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25928;&#29992;&#19982;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25928;&#29992;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of adversarial examples has been a mystery for years and attracted much interest. A well-known theory by \citet{ilyas2019adversarial} explains adversarial vulnerability from a data perspective by showing that one can extract non-robust features from adversarial examples and these features alone are useful for classification. However, the explanation remains quite counter-intuitive since non-robust features are mostly noise features to humans. In this paper, we re-examine the theory from a larger context by incorporating multiple learning paradigms. Notably, we find that contrary to their good usefulness under supervised learning, non-robust features attain poor usefulness when transferred to other self-supervised learning paradigms, such as contrastive learning, masked image modeling, and diffusion models. It reveals that non-robust features are not really as useful as robust or natural features that enjoy good transferability between these paradigms. Meanwhile, for robus
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#20013;&#30340;&#28608;&#27963;&#24322;&#24120;&#20540;&#19982;&#32593;&#32476;&#23618;&#31232;&#30095;&#24230;&#30340;&#38750;&#22343;&#21248;&#24615;&#30456;&#20851;&#65292;&#24182;&#25552;&#20986;&#20102;Outlier Weighed Layerwise Sparsity&#65288;OWL&#65289;&#20316;&#20026;&#21098;&#26525;LLMs&#21040;&#39640;&#31232;&#30095;&#24230;&#30340;&#31192;&#23494;&#35843;&#21619;&#26009;&#12290;</title><link>https://arxiv.org/abs/2310.05175</link><description>&lt;p&gt;
Outlier Weighed Layerwise Sparsity (OWL): &#20026;&#21098;&#26525;LLMs&#36798;&#21040;&#39640;&#31232;&#30095;&#24230;&#25552;&#20379;&#32570;&#22833;&#30340;&#31192;&#23494;&#35843;&#21619;&#26009;
&lt;/p&gt;
&lt;p&gt;
Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#20013;&#30340;&#28608;&#27963;&#24322;&#24120;&#20540;&#19982;&#32593;&#32476;&#23618;&#31232;&#30095;&#24230;&#30340;&#38750;&#22343;&#21248;&#24615;&#30456;&#20851;&#65292;&#24182;&#25552;&#20986;&#20102;Outlier Weighed Layerwise Sparsity&#65288;OWL&#65289;&#20316;&#20026;&#21098;&#26525;LLMs&#21040;&#39640;&#31232;&#30095;&#24230;&#30340;&#31192;&#23494;&#35843;&#21619;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#30340;&#21331;&#36234;&#24615;&#33021;&#32780;&#38395;&#21517;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#26102;&#30001;&#20110;&#27169;&#22411;&#24222;&#22823;&#32780;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20154;&#20204;&#21162;&#21147;&#23558;&#20256;&#32479;&#30340;&#32593;&#32476;&#21098;&#26525;&#25216;&#26415;&#24212;&#29992;&#20110;LLMs&#65292;&#21457;&#29616;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#19968;&#27425;&#24615;&#21098;&#25481;&#22823;&#37327;&#21442;&#25968;&#12290;&#29616;&#26377;&#30340;LLM&#21098;&#26525;&#31574;&#30053;&#19968;&#30452;&#22362;&#25345;&#20197;&#31561;&#20215;&#31232;&#30095;&#24230;&#22343;&#21248;&#21098;&#35009;&#25152;&#26377;&#23618;&#30340;&#20570;&#27861;&#65292;&#32467;&#26524;&#34920;&#29616;&#24378;&#21170;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#19982;&#22312;&#35270;&#35273;&#27169;&#22411;&#39046;&#22495;&#35266;&#23519;&#21040;&#30340;&#38750;&#22343;&#21248;&#36880;&#23618;&#31232;&#30095;&#30340;&#20027;&#27969;&#36235;&#21183;&#30456;&#30683;&#30462;&#65292;&#21518;&#32773;&#36890;&#24120;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20102;&#35299;&#36825;&#31181;&#24046;&#24322;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#19982;LLMs&#20013;&#24322;&#24120;&#20540;&#30340;&#20986;&#29616;&#24378;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05175v2 Announce Type: replace  Abstract: Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Ins
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2204.04510</link><description>&lt;p&gt;
&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#35753;&#31616;&#21333;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#19978;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.04510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#24120;&#20351;&#29992;&#19987;&#38376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#22823;&#22411;&#20840;&#23616;&#22270;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20294;&#25361;&#25112;&#23376;&#22270;&#30340;&#23618;&#27425;&#32467;&#26500;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23376;&#22270;&#21040;&#33410;&#28857;&#65288;S2N&#65289;&#36716;&#25442;&#30340;&#26032;&#39062;&#20844;&#24335;&#65292;&#29992;&#20110;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20840;&#23616;&#22270;&#20013;&#30340;&#19968;&#32452;&#23376;&#22270;&#65292;&#25105;&#20204;&#36890;&#36807;&#31895;&#30053;&#22320;&#23558;&#23376;&#22270;&#36716;&#25442;&#25104;&#33410;&#28857;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#22270;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;S2N&#19981;&#20165;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#19988;&#36890;&#36807;&#25429;&#25417;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#20063;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#23427;&#20204;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31895;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35843;&#25972;&#27169;&#22411;&#21518;&#25928;&#26524;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.14423</link><description>&lt;p&gt;
&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#65306;&#20171;&#32461;&#19982;&#39640;&#32423;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompt Design and Engineering: Introduction and Advanced Methods. (arXiv:2401.14423v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#23398;&#31185;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt design and engineering has become an important discipline in just the past few months. In this paper, we provide an introduction to the main concepts as well as review basic and more advanced approaches to prompt design and engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32858;&#31867;&#23558;&#22810;&#27169;&#24577;&#26679;&#26412;&#38598;&#20998;&#21106;&#25104;&#22810;&#20010;&#21333;&#27169;&#24577;&#26679;&#26412;&#38598;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;KDE&#20272;&#35745;&#26469;&#20272;&#35745;&#25972;&#20307;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#12289;&#38750;&#27491;&#24577;&#21644;&#39640;&#30456;&#20851;&#20998;&#24067;&#20272;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10566</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#22810;&#27169;&#24577;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-Modal Density Estimation. (arXiv:2401.10566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32858;&#31867;&#23558;&#22810;&#27169;&#24577;&#26679;&#26412;&#38598;&#20998;&#21106;&#25104;&#22810;&#20010;&#21333;&#27169;&#24577;&#26679;&#26412;&#38598;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;KDE&#20272;&#35745;&#26469;&#20272;&#35745;&#25972;&#20307;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#12289;&#38750;&#27491;&#24577;&#21644;&#39640;&#30456;&#20851;&#20998;&#24067;&#20272;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#30340;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#32508;&#21512;&#35780;&#20272;&#25351;&#26631;&#30340;&#38656;&#27714;&#12290;&#34429;&#28982;&#26377;&#20960;&#20010;&#25351;&#26631;&#21487;&#20197;&#34920;&#24449;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65288;&#20363;&#22914;&#65292;&#36127;&#23545;&#25968;&#20284;&#28982;&#12289;Jensen-Shannon&#25955;&#24230;&#65289;&#65292;&#20294;&#36825;&#20123;&#25351;&#26631;&#36890;&#24120;&#20316;&#29992;&#20110;&#27010;&#29575;&#23494;&#24230;&#19978;&#12290;&#22240;&#27492;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#32431;&#31929;&#22522;&#20110;&#26679;&#26412;&#30340;&#39044;&#27979;&#27169;&#22411;&#38656;&#35201;&#20272;&#35745;&#24213;&#23618;&#23494;&#24230;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#22914;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#20272;&#35745;&#38382;&#39064;&#20013;&#23578;&#26410;&#24471;&#21040;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;ROME&#65288;RObust Multi-modal density Estimator&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#20272;&#35745;&#22810;&#27169;&#24577;&#12289;&#38750;&#27491;&#24577;&#21644;&#39640;&#30456;&#20851;&#20998;&#24067;&#30340;&#25361;&#25112;&#12290;ROME&#21033;&#29992;&#32858;&#31867;&#23558;&#22810;&#27169;&#24577;&#26679;&#26412;&#38598;&#20998;&#21106;&#25104;&#22810;&#20010;&#21333;&#27169;&#24577;&#26679;&#26412;&#38598;&#65292;&#28982;&#21518;&#32467;&#21512;&#31616;&#21333;&#30340;KDE&#20272;&#35745;&#26469;&#24471;&#21040;&#24635;&#20307;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Development of multi-modal, probabilistic prediction models has lead to a need for comprehensive evaluation metrics. While several metrics can characterize the accuracy of machine-learned models (e.g., negative log-likelihood, Jensen-Shannon divergence), these metrics typically operate on probability densities. Applying them to purely sample-based prediction models thus requires that the underlying density function is estimated. However, common methods such as kernel density estimation (KDE) have been demonstrated to lack robustness, while more complex methods have not been evaluated in multi-modal estimation problems. In this paper, we present ROME (RObust Multi-modal density Estimator), a non-parametric approach for density estimation which addresses the challenge of estimating multi-modal, non-normal, and highly correlated distributions. ROME utilizes clustering to segment a multi-modal set of samples into multiple uni-modal ones and then combines simple KDE estimates obtained for i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#12290;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;MOCO&#24494;&#35843;&#30340;ResNet50&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.09243</link><description>&lt;p&gt;
DiffClone: &#20351;&#29992;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#26426;&#22120;&#20154;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning. (arXiv:2401.09243v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#12290;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;MOCO&#24494;&#35843;&#30340;ResNet50&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#23494;&#38598;&#19988;&#30828;&#20214;&#29305;&#23450;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#25805;&#20316;&#20195;&#29702;&#65292;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#24335;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;Train-Offline-Test-Online&#65288;TOTO&#65289;&#22522;&#20934;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#24320;&#28304;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#30001;&#19987;&#23478;&#25968;&#25454;&#32452;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#24120;&#35265;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#22522;&#20934;&#20998;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#22312;NeurIPS 2023&#20030;&#21150;&#30340;Train-Offline-Test-Online&#65288;TOTO&#65289;&#22522;&#20934;&#25361;&#25112;&#36187;&#20013;&#30340;&#23448;&#26041;&#25552;&#20132;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#20195;&#29702;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MOCO&#24494;&#35843;&#30340;ResNet50&#30456;&#27604;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning tasks are extremely compute-intensive and hardware-specific. Thus the avenues of tackling these challenges, using a diverse dataset of offline demonstrations that can be used to train robot manipulation agents, is very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a well-curated open-source dataset for offline training comprised mostly of expert data and also benchmark scores of the common offline-RL and behaviour cloning agents. In this paper, we introduce DiffClone, an offline algorithm of enhanced behaviour cloning agent with diffusion-based policy learning, and measured the efficacy of our method on real online physical robots at test time. This is also our official submission to the Train-Offline-Test-Online (TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both pre-trained visual representation and agent policies. In our experiments, we find that MOCO finetuned ResNet50 performs the best in comparison to other finetuned
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;</title><link>http://arxiv.org/abs/2312.01185</link><description>&lt;p&gt;
&#26102;&#38388;&#20013;&#30340;&#28063;&#28458;&#65306;&#32654;&#22269;&#21382;&#21490;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22269;&#24773;&#21672;&#25991;&#25968;&#25454;&#38598;&#23545;&#32654;&#22269;&#21382;&#21490;&#30340;&#24635;&#20307;&#26102;&#38388;&#32447;&#21450;&#21672;&#25991;&#26412;&#36523;&#30340;&#29305;&#28857;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#65288;&#20063;&#26377;&#20123;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65289;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#65292;&#22914;BERT&#65288;DistilBERT&#65289;&#21644;GPT-2&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#26368;&#36866;&#21512;NLP&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;GPT-2&#32467;&#21512;UMAP&#31561;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#26356;&#24378;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#36825;&#20351;&#24471;GPT-2 + UMAP&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#23601;&#36275;&#22815;&#22909;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#26469;&#26816;&#27979;&#21738;&#20301;&#24635;&#32479;&#21457;&#34920;&#20102;&#21738;&#31687;&#28436;&#35762;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#65288;&#20934;&#30830;&#29575;&#20026;93\% - 95\%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#36816;&#34892;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#20889;&#20316;&#24180;&#20221;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#31350;&#22312;&#22810;&#27969;&#24418;&#27169;&#22411;&#19979;&#65292;&#23398;&#20064;&#30340;&#34920;&#31034;&#20309;&#26102;&#21487;&#20197;&#32447;&#24615;&#20998;&#31163;&#27969;&#24418;&#65292;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#25968;&#25454;&#22686;&#24378;&#26041;&#38754;&#30340;&#39069;&#22806;&#22909;&#22788;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#32447;&#24615;&#20998;&#31163;&#33021;&#21147;&#30340;&#20449;&#24687;&#35770;&#26368;&#20248;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.19041</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#32447;&#24615;&#20998;&#31163;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Linear Separation Capacity of Self-Supervised Representation Learning. (arXiv:2310.19041v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#31350;&#22312;&#22810;&#27969;&#24418;&#27169;&#22411;&#19979;&#65292;&#23398;&#20064;&#30340;&#34920;&#31034;&#20309;&#26102;&#21487;&#20197;&#32447;&#24615;&#20998;&#31163;&#27969;&#24418;&#65292;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#25968;&#25454;&#22686;&#24378;&#26041;&#38754;&#30340;&#39069;&#22806;&#22909;&#22788;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#32447;&#24615;&#20998;&#31163;&#33021;&#21147;&#30340;&#20449;&#24687;&#35770;&#26368;&#20248;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24378;&#35843;&#20102;&#25968;&#25454;&#22686;&#24378;&#22312;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20123;&#22686;&#24378;&#34920;&#31034;&#20043;&#19978;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#29087;&#32451;&#30340;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#23558;&#38750;&#32447;&#24615;&#25968;&#25454;&#32467;&#26500;&#35299;&#24320;&#20026;&#32447;&#24615;&#21487;&#20998;&#31163;&#34920;&#31034;&#30340;&#26426;&#21046;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#22312;&#20174;&#22810;&#27969;&#24418;&#27169;&#22411;&#20013;&#32472;&#21046;&#25968;&#25454;&#26102;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#21487;&#20197;&#32447;&#24615;&#20998;&#31163;&#27969;&#24418;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25968;&#25454;&#22686;&#24378;&#38500;&#20102;&#25552;&#20379;&#35266;&#23519;&#25968;&#25454;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#21487;&#20197;&#25913;&#21892;&#32447;&#24615;&#20998;&#31163;&#23481;&#37327;&#30340;&#20449;&#24687;&#35770;&#26368;&#20248;&#36895;&#29575;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#20197;&#27604;&#26080;&#30417;&#30563;&#23398;&#20064;&#26356;&#23567;&#30340;&#36317;&#31163;&#32447;&#24615;&#20998;&#31163;&#27969;&#24418;&#65292;&#31361;&#26174;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#39069;&#22806;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning have highlighted the efficacy of data augmentation in learning data representation from unlabeled data. Training a linear model atop these enhanced representations can yield an adept classifier. Despite the remarkable empirical performance, the underlying mechanisms that enable data augmentation to unravel nonlinear data structures into linearly separable representations remain elusive. This paper seeks to bridge this gap by investigating under what conditions learned representations can linearly separate manifolds when data is drawn from a multi-manifold model. Our investigation reveals that data augmentation offers additional information beyond observed data and can thus improve the information-theoretic optimal rate of linear separation capacity. In particular, we show that self-supervised learning can linearly separate manifolds with a smaller distance than unsupervised learning, underscoring the additional benefits of data augmentation. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.17191</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23558;&#23454;&#20307;&#32465;&#23450;&#21040;&#19978;&#19979;&#25991;&#20013;?
&lt;/p&gt;
&lt;p&gt;
How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27491;&#30830;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24517;&#39035;&#23558;&#23454;&#20307;&#19982;&#20854;&#23646;&#24615;&#36827;&#34892;&#32465;&#23450;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#25551;&#36848;&#8220;&#32511;&#33394;&#26041;&#22359;&#8221;&#21644;&#8220;&#34013;&#33394;&#22278;&#24418;&#8221;&#30340;&#19978;&#19979;&#25991;&#65292;LMs&#24517;&#39035;&#23558;&#24418;&#29366;&#19982;&#23427;&#20204;&#23545;&#24212;&#30340;&#39068;&#33394;&#36827;&#34892;&#32465;&#23450;&#12290;&#25105;&#20204;&#20998;&#26512;LM&#34920;&#31034;&#24182;&#30830;&#23450;&#32465;&#23450;ID&#26426;&#21046;&#65306;&#36825;&#26159;&#19968;&#31181;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#25105;&#20204;&#22312;Pythia&#21644;LLaMA&#23478;&#26063;&#30340;&#27599;&#20010;&#36275;&#22815;&#22823;&#30340;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#12290;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LMs&#20869;&#37096;&#28608;&#27963;&#36890;&#36807;&#23558;&#32465;&#23450;ID&#21521;&#37327;&#38468;&#21152;&#21040;&#30456;&#24212;&#30340;&#23454;&#20307;&#21644;&#23646;&#24615;&#19978;&#26469;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#32465;&#23450;ID&#21521;&#37327;&#24418;&#25104;&#36830;&#32493;&#30340;&#23376;&#31354;&#38388;&#65292;&#22312;&#36825;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#32465;&#23450;ID&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#21453;&#26144;&#20102;&#23427;&#20204;&#30340;&#21306;&#21035;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;LMs&#22312;&#19978;&#19979;&#25991;&#20013;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;&#65292;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;LMs&#20013;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15872</link><description>&lt;p&gt;
KirchhoffNet&#65306;&#19968;&#31181;&#36830;&#25509;&#28040;&#24687;&#20256;&#36882;&#21644;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#30005;&#36335;&#26725;&#25509;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models. (arXiv:2310.15872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#27169;&#25311;&#30005;&#36335;&#30340;&#22522;&#26412;&#21407;&#29702;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#65292;&#24341;&#20837;&#20102;&#19968;&#31867;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#12290;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#20102;&#23494;&#20999;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#20256;&#32479;&#23618;&#65288;&#22914;&#21367;&#31215;&#12289;&#27744;&#21270;&#25110;&#32447;&#24615;&#23618;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#35753;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#26356;&#21152;&#26377;&#36259;&#30340;&#26159;&#20854;&#22312;&#30828;&#20214;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#24403;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#37096;&#32626;&#22312;GPU&#19978;&#12290;&#30456;&#21453;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#30005;&#36335;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#35770;&#22312;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20869;&#26377;&#22810;&#23569;&#21442;&#25968;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20854;&#20013;f&#34920;&#31034;&#30828;&#20214;&#30340;&#26102;&#38047;&#39057;&#29575;&#12290;&#36825;&#31181;&#29305;&#24615;&#34920;&#26126;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20855;&#26377;&#28508;&#21147;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we exploit a fundamental principle of analog electronic circuitry, Kirchhoff's current law, to introduce a unique class of neural network models that we refer to as KirchhoffNet. KirchhoffNet establishes close connections with message passing neural networks and continuous-depth networks. We demonstrate that even in the absence of any traditional layers (such as convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test accuracy on the MNIST dataset, comparable with state of the art (SOTA) results. What makes KirchhoffNet more intriguing is its potential in the realm of hardware. Contemporary deep neural networks are conventionally deployed on GPUs. In contrast, KirchhoffNet can be physically realized by an analog electronic circuit. Moreover, we justify that irrespective of the number of parameters within a KirchhoffNet, its forward calculation can always be completed within 1/f seconds, with f representing the hardware's clock frequency. This characteris
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#25277;&#26679;&#20998;&#26512;&#30697;&#38453;&#26426;&#21046;&#30340;&#38544;&#31169;&#25918;&#22823;&#31639;&#27861;MMCC&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#26465;&#20214;&#24615;&#32452;&#21512;&#23450;&#29702;&#21487;&#20197;&#23558;&#30456;&#20851;&#36755;&#20986;&#35270;&#20026;&#29420;&#31435;&#36755;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;DP-FTRL&#31639;&#27861;&#26102;&#21487;&#20197;&#28176;&#36817;&#22320;&#19982;DP-SGD&#31639;&#27861;&#20013;&#25918;&#22823;&#21518;&#28155;&#21152;&#30340;&#22122;&#22768;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2310.15526</link><description>&lt;p&gt;
&#30697;&#38453;&#26426;&#21046;&#30340;&#38544;&#31169;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Privacy Amplification for Matrix Mechanisms. (arXiv:2310.15526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15526
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#25277;&#26679;&#20998;&#26512;&#30697;&#38453;&#26426;&#21046;&#30340;&#38544;&#31169;&#25918;&#22823;&#31639;&#27861;MMCC&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#26465;&#20214;&#24615;&#32452;&#21512;&#23450;&#29702;&#21487;&#20197;&#23558;&#30456;&#20851;&#36755;&#20986;&#35270;&#20026;&#29420;&#31435;&#36755;&#20986;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;DP-FTRL&#31639;&#27861;&#26102;&#21487;&#20197;&#28176;&#36817;&#22320;&#19982;DP-SGD&#31639;&#27861;&#20013;&#25918;&#22823;&#21518;&#28155;&#21152;&#30340;&#22122;&#22768;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25918;&#22823;&#21033;&#29992;&#25968;&#25454;&#36873;&#25321;&#20013;&#30340;&#38543;&#26426;&#24615;&#26469;&#25552;&#20379;&#26356;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;&#36825;&#31181;&#20998;&#26512;&#23545;&#20110;DP-SGD&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#24182;&#19981;&#36866;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20123;&#31639;&#27861;&#65292;&#21363;DP-FTRL&#65292;&#20351;&#29992;&#30697;&#38453;&#26426;&#21046;&#26469;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#65292;&#32780;&#19981;&#26159;&#20687;DP-SGD&#37027;&#26679;&#28155;&#21152;&#29420;&#31435;&#22122;&#22768;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MMCC&#8221;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#36890;&#36807;&#25277;&#26679;&#26469;&#20998;&#26512;&#20219;&#20309;&#36890;&#29992;&#30697;&#38453;&#26426;&#21046;&#30340;&#38544;&#31169;&#25918;&#22823;&#30340;&#39318;&#20010;&#31639;&#27861;&#12290;MMCC&#25509;&#36817;&#19968;&#20010;&#19979;&#30028;&#65292;&#24403;&#949;&#8594;0&#26102;&#65292;&#23427;&#36235;&#36817;&#20110;&#35813;&#19979;&#30028;&#12290;&#20026;&#20102;&#20998;&#26512;MMCC&#20013;&#30340;&#30456;&#20851;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#23558;&#20854;&#35270;&#20026;&#29420;&#31435;&#36755;&#20986;&#65292;&#36890;&#36807;&#23545;&#20808;&#21069;&#36755;&#20986;&#36827;&#34892;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#8220;&#26465;&#20214;&#24615;&#32452;&#21512;&#23450;&#29702;&#8221;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#29992;&#24615;&#65306;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#26174;&#31034;&#28155;&#21152;&#21040;&#20108;&#21449;&#26641;DP-FTRL&#30340;&#22122;&#22768;&#21487;&#20197;&#28176;&#36817;&#22320;&#19982;&#36890;&#36807;&#25918;&#22823;&#28155;&#21152;&#21040;DP-SGD&#20013;&#30340;&#22122;&#22768;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#25918;&#22823;&#31639;&#27861;&#20063;&#20855;&#26377;&#23454;&#38469;&#30340;&#32463;&#39564;&#24615;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy amplification exploits randomness in data selection to provide tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's success in machine learning, but, is not readily applicable to the newer state-of-the-art algorithms. This is because these algorithms, known as DP-FTRL, use the matrix mechanism to add correlated noise instead of independent noise as in DP-SGD.  In this paper, we propose "MMCC", the first algorithm to analyze privacy amplification via sampling for any generic matrix mechanism. MMCC is nearly tight in that it approaches a lower bound as $\epsilon\to0$. To analyze correlated outputs in MMCC, we prove that they can be analyzed as if they were independent, by conditioning them on prior outputs. Our "conditional composition theorem" has broad utility: we use it to show that the noise added to binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with amplification. Our amplification algorithm also has practical empirical util
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;Vendi&#20998;&#25968;&#26063;&#32676;&#65292;&#20026;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#20934;&#30830;&#27979;&#37327;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12952</link><description>&lt;p&gt;
&#12298;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;Vendi&#20998;&#25968;&#30340;&#36817;&#20146;&#65306;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#26063;&#32676;&#12299;
&lt;/p&gt;
&lt;p&gt;
Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning. (arXiv:2310.12952v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;Vendi&#20998;&#25968;&#26063;&#32676;&#65292;&#20026;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#20934;&#30830;&#27979;&#37327;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#27979;&#37327;&#22810;&#26679;&#24615;&#23545;&#20110;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#12289;&#29983;&#24577;&#23398;&#21644;&#21270;&#23398;&#12290; Vendi&#20998;&#25968;&#26159;&#19968;&#31181;&#25193;&#23637;&#20102;q=1&#38454;Hill&#25968;&#30340;&#36890;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#20511;&#37492;&#37327;&#23376;&#32479;&#35745;&#21147;&#23398;&#30340;&#24605;&#24819;&#12290;&#19982;&#29983;&#24577;&#23398;&#20013;&#30340;&#35768;&#22810;&#22810;&#26679;&#24615;&#24230;&#37327;&#26041;&#27861;&#19981;&#21516;&#65292;Vendi&#20998;&#25968;&#32771;&#34385;&#20102;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20102;&#35299;&#38598;&#21512;&#20013;&#21508;&#20010;&#31867;&#21035;&#30340;&#26222;&#36941;&#24615;&#26469;&#35780;&#20272;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;Vendi&#20998;&#25968;&#23545;&#20110;&#32473;&#23450;&#38598;&#21512;&#20013;&#30340;&#27599;&#20010;&#39033;&#30446;&#37117;&#20197;&#19982;&#35813;&#39033;&#30446;&#30340;&#26222;&#36941;&#24615;&#25104;&#27604;&#20363;&#30340;&#25935;&#24863;&#24230;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#22312;&#39033;&#30446;&#26222;&#36941;&#24615;&#23384;&#22312;&#26174;&#33879;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#30456;&#20284;&#24615;&#25193;&#23637;&#20102;&#20854;&#20182;Hill&#25968;&#65292;&#20197;&#25552;&#20379;&#23545;&#31232;&#26377;&#25110;&#24120;&#35265;&#39033;&#30446;&#20998;&#37197;&#25935;&#24863;&#24230;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#26063;&#22810;&#26679;&#24615;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#20855;&#26377;&#19981;&#21516;&#25935;&#24863;&#24230;&#27700;&#24179;&#30340;Vendi&#20998;&#25968;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring diversity accurately is important for many scientific fields, including machine learning (ML), ecology, and chemistry. The Vendi Score was introduced as a generic similarity-based diversity metric that extends the Hill number of order q=1 by leveraging ideas from quantum statistical mechanics. Contrary to many diversity metrics in ecology, the Vendi Score accounts for similarity and does not require knowledge of the prevalence of the categories in the collection to be evaluated for diversity. However, the Vendi Score treats each item in a given collection with a level of sensitivity proportional to the item's prevalence. This is undesirable in settings where there is a significant imbalance in item prevalence. In this paper, we extend the other Hill numbers using similarity to provide flexibility in allocating sensitivity to rare or common items. This leads to a family of diversity metrics -- Vendi scores with different levels of sensitivity -- that can be used in a variety o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.08744</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#20219;&#21153;&#30340;&#30005;&#36335;&#32452;&#20214;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
Circuit Component Reuse Across Tasks in Transformer Language Models. (arXiv:2310.08744v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08744
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#30005;&#36335;&#20998;&#26512;&#21487;&#20197;&#25104;&#21151;&#22320;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25209;&#35780;&#26159;&#27599;&#20010;&#30005;&#36335;&#37117;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#20998;&#26512;&#19981;&#33021;&#20026;&#26356;&#39640;&#32423;&#30340;&#29702;&#35299;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#35777;&#25454;&#34920;&#26126;&#27934;&#23519;&#21147;&#65288;&#20851;&#20110;&#29305;&#23450;&#22836;&#37096;&#30340;&#20302;&#32423;&#21457;&#29616;&#21644;&#20851;&#20110;&#19968;&#33324;&#31639;&#27861;&#30340;&#39640;&#32423;&#21457;&#29616;&#65289;&#30830;&#23454;&#21487;&#20197;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Wang&#31561;&#20154;&#65288;2022&#65289;&#22312;&#38388;&#25509;&#23486;&#35821;&#35782;&#21035;&#20219;&#21153;&#65288;IOI&#65289;&#20013;&#21457;&#29616;&#30340;&#30005;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20010;&#30005;&#36335;&#22312;&#26356;&#22823;&#30340;GPT2&#27169;&#22411;&#19978;&#30340;&#37325;&#29616;&#65292;&#20197;&#21450;&#22312;&#30475;&#20284;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#22823;&#37096;&#20998;&#34987;&#22797;&#29992;&#26469;&#35299;&#20915;&#38382;&#39064;&#65306;&#24425;&#33394;&#29289;&#20307;&#65288;Ippolito&#21644;Callison-Burch&#65292;2023&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#20004;&#20010;&#20219;&#21153;&#24213;&#23618;&#30340;&#36807;&#31243;&#22312;&#21151;&#33021;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#30005;&#36335;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#20043;&#38388;&#26377;&#22823;&#32422;78&#65285;&#30340;&#37325;&#21472;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#24178;&#39044;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito &amp; Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#27604;&#20363;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#22810;&#31181;&#20010;&#20154;&#23646;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#38544;&#31169;&#27844;&#38706;&#30340;&#26032;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.07298</link><description>&lt;p&gt;
&#36229;&#36234;&#35760;&#24518;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#26469;&#20405;&#29359;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Beyond Memorization: Violating Privacy Via Inference with Large Language Models. (arXiv:2310.07298v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07298
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#27604;&#20363;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#22810;&#31181;&#20010;&#20154;&#23646;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#38544;&#31169;&#27844;&#38706;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38544;&#31169;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#21462;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#19978;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#24050;&#22823;&#24133;&#22686;&#24378;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#24403;&#21069;&#30340;LLMs&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#20174;&#25512;&#29702;&#26102;&#32473;&#20986;&#30340;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#26469;&#20405;&#29359;&#20010;&#20154;&#38544;&#31169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#39044;&#35757;&#32451;LLMs&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;Reddit&#20010;&#20154;&#36164;&#26009;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26174;&#31034;&#24403;&#21069;&#30340;LLMs&#21487;&#20197;&#25512;&#26029;&#20986;&#21508;&#31181;&#21508;&#26679;&#30340;&#20010;&#20154;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#20301;&#32622;&#12289;&#25910;&#20837;&#12289;&#24615;&#21035;&#65289;&#65292;&#22312;&#25104;&#26412;&#65288;100&#20493;&#65289;&#21644;&#26102;&#38388;&#65288;240&#20493;&#65289;&#19978;&#20165;&#38656;&#20154;&#31867;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#36798;&#21040;&#20102;&#26368;&#39640;1&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;85&#65285;&#65292;&#26368;&#39640;3&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;95.8&#65285;&#12290;&#38543;&#30528;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#30001;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20114;&#21160;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20405;&#29359;&#38544;&#31169;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#36890;&#36807;&#20284;&#20046;&#26080;&#20851;&#30340;&#23545;&#35805;&#35797;&#22270;&#25552;&#21462;&#20010;&#20154;&#20449;&#24687;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06387</link><description>&lt;p&gt;
&#21482;&#38656;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#21363;&#21487;&#23454;&#29616;&#36234;&#29425;&#21644;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20063;&#28014;&#29616;&#20986;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#25805;&#32437;LLM&#23545;&#40784;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#33539;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#23601;&#21487;&#20197;&#25805;&#32437;LLM&#22686;&#21152;&#25110;&#38477;&#20302;&#36234;&#29425;&#27010;&#29575;&#65292;&#21363;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30446;&#30340;&#30340;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#65288;ICA&#65289;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#65288;ICD&#65289;&#26041;&#27861;&#12290;ICA&#36890;&#36807;&#26500;&#36896;&#24694;&#24847;&#19978;&#19979;&#25991;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#65292;&#32780;ICD&#36890;&#36807;&#25298;&#32477;&#22238;&#31572;&#26377;&#23475;&#25552;&#31034;&#30340;&#31034;&#33539;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ICA&#21644;ICD&#22312;&#22686;&#21152;&#25110;&#38477;&#20302;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ICL&#22312;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#36866;&#24212;&#26680;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#24072;&#29983;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#29305;&#24449;&#23398;&#20064;&#21644; Grokking &#30340;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102; Grokking &#19982;&#30456;&#21464;&#29702;&#35770;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03789</link><description>&lt;p&gt;
&#22909;&#34920;&#31034;&#30340;&#28082;&#28404;&#65306;&#22312;&#20004;&#23618;&#32593;&#32476;&#20013; grokking &#20316;&#20026;&#19968;&#38454;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks. (arXiv:2310.03789v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#36866;&#24212;&#26680;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#24072;&#29983;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#29305;&#24449;&#23398;&#20064;&#21644; Grokking &#30340;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102; Grokking &#19982;&#30456;&#21464;&#29702;&#35770;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNN) &#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33021;&#22815;&#23398;&#20064;&#26032;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#36259;&#26041;&#38754;&#22312;&#26368;&#36817;&#25253;&#36947;&#30340; Grokking &#29616;&#35937;&#20013;&#34920;&#29616;&#24471;&#26368;&#20026;&#26126;&#26174;&#12290;&#34429;&#28982;&#20027;&#35201;&#20307;&#29616;&#20026;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#31361;&#21464;&#22686;&#21152;&#65292;&#20294; Grokking &#20063;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#36229;&#36234;&#25042;&#24816;&#23398;&#20064;/&#39640;&#26031;&#36807;&#31243; (GP) &#30340;&#29616;&#35937;&#65292;&#28041;&#21450;&#29305;&#24449;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#33258;&#36866;&#24212;&#26680;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#20855;&#26377;&#31435;&#26041;&#22810;&#39033;&#24335;&#21644;&#27169;&#21152;&#27861;&#25945;&#24072;&#30340;&#20004;&#20010;&#24072;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#20851;&#20110;&#29305;&#24449;&#23398;&#20064;&#21644; Grokking &#24615;&#36136;&#30340;&#20998;&#26512;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102; Grokking &#19982;&#30456;&#21464;&#29702;&#35770;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312; Grokking &#20043;&#21518;&#65292;DNN &#30340;&#29366;&#24577;&#31867;&#20284;&#20110;&#19968;&#38454;&#30456;&#21464;&#21518;&#30340;&#28151;&#21512;&#30456;&#12290;&#22312;&#36825;&#20010;&#28151;&#21512;&#30456;&#20013;&#65292;DNN &#29983;&#25104;&#20102;&#19982;&#20043;&#21069;&#26126;&#26174;&#19981;&#21516;&#30340;&#25945;&#24072;&#30340;&#26377;&#29992;&#20869;&#37096;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key property of deep neural networks (DNNs) is their ability to learn new features during training. This intriguing aspect of deep learning stands out most clearly in recently reported Grokking phenomena. While mainly reflected as a sudden increase in test accuracy, Grokking is also believed to be a beyond lazy-learning/Gaussian Process (GP) phenomenon involving feature learning. Here we apply a recent development in the theory of feature learning, the adaptive kernel approach, to two teacher-student models with cubic-polynomial and modular addition teachers. We provide analytical predictions on feature learning and Grokking properties of these models and demonstrate a mapping between Grokking and the theory of phase transitions. We show that after Grokking, the state of the DNN is analogous to the mixed phase following a first-order phase transition. In this mixed phase, the DNN generates useful internal representations of the teacher that are sharply distinct from those before the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;Softmax&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#35757;&#32451;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#30456;&#20851;&#24615;&#32467;&#26500;&#65292;&#23454;&#29616;&#21521;&#20840;&#23616;&#26368;&#20248;&#20540;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2310.02671</link><description>&lt;p&gt;
&#36229;&#36234;&#31283;&#23450;&#24615;&#65306;&#38543;&#26426;Softmax&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods. (arXiv:2310.02671v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;Softmax&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#35757;&#32451;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#30456;&#20851;&#24615;&#32467;&#26500;&#65292;&#23454;&#29616;&#21521;&#20840;&#23616;&#26368;&#20248;&#20540;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26159;&#19968;&#31181;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#35299;&#20915;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#12290;&#22312;&#26377;&#38480;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#36825;&#20123;&#38382;&#39064;&#19982;&#26368;&#20248;&#20572;&#27490;&#25110;&#29305;&#23450;&#20379;&#24212;&#38142;&#38382;&#39064;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#30456;&#20851;&#12290;&#19982;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;MDP&#19981;&#21516;&#65292;&#26368;&#20248;&#31574;&#30053;&#24182;&#19981;&#26159;&#31283;&#23450;&#30340;&#65292;&#31574;&#30053;&#24517;&#39035;&#22312;&#27599;&#20010;&#26102;&#26399;&#21333;&#29420;&#36827;&#34892;&#23398;&#20064;&#12290;&#23454;&#38469;&#19978;&#65292;&#24448;&#24448;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#65292;&#24573;&#35270;&#20102;&#21160;&#24577;&#35268;&#21010;&#25152;&#26263;&#31034;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#21644;&#31574;&#30053;&#26799;&#24230;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#65292;&#20854;&#20013;&#21442;&#25968;&#22312;&#26102;&#38388;&#19978;&#20197;&#21453;&#21521;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#23545;&#20110;&#34920;&#26684;Softmax&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#23545;&#21516;&#26102;&#21644;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#22312;&#31934;&#30830;&#26799;&#24230;&#21644;&#37319;&#26679;&#26799;&#24230;&#35774;&#32622;&#19979;&#21521;&#20840;&#23616;&#26368;&#20248;&#20540;&#36827;&#34892;&#20102;&#25910;&#25947;&#20998;&#26512;&#65292;&#19988;&#27809;&#26377;&#24341;&#20837;&#27491;&#21017;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21160;&#24577;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#30456;&#20851;&#24615;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov Decision Processes (MDPs) are a formal framework for modeling and solving sequential decision-making problems. In finite-time horizons such problems are relevant for instance for optimal stopping or specific supply chain problems, but also in the training of large language models. In contrast to infinite horizon MDPs optimal policies are not stationary, policies must be learned for every single epoch. In practice all parameters are often trained simultaneously, ignoring the inherent structure suggested by dynamic programming. This paper introduces a combination of dynamic programming and policy gradient called dynamic policy gradient, where the parameters are trained backwards in time. For the tabular softmax parametrisation we carry out the convergence analysis for simultaneous and dynamic policy gradient towards global optima, both in the exact and sampled gradient settings without regularisation. It turns out that the use of dynamic policy gradient training much better exploi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;GNN&#20013;&#32771;&#34385;&#23616;&#37096;&#24615;&#30340;&#22270;&#37325;&#36830;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#36807;&#24230;&#21387;&#32553;&#12289;&#20445;&#25345;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#25913;&#21892;&#38271;&#31243;&#20132;&#20114;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01668</link><description>&lt;p&gt;
GNN&#20013;&#30340;&#23616;&#37096;&#24863;&#30693;&#22270;&#37325;&#36830;
&lt;/p&gt;
&lt;p&gt;
Locality-Aware Graph-Rewiring in GNNs. (arXiv:2310.01668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;GNN&#20013;&#32771;&#34385;&#23616;&#37096;&#24615;&#30340;&#22270;&#37325;&#36830;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#36807;&#24230;&#21387;&#32553;&#12289;&#20445;&#25345;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#25913;&#21892;&#38271;&#31243;&#20132;&#20114;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#19978;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#34892;&#27169;&#22411;&#65292;&#36890;&#24120;&#36981;&#24490;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#22312;&#23545;&#37051;&#23621;&#33410;&#28857;&#30340;&#20449;&#24687;&#36827;&#34892;&#32858;&#21512;&#26102;&#65292;&#33410;&#28857;&#30340;&#29305;&#24449;&#20250;&#34987;&#36882;&#24402;&#22320;&#26356;&#26032;&#12290;&#34429;&#28982;&#22312;&#36755;&#20837;&#22270;&#19978;&#20132;&#25442;&#28040;&#24687;&#36171;&#20104;&#20102;GNNs&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#20294;&#20063;&#20351;&#24471;GNNs&#23481;&#26131;&#36807;&#24230;&#21387;&#32553;&#65292;&#20174;&#32780;&#26080;&#27861;&#25429;&#25417;&#32473;&#23450;&#22270;&#20013;&#30340;&#38271;&#31243;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22270;&#37325;&#36830;&#25216;&#26415;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#25913;&#21892;&#20449;&#24687;&#27969;&#30340;&#25163;&#27573;&#65292;&#36890;&#36807;&#25913;&#21464;&#22270;&#30340;&#36830;&#25509;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#37325;&#36830;&#30340;&#19977;&#20010;&#26399;&#26395;&#65306;&#65288;i&#65289;&#20943;&#23569;&#36807;&#24230;&#21387;&#32553;&#65292;&#65288;ii&#65289;&#23562;&#37325;&#22270;&#30340;&#23616;&#37096;&#24615;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#20445;&#25345;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#31354;&#38388;&#21644;&#39057;&#35889;&#37325;&#36830;&#25216;&#26415;&#20043;&#38388;&#23384;&#22312;&#30340;&#26681;&#26412;&#26435;&#34913;&#65307;&#23613;&#31649;&#21069;&#32773;&#36890;&#24120;&#28385;&#36275;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#20294;&#19981;&#28385;&#36275;&#65288;iii&#65289;&#65292;&#21518;&#32773;&#36890;&#24120;&#22312;&#28385;&#36275;&#65288;i&#65289;&#21644;&#65288;iii&#65289;&#30340;&#21516;&#26102;&#29306;&#29298;&#20102;&#65288;ii&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are popular models for machine learning on graphs that typically follow the message-passing paradigm, whereby the feature of a node is updated recursively upon aggregating information over its neighbors. While exchanging messages over the input graph endows GNNs with a strong inductive bias, it can also make GNNs susceptible to over-squashing, thereby preventing them from capturing long-range interactions in the given graph. To rectify this issue, graph rewiring techniques have been proposed as a means of improving information flow by altering the graph connectivity. In this work, we identify three desiderata for graph-rewiring: (i) reduce over-squashing, (ii) respect the locality of the graph, and (iii) preserve the sparsity of the graph. We highlight fundamental trade-offs that occur between spatial and spectral rewiring techniques; while the former often satisfy (i) and (ii) but not (iii), the latter generally satisfy (i) and (iii) at the expense of (ii)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2310.00809</link><description>&lt;p&gt;
&#25351;&#21521;&#22240;&#26524;&#22522;&#30784;&#27169;&#22411;: &#22240;&#26524;&#25512;&#26029;&#19982;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Foundation Model: on Duality between Causal Inference and Attention. (arXiv:2310.00809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00809
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#23545;&#20598;&#36830;&#25509;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#29702;&#35770;&#19978;&#23436;&#22791;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#30417;&#30563;&#22240;&#26524;&#23398;&#20064;&#65292;&#24182;&#22312;&#26032;&#25968;&#25454;&#30340;&#26410;&#35265;&#20219;&#21153;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#22240;&#26524;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.14726</link><description>&lt;p&gt;
PLMM&#65306;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PLMM: Personal Large Models on Mobile Devices. (arXiv:2309.14726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26356;&#36866;&#24212;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#22914;&#25945;&#32946;&#32972;&#26223;&#21644;&#29233;&#22909;&#12290;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#20026;&#19977;&#20010;&#32423;&#21035;&#65306;&#20010;&#20154;&#32423;&#21035;&#65292;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#12290;&#20010;&#20154;&#32423;&#21035;&#27169;&#22411;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#23545;&#29992;&#25143;&#30340;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#24182;&#20445;&#25252;&#20854;&#38544;&#31169;&#12290;&#19987;&#23478;&#32423;&#21035;&#27169;&#22411;&#19987;&#27880;&#20110;&#21512;&#24182;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22914;&#37329;&#34701;&#12289;IT&#21644;&#33402;&#26415;&#12290;&#20256;&#32479;&#27169;&#22411;&#19987;&#27880;&#20110;&#26222;&#36941;&#30693;&#35782;&#30340;&#21457;&#29616;&#21644;&#25552;&#21319;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#36825;&#26679;&#30340;&#20998;&#31867;&#20013;&#65292;&#20010;&#20154;&#27169;&#22411;&#30452;&#25509;&#19982;&#29992;&#25143;&#20132;&#20114;&#12290;&#23545;&#20110;&#25972;&#20010;&#31995;&#32479;&#26469;&#35828;&#65292;&#20010;&#20154;&#27169;&#22411;&#20855;&#26377;&#29992;&#25143;&#30340;&#65288;&#21152;&#23494;&#30340;&#65289;&#20010;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#36275;&#22815;&#23567;&#20197;&#22312;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#26368;&#21518;&#65292;&#23427;&#20204;&#36824;&#24517;&#39035;&#23454;&#26102;&#21709;&#24212;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user exper
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;&#30340;&#21019;&#26032;&#26694;&#26550;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13005</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#22495;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;
&lt;/p&gt;
&lt;p&gt;
Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains. (arXiv:2309.13005v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;&#30340;&#21019;&#26032;&#26694;&#26550;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#39046;&#22495;&#36716;&#31227;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#21508;&#31181;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#22788;&#29702;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#26102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#22312;&#36830;&#32493;&#30340;&#24207;&#21015;&#39046;&#22495;&#20013;&#36880;&#28176;&#21464;&#21270;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#22312;&#36825;&#20123;&#26032;&#39046;&#22495;&#20869;&#30340;&#27169;&#22411;&#25928;&#26524;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20855;&#26377;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#65288;CDSAE&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#12290;&#36825;&#31181;&#24182;&#34892;&#20998;&#31163;&#19981;&#20165;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#30340;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing the prevalence of domain shift as a common challenge in machine learning, various domain generalization (DG) techniques have been developed to enhance the performance of machine learning systems when dealing with out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data distributions can gradually change across a sequence of sequential domains. While current methodologies primarily focus on improving model effectiveness within these new domains, they often overlook fairness issues throughout the learning process. In response, we introduce an innovative framework called Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder (CDSAE). This approach effectively separates environmental information and sensitive attributes from the embedded representation of classification features. This concurrent separation not only greatly improves model generalization across diverse and unfamiliar domains but also effectively addresses challenges rela
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;&#20013;&#22797;&#21512;&#23376;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#26500;&#36896;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#65292;&#30740;&#31350;&#20854;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#35777;&#26126;&#20102;&#22797;&#21512;&#23376;&#25910;&#25947;&#26102;&#23545;&#24212;&#30340;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#20250;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23637;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.07169</link><description>&lt;p&gt;
&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#39057;&#29575;&#25910;&#25947;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Frequency Convergence of Complexon Shift Operators. (arXiv:2309.07169v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;&#20013;&#22797;&#21512;&#23376;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#26500;&#36896;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#65292;&#30740;&#31350;&#20854;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#35777;&#26126;&#20102;&#22797;&#21512;&#23376;&#25910;&#25947;&#26102;&#23545;&#24212;&#30340;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#20250;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23637;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;(TSP)&#21033;&#29992;&#21333;&#32431;&#24418;&#22797;&#21512;&#26469;&#24314;&#27169;&#27604;&#39030;&#28857;&#21644;&#36793;&#26356;&#39640;&#38454;&#30340;&#32467;&#26500;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;TSP&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#22797;&#21512;&#23376;&#30340;&#24191;&#20041;&#39640;&#38454;&#22270;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#22797;&#21512;&#23376;&#30340;&#27010;&#24565;&#65292;&#21363;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#30340;&#26497;&#38480;[1]&#12290;&#21463;&#22270;&#31227;&#20301;&#31639;&#23376;&#30340;&#31215;&#20998;&#31639;&#23376;&#24418;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26681;&#25454;&#22797;&#21512;&#23376;&#30340;&#25152;&#26377;&#21487;&#33021;&#23610;&#23544;&#30340;&#32452;&#20214;&#26500;&#36896;&#20102;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;(CSO)&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;CSO&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#19968;&#31867;&#26032;&#30340;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#19968;&#20010;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#25910;&#25947;&#21040;&#19968;&#20010;&#22797;&#21512;&#23376;&#26102;&#65292;&#30456;&#24212;&#30340;CSO&#30340;&#29305;&#24449;&#20540;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#26263;&#31034;&#20102;&#22312;&#22823;&#22411;&#21333;&#32431;&#24418;&#22797;&#21512;&#25110;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#19978;&#30340;&#23398;&#20064;&#21487;&#36716;&#31227;&#24615;&#65292;&#20174;&#32780;&#25512;&#24191;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological signal processing (TSP) utilizes simplicial complexes to model structures with higher order than vertices and edges. In this paper, we study the transferability of TSP via a generalized higher-order version of graphon, known as complexon. We recall the notion of a complexon as the limit of a simplicial complex sequence [1]. Inspired by the integral operator form of graphon shift operators, we construct a marginal complexon and complexon shift operator (CSO) according to components of all possible dimensions from the complexon. We investigate the CSO's eigenvalues and eigenvectors, and relate them to a new family of weighted adjacency matrices. We prove that when a simplicial complex sequence converges to a complexon, the eigenvalues of the corresponding CSOs converge to that of the limit complexon. These results hint at learning transferability on large simplicial complexes or simplicial complex sequences, which generalize the graphon signal processing framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#20840;&#23616;&#20449;&#24687;&#21644;&#26412;&#22320;&#29305;&#24449;&#65292;&#23454;&#29616;&#23545;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#36890;&#29992;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14104</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#21487;&#36801;&#31227;&#26412;&#22320;&#31574;&#30053;&#30340;&#38598;&#25104;&#65292;&#23454;&#29616;&#23545;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#36890;&#29992;&#31070;&#32463;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizable Neural Solvers for Vehicle Routing Problems via Ensemble with Transferrable Local Policy. (arXiv:2308.14104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#20840;&#23616;&#20449;&#24687;&#21644;&#26412;&#22320;&#29305;&#24449;&#65292;&#23454;&#29616;&#23545;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#36890;&#29992;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#34987;&#24212;&#29992;&#20110;&#24110;&#21161;&#35299;&#20915;NP&#22256;&#38590;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#30446;&#21069;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#24335;&#26159;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26500;&#24314;&#35299;&#20915;&#26041;&#26696;&#65292;&#30001;&#20110;&#20854;&#39640;&#25928;&#24615;&#21644;&#23545;&#19987;&#19994;&#30693;&#35782;&#35201;&#27714;&#36739;&#23569;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29992;&#20110;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRPs&#65289;&#30340;&#31070;&#32463;&#26500;&#24314;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#20855;&#26377;&#25351;&#23450;&#33410;&#28857;&#20998;&#24067;&#21644;&#26377;&#38480;&#35268;&#27169;&#30340;&#21512;&#25104;&#38382;&#39064;&#23454;&#20363;&#19978;&#65292;&#23548;&#33268;&#22312;&#36890;&#24120;&#28041;&#21450;&#22797;&#26434;&#19988;&#26410;&#30693;&#33410;&#28857;&#20998;&#24067;&#20197;&#21450;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#19978;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#20351;&#31070;&#32463;VRP&#27714;&#35299;&#22120;&#26356;&#23454;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20174;&#26412;&#22320;&#21487;&#36801;&#31227;&#25299;&#25169;&#29305;&#24449;&#20013;&#23398;&#20064;&#30340;&#36741;&#21161;&#31574;&#30053;&#65292;&#31216;&#20026;&#26412;&#22320;&#31574;&#30053;&#65292;&#24182;&#19982;&#20856;&#22411;&#30340;&#26500;&#24314;&#31574;&#30053;&#65288;&#20174;VRP&#23454;&#20363;&#30340;&#20840;&#23616;&#20449;&#24687;&#20013;&#23398;&#20064;&#65289;&#30456;&#32467;&#21512;&#24418;&#25104;&#19968;&#20010;&#38598;&#25104;&#31574;&#30053;&#12290;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#65292;&#32858;&#21512;&#30340;&#31574;&#30053;&#30456;&#20114;&#21327;&#20316;&#21644;&#20114;&#34917;&#65292;&#20197;&#25552;&#21319;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has been adapted to help solve NP-hard combinatorial optimization problems. One prevalent way is learning to construct solutions by deep neural networks, which has been receiving more and more attention due to the high efficiency and less requirement for expert knowledge. However, many neural construction methods for Vehicle Routing Problems (VRPs) focus on synthetic problem instances with specified node distributions and limited scales, leading to poor performance on real-world problems which usually involve complex and unknown node distributions together with large scales. To make neural VRP solvers more practical, we design an auxiliary policy that learns from the local transferable topological features, named local policy, and integrate it with a typical construction policy (which learns from the global information of VRP instances) to form an ensemble policy. With joint training, the aggregated policies perform cooperatively and complementarily to boost generaliza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12517</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#22870;&#21169;&#65292;&#36824;&#26377;&#32422;&#26463;&#65306;&#29992;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#30340;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#24182;&#20351;&#29992;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20855;&#26377;&#33258;&#28982;&#21160;&#20316;&#39118;&#26684;&#21644;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#20986;&#33394;&#25511;&#21046;&#22120;&#26159;&#36890;&#36807;&#36827;&#34892;&#22823;&#37327;&#22870;&#21169;&#24037;&#31243;&#32780;&#24320;&#21457;&#30340;&#65292;&#35813;&#36807;&#31243;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#38656;&#35201;&#35774;&#35745;&#22823;&#37327;&#22870;&#21169;&#39033;&#24182;&#30830;&#23450;&#21512;&#36866;&#30340;&#22870;&#21169;&#31995;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21516;&#26102;&#21253;&#21547;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#35753;&#24037;&#31243;&#24072;&#33021;&#22815;&#36866;&#24403;&#22320;&#21453;&#26144;&#20182;&#20204;&#23545;&#32422;&#26463;&#30340;&#24847;&#22270;&#24182;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#22788;&#29702;&#23427;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32422;&#26463;&#31867;&#22411;&#21644;&#19968;&#31181;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#34987;&#24212;&#29992;&#20110;&#35757;&#32451;&#19981;&#21516;&#24418;&#24577;&#21644;&#29289;&#29702;&#23646;&#24615;&#30340;&#20960;&#20010;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#32780;&#23545;&#25239;&#24615;RCPG&#36890;&#36807;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#24335;&#30452;&#25509;&#21644;&#22686;&#37327;&#23398;&#20064;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2308.11267</link><description>&lt;p&gt;
&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#21644;&#23545;&#25239;&#24615;&#31574;&#30053;&#26799;&#24230;&#22312;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes. (arXiv:2308.11267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#32780;&#23545;&#25239;&#24615;RCPG&#36890;&#36807;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#24335;&#30452;&#25509;&#21644;&#22686;&#37327;&#23398;&#20064;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RCMDP&#65289;&#26159;&#19968;&#20010;&#26368;&#36817;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#24314;&#27169;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#22312;&#36716;&#31227;&#21160;&#24577;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#23545;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27169;&#25311;RCMDPs&#38656;&#35201;&#22522;&#20110;&#27599;&#20010;&#29366;&#24577;&#30340;&#20540;&#20272;&#35745;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#36825;&#31181;&#26041;&#27861;&#20043;&#21069;&#22312;&#40065;&#26834;&#32422;&#26463;&#31574;&#30053;&#26799;&#24230;&#65288;RCPG&#65289;&#20013;&#20351;&#29992;&#36807;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#31216;&#20026;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#32780;&#19981;&#26159;&#20540;&#25110;&#32422;&#26463;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#20174;&#32780;&#20462;&#25913;RCPG&#12290;&#23545;&#25239;&#24615;RCPG&#20063;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#20844;&#24335;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#20294;&#26159;&#23558;&#20854;&#20316;&#20026;&#23545;&#25239;&#31574;&#30053;&#30452;&#25509;&#21644;&#22686;&#37327;&#22320;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy throug
&lt;/p&gt;</description></item><item><title>GPLaSDI&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#23436;&#20840;&#38454;&#25968;&#30340;PDE&#35299;&#26144;&#23556;&#21040;&#28508;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#25554;&#20540;&#21644;&#35299;&#20915;ODE&#31995;&#32479;&#36827;&#34892;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;ROM&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.05882</link><description>&lt;p&gt;
GPLaSDI: &#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder. (arXiv:2308.05882v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05882
&lt;/p&gt;
&lt;p&gt;
GPLaSDI&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#23436;&#20840;&#38454;&#25968;&#30340;PDE&#35299;&#26144;&#23556;&#21040;&#28508;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#25554;&#20540;&#21644;&#35299;&#20915;ODE&#31995;&#32479;&#36827;&#34892;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;ROM&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#36825;&#23548;&#33268;&#20102;&#20943;&#23569;&#38454;&#25968;&#27169;&#22411;(ROMs)&#30340;&#21457;&#23637;&#65292;&#20854;&#31934;&#30830;&#24615;&#39640;&#20110;&#23436;&#20840;&#38454;&#25968;&#27169;&#22411;(FOMs)&#20294;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#23454;&#29616;&#20102;&#38750;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#30340;&#21019;&#24314;&#65292;&#20363;&#22914;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;(LaSDI)&#12290;LaSDI&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#23436;&#20840;&#38454;&#25968;&#30340;PDE&#35299;&#26144;&#23556;&#21040;&#28508;&#31354;&#38388;&#65292;&#24182;&#23398;&#20064;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#30340;ODE&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#20943;&#23569;&#30340;&#28508;&#31354;&#38388;&#20013;&#25554;&#20540;&#21644;&#35299;&#20915;ODE&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#36755;&#20837;&#35299;&#30721;&#22120;&#26469;&#36827;&#34892;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;ROM&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;(GP)&#30340;&#26032;&#22411;LaSDI&#26694;&#26550;&#65292;&#29992;&#20110;&#28508;&#31354;&#38388;ODE&#25554;&#20540;&#12290;&#20351;&#29992;GP&#24102;&#26469;&#20004;&#20010;&#37325;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#33021;&#22815;&#37327;&#21270;ROM&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#36825;&#20010;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerically solving partial differential equations (PDEs) can be challenging and computationally expensive. This has led to the development of reduced-order models (ROMs) that are accurate but faster than full order models (FOMs). Recently, machine learning advances have enabled the creation of non-linear projection methods, such as Latent Space Dynamics Identification (LaSDI). LaSDI maps full-order PDE solutions to a latent space using autoencoders and learns the system of ODEs governing the latent space dynamics. By interpolating and solving the ODE system in the reduced latent space, fast and accurate ROM predictions can be made by feeding the predicted latent space dynamics into the decoder. In this paper, we introduce GPLaSDI, a novel LaSDI-based framework that relies on Gaussian process (GP) for latent space ODE interpolations. Using GPs offers two significant advantages. First, it enables the quantification of uncertainty over the ROM predictions. Second, leveraging this predict
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#65292;&#21152;&#36895;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15034</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#21152;&#36895;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Speeding up Fourier Neural Operators via Mixed Precision. (arXiv:2307.15034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15034
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#65292;&#21152;&#36895;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#35299;&#31639;&#22120;&#30340;&#20195;&#29702;&#26144;&#23556;&#12290;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#36890;&#24120;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#28857;&#65292;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#37117;&#26159;&#37325;&#35201;&#29942;&#39048;&#12290;&#34429;&#28982;&#23545;&#20110;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#26377;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#25216;&#26415;&#65292;&#20294;&#37027;&#20123;&#21482;&#36866;&#29992;&#20110;&#26377;&#38480;&#32500;&#24230;&#19978;&#30340;&#23454;&#20540;&#25968;&#25454;&#31867;&#22411;&#65292;&#22240;&#27492;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#22312;&#22797;&#20540;&#65288;&#20613;&#37324;&#21494;&#65289;&#22495;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#37325;&#35201;&#25805;&#20316;&#30340;FNO&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#26412;&#36523;&#23601;&#26159;&#19968;&#27425;&#36817;&#20284;&#65288;&#30001;&#20110;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;&#23384;&#22312;&#65289;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#20197;&#23436;&#20840;&#31934;&#24230;&#25191;&#34892;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#65288;i&#65289;&#23545;&#20351;&#29992;&#20840;&#31934;&#24230;&#21644;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#30340;FNO&#36827;&#34892;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#38388;&#21078;&#26512;&#65292;&#65288;ii&#65289;&#23545;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#35757;&#32451;&#36807;&#31243;&#65292;&#22823;&#22823;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memor
&lt;/p&gt;</description></item><item><title>ExeDec&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#21512;&#25104;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#27979;&#25191;&#34892;&#23376;&#30446;&#26631;&#24182;&#22312;&#27599;&#20010;&#27493;&#39588;&#30340;&#31243;&#24207;&#25191;&#34892;&#30340;&#25351;&#23548;&#19979;&#36880;&#27493;&#35299;&#20915;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#21512;&#25104;&#24615;&#33021;&#21644;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13883</link><description>&lt;p&gt;
ExeDec: &#22312;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#20013;&#36827;&#34892;&#25191;&#34892;&#20998;&#35299;&#20197;&#23454;&#29616;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis. (arXiv:2307.13883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13883
&lt;/p&gt;
&lt;p&gt;
ExeDec&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#21512;&#25104;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#27979;&#25191;&#34892;&#23376;&#30446;&#26631;&#24182;&#22312;&#27599;&#20010;&#27493;&#39588;&#30340;&#31243;&#24207;&#25191;&#34892;&#30340;&#25351;&#23548;&#19979;&#36880;&#27493;&#35299;&#20915;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#21512;&#25104;&#24615;&#33021;&#21644;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32534;&#20889;&#31243;&#24207;&#26102;&#65292;&#20154;&#20204;&#36890;&#36807;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#19988;&#26356;&#29087;&#24713;&#30340;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#12290;&#34429;&#28982;&#34913;&#37327;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#30340;&#33021;&#21147;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#25105;&#20204;&#21487;&#20197;&#34913;&#37327;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#65292;&#21363;&#32463;&#36807;&#35757;&#32451;&#22312;&#36739;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#19978;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#31243;&#24207;&#21512;&#25104;&#20013;&#24076;&#26395;&#30340;&#20960;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#32452;&#21512;&#27867;&#21270;&#65292;&#24182;&#24418;&#25104;&#19968;&#20010;&#20803;&#22522;&#20934;&#65292;&#29992;&#20110;&#20026;&#20004;&#20010;&#21463;&#27426;&#36814;&#30340;&#25968;&#25454;&#38598;RobustFill&#21644;DeepCoder&#21019;&#24314;&#27867;&#21270;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#35299;&#30340;&#21512;&#25104;&#31574;&#30053;ExeDec&#65292;&#23427;&#36890;&#36807;&#22312;&#27599;&#20010;&#27493;&#39588;&#30340;&#31243;&#24207;&#25191;&#34892;&#30340;&#25351;&#23548;&#19979;&#36880;&#27493;&#39044;&#27979;&#25191;&#34892;&#23376;&#30446;&#26631;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;ExeDec&#20855;&#26377;&#26356;&#20339;&#30340;&#21512;&#25104;&#24615;&#33021;&#21644;&#22823;&#22823;&#25913;&#36827;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaBE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.02891</link><description>&lt;p&gt;
BaBE:&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#22686;&#24378;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables. (arXiv:2307.02891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaBE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20004;&#20010;&#32676;&#20307;&#20043;&#38388;&#19981;&#20844;&#24179;&#27495;&#35270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#30340;BaBE (Bayesian Bias Elimination)&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;&#30340;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of unfair discrimination between two groups and propose a pre-processing method to achieve fairness. Corrective methods like statistical parity usually lead to bad accuracy and do not really achieve fairness in situations where there is a correlation between the sensitive attribute S and the legitimate attribute E (explanatory variable) that should determine the decision. To overcome these drawbacks, other notions of fairness have been proposed, in particular, conditional statistical parity and equal opportunity. However, E is often not directly observable in the data, i.e., it is a latent variable. We may observe some other variable Z representing E, but the problem is that Z may also be affected by S, hence Z itself can be biased. To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an approach based on a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of E for a given Z for each grou
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.15447</link><description>&lt;p&gt;
&#23545;&#40784;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#23545;&#25239;&#23545;&#40784;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are aligned neural networks adversarially aligned?. (arXiv:2306.15447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15447
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29616;&#22312;&#34987;&#35843;&#25972;&#20026;&#19982;&#20854;&#21019;&#24314;&#32773;&#30340;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#65292;&#21363;"&#26377;&#30410;&#19988;&#26080;&#23475;"&#12290;&#36825;&#20123;&#27169;&#22411;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#38382;&#39064;&#32473;&#20986;&#26377;&#30410;&#30340;&#22238;&#31572;&#65292;&#20294;&#25298;&#32477;&#22238;&#31572;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#30340;&#35831;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#32469;&#36807;&#23545;&#40784;&#23581;&#35797;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19982;&#26500;&#36896;&#26368;&#22351;&#24773;&#20917;&#36755;&#20837;&#65288;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#30340;&#23545;&#25239;&#29992;&#25143;&#20132;&#20114;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20445;&#25345;&#22810;&#22823;&#31243;&#24230;&#30340;&#23545;&#40784;&#12290;&#36825;&#20123;&#36755;&#20837;&#34987;&#35774;&#35745;&#25104;&#23548;&#33268;&#27169;&#22411;&#21457;&#20986;&#26412;&#24212;&#31105;&#27490;&#30340;&#26377;&#23475;&#20869;&#23481;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20248;&#21270;&#25915;&#20987;&#25163;&#27861;&#22312;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#26041;&#38754;&#30340;&#19981;&#36275;&#20043;&#22788;&#65306;&#21363;&#20351;&#22312;&#24403;&#21069;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25915;&#20987;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#25915;&#20987;&#30340;&#22833;&#36133;&#19981;&#24212;&#34987;&#35270;&#20026;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#20173;&#28982;&#20445;&#25345;&#23545;&#40784;&#30340;&#35777;&#26126;&#12290;&#20294;&#26159;&#36817;&#26399;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#26159;&#22810;&#27169;&#24577;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.  However the recent trend in large-scale ML models is multim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#20808;&#20351;&#29992;&#21333;&#35843;&#38750;&#22686;&#20989;&#25968;&#36827;&#34892;&#26102;&#38388;&#27493;&#37319;&#26679;&#30340;NeRF&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;NeRF&#20248;&#21270;&#36807;&#31243;&#21644;&#24471;&#20998;&#33976;&#39311;&#20013;&#22343;&#21248;&#26102;&#38388;&#27493;&#37319;&#26679;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12422</link><description>&lt;p&gt;
DreamTime: &#19968;&#31181;&#25913;&#36827;&#30340;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#21019;&#20316;&#20248;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation. (arXiv:2306.12422v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#20808;&#20351;&#29992;&#21333;&#35843;&#38750;&#22686;&#20989;&#25968;&#36827;&#34892;&#26102;&#38388;&#27493;&#37319;&#26679;&#30340;NeRF&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;NeRF&#20248;&#21270;&#36807;&#31243;&#21644;&#24471;&#20998;&#33976;&#39311;&#20013;&#22343;&#21248;&#26102;&#38388;&#27493;&#37319;&#26679;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#39044;&#20808;&#35757;&#32451;&#20102;&#25968;&#21313;&#20159;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#24471;&#20998;&#33976;&#39311;&#26469;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#30340;&#21019;&#24314;&#12290; &#28982;&#32780;&#65292;&#25152;&#24471;&#21040;&#30340;3D&#27169;&#22411;&#23384;&#22312;&#20004;&#20010;&#23616;&#38480;&#24615;&#65306;&#65288;a&#65289;&#36136;&#37327;&#38382;&#39064;&#65292;&#20363;&#22914;&#39281;&#21644;&#30340;&#39068;&#33394;&#21644;Janus&#38382;&#39064;&#65307;&#65288;b&#65289;&#19982;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#30456;&#27604;&#65292;&#26497;&#20302;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;NeRF&#20248;&#21270;&#36807;&#31243;&#21644;&#24471;&#20998;&#33976;&#39311;&#20013;&#22343;&#21248;&#26102;&#38388;&#27493;&#37319;&#26679;&#20043;&#38388;&#30340;&#20914;&#31361;&#26159;&#36825;&#20123;&#23616;&#38480;&#24615;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20914;&#31361;&#65292;&#25105;&#20204;&#24314;&#35758;&#20248;&#20808;&#20351;&#29992;&#21333;&#35843;&#38750;&#22686;&#20989;&#25968;&#36827;&#34892;&#26102;&#38388;&#27493;&#37319;&#26679;&#65292;&#36825;&#20351;&#24471;NeRF&#20248;&#21270;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#36807;&#31243;&#30456;&#19968;&#33268;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#37325;&#35774;&#35745;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#21040;3D&#20869;&#23481;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models pre-trained on billions of image-text pairs have recently enabled text-to-3D content creation by optimizing a randomly initialized Neural Radiance Fields (NeRF) with score distillation. However, the resultant 3D models exhibit two limitations: (a) quality concerns such as saturated color and the Janus problem; (b) extremely low diversity comparing to text-guided image synthesis. In this paper, we show that the conflict between NeRF optimization process and uniform timestep sampling in score distillation is the main reason for these limitations. To resolve this conflict, we propose to prioritize timestep sampling with monotonically non-increasing functions, which aligns NeRF optimization with the sampling process of diffusion model. Extensive experiments show that our simple redesign significantly improves text-to-3D content creation with higher quality and diversity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2306.11695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Effective Pruning Approach for Large Language Models. (arXiv:2306.11695v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#30340;&#33258;&#28982;&#20505;&#36873;&#23545;&#35937;&#65306;&#36825;&#20123;&#26041;&#27861;&#22312;&#21162;&#21147;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20002;&#24323;&#20102;&#32593;&#32476;&#26435;&#37325;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#21313;&#20159;&#32423;&#21035;&#30340;LLMs&#26469;&#35828;&#24456;&#23569;&#21487;&#34892;&#65292;&#35201;&#20040;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#20381;&#36182;&#20108;&#38454;&#20449;&#24687;&#30340;&#26435;&#37325;&#37325;&#26500;&#38382;&#39064;&#65292;&#36825;&#20063;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#31216;&#20026;Wanda&#65288;&#22522;&#20110;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#21098;&#26525;&#65289;&#65292;&#26088;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#21463;&#21040;&#26368;&#36817;&#23545;LLMs&#20013;&#20986;&#29616;&#30340;&#22823;&#24133;&#29305;&#24449;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#36755;&#20986;&#19978;&#25353;&#29031;&#26435;&#37325;&#21644;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#30456;&#20056;&#30340;&#26368;&#23567;&#24133;&#24230;&#26469;&#21098;&#26525;&#26435;&#37325;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Wanda&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#65292;&#21098;&#26525;&#21518;&#30340;LLM&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;LLaMA&#21644;LLaMA-2&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;Wanda&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across vari
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#20391;&#38754;&#20449;&#24687;&#21644;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32416;&#27491;&#22240;&#27835;&#30103;&#21464;&#37327;&#19981;&#20934;&#30830;&#27979;&#37327;&#24341;&#36215;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#28145;&#24230;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#21644;&#20998;&#25674;&#26435;&#37325;&#21464;&#20998;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.10614</link><description>&lt;p&gt;
&#24102;&#26377;&#22024;&#26434;&#27835;&#30103;&#21644;&#27809;&#26377;&#20391;&#38754;&#20449;&#24687;&#30340;&#21487;&#35782;&#21035;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Identifiable causal inference with noisy treatment and no side information. (arXiv:2306.10614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#20391;&#38754;&#20449;&#24687;&#21644;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32416;&#27491;&#22240;&#27835;&#30103;&#21464;&#37327;&#19981;&#20934;&#30830;&#27979;&#37327;&#24341;&#36215;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#28145;&#24230;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#21644;&#20998;&#25674;&#26435;&#37325;&#21464;&#20998;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#22240;&#26524;&#25512;&#26029;&#22330;&#26223;&#20013;&#65292;&#27835;&#30103;&#65288;&#21363;&#21407;&#22240;&#65289;&#21464;&#37327;&#30340;&#27979;&#37327;&#23384;&#22312;&#19981;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#22312;&#27969;&#34892;&#30149;&#23398;&#25110;&#35745;&#37327;&#32463;&#27982;&#23398;&#20013;&#12290;&#26410;&#33021;&#32416;&#27491;&#27979;&#37327;&#35823;&#24046;&#30340;&#24433;&#21709;&#21487;&#33021;&#23548;&#33268;&#20559;&#24046;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#20174;&#22240;&#26524;&#35270;&#35282;&#30740;&#31350;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20801;&#35768;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#24182;&#19988;&#19981;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#20391;&#38754;&#20449;&#24687;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#22330;&#26223;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#23427;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#36830;&#32493;&#30340;&#27835;&#30103;&#21464;&#37327;&#65292;&#35813;&#21464;&#37327;&#27979;&#37327;&#19981;&#20934;&#30830;&#12290;&#24314;&#31435;&#22312;&#29616;&#26377;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#21363;&#20351;&#27809;&#26377;&#27979;&#37327;&#35823;&#24046;&#26041;&#24046;&#25110;&#20854;&#20182;&#20391;&#38754;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#28145;&#24230;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#39640;&#26031;&#26465;&#20214;&#30001;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#24182;&#19988;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#25674;&#26435;&#37325;&#21464;&#20998;&#23458;&#35266;&#20989;&#25968;&#26469;&#35757;&#32451;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In some causal inference scenarios, the treatment (i.e. cause) variable is measured inaccurately, for instance in epidemiology or econometrics. Failure to correct for the effect of this measurement error can lead to biased causal effect estimates. Previous research has not studied methods that address this issue from a causal viewpoint while allowing for complex nonlinear dependencies and without assuming access to side information. For such as scenario, this paper proposes a model that assumes a continuous treatment variable which is inaccurately measured. Building on existing results for measurement error models, we prove that our model's causal effect estimates are identifiable, even without knowledge of the measurement error variance or other side information. Our method relies on a deep latent variable model where Gaussian conditionals are parameterized by neural networks, and we develop an amortized importance-weighted variational objective for training the model. Empirical resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SequenceMatch&#30340;&#24102;&#26377;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#24207;&#21015;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#26469;&#20943;&#23569;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22797;&#21512;&#35823;&#24046;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#22238;&#28335;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05426</link><description>&lt;p&gt;
SequenceMatch&#65306;&#24102;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking. (arXiv:2306.05426v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SequenceMatch&#30340;&#24102;&#26377;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#24207;&#21015;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#26469;&#20943;&#23569;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22797;&#21512;&#35823;&#24046;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#22238;&#28335;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35266;&#27979;&#20540;&#30340;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20284;&#28982;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26368;&#22823;&#20284;&#28982;&#65288;MLE&#65289;&#30446;&#26631;&#19981;&#19968;&#23450;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;&#39640;&#36136;&#37327;&#24207;&#21015;&#30340;&#19979;&#28216;&#29992;&#20363;&#30456;&#21305;&#37197;&#12290;MLE&#30446;&#26631;&#25353;&#29031;&#25968;&#25454;&#20998;&#24067;&#19979;&#24207;&#21015;&#30340;&#39057;&#29575;&#21152;&#26435;&#65292;&#19981;&#25552;&#20379;&#27169;&#22411;&#22312;&#20998;&#24067;&#20043;&#22806;&#34892;&#20026;&#30340;&#25351;&#23548;&#65292;&#36825;&#20250;&#23548;&#33268;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#22797;&#21512;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22797;&#21512;&#35823;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#24207;&#21015;&#29983;&#25104;&#23450;&#20026;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20998;&#24067;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#65292;&#21253;&#25324;&#32771;&#34385;&#20986;&#20998;&#24067;&#24207;&#21015;&#30340;&#20998;&#27495;&#12290;IL&#26694;&#26550;&#36824;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#24341;&#20837;&#22238;&#26684;&#21160;&#20316;&#26469;&#24341;&#20837;&#22238;&#28335;&#12290;&#36825;&#36827;&#19968;&#27493;&#20943;&#36731;&#20102;&#22797;&#21512;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compound
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#19968;&#31181;&#26131;&#20110;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20351;&#20854;&#20135;&#29983;&#22522;&#20110;&#33033;&#20914;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#36827;&#34892;&#20102;&#33033;&#20914;&#32593;&#32476;&#30340;&#35757;&#32451;&#21518;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03623</link><description>&lt;p&gt;
&#32463;&#20856;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#33033;&#20914;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Spike-based computation using classical recurrent neural networks. (arXiv:2306.03623v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#19968;&#31181;&#26131;&#20110;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20351;&#20854;&#20135;&#29983;&#22522;&#20110;&#33033;&#20914;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#36827;&#34892;&#20102;&#33033;&#20914;&#32593;&#32476;&#30340;&#35757;&#32451;&#21518;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#36890;&#20449;&#20165;&#30001;&#20107;&#20214;&#25110;&#25152;&#35859;&#30340;&#33033;&#20914;&#32452;&#25104;&#12290;&#36825;&#31181;&#29305;&#24615;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36827;&#34892;&#24322;&#27493;&#21644;&#31232;&#30095;&#35745;&#31639;&#65292;&#24182;&#22240;&#27492;&#22312;&#19987;&#29992;&#30828;&#20214;&#19978;&#36816;&#34892;&#26102;&#22823;&#24133;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#37319;&#29992;&#19968;&#31181;&#23545;&#31216;&#30340;&#26041;&#27861;&#65306;&#20462;&#25913;&#19968;&#31181;&#24050;&#30693;&#30340;&#12289;&#26131;&#20110;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20351;&#20854;&#20135;&#29983;&#22522;&#20110;&#33033;&#20914;&#30340;&#35745;&#31639;&#12290;&#36890;&#36807;&#26126;&#30830;&#24341;&#20837;&#33033;&#20914;&#38408;&#20540;&#21644;&#37325;&#32622;&#26426;&#21046;&#65292;&#25105;&#20204;&#20351;&#32593;&#32476;&#33021;&#22815;&#20165;&#20351;&#29992;&#33033;&#20914;&#26469;&#25191;&#34892;&#21069;&#21521;&#21644;&#24490;&#29615;&#35745;&#31639;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20462;&#25913;&#21518;&#30340;&#26500;&#26550;&#26082;&#21487;&#20197;&#23454;&#29616;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;ImageNet&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks are a type of artificial neural networks in which communication between neurons is only made of events, also called spikes. This property allows neural networks to make asynchronous and sparse computations and therefore to drastically decrease energy consumption when run on specialized hardware. However, training such networks is known to be difficult, mainly due to the non-differentiability of the spike activation, which prevents the use of classical backpropagation. This is because state-of-the-art spiking neural networks are usually derived from biologically-inspired neuron models, to which are applied machine learning methods for training. Nowadays, research about spiking neural networks focuses on the design of training algorithms whose goal is to obtain networks that compete with their non-spiking version on specific tasks. In this paper, we attempt the symmetrical approach: we modify the dynamics of a well-known, easily trainable type of recurrent neural 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.02786</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36335;&#24452;&#20960;&#20309;&#23548;&#33322;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;
&lt;/p&gt;
&lt;p&gt;
Navigating Explanatory Multiverse Through Counterfactual Path Geometry. (arXiv:2306.02786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#35299;&#37322;&#65288;&#19981;&#36879;&#26126;&#30340;&#65289;&#39044;&#27979;&#27169;&#22411;&#20915;&#31574;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#20854;&#29983;&#25104;&#24448;&#24448;&#21463;&#21040;&#31639;&#27861;&#21644;&#29305;&#23450;&#39046;&#22495;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#22914;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#21644;&#23646;&#24615;&#30340;&#65288;&#19981;&#65289;&#21487;&#21464;&#24615;&#25110;&#21464;&#21270;&#30340;&#26041;&#21521;&#24615;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20854;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#38500;&#20102;&#23545;&#21453;&#20107;&#23454;&#23454;&#20363;&#26412;&#36523;&#30340;&#35201;&#27714;&#20043;&#22806;&#65292;&#24050;&#30693;&#31639;&#27861;&#21487;&#34892;&#24615;&#36335;&#24452;&#19982;&#20107;&#23454;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21363;&#31639;&#27861;&#21487;&#35785;&#27714;&#65292;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#25216;&#26415;&#32771;&#34385;&#22240;&#32032;&#12290;&#23613;&#31649;&#36825;&#20004;&#20010;&#35201;&#27714;&#30830;&#20445;&#20102;&#26053;&#31243;&#30340;&#27493;&#39588;&#21644;&#30446;&#30340;&#22320;&#30340;&#21512;&#29702;&#24615;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#24573;&#30053;&#20102;&#36825;&#31181;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#26053;&#31243;&#65307;&#28982;&#21518;&#23637;&#31034;&#20102;&#22914;&#20309;&#23548;&#33322;&#12289;&#25512;&#29702;&#21644;&#27604;&#36739;&#36825;&#20123;&#36712;&#36857;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations are the de facto standard when tasked with interpreting decisions of (opaque) predictive models. Their generation is often subject to algorithmic and domain-specific constraints -- such as density-based feasibility and attribute (im)mutability or directionality of change -- that aim to maximise their real-life utility. In addition to desiderata with respect to the counterfactual instance itself, existence of a viable path connecting it with the factual data point, known as algorithmic recourse, has become an important technical consideration. While both of these requirements ensure that the steps of the journey as well as its destination are admissible, current literature neglects the multiplicity of such counterfactual paths. To address this shortcoming we introduce the novel concept of explanatory multiverse that encompasses all the possible counterfactual journeys; we then show how to navigate, reason about and compare the geometry of these trajectories -
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21644;Gumbel&#20256;&#25773;&#22312;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#33719;&#24471;&#32467;&#26500;&#21270;&#31232;&#30095;&#26368;&#20248;&#36335;&#24452;&#65292;&#20174;&#32780;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#20381;&#36182;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#32467;&#26500;&#29305;&#24449;&#20449;&#24687;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#25991;&#26412;&#36716;&#35821;&#38899;&#21644;&#27468;&#22768;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.02568</link><description>&lt;p&gt;
Gumbel&#20256;&#25773;&#19979;&#30340;&#28508;&#22312;&#26368;&#20248;&#36335;&#24452;&#21464;&#20998;&#36125;&#21494;&#26031;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Latent Optimal Paths by Gumbel Propagation for Variational Bayesian Dynamic Programming. (arXiv:2306.02568v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21644;Gumbel&#20256;&#25773;&#22312;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#33719;&#24471;&#32467;&#26500;&#21270;&#31232;&#30095;&#26368;&#20248;&#36335;&#24452;&#65292;&#20174;&#32780;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#20381;&#36182;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#32467;&#26500;&#29305;&#24449;&#20449;&#24687;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#25991;&#26412;&#36716;&#35821;&#38899;&#21644;&#27468;&#22768;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21644;Gumbel&#20256;&#25773;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#33719;&#21462;&#32467;&#26500;&#21270;&#31232;&#30095;&#26368;&#20248;&#36335;&#24452;&#12290;&#25105;&#20204;&#36890;&#36807;&#27010;&#29575;&#36719;&#21270;&#35299;&#65292;&#21363;&#38543;&#26426;&#26368;&#20248;&#36335;&#24452;&#65292;&#26469;&#35299;&#20915;&#32463;&#20856;&#26368;&#20248;&#36335;&#24452;&#38382;&#39064;&#65292;&#24182;&#23558;&#24191;&#27867;&#30340;DP&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#20854;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#36335;&#24452;&#36981;&#24490;Gibbs&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;Gumbel&#20998;&#24067;&#30340;&#23646;&#24615;&#26174;&#31034;Gibbs&#20998;&#24067;&#19982;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#29702;&#25152;&#38656;&#30340;&#25152;&#26377;&#35201;&#32032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33719;&#21462;&#20102;&#28508;&#22312;&#26368;&#20248;&#36335;&#24452;&#65292;&#20351;&#29983;&#25104;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;&#27169;&#22411;&#20381;&#36182;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#32467;&#26500;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#65306;&#25991;&#26412;&#36716;&#35821;&#38899;&#21644;&#27468;&#22768;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a unified approach to obtain structured sparse optimal paths in the latent space of a variational autoencoder (VAE) using dynamic programming and Gumbel propagation. We solve the classical optimal path problem by a probability softening solution, called the stochastic optimal path, and transform a wide range of DP problems into directed acyclic graphs in which all possible paths follow a Gibbs distribution. We show the equivalence of the Gibbs distribution to a message-passing algorithm by the properties of the Gumbel distribution and give all the ingredients required for variational Bayesian inference. Our approach obtaining latent optimal paths enables end-to-end training for generative tasks in which models rely on the information of unobserved structural features. We validate the behavior of our approach and showcase its applicability in two real-world applications: text-to-speech and singing voice synthesis.
&lt;/p&gt;</description></item><item><title>&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#28040;&#38500;&#20013;&#24515;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#36890;&#20449;&#20174;&#32780;&#33410;&#30465;&#20102;&#36890;&#20449;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;DFL&#30340;&#20840;&#38754;&#35843;&#26597;&#12289;&#28145;&#20837;&#23637;&#26395;&#21644;&#25193;&#23637;&#21464;&#20307;&#19982;&#20998;&#31867;&#30340;&#20171;&#32461;&#65292;&#37325;&#28857;&#22312;&#20110;DFL&#30340;&#31995;&#32479;&#19982;&#35814;&#32454;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.01603</link><description>&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65306;&#19968;&#20221;&#32508;&#36848;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning: A Survey and Perspective. (arXiv:2306.01603v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01603
&lt;/p&gt;
&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#28040;&#38500;&#20013;&#24515;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#36890;&#20449;&#20174;&#32780;&#33410;&#30465;&#20102;&#36890;&#20449;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;DFL&#30340;&#20840;&#38754;&#35843;&#26597;&#12289;&#28145;&#20837;&#23637;&#26395;&#21644;&#25193;&#23637;&#21464;&#20307;&#19982;&#20998;&#31867;&#30340;&#20171;&#32461;&#65292;&#37325;&#28857;&#22312;&#20110;DFL&#30340;&#31995;&#32479;&#19982;&#35814;&#32454;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#22312;&#20849;&#20139;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#12289;&#20445;&#25252;&#38544;&#31169;&#12289;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#24182;&#20943;&#23569;&#36890;&#20449;&#36127;&#36733;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#19982;&#38598;&#20013;&#24335;FL&#65288;CFL&#65289;&#30456;&#27604;&#65292;DFL&#28040;&#38500;&#20102;&#20013;&#24515;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#12290;DFL&#20351;&#23458;&#25143;&#31471;&#20043;&#38388;&#21487;&#20197;&#30452;&#25509;&#36890;&#20449;&#65292;&#20174;&#32780;&#26174;&#33879;&#33410;&#30465;&#20102;&#36890;&#20449;&#36164;&#28304;&#12290;&#26412;&#25991;&#23545;DFL&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#28145;&#20837;&#30340;&#23637;&#26395;&#12290;&#39318;&#20808;&#65292;&#23545;CFL&#30340;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#21464;&#20307;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#22880;&#23450;&#20102;DFL&#30340;&#32972;&#26223;&#12290;&#28982;&#21518;&#65292;&#20171;&#32461;&#20102;DFL&#30340;&#31995;&#32479;&#21644;&#35814;&#32454;&#30340;&#21069;&#26223;&#65292;&#21253;&#25324;&#36845;&#20195;&#39034;&#24207;&#12289;&#36890;&#20449;&#21327;&#35758;&#12289;&#32593;&#32476;&#25299;&#25169;&#12289;&#33539;&#20363;&#25552;&#35758;&#21644;&#26102;&#38388;&#21464;&#24322;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#22522;&#20110;DFL&#30340;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#25193;&#23637;&#21464;&#20307;&#21644;&#20998;&#31867;&#24182;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#38500;&#20102;&#24635;&#32467;DFL&#30340;&#20248;&#21183;&#19982;&#21155;&#21183;&#65292;&#23545;DFL&#26410;&#26469;&#30740;&#31350;&#21644;&#24212;&#29992;&#21069;&#26223;&#36827;&#34892;&#20102;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has been gaining attention for its ability to share knowledge while maintaining user data, protecting privacy, increasing learning efficiency, and reducing communication overhead. Decentralized FL (DFL) is a decentralized network architecture that eliminates the need for a central server in contrast to centralized FL (CFL). DFL enables direct communication between clients, resulting in significant savings in communication resources. In this paper, a comprehensive survey and profound perspective is provided for DFL. First, a review of the methodology, challenges, and variants of CFL is conducted, laying the background of DFL. Then, a systematic and detailed perspective on DFL is introduced, including iteration order, communication protocols, network topologies, paradigm proposals, and temporal variability. Next, based on the definition of DFL, several extended variants and categorizations are proposed with state-of-the-art technologies. Lastly, in addition to sum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.18569</link><description>&lt;p&gt;
ChatGPT&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#35299;&#20915;LLM&#20013;&#19981;&#20844;&#24179;&#30340;&#38382;&#39064;&#23545;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;LLM&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#39046;&#22495;&#26102;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#20844;&#24179;&#35780;&#20272;&#26041;&#38754;&#65292;&#25968;&#37327;&#20998;&#26512;&#21644;&#28145;&#20837;&#30740;&#31350;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35780;&#20272;ChatGPT&#22312;&#21253;&#25324;&#25945;&#32946;&#12289;&#29359;&#32618;&#23398;&#12289;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#21644;&#20010;&#20154;&#20844;&#24179;&#24615;&#65292;&#24182;&#35266;&#23519;&#20102;&#22312;&#19968;&#31995;&#21015;&#26377;&#20559;&#25110;&#26080;&#20559;&#25552;&#31034;&#19979;ChatGPT&#36755;&#20986;&#30340;&#24046;&#24322;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#20415;&#20110;&#20559;&#35265;&#32531;&#35299;&#65292;&#20419;&#36827;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20855;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01322</link><description>&lt;p&gt;
&#22522;&#20110;Option&#26694;&#26550;&#30340;&#22810;&#27169;&#24335;&#25506;&#32034;&#33258;&#20027;&#38750;&#21333;&#20307;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework. (arXiv:2305.01322v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25506;&#32034;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#8220;&#22914;&#20309;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#32780;&#8220;&#20309;&#26102;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#30740;&#31350;&#19968;&#30452;&#27809;&#26377;&#25104;&#20026;&#37325;&#28857;&#12290;&#20856;&#22411;&#30340;&#25506;&#32034;&#34892;&#20026;&#36890;&#24120;&#23558;&#25506;&#32034;&#34892;&#20026;&#19982;&#26234;&#33021;&#20307;&#30340;&#24320;&#21457;&#21033;&#29992;&#34892;&#20026;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#38750;&#21333;&#20307;&#25506;&#32034;&#34892;&#20026;&#30340;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#27169;&#24335;&#20999;&#25442;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#30340;&#26368;&#32456;&#30446;&#30340;&#26159;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#20027;&#20915;&#23450;&#20309;&#26102;&#25506;&#32034;&#25110;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;Option&#26694;&#26550;&#20013;&#25551;&#36848;&#20102;&#33258;&#20027;&#22810;&#27169;&#24335;&#25506;&#32034;&#30340;&#21021;&#22987;&#30740;&#31350;&#12290;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#30340;&#26356;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most exploration research on reinforcement learning (RL) has paid attention to `the way of exploration', which is `how to explore'. The other exploration research, `when to explore', has not been the main focus of RL exploration research. \textcolor{black}{The issue of `when' of a monolithic exploration in the usual RL exploration behaviour binds an exploratory action to an exploitational action of an agent. Recently, a non-monolithic exploration research has emerged to examine the mode-switching exploration behaviour of humans and animals.} The ultimate purpose of our research is to enable an agent to decide when to explore or exploit autonomously. We describe the initial research of an autonomous multi-mode exploration of non-monolithic behaviour in an options framework. The higher performance of our method is shown against the existing non-monolithic exploration method through comparative experimental results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21516;&#26102;&#20108;&#20803;&#37327;&#21270;&#22238;&#24402;&#65288;SBQR&#65289;&#20013;&#20998;&#20301;&#25968;&#19982;&#39044;&#27979;&#27010;&#29575;&#30340;&#20108;&#20803;&#23545;&#20598;&#24615;&#65292;&#20351;&#20998;&#20301;&#25968;&#34920;&#36798;&#24335;&#22312;&#19981;&#21516;tau&#20540;&#19979;&#30340;&#26500;&#36896;&#19981;&#20877;&#20381;&#36182;&#20110;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#21644;&#35843;&#25972;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2304.12766</link><description>&lt;p&gt;
&#20174;&#25439;&#22833;&#20989;&#25968;&#20013;&#35299;&#32806;&#20998;&#20301;&#25968;&#34920;&#36798;&#24335;
&lt;/p&gt;
&lt;p&gt;
Decoupling Quantile Representations from Loss Functions. (arXiv:2304.12766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21516;&#26102;&#20108;&#20803;&#37327;&#21270;&#22238;&#24402;&#65288;SBQR&#65289;&#20013;&#20998;&#20301;&#25968;&#19982;&#39044;&#27979;&#27010;&#29575;&#30340;&#20108;&#20803;&#23545;&#20598;&#24615;&#65292;&#20351;&#20998;&#20301;&#25968;&#34920;&#36798;&#24335;&#22312;&#19981;&#21516;tau&#20540;&#19979;&#30340;&#26500;&#36896;&#19981;&#20877;&#20381;&#36182;&#20110;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#21644;&#35843;&#25972;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#37327;&#21270;&#22238;&#24402;&#65288;SQR&#65289;&#25216;&#26415;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20854;&#24212;&#29992;&#21463;&#38480;&#20110;&#35201;&#27714;&#20013;&#20301;&#25968;&#20998;&#20301;&#25968;&#65288;&#964; = 0.5&#65289;&#22788;&#30340;&#35299;&#20915;&#26041;&#26696;&#24517;&#39035;&#26368;&#23567;&#21270;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#21516;&#26102;&#20108;&#20803;&#37327;&#21270;&#22238;&#24402;&#65288;SBQR&#65289;&#20013;&#20998;&#20301;&#25968;&#19982;&#39044;&#27979;&#27010;&#29575;&#30340;&#20108;&#20803;&#23545;&#20598;&#24615;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#25439;&#22833;&#20989;&#25968;&#20013;&#35299;&#32806;&#20998;&#20301;&#25968;&#34920;&#36798;&#24335;&#30340;&#26500;&#36896;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20013;&#20301;&#20998;&#20301;&#25968;&#22788;&#20998;&#37197;&#20219;&#24847;&#20998;&#31867;&#22120;f(x)&#65292;&#24182;&#29983;&#25104;&#19981;&#21516;&#964;&#20540;&#30340;&#23436;&#25972;SBQR&#20998;&#20301;&#25968;&#34920;&#31034;&#30340;&#20840;&#35889;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;&#31243;&#24207;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#65288;i&#65289;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#65292;&#20854;&#20013;&#25105;&#20204;&#26174;&#31034;&#20998;&#20301;&#25968;&#34920;&#31034;&#20248;&#20110;&#26631;&#20934;&#27010;&#29575;&#36755;&#20986;&#65307;&#65288;ii&#65289;&#35843;&#25972;&#27169;&#22411;&#65292;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#20301;&#25968;&#34920;&#31034;&#23545;&#22833;&#30495;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The simultaneous quantile regression (SQR) technique has been used to estimate uncertainties for deep learning models, but its application is limited by the requirement that the solution at the median quantile ({\tau} = 0.5) must minimize the mean absolute error (MAE). In this article, we address this limitation by demonstrating a duality between quantiles and estimated probabilities in the case of simultaneous binary quantile regression (SBQR). This allows us to decouple the construction of quantile representations from the loss function, enabling us to assign an arbitrary classifier f(x) at the median quantile and generate the full spectrum of SBQR quantile representations at different {\tau} values. We validate our approach through two applications: (i) detecting out-of-distribution samples, where we show that quantile representations outperform standard probability outputs, and (ii) calibrating models, where we demonstrate the robustness of quantile representations to distortions. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;PopulAtion Parameter Averaging (PAPA)&#65292;&#33021;&#21516;&#26102;&#25317;&#26377;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03094</link><description>&lt;p&gt;
PopulAtion Parameter Averaging (PAPA)&#65288;&#20154;&#21475;&#21442;&#25968;&#24179;&#22343;&#65289;
&lt;/p&gt;
&lt;p&gt;
PopulAtion Parameter Averaging (PAPA). (arXiv:2304.03094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03094
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;PopulAtion Parameter Averaging (PAPA)&#65292;&#33021;&#21516;&#26102;&#25317;&#26377;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#23558;&#22810;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#32452;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#25104;&#26412;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#36827;&#34892;&#24179;&#22343;&#26469;&#23558;&#23427;&#20204;&#21512;&#24182;&#25104;&#19968;&#20010;&#65288;&#27169;&#22411;&#27748;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#27604;&#38598;&#25104;&#34920;&#29616;&#26356;&#24046;&#12290;&#24403;&#26435;&#37325;&#36275;&#22815;&#30456;&#20284;&#65288;&#22312;&#26435;&#37325;&#25110;&#29305;&#24449;&#31354;&#38388;&#20013;&#65289;&#21487;&#20197;&#24456;&#22909;&#22320;&#24179;&#22343;&#65292;&#20294;&#36275;&#22815;&#19981;&#21516;&#20197;&#20174;&#32452;&#21512;&#20013;&#21463;&#30410;&#26102;&#65292;&#26435;&#37325;&#24179;&#22343;&#25165;&#26159;&#26377;&#30410;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PopulAtion Parameter Averaging (PAPA)&#65292;&#19968;&#31181;&#23558;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;PAPA&#21033;&#29992;&#19981;&#21516;&#27169;&#22411;&#65288;&#22312;&#19981;&#21516;&#25968;&#25454;&#39034;&#24207;&#65292;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#19978;&#35757;&#32451;&#65289;&#30340;&#20154;&#21475;&#65292;&#32780;&#20598;&#23572;&#65288;&#19981;&#35201;&#22826;&#39057;&#32321;&#65292;&#20063;&#19981;&#35201;&#22826;&#31232;&#30095;&#65289;&#29992;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#20195;&#26367;&#20154;&#21475;&#26435;&#37325;&#30340;&#24179;&#22343;&#20540;&#12290;PAPA&#20943;&#23569;&#20102;&#24179;&#22343;&#20540;&#21644;&#38598;&#25104;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods combine the predictions of multiple models to improve performance, but they require significantly higher computation costs at inference time. To avoid these costs, multiple neural networks can be combined into one by averaging their weights (model soups). However, this usually performs significantly worse than ensembling. Weight averaging is only beneficial when weights are similar enough (in weight or feature space) to average well but different enough to benefit from combining them. Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a method that combines the generality of ensembling with the efficiency of weight averaging. PAPA leverages a population of diverse models (trained on different data orders, augmentations, and regularizations) while occasionally (not too often, not too rarely) replacing the weights of the networks with the population average of the weights. PAPA reduces the performance gap between averaging and ensembling, increasing th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#26041;&#27861;InnerCore&#29992;&#20110;&#20998;&#26512;&#21306;&#22359;&#38142;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#32593;&#32476;&#23545;&#20851;&#38190;&#21442;&#19982;&#32773;&#30340;&#35782;&#21035;&#24182;&#25552;&#20379;&#24773;&#24863;&#25351;&#26631;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#36235;&#21183;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.14241</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#24515;&#30340;&#21306;&#22359;&#38142;&#32593;&#32476;&#36235;&#21183;&#30417;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Core-based Trend Detection in Blockchain Networks. (arXiv:2303.14241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#26041;&#27861;InnerCore&#29992;&#20110;&#20998;&#26512;&#21306;&#22359;&#38142;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#32593;&#32476;&#23545;&#20851;&#38190;&#21442;&#19982;&#32773;&#30340;&#35782;&#21035;&#24182;&#25552;&#20379;&#24773;&#24863;&#25351;&#26631;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#36235;&#21183;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#22312;&#36152;&#26131;&#37329;&#34701;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#27599;&#22825;&#20132;&#26131;&#30340;&#36164;&#20135;&#20215;&#20540;&#36798;&#25968;&#21313;&#20159;&#32654;&#20803;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#65292;&#23545;&#36825;&#20123;&#32593;&#32476;&#36827;&#34892;&#20998;&#26512;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#26041;&#27861;- InnerCore&#65292;&#29992;&#20110;&#35782;&#21035;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#21442;&#19982;&#32773;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#28145;&#24230;&#30340;&#20869;&#26680;&#20998;&#35299;&#21644;&#20013;&#24515;&#27169;&#24335;&#21457;&#29616;&#25552;&#20379;&#32593;&#32476;&#24773;&#24863;&#25351;&#26631;&#12290; InnerCore&#26159;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#12289;&#38750;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20998;&#26512;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;LunaTerra&#30340;&#26368;&#36817;&#23849;&#28291;&#21644;&#20197;&#22826;&#22346;&#30340;PoS&#20999;&#25442;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#19968;&#23478;&#39046;&#20808;&#30340;&#21306;&#22359;&#38142;&#20998;&#26512;&#20844;&#21496;&#25910;&#38598;&#30340;&#22806;&#37096;&#22522;&#26412;&#20107;&#23454;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;InnerCore&#21487;&#20197;&#19982;&#21512;&#26684;&#30340;&#20998;&#26512;&#20934;&#30830;&#21305;&#37197;&#65292;&#19981;&#38656;&#35201;&#20154;&#20026;&#24178;&#39044;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#20280;&#32553;&#30340;&#21306;&#22359;&#38142;&#20998;&#26512;&#21644;&#36235;&#21183;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchains are now significantly easing trade finance, with billions of dollars worth of assets being transacted daily. However, analyzing these networks remains challenging due to the large size and complexity of the data. We introduce a scalable approach called "InnerCore" for identifying key actors in blockchain-based networks and providing a sentiment indicator for the networks using data depth-based core decomposition and centered-motif discovery. InnerCore is a computationally efficient, unsupervised approach suitable for analyzing large temporal graphs. We demonstrate its effectiveness through case studies on the recent collapse of LunaTerra and the Proof-of-Stake (PoS) switch of Ethereum, using external ground truth collected by a leading blockchain analysis company. Our experiments show that InnerCore can match the qualified analysis accurately without human involvement, automating blockchain analysis and its trend detection in a scalable manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PU&#26041;&#27861;PULSNAR&#65292;&#21363;&#20351;&#22312;SCAR&#19981;&#25104;&#31435;&#26102;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#945;&#20272;&#35745;&#21644;&#26657;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08269</link><description>&lt;p&gt;
PULSNAR -- &#22312;SCAR&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#36873;&#25321;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#65306;&#20998;&#31867;&#27604;&#20363;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PULSNAR -- Positive unlabeled learning selected not at random: class proportion estimation when the SCAR assumption does not hold. (arXiv:2303.08269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PU&#26041;&#27861;PULSNAR&#65292;&#21363;&#20351;&#22312;SCAR&#19981;&#25104;&#31435;&#26102;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#945;&#20272;&#35745;&#21644;&#26657;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#26080;&#26631;&#35760;&#65288;PU&#65289;&#23398;&#20064;&#26159;&#21322;&#30417;&#30563;&#20108;&#20803;&#20998;&#31867;&#30340;&#19968;&#31181;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21306;&#20998;&#19968;&#32452;&#27491;&#23454;&#20363;&#65288;&#24102;&#26377;&#26631;&#31614;&#65289;&#21644;&#19968;&#32452;&#26082;&#26377;&#27491;&#31867;&#21448;&#26377;&#36127;&#31867;&#23454;&#20363;&#65288;&#27809;&#26377;&#26631;&#31614;&#65289;&#12290;&#22312;&#30830;&#35748;&#36127;&#20363;&#19981;&#21487;&#29992;&#25110;&#38590;&#20197;&#33719;&#21462;&#65292;&#24182;&#19988;&#22312;&#26410;&#26631;&#35760;&#30340;&#23454;&#20363;&#20013;&#21457;&#29616;&#27491;&#20363;&#20855;&#26377;&#20215;&#20540;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#22312;&#26410;&#27979;&#35797;&#30340;&#21270;&#21512;&#29289;&#20013;&#25214;&#21040;&#21487;&#34892;&#33647;&#29289;&#65289;&#65292;PU&#23398;&#20064;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;PU&#23398;&#20064;&#31639;&#27861;&#35748;&#20026;&#36873;&#25321;&#27491;&#23454;&#20363;&#29420;&#31435;&#20110;&#23427;&#20204;&#30340;&#29305;&#24449;&#65292;&#21363;&#36827;&#34892;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#65288;SCAR&#65289;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#27491;&#23454;&#20363;&#19981;&#26159;SCAR&#65288;&#20363;&#22914;&#65292;&#20005;&#37325;&#24773;&#20917;&#26356;&#23481;&#26131;&#34987;&#35786;&#26029;&#20986;&#65289;&#65292;&#23548;&#33268;&#22312;&#26080;&#26631;&#35760;&#31034;&#20363;&#20013;&#20272;&#35745;&#38451;&#24615;&#27604;&#20363;&#945;&#21644;&#27169;&#22411;&#26657;&#20934;&#24615;&#33021;&#24046;&#65292;&#36827;&#32780;&#23548;&#33268;&#36873;&#25321;&#27491;&#20363;&#30340;&#19981;&#30830;&#23450;&#20915;&#31574;&#38408;&#20540;&#12290;PU&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#26410;&#26631;&#35760;&#23454;&#20363;&#26159;&#38451;&#24615;&#30340;&#27010;&#29575;&#26469;&#20272;&#35745;&#945;&#24182;&#25552;&#20379;&#26657;&#20934;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;SCAR&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#65292;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PU&#26041;&#27861;PULSNAR&#65292;&#21363;&#20351;&#22312;SCAR&#19981;&#25104;&#31435;&#26102;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#945;&#20272;&#35745;&#21644;&#26657;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23545;&#26410;&#26631;&#35760;&#31034;&#20363;&#20998;&#24067;&#30340;&#26032;&#20551;&#35774;&#65292;&#31216;&#20026;&#38451;&#24615;&#22343;&#21248;&#26465;&#20214;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;PU&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positive and Unlabeled (PU) learning is a type of semi-supervised binary classification where the machine learning algorithm differentiates between a set of positive instances (labeled) and a set of both positive and negative instances (unlabeled). PU learning has broad applications in settings where confirmed negatives are unavailable or difficult to obtain, and there is value in discovering positives among the unlabeled (e.g., viable drugs among untested compounds). Most PU learning algorithms make the selected completely at random (SCAR) assumption, namely that positives are selected independently of their features. However, in many real-world applications, such as healthcare, positives are not SCAR (e.g., severe cases are more likely to be diagnosed), leading to a poor estimate of the proportion, $\alpha$, of positives among unlabeled examples and poor model calibration, resulting in an uncertain decision threshold for selecting positives. PU learning algorithms can estimate $\alph
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12461</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#20013;&#30340;&#27602;&#21270;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#21487;&#33021;&#23548;&#33268;&#26263;&#34255;&#21518;&#38376;&#30340;&#27169;&#22411;&#12290;&#20851;&#20110;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#35302;&#21457;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20999;&#25442;&#33267;&#26377;&#27602;&#35821;&#35328;&#65289;&#30340;&#25551;&#36848;&#23578;&#26410;&#25214;&#21040;&#12290;&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;Transformer&#30340;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#30830;&#23450;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#19982;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#32467;&#21512;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#29992;&#24037;&#31243;&#21270;&#26367;&#20195;&#29289;&#38477;&#20302;MLP&#27169;&#22359;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20027;&#35201;&#25104;&#20998;&#30340;&#20302;&#31209;&#30697;&#38453;&#30340;PCP&#28040;&#34701;&#25216;&#26415;&#65292;&#29992;&#20854;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#26263;&#34255;&#21518;&#38376;&#30340;&#29609;&#20855;&#27169;&#22411;&#12289;&#26263;&#34255;&#21518;&#38376;&#30340;&#22823;&#22411;&#27169;&#22411;&#21644;&#38750;&#26263;&#34255;&#21518;&#38376;&#30340;&#24320;&#28304;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#25913;&#21892;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#19982;&#27969;&#34892;&#30340;TreeSHAP&#31639;&#27861;&#30456;&#27604;&#65292;&#36793;&#38469;Shapley&#20540;&#22312;&#30456;&#21516;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#26641;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#35745;&#31639;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#12290;</title><link>http://arxiv.org/abs/2302.08434</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On marginal feature attributions of tree-based models. (arXiv:2302.08434v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#19982;&#27969;&#34892;&#30340;TreeSHAP&#31639;&#27861;&#30456;&#27604;&#65292;&#36793;&#38469;Shapley&#20540;&#22312;&#30456;&#21516;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#26641;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#35745;&#31639;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24378;&#22823;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#29305;&#28857;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#26641;&#38598;&#25104;&#31561;&#22522;&#20110;&#26641;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#36793;&#38469;&#26399;&#26395;&#30340;&#23616;&#37096;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#20363;&#22914;&#36793;&#38469;&#65288;&#24178;&#39044;&#65289;Shapley&#12289;Owen&#25110;Banzhaf&#20540;&#12290;&#36825;&#20123;&#26041;&#27861;&#23545;&#27169;&#22411;&#30495;&#23454;&#19988;&#23454;&#29616;&#19981;&#21464;&#65292;&#21363;&#20165;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#36755;&#20837;&#36755;&#20986;&#20989;&#25968;&#12290;&#36890;&#36807;&#25552;&#20379;&#20004;&#20010;&#65288;&#20855;&#26377;&#30456;&#20284;&#32479;&#35745;&#24615;&#36136;&#30340;&#65289;&#20915;&#31574;&#26641;&#26469;&#23545;&#27604;&#36825;&#19968;&#28857;&#65292;&#36825;&#20004;&#20010;&#20915;&#31574;&#26641;&#35745;&#31639;&#23436;&#20840;&#30456;&#21516;&#30340;&#20989;&#25968;&#65292;&#20294;&#8220;&#36335;&#24452;&#30456;&#20851;&#8221;&#30340;TreeSHAP&#26041;&#27861;&#32473;&#20986;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#25490;&#24207;&#65292;&#32780;&#36793;&#38469;Shapley&#20540;&#37325;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#26469;&#24110;&#21161;&#35745;&#31639;&#23427;&#20204;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#65292;&#20197;&#24471;&#21040;&#32447;&#24615;&#21338;&#24328;&#20540;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#35266;&#23519;&#26159;&#65292;&#36825;&#20123;&#20989;&#25968;&#22312;&#26576;&#20010;&#24120;&#25968;&#21306;&#38388;&#20869;&#26159;&#31616;&#21333;&#30340;&#65288;&#20998;&#27573;&#24120;&#25968;&#65289;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their power and ease of use, tree-based machine learning models, such as random forests and gradient-boosted tree ensembles, have become very popular. To interpret them, local feature attributions based on marginal expectations, e.g. marginal (interventional) Shapley, Owen or Banzhaf values, may be employed. Such methods are true to the model and implementation invariant, i.e. dependent only on the input-output function of the model. We contrast this with the popular TreeSHAP algorithm by presenting two (statistically similar) decision trees that compute the exact same function for which the "path-dependent" TreeSHAP yields different rankings of features, whereas the marginal Shapley values coincide. Furthermore, we discuss how the internal structure of tree-based models may be leveraged to help with computing their marginal feature attributions according to a linear game value. One important observation is that these are simple (piecewise-constant) functions with respect to a c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.00808</link><description>&lt;p&gt;
&#24179;&#22343;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Average-Constrained Policy Optimization. (arXiv:2302.00808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#21046;&#26465;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36890;&#24120;&#65292;&#24179;&#22343;&#26631;&#20934;&#27604;&#25240;&#25187;&#26631;&#20934;&#26356;&#21512;&#36866;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#24179;&#22343;&#38480;&#21046; CMDP &#30340;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#25240;&#25187;&#38480;&#21046; RL &#38382;&#39064;&#35774;&#35745;&#30340;&#31639;&#27861;&#36890;&#24120;&#22312;&#24179;&#22343; CMDP &#29615;&#22659;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#24179;&#22343;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#65288;ACPO&#65289;&#31639;&#27861;&#30340;&#28789;&#24863;&#26469;&#33258;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#30340;&#33879;&#21517; PPO &#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#22522;&#26412;&#30340;&#24179;&#22343; MDP &#25935;&#24863;&#24615;&#29702;&#35770;&#65292;&#28982;&#21518;&#22312;&#31639;&#27861;&#35774;&#35745;&#20013;&#20351;&#29992;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; MuJoCo &#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#24037;&#20316;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#19982;&#20854;&#20182;&#24120;&#35268;&#31639;&#27861;&#30456;&#27604;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) with constraints is becoming an increasingly important problem for various applications. Often, the average criterion is more suitable than the discounted criterion. Yet, RL for average criterion-constrained MDPs remains a challenging problem. Algorithms designed for discounted constrained RL problems often do not perform well for the average CMDP setting. In this paper, we introduce a new policy optimization with function approximation algorithm for constrained MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by the famed PPO-type algorithms based on trust region methods. We develop basic sensitivity theory for average MDPs, and then use the corresponding bounds in the design of the algorithm. We provide theoretical guarantees on its performance, and through extensive experimental work in various challenging MuJoCo environments, show the superior performance of the algorithm when compared to other sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2212.03932</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#34892;&#20026;&#31574;&#30053;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#32447;&#35780;&#20272;&#20013;&#65292;&#38656;&#35201;&#35780;&#20272;&#30446;&#26631;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#38656;&#35201;&#20351;&#29992;&#30001;&#34892;&#20026;&#31574;&#30053;&#37319;&#38598;&#30340;&#26679;&#26412;&#25968;&#25454;&#12290;&#20256;&#32479;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#30001;&#20110;&#35745;&#31639;&#21160;&#20316;&#27010;&#29575;&#27604;&#20540;&#30340;&#20056;&#31215;&#32780;&#23548;&#33268;&#26041;&#24046;&#22686;&#21152;&#65292;&#20174;&#32780;&#22312;&#28041;&#21450;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#20986;&#29616;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In off-policy reinforcement learning, a behaviour policy performs exploratory interactions with the environment to obtain state-action-reward samples which are then used to learn a target policy that optimises the expected return. This leads to a problem of off-policy evaluation, where one needs to evaluate the target policy from samples collected by the often unrelated behaviour policy. Importance sampling is a traditional statistical technique that is often applied to off-policy evaluation. While importance sampling estimators are unbiased, their variance increases exponentially with the horizon of the decision process due to computing the importance weight as a product of action probability ratios, yielding estimates with low accuracy for domains involving long-term planning. This paper proposes state-based importance sampling (SIS), which drops the action probability ratios of sub-trajectories with "negligible states" -- roughly speaking, those for which the chosen actions have no 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.06348</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#33041;&#20449;&#21495;&#25581;&#31034;&#20154;&#31867;&#35821;&#35328;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#20449;&#21495;&#65288;&#22914;&#33041;&#30005;&#22270;&#65289;&#21644;&#20154;&#31867;&#35821;&#35328;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20108;&#32773;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33041;&#30005;&#22270;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20381;&#36182;&#24615;&#12290;&#22312;&#34920;&#31034;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#65288;Multimodal Transformer Alignment Model&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35266;&#23519;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#21327;&#35843;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#20851;&#31995;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#21644;Wasserstein&#36317;&#31163;&#65292;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#36716;&#25442;&#29305;&#24449;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#22312;ZuCo&#21644;K-EmoCon&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#20351;K-EmoCon&#25968;&#25454;&#38598;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;1.7&#65285;&#65292;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;9.3&#65285;&#65292;&#22312;&#20851;&#31995;&#26816;&#27979;&#26041;&#38754;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;7.4&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22269;&#38469;&#19978;&#26368;&#22823;&#30340;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal \textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide inter
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#20540;&#20989;&#25968;&#20013;&#23545;&#25239;&#40065;&#26834;PAC&#23398;&#20064;&#24615;&#65292;&#21457;&#29616;&#26377;&#38480;&#32982;&#25240;&#23556;&#32500;&#30340;&#31867;&#26082;&#21487;&#20197;&#22312;&#23454;&#29616;&#21644;&#19981;&#21487;&#30693;&#35774;&#32622;&#20013;&#34987;&#23398;&#20064;&#65292;&#20984;&#20989;&#25968;&#31867;&#21487;&#20197;&#27491;&#30830;&#23398;&#20064;&#65292;&#32780;&#19968;&#20123;&#38750;&#20984;&#20989;&#25968;&#31867;&#38656;&#35201;&#19981;&#27491;&#24403;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.12977</link><description>&lt;p&gt;
&#22312;&#23454;&#20540;&#20989;&#25968;&#20013;&#23545;&#25239;&#40065;&#26834;PAC&#23398;&#20064;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarially Robust PAC Learnability of Real-Valued Functions. (arXiv:2206.12977v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#20540;&#20989;&#25968;&#20013;&#23545;&#25239;&#40065;&#26834;PAC&#23398;&#20064;&#24615;&#65292;&#21457;&#29616;&#26377;&#38480;&#32982;&#25240;&#23556;&#32500;&#30340;&#31867;&#26082;&#21487;&#20197;&#22312;&#23454;&#29616;&#21644;&#19981;&#21487;&#30693;&#35774;&#32622;&#20013;&#34987;&#23398;&#20064;&#65292;&#20984;&#20989;&#25968;&#31867;&#21487;&#20197;&#27491;&#30830;&#23398;&#20064;&#65292;&#32780;&#19968;&#20123;&#38750;&#20984;&#20989;&#25968;&#31867;&#38656;&#35201;&#19981;&#27491;&#24403;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;$\ell_p$&#25439;&#22833;&#21644;&#20219;&#24847;&#25200;&#21160;&#38598;&#30340;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#23545;&#27979;&#35797;&#26102;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21738;&#20123;&#20989;&#25968;&#31867;&#26159;PAC&#21487;&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#26377;&#38480;&#32982;&#25240;&#23556;&#32500;&#30340;&#31867;&#26082;&#21487;&#20197;&#22312;&#23454;&#29616;&#21644;&#19981;&#21487;&#30693;&#35774;&#32622;&#20013;&#34987;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20984;&#20989;&#25968;&#31867;&#65292;&#23427;&#20204;&#29978;&#33267;&#21487;&#20197;&#27491;&#30830;&#23398;&#20064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19968;&#20123;&#38750;&#20984;&#20989;&#25968;&#31867;&#26174;&#28982;&#38656;&#35201;&#19981;&#27491;&#24403;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#22522;&#20110;&#26500;&#24314;&#19968;&#20010;&#30001;&#32982;&#25240;&#23556;&#32500;&#20915;&#23450;&#22823;&#23567;&#30340;&#20855;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#23454;&#20540;&#20989;&#25968;&#30340;&#19981;&#21487;&#30693;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study robustness to test-time adversarial attacks in the regression setting with $\ell_p$ losses and arbitrary perturbation sets. We address the question of which function classes are PAC learnable in this setting. We show that classes of finite fat-shattering dimension are learnable in both realizable and agnostic settings. Moreover, for convex function classes, they are even properly learnable. In contrast, some non-convex function classes provably require improper learning algorithms. Our main technique is based on a construction of an adversarially robust sample compression scheme of a size determined by the fat-shattering dimension. Along the way, we introduce a novel agnostic sample compression scheme for real-valued functions, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24402;&#30340;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#27979;&#26469;&#20272;&#35745;&#36229;&#26631;&#27010;&#29575;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.09821</link><description>&lt;p&gt;
&#22522;&#20110;&#22238;&#24402;&#30340;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#29992;&#20110;&#26174;&#33879;&#27874;&#39640;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exceedance Probability Forecasting via Regression for Significant Wave Height Prediction. (arXiv:2206.09821v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24402;&#30340;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#27979;&#26469;&#20272;&#35745;&#36229;&#26631;&#27010;&#29575;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#27874;&#39640;&#39044;&#27979;&#26159;&#28023;&#27915;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#23545;&#20110;&#20272;&#35745;&#27874;&#33021;&#20135;&#29983;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#27492;&#22806;&#65292;&#21450;&#26102;&#39044;&#27979;&#22823;&#28010;&#30340;&#21040;&#26469;&#23545;&#20110;&#30830;&#20445;&#33322;&#28023;&#20316;&#19994;&#30340;&#23433;&#20840;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#30340;&#26497;&#31471;&#20540;&#20316;&#20026;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#20272;&#35745;&#26174;&#33879;&#27874;&#39640;&#23558;&#36229;&#36807;&#39044;&#23450;&#20041;&#38408;&#20540;&#30340;&#27010;&#29575;&#12290;&#36890;&#24120;&#20351;&#29992;&#27010;&#29575;&#20108;&#20998;&#31867;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26410;&#26469;&#35266;&#27979;&#30340;&#39044;&#27979;&#26469;&#26681;&#25454;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#20272;&#35745;&#36229;&#26631;&#27010;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#21152;&#25343;&#22823;&#21704;&#21033;&#27861;&#20811;&#26031;&#28023;&#23736;&#30340;&#28014;&#26631;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant wave height forecasting is a key problem in ocean data analytics. Predicting the significant wave height is crucial for estimating the energy production from waves. Moreover, the timely prediction of large waves is important to ensure the safety of maritime operations, e.g. passage of vessels. We frame the task of predicting extreme values of significant wave height as an exceedance probability forecasting problem. Accordingly, we aim at estimating the probability that the significant wave height will exceed a predefined threshold. This task is usually solved using a probabilistic binary classification model. Instead, we propose a novel approach based on a forecasting model. The method leverages the forecasts for the upcoming observations to estimate the exceedance probability according to the cumulative distribution function. We carried out experiments using data from a buoy placed in the coast of Halifax, Canada. The results suggest that the proposed methodology is better
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20837;&#20405;&#25915;&#20987;&#24182;&#20026;&#21306;&#22359;&#38142;&#32593;&#32476;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#26694;&#26550;&#12290;&#36890;&#36807;&#23454;&#39564;&#23460;&#21512;&#25104;&#30340;&#32593;&#32476;&#25915;&#20987;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#65292;&#20801;&#35768;&#21306;&#22359;&#38142;&#33410;&#28857;&#20849;&#20139;&#23398;&#20064;&#30693;&#35782;&#20197;&#26816;&#27979;&#25915;&#20987;&#65292;&#24182;&#22686;&#24378;&#25972;&#20010;&#21306;&#22359;&#38142;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.11076</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#32593;&#32476;&#20013;&#30340;&#21327;&#20316;&#23398;&#20064;&#29992;&#20110;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning for Cyberattack Detection in Blockchain Networks. (arXiv:2203.11076v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20837;&#20405;&#25915;&#20987;&#24182;&#20026;&#21306;&#22359;&#38142;&#32593;&#32476;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#26694;&#26550;&#12290;&#36890;&#36807;&#23454;&#39564;&#23460;&#21512;&#25104;&#30340;&#32593;&#32476;&#25915;&#20987;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#65292;&#20801;&#35768;&#21306;&#22359;&#38142;&#33410;&#28857;&#20849;&#20139;&#23398;&#20064;&#30693;&#35782;&#20197;&#26816;&#27979;&#25915;&#20987;&#65292;&#24182;&#22686;&#24378;&#25972;&#20010;&#21306;&#22359;&#38142;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20837;&#20405;&#25915;&#20987;&#65292;&#24182;&#20026;&#21306;&#22359;&#38142;&#32593;&#32476;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#22312;&#23454;&#39564;&#23460;&#20013;&#35774;&#35745;&#21644;&#23454;&#29616;&#19968;&#20010;&#21306;&#22359;&#38142;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#23558;&#29992;&#20110;&#29983;&#25104;&#30495;&#23454;&#30340;&#27969;&#37327;&#25968;&#25454;&#65288;&#21253;&#25324;&#27491;&#24120;&#25968;&#25454;&#21644;&#25915;&#20987;&#25968;&#25454;&#65289;&#65292;&#20197;&#29992;&#20110;&#25105;&#20204;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#23454;&#26102;&#23454;&#39564;&#65292;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#23454;&#39564;&#23460;&#20013;&#21512;&#25104;&#30340;&#29992;&#20110;&#21306;&#22359;&#38142;&#32593;&#32476;&#30340;&#32593;&#32476;&#25915;&#20987;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21306;&#22359;&#38142;&#32593;&#32476;&#20013;&#23454;&#29616;&#39640;&#25928;&#37096;&#32626;&#20197;&#26816;&#27979;&#25915;&#20987;&#12290;&#25152;&#25552;&#20986;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#21306;&#22359;&#38142;&#33410;&#28857;&#33021;&#22815;&#20027;&#21160;&#25910;&#38598;&#25968;&#25454;&#65292;&#20849;&#20139;&#20174;&#20854;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#24182;&#19982;&#32593;&#32476;&#20013;&#30340;&#20854;&#20182;&#21306;&#22359;&#38142;&#33410;&#28857;&#20132;&#25442;&#30693;&#35782;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#19981;&#20165;&#33021;&#22815;&#21033;&#29992;&#20854;&#20182;&#33410;&#28857;&#30340;&#30693;&#35782;&#36827;&#34892;&#25915;&#20987;&#26816;&#27979;&#65292;&#36824;&#33021;&#22815;&#22686;&#24378;&#25972;&#20010;&#21306;&#22359;&#38142;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article aims to study intrusion attacks and then develop a novel cyberattack detection framework for blockchain networks. Specifically, we first design and implement a blockchain network in our laboratory. This blockchain network will serve two purposes, i.e., to generate the real traffic data (including both normal data and attack data) for our learning models and implement real-time experiments to evaluate the performance of our proposed intrusion detection framework. To the best of our knowledge, this is the first dataset that is synthesized in a laboratory for cyberattacks in a blockchain network. We then propose a novel collaborative learning model that allows efficient deployment in the blockchain network to detect attacks. The main idea of the proposed learning model is to enable blockchain nodes to actively collect data, share the knowledge learned from its data, and then exchange the knowledge with other blockchain nodes in the network. In this way, we can not only levera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#35823;&#24046;&#27169;&#22411;&#19979;&#37051;&#25509;&#30697;&#38453;&#30340;&#21463;&#38480;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#30740;&#31350;&#20102;GCN&#22312;&#36825;&#31181;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#20934;&#30830;&#24615;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.07831</link><description>&lt;p&gt;
&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#22312;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network Sensitivity Under Probabilistic Error Model. (arXiv:2203.07831v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#35823;&#24046;&#27169;&#22411;&#19979;&#37051;&#25509;&#30697;&#38453;&#30340;&#21463;&#38480;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#30740;&#31350;&#20102;GCN&#22312;&#36825;&#31181;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#20934;&#30830;&#24615;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21487;&#20197;&#36890;&#36807;&#22270;&#21367;&#31215;&#25104;&#21151;&#23398;&#20064;&#22270;&#20449;&#21495;&#34920;&#31034;&#12290;&#22270;&#21367;&#31215;&#20381;&#36182;&#20110;&#22270;&#28388;&#27874;&#22120;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#25454;&#30340;&#25299;&#25169;&#20381;&#36182;&#20851;&#31995;&#24182;&#20256;&#25773;&#25968;&#25454;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#20256;&#25773;&#30697;&#38453;&#65288;&#20363;&#22914;&#37051;&#25509;&#30697;&#38453;&#65289;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#21487;&#33021;&#23545;&#22270;&#28388;&#27874;&#22120;&#21644;GCNs&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#27010;&#29575;&#22270;&#35823;&#24046;&#27169;&#22411;&#23545;GCN&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#37051;&#25509;&#30697;&#38453;&#21463;&#21040;&#22270;&#22823;&#23567;&#21644;&#35823;&#24046;&#27010;&#29575;&#20989;&#25968;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#24102;&#26377;&#33258;&#24490;&#29615;&#30340;&#24402;&#19968;&#21270;&#37051;&#25509;&#30697;&#38453;&#30340;&#19978;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#23454;&#39564;&#26469;&#35828;&#26126;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#30740;&#31350;&#31616;&#21333;GCN&#22312;&#36825;&#31181;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#20934;&#30830;&#24615;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) can successfully learn the graph signal representation by graph convolution. The graph convolution depends on the graph filter, which contains the topological dependency of data and propagates data features. However, the estimation errors in the propagation matrix (e.g., the adjacency matrix) can have a significant impact on graph filters and GCNs. In this paper, we study the effect of a probabilistic graph error model on the performance of the GCNs. We prove that the adjacency matrix under the error model is bounded by a function of graph size and error probability. We further analytically specify the upper bound of a normalized adjacency matrix with self-loop added. Finally, we illustrate the error bounds by running experiments on a synthetic dataset and study the sensitivity of a simple GCN under this probabilistic error model on accuracy.
&lt;/p&gt;</description></item></channel></rss>