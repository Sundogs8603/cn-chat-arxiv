<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;&#34892;&#20026;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#20998;&#26512;&#20102;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.00690</link><description>&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#22312;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#20013;&#30340;&#24046;&#24322;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What User Behaviors Make the Differences During the Process of Visual Analytics?. (arXiv:2311.00690v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;&#34892;&#20026;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#20998;&#26512;&#20102;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#30340;&#29702;&#35299;&#21487;&#20197;&#20174;&#22810;&#20010;&#26041;&#38754;&#21463;&#30410;&#20110;&#21487;&#35270;&#21270;&#30740;&#31350;&#20154;&#21592;&#65292;&#21253;&#25324;&#25913;&#36827;&#21487;&#35270;&#21270;&#35774;&#35745;&#21644;&#24320;&#21457;&#20808;&#36827;&#30340;&#20132;&#20114;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24863;&#30693;&#30340;&#22797;&#26434;&#24615;&#21644;&#25105;&#20204;&#23545;&#30456;&#20851;&#29992;&#25143;&#34892;&#20026;&#30340;&#32570;&#20047;&#20102;&#35299;&#65292;&#29992;&#25143;&#34892;&#20026;&#30340;&#26085;&#24535;&#25991;&#20214;&#20173;&#28982;&#38590;&#20197;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#29992;&#25143;&#34892;&#20026;&#30340;&#20840;&#38754;&#25968;&#25454;&#37319;&#38598;&#30340;&#30740;&#31350;&#65292;&#24182;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#32463;&#20856;&#30340;&#21487;&#35270;&#21270;&#24212;&#29992;&#65292;Covid-19&#25968;&#25454;&#20998;&#26512;&#65292;&#28085;&#30422;&#22320;&#29702;&#31354;&#38388;&#12289;&#26102;&#38388;&#24207;&#21015;&#21644;&#22810;&#23646;&#24615;&#30340;&#24120;&#35265;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#25910;&#38598;&#20102;&#20851;&#20110;&#22810;&#20010;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#29992;&#25143;&#34892;&#20026;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#21487;&#27604;&#36739;&#30340;&#31995;&#32479;&#65292;&#26700;&#38754;&#21644;&#27785;&#28024;&#24335;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#20004;&#20010;&#23610;&#24230;&#19978;&#20351;&#29992;&#19977;&#31181;&#26102;&#38388;&#24207;&#21015;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#34892;&#20026;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of visual analytics process can benefit visualization researchers from multiple aspects, including improving visual designs and developing advanced interaction functions. However, the log files of user behaviors are still hard to analyze due to the complexity of sensemaking and our lack of knowledge on the related user behaviors. This work presents a study on a comprehensive data collection of user behaviors, and our analysis approach with time-series classification methods. We have chosen a classical visualization application, Covid-19 data analysis, with common analysis tasks covering geo-spatial, time-series and multi-attributes. Our user study collects user behaviors on a diverse set of visualization tasks with two comparable systems, desktop and immersive visualizations. We summarize the classification results with three time-series machine learning algorithms at two scales, and explore the influences of behavior features. Our results reveal that user behaviors c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20272;&#35745;&#26469;&#35299;&#20915;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#20449;&#36947;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#26469;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20256;&#36755;&#31526;&#21495;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2311.00226</link><description>&lt;p&gt;
Transformers&#26159;&#26080;&#32447;&#36890;&#20449;&#20013;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers are Efficient In-Context Estimators for Wireless Communication. (arXiv:2311.00226v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00226
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20272;&#35745;&#26469;&#35299;&#20915;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#20449;&#36947;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#26469;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20256;&#36755;&#31526;&#21495;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;Transformers&#21487;&#20197;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#21482;&#26377;&#23569;&#37327;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26174;&#24335;&#30340;&#27169;&#22411;&#20248;&#21270;&#12290;&#21463;&#21040;&#36825;&#20010;&#23646;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#20272;&#35745;&#65292;&#29992;&#20110;&#20272;&#35745;&#20174;&#25509;&#25910;&#21040;&#30340;&#31526;&#21495;&#20013;&#30340;&#20256;&#36755;&#31526;&#21495;&#30340;&#32463;&#20856;&#36890;&#20449;&#38382;&#39064;&#12290;&#36890;&#20449;&#20449;&#36947;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#23558;&#20256;&#36755;&#31526;&#21495;&#26144;&#23556;&#21040;&#25509;&#25910;&#31526;&#21495;&#30340;&#22122;&#22768;&#20989;&#25968;&#65292;&#36825;&#20010;&#20989;&#25968;&#21487;&#20197;&#30001;&#19968;&#20010;&#26410;&#30693;&#21442;&#25968;&#34920;&#31034;&#65292;&#20854;&#32479;&#35745;&#25968;&#25454;&#20381;&#36182;&#20110;&#19968;&#20010;&#65288;&#20063;&#26159;&#26410;&#30693;&#30340;&#65289;&#28508;&#22312;&#19978;&#19979;&#25991;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#65292;&#21482;&#26159;&#35797;&#22270;&#20351;&#29992;&#24050;&#30693;&#30340;&#20256;&#36755;&#20449;&#21495;&#36827;&#34892;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#28982;&#21518;&#29992;&#20110;&#20272;&#35745;&#36830;&#32493;&#30340;&#26410;&#30693;&#20256;&#36755;&#31526;&#21495;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#26412;&#32852;&#31995;&#65292;&#21363;Transformers&#22312;&#23569;&#37327;&#25552;&#31034;&#19979;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#19978;&#19979;&#25991;&#24207;&#21015;&#23436;&#25104;&#33021;&#21147;&#65292;&#22240;&#27492;&#23427;&#20204;&#24212;&#35813;&#33021;&#22815;&#38544;&#24335;&#30830;&#23450;...
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers can perform in-context learning, where they adapt to a new task using only a small number of prompts without any explicit model optimization. Inspired by this attribute, we propose a novel approach, called in-context estimation, for the canonical communication problem of estimating transmitted symbols from received symbols. A communication channel is essentially a noisy function that maps transmitted symbols to received symbols, and this function can be represented by an unknown parameter whose statistics depend on an (also unknown) latent context. Conventional approaches ignore this hierarchical structure and simply attempt to use known transmissions, called pilots, to perform a least-squares estimate of the channel parameter, which is then used to estimate successive, unknown transmitted symbols. We make the basic connection that transformers show excellent contextual sequence completion with a few prompts, and so they should be able to implicitly determine t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#35782;&#21035;&#20855;&#26377;&#26368;&#39640;&#39044;&#26399;&#25928;&#26524;&#30340;&#27835;&#30103;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#22266;&#23450;&#39044;&#31639;&#30340;&#23616;&#37096;&#26368;&#20248;&#31639;&#27861;&#26469;&#38477;&#20302;&#38169;&#35823;&#35782;&#21035;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.19788</link><description>&lt;p&gt;
&#20855;&#26377;&#22266;&#23450;&#39044;&#31639;&#30340;&#23616;&#37096;&#26368;&#20248;&#26368;&#20339;&#33218;&#35782;&#21035;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Locally Optimal Best Arm Identification with a Fixed Budget. (arXiv:2310.19788v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#35782;&#21035;&#20855;&#26377;&#26368;&#39640;&#39044;&#26399;&#25928;&#26524;&#30340;&#27835;&#30103;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#22266;&#23450;&#39044;&#31639;&#30340;&#23616;&#37096;&#26368;&#20248;&#31639;&#27861;&#26469;&#38477;&#20302;&#38169;&#35823;&#35782;&#21035;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#21363;&#20855;&#26377;&#26368;&#39640;&#39044;&#26399;&#25928;&#26524;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#38477;&#20302;&#38169;&#35823;&#35782;&#21035;&#30340;&#27010;&#29575;&#26469;&#30830;&#23450;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#65292;&#36825;&#19968;&#38382;&#39064;&#22312;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#24050;&#34987;&#25506;&#32034;&#65292;&#21253;&#25324;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;Best Arm Identification&#65292;BAI&#65289;&#21644;&#24207;&#21015;&#20248;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#27835;&#30103;&#20998;&#37197;&#30340;&#36718;&#25968;&#26159;&#22266;&#23450;&#30340;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#20915;&#31574;&#32773;&#23558;&#19968;&#31181;&#27835;&#30103;&#26041;&#26696;&#20998;&#37197;&#32473;&#19968;&#20010;&#23454;&#39564;&#21333;&#20803;&#65292;&#24182;&#35266;&#23519;&#30456;&#24212;&#30340;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#36981;&#24490;&#19981;&#21516;&#27835;&#30103;&#26041;&#26696;&#20043;&#38388;&#26041;&#24046;&#19981;&#21516;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#23454;&#39564;&#32467;&#26463;&#26102;&#65292;&#25105;&#20204;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#25512;&#33616;&#19968;&#31181;&#27835;&#30103;&#26041;&#26696;&#20316;&#20026;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#30340;&#20272;&#35745;&#20540;&#12290;&#20915;&#31574;&#32773;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#23454;&#39564;&#65292;&#20351;&#38169;&#35823;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#30340;&#27010;&#29575;&#26368;&#23567;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35823;&#35782;&#21035;&#27010;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the problem of identifying the best treatment arm, a treatment arm with the highest expected outcome. We aim to identify the best treatment arm with a lower probability of misidentification, which has been explored under various names across numerous research fields, including \emph{best arm identification} (BAI) and ordinal optimization. In our experiments, the number of treatment-allocation rounds is fixed. In each round, a decision-maker allocates a treatment arm to an experimental unit and observes a corresponding outcome, which follows a Gaussian distribution with a variance different among treatment arms. At the end of the experiment, we recommend one of the treatment arms as an estimate of the best treatment arm based on the observations. The objective of the decision-maker is to design an experiment that minimizes the probability of misidentifying the best treatment arm. With this objective in mind, we develop lower bounds for the probability of misident
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.18144</link><description>&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#36827;&#20869;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#22870;&#21169;&#36890;&#36807;&#23450;&#20041;&#33258;&#23450;&#20041;&#30340;&#20869;&#22312;&#30446;&#26631;&#26469;&#24341;&#23548;&#38271;&#26399;&#25506;&#32034;&#12290;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#20351;&#29992;&#29366;&#24577;&#35775;&#38382;&#39057;&#29575;&#26469;&#33719;&#24471;&#25506;&#32034;&#22870;&#21169;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20219;&#20309;&#20174;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#37117;&#26159;&#38750;&#22266;&#23450;&#30340;&#65292;&#22240;&#27492;&#20026;&#20195;&#29702;&#20154;&#26500;&#24314;&#20102;&#19968;&#20010;&#38590;&#20197;&#20248;&#21270;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;&#22686;&#24378;&#29366;&#24577;&#34920;&#31034;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25506;&#32034;&#30340;&#22266;&#23450;&#30446;&#26631;&#65288;SOFE&#65289;&#26694;&#26550;&#12290;SOFE&#38656;&#35201;&#35782;&#21035;&#19981;&#21516;&#25506;&#32034;&#22870;&#21169;&#30340;&#36275;&#22815;&#32479;&#35745;&#37327;&#65292;&#24182;&#25214;&#21040;&#19968;&#31181;&#23558;&#36825;&#20123;&#32479;&#35745;&#37327;&#39640;&#25928;&#32534;&#30721;&#20316;&#20026;&#28145;&#24230;&#32593;&#32476;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;SOFE&#22522;&#20110;&#25552;&#20986;&#25193;&#23637;&#29366;&#24577;&#31354;&#38388;&#30340;&#29366;&#24577;&#22686;&#24378;&#65292;&#20294;&#26377;&#24076;&#26395;&#31616;&#21270;&#20195;&#29702;&#30446;&#26631;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SOFE&#25913;&#21892;&#20102;&#25506;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20272;&#35745;&#26799;&#24230;&#19979;&#38477;&#24471;&#21040;&#30340;&#21021;&#22987;&#21442;&#25968;&#21521;&#37327;&#20013;&#21487;&#29992;&#30340;&#32593;&#32476;&#38598;&#21512;&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21069;&#21521;ReLU&#32593;&#32476;&#27867;&#21270;&#35823;&#24046;&#30340;PAC&#31867;&#22411;&#30028;&#38480;&#65292;&#36890;&#36807;&#38480;&#21046;&#32593;&#32476;&#26799;&#24230;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#22312;&#20248;&#21270;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#30340;&#28789;&#25935;&#24230;&#65292;&#19981;&#26174;&#24335;&#22320;&#20381;&#36182;&#32593;&#32476;&#30340;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17378</link><description>&lt;p&gt;
&#22522;&#20110;&#20999;&#21521;&#31354;&#38388;&#20013;&#30340;&#28789;&#25935;&#24230;&#30340;ReLU&#32593;&#32476;&#30340;&#20248;&#21270;&#30456;&#20851;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle. (arXiv:2310.17378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20272;&#35745;&#26799;&#24230;&#19979;&#38477;&#24471;&#21040;&#30340;&#21021;&#22987;&#21442;&#25968;&#21521;&#37327;&#20013;&#21487;&#29992;&#30340;&#32593;&#32476;&#38598;&#21512;&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21069;&#21521;ReLU&#32593;&#32476;&#27867;&#21270;&#35823;&#24046;&#30340;PAC&#31867;&#22411;&#30028;&#38480;&#65292;&#36890;&#36807;&#38480;&#21046;&#32593;&#32476;&#26799;&#24230;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#22312;&#20248;&#21270;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#30340;&#28789;&#25935;&#24230;&#65292;&#19981;&#26174;&#24335;&#22320;&#20381;&#36182;&#32593;&#32476;&#30340;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#19968;&#20123;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#28982;&#32780;&#25991;&#29486;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#35299;&#37322;&#20026;&#20160;&#20040;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#65292;&#21516;&#26102;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#26799;&#24230;&#19979;&#38477;&#24471;&#21040;&#30340;&#21021;&#22987;&#21442;&#25968;&#21521;&#37327;&#20013;&#21487;&#29992;&#30340;&#32593;&#32476;&#38598;&#21512;&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21069;&#21521;ReLU&#32593;&#32476;&#27867;&#21270;&#35823;&#24046;&#30340;PAC&#31867;&#22411;&#30028;&#38480;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#32593;&#32476;&#26799;&#24230;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#22312;&#20248;&#21270;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#30340;&#28789;&#25935;&#24230;&#38480;&#21046;&#22312;&#19968;&#20010;&#30028;&#38480;&#20869;&#12290;&#25152;&#24471;&#21040;&#30340;&#30028;&#38480;&#19981;&#26174;&#24335;&#22320;&#20381;&#36182;&#32593;&#32476;&#30340;&#28145;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have given us some very promising results on the generalization ability of deep neural networks, however literature still lacks a comprehensive theory explaining why heavily over-parametrized models are able to generalize well while fitting the training data. In this paper we propose a PAC type bound on the generalization error of feedforward ReLU networks via estimating the Rademacher complexity of the set of networks available from an initial parameter vector via gradient descent. The key idea is to bound the sensitivity of the network's gradient to perturbation of the input data along the optimization trajectory. The obtained bound does not explicitly depend on the depth of the network. Our results are experimentally verified on the MNIST and CIFAR-10 datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#65288;GFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23545;&#20854;&#20851;&#38190;&#29305;&#24449;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#38416;&#36848;&#12290;&#21516;&#26102;&#65292;&#23558;&#29616;&#26377;GFMs&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#22270;&#23398;&#20064;&#33539;&#24335;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.11829</link><description>&lt;p&gt;
&#36208;&#21521;&#22270;&#22522;&#30784;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;&#19982;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Towards Graph Foundation Models: A Survey and Beyond. (arXiv:2310.11829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#65288;GFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23545;&#20854;&#20851;&#38190;&#29305;&#24449;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#38416;&#36848;&#12290;&#21516;&#26102;&#65292;&#23558;&#29616;&#26377;GFMs&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#22270;&#23398;&#20064;&#33539;&#24335;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#25104;&#21151;&#65292;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#32463;&#21382;&#20102;&#30001;&#27973;&#23618;&#26041;&#27861;&#21521;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36716;&#21464;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#21644;&#21516;&#21270;&#33021;&#21147;&#24341;&#36215;&#20102;&#22270;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#30340;&#20852;&#36259;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#24320;&#21457;&#19979;&#19968;&#20010;&#39044;&#35757;&#32451;&#20110;&#24191;&#27867;&#22270;&#25968;&#25454;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#22270;&#20219;&#21153;&#30340;&#22270;&#23398;&#20064;&#33539;&#24335;&#30340;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#36825;&#31867;&#24037;&#20316;&#23578;&#26080;&#26126;&#30830;&#30340;&#23450;&#20041;&#21644;&#31995;&#32479;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;(GFMs)&#30340;&#27010;&#24565;&#65292;&#24182;&#39318;&#27425;&#23545;&#20854;&#20851;&#38190;&#29305;&#24449;&#21644;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#38416;&#36848;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26681;&#25454;&#20854;&#21487;&#38752;&#24615;&#23558;&#29616;&#26377;GFMs&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Parallelly, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. The emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. However, there is currently no clear definition and systematic analysis for this type of work. In this article, we propose the concept of graph foundation models (GFMs), and provide the first comprehensive elucidation on their key characteristics and technologies. Following that, we categorize existing works towards GFMs into three categories based on their relia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.11730</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#30340;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#36890;&#36807;&#20803;&#36335;&#24452;&#25551;&#36848;&#20016;&#23500;&#30340;&#35821;&#20041;&#65292;&#24050;&#25104;&#20026;&#32531;&#35299;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;HIN&#30340;&#25512;&#33616;&#31995;&#32479;&#25345;&#26377;&#25968;&#25454;&#30340;&#38598;&#20013;&#23384;&#20648;&#20551;&#35774;&#65292;&#24182;&#36827;&#34892;&#38598;&#20013;&#24335;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#23384;&#20648;&#65292;&#23548;&#33268;&#38598;&#20013;&#24335;HIN&#25512;&#33616;&#26080;&#27861;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;HIN&#20998;&#20026;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#31169;&#26377;HIN&#21644;&#26381;&#21153;&#22120;&#31471;&#30340;&#20849;&#20139;HIN&#12290;&#22312;&#27492;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;HIN&#19978;&#21327;&#20316;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#27844;&#38706;&#29992;&#25143;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#22522;&#20110;HIN&#30340;&#32852;&#21512;&#25512;&#33616;&#65292;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#20809;&#19979;&#30830;&#23450;&#20102;&#38544;&#31169;&#23450;&#20041;&#65292;&#26088;&#22312;&#20445;&#25252;&#31169;&#26377;HIN&#30340;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#65292;&#20197;&#21450;&#29992;&#25143;&#30340;&#38544;&#31169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.11211</link><description>&lt;p&gt;
&#29702;&#35299;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Understanding Fairness Surrogate Functions in Algorithmic Fairness. (arXiv:2310.11211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#35266;&#23519;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26576;&#20123;&#20154;&#32676;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#24182;&#23454;&#29616;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#24341;&#20837;&#28041;&#21450;&#20844;&#24179;&#24615;&#23450;&#20041;&#30340;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#36825;&#31181;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#28145;&#20837;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#8212;&#8212;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#8212;&#8212;&#20026;&#20363;&#65292;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#23450;&#20041;&#21644;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20195;&#29702;-&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;&#36825;&#20010;"&#24046;&#36317;"&#30452;&#25509;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#26159;&#21542;&#36866;&#21512;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#36825;&#20010;"&#24046;&#36317;"&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#28608;&#21457;&#20102;&#25105;&#20204;&#30340;&#20852;&#36259;&#65292;&#34920;&#26126;&#26080;&#38480;&#21046;&#30340;&#20195;&#29702;&#20989;&#25968;&#23558;&#21463;&#21040;&#20915;&#31574;&#36793;&#30028;&#36828;&#31163;&#30340;&#28857;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The "gap" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the "gap" motivate us that the unbounded surrogate functions will be affected by the points far from the decisi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;URL&#36827;&#34892;&#22403;&#22334;&#21644;&#38750;&#22403;&#22334;&#30340;&#20998;&#31867;&#65292;&#21457;&#29616;bagging&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;96.5%&#12290;</title><link>http://arxiv.org/abs/2310.05953</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22403;&#22334;URL&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Spam URLs Using Machine Learning Approaches. (arXiv:2310.05953v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05953
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;URL&#36827;&#34892;&#22403;&#22334;&#21644;&#38750;&#22403;&#22334;&#30340;&#20998;&#31867;&#65292;&#21457;&#29616;bagging&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;96.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25552;&#20379;&#20102;&#24555;&#36895;&#21644;&#20813;&#36153;&#30340;&#36890;&#20449;&#24037;&#20855;&#21644;&#24179;&#21488;&#65292;&#20114;&#32852;&#32593;&#27599;&#22825;&#34987;&#25968;&#21313;&#20159;&#29992;&#25143;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#22686;&#21152;&#65292;&#27599;&#31186;&#38047;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#22403;&#22334;&#37038;&#20214;&#65292;&#36825;&#19981;&#20165;&#28010;&#36153;&#20102;&#20114;&#32852;&#32593;&#36164;&#28304;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#28010;&#36153;&#20102;&#29992;&#25143;&#30340;&#26102;&#38388;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;URL&#20998;&#31867;&#20026;&#22403;&#22334;&#25110;&#38750;&#22403;&#22334;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;URL&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#27604;&#36739;&#20102;&#20960;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;k&#26368;&#36817;&#37051;&#12289;bagging&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#36923;&#36753;&#22238;&#24402;&#31561;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;bagging&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26368;&#39640;&#65292;&#36798;&#21040;&#20102;96.5%&#12290;&#36825;&#34920;&#26126;&#65292;bagging&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#29992;&#20110;&#23558;URL&#20998;&#31867;&#20026;&#22403;&#22334;&#25110;&#38750;&#22403;&#22334;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet is used by billions of users daily because it offers fast and free communication tools and platforms. Nevertheless, with this significant increase in usage, huge amounts of spam are generated every second, which wastes internet resources and, more importantly, users time. This study investigates using machine learning models to classify URLs as spam or non-spam. We first extract the features from the URL as it has only one feature, and then we compare the performance of several models, including k-nearest neighbors, bagging, random forest, logistic regression, and others. We find that bagging achieves the best accuracy, with an accuracy of 96.5%. This suggests that bagging is a promising approach for classifying URLs as spam or nonspam.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.03234</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#12290;&#30001;&#20110;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#20854;&#35299;&#20915;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;FCCO&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;FCCO&#30340;&#30740;&#31350;&#20551;&#35774;&#20869;&#22806;&#20989;&#25968;&#37117;&#26159;&#20809;&#28369;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#31181;&#31867;&#30340;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20854;&#20013;&#22806;&#20989;&#25968;&#26159;&#24369;&#20984;&#19988;&#38750;&#36882;&#20943;&#30340;&#65292;&#20869;&#20989;&#25968;&#26159;&#24369;&#20984;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02980</link><description>&lt;p&gt;
&#27704;&#36828;&#19981;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65306;&#20844;&#27491;&#27604;&#36739;&#38271;&#24207;&#21015;&#27169;&#22411;&#38656;&#35201;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#65292;&#24182;&#23548;&#33268;&#20102;&#19968;&#20123;&#26550;&#26500;&#65292;&#22914;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27604;Transformers&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#24615;&#36827;&#23637;&#20027;&#35201;&#26159;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#24207;&#21015;&#30340;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;&#20363;&#22914;Long Range Arena&#65289;&#19978;&#23637;&#31034;&#20986;&#26469;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#23548;&#33268;&#23545;&#26550;&#26500;&#20043;&#38388;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#24182;&#19988;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20165;&#20351;&#29992;&#19979;&#28216;&#20219;&#21153;&#25968;&#25454;&#65289;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Transformers&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#20043;&#38388;&#24471;&#21040;&#24456;&#23567;&#30340;&#24046;&#36317;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#19982;S4&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;PathX-256&#20219;&#21153;&#19978;&#23558;SSMs&#30340;&#26368;&#20339;&#25253;&#21578;&#32467;&#26524;&#25552;&#39640;&#20102;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#26469;&#35299;&#20915;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#29983;&#25104;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#30446;&#26631;&#23646;&#24615;&#12290;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#36890;&#36807;&#32852;&#21512;Transformer&#29983;&#25104;&#30340;&#26032;&#39062;&#20998;&#23376;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02066</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;Transformer&#36827;&#34892;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02066
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#26469;&#35299;&#20915;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#29983;&#25104;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#30446;&#26631;&#23646;&#24615;&#12290;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#36890;&#36807;&#32852;&#21512;Transformer&#29983;&#25104;&#30340;&#26032;&#39062;&#20998;&#23376;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#38656;&#35201;&#21516;&#26102;&#29983;&#25104;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#20854;&#30446;&#26631;&#23646;&#24615;&#65292;&#36825;&#23545;&#29983;&#25104;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#39033;&#33392;&#24040;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#65292;&#23427;&#23558;Transformer&#30340;&#35299;&#30721;&#22120;&#12289;&#32534;&#30721;&#22120;&#21644;&#39044;&#27979;&#22120;&#32467;&#21512;&#20026;&#19968;&#20010;&#20855;&#26377;&#20849;&#20139;&#26435;&#37325;&#30340;&#32852;&#21512;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#26469;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32852;&#21512;Transformer&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#30446;&#26631;&#23646;&#24615;&#30340;&#26032;&#39062;&#20998;&#23376;&#65292;&#30456;&#27604;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
De novo drug design requires simultaneously generating novel molecules outside of training data and predicting their target properties, making it a hard task for generative models. To address this, we propose Joint Transformer that combines a Transformer decoder, a Transformer encoder, and a predictor in a joint generative model with shared weights. We show that training the model with a penalized log-likelihood objective results in state-of-the-art performance in molecule generation, while decreasing the prediction error on newly sampled molecules, as compared to a fine-tuned decoder-only Transformer, by 42%. Finally, we propose a probabilistic black-box optimization algorithm that employs Joint Transformer to generate novel molecules with improved target properties, as compared to the training data, outperforming other SMILES-based optimization methods in de novo drug design.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20559;&#31227;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#36136;&#37327;&#21644;&#39044;&#27979;&#30340;&#20559;&#31227;&#33539;&#22260;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#32416;&#27491;&#23567;&#30340;&#31354;&#38388;&#35823;&#24046;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20869;&#23384;&#28040;&#32791;&#19978;&#20943;&#23569;&#20102;10&#20493;&#20197;&#19978;&#65292;&#24182;&#19988;&#36895;&#24230;&#25552;&#39640;&#20102;3&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2309.16849</link><description>&lt;p&gt;
&#20855;&#26377;&#20559;&#31227;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#26102;&#31354;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Space-Time Attention with Shifted Non-Local Search. (arXiv:2309.16849v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20559;&#31227;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#36136;&#37327;&#21644;&#39044;&#27979;&#30340;&#20559;&#31227;&#33539;&#22260;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#32416;&#27491;&#23567;&#30340;&#31354;&#38388;&#35823;&#24046;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20869;&#23384;&#28040;&#32791;&#19978;&#20943;&#23569;&#20102;10&#20493;&#20197;&#19978;&#65292;&#24182;&#19988;&#36895;&#24230;&#25552;&#39640;&#20102;3&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35270;&#39057;&#20013;&#29289;&#20307;&#30340;&#36816;&#21160;&#65292;&#26377;&#25928;&#35745;&#31639;&#27880;&#24847;&#21147;&#22270;&#23545;&#20110;&#35270;&#39057;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#26631;&#20934;&#30340;&#38750;&#23616;&#37096;&#25628;&#32034;&#23545;&#20110;&#27599;&#20010;&#26597;&#35810;&#28857;&#21608;&#22260;&#30340;&#31383;&#21475;&#20855;&#26377;&#39640;&#36136;&#37327;&#65292;&#20294;&#31383;&#21475;&#30340;&#22823;&#23567;&#26080;&#27861;&#23481;&#32435;&#36816;&#21160;&#12290;&#38271;&#31243;&#36816;&#21160;&#30340;&#26041;&#27861;&#20351;&#29992;&#36741;&#21161;&#32593;&#32476;&#39044;&#27979;&#19982;&#27599;&#20010;&#26597;&#35810;&#20301;&#32622;&#30340;&#20559;&#31227;&#37327;&#26368;&#30456;&#20284;&#30340;&#20851;&#38190;&#22352;&#26631;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#39044;&#27979;&#27492;&#20559;&#31227;&#30340;&#20809;&#27969;&#22330;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#35268;&#27169;&#32593;&#32476;&#20063;&#26159;&#22914;&#27492;&#12290;&#23567;&#30340;&#31354;&#38388;&#19981;&#20934;&#30830;&#24615;&#20250;&#20005;&#37325;&#24433;&#21709;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38750;&#23616;&#37096;&#25628;&#32034;&#30340;&#36136;&#37327;&#19982;&#39044;&#27979;&#30340;&#20559;&#31227;&#37327;&#33539;&#22260;&#30456;&#32467;&#21512;&#30340;&#25628;&#32034;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#21629;&#21517;&#20026;&#20559;&#31227;&#38750;&#23616;&#37096;&#25628;&#32034;&#65292;&#23427;&#22312;&#39044;&#27979;&#30340;&#20559;&#31227;&#21608;&#22260;&#25191;&#34892;&#19968;&#20010;&#23567;&#30340;&#32593;&#26684;&#25628;&#32034;&#65292;&#20197;&#32416;&#27491;&#23567;&#30340;&#31354;&#38388;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21407;&#22320;&#35745;&#31639;&#28040;&#32791;&#30340;&#20869;&#23384;&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#23569;&#20102;10&#20493;&#65292;&#36895;&#24230;&#24555;&#20102;3&#20493;&#20197;&#19978;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#32416;&#27491;&#20102;&#23567;&#30340;&#31354;&#38388;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Efficiently computing attention maps for videos is challenging due to the motion of objects between frames. While a standard non-local search is high-quality for a window surrounding each query point, the window's small size cannot accommodate motion. Methods for long-range motion use an auxiliary network to predict the most similar key coordinates as offsets from each query location. However, accurately predicting this flow field of offsets remains challenging, even for large-scale networks. Small spatial inaccuracies significantly impact the attention module's quality. This paper proposes a search strategy that combines the quality of a non-local search with the range of predicted offsets. The method, named Shifted Non-Local Search, executes a small grid search surrounding the predicted offsets to correct small spatial errors. Our method's in-place computation consumes 10 times less memory and is over 3 times faster than previous work. Experimentally, correcting the small spatial err
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;AI&#39537;&#21160;&#30340;&#30693;&#35782;&#21457;&#29616;&#25216;&#26415;&#65292;&#39318;&#27425;&#25581;&#31034;&#20102;&#22320;&#24418;&#29305;&#24449;&#19982;&#38477;&#27700;&#27169;&#24335;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#22312;1995&#24180;&#24038;&#21491;&#25581;&#31034;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#22320;&#24418;&#19982;&#38477;&#27700;&#20851;&#31995;&#36716;&#21464;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2309.15400</link><description>&lt;p&gt;
&#36890;&#36807;AI&#39537;&#21160;&#30340;&#30693;&#35782;&#21457;&#29616;&#65292;&#38761;&#26032;&#23545;&#22320;&#24418;&#38477;&#27700;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Terrain-Precipitation Understanding through AI-driven Knowledge Discovery. (arXiv:2309.15400v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15400
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;AI&#39537;&#21160;&#30340;&#30693;&#35782;&#21457;&#29616;&#25216;&#26415;&#65292;&#39318;&#27425;&#25581;&#31034;&#20102;&#22320;&#24418;&#29305;&#24449;&#19982;&#38477;&#27700;&#27169;&#24335;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#22312;1995&#24180;&#24038;&#21491;&#25581;&#31034;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#22320;&#24418;&#19982;&#38477;&#27700;&#20851;&#31995;&#36716;&#21464;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#27668;&#20505;&#31185;&#23398;&#20013;&#65292;&#25512;&#36827;&#25105;&#20204;&#23545;&#22797;&#26434;&#22320;&#24418;&#22320;&#21306;&#27668;&#20505;&#36807;&#31243;&#30340;&#29702;&#35299;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#20840;&#29699;&#27668;&#20505;&#21464;&#21270;&#30340;&#32972;&#26223;&#19979;&#12290;&#23588;&#20854;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#36825;&#20123;&#22320;&#21306;&#35266;&#27979;&#25968;&#25454;&#30340;&#32570;&#20047;&#23545;&#20110;&#29702;&#35299;&#20854;&#20013;&#24494;&#22937;&#30340;&#27668;&#20505;&#21160;&#21147;&#23398;&#20135;&#29983;&#20102;&#26174;&#33879;&#38480;&#21046;&#12290;&#39318;&#27425;&#21033;&#29992;&#23574;&#31471;&#30340;AI&#39537;&#21160;&#30340;&#30693;&#35782;&#21457;&#29616;&#25216;&#26415;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#26126;&#30830;&#30340;&#26041;&#31243;&#26469;&#38416;&#26126;&#22320;&#24418;&#29305;&#24449;&#21644;&#38477;&#27700;&#27169;&#24335;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20043;&#21069;&#38544;&#34255;&#30340;&#25511;&#21046;&#36825;&#20123;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123;&#36804;&#20170;&#26410;&#25259;&#38706;&#30340;&#26041;&#31243;&#22312;&#24212;&#29992;&#20110;&#38477;&#27700;&#25968;&#25454;&#26102;&#19982;&#20256;&#32479;&#32463;&#39564;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#8220;1995&#24180;&#36716;&#25240;&#28857;&#8221;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#20102;&#22823;&#32422;&#22312;1995&#24180;&#24038;&#21491;&#22320;&#24418;&#19982;&#38477;&#27700;&#20851;&#31995;&#30340;&#26174;&#33879;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancing our understanding of climate processes in regions characterized by intricate terrain complexity is a paramount challenge in contemporary climate science, particularly in the context of global climate change. Notably, the scarcity of observational data in these regions has imposed substantial limitations on understanding the nuanced climate dynamics therein. For the first time, utilizing cutting-edge AI-driven knowledge discovery techniques, we have uncovered explicit equations that elucidate the intricate relationship between terrain features and precipitation patterns, illuminating the previously concealed complexities governing these relationships. These equations, thus far undisclosed, exhibit remarkable accuracy compared to conventional empirical models when applied to precipitation data. Building on this foundation, we reveal a phenomenon known as the '1995 turning point,' indicating a significant shift in the terrain-precipitation relationship in approximately 1995, rel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;&#20013;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23454;&#29616;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#20197;&#21450;&#26367;&#20195;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.15325</link><description>&lt;p&gt;
&#31070;&#32463;&#36816;&#31639;&#31526;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Operators for Accelerating Scientific Simulations and Design. (arXiv:2309.15325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#27169;&#25311;&#21644;&#35774;&#35745;&#20013;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23454;&#29616;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#20197;&#21450;&#26367;&#20195;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#31185;&#23398;&#21457;&#29616;&#21644;&#24037;&#31243;&#35774;&#35745;&#21463;&#38480;&#20110;&#29289;&#29702;&#23454;&#39564;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#36825;&#20123;&#23454;&#39564;&#36890;&#24120;&#26159;&#36890;&#36807;&#35797;&#39564;&#21644;&#30452;&#35273;&#36873;&#25321;&#30340;&#65292;&#24182;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25968;&#20540;&#27169;&#25311;&#26159;&#29289;&#29702;&#23454;&#39564;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#39046;&#22495;&#26469;&#35828;&#65292;&#30001;&#20110;&#29616;&#26377;&#25968;&#20540;&#26041;&#27861;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36890;&#36807;&#24320;&#21457;&#24555;&#36895;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#29305;&#21035;&#26159;&#65292;&#19968;&#20010;&#31216;&#20026;&#31070;&#32463;&#36816;&#31639;&#31526;&#30340;AI&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#36830;&#32493;&#22495;&#20989;&#25968;&#20043;&#38388;&#26144;&#23556;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#20363;&#22914;&#26102;&#31354;&#36807;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#26032;&#20301;&#32622;&#36827;&#34892;&#22806;&#25512;&#21644;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36827;&#34892;&#38646;&#23556;&#36229;&#20998;&#36776;&#29575;&#12290;&#31070;&#32463;&#36816;&#31639;&#31526;&#21487;&#20197;&#22686;&#24378;&#29978;&#33267;&#26367;&#20195;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#29616;&#26377;&#27169;&#25311;&#22120;&#65292;&#20363;&#22914;&#35745;&#31639;&#21147;&#23398;&#27969;&#20307;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific discovery and engineering design are currently limited by the time and cost of physical experiments, selected mostly through trial-and-error and intuition that require deep domain expertise. Numerical simulations present an alternative to physical experiments, but are usually infeasible for complex real-world domains due to the computational requirements of existing numerical methods. Artificial intelligence (AI) presents a potential paradigm shift through the development of fast data-driven surrogate models. In particular, an AI framework, known as neural operators, presents a principled framework for learning mappings between functions defined on continuous domains, e.g., spatiotemporal processes and partial differential equations (PDE). They can extrapolate and predict solutions at new locations unseen during training, i.e., perform zero-shot super-resolution. Neural operators can augment or even replace existing simulators in many applications, such as computational flui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#37329;&#34701;&#25968;&#25454;&#24182;&#32467;&#21512;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;FD&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.13409</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#25968;&#25454;&#37322;&#25918;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data. (arXiv:2309.13409v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#37329;&#34701;&#25968;&#25454;&#24182;&#32467;&#21512;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;FD&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#27979;&#31574;&#30053;&#65292;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#65288;FD&#65289;&#30340;&#33021;&#21147;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#19982;&#20256;&#32479;&#30340;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#19981;&#21516;&#65292;FD&#22312;&#20445;&#25345;&#31995;&#21015;&#35760;&#24518;&#30340;&#21516;&#26102;&#31283;&#23450;&#20102;&#23427;&#20197;&#20379;&#24314;&#27169;&#30446;&#30340;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#26469;&#33258;SPY&#25351;&#25968;&#30340;&#37329;&#34701;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#26032;&#38395;&#25253;&#36947;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36825;&#20010;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;FD&#19982;&#30446;&#26631;&#21464;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;&#37319;&#29992;&#20102;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#26469;&#39564;&#35777;FD&#31995;&#21015;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;FD&#30456;&#27604;&#25972;&#25968;&#24046;&#20998;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#36825;&#19968;&#28857;&#36890;&#36807;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;/&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;ROCAUC&#65289;&#21644;&#39532;&#20462;&#26031;&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#30340;&#35780;&#20272;&#24471;&#21040;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel forecasting strategy that leverages the power of fractional differencing (FD) to capture both short- and long-term dependencies in time series data. Unlike traditional integer differencing methods, FD preserves memory in series while stabilizing it for modeling purposes. By applying FD to financial data from the SPY index and incorporating sentiment analysis from news reports, this empirical analysis explores the effectiveness of FD in conjunction with binary classification of target variables. Supervised classification algorithms were employed to validate the performance of FD series. The results demonstrate the superiority of FD over integer differencing, as confirmed by Receiver Operating Characteristic/Area Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#19981;&#21464;&#19968;&#33268;&#24615;&#23398;&#20064;"&#65288;ICON&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#20104;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30456;&#31561;&#30340;&#22320;&#20301;&#65292;&#23398;&#20064;&#19968;&#20010;&#19981;&#21464;&#30340;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#22312;&#30446;&#26631;&#22495;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2309.12742</link><description>&lt;p&gt;
&#35753;UDA&#20013;&#30340;U&#21464;&#24471;&#37325;&#35201;&#65306;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#19981;&#21464;&#19968;&#33268;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation. (arXiv:2309.12742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#19981;&#21464;&#19968;&#33268;&#24615;&#23398;&#20064;"&#65288;ICON&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#20104;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30456;&#31561;&#30340;&#22320;&#20301;&#65292;&#23398;&#20064;&#19968;&#20010;&#19981;&#21464;&#30340;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#22312;&#30446;&#26631;&#22495;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;(DA)&#22312;&#22788;&#29702;&#39046;&#22495;&#20869;&#37096;&#30340;&#30456;&#20851;&#29305;&#24449;&#65288;&#22914;&#31867;&#21035;&#36523;&#20221;&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#65288;&#22914;&#29615;&#22659;&#65289;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#26102;&#65292;&#24635;&#26159;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#31181;&#30456;&#20851;&#24615;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#19981;&#20855;&#26377;&#26222;&#36941;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#39069;&#22806;&#30340;&#26080;&#30417;&#30563;&#30446;&#26631;&#39046;&#22495;&#65292;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;DA(UDA)&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#24433;&#21709;&#12290;&#36825;&#26159;&#22240;&#20026;&#28304;&#22495;&#30417;&#30563;&#21482;&#23558;&#30446;&#26631;&#22495;&#26679;&#26412;&#35270;&#20026;&#36741;&#21161;&#25968;&#25454;&#65288;&#22914;&#36890;&#36807;&#20266;&#26631;&#31614;&#65289;&#65292;&#32780;&#30446;&#26631;&#22495;&#20013;&#26377;&#20215;&#20540;&#30340;&#21435;&#30456;&#20851;&#32447;&#32034;&#30340;&#22266;&#26377;&#20998;&#24067;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#32473;&#20104;&#20004;&#20010;&#22495;&#30456;&#31561;&#30340;&#22320;&#20301;&#65292;&#26469;&#20351;UDA&#20013;&#30340;U&#21464;&#24471;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#19981;&#21464;&#30340;&#20998;&#31867;&#22120;&#65292;&#20854;&#39044;&#27979;&#21516;&#26102;&#19982;&#28304;&#22495;&#26631;&#31614;&#21644;&#30446;&#26631;&#22495;&#32858;&#31867;&#19968;&#33268;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#30446;&#26631;&#22495;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;"&#19981;&#21464;&#19968;&#33268;&#24615;&#23398;&#20064;"&#65288;ICON&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation (DA) is always challenged by the spurious correlation between domain-invariant features (e.g., class identity) and domain-specific features (e.g., environment) that does not generalize to the target domain. Unfortunately, even enriched with additional unsupervised target domains, existing Unsupervised DA (UDA) methods still suffer from it. This is because the source domain supervision only considers the target domain samples as auxiliary data (e.g., by pseudo-labeling), yet the inherent distribution in the target domain -- where the valuable de-correlation clues hide -- is disregarded. We propose to make the U in UDA matter by giving equal status to the two domains. Specifically, we learn an invariant classifier whose prediction is simultaneously consistent with the labels in the source domain and clusters in the target domain, hence the spurious correlation inconsistent in the target domain is removed. We dub our approach "Invariant CONsistency learning" (ICON). Exte
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.08420</link><description>&lt;p&gt;
FedDCSR: &#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#24207;&#21015;&#25968;&#25454;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;(CSR)&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CSR&#26041;&#27861;&#38656;&#35201;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#32852;&#37030;&#23398;&#20064;(FL)&#21644;CSR&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#23545;FL&#30340;&#25972;&#20307;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDCSR&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#39046;&#22495;&#20869;-&#39046;&#22495;&#38388;&#24207;&#21015;&#34920;&#31034;&#35299;&#32544;(SRD)&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24207;&#21015;&#29305;&#24449;&#35299;&#32544;&#25104;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#19987;&#23646;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#25605;&#25292;&#25705;&#25830;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#23792;&#20540;&#28201;&#24230;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#26368;&#20339;&#30340;SML&#26041;&#27861;&#20026;&#26799;&#24230;&#25552;&#21319;&#27861;&#65292;&#26368;&#20302;&#30340;&#22343;&#26041;&#35823;&#24046;&#20026;165.78&#12290;</title><link>http://arxiv.org/abs/2309.06838</link><description>&lt;p&gt;
&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#38109;&#21512;&#37329;&#25605;&#25292;&#25705;&#25830;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#23792;&#20540;&#28201;&#24230;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy. (arXiv:2309.06838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#25605;&#25292;&#25705;&#25830;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#23792;&#20540;&#28201;&#24230;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#26368;&#20339;&#30340;SML&#26041;&#27861;&#20026;&#26799;&#24230;&#25552;&#21319;&#27861;&#65292;&#26368;&#20302;&#30340;&#22343;&#26041;&#35823;&#24046;&#20026;165.78&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#26448;&#25605;&#25292;&#25705;&#25830;&#27785;&#31215;&#65288;AFSD&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22266;&#24577;&#22686;&#26448;&#21046;&#36896;&#25216;&#26415;&#65292;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;&#31881;&#26411;&#24202;&#29076;&#28860;&#21644;&#23450;&#21521;&#33021;&#37327;&#27785;&#31215;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#23380;&#38553;&#29575;&#12289;&#24320;&#35010;&#21644;&#24615;&#33021;&#21508;&#21521;&#24322;&#24615;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;AFSD&#20013;&#30340;&#24037;&#33402;&#21442;&#25968;&#12289;&#28909;&#37327;&#20998;&#24067;&#21644;&#24471;&#21040;&#30340;&#26174;&#24494;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20173;&#28982;&#19981;&#22815;&#28165;&#26970;&#65292;&#36825;&#22952;&#30861;&#20102;&#24615;&#33021;&#30340;&#24037;&#33402;&#20248;&#21270;&#12290;&#26412;&#30740;&#31350;&#36816;&#29992;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#23558;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65288;SML&#65289;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#20174;&#24037;&#33402;&#21442;&#25968;&#39044;&#27979;AFSD&#20013;&#30340;&#23792;&#20540;&#28201;&#24230;&#20998;&#24067;&#12290;&#23545;&#20110;SML&#24314;&#27169;&#65292;&#20351;&#29992;&#20102;&#20843;&#31181;&#22238;&#24402;&#31639;&#27861;&#65292;&#32780;&#23545;&#20110;PINNs&#65292;&#20351;&#29992;&#20102;&#36816;&#36755;&#12289;&#27874;&#20256;&#25773;&#12289;&#28909;&#20256;&#23548;&#21644;&#37327;&#23376;&#21147;&#23398;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#22312;&#22810;&#20010;&#32479;&#35745;&#25351;&#26631;&#19978;&#65292;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#26799;&#24230;&#25552;&#21319;&#27861;&#26159;&#26368;&#20339;&#30340;SML&#26041;&#27861;&#65292;&#26368;&#20302;&#30340;&#22343;&#26041;&#35823;&#24046;&#20026;165.78&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive friction stir deposition (AFSD) is a novel solid-state additive manufacturing technique that circumvents issues of porosity, cracking, and properties anisotropy that plague traditional powder bed fusion and directed energy deposition approaches. However, correlations between process parameters, thermal profiles, and resulting microstructure in AFSD remain poorly understood. This hinders process optimization for properties. This work employs a cutting-edge framework combining supervised machine learning (SML) and physics-informed neural networks (PINNs) to predict peak temperature distribution in AFSD from process parameters. Eight regression algorithms were implemented for SML modeling, while four PINNs leveraged governing equations for transport, wave propagation, heat transfer, and quantum mechanics. Across multiple statistical measures, ensemble techniques like gradient boosting proved superior for SML, with lowest MSE of 165.78. The integrated ML approach was also applied 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#26679;&#26412;&#31354;&#38388;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#29983;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30446;&#26631;&#20998;&#23376;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#36890;&#36807;&#22312;&#21270;&#23398;&#31354;&#38388;&#20195;&#29702;&#20869;&#37096;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#29983;&#25104;&#20998;&#23376;&#19982;&#34507;&#30333;&#36136;&#38774;&#26631;&#20043;&#38388;&#21560;&#24341;&#21147;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05853</link><description>&lt;p&gt;
ChemSpaceAL:&#19968;&#20010;&#24212;&#29992;&#20110;&#29305;&#23450;&#34507;&#30333;&#36136;&#20998;&#23376;&#29983;&#25104;&#30340;&#39640;&#25928;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation. (arXiv:2309.05853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05853
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#26679;&#26412;&#31354;&#38388;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#29983;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30446;&#26631;&#20998;&#23376;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#36890;&#36807;&#22312;&#21270;&#23398;&#31354;&#38388;&#20195;&#29702;&#20869;&#37096;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#29983;&#25104;&#20998;&#23376;&#19982;&#34507;&#30333;&#36136;&#38774;&#26631;&#20043;&#38388;&#21560;&#24341;&#21147;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#19981;&#21487;&#24605;&#35758;&#33021;&#21147;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#23427;&#20204;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#33021;&#22815;&#22686;&#24378;&#36825;&#20123;&#24378;&#22823;&#24037;&#20855;&#30340;&#33021;&#21147;&#21644;&#36866;&#29992;&#24615;&#30340;&#26041;&#27861;&#35770;&#20855;&#26377;&#24040;&#22823;&#30340;&#20852;&#36259;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#26500;&#24314;&#30340;&#26679;&#26412;&#31354;&#38388;&#34920;&#31034;&#20013;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#65292;&#21487;&#20197;&#20351;&#29983;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30446;&#26631;&#20998;&#23376;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#33021;&#22815;&#30456;&#23545;&#20110;&#22522;&#20110;&#21560;&#24341;&#21147;&#30456;&#20114;&#20316;&#29992;&#35780;&#20998;&#20989;&#25968;&#36827;&#34892;&#24494;&#35843;&#30340;GPT&#22522;&#30784;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#21270;&#23398;&#31354;&#38388;&#20195;&#29702;&#20869;&#37096;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#29983;&#25104;&#30340;&#20998;&#23376;&#19982;&#34507;&#30333;&#36136;&#38774;&#26631;&#20043;&#38388;&#30340;&#21560;&#24341;&#21147;&#30456;&#20114;&#20316;&#29992;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#29992;&#20110;&#24494;&#35843;&#30340;&#25152;&#26377;&#25968;&#25454;&#28857;&#36827;&#34892;&#21333;&#29420;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The incredible capabilities of generative artificial intelligence models have inevitably led to their application in the domain of drug discovery. It is therefore of tremendous interest to develop methodologies that enhance the abilities and applicability of these powerful tools. In this work, we present a novel and efficient semi-supervised active learning methodology that allows for the fine-tuning of a generative model with respect to an objective function by strategically operating within a constructed representation of the sample space. In the context of targeted molecular generation, we demonstrate the ability to fine-tune a GPT-based molecular generator with respect to an attractive interaction-based scoring function by strategically operating within a chemical space proxy, thereby maximizing attractive interactions between the generated molecules and a protein target. Importantly, our approach does not require the individual evaluation of all data points that are used for fine-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#32500;&#24230;&#23545;&#21387;&#32553;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#21387;&#32553;&#21518;&#22312;&#20302;&#32500;&#31354;&#38388;&#20869;&#35757;&#32451;&#20840;&#31209;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#29702;&#35770;&#20445;&#35777;&#20102;&#22312;&#19981;&#20381;&#36182;&#29615;&#22659;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#24230;&#37327;&#23398;&#20064;&#30340;&#35823;&#24046;&#21487;&#20197;&#34987;&#25511;&#21046;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#33391;&#24615;&#20960;&#20309;&#32467;&#26500;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.05751</link><description>&lt;p&gt;
&#20869;&#22312;&#32500;&#24230;&#23545;&#21387;&#32553;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Intrinsic Dimension on Metric Learning under Compression. (arXiv:2309.05751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#32500;&#24230;&#23545;&#21387;&#32553;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#21387;&#32553;&#21518;&#22312;&#20302;&#32500;&#31354;&#38388;&#20869;&#35757;&#32451;&#20840;&#31209;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#29702;&#35770;&#20445;&#35777;&#20102;&#22312;&#19981;&#20381;&#36182;&#29615;&#22659;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#24230;&#37327;&#23398;&#20064;&#30340;&#35823;&#24046;&#21487;&#20197;&#34987;&#25511;&#21046;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#33391;&#24615;&#20960;&#20309;&#32467;&#26500;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24230;&#37327;&#23398;&#20064;&#26088;&#22312;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#36866;&#24403;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#20197;&#25913;&#21892;&#22522;&#20110;&#36317;&#31163;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#65292;&#24230;&#37327;&#23398;&#20064;&#36824;&#21487;&#20197;&#20316;&#20026;&#38477;&#32500;&#30340;&#25163;&#27573;&#65292;&#36890;&#36807;&#23545;&#23398;&#20064;&#30340;&#24230;&#37327;&#26045;&#21152;&#19968;&#20010;&#20302;&#31209;&#32422;&#26463;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#30340;&#26159;&#23545;&#25968;&#25454;&#30340;&#19968;&#20010;&#38543;&#26426;&#21387;&#32553;&#29256;&#26412;&#65292;&#28982;&#21518;&#22312;&#20854;&#20013;&#35757;&#32451;&#19968;&#20010;&#20840;&#31209;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20851;&#20110;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#30340;&#35823;&#24046;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#19981;&#20381;&#36182;&#20110;&#29615;&#22659;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#36793;&#30028;&#38500;&#20102;&#23545;&#26469;&#33258;&#26377;&#30028;&#25903;&#25345;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#27809;&#26377;&#26174;&#24335;&#30340;&#20551;&#35774;&#20043;&#22806;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#33391;&#24615;&#20960;&#20309;&#32467;&#26500;&#26102;&#33258;&#21160;&#25910;&#25947;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metric learning aims at finding a suitable distance metric over the input space, to improve the performance of distance-based learning algorithms. In high-dimensional settings, metric learning can also play the role of dimensionality reduction, by imposing a low-rank restriction to the learnt metric. In this paper, instead of training a low-rank metric on high-dimensional data, we consider a randomly compressed version of the data, and train a full-rank metric there. We give theoretical guarantees on the error of distance-based metric learning, with respect to the random compression, which do not depend on the ambient dimension. Our bounds do not make any explicit assumptions, aside from i.i.d. data from a bounded support, and automatically tighten when benign geometrical structures are present. Experimental results on both synthetic and real data sets support our theoretical findings in high-dimensional settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.03886</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21151;&#33021;&#35299;&#37322;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21487;&#35835;&#30340;&#25551;&#36848;&#26631;&#35760;&#31070;&#32463;&#32593;&#32476;&#23376;&#27169;&#22359;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#36825;&#20123;&#25551;&#36848;&#21487;&#20197;&#26292;&#38706;&#22833;&#36133;&#12289;&#24341;&#23548;&#24178;&#39044;&#65292;&#29978;&#33267;&#21487;&#20197;&#35299;&#37322;&#37325;&#35201;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#26426;&#26800;&#21407;&#29702;&#30340;&#24050;&#35757;&#32451;&#32593;&#32476;&#25551;&#36848;&#37117;&#28041;&#21450;&#21040;&#23567;&#27169;&#22411;&#12289;&#29421;&#20041;&#29616;&#35937;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#22312;&#19981;&#26029;&#22686;&#21152;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#20013;&#26631;&#35760;&#20986;&#25152;&#26377;&#20154;&#21487;&#35299;&#37322;&#30340;&#23376;&#35745;&#31639;&#20960;&#20046;&#32943;&#23450;&#38656;&#35201;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#39564;&#35777;&#25551;&#36848;&#30340;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#26631;&#35760;&#30340;&#25216;&#26415;&#24320;&#22987;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#26377;&#38480;&#19988;&#20020;&#26102;&#12290;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#39564;&#35777;&#21644;&#27604;&#36739;&#24320;&#25918;&#24335;&#26631;&#35760;&#24037;&#20855;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;FIND&#65288;&#20989;&#25968;&#35299;&#37322;&#21644;&#25551;&#36848;&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#26041;&#27861;&#26500;&#24314;&#27169;&#22359;&#30340;&#22522;&#20934;&#22871;&#20214;&#12290;FIND&#21253;&#21547;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#30340;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24102;&#36890;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#22768;&#23398;&#30340;&#39057;&#29575;&#23610;&#24230;&#26469;&#23450;&#20041;&#39057;&#24102;&#24182;&#19988;&#21033;&#29992;&#20849;&#21516;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#29305;&#24615;&#25552;&#39640;&#20102;&#20998;&#31163;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02539</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#36890;&#29992;&#24102;&#36890;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation. (arXiv:2309.02539v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24102;&#36890;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#22768;&#23398;&#30340;&#39057;&#29575;&#23610;&#24230;&#26469;&#23450;&#20041;&#39057;&#24102;&#24182;&#19988;&#21033;&#29992;&#20849;&#21516;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#29305;&#24615;&#25552;&#39640;&#20102;&#20998;&#31163;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#26159;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#19968;&#20010;&#30456;&#23545;&#36739;&#26032;&#30340;&#23376;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#20174;&#28151;&#38899;&#20013;&#25552;&#21462;&#23545;&#35805;&#38899;&#36712;&#12289;&#38899;&#20048;&#38899;&#36712;&#21644;&#29305;&#25928;&#38899;&#36712;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#23545;&#39057;&#29575;&#36724;&#30340;&#20219;&#20309;&#23436;&#20840;&#25110;&#36807;&#23436;&#22791;&#30340;&#20998;&#21306;&#36827;&#34892;&#27867;&#21270;&#12290;&#22522;&#20110;&#24515;&#29702;&#22768;&#23398;&#30340;&#39057;&#29575;&#23610;&#24230;&#29992;&#20110;&#30830;&#23450;&#24102;&#36890;&#30340;&#23450;&#20041;&#65292;&#29616;&#22312;&#20855;&#22791;&#20887;&#20313;&#24615;&#20197;&#36827;&#34892;&#26356;&#21487;&#38752;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;&#20449;&#22122;&#27604;&#21644;1-&#33539;&#25968;&#30340;&#31232;&#30095;&#20419;&#36827;&#23646;&#24615;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#20849;&#21516;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#29305;&#24615;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25913;&#21892;&#38590;&#20197;&#27867;&#21270;&#30340;&#22768;&#38899;&#31867;&#21035;&#30340;&#20998;&#31163;&#24615;&#33021;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#21487;&#36731;&#26494;&#20998;&#31163;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;Divide and Remaster&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue stem, the music stem, and the effects stem from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psycho-acoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-to-noise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with easily detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with perfor
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23558;&#20256;&#32479;&#30340;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;&#38382;&#39064;&#20013;&#30340;&#23376;&#38382;&#39064;&#26367;&#25442;&#20026;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#24207;&#21015;&#30340;&#24555;&#36895;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#36816;&#21160;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15984</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning Structure-from-Motion with Graph Attention Networks. (arXiv:2308.15984v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23558;&#20256;&#32479;&#30340;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;&#38382;&#39064;&#20013;&#30340;&#23376;&#38382;&#39064;&#26367;&#25442;&#20026;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#24207;&#21015;&#30340;&#24555;&#36895;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#36816;&#21160;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#35299;&#20915;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;&#65288;SfM&#65289;&#30340;&#38382;&#39064;&#12290;SfM&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#36890;&#36807;&#36845;&#20195;&#26368;&#23567;&#21270;&#37325;&#25237;&#24433;&#35823;&#24046;&#65288;&#31216;&#20026;&#26463;&#35843;&#25972;&#65289;&#35299;&#20915;&#65292;&#20174;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#12290;&#20026;&#20102;&#33719;&#24471;&#36275;&#22815;&#22909;&#30340;&#21021;&#22987;&#21270;&#32467;&#26524;&#65292;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31995;&#21015;&#23376;&#38382;&#39064;&#65288;&#22914;&#25104;&#23545;&#23039;&#24577;&#20272;&#35745;&#12289;&#23039;&#24577;&#24179;&#22343;&#21270;&#25110;&#19977;&#35282;&#27979;&#37327;&#65289;&#65292;&#36825;&#20123;&#23376;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#22987;&#35299;&#65292;&#28982;&#21518;&#20351;&#29992;&#26463;&#35843;&#25972;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#26367;&#25442;&#20026;&#20197;&#22810;&#20010;&#35270;&#22270;&#19978;&#26816;&#27979;&#21040;&#30340;2D&#20851;&#38190;&#28857;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#30456;&#24212;&#30340;&#30456;&#26426;&#23039;&#24577;&#21644;3D&#20851;&#38190;&#28857;&#22352;&#26631;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;SfM&#29305;&#23450;&#30340;&#21407;&#35821;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#23427;&#21487;&#20197;&#29992;&#20110;&#24555;&#36895;&#25512;&#26029;&#26032;&#30340;&#21644;&#26410;&#35265;&#36807;&#30340;&#24207;&#21015;&#30340;&#37325;&#24314;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32467;&#26500;&#36816;&#21160;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#19981;&#21516;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#65288;DST&#65289;&#32452;&#20214;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#22635;&#34917;&#20851;&#38190;&#30740;&#31350;&#31354;&#30333;&#24182;&#20026;&#25345;&#32493;&#23398;&#20064;&#19979;&#30340;DST&#25552;&#20379;&#26368;&#20339;&#37197;&#32622;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.14831</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#30340;&#25345;&#32493;&#23398;&#20064;&#65306;&#25506;&#32034;&#26377;&#25928;&#27169;&#22411;&#26356;&#26032;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates. (arXiv:2308.14831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#19981;&#21516;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#65288;DST&#65289;&#32452;&#20214;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#22635;&#34917;&#20851;&#38190;&#30740;&#31350;&#31354;&#30333;&#24182;&#20026;&#25345;&#32493;&#23398;&#20064;&#19979;&#30340;DST&#25552;&#20379;&#26368;&#20339;&#37197;&#32622;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#25351;&#26234;&#33021;&#31995;&#32479;&#20174;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#20013;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#35745;&#31639;&#24320;&#38144;&#39034;&#24207;&#33719;&#21462;&#21644;&#20445;&#30041;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24050;&#22312;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#12289;&#37325;&#25918;&#12289;&#26550;&#26500;&#21644;&#21442;&#25968;&#38548;&#31163;&#31561;&#26041;&#27861;&#12290;&#20351;&#29992;&#31232;&#30095;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#38548;&#31163;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21516;&#37096;&#20998;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#22312;&#20219;&#21153;&#30456;&#20284;&#26102;&#20849;&#20139;&#21442;&#25968;&#12290;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;(DST)&#26159;&#21457;&#29616;&#36825;&#20123;&#31232;&#30095;&#32593;&#32476;&#24182;&#20026;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;&#38548;&#31163;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#23545;CL&#33539;&#24335;&#19979;&#19981;&#21516;DST&#32452;&#20214;&#25928;&#26524;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#22635;&#34917;&#20851;&#38190;&#30740;&#31350;&#31354;&#30333;&#24182;&#20026;CL&#19979;&#30340;DST&#30340;&#26368;&#20339;&#37197;&#32622;&#25552;&#20379;&#25351;&#23548;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#33879;&#21517;&#30340;CIFAR100&#21644;miniImage&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#21508;&#31181;DST&#32452;&#20214;&#20197;&#25214;&#21040;&#27599;&#20010;&#20219;&#21153;&#30340;&#26368;&#20339;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) refers to the ability of an intelligent system to sequentially acquire and retain knowledge from a stream of data with as little computational overhead as possible. To this end; regularization, replay, architecture, and parameter isolation approaches were introduced to the literature. Parameter isolation using a sparse network which enables to allocate distinct parts of the neural network to different tasks and also allows to share of parameters between tasks if they are similar. Dynamic Sparse Training (DST) is a prominent way to find these sparse networks and isolate them for each task. This paper is the first empirical study investigating the effect of different DST components under the CL paradigm to fill a critical research gap and shed light on the optimal configuration of DST for CL if it exists. Therefore, we perform a comprehensive study in which we investigate various DST components to find the best topology per task on well-known CIFAR100 and miniImag
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PiZero&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#24049;&#21019;&#24314;&#30340;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#32423;&#35268;&#21010;&#65292;&#19981;&#21463;&#30495;&#23454;&#29615;&#22659;&#38480;&#21046;&#65292;&#21487;&#22312;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#22788;&#29702;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#35774;&#32622;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#32780;&#26080;&#38656;&#20551;&#35774;&#35775;&#38382;&#29615;&#22659;&#27169;&#25311;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.08693</link><description>&lt;p&gt;
&#35745;&#21010;&#22312;&#24819;&#35937;&#20013;&#65306;&#22522;&#20110;&#23398;&#20064;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#30340;&#39640;&#32423;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning in the imagination: High-level planning on learned abstract search spaces. (arXiv:2308.08693v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PiZero&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#24049;&#21019;&#24314;&#30340;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#32423;&#35268;&#21010;&#65292;&#19981;&#21463;&#30495;&#23454;&#29615;&#22659;&#38480;&#21046;&#65292;&#21487;&#22312;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#22788;&#29702;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#35774;&#32622;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#32780;&#26080;&#38656;&#20551;&#35774;&#35775;&#38382;&#29615;&#22659;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;PiZero&#65292;&#23427;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#33258;&#24049;&#21019;&#24314;&#30340;&#25277;&#35937;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#35745;&#21010;&#65292;&#35813;&#25628;&#32034;&#31354;&#38388;&#19982;&#30495;&#23454;&#29615;&#22659;&#23436;&#20840;&#35299;&#32806;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#20197;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#39640;&#32423;&#35268;&#21010;&#65292;&#24182;&#20197;&#22797;&#21512;&#25110;&#26102;&#38388;&#25193;&#23637;&#21160;&#20316;&#30340;&#24418;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#22312;&#38656;&#35201;&#25191;&#34892;&#22823;&#37327;&#22522;&#26412;&#24494;&#25805;&#20316;&#20197;&#25191;&#34892;&#30456;&#20851;&#23439;&#25805;&#20316;&#30340;&#29615;&#22659;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#26356;&#36890;&#29992;&#65292;&#22240;&#20026;&#23427;&#22788;&#29702;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#23548;&#33322;&#20219;&#21153;&#21644;Sokoban&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#27809;&#26377;&#20551;&#35774;&#35775;&#38382;&#29615;&#22659;&#27169;&#25311;&#22120;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#21487;&#27604;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method, called PiZero, that gives an agent the ability to plan in an abstract search space of its own creation that is completely decoupled from the real environment. Unlike prior approaches, this enables the agent to perform high-level planning at arbitrary timescales and reason in terms of compound or temporally-extended actions, which can be useful in environments where large numbers of base-level micro-actions are needed to perform relevant macro-actions. In addition, our method is more general than comparable prior methods because it handles settings with continuous action spaces and partial observability. We evaluate our method on multiple domains, including navigation tasks and Sokoban. Experimentally, it outperforms comparable prior methods without assuming access to an environment simulator.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#28082;&#20307;&#30340;&#31896;&#24230;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#39057;&#20013;&#19981;&#21516;&#28082;&#20307;&#25391;&#33633;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#20174;&#35270;&#35273;&#19978;&#25512;&#26029;&#28082;&#20307;&#30340;&#31867;&#21035;&#25110;&#21160;&#24577;&#31896;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.02715</link><description>&lt;p&gt;
AI&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#27969;&#20307;&#23646;&#24615;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fluid Property Prediction Leveraging AI and Robotics. (arXiv:2308.02715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02715
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#28082;&#20307;&#30340;&#31896;&#24230;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#39057;&#20013;&#19981;&#21516;&#28082;&#20307;&#25391;&#33633;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#20174;&#35270;&#35273;&#19978;&#25512;&#26029;&#28082;&#20307;&#30340;&#31867;&#21035;&#25110;&#21160;&#24577;&#31896;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#35273;&#19978;&#25512;&#27979;&#28082;&#20307;&#23646;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28082;&#20307;&#30340;&#34892;&#20026;&#21644;&#26816;&#27979;&#37117;&#21313;&#20998;&#22797;&#26434;&#12290;&#28982;&#32780;&#65292;&#33021;&#22815;&#30452;&#25509;&#20174;&#35270;&#35273;&#20449;&#24687;&#20013;&#25512;&#26029;&#28082;&#20307;&#23646;&#24615;&#23545;&#20110;&#33258;&#20027;&#27969;&#20307;&#22788;&#29702;&#31995;&#32479;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#25668;&#20687;&#22836;&#24456;&#23481;&#26131;&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#20165;&#36890;&#36807;&#35270;&#35273;&#39044;&#27979;&#28082;&#20307;&#23646;&#24615;&#21487;&#20197;&#21152;&#24555;&#27969;&#20307;&#34920;&#24449;&#30340;&#36807;&#31243;&#65292;&#22312;&#21508;&#31181;&#23454;&#39564;&#29615;&#22659;&#20013;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#35270;&#35273;&#26041;&#27861;&#26469;&#20272;&#35745;&#31896;&#24230;&#65292;&#21033;&#29992;&#28082;&#20307;&#25391;&#33633;&#34892;&#20026;&#19982;&#31896;&#24230;&#30452;&#25509;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;3D&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#35270;&#39057;&#20013;&#19981;&#21516;&#28082;&#20307;&#25391;&#33633;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20511;&#21161;&#36825;&#20123;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#35270;&#39057;&#20013;&#30452;&#35266;&#22320;&#25512;&#26029;&#28082;&#20307;&#30340;&#31867;&#21035;&#25110;&#28082;&#20307;&#30340;&#21160;&#24577;&#31896;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring liquid properties from vision is a challenging task due to the complex nature of fluids, both in behavior and detection. Nevertheless, the ability to infer their properties directly from visual information is highly valuable for autonomous fluid handling systems, as cameras are readily available. Moreover, predicting fluid properties purely from vision can accelerate the process of fluid characterization saving considerable time and effort in various experimental environments. In this work, we present a purely vision-based approach to estimate viscosity, leveraging the fact that the behavior of the fluid oscillations is directly related to the viscosity. Specifically, we utilize a 3D convolutional autoencoder to learn latent representations of different fluid-oscillating patterns present in videos. We leverage this latent representation to visually infer the category of fluid or the dynamics viscosity of fluid from video.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#30340;&#27178;&#26029;&#38754;&#21644;&#32437;&#21521;&#32034;&#36180;&#35745;&#25968;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#31934;&#31639;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#20004;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.01729</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;&#30340;&#27178;&#26029;&#38754;&#21644;&#32437;&#21521;&#32034;&#36180;&#35745;&#25968;&#25968;&#25454;&#30340;&#36710;&#36733;&#36890;&#20449;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data. (arXiv:2308.01729v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#30340;&#27178;&#26029;&#38754;&#21644;&#32437;&#21521;&#32034;&#36180;&#35745;&#25968;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#31934;&#31639;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#20004;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Mario W\"uthrich&#21644;Michael Merz&#25552;&#20986;&#30340;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;&#65288;CANN&#65289;&#26694;&#26550;&#30340;&#27178;&#26029;&#38754;&#21644;&#32437;&#21521;&#32034;&#36180;&#35745;&#25968;&#27169;&#22411;&#12290;CANN&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#31934;&#31639;&#27169;&#22411;&#65288;&#22914;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65289;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;&#32463;&#20856;&#22238;&#24402;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#37096;&#20998;&#30340;&#21452;&#32452;&#20214;&#27169;&#22411;&#12290;CANN&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#20102;&#20004;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#26082;&#21487;&#20197;&#25552;&#20379;&#32463;&#20856;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#21487;&#20197;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#22797;&#26434;&#20851;&#31995;&#21644;&#20132;&#20114;&#20316;&#29992;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#24191;&#20026;&#20154;&#30693;&#30340;&#23545;&#25968;&#32447;&#24615;&#32034;&#36180;&#35745;&#25968;&#22238;&#24402;&#27169;&#22411;&#20316;&#20026;&#32463;&#20856;&#22238;&#24402;&#37096;&#20998;&#65292;&#20351;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#37096;&#20998;&#12290;MLP&#37096;&#20998;&#29992;&#20110;&#22788;&#29702;&#20197;&#21521;&#37327;&#24418;&#24335;&#34920;&#31034;&#30340;&#36710;&#36742;&#39550;&#39542;&#34892;&#20026;&#30340;&#36710;&#36733;&#36890;&#20449;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present novel cross-sectional and longitudinal claim count models for vehicle insurance built upon the Combined Actuarial Neural Network (CANN) framework proposed by Mario W\"uthrich and Michael Merz. The CANN approach combines a classical actuarial model, such as a generalized linear model, with a neural network. This blending of models results in a two-component model comprising a classical regression model and a neural network part. The CANN model leverages the strengths of both components, providing a solid foundation and interpretability from the classical model while harnessing the flexibility and capacity to capture intricate relationships and interactions offered by the neural network. In our proposed models, we use well-known log-linear claim count regression models for the classical regression part and a multilayer perceptron (MLP) for the neural network part. The MLP part is used to process telematics car driving data given as a vector characterizing the driving behavior 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28155;&#21152;&#39044;&#35757;&#32451;&#27169;&#22359;&#65292;&#22312;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13771</link><description>&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#20013;&#30340;&#20934;&#30830;&#24615;&#22686;&#24378;&#65306;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach. (arXiv:2307.13771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28155;&#21152;&#39044;&#35757;&#32451;&#27169;&#22359;&#65292;&#22312;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#20405;&#29359;&#20010;&#20154;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21487;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20445;&#30041;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22359;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#20844;&#24320;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#38598;&#21644;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#28155;&#21152;&#39044;&#35757;&#32451;&#27169;&#22359;&#26174;&#33879;&#25552;&#39640;&#20102;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models can memorize training datasets. As a result, training ML models over private datasets can violate the privacy of individuals. Differential privacy (DP) is a rigorous privacy notion to preserve the privacy of underlying training datasets in ML models. Yet, training ML models in a DP framework usually degrades the accuracy of ML models. This paper aims to boost the accuracy of a DP-ML model, specifically a logistic regression model, via a pre-training module. In more detail, we initially pre-train our model on a public training dataset that there is no privacy concern about it. Then, we fine-tune our model via the DP logistic regression with the private dataset. In the numerical results, we show that adding a pre-training module significantly improves the accuracy of the DP logistic regression.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#25932;&#23545;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25932;&#23545;&#35757;&#32451;&#26694;&#26550;--&#37325;&#26032;&#24179;&#34913;&#25932;&#23545;&#35757;&#32451;&#65288;REAT&#65289;&#12290;REAT&#33021;&#22815;&#35299;&#20915;&#25932;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#26377;&#25928;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10205</link><description>&lt;p&gt;
&#38271;&#23614;&#20998;&#24067;&#19978;&#30340;&#25932;&#23545;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training Over Long-Tailed Distribution. (arXiv:2307.10205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#25932;&#23545;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25932;&#23545;&#35757;&#32451;&#26694;&#26550;--&#37325;&#26032;&#24179;&#34913;&#25932;&#23545;&#35757;&#32451;&#65288;REAT&#65289;&#12290;REAT&#33021;&#22815;&#35299;&#20915;&#25932;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#26377;&#25928;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26381;&#20174;&#38271;&#23614;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#25932;&#23545;&#35757;&#32451;&#65292;&#36825;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#19982;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#30340;&#20256;&#32479;&#25932;&#23545;&#35757;&#32451;&#30456;&#27604;&#65292;&#35813;&#36807;&#31243;&#38754;&#20020;&#30528;&#20135;&#29983;&#19981;&#24179;&#34913;&#30340;&#25932;&#23545;&#26679;&#26412;&#21644;&#19981;&#24179;&#34913;&#30340;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#30340;&#22256;&#22659;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#23614;&#37096;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20302;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25932;&#23545;&#35757;&#32451;&#26694;&#26550;--&#37325;&#26032;&#24179;&#34913;&#25932;&#23545;&#35757;&#32451;&#65288;REAT&#65289;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#19968;&#31181;&#21463;&#26377;&#25928;&#26679;&#26412;&#25968;&#21551;&#21457;&#30340;&#26032;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#26356;&#24179;&#34913;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25932;&#23545;&#26679;&#26412;&#65307;&#65288;2&#65289;&#19968;&#31181;&#31934;&#24515;&#26500;&#24314;&#30340;&#24809;&#32602;&#20989;&#25968;&#65292;&#29992;&#20110;&#24378;&#21046;&#28385;&#36275;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#65292;REAT&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://&#20013;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study adversarial training on datasets that obey the long-tailed distribution, which is practical but rarely explored in previous works. Compared with conventional adversarial training on balanced datasets, this process falls into the dilemma of generating uneven adversarial examples (AEs) and an unbalanced feature embedding space, causing the resulting model to exhibit low robustness and accuracy on tail data. To combat that, we propose a new adversarial training framework -- Re-balancing Adversarial Training (REAT). This framework consists of two components: (1) a new training strategy inspired by the term effective number to guide the model to generate more balanced and informative AEs; (2) a carefully constructed penalty function to force a satisfactory feature space. Evaluation results on different datasets and model structures prove that REAT can effectively enhance the model's robustness and preserve the model's clean accuracy. The code can be found in https://
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#23567;&#38543;&#26426;&#32534;&#30721;&#23398;&#20064;&#65288;MIRACLE&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;Mean-KL&#65292;&#22312;&#21387;&#32553;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07816</link><description>&lt;p&gt;
&#24102;&#26377;Mean-KL&#21442;&#25968;&#21270;&#30340;&#26368;&#23567;&#38543;&#26426;&#32534;&#30721;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Minimal Random Code Learning with Mean-KL Parameterization. (arXiv:2307.07816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#23567;&#38543;&#26426;&#32534;&#30721;&#23398;&#20064;&#65288;MIRACLE&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;Mean-KL&#65292;&#22312;&#21387;&#32553;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#23567;&#38543;&#26426;&#32534;&#30721;&#23398;&#20064;&#65288;MIRACLE&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#22312;&#21387;&#32553;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#24615;&#34892;&#20026;&#21644;&#40065;&#26834;&#24615;&#12290;MIRACLE&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#26465;&#20214;&#39640;&#26031;&#21464;&#20998;&#36817;&#20284;&#26435;&#37325;&#21518;&#39564;$Q_{\mathbf{w}}$&#65292;&#24182;&#20351;&#29992;&#30456;&#23545;&#29109;&#32534;&#30721;&#26469;&#21387;&#32553;&#20174;&#21518;&#39564;&#20013;&#25277;&#26679;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#39640;&#26031;&#32534;&#30721;&#20998;&#24067;$P_{\mathbf{w}}$&#12290;&#20026;&#20102;&#36798;&#21040;&#25152;&#38656;&#30340;&#21387;&#32553;&#29575;&#65292;&#24517;&#39035;&#23545;$Q_{\mathbf{w}} \Vert P_{\mathbf{w}}$&#36827;&#34892;&#32422;&#26463;&#65292;&#36825;&#38656;&#35201;&#22312;&#20256;&#32479;&#30340;&#22343;&#20540;-&#26041;&#24046;&#65288;Mean-Var&#65289;&#21442;&#25968;&#21270;&#19979;&#36827;&#34892;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#36864;&#28779;&#36807;&#31243;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#20854;&#24179;&#22343;&#20540;&#21644;KL&#25955;&#24230;&#26469;&#21442;&#25968;&#21270;$Q_{\mathbf{w}}$&#65292;&#20197;&#36890;&#36807;&#26500;&#36896;&#23558;&#21387;&#32553;&#25104;&#26412;&#32422;&#26463;&#20026;&#25152;&#38656;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;Mean-KL&#21442;&#25968;&#21270;&#30340;&#21464;&#20998;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#26159;&#20256;&#32479;&#26041;&#27861;&#30340;&#20004;&#20493;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#21518;&#20445;&#25345;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the qualitative behavior and robustness of two variants of Minimal Random Code Learning (MIRACLE) used to compress variational Bayesian neural networks. MIRACLE implements a powerful, conditionally Gaussian variational approximation for the weight posterior $Q_{\mathbf{w}}$ and uses relative entropy coding to compress a weight sample from the posterior using a Gaussian coding distribution $P_{\mathbf{w}}$. To achieve the desired compression rate, $D_{\mathrm{KL}}[Q_{\mathbf{w}} \Vert P_{\mathbf{w}}]$ must be constrained, which requires a computationally expensive annealing procedure under the conventional mean-variance (Mean-Var) parameterization for $Q_{\mathbf{w}}$. Instead, we parameterize $Q_{\mathbf{w}}$ by its mean and KL divergence from $P_{\mathbf{w}}$ to constrain the compression cost to the desired value by construction. We demonstrate that variational training with Mean-KL parameterization converges twice as fast and maintains predictive performance after 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20174;&#21491;&#21040;&#24038;&#21644;&#22836;&#21040;&#33050;&#30340;&#24515;&#25615;&#22270;&#25104;&#20998;&#20013;&#39044;&#27979;&#32972;&#33145;&#26041;&#21521;&#30340;&#20449;&#21495;&#12290;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#22343;&#26041;&#35823;&#24046;&#20026;0.09&#12290;</title><link>http://arxiv.org/abs/2307.07566</link><description>&lt;p&gt;
&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20174;&#21491;&#21040;&#24038;&#21644;&#22836;&#21040;&#33050;&#30340;&#25104;&#20998;&#37325;&#24314;&#19977;&#36724;&#24515;&#25615;&#22270;
&lt;/p&gt;
&lt;p&gt;
Reconstruction of 3-Axis Seismocardiogram from Right-to-left and Head-to-foot Components Using A Long Short-Term Memory Network. (arXiv:2307.07566v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20174;&#21491;&#21040;&#24038;&#21644;&#22836;&#21040;&#33050;&#30340;&#24515;&#25615;&#22270;&#25104;&#20998;&#20013;&#39044;&#27979;&#32972;&#33145;&#26041;&#21521;&#30340;&#20449;&#21495;&#12290;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#22343;&#26041;&#35823;&#24046;&#20026;0.09&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20174;&#21491;&#21040;&#24038;&#21644;&#22836;&#21040;&#33050;&#26041;&#21521;&#30340;&#24515;&#25615;&#22270;&#65288;SCG&#65289;&#20449;&#21495;&#22312;&#32972;&#33145;&#26041;&#21521;&#30340;&#20449;&#21495;&#65288;SCG_x&#21644;SCG_y&#65289;&#12290;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#26469;&#33258;15&#21517;&#20581;&#24247;&#25104;&#24180;&#20154;&#12290;&#20351;&#29992;&#19977;&#36724;&#21152;&#36895;&#24230;&#35745;&#23558;SCG&#20449;&#21495;&#35760;&#24405;&#22312;&#27599;&#20010;&#34987;&#35797;&#30340;&#33016;&#37096;&#19978;&#12290;&#28982;&#21518;&#20351;&#29992;&#24515;&#30005;&#22270;R&#27874;&#36827;&#34892;&#20998;&#27573;&#65292;&#23558;&#29255;&#27573;&#36827;&#34892;&#38477;&#37319;&#26679;&#12289;&#24402;&#19968;&#21270;&#21644;&#38646;&#23621;&#20013;&#22788;&#29702;&#12290;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#35757;&#32451;&#21644;&#39564;&#35777;&#19968;&#20010;&#20855;&#26377;&#20004;&#23618;&#21644;&#19968;&#20010;&#36991;&#20813;&#36807;&#25311;&#21512;&#30340;dropout&#23618;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#12290;&#32593;&#32476;&#20197;100&#20010;&#26102;&#38388;&#27493;&#38271;&#30340;SCG_x&#21644;SCG_y&#20316;&#20026;&#36755;&#20837;&#65292;&#34920;&#31034;&#19968;&#20010;&#24515;&#33039;&#21608;&#26399;&#65292;&#36755;&#20986;&#26144;&#23556;&#21040;&#30446;&#26631;&#21464;&#37327;&#30340;&#21521;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LSTM&#27169;&#22411;&#30340;&#22343;&#26041;&#35823;&#24046;&#20026;0.09&#12290;
&lt;/p&gt;
&lt;p&gt;
This pilot study aims to develop a deep learning model for predicting seismocardiogram (SCG) signals in the dorsoventral direction from the SCG signals in the right-to-left and head-to-foot directions ($\textrm{SCG}_x$ and $\textrm{SCG}_y$). The dataset used for the training and validation of the model was obtained from 15 healthy adult subjects. The SCG signals were recorded using tri-axial accelerometers placed on the chest of each subject. The signals were then segmented using electrocardiogram R waves, and the segments were downsampled, normalized, and centered around zero. The resulting dataset was used to train and validate a long short-term memory (LSTM) network with two layers and a dropout layer to prevent overfitting. The network took as input 100-time steps of $\textrm{SCG}_x$ and $\textrm{SCG}_y$, representing one cardiac cycle, and outputted a vector that mapped to the target variable being predicted. The results showed that the LSTM model had a mean square error of 0.09 b
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.03288</link><description>&lt;p&gt;
&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Scalarizations for Sublinear Hypervolume Regret. (arXiv:2307.03288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03288
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#65292;&#23558;&#22810;&#20010;&#30446;&#26631;&#20943;&#23569;&#20026;&#19968;&#20010;&#65292;&#20363;&#22914;&#26368;&#36817;&#22312;RLHF&#20013;&#29992;&#20110;&#35757;&#32451;&#26657;&#20934;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20154;&#23545;&#36825;&#31181;&#32463;&#20856;&#26041;&#27861;&#25345;&#21542;&#23450;&#24577;&#24230;&#65292;&#22240;&#20026;&#24050;&#30693;&#32447;&#24615;&#26631;&#37327;&#21270;&#20250;&#24573;&#30053;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#20985;&#21306;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#36890;&#36807;&#34987;&#25903;&#37197;&#30340;&#36229;&#20307;&#31215;&#26469;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#19978;&#30340;&#22810;&#26679;&#21270;&#30446;&#26631;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#20196;&#20154;&#24778;&#35766;&#22320;&#26159;&#20026;&#20102;&#35777;&#26126;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#32780;&#26368;&#20248;&#30340;&#65292;&#23454;&#29616;&#20102; $O(T^{-1/k})$ &#30340;&#26368;&#20248;&#20122;&#32447;&#24615;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#21305;&#37197;&#30340;&#19979;&#30028;&#34920;&#26126;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#27809;&#26377;&#20219;&#20309;&#31639;&#27861;&#33021;&#20570;&#24471;&#26356;&#22909;&#12290;&#20316;&#20026;&#19968;&#20010;&#29702;&#35770;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#36229;&#32447;&#24615;&#36951;&#25022;&#30028;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Scalarization is a general technique that can be deployed in any multiobjective setting to reduce multiple objectives into one, such as recently in RLHF for training reward models that align human preferences. Yet some have dismissed this classical approach because linear scalarizations are known to miss concave regions of the Pareto frontier. To that end, we aim to find simple non-linear scalarizations that can explore a diverse set of $k$ objectives on the Pareto frontier, as measured by the dominated hypervolume. We show that hypervolume scalarizations with uniformly random weights are surprisingly optimal for provably minimizing the hypervolume regret, achieving an optimal sublinear regret bound of $O(T^{-1/k})$, with matching lower bounds that preclude any algorithm from doing better asymptotically. As a theoretical case study, we consider the multiobjective stochastic linear bandits problem and demonstrate that by exploiting the sublinear regret bounds of the hypervolume scalariz
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#30446;&#26631;&#20197;&#21450;&#32447;&#24615;&#21644;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#20110;&#19981;&#21516;&#36924;&#36817;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02842</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#39640;&#25928;&#30340;&#36845;&#20195;CVaR&#24378;&#21270;&#23398;&#20064;&#19982;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation. (arXiv:2307.02842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02842
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#30446;&#26631;&#20197;&#21450;&#32447;&#24615;&#21644;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#20110;&#19981;&#21516;&#36924;&#36817;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20248;&#21270;&#24179;&#34913;&#26399;&#26395;&#22870;&#21169;&#21644;&#39118;&#38505;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#24418;&#24335;&#65292;&#37319;&#29992;&#36845;&#20195;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65288;CVaR&#65289;&#30446;&#26631;&#20197;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#12290;&#36825;&#31181;&#21517;&#20026;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;ICVaR-RL&#30340;&#26032;&#24418;&#24335;&#65292;&#20026;&#27599;&#20010;&#20915;&#31574;&#27493;&#39588;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#23433;&#20840;&#20445;&#35777;&#26041;&#24335;&#12290;&#23545;&#20110;&#37319;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;ICVaR-RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;ICVaR-L&#65292;&#35813;&#31639;&#27861;&#30340;&#21518;&#24724;&#24230;&#20026;$\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$&#65292;&#20854;&#20013;$\alpha$&#26159;&#39118;&#38505;&#27700;&#24179;&#65292;$d$&#26159;&#29366;&#24577;&#34892;&#21160;&#29305;&#24449;&#30340;&#32500;&#24230;&#65292;$H$&#26159;&#27599;&#20010;episode&#30340;&#38271;&#24230;&#65292;$K$&#26159;episode&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#30456;&#21305;&#37197;&#30340;&#19979;&#30028;$\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$&#65292;&#20197;&#39564;&#35777;ICVaR-L&#22312;$d$&#21644;$K$&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;&#23545;&#20110;&#37319;&#29992;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#30340;ICVaR-RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31639;&#27861;ICVaR-G&#65292;&#23427;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we investigate a novel risk-sensitive RL formulation with an Iterated Conditional Value-at-Risk (CVaR) objective under linear and general function approximations. This new formulation, named ICVaR-RL with function approximation, provides a principled way to guarantee safety at each decision step. For ICVaR-RL with linear function approximation, we propose a computationally efficient algorithm ICVaR-L, which achieves an $\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$ regret, where $\alpha$ is the risk level, $d$ is the dimension of state-action features, $H$ is the length of each episode, and $K$ is the number of episodes. We also establish a matching lower bound $\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$ to validate the optimality of ICVaR-L with respect to $d$ and $K$. For ICVaR-RL with general function approximation, we propose algorithm ICVaR-G, which achiev
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36830;&#25509;&#26234;&#33021;&#30340;&#33258;&#20027;&#36793;&#32536;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#20113;-&#36793;&#32536;-&#23458;&#25143;&#31471;&#30340;&#20998;&#23618;&#26550;&#26500;&#21644;&#24378;&#22823;&#30340;GPT&#27169;&#22411;&#33021;&#21147;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#12289;&#20302;&#24310;&#36831;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;AI&#26381;&#21153;&#65292;&#28385;&#36275;&#29992;&#25143;&#20010;&#20154;&#38656;&#27714;&#24182;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.02779</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36830;&#25509;&#26234;&#33021;&#30340;&#33258;&#20027;&#36793;&#32536;AI
&lt;/p&gt;
&lt;p&gt;
Large Language Models Empowered Autonomous Edge AI for Connected Intelligence. (arXiv:2307.02779v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02779
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36830;&#25509;&#26234;&#33021;&#30340;&#33258;&#20027;&#36793;&#32536;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#20113;-&#36793;&#32536;-&#23458;&#25143;&#31471;&#30340;&#20998;&#23618;&#26550;&#26500;&#21644;&#24378;&#22823;&#30340;GPT&#27169;&#22411;&#33021;&#21147;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#12289;&#20302;&#24310;&#36831;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;AI&#26381;&#21153;&#65292;&#28385;&#36275;&#29992;&#25143;&#20010;&#20154;&#38656;&#27714;&#24182;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#30340;&#21457;&#23637;&#26397;&#30528;&#36830;&#25509;&#26234;&#33021;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#36825;&#19968;&#27010;&#24565;&#35774;&#24819;&#20102;&#22312;&#36229;&#36830;&#25509;&#30340;&#32593;&#32476;&#29289;&#29702;&#19990;&#30028;&#20013;&#65292;&#20154;&#31867;&#12289;&#29289;&#20307;&#21644;&#26234;&#33021;&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#20114;&#32852;&#12290;&#36793;&#32536;AI&#20316;&#20026;&#23454;&#29616;&#36830;&#25509;&#26234;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#36793;&#32536;&#25552;&#20379;&#39640;&#36136;&#37327;&#12289;&#20302;&#24310;&#36831;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;AI&#26381;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#36793;&#32536;AI&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#32452;&#32455;&#12289;&#36866;&#24212;&#21644;&#20248;&#21270;&#33258;&#24049;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#21508;&#31181;&#38656;&#27714;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#20113;-&#36793;&#32536;-&#23458;&#25143;&#31471;&#30340;&#20998;&#23618;&#26550;&#26500;&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#23384;&#25918;&#22312;&#20113;&#31471;&#65292;&#20854;&#20182;AI&#27169;&#22411;&#34987;&#20849;&#21516;&#37096;&#32626;&#22312;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#12290;&#36890;&#36807;&#21033;&#29992;GPT&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#35268;&#21010;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#21327;&#35843;&#36793;&#32536;AI&#27169;&#22411;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#20010;&#20154;&#38656;&#27714;&#65292;&#21516;&#26102;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of wireless networks gravitates towards connected intelligence, a concept that envisions seamless interconnectivity among humans, objects, and intelligence in a hyper-connected cyber-physical world. Edge AI emerges as a promising solution to achieve connected intelligence by delivering high-quality, low-latency, and privacy-preserving AI services at the network edge. In this article, we introduce an autonomous edge AI system that automatically organizes, adapts, and optimizes itself to meet users' diverse requirements. The system employs a cloud-edge-client hierarchical architecture, where the large language model, i.e., Generative Pretrained Transformer (GPT), resides in the cloud, and other AI models are co-deployed on devices and edge servers. By leveraging the powerful abilities of GPT in language understanding, planning, and code generation, we present a versatile framework that efficiently coordinates edge AI models to cater to users' personal demands while automati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2307.02129</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#33324;&#39640;&#32500;&#20219;&#21153;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#19982;&#32500;&#24230;&#25104;&#25351;&#25968;&#22686;&#38271;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;&#21487;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;CNN&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#24314;&#31435;&#20102;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#38656;&#35201;&#22810;&#23569;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#36825;&#20010;&#25968;&#23383;&#22914;&#20309;&#21462;&#20915;&#20110;&#25968;&#25454;&#32467;&#26500;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#38024;&#23545;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#25429;&#25417;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#20851;&#26041;&#38754;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;$n_c$&#20010;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#23545;&#24212;&#20110;$m$&#20010;&#21516;&#20041;&#32452;&#21512;&#30340;&#39640;&#23618;&#27425;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#21448;&#36890;&#36807;&#19968;&#20010;&#37325;&#22797;$L$&#27425;&#30340;&#36845;&#20195;&#36807;&#31243;&#30001;&#23376;&#29305;&#24449;&#32452;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38656;&#35201;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;$P^*$&#65288;i&#65289;&#38543;&#30528;$n_c m^L$&#30340;&#22686;&#38271;&#32780;&#28176;&#36827;&#22320;&#22686;&#38271;&#65292;&#36825;&#21482;&#26377;...
&lt;/p&gt;
&lt;p&gt;
Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#27169;&#22411;&#31561;&#25928;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#27169;&#22411;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#27010;&#24565;&#24212;&#29992;&#20110;&#22686;&#24378;&#20219;&#20309;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01708</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#27169;&#22411;&#31561;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. (arXiv:2307.01708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#27169;&#22411;&#31561;&#25928;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#27169;&#22411;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#27010;&#24565;&#24212;&#29992;&#20110;&#22686;&#24378;&#20219;&#20309;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23398;&#20064;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36866;&#24403;&#30340;&#20215;&#20540;&#31561;&#20215;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#39118;&#38505;&#20013;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26368;&#20248;&#35268;&#21010;&#65292;&#20294;&#22312;&#39118;&#38505;&#25935;&#24863;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#36827;&#34892;&#26368;&#20248;&#35268;&#21010;&#12290;&#25105;&#20204;&#21033;&#29992;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#27169;&#22411;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#38024;&#23545;&#20219;&#20309;&#39118;&#38505;&#24230;&#37327;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#26159;&#35745;&#31639;&#22797;&#26434;&#65307;&#21478;&#19968;&#20010;&#26159;&#23454;&#38469;&#30340;&#21464;&#20307;&#65292;&#20801;&#35768;&#36873;&#25321;&#21487;&#20197;&#36827;&#34892;&#26368;&#20248;&#35268;&#21010;&#30340;&#39118;&#38505;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;&#20219;&#20309;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#34920;&#26684;&#21644;&#22823;&#35268;&#27169;&#23454;&#39564;&#26469;&#23637;&#31034;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.
&lt;/p&gt;</description></item><item><title>RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17100</link><description>&lt;p&gt;
RL4CO: &#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark. (arXiv:2306.17100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17100
&lt;/p&gt;
&lt;p&gt;
RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;RL4CO&#65292;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;RL4CO&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#24211;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#22914;&#27169;&#22359;&#21270;&#21644;&#37197;&#32622;&#31649;&#29702;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#29615;&#22659;&#21644;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65289;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24378;&#35843;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#21508;&#31181;&#20248;&#21270;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;&#27169;&#22411;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#38646;-shot&#27867;&#21270;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#20351;&#29992;&#36825;&#20123;&#26032;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#26102;&#33853;&#21518;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#26356;&#21152;&#24179;&#34913;&#22320;&#35780;&#20272;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;RL4CO&#33021;&#22815;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse optimization tasks. We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these new metrics, suggesting the necessity for a more balanced view of the performance of neural CO solvers. We hope RL4CO will 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20811;&#26381;&#22270;&#23398;&#20064;&#35774;&#32622;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#21487;&#23398;&#20064;&#22270;&#25193;&#25955;&#30340;&#28789;&#27963;GNNs&#20197;&#21450;&#38024;&#23545;&#32467;&#26500;&#25200;&#21160;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23545;&#25239;&#35757;&#32451;&#26159;&#38024;&#23545;&#23545;&#25239;&#32467;&#26500;&#25200;&#21160;&#30340;&#26368;&#20808;&#36827;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15427</link><description>&lt;p&gt;
&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training for Graph Neural Networks. (arXiv:2306.15427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20811;&#26381;&#22270;&#23398;&#20064;&#35774;&#32622;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#21487;&#23398;&#20064;&#22270;&#25193;&#25955;&#30340;&#28789;&#27963;GNNs&#20197;&#21450;&#38024;&#23545;&#32467;&#26500;&#25200;&#21160;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23545;&#25239;&#35757;&#32451;&#26159;&#38024;&#23545;&#23545;&#25239;&#32467;&#26500;&#25200;&#21160;&#30340;&#26368;&#20808;&#36827;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22270;&#20687;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#25239;&#35757;&#32451;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23545;&#25239;&#22270;&#32467;&#26500;&#25200;&#21160;&#26041;&#38754;&#24182;&#27809;&#26377;&#26126;&#26174;&#25928;&#26524;&#12290;&#22312;&#20462;&#22797;&#23545;&#25239;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#20811;&#26381;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#37319;&#29992;&#30340;&#22270;&#23398;&#20064;&#35774;&#32622;&#30340;&#22522;&#26412;&#29702;&#35770;&#21644;&#23454;&#38469;&#38480;&#21046;&#65307;&#25105;&#20204;&#25581;&#31034;&#20102;&#22522;&#20110;&#21487;&#23398;&#20064;&#22270;&#25193;&#25955;&#30340;&#26356;&#28789;&#27963; GNNs &#33021;&#22815;&#36866;&#24212;&#23545;&#25239;&#25200;&#21160;&#65292;&#21516;&#26102;&#23398;&#20064;&#21040;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#20855;&#26377;&#33258;&#28982;&#30340;&#21487;&#35299;&#37322;&#24615;&#65307;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31532;&#19968;&#31181;&#38024;&#23545;&#32467;&#26500;&#25200;&#21160;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23545;&#22810;&#20010;&#33410;&#28857;&#36827;&#34892;&#25915;&#20987;&#65292;&#24182;&#33021;&#22788;&#29702;&#20840;&#23616;&#65288;&#22270;&#32423;&#21035;&#65289;&#21644;&#23616;&#37096;&#65288;&#33410;&#28857;&#32423;&#21035;&#65289;&#30340;&#32422;&#26463;&#12290;&#36890;&#36807;&#36825;&#20123;&#36129;&#29486;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#25239;&#35757;&#32451;&#26159;&#23545;&#25239;&#32467;&#26500;&#25200;&#21160;&#30340;&#26368;&#20808;&#36827;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its success in the image domain, adversarial training does not (yet) stand out as an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations. In the pursuit of fixing adversarial training (1) we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) we reveal that more flexible GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable; (3) we introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) constraints. Including these contributions, we demonstrate that adversarial training is a state-of-the-art defense against adversarial structure perturbations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26435;&#34913;&#20102;&#20215;&#26684;&#21644;&#25968;&#25454;&#36136;&#37327;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#20351;&#24471;&#23545;&#20110;&#28304;&#30340;&#36873;&#25321;&#65292;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#20844;&#24179;&#27010;&#24565;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#20195;&#20215;&#21487;&#20197;&#20943;&#23569;&#20844;&#24179;&#24809;&#32602;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#31639;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.13440</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#36136;&#37327;&#21644;&#20844;&#24179;&#20043;&#38388;&#26435;&#34913;&#20215;&#26684;&#65292;&#23454;&#29616;&#22312;&#32447;&#20998;&#37197;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Trading-off price for data quality to achieve fair online allocation. (arXiv:2306.13440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26435;&#34913;&#20102;&#20215;&#26684;&#21644;&#25968;&#25454;&#36136;&#37327;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#20351;&#24471;&#23545;&#20110;&#28304;&#30340;&#36873;&#25321;&#65292;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#20844;&#24179;&#27010;&#24565;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#20195;&#20215;&#21487;&#20197;&#20943;&#23569;&#20844;&#24179;&#24809;&#32602;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#31639;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#38271;&#26399;&#30340;&#20844;&#24179;&#24809;&#32602;&#12290;&#20294;&#19982;&#29616;&#26377;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#20915;&#31574;&#32773;&#21487;&#20197;&#35266;&#23519;&#21040;&#21463;&#20445;&#25252;&#30340;&#23646;&#24615;&#8212;&#8212;&#36825;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#30456;&#21453;&#65292;&#20182;&#20204;&#21487;&#20197;&#36141;&#20080;&#26469;&#33258;&#19981;&#21516;&#36136;&#37327;&#26469;&#28304;&#30340;&#25968;&#25454;&#20197;&#24110;&#21161;&#20272;&#35745;&#23427;&#20204;&#65292;&#20174;&#32780;&#20197;&#19968;&#23450;&#30340;&#20195;&#20215;&#20943;&#23569;&#20844;&#24179;&#24809;&#32602;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#33218;&#23545;&#24212;&#20110;&#25968;&#25454;&#28304;&#30340;&#36873;&#25321;&#65292;&#21516;&#26102;&#21253;&#21547;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#23427;&#20855;&#26377; $\mathcal{O}(\sqrt{T})$ &#30340;&#36951;&#25022;&#30028;&#12290;&#19968;&#20010;&#20851;&#38190;&#22256;&#38590;&#26159;&#36873;&#25321;&#28304;&#25152;&#33719;&#24471;&#30340;&#22238;&#25253;&#30001;&#20844;&#24179;&#24809;&#32602;&#30456;&#20851;&#65292;&#36825;&#23548;&#33268;&#23613;&#31649;&#26159;&#38543;&#26426;&#35774;&#32622;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#38543;&#26426;&#21270;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32771;&#34385;&#21040;&#28304;&#36873;&#25321;&#20043;&#21069;&#21487;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#36866;&#24212;&#35768;&#22810;&#19981;&#21516;&#30340;&#20844;&#24179;&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23454;&#39564;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of online allocation subject to a long-term fairness penalty. Contrary to existing works, however, we do not assume that the decision-maker observes the protected attributes -- which is often unrealistic in practice. Instead they can purchase data that help estimate them from sources of different quality; and hence reduce the fairness penalty at some cost. We model this problem as a multi-armed bandit problem where each arm corresponds to the choice of a data source, coupled with the online allocation problem. We propose an algorithm that jointly solves both problems and show that it has a regret bounded by $\mathcal{O}(\sqrt{T})$. A key difficulty is that the rewards received by selecting a source are correlated by the fairness penalty, which leads to a need for randomization (despite a stochastic setting). Our algorithm takes into account contextual information available before the source selection, and can adapt to many different fairness notions. We also sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65288;DLN&#65289;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;&#30340;&#35821;&#35328;&#27169;&#22411;&#23618;&#65288;LLMs&#65289;&#65292;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#35757;&#32451;&#65292;&#20351;&#24471;DLN-2&#30340;&#24615;&#33021;&#29978;&#33267;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.12509</link><description>&lt;p&gt;
&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65306;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;LLM&#30340;&#25552;&#31034;&#23618;
&lt;/p&gt;
&lt;p&gt;
Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference. (arXiv:2306.12509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#65288;DLN&#65289;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21472;&#21152;&#30340;&#35821;&#35328;&#27169;&#22411;&#23618;&#65288;LLMs&#65289;&#65292;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#35757;&#32451;&#65292;&#20351;&#24471;DLN-2&#30340;&#24615;&#33021;&#29978;&#33267;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35270;&#20026;&#32593;&#32476;&#20013;&#30340;&#38543;&#26426;&#8220;&#35821;&#35328;&#23618;&#8221;&#65292;&#20854;&#20013;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#26159;&#27599;&#20010;&#23618;&#30340;&#33258;&#28982;&#35821;&#35328;&#8220;&#25552;&#31034;&#8221;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#36825;&#26679;&#30340;&#23618;&#21472;&#21152;&#22312;&#19968;&#36215;&#65292;&#23558;&#19968;&#20010;&#23618;&#30340;&#36755;&#20986;&#39304;&#36865;&#21040;&#19979;&#19968;&#20010;&#23618;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#22534;&#21472;&#30340;&#32467;&#26500;&#31216;&#20026;&#8220;&#28145;&#24230;&#35821;&#35328;&#32593;&#32476;&#8221;&#65288;DLN&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#26377;&#25928;&#22320;&#38024;&#23545;&#21333;&#23618;&#35821;&#35328;&#32593;&#32476;&#65288;DLN-1&#65289;&#25191;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#35757;&#32451;2&#23618;DLNs&#65288;DLN-2&#65289;&#65292;&#20854;&#20013;&#24517;&#39035;&#23398;&#20064;&#20004;&#20010;&#25552;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#31532;&#19968;&#23618;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#65292;&#38656;&#35201;&#36827;&#34892;&#36793;&#32536;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32852;&#21512;&#25552;&#31034;&#35757;&#32451;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#12290;DLN-2&#27604;&#21333;&#23618;&#36798;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#21363;&#20351;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;LLM&#26356;&#23567;&#19988;&#26356;&#24369;&#65292;&#20063;&#21487;&#20197;&#19982;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;GPT-4&#30456;&#23218;&#32654;&#12290;DLN&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65306;https://github.com/microsoft/deep-language-networks&#12290;
&lt;/p&gt;
&lt;p&gt;
We view large language models (LLMs) as stochastic \emph{language layers} in a network, where the learnable parameters are the natural language \emph{prompts} at each layer. We stack two such layers, feeding the output of one layer to the next. We call the stacked architecture a \emph{Deep Language Network} (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs (DLN-2), where two prompts must be learnt. We consider the output of the first layer as a latent variable to marginalize, and devise a variational inference algorithm for joint prompt training. A DLN-2 reaches higher performance than a single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the network is smaller and less powerful. The DLN code is open source: https://github.com/microsoft/deep-language-networks .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#21367;&#31215;&#31867;&#22411;&#21644;&#26550;&#26500;&#25913;&#36827;&#65292;&#25193;&#23637;&#20102;&#31561;&#21464;Transformer&#21040;&#26356;&#39640;&#30340;&#31561;&#21464;&#34920;&#31034;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#37327;&#21644;&#21147;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#35745;&#31639;&#25928;&#29575;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.12059</link><description>&lt;p&gt;
EquiformerV2: &#25913;&#36827;&#30340;&#31561;&#21464;Transformer&#65292;&#29992;&#20110;&#25193;&#23637;&#21040;&#26356;&#39640;&#27425;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#21367;&#31215;&#31867;&#22411;&#21644;&#26550;&#26500;&#25913;&#36827;&#65292;&#25193;&#23637;&#20102;&#31561;&#21464;Transformer&#21040;&#26356;&#39640;&#30340;&#31561;&#21464;&#34920;&#31034;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#37327;&#21644;&#21147;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#35745;&#31639;&#25928;&#29575;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;Transformer&#65288;&#20363;&#22914;Equiformer&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;3D&#21407;&#23376;&#31995;&#32479;&#39046;&#22495;&#30340;&#21151;&#25928;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#20173;&#28982;&#23616;&#38480;&#20110;&#23567;&#25968;&#27425;&#31561;&#21464;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#26550;&#26500;&#26159;&#21542;&#33021;&#22815;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#26356;&#39640;&#30340;&#27425;&#25968;&#12290;&#20174;Equiformer&#24320;&#22987;&#65292;&#25105;&#20204;&#39318;&#20808;&#29992;eSCN&#21367;&#31215;&#26367;&#25442;&#20102;$SO(3)$&#21367;&#31215;&#65292;&#20197;&#26377;&#25928;&#22320;&#21512;&#24182;&#26356;&#39640;&#27425;&#30340;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#26356;&#39640;&#27425;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26550;&#26500;&#25913;&#36827;&#8212;&#8212;&#27880;&#24847;&#21147;&#37325;&#26631;&#20934;&#21270;&#12289;&#21487;&#20998;&#31163;&#30340;$S^2$&#28608;&#27963;&#21644;&#21487;&#20998;&#31163;&#23618;&#24402;&#19968;&#21270;&#12290;&#23558;&#36825;&#19968;&#20999;&#25918;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#22312;&#22823;&#22411;OC20&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#21147;&#19978;&#25552;&#39640;&#20102;&#26368;&#22810;$12\%$&#65292;&#33021;&#37327;&#19978;&#25552;&#39640;&#20102;$4\%$&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#36895;&#24230;-&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#21560;&#38468;&#33021;&#25152;&#38656;&#30340;DFT&#35745;&#31639;&#37327;&#26041;&#38754;&#32553;&#20943;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements -- attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to $12\%$ on forces, $4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$ reduction in DFT calculations needed for computing adsorption energies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;sketching&#31639;&#27861;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#22914;&#20309;&#26681;&#25454;&#24213;&#23618;&#38382;&#39064;&#30340;&#38590;&#24230;&#35774;&#32622;sketch size&#12290;</title><link>http://arxiv.org/abs/2306.09396</link><description>&lt;p&gt;
&#31169;&#26377;&#32852;&#37030;&#39057;&#29575;&#20272;&#35745;&#65306;&#36866;&#24212;&#23454;&#20363;&#38590;&#24230;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Private Federated Frequency Estimation: Adapting to the Hardness of the Instance. (arXiv:2306.09396v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;sketching&#31639;&#27861;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#22914;&#20309;&#26681;&#25454;&#24213;&#23618;&#38382;&#39064;&#30340;&#38590;&#24230;&#35774;&#32622;sketch size&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#39057;&#29575;&#20272;&#35745;&#65288;FFE&#65289;&#20013;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#36981;&#23432;Secure Summation(SecSum)&#38544;&#31169;&#32422;&#26463;&#30340;&#26381;&#21153;&#22120;&#21327;&#35758;&#36890;&#20449;&#65292;&#20849;&#21516;&#20272;&#35745;&#20854;&#25968;&#25454;&#30340;&#39057;&#29575;&#12290;&#23545;&#20110;&#21333;&#36718;FFE&#65292;&#30740;&#31350;&#32773;&#24050;&#30693;&#20351;&#29992;count sketching&#31639;&#27861;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#22522;&#26412;&#30340;&#31934;&#24230;-&#36890;&#20449;&#22797;&#26434;&#24230;&#24179;&#34913;&#12290;&#20294;&#26159;&#22312;&#26356;&#23454;&#38469;&#30340;&#22810;&#36718;FEE&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;count sketching&#31639;&#27861;&#26159;&#20005;&#26684;&#27425;&#20248;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;sketching&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#26356;&#31934;&#30830;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#20026;&#20102;&#36866;&#24212;&#24213;&#23618;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#20174;&#19994;&#32773;&#24212;&#35813;&#22914;&#20309;&#35774;&#32622;sketch size&#65311; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated frequency estimation (FFE), multiple clients work together to estimate the frequencies of their collective data by communicating with a server that respects the privacy constraints of Secure Summation (SecSum), a cryptographic multi-party computation protocol that ensures that the server can only access the sum of client-held vectors. For single-round FFE, it is known that count sketching is nearly information-theoretically optimal for achieving the fundamental accuracy-communication trade-offs [Chen et al., 2022]. However, we show that under the more practical multi-round FEE setting, simple adaptations of count sketching are strictly sub-optimal, and we propose a novel hybrid sketching algorithm that is provably more accurate. We also address the following fundamental question: how should a practitioner set the sketch size in a way that adapts to the hardness of the underlying problem? We propose a two-phase approach that allows for the use of a smaller sketch size for s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CASSLE&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#21033;&#29992;&#22686;&#24378;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06082</link><description>&lt;p&gt;
&#22686;&#24378;&#24863;&#30693;&#30340;&#26377;&#21521;&#25237;&#24433;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmentation-aware Self-supervised Learning with Guided Projector. (arXiv:2306.06082v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CASSLE&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#21033;&#29992;&#22686;&#24378;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#20581;&#22766;&#34920;&#31034;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;SimCLR&#21644;MoCo&#31561;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#23545;&#24212;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#20445;&#25345;&#19981;&#21464;&#65292;&#33021;&#22815;&#36798;&#21040;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19981;&#21464;&#24615;&#21487;&#33021;&#23545;&#35299;&#20915;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#26377;&#23475;&#65292;&#36825;&#20123;&#20219;&#21153;&#20381;&#36182;&#20110;&#21463;&#21040;&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#22686;&#24378;&#24433;&#21709;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#39068;&#33394;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#26550;&#26500;&#30340;&#24120;&#35265;&#32452;&#20214;&#20043;&#19968;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#26469;&#20419;&#36827;&#34920;&#31034;&#31354;&#38388;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#25237;&#24433;&#22120;&#34917;&#20805;&#26377;&#20851;&#24212;&#29992;&#20110;&#22270;&#20687;&#30340;&#22686;&#24378;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35753;&#25237;&#24433;&#22120;&#22312;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26102;&#21033;&#29992;&#36825;&#31181;&#36741;&#21161;&#25351;&#23548;&#65292;&#29305;&#24449;&#25552;&#21462;&#22120;&#23398;&#20064;&#22312;&#20854;&#34920;&#31034;&#20013;&#20445;&#30041;&#22686;&#24378;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#26377;&#21521;&#25237;&#24433;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;CASSLE&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a powerful technique for learning robust representations from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo are able to reach quality on par with supervised approaches. However, this invariance may be harmful to solving some downstream tasks which depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. In order for the projector to take advantage of this auxiliary guidance when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Selfsupervised Learning (CASSLE), is d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#12290;&#37319;&#26679;&#22120;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.04848</link><description>&lt;p&gt;
&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21644;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#12290;&#37319;&#26679;&#22120;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#30452;&#35273;&#19978;&#19982;&#25237;&#24433;&#26377;&#20851;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#27969;&#24418;&#20551;&#35774;&#19979;&#65292;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#36817;&#20284;&#31561;&#20215;&#20110;&#27491;&#20132;&#25200;&#21160;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#21435;&#22122;&#36817;&#20284;&#20110;&#23398;&#20064;&#25237;&#24433;&#12290;&#26412;&#25991;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#23558;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#35299;&#37322;&#20026;&#24212;&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#30340;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#21435;&#22122;&#22120;&#25237;&#24433;&#35823;&#24046;&#30340;&#31616;&#21333;&#20551;&#35774;&#65292;&#25552;&#20379;DDIM&#65288;Denoising Diffusion Implicit Models&#65289;&#37319;&#26679;&#22120;&#30340;&#31616;&#21333;&#25910;&#25947;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#29702;&#35770;&#32467;&#26524;&#30340;&#27934;&#35265;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;DDIM&#30340;&#20004;&#20010;&#31616;&#21333;&#20462;&#25913;&#30340;&#26032;&#37319;&#26679;&#22120;&#12290;&#20165;&#38656;&#35201;5-10&#20010;&#20989;&#25968;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#37319;&#26679;&#22120;&#23601;&#33021;&#22312;&#39044;&#35757;&#32451;&#30340;CIFAR-10&#21644;CelebA&#27169;&#22411;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to reinterpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection-error of the denoiser. Finally, we propose a new sampler based on two simple modifications to DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#32500;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#23454;&#29616;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#37327;&#29289;&#20307;&#30340;&#22330;&#26223;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#29289;&#20307;&#25968;&#37327;&#30340;&#19978;&#30028;&#38480;&#21046;&#65292;&#36890;&#36807;&#38480;&#21046;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21322;&#30495;&#23454;&#25968;&#25454;&#38598;(Messy Rooms)&#12290;</title><link>http://arxiv.org/abs/2306.04633</link><description>&lt;p&gt;
&#23545;&#27604;&#25552;&#21319;&#65306;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#23454;&#29616;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion. (arXiv:2306.04633v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#32500;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#23454;&#29616;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#37327;&#29289;&#20307;&#30340;&#22330;&#26223;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#29289;&#20307;&#25968;&#37327;&#30340;&#19978;&#30028;&#38480;&#21046;&#65292;&#36890;&#36807;&#38480;&#21046;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21322;&#30495;&#23454;&#25968;&#25454;&#38598;(Messy Rooms)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#27880;&#37322;&#30340;&#19977;&#32500;&#25968;&#25454;&#38598;&#65292;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20108;&#32500;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#26377;&#25928;&#35299;&#20915;&#35813;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#22330;&#34920;&#31034;&#23558;2D&#20998;&#27573;&#21521;&#19978;&#25552;&#21319;&#21040;3D&#65292;&#24182;&#23558;&#23427;&#20204;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#40723;&#21169;&#36328;&#24103;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#12289;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29289;&#20307;&#30340;&#22330;&#26223;&#30340;&#24930;-&#24555;&#32858;&#31867;&#30446;&#26631;&#20989;&#25968;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#29289;&#20307;&#25968;&#37327;&#25110;&#36328;&#24103;&#29289;&#20307;&#36319;&#36394;&#36827;&#34892;&#35774;&#32622;&#19978;&#30028;&#12290;&#20026;&#20102;&#23637;&#31034;&#24930;-&#24555;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Messy Rooms&#30340;&#26032;&#30340;&#21322;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22330;&#26223;&#20013;&#26368;&#22810;&#26377;500&#20010;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ScanNet&#12289;Hypersim&#21644;Replica&#25968;&#25454;&#38598;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a slow-fast clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as o
&lt;/p&gt;</description></item><item><title>ContriMix&#26159;&#19968;&#31181;&#26080;&#38656;&#26631;&#35782;&#21644;&#25163;&#24037;&#35843;&#20248;&#30340;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04527</link><description>&lt;p&gt;
ContriMix&#65306;&#26174;&#24494;&#38236;&#22270;&#20687;&#20998;&#26512;&#20013;&#22522;&#20110;&#26080;&#30417;&#30563;&#20869;&#23481;&#23646;&#24615;&#20998;&#31163;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis. (arXiv:2306.04527v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04527
&lt;/p&gt;
&lt;p&gt;
ContriMix&#26159;&#19968;&#31181;&#26080;&#38656;&#26631;&#35782;&#21644;&#25163;&#24037;&#35843;&#20248;&#30340;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32452;&#32455;&#23398;&#21644;&#33639;&#20809;&#25104;&#20687;&#31561;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ContriMix&#65292;&#23427;&#37319;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#24335;&#20998;&#31163;&#20986;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#29983;&#29289;&#23398;&#20869;&#23481;&#21644;&#25216;&#26415;&#21464;&#24322;&#65292;&#24182;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#25163;&#24037; fine-tuning &#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#32452;&#32455;&#23398;&#21644;&#33639;&#20809;&#25104;&#20687;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102; ContriMix &#30340;&#26377;&#25928;&#24615;&#65292;&#21462;&#24471;&#20102;&#22522;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization is critical for real-world applications of machine learning models to microscopy images, including histopathology and fluorescence imaging. Artifacts in histopathology arise through a complex combination of factors relating to tissue collection and laboratory processing, as well as factors intrinsic to patient samples. In fluorescence imaging, these artifacts stem from variations across experimental batches. The complexity and subtlety of these artifacts make the enumeration of data domains intractable. Therefore, augmentation-based methods of domain generalization that require domain identifiers and manual fine-tuning are inadequate in this setting. To overcome this challenge, we introduce ContriMix, a domain generalization technique that learns to generate synthetic images by disentangling and permuting the biological content ("content") and technical variations ("attributes") in microscopy images. ContriMix does not rely on domain identifiers or handcrafted aug
&lt;/p&gt;</description></item><item><title>&#8220;&#38181;&#27880;&#24847;&#21147;&#8221;&#26159;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#21452;&#26354;&#38181;&#30340;&#23618;&#27425;&#32467;&#26500;&#32852;&#31995;&#25968;&#25454;&#28857;&#65292;&#26126;&#30830;&#24314;&#27169;&#20102;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#32467;&#26500;&#23646;&#24615;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#32423;&#24615;&#33021;&#24182;&#23454;&#29616;&#20102;&#20248;&#20110;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#25928;&#26524;&#65292;&#32780;&#19988;&#21442;&#25968;&#37327;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2306.00392</link><description>&lt;p&gt;
&#38181;&#26426;&#21046;: &#23618;&#27425;&#24863;&#30693;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Coneheads: Hierarchy Aware Attention. (arXiv:2306.00392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00392
&lt;/p&gt;
&lt;p&gt;
&#8220;&#38181;&#27880;&#24847;&#21147;&#8221;&#26159;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#21452;&#26354;&#38181;&#30340;&#23618;&#27425;&#32467;&#26500;&#32852;&#31995;&#25968;&#25454;&#28857;&#65292;&#26126;&#30830;&#24314;&#27169;&#20102;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#32467;&#26500;&#23646;&#24615;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#32423;&#24615;&#33021;&#24182;&#23454;&#29616;&#20102;&#20248;&#20110;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#25928;&#26524;&#65292;&#32780;&#19988;&#21442;&#25968;&#37327;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#22914;transformers&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32593;&#32476;&#20005;&#37325;&#20381;&#36182;&#20110;&#28857;&#31215;&#27880;&#24847;&#21147;&#36816;&#31639;&#31526;&#65292;&#23427;&#36890;&#36807;&#21462;&#20004;&#20010;&#28857;&#30340;&#20869;&#31215;&#26469;&#35745;&#31639;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#20869;&#31215;&#19981;&#33021;&#26126;&#30830;&#22320;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#32467;&#26500;&#23646;&#24615;&#65288;&#22914;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38181;&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#22522;&#20110;&#21452;&#26354;&#38181;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#38181;&#27880;&#24847;&#21147;&#36890;&#36807;&#21452;&#26354;&#38181;&#23450;&#20041;&#30340;&#23618;&#27425;&#32467;&#26500;&#23558;&#20004;&#20010;&#28857;&#32852;&#31995;&#36215;&#26469;&#65292;&#30452;&#35266;&#22320;&#34913;&#37327;&#20102;&#20004;&#20010;&#28857;&#30340;&#20998;&#27495;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#24863;&#30693;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#38181;&#27880;&#24847;&#21147;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#20219;&#21153;&#32423;&#24615;&#33021;&#19978;&#20248;&#20110;&#28857;&#31215;&#27880;&#24847;&#21147;&#21644;&#20854;&#20182;&#22522;&#32447;&#31639;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26174;&#33879;&#36739;&#23569;&#30340;&#21442;&#25968;&#21305;&#37197;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention networks such as transformers have achieved state-of-the-art performance in many domains. These networks rely heavily on the dot product attention operator, which computes the similarity between two points by taking their inner product. However, the inner product does not explicitly model the complex structural properties of real world datasets, such as hierarchies between data points. To remedy this, we introduce cone attention, a drop-in replacement for dot product attention based on hyperbolic entailment cones. Cone attention associates two points by the depth of their lowest common ancestor in a hierarchy defined by hyperbolic cones, which intuitively measures the divergence of two points and gives a hierarchy aware similarity score. We test cone attention on a wide variety of models and tasks and show that it improves task-level performance over dot product attention and other baselines, and is able to match dot-product attention with significantly fewer parameters. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#26816;&#32034;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#26500;&#36896;&#24182;&#21033;&#29992;&#20266;&#24178;&#20928;&#26631;&#31614;&#65292;&#20197;&#20943;&#23567;&#22024;&#26434;&#26631;&#31614;&#23545;&#20998;&#31867;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.19518</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#31614;&#26816;&#32034;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#22024;&#26434;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels. (arXiv:2305.19518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#26816;&#32034;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#26500;&#36896;&#24182;&#21033;&#29992;&#20266;&#24178;&#20928;&#26631;&#31614;&#65292;&#20197;&#20943;&#23567;&#22024;&#26434;&#26631;&#31614;&#23545;&#20998;&#31867;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#20174;&#24102;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#35838;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#27604;&#36739;&#20005;&#26684;&#30340;&#20551;&#35774;&#65292;&#32780;&#19988;&#21463;&#38480;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#21028;&#26029;&#38543;&#26426;&#30340;&#26631;&#31614;&#12290;&#20316;&#32773;&#21033;&#29992;&#36825;&#19968;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26631;&#31614;&#26816;&#32034;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#26469;&#26377;&#25928;&#30340;&#26500;&#36896;&#21644;&#21033;&#29992;&#20266;&#24178;&#20928;&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23567;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy labels is an important and long-standing problem in machine learning for real applications. One of the main research lines focuses on learning a label corrector to purify potential noisy labels. However, these methods typically rely on strict assumptions and are limited to certain types of label noise. In this paper, we reformulate the label-noise problem from a generative-model perspective, $\textit{i.e.}$, labels are generated by gradually refining an initial random guess. This new perspective immediately enables existing powerful diffusion models to seamlessly learn the stochastic generative process. Once the generative uncertainty is modeled, we can perform classification inference using maximum likelihood estimation of labels. To mitigate the impact of noisy labels, we propose the $\textbf{L}$abel-$\textbf{R}$etrieval-$\textbf{A}$ugmented (LRA) diffusion model, which leverages neighbor consistency to effectively construct pseudo-clean labels for diffusion train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$K^2$-&#26641;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32039;&#20945;&#29983;&#25104;&#65292;&#24182;&#21516;&#26102;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#36890;&#36807;&#25552;&#20986;&#39034;&#24207;$K^2$-&#26641;&#34920;&#31034;&#21644;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19125</link><description>&lt;p&gt;
&#22522;&#20110;$K^2$-&#26641;&#30340;&#20998;&#32423;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Graph Generation with $K^2$-trees. (arXiv:2305.19125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$K^2$-&#26641;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32039;&#20945;&#29983;&#25104;&#65292;&#24182;&#21516;&#26102;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#36890;&#36807;&#25552;&#20986;&#39034;&#24207;$K^2$-&#26641;&#34920;&#31034;&#21644;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30446;&#26631;&#20998;&#24067;&#29983;&#25104;&#22270;&#26159;&#35768;&#22810;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#33647;&#29289;&#21457;&#29616;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#26080;&#25439;&#22270;&#21387;&#32553;&#30340;$K^2$-&#26641;&#34920;&#31034;&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;$K^2$-&#26641;&#33021;&#22815;&#22312;&#36827;&#34892;&#32039;&#20945;&#29983;&#25104;&#30340;&#21516;&#26102;&#65292;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;(1)&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#21098;&#26525;&#12289;&#25153;&#24179;&#21270;&#21644;&#35760;&#21495;&#21270;&#36807;&#31243;&#30340;&#39034;&#24207;K2&#26641;&#34920;&#31034;&#21644;(2)&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#19987;&#19994;&#26641;&#24418;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#26469;&#29983;&#25104;&#24207;&#21015;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#24120;&#35268;&#21644;&#20004;&#20010;&#20998;&#23376;&#22270;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#20197;&#35777;&#23454;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation which was originally designed for lossless graph compression. Our motivation stems from the ability of the $K^2$-trees to enable compact generation while concurrently capturing the inherent hierarchical structure of a graph. In addition, we make further contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.
&lt;/p&gt;</description></item><item><title>SVDinsTN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23436;&#20840;&#36830;&#25509;&#30340;&#24352;&#37327;&#32593;&#32476;&#20013;&#25554;&#20837;&#23545;&#35282;&#22240;&#23376;&#65292;&#21516;&#26102;&#35745;&#31639;&#24352;&#37327;&#26680;&#21644;&#23545;&#35282;&#22240;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#32039;&#20945;&#30340;TN&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#30456;&#27604;&#65292;SVDinsTN&#22312;&#36895;&#24230;&#26041;&#38754;&#21152;&#24555;&#20102;10&#21040;10^3&#20493;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.14912</link><description>&lt;p&gt;
SVDinsTN: &#19968;&#31181;&#38598;&#25104;&#30340;&#24352;&#37327;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#21450;&#26377;&#25928;&#30340;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SVDinsTN: An Integrated Method for Tensor Network Representation with Efficient Structure Search. (arXiv:2305.14912v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14912
&lt;/p&gt;
&lt;p&gt;
SVDinsTN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24352;&#37327;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23436;&#20840;&#36830;&#25509;&#30340;&#24352;&#37327;&#32593;&#32476;&#20013;&#25554;&#20837;&#23545;&#35282;&#22240;&#23376;&#65292;&#21516;&#26102;&#35745;&#31639;&#24352;&#37327;&#26680;&#21644;&#23545;&#35282;&#22240;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#32039;&#20945;&#30340;TN&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#30456;&#27604;&#65292;SVDinsTN&#22312;&#36895;&#24230;&#26041;&#38754;&#21152;&#24555;&#20102;10&#21040;10^3&#20493;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#34920;&#31034;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;TN-SS&#65289;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;&#26368;&#20248;&#32467;&#26500;&#20197;&#23454;&#29616;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#30001;&#20110;&#37325;&#22797;&#30340;&#32467;&#26500;&#35780;&#20272;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38598;&#25104;&#65288;&#21333;&#23618;&#65289;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SVDinsTN&#65292;&#28040;&#38500;&#20102;&#37325;&#22797;&#32321;&#29712;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#36890;&#36807;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;TN&#30340;&#27599;&#20010;&#36793;&#25554;&#20837;&#19968;&#20010;&#23545;&#35282;&#22240;&#23376;&#65292;&#25105;&#20204;&#21516;&#26102;&#35745;&#31639;TN&#26680;&#21644;&#23545;&#35282;&#22240;&#23376;&#65292;&#22240;&#23376;&#31232;&#30095;&#24615;&#25581;&#31034;&#20102;&#26368;&#32039;&#20945;&#30340;TN&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;TN-SS&#26041;&#27861;&#30456;&#27604;&#65292;SVDinsTN&#22312;&#36816;&#34892;&#26102;&#38388;&#19978;&#23454;&#29616;&#20102;&#32422;10&#21040;10^3&#20493;&#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#36739;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor network (TN) representation is a powerful technique for data analysis and machine learning. It practically involves a challenging TN structure search (TN-SS) problem, which aims to search for the optimal structure to achieve a compact representation. Existing TN-SS methods mainly adopt a bi-level optimization method that leads to excessive computational costs due to repeated structure evaluations. To address this issue, we propose an efficient integrated (single-level) method named SVD-inspired TN decomposition (SVDinsTN), eliminating the need for repeated tedious structure evaluation. By inserting a diagonal factor for each edge of the fully-connected TN, we calculate TN cores and diagonal factors simultaneously, with factor sparsity revealing the most compact TN structure. Experimental results on real-world data demonstrate that SVDinsTN achieves approximately $10\sim{}10^3$ times acceleration in runtime compared to the existing TN-SS methods while maintaining a comparable lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.14735</link><description>&lt;p&gt;
&#36793;&#32536;&#32858;&#28966;&#65306;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#25439;&#20154;&#32676;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#23545;&#36793;&#32536;&#31038;&#21306;&#24433;&#21709;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#30830;&#23450;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#38024;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#20260;&#23475;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20250;&#25513;&#30422;&#30001;&#20132;&#21449;&#23376;&#32676;&#25110;&#36328;&#20154;&#21475;&#32676;&#20307;&#20849;&#20139;&#30340;&#20260;&#23475;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#8220;&#36793;&#32536;&#8221;&#23450;&#20041;&#20026;&#20855;&#26377;&#36828;&#31163;&#8220;&#24120;&#24577;&#8221; &#30340;&#20154;&#21475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#24230;&#37327;&#38024;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#30340;&#20260;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#25351;&#25968;&#65288;GPDI&#65289;&#65292;&#20197;&#34913;&#37327;&#25968;&#25454;&#38598;&#32454;&#20998;&#20026;&#23376;&#32452;&#23545;&#38754;&#20020;&#22686;&#21152;&#30340;&#20260;&#23475;&#30340;&#35782;&#21035;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26816;&#27979;&#27602;&#24615;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#24322;&#24120;&#20540;&#30340;&#25991;&#26412;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#27602;&#24615;&#26816;&#39564;&#20013;&#27602;&#24615;&#26356;&#39640;&#65292;&#39640;&#36798;28&#65285;&#33267;86&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#21475;&#23398;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#22987;&#32456;&#36739;&#24046;&#65292;&#24322;&#24120;&#20540;&#21644;&#38750;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#38169;&#35823;&#24046;&#36317;&#39640;&#36798;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A standard method for measuring the impacts of AI on marginalized communities is to determine performance discrepancies between specified demographic groups. These approaches aim to address harms toward vulnerable groups, but they obscure harm patterns faced by intersectional subgroups or shared across demographic groups. We instead operationalize "the margins" as data points that are statistical outliers due to having demographic attributes distant from the "norm" and measure harms toward these outliers. We propose a Group-Based Performance Disparity Index (GPDI) that measures the extent to which a subdivision of a dataset into subgroups identifies those facing increased harms. We apply our approach to detecting disparities in toxicity detection and find that text targeting outliers is 28% to 86% more toxic for all types of toxicity examined. We also discover that model performance is consistently worse for demographic outliers, with disparities in error between outliers and non-outli
&lt;/p&gt;</description></item><item><title>SMT 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;&#65292;&#24341;&#20837;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;</title><link>http://arxiv.org/abs/2305.13998</link><description>&lt;p&gt;
SMT 2.0&#65306;&#19968;&#20010;&#20851;&#27880;&#23618;&#27425;&#21644;&#28151;&#21512;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes. (arXiv:2305.13998v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13998
&lt;/p&gt;
&lt;p&gt;
SMT 2.0&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20195;&#29702;&#27169;&#22411;&#24037;&#20855;&#21253;&#65292;&#24341;&#20837;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Surrogate Modeling Toolbox (SMT)&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#12289;&#37319;&#26679;&#25216;&#26415;&#21644;&#19968;&#22871;&#31034;&#20363;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SMT 2.0&#65292;&#36825;&#26159;SMT&#30340;&#19968;&#20010;&#37325;&#35201;&#26032;&#29256;&#26412;&#65292;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#21319;&#32423;&#21644;&#26032;&#21151;&#33021;&#12290;&#36825;&#20010;&#29256;&#26412;&#22686;&#21152;&#20102;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#20195;&#29702;&#27169;&#22411;&#21644;&#23618;&#27425;&#21464;&#37327;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#31867;&#22411;&#30340;&#21464;&#37327;&#22312;&#22810;&#20010;&#20195;&#29702;&#24314;&#27169;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;SMT 2.0&#36824;&#36890;&#36807;&#25193;&#23637;&#37319;&#26679;&#26041;&#27861;&#12289;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#35745;&#31639;Kriging&#30340;&#26041;&#24046;&#21644;&#26680;&#23548;&#25968;&#26469;&#25913;&#36827;&#20102;SMT&#12290;&#36825;&#20010;&#29256;&#26412;&#36824;&#21253;&#25324;&#20102;&#22788;&#29702;&#24102;&#22122;&#22768;&#21644;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#25968;&#25454;&#30340;&#26032;&#20989;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SMT 2.0&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#23618;&#27425;&#21644;&#28151;&#21512;&#36755;&#20837;&#30340;&#24320;&#28304;&#20195;&#29702;&#24211;&#12290;&#36825;&#20010;&#24320;&#28304;&#36719;&#20214;&#37319;&#29992;New BSD&#35768;&#21487;&#35777;&#36827;&#34892;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#26377;&#38480;&#24615;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30452;&#25509;&#24314;&#27169;&#36716;&#31227;&#26680;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23547;&#25214;&#22312;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#26368;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13289</link><description>&lt;p&gt;
&#23454;&#29616;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#26679;&#26412;&#22797;&#26434;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;DRO&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach. (arXiv:2305.13289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#26377;&#38480;&#24615;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30452;&#25509;&#24314;&#27169;&#36716;&#31227;&#26680;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23547;&#25214;&#22312;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#26368;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#20027;&#21160;&#25506;&#32034;&#12290;&#35813;&#38382;&#39064;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#20998;&#24067;&#36716;&#31227;&#12290;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#24754;&#35266;&#30340;&#24577;&#24230;&#23545;&#24453;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#24809;&#32602;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#29366;&#24577;-&#34892;&#20026;&#23545;&#30340;&#22870;&#21169;&#26469;&#20445;&#23432;&#20272;&#35745;&#20540;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#22522;&#20110;&#26041;&#27861;&#20063;&#21487;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#19988;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30452;&#25509;&#24314;&#27169;&#36716;&#31227;&#26680;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#32479;&#35745;&#21512;&#29702;&#30340;&#36716;&#31227;&#26680;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#12290;&#28982;&#21518;&#25105;&#20204;&#23547;&#25214;&#22312;&#35813;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#19978;&#26368;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#30340;&#38669;&#22827;&#19969;&#39118;&#26684;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36825;&#26679;&#30495;&#23454;&#30340;&#36716;&#31227;&#26680;&#20197;&#39640;&#27010;&#29575;&#20301;&#20110;&#35813;&#38598;&#21512;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#20102;&#23454;&#29616;$\epsilon$&#30340;&#27425;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mat&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning aims to learn from pre-collected datasets without active exploration. This problem faces significant challenges, including limited data availability and distributional shifts. Existing approaches adopt a pessimistic stance towards uncertainty by penalizing rewards of under-explored state-action pairs to estimate value functions conservatively. In this paper, we show that the distributionally robust optimization (DRO) based approach can also address these challenges and is minimax optimal. Specifically, we directly model the uncertainty in the transition kernel and construct an uncertainty set of statistically plausible transition kernels. We then find the policy that optimizes the worst-case performance over this uncertainty set. We first design a metric-based Hoeffding-style uncertainty set such that with high probability the true transition kernel is in this set. We prove that to achieve a sub-optimality gap of $\epsilon$, the sample complexity is $\mat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#37319;&#29992;&#20048;&#35266;&#31574;&#30053;&#35780;&#20272;&#23376;&#31243;&#24207;&#20197;&#40723;&#21169;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#32447;&#24615;MDP&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20855;&#26377;&#26368;&#20248;&#32500;&#24230;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.11032</link><description>&lt;p&gt;
&#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65306;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL. (arXiv:2305.11032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#37319;&#29992;&#20048;&#35266;&#31574;&#30053;&#35780;&#20272;&#23376;&#31243;&#24207;&#20197;&#40723;&#21169;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#32447;&#24615;MDP&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20855;&#26377;&#26368;&#20248;&#32500;&#24230;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#23545;&#20110;&#36817;&#26399;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#35777;&#25104;&#21151;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#31574;&#30053;&#20248;&#21270;&#30340;&#29616;&#26377;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#30456;&#24403;&#26377;&#38480; - &#23427;&#20204;&#35201;&#20040;&#23616;&#38480;&#20110;&#34920;&#26684;MDP&#65292;&#35201;&#20040;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#39640;&#24230;&#20122;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550; - &#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#12290;&#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#32463;&#20856;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;[Kakade&#65292;2001]&#19982;&#20048;&#35266;&#31574;&#30053;&#35780;&#20272;&#23376;&#31243;&#24207;&#31616;&#21333;&#32452;&#21512;&#20197;&#40723;&#21169;&#25506;&#32034;&#12290;&#23545;&#20110;$d$-&#32500;&#32447;&#24615;MDP&#65292;&#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;$\tilde{O}(d^2/\varepsilon^3)$ &#27425;&#37319;&#26679;&#20869;&#23398;&#20064; $\varepsilon$ -&#26368;&#20248;&#31574;&#30053;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20248;&#32500;&#24230;&#20381;&#36182;&#20851;&#31995;$\tilde {\Theta}(d^2)$&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#35745;&#31639;&#39640;&#25928;&#31639;&#27861;&#12290;&#23427;&#20063;&#36229;&#36234;&#20102;&#30446;&#21069;&#39046;&#20808;&#30340;&#19968;&#20123;&#29366;&#24577;of-the-art&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While policy optimization algorithms have played an important role in recent empirical success of Reinforcement Learning (RL), the existing theoretical understanding of policy optimization remains rather limited -- they are either restricted to tabular MDPs or suffer from highly suboptimal sample complexity, especial in online RL where exploration is necessary. This paper proposes a simple efficient policy optimization framework -- Optimistic NPG for online RL. Optimistic NPG can be viewed as simply combining of the classic natural policy gradient (NPG) algorithm [Kakade, 2001] with optimistic policy evaluation subroutines to encourage exploration. For $d$-dimensional linear MDPs, Optimistic NPG is computationally efficient, and learns an $\varepsilon$-optimal policy within $\tilde{O}(d^2/\varepsilon^3)$ samples, which is the first computationally efficient algorithm whose sample complexity has the optimal dimension dependence $\tilde{\Theta}(d^2)$. It also improves over state-of-the-a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#21487;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#20197;&#21450;&#33258;&#25105;&#35780;&#20272;&#21644;&#20840;&#23616;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2305.10601</link><description>&lt;p&gt;
Tree of Thoughts: &#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#21487;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#20197;&#21450;&#33258;&#25105;&#35780;&#20272;&#21644;&#20840;&#23616;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#36890;&#29992;&#38382;&#39064;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20173;&#28982;&#21463;&#38480;&#20110;&#22522;&#20110;&#26631;&#35760;&#12289;&#20174;&#24038;&#21040;&#21491;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#38656;&#35201;&#25506;&#32034;&#12289;&#25112;&#30053;&#21069;&#30651;&#25110;&#21021;&#22987;&#20915;&#31574;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#20219;&#21153;&#20013;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#23427;&#23558;&#36890;&#24120;&#29992;&#20110;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#26041;&#27861;&#27867;&#21270;&#65292;&#24182;&#20351;&#29992;&#19968;&#33268;&#30340;&#25991;&#26412;&#21333;&#20301;&#65288;&#24605;&#32500;&#65289;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#24605;&#32500;&#20316;&#20026;&#35299;&#20915;&#38382;&#39064;&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;&#24605;&#32500;&#20043;&#26641;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#22810;&#20010;&#19981;&#21516;&#30340;&#25512;&#29702;&#36335;&#24452;&#21644;&#33258;&#25105;&#35780;&#20272;&#26469;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#24182;&#20915;&#23450;&#19979;&#19968;&#27493;&#30340;&#34892;&#21160;&#65292;&#21516;&#26102;&#22312;&#24517;&#35201;&#26102;&#21521;&#21069;&#25110;&#21521;&#21518;&#36319;&#36394;&#20197;&#36827;&#34892;&#20840;&#23616;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ToT&#26174;&#33879;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#38382;&#39064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abil
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SELF-ALIGN&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.03047</link><description>&lt;p&gt;
&#21407;&#21017;&#39537;&#21160;&#33258;&#25105;&#23545;&#40784;&#30340;&#26368;&#23567;&#20154;&#21147;&#30417;&#30563;&#30340;&#35821;&#35328;&#27169;&#22411;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03047
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SELF-ALIGN&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;AI&#21161;&#25163;&#20195;&#29702;&#65292;&#22914;ChatGPT&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#24847;&#22270;&#65292;&#30830;&#20445;&#23427;&#20204;&#26159;&#26377;&#29992;&#30340;&#12289;&#36947;&#24503;&#30340;&#12289;&#21487;&#38752;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20381;&#36182;&#24615;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#38480;&#21046;AI&#21161;&#25163;&#20195;&#29702;&#30340;&#30495;&#27491;&#28508;&#21147;&#65292;&#22240;&#20026;&#33719;&#24471;&#20154;&#31867;&#30417;&#30563;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#30456;&#20851;&#38382;&#39064;&#26377;&#36136;&#37327;&#12289;&#21487;&#38752;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#33258;&#19968;&#33268;&#24615;&#21644;&#19981;&#33391;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; SELF-ALIGN&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;&#26041;&#27861;&#21253;&#25324;&#22235;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21512;&#25104;&#25552;&#31034;&#65292;&#20351;&#29992;&#20027;&#39064;&#24341;&#23548;&#26041;&#27861;&#22686;&#21152;&#25552;&#31034;&#22810;&#26679;&#24615;&#65307;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#23567;&#32452;&#20154;&#24037;&#32534;&#20889;&#30340;AI&#27169;&#22411;&#21407;&#21017;&#65292;&#24182;&#25351;&#23548;AI&#27169;&#22411;&#36981;&#24490;&#65307;
&lt;/p&gt;
&lt;p&gt;
Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and gu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#27969;&#39640;&#25928;&#23398;&#20064;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26088;&#22312;&#35299;&#20915;&#20174;&#25968;&#25454;&#27969;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20854;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#21450;&#26102;&#26377;&#25928;&#22320;&#34987;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2305.02217</link><description>&lt;p&gt;
&#27969;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stream Efficient Learning. (arXiv:2305.02217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#27969;&#39640;&#25928;&#23398;&#20064;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26088;&#22312;&#35299;&#20915;&#20174;&#25968;&#25454;&#27969;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20854;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#21450;&#26102;&#26377;&#25928;&#22320;&#34987;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#25968;&#25454;&#24448;&#24448;&#38543;&#30528;&#26102;&#38388;&#30340;&#31215;&#32047;&#20197;&#27969;&#30340;&#24418;&#24335;&#36827;&#34892;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20851;&#27880;&#20110;&#20174;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19981;&#21516;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#19981;&#33021;&#24573;&#35270;&#27969;&#20837;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#26159;&#26080;&#20241;&#27490;&#30340;&#12289;&#35268;&#27169;&#24040;&#22823;&#12289;&#21464;&#21270;&#26410;&#30693;&#65292;&#24182;&#19988;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;&#35745;&#31639;/&#23384;&#20648;&#36164;&#28304;&#21487;&#20197;&#21450;&#26102;&#22788;&#29702;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#25968;&#25454;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#34987;&#21450;&#26102;&#22320;&#26377;&#25928;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#65292;&#20877;&#21152;&#19978;&#23398;&#20064;&#31639;&#27861;&#30340;&#33021;&#21147;&#21644;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21534;&#21520;&#37327;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20102;&#27969;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in many real-world applications are often accumulated over time, like a stream. In contrast to conventional machine learning studies that focus on learning from a given training data set, learning from data streams cannot ignore the fact that the incoming data stream can be potentially endless with overwhelming size and unknown changes, and it is impractical to assume to have sufficient computational/storage resource such that all received data can be handled in time. Thus, the generalization performance of learning from data streams depends not only on how many data have been received, but also on how many data can be well exploited timely, with resource and rapidity concerns, in addition to the ability of learning algorithm and complexity of the problem. For this purpose, in this article we introduce the notion of machine learning throughput, define Stream Efficient Learning and present a preliminary theoretical framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38543;&#26426;&#22806;&#25512;&#25216;&#26415;&#65292;&#29992;&#20110;&#38477;&#20302;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#35777;&#26126;&#22312;&#38750;&#20984;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#23558;&#22806;&#25512;&#19982;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.10613</link><description>&lt;p&gt;
&#28040;&#38500;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Debiasing Conditional Stochastic Optimization. (arXiv:2304.10613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38543;&#26426;&#22806;&#25512;&#25216;&#26415;&#65292;&#29992;&#20110;&#38477;&#20302;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#35777;&#26126;&#22312;&#38750;&#20984;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#23558;&#22806;&#25512;&#19982;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35206;&#30422;&#20102;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#65292;&#21253;&#25324;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#40065;&#26834;&#23398;&#20064;&#12289;&#22240;&#26524;&#25512;&#26029;&#31561;&#30340;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#65288;CSO&#65289;&#38382;&#39064;&#12290;&#30001;&#20110;&#20854;&#23884;&#22871;&#32467;&#26500;&#65292;CSO&#30446;&#26631;&#30340;&#26679;&#26412;&#24179;&#22343;&#26799;&#24230;&#23384;&#22312;&#20559;&#24046;&#65292;&#22240;&#27492;&#38656;&#35201;&#36739;&#39640;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25165;&#33021;&#36798;&#21040;&#25910;&#25947;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#38477;&#20302;&#20559;&#24046;&#30340;&#36890;&#29992;&#38543;&#26426;&#22806;&#25512;&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#38750;&#20984;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#23558;&#36825;&#31181;&#22806;&#25512;&#19982;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#36798;&#21040;&#27604;&#29616;&#26377;&#30028;&#38480;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#29992;&#20110;&#26377;&#38480;&#21644;&#21464;&#37327;&#30340;CSO&#30340;&#26032;&#31639;&#27861;&#65292;&#20063;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#21435;&#20559;&#25216;&#26415;&#20063;&#21487;&#33021;&#26159;&#36866;&#29992;&#20110;&#20854;&#20182;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#26377;&#36259;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the conditional stochastic optimization (CSO) problem which covers a variety of applications including portfolio selection, reinforcement learning, robust learning, causal inference, etc. The sample-averaged gradient of the CSO objective is biased due to its nested structure and therefore requires a high sample complexity to reach convergence. We introduce a general stochastic extrapolation technique that effectively reduces the bias. We show that for nonconvex smooth objectives, combining this extrapolation with variance reduction techniques can achieve a significantly better sample complexity than existing bounds. We also develop new algorithms for the finite-sum variant of CSO that also significantly improve upon existing results. Finally, we believe that our debiasing technique could be an interesting tool applicable to other stochastic optimization problems too.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#26368;&#32456;&#24635;&#32467;&#20102;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.10159</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Using Hybrid Quantum Neural Network. (arXiv:2304.10159v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10159
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#26368;&#32456;&#24635;&#32467;&#20102;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#23545;&#20110;&#20419;&#36827;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#26356;&#39640;&#25968;&#25454;&#32500;&#24230;&#25110;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24635;&#20307;&#35757;&#32451;&#21442;&#25968;&#30340;&#38480;&#21046;&#20855;&#26377;&#24378;&#28872;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230; Q-Learning &#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#24182;&#22521;&#35757;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#26032;&#30340; Qiskit &#21644; PyTorch &#26694;&#26550;&#30340;&#26032;&#22411; PQC&#65292;&#20197;&#19982;&#23436;&#20840;&#32463;&#20856;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#24102;&#25110;&#19981;&#24102;&#38598;&#25104; PQC&#12290;&#30740;&#31350;&#26368;&#21518;&#24635;&#32467;&#20102;&#20854;&#20851;&#20110;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#35299;&#20915;&#36855;&#23467;&#38382;&#39064;&#25110;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computation has a strong implication for advancing the current limitation of machine learning algorithms to deal with higher data dimensions or reducing the overall training parameters for a deep neural network model. Based on a gate-based quantum computer, a parameterized quantum circuit was designed to solve a model-free reinforcement learning problem with the deep-Q learning method. This research has investigated and evaluated its potential. Therefore, a novel PQC based on the latest Qiskit and PyTorch framework was designed and trained to compare with a full-classical deep neural network with and without integrated PQC. At the end of the research, the research draws its conclusion and prospects on developing deep quantum learning in solving a maze problem or other reinforcement learning problems.
&lt;/p&gt;</description></item><item><title>TiDE&#26159;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20855;&#22791;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#65292;&#30456;&#36739;&#20110;&#26368;&#20339;&#30340;Transformer&#27169;&#22411;&#65292;&#36895;&#24230;&#24555;5-10&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.08424</link><description>&lt;p&gt;
&#29992;TiDE&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#65306;&#26102;&#38388;&#24207;&#21015;&#31264;&#23494;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Long-term Forecasting with TiDE: Time-series Dense Encoder. (arXiv:2304.08424v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08424
&lt;/p&gt;
&lt;p&gt;
TiDE&#26159;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20855;&#22791;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#65292;&#30456;&#36739;&#20110;&#26368;&#20339;&#30340;Transformer&#27169;&#22411;&#65292;&#36895;&#24230;&#24555;5-10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#21363;&#26102;&#38388;&#24207;&#21015;&#31264;&#23494;&#32534;&#30721;&#22120;(TiDE)&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20139;&#26377;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26368;&#31616;&#32447;&#24615;&#31867;&#27604;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#21487;&#20197;&#36798;&#21040;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;(LDS)&#30340;&#36817;&#20046;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#21305;&#37197;&#25110;&#32988;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#27604;&#26368;&#20339;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24555;5-10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, Time-series Dense Encoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.
&lt;/p&gt;</description></item><item><title>&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17580</link><description>&lt;p&gt;
HuggingGPT: &#22312;HugingFace&#20013;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20249;&#20276;&#35299;&#20915;AI&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17580
&lt;/p&gt;
&lt;p&gt;
&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22797;&#26434;AI&#20219;&#21153;&#26159;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#31649;&#29702;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#20197;&#35299;&#20915;AI&#20219;&#21153;&#65292;&#35821;&#35328;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#26469;&#36171;&#33021;&#23427;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#26681;&#25454;HuggingFace&#20013;&#21487;&#29992;&#30340;&#27169;&#22411;&#21151;&#33021;&#25551;&#36848;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#22312;&#36873;&#23450;AI&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340; Minecraft &#24320;&#25918;&#24335;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22522;&#26412;&#25216;&#33021;&#21644;&#25216;&#33021;&#35268;&#21010;&#30340;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#35299;&#20915;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;24&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#24182;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16563</link><description>&lt;p&gt;
Plan4MC: &#22522;&#20110;&#25216;&#33021;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340; Minecraft &#24320;&#25918;&#24335;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks. (arXiv:2303.16563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340; Minecraft &#24320;&#25918;&#24335;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22522;&#26412;&#25216;&#33021;&#21644;&#25216;&#33021;&#35268;&#21010;&#30340;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#35299;&#20915;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;24&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#24182;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312; Minecraft &#20013;&#26500;&#24314;&#19968;&#20010;&#22810;&#20219;&#21153;&#26234;&#33021;&#20307;&#12290;&#22312;&#27809;&#26377;&#20154;&#24037;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#36825;&#20010;&#24320;&#25918;&#24335;&#29615;&#22659;&#20013;&#30340;&#38271;&#31243;&#20219;&#21153;&#26159;&#26497;&#20854;&#26679;&#26412;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558; Minecraft &#20219;&#21153;&#30340;&#35299;&#20915;&#20998;&#35299;&#25104;&#23398;&#20064;&#22522;&#26412;&#25216;&#33021;&#21644;&#22522;&#20110;&#25216;&#33021;&#36827;&#34892;&#35268;&#21010;&#20004;&#20010;&#38454;&#27573;&#12290;&#25105;&#20204;&#22312; Minecraft &#20013;&#25552;&#20986;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#32454;&#31890;&#24230;&#22522;&#26412;&#25216;&#33021;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#20869;&#22312;&#22870;&#21169;&#30340; RL &#26041;&#27861;&#26469;&#23454;&#29616;&#25104;&#21151;&#29575;&#39640;&#30340;&#22522;&#26412;&#25216;&#33021;&#23398;&#20064;&#12290;&#22312;&#25216;&#33021;&#35268;&#21010;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21457;&#29616;&#25216;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#39044;&#20808;&#26500;&#24314;&#25216;&#33021;&#22270;&#12290;&#24403;&#26234;&#33021;&#20307;&#35299;&#20915;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#30340;&#25216;&#33021;&#25628;&#32034;&#31639;&#27861;&#22312;&#25216;&#33021;&#22270;&#19978;&#34892;&#36208;&#24182;&#29983;&#25104;&#36866;&#24403;&#30340;&#25216;&#33021;&#35745;&#21010;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102; 24 &#20010;&#19981;&#21516;&#30340; Minecraft &#20219;&#21153;&#65292;&#20854;&#20013;&#35768;&#22810;&#20219;&#21153;&#38656;&#35201;&#36830;&#32493;&#25191;&#34892;&#36229;&#36807; 10 &#20010;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;&#39033;&#30446;&#30340;&#32593;&#22336;&#21644;&#20195;&#30721;&#21487;&#20197;&#22312; https://www.rocwang.me/plan4mc.html &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study building a multi-task agent in Minecraft. Without human demonstrations, solving long-horizon tasks in this open-ended environment with reinforcement learning (RL) is extremely sample inefficient. To tackle the challenge, we decompose solving Minecraft tasks into learning basic skills and planning over the skills. We propose three types of fine-grained basic skills in Minecraft, and use RL with intrinsic rewards to accomplish basic skills with high success rates. For skill planning, we use Large Language Models to find the relationships between skills and build a skill graph in advance. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 24 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines in most tasks by a large margin. The project's website and code can be found at https:
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#22522;&#20110;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#30340;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#34913;&#37327;&#20928;&#21270;&#26041;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20928;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.09051</link><description>&lt;p&gt;
&#25193;&#25955;&#24335;&#23545;&#25239;&#20928;&#21270;&#30340;&#40065;&#26834;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robust Evaluation of Diffusion-Based Adversarial Purification. (arXiv:2303.09051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#22522;&#20110;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#30340;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#34913;&#37327;&#20928;&#21270;&#26041;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20928;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36136;&#30097;&#24403;&#21069;&#23545;&#22522;&#20110;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#30340;&#35780;&#20272;&#26041;&#24335;&#12290;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#26088;&#22312;&#28040;&#38500;&#27979;&#35797;&#25968;&#25454;&#28857;&#20013;&#30340;&#23545;&#25239;&#24615;&#24433;&#21709;&#12290;&#30001;&#20110;&#22522;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#35299;&#32806;&#65292;&#35813;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20316;&#20026;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#20102;&#27979;&#37327;&#20928;&#21270;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#24120;&#37319;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340;&#30333;&#30418;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25915;&#20987;&#36890;&#24120;&#26159;&#20026;&#23545;&#25239;&#24615;&#35757;&#32451;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#65292;&#22240;&#27492;&#19981;&#30693;&#36947;&#36825;&#20123;&#25915;&#20987;&#26159;&#21542;&#23545;&#25193;&#25955;&#24335;&#20928;&#21270;&#26368;&#26377;&#25928;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#21069;&#30340;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#34913;&#37327;&#20928;&#21270;&#26041;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20928;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We question the current evaluation practice on diffusion-based purification methods. Diffusion-based purification methods aim to remove adversarial effects from an input data point at test time. The approach gains increasing attention as an alternative to adversarial training due to the disentangling between training and testing. Well-known white-box attacks are often employed to measure the robustness of the purification. However, it is unknown whether these attacks are the most effective for the diffusion-based purification since the attacks are often tailored for adversarial training. We analyze the current practices and provide a new guideline for measuring the robustness of purification methods against adversarial attacks. Based on our analysis, we further propose a new purification strategy showing competitive results against the state-of-the-art adversarial training approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06470</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20998;&#23376;&#36136;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prefix-tree Decoding for Predicting Mass Spectra from Molecules. (arXiv:2303.06470v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms, and decoding the formula set using a prefix tree structure, atom-type by atom-type, overcoming the combinatorial possibilities for chemical subformulae.
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#24050;&#32463;&#23454;&#29616;&#20102;&#20020;&#24202;&#30456;&#20851;&#20195;&#35874;&#29289;&#30340;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#39044;&#27979;&#24037;&#20855;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21344;&#25454;&#20102;&#20004;&#20010;&#26497;&#31471;&#65292;&#35201;&#20040;&#36890;&#36807;&#36807;&#24230;&#21018;&#24615;&#30340;&#32422;&#26463;&#21644;&#36739;&#24046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#32452;&#21512;&#20998;&#23376;&#26469;&#36827;&#34892;&#25805;&#20316;&#65292;&#35201;&#20040;&#36890;&#36807;&#35299;&#30721;&#26377;&#25439;&#21644;&#38750;&#29289;&#29702;&#31163;&#25955;&#21270;&#30340;&#20809;&#35889;&#21521;&#37327;&#26469;&#36827;&#34892;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#36825;&#20123;&#21270;&#23398;&#20844;&#24335;&#26412;&#36523;&#26159;&#21407;&#23376;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;&#22312;&#39318;&#20808;&#23545;&#36755;&#20837;&#20998;&#23376;&#22270;&#36827;&#34892;&#32534;&#30721;&#21518;&#65292;&#25105;&#20204;&#35299;&#30721;&#19968;&#32452;&#21270;&#23398;&#23376;&#20844;&#24335;&#65292;&#27599;&#20010;&#21270;&#23398;&#23376;&#20844;&#24335;&#25351;&#23450;&#36136;&#35889;&#20013;&#30340;&#19968;&#20010;&#39044;&#27979;&#23792;&#65292;&#20854;&#24378;&#24230;&#30001;&#31532;&#20108;&#20010;&#27169;&#22411;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#36890;&#36807;&#20351;&#29992;&#21069;&#32512;&#26641;&#32467;&#26500;&#65292;&#36880;&#20010;&#21407;&#23376;&#31867;&#22411;&#22320;&#35299;&#30721;&#20844;&#24335;&#38598;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational predictions of mass spectra from molecules have enabled the discovery of clinically relevant metabolites. However, such predictive tools are still limited as they occupy one of two extremes, either operating (a) by fragmenting molecules combinatorially with overly rigid constraints on potential rearrangements and poor time complexity or (b) by decoding lossy and nonphysical discretized spectra vectors. In this work, we introduce a new intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms. After first encoding an input molecular graph, we decode a set of chemical subformulae, each of which specify a predicted peak in the mass spectra, the intensities of which are predicted by a second model. Our key insight is to overcome the combinatorial possibilities for chemical subformulae by decoding the formula set using a prefix tree structure, atom-type by atom-type, represent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#24191;&#20041;QSplines&#65292;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#35745;&#31639;&#26469;&#36817;&#20284;&#38750;&#32447;&#24615;&#37327;&#23376;&#28608;&#27963;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#21407;&#22987;QSplines&#22312;&#37327;&#23376;&#30828;&#20214;&#26041;&#38754;&#30340;&#39640;&#35201;&#27714;&#65292;&#24182;&#36866;&#21512;&#23884;&#20837;&#29616;&#26377;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.04788</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#37327;&#23376;&#26679;&#26465;&#20351;&#38750;&#32447;&#24615;&#37327;&#23376;&#25805;&#20316;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enabling Non-Linear Quantum Operations through Variational Quantum Splines. (arXiv:2303.04788v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#24191;&#20041;QSplines&#65292;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#35745;&#31639;&#26469;&#36817;&#20284;&#38750;&#32447;&#24615;&#37327;&#23376;&#28608;&#27963;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#21407;&#22987;QSplines&#22312;&#37327;&#23376;&#30828;&#20214;&#26041;&#38754;&#30340;&#39640;&#35201;&#27714;&#65292;&#24182;&#36866;&#21512;&#23884;&#20837;&#29616;&#26377;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel method, Generalised QSplines (GQSplines), for approximating non-linear quantum activation functions using hybrid quantum-classical computation, which overcomes the highly demanding requirements of the original QSplines in terms of quantum hardware and is suitable to be embedded in existing quantum neural network architectures.
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21147;&#23398;&#30340;&#20551;&#35774;&#20165;&#23545;&#37327;&#23376;&#29366;&#24577;&#26045;&#21152;&#24186;&#27491;&#21464;&#25442;&#65292;&#36825;&#23545;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#37327;&#23376;&#26679;&#26465;&#65288;QSplines&#65289;&#26469;&#36817;&#20284;&#37327;&#23376;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22312;&#37327;&#23376;&#31639;&#27861;&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#12290;&#28982;&#32780;&#65292;QSplines&#20351;&#29992;HHL&#20316;&#20026;&#23376;&#31243;&#24207;&#65292;&#24182;&#38656;&#35201;&#19968;&#20010;&#23481;&#38169;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#25165;&#33021;&#27491;&#30830;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;QSplines&#65288;GQSplines&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#35745;&#31639;&#26469;&#36817;&#20284;&#38750;&#32447;&#24615;&#37327;&#23376;&#28608;&#27963;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;GQSplines&#20811;&#26381;&#20102;&#21407;&#22987;QSplines&#22312;&#37327;&#23376;&#30828;&#20214;&#26041;&#38754;&#30340;&#39640;&#35201;&#27714;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#36817;&#26399;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#28789;&#27963;&#30340;&#38382;&#39064;&#34920;&#31034;&#65292;&#36866;&#21512;&#23884;&#20837;&#29616;&#26377;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The postulates of quantum mechanics impose only unitary transformations on quantum states, which is a severe limitation for quantum machine learning algorithms. Quantum Splines (QSplines) have recently been proposed to approximate quantum activation functions to introduce non-linearity in quantum algorithms. However, QSplines make use of the HHL as a subroutine and require a fault-tolerant quantum computer to be correctly implemented. This work proposes the Generalised QSplines (GQSplines), a novel method for approximating non-linear quantum activation functions using hybrid quantum-classical computation. The GQSplines overcome the highly demanding requirements of the original QSplines in terms of quantum hardware and can be implemented using near-term quantum computers. Furthermore, the proposed method relies on a flexible problem representation for non-linear approximation and it is suitable to be embedded in existing quantum neural network architectures. In addition, we provide a pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23637;&#31034;&#65292;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;GPT-2&#21644;GPT-3&#31561;LM&#30340;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.04729</link><description>&lt;p&gt;
&#35770;&#30423;&#29992;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#31639;&#27861;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23637;&#31034;&#65292;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;GPT-2&#21644;GPT-3&#31561;LM&#30340;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#29983;&#25104;&#25991;&#26412;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#21644;&#35843;&#25972;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#30830;&#23450;&#22914;&#20309;&#20174;LM&#29983;&#25104;&#30340;&#20869;&#37096;&#27010;&#29575;&#20998;&#24067;&#20013;&#29983;&#25104;&#25991;&#26412;&#12290;&#36873;&#25321;&#35299;&#30721;&#31639;&#27861;&#24182;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#30340;&#36807;&#31243;&#38656;&#35201;&#26174;&#33879;&#30340;&#26102;&#38388;&#12289;&#25163;&#21160;&#24037;&#20316;&#21644;&#35745;&#31639;&#65292;&#36824;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#35299;&#30721;&#31639;&#27861;&#30340;&#36523;&#20221;&#21644;&#36229;&#21442;&#25968;&#34987;&#35748;&#20026;&#26159;&#26497;&#20854;&#26377;&#20215;&#20540;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#20010;&#25317;&#26377;&#20856;&#22411;API&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;&#21487;&#20197;&#20197;&#26497;&#20302;&#30340;&#37329;&#38065;&#25104;&#26412;&#31363;&#21462;&#20854;&#35299;&#30721;&#31639;&#27861;&#30340;&#31867;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;API&#30340;&#27969;&#34892;LM&#26377;&#25928;&#65292;&#21253;&#25324;GPT-2&#21644;GPT-3&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21482;&#38656;&#33457;&#36153;&#20960;&#32654;&#20803;&#65292;&#20363;&#22914;0.8&#32654;&#20803;&#12289;1&#32654;&#20803;&#12289;4&#32654;&#20803;&#21644;40&#32654;&#20803;&#65292;&#23601;&#21487;&#20197;&#30423;&#21462;&#27492;&#31867;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#20284;&#28982;&#20998;&#25968;&#65288;FLS&#65289;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#26679;&#26412;&#30340;&#26032;&#39062;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04440</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#26412;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples. (arXiv:2302.04440v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#20284;&#28982;&#20998;&#25968;&#65288;FLS&#65289;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#26679;&#26412;&#30340;&#26032;&#39062;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#32500;&#12289;&#22797;&#26434;&#21644;&#29031;&#29255;&#33324;&#36924;&#30495;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#20173;&#28982;&#19981;&#23436;&#20840;&#65306;&#26631;&#20934;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#36866;&#29992;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#20063;&#24456;&#23569;&#19982;&#24863;&#30693;&#20445;&#30495;&#24230;&#30456;&#20851;&#65292;&#32780;&#22522;&#20110;&#26679;&#26412;&#30340;&#25351;&#26631;&#65288;&#22914;FID&#65289;&#23545;&#36807;&#25311;&#21512;&#19981;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#29305;&#24449;&#20284;&#28982;&#20998;&#25968;&#65288;FLS&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#20998;&#25968;&#65292;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#26469;&#25552;&#20379;&#20840;&#38754;&#30340;&#19977;&#30456;&#35780;&#20272;&#65292;&#32771;&#34385;&#29983;&#25104;&#26679;&#26412;&#30340;&#26032;&#39062;&#24615;&#65288;&#21363;&#19982;&#35757;&#32451;&#26679;&#26412;&#19981;&#21516;&#65289;&#12289;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;FLS&#22312;&#26816;&#27979;&#36807;&#25311;&#21512;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#65292;&#20808;&#21069;&#25552;&#20986;&#30340;&#24230;&#37327;&#25351;&#26631;&#26080;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#31867;&#21035;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;FLS&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Score (FLS), a parametric sample-based score that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLS to identify specific overfitting problem cases, where previously proposed metrics fail. We also extensively evaluate FLS on various image datasets and model classes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21367;&#31215;&#31070;&#32463;&#31639;&#23376;&#65288;CNO&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#19978;&#19979;&#25991;&#20013;&#22788;&#29702;&#20989;&#25968;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102; CNOs &#21487;&#20197;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20986;&#29616;&#30340;&#31639;&#23376;&#21040;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;CNOs &#26174;&#30528;&#20248;&#20110;&#22522;&#32447;&#65292;&#36825;&#20026;&#40065;&#26834;&#20934;&#30830;&#25805;&#20316;&#23398;&#20064;&#30340;&#21478;&#19968;&#31181;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2302.01178</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#40065;&#26834;&#20934;&#30830;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Operators for robust and accurate learning of PDEs. (arXiv:2302.01178v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21367;&#31215;&#31070;&#32463;&#31639;&#23376;&#65288;CNO&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#19978;&#19979;&#25991;&#20013;&#22788;&#29702;&#20989;&#25968;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102; CNOs &#21487;&#20197;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20986;&#29616;&#30340;&#31639;&#23376;&#21040;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;CNOs &#26174;&#30528;&#20248;&#20110;&#22522;&#32447;&#65292;&#36825;&#20026;&#40065;&#26834;&#20934;&#30830;&#25805;&#20316;&#23398;&#20064;&#30340;&#21478;&#19968;&#31181;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#25104;&#21151;&#65292;&#22522;&#20110;&#21367;&#31215;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#22240;&#27492;&#22312;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#30340;&#19978;&#19979;&#25991;&#20013;&#34987;&#24191;&#27867;&#24573;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30830;&#23454;&#33021;&#22815;&#22788;&#29702;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20989;&#25968;&#12290;&#32467;&#26524;&#20135;&#29983;&#30340;&#26550;&#26500;&#31216;&#20026;&#21367;&#31215;&#31070;&#32463;&#31639;&#23376;&#65288;CNO&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#22312;&#35745;&#31639;&#26426;&#19978;&#20197;&#31163;&#25955;&#24418;&#24335;&#23454;&#29616;&#26102;&#20445;&#25345;&#20854;&#22522;&#26412;&#30340;&#36830;&#32493;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26222;&#36866;&#23450;&#29702;&#65292;&#20197;&#23637;&#31034;CNOs&#21487;&#20197;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20986;&#29616;&#30340;&#31639;&#23376;&#21040;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290; CNOs&#22312;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#20855;&#26377;&#21487;&#33021;&#20855;&#26377;&#22810;&#23610;&#24230;&#35299;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26032;&#22871;&#20214;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#34987;&#35266;&#23519;&#21040;&#26174;&#30528;&#20248;&#20110;&#22522;&#32447;&#65292;&#20026;&#40065;&#26834;&#20934;&#30830;&#25805;&#20316;&#23398;&#20064;&#30340;&#21478;&#19968;&#31181;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with possibly multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; ODoS &#28388;&#27874;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#26354;&#32447;&#32467;&#26500;&#20998;&#21106;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20013;&#26354;&#32447;&#23545;&#35937;&#30340;&#33258;&#21160;&#20998;&#21106;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.07475</link><description>&lt;p&gt;
&#22522;&#20110; ODoS &#28388;&#27874;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#21307;&#23398;&#22270;&#20687;&#26354;&#32447;&#23545;&#35937;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Curvilinear object segmentation in medical images based on ODoS filter and deep learning network. (arXiv:2301.07475v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07475
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; ODoS &#28388;&#27874;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#26354;&#32447;&#32467;&#26500;&#20998;&#21106;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20013;&#26354;&#32447;&#23545;&#35937;&#30340;&#33258;&#21160;&#20998;&#21106;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#65292;&#26354;&#32447;&#23545;&#35937;&#30340;&#33258;&#21160;&#20998;&#21106;&#23545;&#20110;&#20154;&#31867;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#35780;&#20272;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#22806;&#35266;&#65292;&#26354;&#32447;&#23545;&#35937;&#19982;&#20854;&#21608;&#22260;&#32972;&#26223;&#20043;&#38388;&#30340;&#20302;&#23545;&#27604;&#24230;&#65292;&#34180;&#32780;&#19981;&#22343;&#21248;&#30340;&#26354;&#32447;&#32467;&#26500;&#20197;&#21450;&#19981;&#36866;&#24403;&#30340;&#32972;&#26223;&#20809;&#29031;&#26465;&#20214;&#31561;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ODoS&#28388;&#27874;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#29420;&#29305;&#26354;&#32447;&#32467;&#26500;&#20998;&#21106;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#36827;&#34892;&#26354;&#32447;&#23545;&#35937;&#20998;&#21106;&#12290;&#30446;&#21069;&#65292;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24378;&#35843;&#24320;&#21457;&#28145;&#24230;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#24573;&#30053;&#25429;&#33719;&#26354;&#32447;&#23545;&#35937;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;ODoS&#28388;&#27874;&#22120;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#19968;&#37096;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#31354;&#38388;&#20449;&#24687;&#30340;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic segmentation of curvilinear objects in medical images plays an important role in the diagnosis and evaluation of human diseases, yet it is a challenging uncertainty in the complex segmentation tasks due to different issues such as various image appearances, low contrast between curvilinear objects and their surrounding backgrounds, thin and uneven curvilinear structures, and improper background illumination conditions. To overcome these challenges, we present a unique curvilinear structure segmentation framework based on an oriented derivative of stick (ODoS) filter and a deep learning network for curvilinear object segmentation in medical images. Currently, a large number of deep learning models emphasize developing deep architectures and ignore capturing the structural features of curvilinear objects, which may lead to unsatisfactory results. Consequently, a new approach that incorporates an ODoS filter as part of a deep learning network is presented to improve the spatial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#35299;&#20915;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#26080;&#27861;&#37327;&#21270;&#36817;&#20284;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.12474</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#24212;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers. (arXiv:2212.12474v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#35299;&#20915;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#26080;&#27861;&#37327;&#21270;&#36817;&#20284;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#19968;&#31867;&#37325;&#35201;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#26426;&#26800;&#27169;&#22411;&#65292;&#25551;&#36848;&#20102;&#29289;&#29702;&#36807;&#31243;&#65292;&#20363;&#22914;&#28909;&#20256;&#23548;&#12289;&#30005;&#30913;&#23398;&#21644;&#27874;&#20256;&#25773;&#31561;&#12290;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#31163;&#25955;&#21270;&#30340;&#19987;&#38376;&#25968;&#20540;&#26041;&#27861;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#36825;&#20123;&#27714;&#35299;&#22120;&#36890;&#24120;&#20351;&#29992;&#26410;&#30693;&#27169;&#22411;&#21442;&#25968;&#30340;&#20272;&#35745;&#20540;&#20197;&#21450;&#22914;&#26524;&#21487;&#29992;&#30340;&#35805;&#65292;&#29289;&#29702;&#27979;&#37327;&#20540;&#29992;&#20110;&#21021;&#22987;&#21270;&#12290;&#36825;&#20123;&#27714;&#35299;&#22120;&#32463;&#24120;&#23884;&#20837;&#21040;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#26356;&#22823;&#30340;&#31185;&#23398;&#27169;&#22411;&#20013;&#65292;&#22240;&#27492;&#35823;&#24046;&#37327;&#21270;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#24573;&#30053;&#21442;&#25968;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#33021;&#26080;&#27861;&#20135;&#29983;&#19968;&#33268;&#24615;&#30340;&#20272;&#35745;&#20540;&#65292;&#20197;&#29992;&#20110;&#35745;&#31639;&#20854;&#22266;&#26377;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#27714;&#35299;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#37322;&#20026;&#29289;&#29702;&#23398;&#30693;&#35782;&#25351;&#23548;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#23450;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#25512;&#24191;&#65292;&#35813;&#23450;&#29702;&#36866;&#29992;&#20110;&#36890;&#36807;&#20219;&#24847;&#30028;&#38754;&#36827;&#34892;&#35266;&#23519;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear partial differential equations (PDEs) are an important, widely applied class of mechanistic models, describing physical processes such as heat transfer, electromagnetism, and wave propagation. In practice, specialized numerical methods based on discretization are used to solve PDEs. They generally use an estimate of the unknown model parameters and, if available, physical measurements for initialization. Such solvers are often embedded into larger scientific models with a downstream application and thus error quantification plays a key role. However, by ignoring parameter and measurement uncertainty, classical PDE solvers may fail to produce consistent estimates of their inherent approximation error. In this work, we approach this problem in a principled fashion by interpreting solving linear PDEs as physics-informed Gaussian process (GP) regression. Our framework is based on a key generalization of the Gaussian process inference theorem to observations made via an arbitrary bou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28431;&#26007;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#40065;&#26834;&#28385;&#36275;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#30340;&#26102;&#38388;&#20381;&#36182;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.03181</link><description>&lt;p&gt;
&#22522;&#20110;&#28431;&#26007;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#20219;&#21153;&#30340;&#22870;&#21169;&#22609;&#24418;
&lt;/p&gt;
&lt;p&gt;
Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning. (arXiv:2212.03181v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28431;&#26007;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#40065;&#26834;&#28385;&#36275;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#30340;&#26102;&#38388;&#20381;&#36182;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#65288;STL&#65289;&#26159;&#25551;&#36848;&#21160;&#24577;&#31995;&#32479;&#22797;&#26434;&#26102;&#24577;&#21644;&#36923;&#36753;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#24378;&#21046;&#25191;&#34892;STL&#35268;&#33539;&#30340;&#25511;&#21046;&#22120;&#65292;&#28982;&#32780;&#65292;&#20182;&#20204;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30830;&#20445;&#40065;&#26834;&#28385;&#36275;&#21644;&#20445;&#25345;&#21487;&#25511;&#24615;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20511;&#21161;&#28431;&#26007;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25511;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;STL&#35268;&#33539;&#30340;&#40065;&#26834;&#28385;&#36275;&#30340;&#26102;&#38388;&#20381;&#36182;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signal Temporal Logic (STL) is a powerful framework for describing the complex temporal and logical behaviour of the dynamical system. Numerous studies have attempted to employ reinforcement learning to learn a controller that enforces STL specifications; however, they have been unable to effectively tackle the challenges of ensuring robust satisfaction in continuous state space and maintaining tractability. In this paper, leveraging the concept of funnel functions, we propose a tractable reinforcement learning algorithm to learn a time-dependent policy for robust satisfaction of STL specification in continuous state space. We demonstrate the utility of our approach on several STL tasks using different environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#38646;&#35757;&#32451;&#35823;&#24046;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#20013;&#8220;&#22351;&#8221;&#26041;&#26696;&#30340;&#21344;&#27604;&#38543;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#25351;&#25968;&#32423;&#36882;&#20943;&#65292;&#24182;&#33021;&#35299;&#37322;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.03570</link><description>&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#24378;&#65292;&#22240;&#20026;&#31967;&#31957;&#30340;&#35299;&#20915;&#26041;&#26696;&#24456;&#23569;
&lt;/p&gt;
&lt;p&gt;
Highly over-parameterized classifiers generalize since bad solutions are rare. (arXiv:2211.03570v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#38646;&#35757;&#32451;&#35823;&#24046;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#20013;&#8220;&#22351;&#8221;&#26041;&#26696;&#30340;&#21344;&#27604;&#38543;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#25351;&#25968;&#32423;&#36882;&#20943;&#65292;&#24182;&#33021;&#35299;&#37322;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20854;&#20013;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#23398;&#20064;&#23548;&#33268;&#38646;&#35757;&#32451;&#35823;&#24046;&#12290;&#22312;&#36825;&#20123;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#35774;&#32622;&#20013;&#65292;&#26377;&#35768;&#22810;&#20855;&#26377;&#38646;&#35757;&#32451;&#35823;&#24046;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#20854;&#20013;&#19968;&#20123;&#27604;&#20854;&#20182;&#30340;&#26356;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#8220;&#22351;&#8221;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#20998;&#25968;&#65292;&#20854;&#30495;&#23454;&#35823;&#24046;&#22823;&#20110;&#949;&#65292;&#19982;&#35757;&#32451;&#25968;&#25454;n&#30340;&#25968;&#37327;&#25351;&#25968;&#32423;&#22320;&#36882;&#20943;&#21040;&#38646;&#12290;&#35813;&#33539;&#22260;&#21462;&#20915;&#20110;&#29992;&#20110;&#32473;&#23450;&#20998;&#31867;&#38382;&#39064;&#30340;&#20998;&#31867;&#22120;&#20989;&#25968;&#38598;&#21512;&#19978;&#30495;&#23454;&#35823;&#24046;&#30340;&#20998;&#24067;&#65292;&#19981;&#19968;&#23450;&#21462;&#20915;&#20110;&#20998;&#31867;&#22120;&#20989;&#25968;&#38598;&#21512;&#30340;&#22823;&#23567;&#25110;&#22797;&#26434;&#24230;&#65288;&#20363;&#22914;&#21442;&#25968;&#25968;&#37327;&#65289;&#12290;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#21363;&#20351;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20063;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;MNIST&#30340;&#23376;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the generalization of over-parameterized classifiers where Empirical Risk Minimization (ERM) for learning leads to zero training error. In these over-parameterized settings there are many global minima with zero training error, some of which generalize better than others. We show that under certain conditions the fraction of "bad" global minima with a true error larger than {\epsilon} decays to zero exponentially fast with the number of training data n. The bound depends on the distribution of the true error over the set of classifier functions used for the given classification problem, and does not necessarily depend on the size or complexity (e.g. the number of parameters) of the classifier function set. This might explain the unexpectedly good generalization even of highly over-parameterized Neural Networks. We support our mathematical framework with experiments on a synthetic data set and a subset of MNIST.
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#25104;&#20026;&#28909;&#38376;&#26041;&#27861;&#65292;&#20294;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20250;&#38754;&#20020;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;"&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;</title><link>http://arxiv.org/abs/2211.02658</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#22788;&#29702;&#23398;&#20064;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02658
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#25104;&#20026;&#28909;&#38376;&#26041;&#27861;&#65292;&#20294;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20250;&#38754;&#20020;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;"&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064; (ML) &#24050;&#25104;&#20026;&#25903;&#25345;&#33258;&#36866;&#24212;&#30340;&#28909;&#38376;&#26041;&#27861;&#12290;ML &#24050;&#34987;&#29992;&#26469;&#22788;&#29702;&#33258;&#36866;&#24212;&#20013;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#32500;&#25252;&#26368;&#26032;&#30340;&#36816;&#34892;&#26102;&#27169;&#22411;&#21644;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992; ML &#23384;&#22312;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#38754;&#21521;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31995;&#32479;&#30340;&#19968;&#20010;&#29305;&#21035;&#37325;&#35201;&#30340;&#25361;&#25112;&#65306;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#36866;&#24212;&#31354;&#38388;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#33258;&#36866;&#24212;&#31995;&#32479;&#22312;&#26576;&#19968;&#29305;&#23450;&#26102;&#38388;&#21487;&#20197;&#36873;&#25321;&#30340;&#36866;&#24212;&#36873;&#39033;&#30340;&#38598;&#21512;&#65292;&#20197;&#26681;&#25454;&#36866;&#24212;&#36873;&#39033;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#36866;&#24212;&#12290;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#28304;&#20110;&#24433;&#21709;&#36866;&#24212;&#36873;&#39033;&#36136;&#37327;&#23646;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#28418;&#31227;&#21487;&#33021;&#24847;&#21619;&#30528;&#26368;&#32456;&#27809;&#26377;&#36866;&#24212;&#36873;&#39033;&#33021;&#22815;&#28385;&#36275;&#26368;&#21021;&#30340;&#36866;&#24212;&#30446;&#26631;&#65292;&#20174;&#32780;&#38477;&#20302;&#31995;&#32479;&#30340;&#36136;&#37327;&#65292;&#25110;&#32773;&#21487;&#33021;&#20986;&#29616;&#20801;&#35768;&#22686;&#24378;&#36866;&#24212;&#30446;&#26631;&#30340;&#36866;&#24212;&#36873;&#39033;&#12290;&#22312; ML &#20013;&#65292;&#36825;&#31181;&#28418;&#31227;&#36890;&#24120;&#34987;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#25110;&#23454;&#20363;&#28418;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#23545; ML powered self-adaptation &#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this paper, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space we refer to the set of adaptation options a self-adaptive system can select from at a given time to adapt based on the estimated quality properties of the adaptation options. Drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that eventually no adaptation option can satisfy the initial set of the adaptation goals, deteriorating the quality of the system, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such shift cor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.07675</link><description>&lt;p&gt;
&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#20197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65306;&#22312;&#33647;&#29289;&#24320;&#21457;&#20013;&#21457;&#29616;&#32452;&#32455;&#23398;&#25913;&#21464;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#30340;&#31995;&#32479;&#12290;&#22312;&#32452;&#32455;&#23398;&#20013;&#65292;&#27491;&#24120;&#26679;&#26412;&#36890;&#24120;&#26159;&#22823;&#37327;&#23384;&#22312;&#30340;&#65292;&#32780;&#24322;&#24120;&#65288;&#30149;&#29702;&#65289;&#24773;&#20917;&#36890;&#24120;&#24456;&#23569;&#25110;&#19981;&#21487;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22312;&#20581;&#24247;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21333;&#31867;&#20998;&#31867;&#22120;&#21487;&#20197;&#26816;&#27979;&#21040;&#20998;&#24067;&#22806;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#19982;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#21069;&#24050;&#32463;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#12290;&#20294;&#26159;&#65292;&#39044;&#35757;&#32451;&#30340;&#29616;&#25104;CNN&#34920;&#31034;&#21487;&#33021;&#23545;&#32452;&#32455;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#19981;&#25935;&#24863;&#65292;&#32780;&#20581;&#24247;&#32452;&#32455;&#30340;&#33258;&#28982;&#21464;&#24322;&#21487;&#33021;&#23548;&#33268;&#36828;&#31163;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#20351;&#34920;&#31034;&#36866;&#24212;&#20581;&#24247;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#36741;&#21161;&#20219;&#21153;&#19978;&#35757;&#32451;CNN&#65292;&#35813;&#20219;&#21153;&#21306;&#20998;&#19981;&#21516;&#29289;&#31181;&#12289;&#22120;&#23448;&#21644;&#26579;&#33394;&#35797;&#21058;&#30340;&#20581;&#24247;&#32452;&#32455;&#12290;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#27880;&#24037;&#20316;&#37327;&#65292;&#22240;&#20026;&#20581;&#24247;&#26679;&#26412;&#21487;&#20197;&#33258;&#21160;&#33719;&#24471;&#19978;&#36848;&#26631;&#31614;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#24378;&#21046;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
We present a system for anomaly detection in histopathological images. In histology, normal samples are usually abundant, whereas anomalous (pathological) cases are scarce or not available. Under such settings, one-class classifiers trained on healthy data can detect out-of-distribution anomalous samples. Such approaches combined with pre-trained Convolutional Neural Network (CNN) representations of images were previously employed for anomaly detection (AD). However, pre-trained off-the-shelf CNN representations may not be sensitive to abnormal conditions in tissues, while natural variations of healthy tissue may result in distant representations. To adapt representations to relevant details in healthy tissue we propose training a CNN on an auxiliary task that discriminates healthy tissue of different species, organs, and staining reagents. Almost no additional labeling workload is required, since healthy samples come automatically with aforementioned labels. During training we enforce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24320;&#25918;&#35789;&#27719;&#27169;&#22411;&#29983;&#25104;&#33258;&#23450;&#20041;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#30340;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#22270;&#20687;&#31867;&#21035;&#30340;&#37325;&#35201;&#21306;&#20998;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2209.03320</link><description>&lt;p&gt;
&#38271;&#20160;&#20040;&#26679;&#30340;&#40493;&#22068;&#20861;&#65311;&#29983;&#25104;&#33258;&#23450;&#20041;&#25552;&#31034;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
What does a platypus look like? Generating customized prompts for zero-shot image classification. (arXiv:2209.03320v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24320;&#25918;&#35789;&#27719;&#27169;&#22411;&#29983;&#25104;&#33258;&#23450;&#20041;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#30340;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#22270;&#20687;&#31867;&#21035;&#30340;&#37325;&#35201;&#21306;&#20998;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#27169;&#22411;&#26159;&#22270;&#20687;&#20998;&#31867;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26032;&#33539;&#24335;&#12290;&#19982;&#20256;&#32479;&#30340;&#20998;&#31867;&#27169;&#22411;&#19981;&#21516;&#65292;&#24320;&#25918;&#35789;&#27719;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#23545;&#20219;&#24847;&#25351;&#23450;&#30340;&#31867;&#21035;&#38598;&#21512;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#20123;&#31867;&#21035;&#30001;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#12290;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#36890;&#24120;&#21253;&#21547;&#19968;&#32452;&#25163;&#20889;&#27169;&#26495;&#65288;&#20363;&#22914;&#65292;&#8220;&#19968;&#24352;{}&#30340;&#29031;&#29255;&#8221;&#65289;&#65292;&#27599;&#20010;&#31867;&#21035;&#21517;&#31216;&#37117;&#20250;&#22635;&#20805;&#36827;&#21435;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26356;&#39640;&#20934;&#30830;&#24230;&#30340;&#25552;&#31034;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20219;&#21153;&#39046;&#22495;&#30340;&#26174;&#24335;&#30693;&#35782;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25163;&#24037;&#26500;&#24314;&#21477;&#23376;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#24320;&#25918;&#35789;&#27719;&#27169;&#22411;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#21019;&#24314;&#20986;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#23450;&#20041;&#25552;&#31034;&#65288;CuPL&#65292;&#35835;&#20316;&#8220;couple&#8221;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#20013;&#21253;&#21547;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#21253;&#21547;&#22270;&#20687;&#31867;&#21035;&#30340;&#37325;&#35201;&#21306;&#20998;&#29305;&#24449;&#30340;&#25551;&#36848;&#24615;&#21477;&#23376;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#37325;&#35270;&#36825;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-vocabulary models are a promising new paradigm for image classification. Unlike traditional classification models, open-vocabulary models classify among any arbitrary set of categories specified with natural language during inference. This natural language, called "prompts", typically consists of a set of hand-written templates (e.g., "a photo of a {}") which are completed with each of the category names. This work introduces a simple method to generate higher accuracy prompts, without relying on any explicit knowledge of the task domain and with far fewer hand-constructed sentences. To achieve this, we combine open-vocabulary models with large language models (LLMs) to create Customized Prompts via Language models (CuPL, pronounced "couple"). In particular, we leverage the knowledge contained in LLMs in order to generate many descriptive sentences that contain important discriminating characteristics of the image categories. This allows the model to place a greater importance on 
&lt;/p&gt;</description></item><item><title>&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#65292;&#22914;&#20309;&#35753;&#30693;&#35782;&#25277;&#21462;&#26356;&#22909;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#65311;&#26412;&#25991;&#35843;&#30740;&#20102;&#19977;&#31181;&#35299;&#20915;&#33539;&#24335;&#65306;&#39640;&#36164;&#28304;&#25968;&#25454;&#12289;&#26356;&#24378;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2202.08063</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#30340;&#30693;&#35782;&#25277;&#21462;&#65306;&#35843;&#30740;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08063
&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#65292;&#22914;&#20309;&#35753;&#30693;&#35782;&#25277;&#21462;&#26356;&#22909;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#65311;&#26412;&#25991;&#35843;&#30740;&#20102;&#19977;&#31181;&#35299;&#20915;&#33539;&#24335;&#65306;&#39640;&#36164;&#28304;&#25968;&#25454;&#12289;&#26356;&#24378;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#25277;&#21462;&#65288;KE&#65289;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65292;&#36890;&#24120;&#36973;&#21463;&#25968;&#25454;&#21294;&#20047;&#21644;&#20986;&#29616;&#26410;&#35265;&#31867;&#22411;&#65288;&#20302;&#36164;&#28304;&#24773;&#22659;&#65289;&#30340;&#22256;&#25200;&#12290;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#24050;&#24191;&#27867;&#30740;&#31350;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#23545;&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;KE&#36827;&#34892;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23558;&#29616;&#26377;&#30340;&#24037;&#20316;&#31995;&#32479;&#24615;&#22320;&#20998;&#20026;&#19977;&#31181;&#33539;&#24335;&#65306;&#65288;1&#65289;&#21033;&#29992;&#39640;&#36164;&#28304;&#25968;&#25454;&#65292;&#65288;2&#65289;&#21033;&#29992;&#26356;&#24378;&#30340;&#27169;&#22411;&#65292;&#65288;3&#65289;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20123;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35843;&#30740;&#21487;&#20197;&#24110;&#21161;&#23398;&#26415;&#21644;&#24037;&#19994;&#30028;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#21019;&#24847;&#65292;&#25552;&#21319;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Extraction (KE), aiming to extract structural information from unstructured texts, often suffers from data scarcity and emerging unseen types, i.e., low-resource scenarios. Many neural approaches to low-resource KE have been widely investigated and achieved impressive performance. In this paper, we present a literature review towards KE in low-resource scenarios, and systematically categorize existing works into three paradigms: (1) exploiting higher-resource data, (2) exploiting stronger models, and (3) exploiting data and models together. In addition, we highlight promising applications and outline some potential directions for future research. We hope that our survey can help both the academic and industrial communities to better understand this field, inspire more ideas, and boost broader applications.
&lt;/p&gt;</description></item></channel></rss>