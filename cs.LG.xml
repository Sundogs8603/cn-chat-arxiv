<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>LEAP Hand&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#25928;&#19988;&#25311;&#20154;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25163;&#12290;&#20854;&#26032;&#39062;&#30340;&#36816;&#21160;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#20854;&#25104;&#26412;&#20026;&#20854;&#20182;&#31454;&#20105;&#23545;&#25163;&#30340;1/8&#12290;LEAP Hand&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#22810;&#20010;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.06440</link><description>&lt;p&gt;
LEAP Hand: &#20302;&#25104;&#26412;&#12289;&#39640;&#25928;&#21644;&#25311;&#20154;&#21270;&#26426;&#22120;&#23398;&#20064;&#25163;
&lt;/p&gt;
&lt;p&gt;
LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning. (arXiv:2309.06440v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06440
&lt;/p&gt;
&lt;p&gt;
LEAP Hand&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#25928;&#19988;&#25311;&#20154;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25163;&#12290;&#20854;&#26032;&#39062;&#30340;&#36816;&#21160;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#20854;&#25104;&#26412;&#20026;&#20854;&#20182;&#31454;&#20105;&#23545;&#25163;&#30340;1/8&#12290;LEAP Hand&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#22810;&#20010;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#65292;&#28789;&#24039;&#25805;&#25511;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#36825;&#26041;&#38754;&#26174;&#31034;&#20986;&#19968;&#20123;&#24076;&#26395;&#65292;&#20294;&#32467;&#26524;&#20027;&#35201;&#38480;&#20110;&#27169;&#25311;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#30828;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEAP Hand&#65292;&#36825;&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#28789;&#24039;&#21644;&#25311;&#20154;&#21270;&#26426;&#22120;&#23398;&#20064;&#25163;&#12290;&#19982;&#20197;&#24448;&#30340;&#25163;&#30456;&#27604;&#65292;LEAP Hand&#20855;&#26377;&#19968;&#31181;&#26032;&#39062;&#30340;&#36816;&#21160;&#32467;&#26500;&#65292;&#26080;&#35770;&#25163;&#25351;&#30340;&#23039;&#21183;&#22914;&#20309;&#37117;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;LEAP Hand&#25104;&#26412;&#20302;&#24265;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;4&#20010;&#23567;&#26102;&#20869;&#20351;&#29992;&#29616;&#26377;&#38646;&#20214;&#32452;&#35013;&#32780;&#25104;&#65292;&#25104;&#26412;&#20026;2000&#32654;&#20803;&#12290;&#23427;&#33021;&#22815;&#25345;&#32493;&#38271;&#26102;&#38388;&#22320;&#26045;&#21152;&#22823;&#25197;&#30697;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LEAP Hand&#21487;&#20197;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#22810;&#20010;&#25805;&#20316;&#20219;&#21153;&#65292;&#20174;&#35270;&#35273;&#36828;&#31243;&#25805;&#20316;&#21040;&#20174;&#34987;&#21160;&#35270;&#39057;&#25968;&#25454;&#21644;&#27169;&#25311;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#23398;&#20064;&#12290;LEAP Hand&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#37117;&#26126;&#26174;&#20248;&#20110;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#23545;&#25163;Allegro Hand&#65292;&#21516;&#26102;&#25104;&#26412;&#21482;&#26377;&#20854;1/8&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#35814;&#32454;&#30340;&#35774;&#35745;&#21644;&#32452;&#35013;&#35828;&#26126;&#65292;&#20197;&#20415;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#37325;&#22797;&#25105;&#20204;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dexterous manipulation has been a long-standing challenge in robotics. While machine learning techniques have shown some promise, results have largely been currently limited to simulation. This can be mostly attributed to the lack of suitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous and anthropomorphic hand for machine learning research. In contrast to previous hands, LEAP Hand has a novel kinematic structure that allows maximal dexterity regardless of finger pose. LEAP Hand is low-cost and can be assembled in 4 hours at a cost of 2000 USD from readily available parts. It is capable of consistently exerting large torques over long durations of time. We show that LEAP Hand can be used to perform several manipulation tasks in the real world -- from visual teleoperation to learning from passive video data and sim2real. LEAP Hand significantly outperforms its closest competitor Allegro Hand in all our experiments while being 1/8th of the cost. We release detailed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;GPT-3&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#29983;&#25104;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#35780;&#20272;&#12289;&#20998;&#26512;&#21644;&#39564;&#35777;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#32473;&#23450;&#20195;&#30721;&#29255;&#27573;&#30340;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#21464;&#20307;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.06424</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#26041;&#38754;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the potential of large language models in generating semantic and cross-language clones. (arXiv:2309.06424v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;GPT-3&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#29983;&#25104;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#35780;&#20272;&#12289;&#20998;&#26512;&#21644;&#39564;&#35777;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#32473;&#23450;&#20195;&#30721;&#29255;&#27573;&#30340;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#21464;&#20307;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20195;&#30721;&#20811;&#38534;&#29983;&#25104;&#23545;&#20110;&#20195;&#30721;&#37325;&#29992;&#12289;&#20195;&#30721;&#29702;&#35299;&#12289;&#37325;&#26500;&#21644;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#38750;&#24120;&#26377;&#29992;&#12290;OpenAI&#30340;GPT&#27169;&#22411;&#22312;&#36825;&#31181;&#20811;&#38534;&#29983;&#25104;&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;GPT&#34987;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#12290;&#24403;&#24320;&#21457;&#20154;&#21592;&#20174;Stack Overflow&#65288;SO&#65289;&#25110;&#31995;&#32479;&#20869;&#37096;&#22797;&#21046;/&#31896;&#36148;&#20195;&#30721;&#26102;&#65292;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#19968;&#33268;&#30340;&#26356;&#25913;&#23548;&#33268;&#24847;&#22806;&#34892;&#20026;&#12290;&#21516;&#26679;&#65292;&#22914;&#26524;&#26576;&#20154;&#25317;&#26377;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#20294;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#23547;&#25214;&#31561;&#25928;&#21151;&#33021;&#65292;&#21017;&#35821;&#20041;&#36328;&#35821;&#35328;&#20195;&#30721;&#20811;&#38534;&#29983;&#25104;&#26041;&#27861;&#21487;&#33021;&#20250;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#24110;&#21161;&#12290; &#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;SemanticCloneBench&#20316;&#20026;&#24037;&#20855;&#65292;&#35780;&#20272;&#20102;GPT-3&#27169;&#22411;&#22312;&#29983;&#25104;&#32473;&#23450;&#20195;&#30721;&#29255;&#27573;&#30340;&#35821;&#20041;&#21644;&#36328;&#35821;&#35328;&#20811;&#38534;&#21464;&#20307;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#24182;&#35780;&#20272;&#20102;GPT-3&#22312;&#29983;&#25104;&#20195;&#30721;&#21464;&#20307;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#36890;&#36807;9&#20301;&#27861;&#23448;&#33457;&#36153;&#20102;158&#20010;&#23567;&#26102;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic and Cross-language code clone generation may be useful for code reuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model has potential in such clone generation as GPT is used for text generation. When developers copy/paste codes from Stack Overflow (SO) or within a system, there might be inconsistent changes leading to unexpected behaviours. Similarly, if someone possesses a code snippet in a particular programming language but seeks equivalent functionality in a different language, a semantic cross-language code clone generation approach could provide valuable assistance.In this study, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3 model could help generate semantic and cross-language clone variants for a given fragment.We have comprised a diverse set of code fragments and assessed GPT-3s performance in generating code variants.Through extensive experimentation and analysis, where 9 judges spent 158 hours to validate, we investigate 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#39564;&#35777;&#21644;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#23376;&#31995;&#32479;&#32452;&#21512;&#36215;&#26469;&#23454;&#29616;&#25972;&#20307;&#20219;&#21153;&#12290;&#36890;&#36807;&#23450;&#20041;&#23376;&#31995;&#32479;&#20043;&#38388;&#30340;&#25509;&#21475;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#35268;&#33539;&#30340;&#33258;&#21160;&#20998;&#35299;&#65292;&#24182;&#20801;&#35768;&#23376;&#31995;&#32479;&#30340;&#29420;&#31435;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2309.06420</link><description>&lt;p&gt;
&#21487;&#39564;&#35777;&#30340;&#32452;&#21512;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Verifiable Reinforcement Learning Systems via Compositionality. (arXiv:2309.06420v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#39564;&#35777;&#21644;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#23376;&#31995;&#32479;&#32452;&#21512;&#36215;&#26469;&#23454;&#29616;&#25972;&#20307;&#20219;&#21153;&#12290;&#36890;&#36807;&#23450;&#20041;&#23376;&#31995;&#32479;&#20043;&#38388;&#30340;&#25509;&#21475;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#35268;&#33539;&#30340;&#33258;&#21160;&#20998;&#35299;&#65292;&#24182;&#20801;&#35768;&#23376;&#31995;&#32479;&#30340;&#29420;&#31435;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#39564;&#35777;&#21644;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#19968;&#32452;RL&#23376;&#31995;&#32479;&#34987;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#23436;&#25104;&#19968;&#20010;&#25972;&#20307;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#39640;&#32423;&#27169;&#22411;&#21644;&#19968;&#32452;&#20302;&#32423;&#23376;&#31995;&#32479;&#32452;&#25104;&#12290;&#39640;&#32423;&#27169;&#22411;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#29992;&#20110;&#35268;&#21010;&#21644;&#20998;&#26512;&#23376;&#31995;&#32479;&#30340;&#32452;&#21512;&#65292;&#32780;&#20302;&#32423;&#23376;&#31995;&#32479;&#21017;&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#25805;&#20316;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#23454;&#29616;&#12290;&#36890;&#36807;&#23450;&#20041;&#23376;&#31995;&#32479;&#20043;&#38388;&#30340;&#25509;&#21475;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#20998;&#35299;&#20219;&#21153;&#35268;&#33539;&#25104;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#20801;&#35768;&#23376;&#31995;&#32479;&#30340;&#29420;&#31435;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35770;&#32467;&#26524;&#20445;&#35777;&#20102;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for verifiable and compositional reinforcement learning (RL) in which a collection of RL subsystems, each of which learns to accomplish a separate subtask, are composed to achieve an overall task. The framework consists of a high-level model, represented as a parametric Markov decision process, which is used to plan and analyze compositions of subsystems, and of the collection of low-level subsystems themselves. The subsystems are implemented as deep RL agents operating under partial observability. By defining interfaces between the subsystems, the framework enables automatic decompositions of task specifications, e.g., reach a target set of states with a probability of at least 0.95, into individual subtask specifications, i.e. achieve the subsystem's exit conditions with at least some minimum probability, given that its entry conditions are met. This in turn allows for the independent training and testing of the subsystems. We present theoretical results guaran
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#20934;&#30830;&#23398;&#20064;&#20855;&#26377;&#20219;&#24847;&#31934;&#24230;&#30340;&#33258;&#28982;&#21442;&#25968;&#30340;&#25351;&#25968;&#26063;&#20998;&#24067;&#12290;&#35813;&#20272;&#35745;&#22120;&#26159;&#19968;&#33268;&#30340;&#12289;&#28176;&#36817;&#27491;&#24577;&#30340;&#65292;&#24182;&#21487;&#35270;&#20026;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2309.06413</link><description>&lt;p&gt;
&#35745;&#31639;&#26377;&#25928;&#23398;&#20064;&#25351;&#25968;&#26063;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
On Computationally Efficient Learning of Exponential Family Distributions. (arXiv:2309.06413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#20934;&#30830;&#23398;&#20064;&#20855;&#26377;&#20219;&#24847;&#31934;&#24230;&#30340;&#33258;&#28982;&#21442;&#25968;&#30340;&#25351;&#25968;&#26063;&#20998;&#24067;&#12290;&#35813;&#20272;&#35745;&#22120;&#26159;&#19968;&#33268;&#30340;&#12289;&#28176;&#36817;&#27491;&#24577;&#30340;&#65292;&#24182;&#21487;&#35270;&#20026;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20197;&#35745;&#31639;&#21644;&#32479;&#35745;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#20934;&#30830;&#23398;&#20064;&#20855;&#26377;&#20219;&#24847;&#31934;&#24230;&#30340;&#33258;&#28982;&#21442;&#25968;&#30340;$k$&#21442;&#25968;&#25130;&#26029;\textit{&#26368;&#23567;}&#25351;&#25968;&#26063;&#20998;&#24067;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#25903;&#25345;&#21644;&#33258;&#28982;&#21442;&#25968;&#36866;&#24403;&#26377;&#30028;&#30340;&#24773;&#20917;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#23545;&#20110;&#36825;&#31867;&#25351;&#25968;&#26063;&#20998;&#24067;&#26159;&#19968;&#33268;&#30340;&#12289;&#28176;&#36817;&#27491;&#24577;&#30340;&#21644;&#28176;&#36817;&#26377;&#25928;&#30340;&#65292;&#20294;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#65292;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#19968;&#33268;&#19988;&#28176;&#36817;&#27491;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#24635;&#20307;&#27700;&#24179;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#21516;&#19968;&#31867;&#25351;&#25968;&#26063;&#20998;&#24067;&#30340;&#21442;&#25968;&#21270;&#20998;&#24067;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#21487;&#20197;&#35299;&#37322;&#20026;&#26368;&#23567;&#21270;&#29305;&#23450;Bregman&#24471;&#20998;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the classical problem of learning, with arbitrary accuracy, the natural parameters of a $k$-parameter truncated \textit{minimal} exponential family from i.i.d. samples in a computationally and statistically efficient manner. We focus on the setting where the support as well as the natural parameters are appropriately bounded. While the traditional maximum likelihood estimator for this class of exponential family is consistent, asymptotically normal, and asymptotically efficient, evaluating it is computationally hard. In this work, we propose a novel loss function and a computationally efficient estimator that is consistent as well as asymptotically normal under mild conditions. We show that, at the population level, our method can be viewed as the maximum likelihood estimation of a re-parameterized distribution belonging to the same class of exponential family. Further, we show that our estimator can be interpreted as a solution to minimizing a particular Bregman score as w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#20171;&#32461;&#20102;&#35745;&#31639;&#26041;&#27861;&#22312;&#33647;&#29289;&#20877;&#23450;&#20301;&#20013;&#39044;&#27979;&#33647;&#29289;-&#30142;&#30149;&#20851;&#32852;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06388</link><description>&lt;p&gt;
&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;-&#30142;&#30149;&#20851;&#32852;&#30340;&#35745;&#31639;&#26041;&#27861;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Computational Approaches for Predicting Drug-Disease Associations: A Comprehensive Review. (arXiv:2309.06388v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06388
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#20171;&#32461;&#20102;&#35745;&#31639;&#26041;&#27861;&#22312;&#33647;&#29289;&#20877;&#23450;&#20301;&#20013;&#39044;&#27979;&#33647;&#29289;-&#30142;&#30149;&#20851;&#32852;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#20960;&#21313;&#24180;&#20013;&#65292;&#20256;&#32479;&#30340;&#33647;&#29289;&#30740;&#21457;&#38754;&#20020;&#30528;&#39640;&#25104;&#26412;&#12289;&#38271;&#21608;&#26399;&#21644;&#39640;&#39118;&#38505;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35768;&#22810;&#35745;&#31639;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;&#21644;&#30142;&#30149;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#36890;&#36807;&#33647;&#29289;&#20877;&#23450;&#20301;&#26469;&#38477;&#20302;&#26032;&#33647;&#30740;&#21457;&#30340;&#25104;&#26412;&#12289;&#21608;&#26399;&#21644;&#39118;&#38505;&#12290;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#39044;&#27979;&#33647;&#29289;-&#30142;&#30149;&#20851;&#32852;&#65292;&#21253;&#25324;&#33647;&#29289;&#21103;&#20316;&#29992;-&#30142;&#30149;&#20851;&#32852;&#12289;&#33647;&#29289;&#38774;&#28857;&#20851;&#32852;&#21644;miRNA-&#30142;&#30149;&#20851;&#32852;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;&#39044;&#27979;&#33647;&#29289;&#20877;&#23450;&#20301;&#20013;&#33647;&#29289;-&#30142;&#30149;&#20851;&#32852;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#36825;&#20123;&#26041;&#27861;&#20998;&#20026;&#20960;&#20010;&#31867;&#21035;&#65292;&#21253;&#25324;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#12289;&#22522;&#20110;&#30697;&#38453;&#30340;&#31639;&#27861;&#12289;&#25512;&#33616;&#31639;&#27861;&#12289;&#22522;&#20110;&#38142;&#25509;&#30340;&#25512;&#29702;&#31639;&#27861;&#20197;&#21450;&#25991;&#26412;&#25366;&#25496;&#21644;&#35821;&#20041;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24050;&#26377;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent decades, traditional drug research and development have been facing challenges such as high cost, long timelines, and high risks. To address these issues, many computational approaches have been suggested for predicting the relationship between drugs and diseases through drug repositioning, aiming to reduce the cost, development cycle, and risks associated with developing new drugs. Researchers have explored different computational methods to predict drug-disease associations, including drug side effects-disease associations, drug-target associations, and miRNAdisease associations. In this comprehensive review, we focus on recent advances in predicting drug-disease association methods for drug repositioning. We first categorize these methods into several groups, including neural network-based algorithms, matrixbased algorithms, recommendation algorithms, link-based reasoning algorithms, and text mining and semantic reasoning. Then, we compare the prediction performance of exi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65292;&#28789;&#27963;&#30340;&#25513;&#27169;&#21644;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#65292;&#20351;&#24471;&#19968;&#20010;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#24418;&#27169;&#22411;&#20013;&#21487;&#20197;&#29992;&#26469;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.06382</link><description>&lt;p&gt;
&#38598;&#25104;&#25513;&#27169;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Ensemble Mask Networks. (arXiv:2309.06382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65292;&#28789;&#27963;&#30340;&#25513;&#27169;&#21644;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#65292;&#20351;&#24471;&#19968;&#20010;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#24418;&#27169;&#22411;&#20013;&#21487;&#20197;&#29992;&#26469;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;$\mathbb{R}^n\rightarrow \mathbb{R}^n$&#30340;&#21069;&#39304;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#21527;&#65311;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#31181;&#26426;&#21046;&#65306;&#28789;&#27963;&#30340;&#25513;&#27169;&#29992;&#20110;&#25509;&#25910;&#30697;&#38453;&#36755;&#20837;&#65292;&#20197;&#21450;&#19968;&#31181;&#29420;&#29305;&#30340;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#20197;&#23562;&#37325;&#25513;&#27169;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#32593;&#32476;&#21487;&#20197;&#36817;&#20284;&#22266;&#23450;&#25805;&#20316;&#65292;&#22914;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;$\phi(A,x) \rightarrow Ax$&#65292;&#36825;&#28608;&#21457;&#20102;&#24341;&#20837;&#30340;&#26426;&#21046;&#22312;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20013;&#27979;&#35797;&#20381;&#36182;&#20851;&#31995;&#25110;&#20132;&#20114;&#39034;&#24207;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can an $\mathbb{R}^n\rightarrow \mathbb{R}^n$ feedforward network learn matrix-vector multiplication? This study introduces two mechanisms - flexible masking to take matrix inputs, and a unique network pruning to respect the mask's dependency structure. Networks can approximate fixed operations such as matrix-vector multiplication $\phi(A,x) \rightarrow Ax$, motivating the mechanisms introduced with applications towards litmus-testing dependencies or interaction order in graph-based models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InstaFlow&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#36716;&#21270;&#20026;&#19968;&#27493;&#27169;&#22411;&#65292;&#36890;&#36807;&#20462;&#27491;&#30340;&#27969;&#21160;&#26041;&#27861;&#25552;&#39640;&#20102;&#22122;&#22768;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#12289;&#39640;&#36895;&#24230;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.06380</link><description>&lt;p&gt;
InstaFlow: &#19968;&#27493;&#21363;&#21487;&#23454;&#29616;&#39640;&#36136;&#37327;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation. (arXiv:2309.06380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InstaFlow&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#36716;&#21270;&#20026;&#19968;&#27493;&#27169;&#22411;&#65292;&#36890;&#36807;&#20462;&#27491;&#30340;&#27969;&#21160;&#26041;&#27861;&#25552;&#39640;&#20102;&#22122;&#22768;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#12289;&#39640;&#36895;&#24230;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#20986;&#33394;&#30340;&#36136;&#37327;&#21644;&#21019;&#36896;&#21147;&#24443;&#24213;&#25913;&#21464;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20854;&#22810;&#27493;&#37319;&#26679;&#36807;&#31243;&#34987;&#35748;&#20026;&#24456;&#24930;&#65292;&#36890;&#24120;&#38656;&#35201;&#21313;&#20960;&#27493;&#25512;&#26029;&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20197;&#24448;&#35797;&#22270;&#36890;&#36807;&#33976;&#39311;&#26469;&#25552;&#39640;&#37319;&#26679;&#36895;&#24230;&#21644;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#23581;&#35797;&#37117;&#26410;&#33021;&#23454;&#29616;&#21151;&#33021;&#40784;&#20840;&#30340;&#19968;&#27493;&#27169;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26368;&#36817;&#30340;&#26041;&#27861;&#65292;&#21363;&#20462;&#27491;&#30340;&#27969;&#21160;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21040;&#30446;&#21069;&#20026;&#27490;&#21482;&#24212;&#29992;&#20110;&#23567;&#25968;&#25454;&#38598;&#12290;&#20462;&#27491;&#30340;&#27969;&#21160;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#20854;&#37325;&#26032;&#27969;&#21160;&#30340;&#36807;&#31243;&#65292;&#23427;&#23558;&#27010;&#29575;&#27969;&#30340;&#36712;&#36857;&#21464;&#24471;&#30452;&#32447;&#65292;&#25913;&#36827;&#20102;&#22122;&#22768;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#32806;&#21512;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#23398;&#29983;&#27169;&#22411;&#20415;&#20110;&#33976;&#39311;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#26465;&#20214;&#30340;&#27969;&#31243;&#65292;&#23558;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65288;SD&#65289;&#36716;&#21270;&#20026;&#36229;&#24555;&#36895;&#30340;&#19968;&#27493;&#27169;&#22411;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#21457;&#29616;&#37325;&#26032;&#27969;&#21160;&#22312;&#25913;&#21892;&#22122;&#22768;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20973;&#20511;&#25105;&#20204;&#30340;&#26032;&#27969;&#31243;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#36739;&#24555;&#30340;&#36895;&#24230;&#30452;&#25509;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce computational costs through distillation have been unsuccessful in achieving a functional one-step model. In this paper, we explore a recent method called Rectified Flow, which, thus far, has only been applied to small datasets. The core of Rectified Flow lies in its \emph{reflow} procedure, which straightens the trajectories of probability flows, refines the coupling between noises and images, and facilitates the distillation process with student models. We propose a novel text-conditioned pipeline to turn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find reflow plays a critical role in improving the assignment between noise and images. Leveraging our new pipeline, w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#30284;&#30151;&#26816;&#27979;&#30340;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20256;&#32479;&#27169;&#22411;&#21644;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#23545;&#27604;&#20998;&#26512;&#24615;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06377</link><description>&lt;p&gt;
&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#30284;&#30151;&#26816;&#27979;&#30340;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks on hybrid classical-quantum Deep Learning models for Histopathological Cancer Detection. (arXiv:2309.06377v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#30284;&#30151;&#26816;&#27979;&#30340;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20256;&#32479;&#27169;&#22411;&#21644;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#23545;&#27604;&#20998;&#26512;&#24615;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#30284;&#30151;&#26816;&#27979;&#20013;&#25552;&#20986;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20004;&#20010;&#20027;&#35201;&#24212;&#29992;&#12290;&#31532;&#19968;&#20010;&#24212;&#29992;&#26159;&#20351;&#29992;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#31574;&#30053;&#26500;&#24314;&#32452;&#32455;&#30149;&#29702;&#23398;&#30284;&#30151;&#26816;&#27979;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#31532;&#20108;&#20010;&#24212;&#29992;&#26159;&#27979;&#35797;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#21333;&#20010;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#27979;&#35797;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#27169;&#22411;&#65292;&#36824;&#23588;&#20854;&#20351;&#29992;ResNet18&#12289;VGG-16&#12289;Inception-v3&#21644;AlexNet&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#38598;&#25104;&#20102;&#22810;&#20010;&#22522;&#20110;&#37327;&#23376;&#30005;&#36335;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65288;VQC&#65289;&#20197;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#32463;&#20856;&#27169;&#22411;&#21644;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#30284;&#30151;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#65292;&#32771;&#34385;&#20102;&#22810;&#31181;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an effective application of quantum machine learning in histopathological cancer detection. The study here emphasizes two primary applications of hybrid classical-quantum Deep Learning models. The first application is to build a classification model for histopathological cancer detection using the quantum transfer learning strategy. The second application is to test the performance of this model for various adversarial attacks. Rather than using a single transfer learning model, the hybrid classical-quantum models are tested using multiple transfer learning models, especially ResNet18, VGG-16, Inception-v3, and AlexNet as feature extractors and integrate it with several quantum circuit-based variational quantum circuits (VQC) with high expressibility. As a result, we provide a comparative analysis of classical models and hybrid classical-quantum transfer learning models for histopathological cancer detection under several adversarial attacks. We compared the performance accu
&lt;/p&gt;</description></item><item><title>&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#38656;&#35201;&#32771;&#34385;&#21442;&#19982;&#32773;&#28608;&#21169;&#12289;&#34892;&#20026;&#20197;&#21450;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65292;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#26435;&#34913;&#65292;&#24182;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;</title><link>http://arxiv.org/abs/2309.06375</link><description>&lt;p&gt;
&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#65306;&#26426;&#21046;&#35774;&#35745;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20132;&#21449;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models. (arXiv:2309.06375v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06375
&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#38656;&#35201;&#32771;&#34385;&#21442;&#19982;&#32773;&#28608;&#21169;&#12289;&#34892;&#20026;&#20197;&#21450;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65292;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#26435;&#34913;&#65292;&#24182;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20301;&#20110;&#28085;&#30422;&#29992;&#25143;&#12289;&#20869;&#23481;&#25552;&#20379;&#21830;&#12289;&#24191;&#21578;&#21830;&#21644;&#20854;&#20182;&#21442;&#19982;&#32773;&#34892;&#20026;&#30340;&#22797;&#26434;&#29983;&#24577;&#31995;&#32479;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#30340;&#37325;&#28857;&#65292;&#20197;&#21450;&#22823;&#22810;&#25968;&#37325;&#35201;&#23454;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#20165;&#38480;&#20110;&#20010;&#21035;&#29992;&#25143;&#25512;&#33616;&#30340;&#23616;&#37096;&#12289;&#30701;&#35270;&#20248;&#21270;&#12290;&#36825;&#32473;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#20026;&#29992;&#25143;&#24102;&#26469;&#30340;&#38271;&#26399;&#25928;&#29992;&#24102;&#26469;&#20102;&#37325;&#22823;&#25104;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#35201;&#26368;&#22823;&#21270;&#31995;&#32479;&#23545;&#36825;&#20123;&#21442;&#19982;&#32773;&#30340;&#20215;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#29983;&#24577;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#29366;&#20917;&#65292;&#26377;&#24517;&#35201;&#26126;&#30830;&#22320;&#23545;&#31995;&#32479;&#20013;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#28608;&#21169;&#21644;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23545;&#20854;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#27492;&#38656;&#35201;&#65306;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31561;&#25216;&#26415;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65307;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#20026;&#19981;&#21516;&#21442;&#19982;&#32773;&#30340;&#25928;&#29992;&#36827;&#34892;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#65307;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems lie at the heart of complex ecosystems that couple the behavior of users, content providers, advertisers, and other actors. Despite this, the focus of the majority of recommender research -- and most practical recommenders of any import -- is on the local, myopic optimization of the recommendations made to individual users. This comes at a significant cost to the long-term utility that recommenders could generate for its users. We argue that explicitly modeling the incentives and behaviors of all actors in the system -- and the interactions among them induced by the recommender's policy -- is strictly necessary if one is to maximize the value the system brings to these actors and improve overall ecosystem "health". Doing so requires: optimization over long horizons using techniques such as reinforcement learning; making inevitable tradeoffs in the utility that can be generated for different actors using the methods of social choice; reducing information asymm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Reed-Muller&#30721;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32416;&#27491;&#21644;&#25298;&#32477;&#36755;&#20837;&#65292;&#24182;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#23545;&#25239;&#25915;&#20987;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06359</link><description>&lt;p&gt;
&#20351;&#29992;Reed-Muller&#30721;&#36827;&#34892;&#20855;&#26377;&#25298;&#32477;&#21644;&#24674;&#22797;&#21151;&#33021;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Using Reed-Muller Codes for Classification with Rejection and Recovery. (arXiv:2309.06359v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Reed-Muller&#30721;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32416;&#27491;&#21644;&#25298;&#32477;&#36755;&#20837;&#65292;&#24182;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#23545;&#25239;&#25915;&#20987;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#37096;&#32626;&#20998;&#31867;&#22120;&#26102;&#65292;&#29992;&#25143;&#24076;&#26395;&#23427;&#20204;&#33021;&#22815;&#36866;&#24403;&#22320;&#21709;&#24212;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20998;&#31867;&#22120;&#26080;&#27861;&#22788;&#29702;&#36828;&#31163;&#20854;&#25152;&#35757;&#32451;&#20998;&#24067;&#30340;&#36755;&#20837;&#12290;&#24694;&#24847;&#34892;&#20026;&#32773;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#32570;&#38519;&#65292;&#36890;&#36807;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#25200;&#21160;&#26469;&#23548;&#33268;&#20998;&#31867;&#22120;&#32473;&#20986;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#25298;&#32477;&#20998;&#31867;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#20801;&#35768;&#32593;&#32476;&#22312;&#23545;&#36755;&#20837;&#30340;&#32622;&#20449;&#24230;&#36739;&#20302;&#26102;&#25298;&#32477;&#36827;&#34892;&#20998;&#31867;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#23545;&#20110;&#24378;&#23545;&#25239;&#26679;&#26412;&#25928;&#26524;&#24456;&#22909;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#23545;&#36731;&#24494;&#25200;&#21160;&#22270;&#20687;&#30340;&#25298;&#32477;&#65292;&#32780;&#30452;&#35266;&#19978;&#36825;&#20123;&#22270;&#20687;&#21487;&#33021;&#26159;&#21487;&#20197;&#27491;&#30830;&#20998;&#31867;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Reed-Muller&#32858;&#21512;&#32593;&#32476;&#65288;RMAggNet&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;Reed-Muller&#32416;&#38169;&#30721;&#21551;&#21457;&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#32416;&#27491;&#21644;&#25298;&#32477;&#36755;&#20837;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;RMAggNet&#21487;&#20197;&#22312;&#22810;&#20010;&#19981;&#21516;&#31243;&#24230;&#30340;&#23545;&#25239;&#25915;&#20987;&#19979;&#65292;&#26368;&#23567;&#21270;&#38169;&#35823;&#24182;&#20445;&#25345;&#24456;&#22909;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying classifiers in the real world, users expect them to respond to inputs appropriately. However, traditional classifiers are not equipped to handle inputs which lie far from the distribution they were trained on. Malicious actors can exploit this defect by making adversarial perturbations designed to cause the classifier to give an incorrect output. Classification-with-rejection methods attempt to solve this problem by allowing networks to refuse to classify an input in which they have low confidence. This works well for strongly adversarial examples, but also leads to the rejection of weakly perturbed images, which intuitively could be correctly classified. To address these issues, we propose Reed-Muller Aggregation Networks (RMAggNet), a classifier inspired by Reed-Muller error-correction codes which can correct and reject inputs. This paper shows that RMAggNet can minimise incorrectness while maintaining good correctness over multiple adversarial attacks at different per
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#20998;&#25968;&#21518;&#39564;&#27010;&#29575;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#20041;&#36951;&#25022;&#20998;&#26512;&#65292;&#33719;&#24471;&#20102;&#20381;&#36182;&#20110;&#23454;&#20363;&#21644;&#23454;&#20363;&#29420;&#31435;&#30340;&#39057;&#29575;&#36951;&#25022;&#30028;&#12290;&#36825;&#23545;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#35299;&#20915;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06349</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#25968;&#21518;&#39564;&#27010;&#29575;&#23545;&#27748;&#26222;&#26862;&#25277;&#26679;&#36827;&#34892;&#24191;&#20041;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generalized Regret Analysis of Thompson Sampling using Fractional Posteriors. (arXiv:2309.06349v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06349
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#20998;&#25968;&#21518;&#39564;&#27010;&#29575;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#20041;&#36951;&#25022;&#20998;&#26512;&#65292;&#33719;&#24471;&#20102;&#20381;&#36182;&#20110;&#23454;&#20363;&#21644;&#23454;&#20363;&#29420;&#31435;&#30340;&#39057;&#29575;&#36951;&#25022;&#30028;&#12290;&#36825;&#23545;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#35299;&#20915;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;TS&#65289;&#26159;&#35299;&#20915;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#26368;&#27969;&#34892;&#21644;&#26368;&#26089;&#30340;&#31639;&#27861;&#20043;&#19968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;TS&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#31216;&#20026;&#945;-TS&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#20998;&#25968;&#25110;&#945;-&#21518;&#39564;&#65288;&#945;&#8712;&#65288;0,1&#65289;&#65289;&#20195;&#26367;&#26631;&#20934;&#21518;&#39564;&#20998;&#24067;&#12290;&#20026;&#20102;&#35745;&#31639;&#945;-&#21518;&#39564;&#65292;&#26631;&#20934;&#21518;&#39564;&#30340;&#23450;&#20041;&#20013;&#30340;&#20284;&#28982;&#20989;&#25968;&#34987;&#19968;&#20010;&#22240;&#23376;&#945;&#25605;&#25292;&#12290;&#23545;&#20110;&#945;-TS&#65292;&#25105;&#20204;&#22312;&#38750;&#24120;&#28201;&#21644;&#30340;&#20808;&#39564;&#21644;&#22870;&#21169;&#20998;&#24067;&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#26082;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#927;&#65288;&#8721;_{k&#8800;i^*}&#916;_k&#65288;\frac{\log(T)}{C(&#945;)&#916;_k^2}+\frac{1}{2}&#65289;&#65289;&#20063;&#20381;&#36182;&#20110;&#23454;&#20363;&#29420;&#31435;&#30340;&#927;&#65288;\sqrt{KT\log K}&#65289;&#39057;&#29575;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;&#916;_k&#26159;&#31532;k&#20010;&#21644;&#26368;&#22909;&#30340;&#33218;&#30340;&#30495;&#23454;&#22343;&#20540;&#22870;&#21169;&#20043;&#38388;&#30340;&#24046;&#65292;&#32780;C(&#945;)&#26159;&#24050;&#30693;&#30340;&#24120;&#25968;&#12290;&#23376;&#39640;&#26031;&#21644;&#25351;&#25968;&#26063;&#27169;&#22411;&#37117;&#28385;&#36275;&#25105;&#20204;&#23545;&#22870;&#21169;&#20998;&#24067;&#30340;&#19968;&#33324;&#26465;&#20214;&#12290;&#25105;&#20204;&#23545;&#20808;&#39564;&#30340;&#26465;&#20214;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Thompson sampling (TS) is one of the most popular and earliest algorithms to solve stochastic multi-armed bandit problems. We consider a variant of TS, named $\alpha$-TS, where we use a fractional or $\alpha$-posterior ($\alpha\in(0,1)$) instead of the standard posterior distribution. To compute an $\alpha$-posterior, the likelihood in the definition of the standard posterior is tempered with a factor $\alpha$. For $\alpha$-TS we obtain both instance-dependent $\mathcal{O}\left(\sum_{k \neq i^*} \Delta_k\left(\frac{\log(T)}{C(\alpha)\Delta_k^2} + \frac{1}{2} \right)\right)$ and instance-independent $\mathcal{O}(\sqrt{KT\log K})$ frequentist regret bounds under very mild conditions on the prior and reward distributions, where $\Delta_k$ is the gap between the true mean rewards of the $k^{th}$ and the best arms, and $C(\alpha)$ is a known constant. Both the sub-Gaussian and exponential family models satisfy our general conditions on the reward distribution. Our conditions on the prior di
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;&#20248;&#21270;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#36827;&#34892;&#24102;&#38553;&#22238;&#24402;&#39044;&#27979;&#65292;&#36890;&#36807;&#23545;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#37329;&#23646;&#26448;&#26009;&#24102;&#38553;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.06348</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24102;&#38553;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Band-gap regression with architecture-optimized message-passing neural networks. (arXiv:2309.06348v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06348
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;&#20248;&#21270;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#36827;&#34892;&#24102;&#38553;&#22238;&#24402;&#39044;&#27979;&#65292;&#36890;&#36807;&#23545;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#37329;&#23646;&#26448;&#26009;&#24102;&#38553;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#65292;&#22312;&#39044;&#27979;&#22266;&#20307;&#26448;&#26009;&#30340;&#29289;&#29702;&#24615;&#36136;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#26469;&#33258;AFLOW&#25968;&#25454;&#24211;&#30340;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#25968;&#25454;&#36890;&#36807;MPNN&#23545;&#26448;&#26009;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#20854;&#37329;&#23646;&#25110;&#21322;&#23548;&#20307;/&#32477;&#32536;&#20307;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#25506;&#32034;MPNN&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#31354;&#38388;&#65292;&#20197;&#39044;&#27979;&#34987;&#35782;&#21035;&#20026;&#38750;&#37329;&#23646;&#26448;&#26009;&#30340;&#24102;&#38553;&#12290;&#25628;&#32034;&#20013;&#30340;&#21442;&#25968;&#21253;&#25324;&#28040;&#24687;&#20256;&#36882;&#27493;&#39588;&#30340;&#25968;&#37327;&#12289;&#28508;&#22312;&#31354;&#38388;&#30340;&#22823;&#23567;&#21644;&#28608;&#27963;&#20989;&#25968;&#31561;&#12290;&#20174;&#25628;&#32034;&#20013;&#31579;&#36873;&#20986;&#30340;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#34987;&#27719;&#24635;&#25104;&#19968;&#20010;&#38598;&#21512;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;Dropout&#21644;&#38598;&#25104;&#26041;&#27861;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#38598;&#25104;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38598;&#25104;&#27169;&#22411;&#22312;&#26230;&#20307;&#31995;&#32479;&#12289;&#26230;&#26684;&#23610;&#23544;&#21644;&#20854;&#20182;&#26448;&#26009;&#29305;&#24615;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based neural networks and, specifically, message-passing neural networks (MPNNs) have shown great potential in predicting physical properties of solids. In this work, we train an MPNN to first classify materials through density functional theory data from the AFLOW database as being metallic or semiconducting/insulating. We then perform a neural-architecture search to explore the model architecture and hyperparameter space of MPNNs to predict the band gaps of the materials identified as non-metals. The parameters in the search include the number of message-passing steps, latent size, and activation-function, among others. The top-performing models from the search are pooled into an ensemble that significantly outperforms existing models from the literature. Uncertainty quantification is evaluated with Monte-Carlo Dropout and ensembling, with the ensemble method proving superior. The domain of applicability of the ensemble model is analyzed with respect to the crystal systems, the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#21453;&#39304;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#26469;&#23398;&#20064;&#26368;&#31616;&#21270;&#30340;Tsetlin&#26426;&#22120;&#23376;&#21477;&#65292;&#20197;&#25552;&#20379;&#26368;&#20339;&#30340;&#29305;&#24449;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.06315</link><description>&lt;p&gt;
&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#24341;&#23548;&#20462;&#21098;&#23398;&#20064;&#26368;&#31616;&#21270;Tsetlin&#26426;&#22120;&#23376;&#21477;
&lt;/p&gt;
&lt;p&gt;
Learning Minimalistic Tsetlin Machine Clauses with Markov Boundary-Guided Pruning. (arXiv:2309.06315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#21453;&#39304;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#26469;&#23398;&#20064;&#26368;&#31616;&#21270;&#30340;Tsetlin&#26426;&#22120;&#23376;&#21477;&#65292;&#20197;&#25552;&#20379;&#26368;&#20339;&#30340;&#29305;&#24449;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#21253;&#21547;&#20102;&#39044;&#27979;&#21464;&#37327;&#25152;&#38656;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#37027;&#20040;&#19968;&#32452;&#21464;&#37327;&#23601;&#26159;&#38543;&#26426;&#21464;&#37327;&#30340;&#39532;&#23572;&#31185;&#22827;&#30422;&#34987;&#12290;&#22914;&#26524;&#30422;&#34987;&#26080;&#27861;&#20943;&#23569;&#32780;&#19981;&#20002;&#22833;&#26377;&#29992;&#20449;&#24687;&#65292;&#21017;&#34987;&#31216;&#20026;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#12290;&#35782;&#21035;&#38543;&#26426;&#21464;&#37327;&#30340;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#26159;&#26377;&#20248;&#21183;&#30340;&#65292;&#22240;&#20026;&#36793;&#30028;&#22806;&#30340;&#25152;&#26377;&#21464;&#37327;&#37117;&#26159;&#22810;&#20313;&#30340;&#12290;&#22240;&#27492;&#65292;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#25552;&#20379;&#20102;&#26368;&#20339;&#30340;&#29305;&#24449;&#38598;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#20855;&#26377;&#20004;&#20010;&#25361;&#25112;&#12290;&#22914;&#26524;&#20174;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#20013;&#31227;&#38500;&#19968;&#20010;&#25110;&#22810;&#20010;&#21464;&#37327;&#65292;&#36793;&#30028;&#22806;&#30340;&#21464;&#37327;&#21487;&#33021;&#24320;&#22987;&#25552;&#20379;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#36793;&#30028;&#20869;&#30340;&#21464;&#37327;&#21487;&#33021;&#20572;&#27490;&#25552;&#20379;&#20449;&#24687;&#12290;&#27599;&#20010;&#20505;&#36873;&#21464;&#37327;&#30340;&#30495;&#27491;&#20316;&#29992;&#21482;&#26377;&#22312;&#35782;&#21035;&#20102;&#39532;&#23572;&#31185;&#22827;&#36793;&#30028;&#21518;&#25165;&#20250;&#26174;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#21453;&#39304;&#26041;&#26696;&#65292;&#20197;&#34917;&#20805;&#31867;&#22411;I&#21644;&#31867;&#22411;II&#30340;&#21453;&#39304;&#12290;&#35813;&#26041;&#26696;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426; - &#19968;&#31181;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#26426;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
A set of variables is the Markov blanket of a random variable if it contains all the information needed for predicting the variable. If the blanket cannot be reduced without losing useful information, it is called a Markov boundary. Identifying the Markov boundary of a random variable is advantageous because all variables outside the boundary are superfluous. Hence, the Markov boundary provides an optimal feature set. However, learning the Markov boundary from data is challenging for two reasons. If one or more variables are removed from the Markov boundary, variables outside the boundary may start providing information. Conversely, variables within the boundary may stop providing information. The true role of each candidate variable is only manifesting when the Markov boundary has been identified. In this paper, we propose a new Tsetlin Machine (TM) feedback scheme that supplements Type I and Type II feedback. The scheme introduces a novel Finite State Automaton - a Context-Specific I
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#34892;&#39542;&#36710;&#36742;&#19978;&#36827;&#34892;&#35821;&#20041;&#21644;&#20851;&#33410;&#20154;&#20307;&#24863;&#30693;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;&#36710;&#36733;&#35270;&#39057;&#30340;&#22823;&#37327;&#21069;&#36827;&#36816;&#21160;&#23548;&#33268;&#38590;&#20197;&#36827;&#34892;3D&#37325;&#24314;&#65292;&#20197;&#21450;&#29289;&#20307;&#26816;&#27979;&#21644;&#20154;&#20307;&#24863;&#30693;&#27169;&#22411;&#22312;&#36710;&#36733;&#35270;&#39057;&#19978;&#34920;&#29616;&#36739;&#24046;&#30340;&#21407;&#22240;&#12290;&#20316;&#32773;&#25552;&#20986;&#36890;&#36807;LiDAR&#25968;&#25454;&#36827;&#34892;&#20851;&#33410;&#20154;&#20307;&#24863;&#30693;&#30340;&#22522;&#20934;&#21487;&#20197;&#20419;&#36827;&#20154;&#20307;&#24863;&#30693;&#21644;&#20132;&#36890;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#34892;&#20154;&#20132;&#36890;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06313</link><description>&lt;p&gt;
&#22312;&#34892;&#39542;&#36710;&#36742;&#19978;&#30340;&#35821;&#20041;&#21644;&#20851;&#33410;&#20154;&#20307;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Semantic and Articulated Pedestrian Sensing Onboard a Moving Vehicle. (arXiv:2309.06313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#34892;&#39542;&#36710;&#36742;&#19978;&#36827;&#34892;&#35821;&#20041;&#21644;&#20851;&#33410;&#20154;&#20307;&#24863;&#30693;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;&#36710;&#36733;&#35270;&#39057;&#30340;&#22823;&#37327;&#21069;&#36827;&#36816;&#21160;&#23548;&#33268;&#38590;&#20197;&#36827;&#34892;3D&#37325;&#24314;&#65292;&#20197;&#21450;&#29289;&#20307;&#26816;&#27979;&#21644;&#20154;&#20307;&#24863;&#30693;&#27169;&#22411;&#22312;&#36710;&#36733;&#35270;&#39057;&#19978;&#34920;&#29616;&#36739;&#24046;&#30340;&#21407;&#22240;&#12290;&#20316;&#32773;&#25552;&#20986;&#36890;&#36807;LiDAR&#25968;&#25454;&#36827;&#34892;&#20851;&#33410;&#20154;&#20307;&#24863;&#30693;&#30340;&#22522;&#20934;&#21487;&#20197;&#20419;&#36827;&#20154;&#20307;&#24863;&#30693;&#21644;&#20132;&#36890;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#34892;&#20154;&#20132;&#36890;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36710;&#36742;&#30340;&#22823;&#37327;&#21069;&#36827;&#36816;&#21160;&#65292;&#20174;&#36710;&#36733;&#35270;&#39057;&#20013;&#36827;&#34892;3D&#37325;&#24314;&#38750;&#24120;&#22256;&#38590;&#12290;&#21363;&#20351;&#19982;&#26631;&#20934;&#22522;&#20934;&#30456;&#27604;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20154;&#20307;&#24863;&#30693;&#27169;&#22411;&#22312;&#36710;&#36733;&#35270;&#39057;&#19978;&#30340;&#34920;&#29616;&#20063;&#26126;&#26174;&#36739;&#24046;&#65292;&#22240;&#20026;&#19982;&#26631;&#20934;&#29289;&#20307;&#26816;&#27979;&#22522;&#20934;&#30456;&#27604;&#65292;&#29289;&#20307;&#24448;&#24448;&#36317;&#31163;&#25668;&#20687;&#26426;&#36739;&#36828;&#65292;&#22270;&#20687;&#36136;&#37327;&#24120;&#24120;&#21463;&#21040;&#36816;&#21160;&#27169;&#31946;&#30340;&#24433;&#21709;&#65292;&#19988;&#32463;&#24120;&#21457;&#29983;&#36974;&#25377;&#12290;&#36825;&#23548;&#33268;&#20132;&#36890;&#25968;&#25454;&#29305;&#23450;&#30340;&#22522;&#20934;&#30340;&#26222;&#21450;&#12290;&#26368;&#36817;&#65292;&#20809;&#26816;&#27979;&#19982;&#27979;&#36317;&#65288;LiDAR&#65289;&#20256;&#24863;&#22120;&#24050;&#32463;&#22312;&#30452;&#25509;&#20272;&#35745;&#28145;&#24230;&#26041;&#38754;&#21464;&#24471;&#27969;&#34892;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;3D&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#19982;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;LiDAR&#30340;&#26041;&#27861;&#22312;&#36828;&#31243;&#20851;&#33410;&#20154;&#20307;&#26816;&#27979;&#26041;&#38754;&#20173;&#28982;&#19981;&#36275;&#12290;&#25105;&#20204;&#20551;&#35774;&#38024;&#23545;LiDAR&#25968;&#25454;&#20013;&#30340;&#20851;&#33410;&#20154;&#20307;&#24863;&#30693;&#30340;&#22522;&#20934;&#21487;&#20197;&#20419;&#36827;&#20154;&#20307;&#24863;&#30693;&#21644;&#20132;&#36890;&#39044;&#27979;&#26041;&#38754;&#30340;&#22686;&#21152;&#30740;&#31350;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#34892;&#20154;&#20132;&#36890;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is difficult to perform 3D reconstruction from on-vehicle gathered video due to the large forward motion of the vehicle. Even object detection and human sensing models perform significantly worse on onboard videos when compared to standard benchmarks because objects often appear far away from the camera compared to the standard object detection benchmarks, image quality is often decreased by motion blur and occlusions occur often. This has led to the popularisation of traffic data-specific benchmarks. Recently Light Detection And Ranging (LiDAR) sensors have become popular to directly estimate depths without the need to perform 3D reconstructions. However, LiDAR-based methods still lack in articulated human detection at a distance when compared to image-based methods. We hypothesize that benchmarks targeted at articulated human sensing from LiDAR data could bring about increased research in human sensing and prediction in traffic and could lead to improved traffic safety for pedestr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#24314;&#31435;&#20102;&#20379;&#38656;&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25581;&#31034;&#20102;&#36816;&#33829;&#26381;&#21153;&#20013;&#30340;&#31354;&#32570;&#12290;</title><link>http://arxiv.org/abs/2309.06299</link><description>&lt;p&gt;
&#22312;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#24314;&#27169;&#20379;&#38656;
&lt;/p&gt;
&lt;p&gt;
Modeling Supply and Demand in Public Transportation Systems. (arXiv:2309.06299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#24314;&#31435;&#20102;&#20379;&#38656;&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25581;&#31034;&#20102;&#36816;&#33829;&#26381;&#21153;&#20013;&#30340;&#31354;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21704;&#37324;&#26862;&#22561;&#20844;&#20849;&#20132;&#36890;&#37096;&#38376;&#26088;&#22312;&#21033;&#29992;&#20854;&#25968;&#25454;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#20379;&#38656;&#27169;&#22411;&#65292;&#24110;&#21161;&#37096;&#38376;&#35782;&#21035;&#26381;&#21153;&#20013;&#30340;&#31354;&#32570;&#12290;&#27169;&#22411;&#32771;&#34385;&#20102;&#35768;&#22810;&#21464;&#37327;&#65292;&#21253;&#25324;&#21704;&#37324;&#26862;&#22561;&#24066;&#21521;&#32852;&#37030;&#25919;&#24220;&#25253;&#21578;&#30340;&#26041;&#24335;&#20197;&#21450;&#26368;&#33030;&#24369;&#20154;&#21475;&#32858;&#38598;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Harrisonburg Department of Public Transportation (HDPT) aims to leverage their data to improve the efficiency and effectiveness of their operations. We construct two supply and demand models that help the department identify gaps in their service. The models take many variables into account, including the way that the HDPT reports to the federal government and the areas with the most vulnerable populations in Harrisonburg City. We employ data analysis and machine learning techniques to make our predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#30693;&#35782;&#36801;&#31227;&#24615;&#20998;&#26512;&#26694;&#26550;&#26469;&#25903;&#25345;&#25968;&#25454;&#39537;&#21160;&#30340;&#22686;&#26448;&#21046;&#36896;&#30693;&#35782;&#30340;&#36801;&#31227;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#21508;&#31181;&#22686;&#26448;&#21046;&#36896;&#25216;&#26415;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20174;&#19968;&#20010;&#24037;&#33402;&#25110;&#38382;&#39064;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#24037;&#33402;&#25110;&#38382;&#39064;&#20013;&#12290;</title><link>http://arxiv.org/abs/2309.06286</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#22686;&#26448;&#21046;&#36896;&#30693;&#35782;&#30340;&#21487;&#36801;&#31227;&#24615;&#20998;&#26512;&#65306;&#31881;&#24202;&#29076;&#21270;&#21644;&#23450;&#21521;&#33021;&#37327;&#27785;&#31215;&#20043;&#38388;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition. (arXiv:2309.06286v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#30693;&#35782;&#36801;&#31227;&#24615;&#20998;&#26512;&#26694;&#26550;&#26469;&#25903;&#25345;&#25968;&#25454;&#39537;&#21160;&#30340;&#22686;&#26448;&#21046;&#36896;&#30693;&#35782;&#30340;&#36801;&#31227;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#21508;&#31181;&#22686;&#26448;&#21046;&#36896;&#25216;&#26415;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20174;&#19968;&#20010;&#24037;&#33402;&#25110;&#38382;&#39064;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#24037;&#33402;&#25110;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22686;&#26448;&#21046;&#36896;&#65288;AM&#65289;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#31185;&#23398;&#25991;&#29486;&#12290;&#36825;&#20123;&#30740;&#31350;&#20013;&#30340;&#30693;&#35782;&#28041;&#21450;&#21040;AM&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#65292;&#20294;&#27809;&#26377;&#20197;&#19968;&#31181;&#25972;&#21512;&#30340;&#26041;&#24335;&#36827;&#34892;&#25366;&#25496;&#21644;&#24418;&#24335;&#21270;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#27809;&#26377;&#25903;&#25345;&#25968;&#25454;&#39537;&#21160;&#30693;&#35782;&#20174;&#19968;&#20010;&#19978;&#19979;&#25991;&#36801;&#31227;&#21040;&#21478;&#19968;&#20010;&#19978;&#19979;&#25991;&#30340;&#24037;&#20855;&#25110;&#25351;&#21335;&#12290;&#22240;&#27492;&#65292;&#20165;&#38024;&#23545;&#29305;&#23450;&#30340;AM&#24037;&#33402;&#25216;&#26415;&#24320;&#21457;&#21644;&#39564;&#35777;&#20102;&#29305;&#23450;&#30340;AI&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#12290;&#26377;&#28508;&#21147;&#21033;&#29992;&#21508;&#31181;AM&#25216;&#26415;&#20043;&#38388;&#30340;&#20869;&#22312;&#30456;&#20284;&#24615;&#65292;&#21033;&#29992;AI&#65288;&#22914;&#36801;&#31227;&#23398;&#20064;&#65289;&#20174;&#19968;&#20010;&#24037;&#33402;&#25110;&#38382;&#39064;&#20013;&#36866;&#24212;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#30693;&#35782;&#36801;&#31227;&#24615;&#20998;&#26512;&#26694;&#26550;&#26469;&#25903;&#25345;&#25968;&#25454;&#39537;&#21160;&#30340;AM&#30693;&#35782;&#36801;&#31227;&#12290;&#20316;&#20026;&#36801;&#31227;&#24615;&#20998;&#26512;&#30340;&#20808;&#20915;&#26465;&#20214;&#65292;AM&#30693;&#35782;&#34987;&#36716;&#21270;&#20026;&#35782;&#21035;&#20986;&#30340;&#30693;&#35782;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven research in Additive Manufacturing (AM) has gained significant success in recent years. This has led to a plethora of scientific literature to emerge. The knowledge in these works consists of AM and Artificial Intelligence (AI) contexts that have not been mined and formalized in an integrated way. Moreover, no tools or guidelines exist to support data-driven knowledge transfer from one context to another. As a result, data-driven solutions using specific AI techniques are being developed and validated only for specific AM process technologies. There is a potential to exploit the inherent similarities across various AM technologies and adapt the existing solutions from one process or problem to another using AI, such as Transfer Learning. We propose a three-step knowledge transferability analysis framework in AM to support data-driven AM knowledge transfer. As a prerequisite to transferability analysis, AM knowledge is featurized into identified knowledge components. The fra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20174;&#22522;&#30784;&#21407;&#29702;&#20986;&#21457;&#19988;&#26080;&#36229;&#21442;&#25968;&#20381;&#36182;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24773;&#22659;&#24863;&#30693;&#35843;&#25972;&#23398;&#20064;&#29575;&#65292;&#20855;&#26377;&#39640;&#25104;&#21151;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#20855;&#22791;&#26059;&#36716;&#19981;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06274</link><description>&lt;p&gt;
ELRA: &#25351;&#25968;&#23398;&#20064;&#29575;&#33258;&#36866;&#24212;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ELRA: Exponential learning rate adaption gradient descent optimization method. (arXiv:2309.06274v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06274
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#20174;&#22522;&#30784;&#21407;&#29702;&#20986;&#21457;&#19988;&#26080;&#36229;&#21442;&#25968;&#20381;&#36182;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24773;&#22659;&#24863;&#30693;&#35843;&#25972;&#23398;&#20064;&#29575;&#65292;&#20855;&#26377;&#39640;&#25104;&#21151;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#20855;&#22791;&#26059;&#36716;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#65292;&#24555;&#36895;&#65288;&#25351;&#25968;&#23398;&#20064;&#29575;&#33258;&#36866;&#24212;&#65289;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#20986;&#21457;&#65288;&#26080;&#36229;&#21442;&#25968;&#20381;&#36182;&#65289;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#24773;&#22659;&#24863;&#30693;&#26469;&#35843;&#25972;&#23398;&#20064;&#29575;&#945;&#65292;&#20027;&#35201;&#26159;&#36861;&#27714;&#27491;&#20132;&#37051;&#36817;&#26799;&#24230;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25104;&#21151;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#35843;&#33410;&#30340;&#21442;&#25968;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#36890;&#29992;&#24615;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#24847;&#32500;&#24230;n&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#19982;&#38382;&#39064;&#30340;&#32500;&#24230;&#32447;&#24615;&#65288;O(n)&#38454;&#65289;&#25193;&#23637;&#12290;&#23427;&#20248;&#21270;&#20984;&#21644;&#38750;&#20984;&#36830;&#32493;&#26223;&#35266;&#65292;&#24182;&#25552;&#20379;&#19968;&#23450;&#31243;&#24230;&#30340;&#26799;&#24230;&#12290;&#19982;Ada&#31995;&#21015;&#65288;AdaGrad&#65292;AdaMax&#65292;AdaDelta&#65292;Adam&#31561;&#65289;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26059;&#36716;&#19981;&#21464;&#24615;&#65306;&#20248;&#21270;&#36335;&#24452;&#21644;&#24615;&#33021;&#19982;&#22352;&#26631;&#36873;&#25321;&#26080;&#20851;&#12290;&#36890;&#36807;&#22312;MNIST&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#22120;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26681;&#25454;&#20854;&#26680;&#24515;&#24605;&#24819;&#23558;&#36825;&#31181;&#26032;&#31867;&#20248;&#21270;&#22120;&#21629;&#21517;&#20026;&#25351;&#25968;&#23398;&#20064;&#29575;&#33258;&#36866;&#24212;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#65288;ELRA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel, fast (exponential rate adaption), ab initio (hyper-parameter-free) gradient based optimizer algorithm. The main idea of the method is to adapt the learning rate $\alpha$ by situational awareness, mainly striving for orthogonal neighboring gradients. The method has a high success and fast convergence rate and does not rely on hand-tuned parameters giving it greater universality. It can be applied to problems of any dimensions n and scales only linearly (of order O(n)) with the dimension of the problem. It optimizes convex and non-convex continuous landscapes providing some kind of gradient. In contrast to the Ada-family (AdaGrad, AdaMax, AdaDelta, Adam, etc.) the method is rotation invariant: optimization path and performance are independent of coordinate choices. The impressive performance is demonstrated by extensive experiments on the MNIST benchmark data-set against state-of-the-art optimizers. We name this new class of optimizers after its core idea Exponential 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;DNN&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#21069;&#21015;&#33146;&#32959;&#30244;&#30340;VERDICT&#27169;&#22411;&#21442;&#25968;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.06268</link><description>&lt;p&gt;
ssVERDICT&#65306;&#29992;&#20110;&#22686;&#24378;&#21069;&#21015;&#33146;&#32959;&#30244;&#34920;&#24449;&#30340;&#33258;&#30417;&#30563;VERDICT-MRI
&lt;/p&gt;
&lt;p&gt;
ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation. (arXiv:2309.06268v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;DNN&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#21069;&#21015;&#33146;&#32959;&#30244;&#30340;VERDICT&#27169;&#22411;&#21442;&#25968;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MRI&#22312;&#21069;&#21015;&#33146;&#30284;&#65288;PCa&#65289;&#30340;&#35786;&#26029;&#20013;&#36234;&#26469;&#36234;&#34987;&#20351;&#29992;&#65292;&#20854;&#20013;&#25193;&#25955;MRI&#65288;dMRI&#65289;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#24403;&#19982;&#35745;&#31639;&#27169;&#22411;&#32467;&#21512;&#26102;&#65292;dMRI&#21487;&#20197;&#20272;&#35745;&#32454;&#32990;&#22823;&#23567;&#31561;&#24494;&#35266;&#32467;&#26500;&#20449;&#24687;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#65288;NLLS&#65289;&#26354;&#32447;&#25311;&#21512;&#26041;&#27861;&#36827;&#34892;&#25311;&#21512;&#65292;&#19982;&#39640;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#12290;&#30417;&#30563;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#20854;&#24615;&#33021;&#21463;&#21040;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24213;&#23618;&#20998;&#24067;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#32593;&#32476;&#22312;&#27492;&#26041;&#27861;&#20013;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#20351;&#29992;&#21333;&#29420;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#24212;&#29992;&#20110;&#23545;&#24494;&#19981;&#36275;&#36947;&#30340;dMRI&#27169;&#22411;&#30340;&#25311;&#21512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;DNN&#65292;&#29992;&#20110;&#20272;&#35745;&#21069;&#21015;&#33146;VERDICT&#65288;Tumours&#20013;&#30340;&#34880;&#31649;&#12289;&#32454;&#32990;&#22806;&#21644;&#21463;&#38480;&#25193;&#25955;&#65289;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21069;&#21015;&#33146;&#32959;&#30244;&#30340;VERDICT&#27169;&#22411;&#30340;&#21442;&#25968;&#21487;&#20197;&#26377;&#25928;&#20272;&#35745;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
MRI is increasingly being used in the diagnosis of prostate cancer (PCa), with diffusion MRI (dMRI) playing an integral role. When combined with computational models, dMRI can estimate microstructural information such as cell size. Conventionally, such models are fit with a nonlinear least squares (NLLS) curve fitting approach, associated with a high computational cost. Supervised deep neural networks (DNNs) are an efficient alternative, however their performance is significantly affected by the underlying distribution of the synthetic training data. Self-supervised learning is an attractive alternative, where instead of using a separate training dataset, the network learns the features of the input data itself. This approach has only been applied to fitting of trivial dMRI models thus far. Here, we introduce a self-supervised DNN to estimate the parameters of the VERDICT (Vascular, Extracellular and Restricted DIffusion for Cytometry in Tumours) model for prostate. We demonstrate, for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#31163;&#25955;&#19968;&#33268;&#24615;&#38381;&#22622;&#26041;&#26696;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#22823;&#28065;&#27169;&#25311;&#20013;&#30340;&#38381;&#22622;&#27169;&#22411;&#31995;&#25968;&#35843;&#25972;&#20219;&#21153;&#23450;&#20041;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#21518;&#26399;&#30340;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#23558;&#27169;&#22411;&#35843;&#25972;&#21040;&#23454;&#38469;&#30340;&#31163;&#25955;&#21270;&#20013;&#24182;&#32771;&#34385;&#31163;&#25955;&#21270;&#21644;&#27169;&#22411;&#26412;&#36523;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#20248;&#21270;&#26174;&#24335;&#21644;&#38544;&#24335;&#38381;&#22622;&#27169;&#22411;&#20013;&#30340;&#23616;&#37096;&#28065;&#31896;&#24230;&#27169;&#22411;&#21644;&#28151;&#21512;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#38381;&#22622;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.06260</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#31163;&#25955;&#19968;&#33268;&#24615;&#38381;&#22622;&#26041;&#26696;&#29992;&#20110;&#22823;&#28065;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning. (arXiv:2309.06260v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#31163;&#25955;&#19968;&#33268;&#24615;&#38381;&#22622;&#26041;&#26696;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#22823;&#28065;&#27169;&#25311;&#20013;&#30340;&#38381;&#22622;&#27169;&#22411;&#31995;&#25968;&#35843;&#25972;&#20219;&#21153;&#23450;&#20041;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#21518;&#26399;&#30340;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#23558;&#27169;&#22411;&#35843;&#25972;&#21040;&#23454;&#38469;&#30340;&#31163;&#25955;&#21270;&#20013;&#24182;&#32771;&#34385;&#31163;&#25955;&#21270;&#21644;&#27169;&#22411;&#26412;&#36523;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#20248;&#21270;&#26174;&#24335;&#21644;&#38544;&#24335;&#38381;&#22622;&#27169;&#22411;&#20013;&#30340;&#23616;&#37096;&#28065;&#31896;&#24230;&#27169;&#22411;&#21644;&#28151;&#21512;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#38381;&#22622;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#29992;&#20110;&#38544;&#24335;&#28388;&#27874;&#30340;&#22823;&#28065;&#27169;&#25311;&#65288;LES&#65289;&#20013;&#30340;&#31163;&#25955;&#19968;&#33268;&#24615;&#38381;&#22622;&#26041;&#26696;&#12290;&#22312;&#38544;&#24335;&#28388;&#27874;&#30340;LES&#20013;&#65292;&#24863;&#24212;&#28388;&#27874;&#26680;&#21644;&#38381;&#22622;&#39033;&#26159;&#30001;&#32593;&#26684;&#21644;&#31163;&#25955;&#21270;&#25805;&#20316;&#31526;&#30340;&#24615;&#36136;&#20915;&#23450;&#30340;&#65292;&#20174;&#32780;&#20135;&#29983;&#39069;&#22806;&#30340;&#26410;&#30693;&#35745;&#31639;&#32454;&#32593;&#39033;&#12290;&#22240;&#27492;&#65292;&#23558;LES&#38381;&#22622;&#27169;&#22411;&#30340;&#31995;&#25968;&#35843;&#25972;&#20219;&#21153;&#23450;&#20041;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#21518;&#26399;&#35299;&#20915;&#12290;&#36825;&#20801;&#35768;&#23558;&#27169;&#22411;&#35843;&#25972;&#21040;&#23454;&#38469;&#30340;&#31163;&#25955;&#21270;&#20013;&#65292;&#21516;&#26102;&#36824;&#21253;&#25324;&#31163;&#25955;&#21270;&#21644;&#27169;&#22411;&#26412;&#36523;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20010;&#20248;&#21270;&#26694;&#26550;&#24212;&#29992;&#20110;&#26174;&#24335;&#21644;&#38544;&#24335;&#38381;&#22622;&#27169;&#22411;&#12290;&#36890;&#36807;&#20248;&#21270;&#23616;&#37096;&#28065;&#31896;&#24230;&#27169;&#22411;&#20316;&#20026;&#26174;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#35782;&#21035;&#28151;&#21512;&#19981;&#36830;&#32493;G&#30340;&#26368;&#20248;&#28151;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel method for developing discretization-consistent closure schemes for implicitly filtered Large Eddy Simulation (LES). In implicitly filtered LES, the induced filter kernel, and thus the closure terms, are determined by the properties of the grid and the discretization operator, leading to additional computational subgrid terms that are generally unknown in a priori analysis. Therefore, the task of adapting the coefficients of LES closure models is formulated as a Markov decision process and solved in an a posteriori manner with Reinforcement Learning (RL). This allows to adjust the model to the actual discretization as it also incorporates the interaction between the discretization and the model itself. This optimization framework is applied to both explicit and implicit closure models. An element-local eddy viscosity model is optimized as the explicit model. For the implicit modeling, RL is applied to identify an optimal blending strategy for a hybrid discontinuous G
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#20102;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24494;&#35843;&#36807;&#31243;&#20013;&#36861;&#27714;&#19987;&#19994;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2309.06256</link><description>&lt;p&gt;
&#19987;&#19994;&#24615;&#19982;&#24191;&#27867;&#24615;&#65306;&#20851;&#20110;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models. (arXiv:2309.06256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#20102;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24494;&#35843;&#36807;&#31243;&#20013;&#36861;&#27714;&#19987;&#19994;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#20855;&#26377;&#22788;&#29702;&#22810;&#26679;&#20998;&#24067;&#21644;&#20219;&#21153;&#30340;&#24191;&#27867;&#24615;&#65292;&#36825;&#28304;&#20110;&#23427;&#20204;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26159;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#25110;&#35843;&#25972;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#24120;&#35265;&#20570;&#27861;&#65292;&#20351;&#20854;&#33719;&#24471;&#19987;&#19994;&#24615;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#24494;&#35843;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#35206;&#30422;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#20998;&#24067;&#21644;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36861;&#27714;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#19987;&#19994;&#24615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#65292;&#36825;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;(Catastrophic Forgetting, CF)&#30456;&#20851;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;VLMs&#21644;LLMs&#20013;&#30340;&#23384;&#22312;&#12290;&#20363;&#22914;&#65292;&#23545;&#20687;CLIP&#36825;&#26679;&#30340;VLM&#36827;&#34892;&#22312;ImageNet&#19978;&#30340;&#24494;&#35843;&#20250;&#23548;&#33268;&#22788;&#29702;&#22810;&#26679;&#20998;&#24067;&#30340;&#24191;&#27867;&#24615;&#25439;&#22833;&#65292;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;Galactica&#36827;&#34892;&#24494;&#35843;&#21017;&#20250;&#23548;&#33268;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.06255</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#22686;&#24378;&#22810;&#27169;&#24577;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24322;&#36136;&#20449;&#24687;&#20849;&#21516;&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21327;&#20316;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#23613;&#20154;&#24847;&#30340;&#38382;&#39064;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#20849;&#21516;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#12290;&#19968;&#20123;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#35782;&#21035;&#21644;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#36739;&#24046;&#30340;&#27169;&#24577;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#23545;&#26679;&#26412;&#32423;&#21035;&#22810;&#27169;&#24577;&#21327;&#20316;&#30340;&#32454;&#31890;&#24230;&#35266;&#23519;&#21644;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#35266;&#23519;&#21644;&#25913;&#36827;&#27169;&#24577;&#20043;&#38388;&#32454;&#31890;&#24230;&#30340;&#21327;&#20316;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#27169;&#24577;&#24046;&#24322;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#21487;&#33021;&#21464;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#26102;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#27169;&#24577;&#35780;&#20272;&#65292;&#25105;&#20204;&#36951;&#25022;&#22320;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this iss
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#20351;&#29992;&#24067;&#37324;&#23572;&#20998;&#25968;&#21644;&#39044;&#26399;&#26657;&#20934;&#35823;&#24046;&#20316;&#20026;&#35780;&#20272;&#30005;&#31454;&#39046;&#22495;&#32988;&#29575;&#20272;&#35745;&#27169;&#22411;&#24615;&#33021;&#30340;&#25351;&#26631;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24179;&#34913;&#20998;&#25968;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.06248</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20351;&#29992;&#30005;&#31454;&#25968;&#25454;&#35780;&#20272;&#27010;&#29575;&#20272;&#35745;&#27169;&#22411;&#30340;&#35780;&#20215;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Rethinking Evaluation Metric for Probability Estimation Models Using Esports Data. (arXiv:2309.06248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#20351;&#29992;&#24067;&#37324;&#23572;&#20998;&#25968;&#21644;&#39044;&#26399;&#26657;&#20934;&#35823;&#24046;&#20316;&#20026;&#35780;&#20272;&#30005;&#31454;&#39046;&#22495;&#32988;&#29575;&#20272;&#35745;&#27169;&#22411;&#24615;&#33021;&#30340;&#25351;&#26631;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24179;&#34913;&#20998;&#25968;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#20272;&#35745;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#20307;&#32946;&#20998;&#26512;&#31561;&#39046;&#22495;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#20247;&#22810;&#27010;&#29575;&#20272;&#35745;&#27169;&#22411;&#20013;&#65292;&#30001;&#20110;&#27809;&#26377;&#30495;&#23454;&#27010;&#29575;&#25968;&#25454;&#21487;&#29992;&#65292;&#24456;&#38590;&#35780;&#20272;&#21738;&#20010;&#27169;&#22411;&#25552;&#20379;&#21487;&#38752;&#30340;&#27010;&#29575;&#12290;&#30005;&#31454;&#20013;&#30340;&#32988;&#29575;&#20272;&#35745;&#27169;&#22411;&#26159;&#27010;&#29575;&#20272;&#35745;&#39046;&#22495;&#20013;&#19968;&#20010;&#27491;&#22312;&#34987;&#31215;&#26497;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#20854;&#35745;&#31639;&#22312;&#29305;&#23450;&#28216;&#25103;&#29366;&#24577;&#19979;&#30340;&#32988;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20934;&#30830;&#29575;&#20316;&#20026;&#35780;&#20272;&#27169;&#22411;&#30340;&#25351;&#26631;&#65292;&#20934;&#30830;&#29575;&#21482;&#33021;&#34913;&#37327;&#21306;&#20998;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#24067;&#37324;&#23572;&#20998;&#25968;&#21644;&#39044;&#26399;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#26367;&#20195;&#20934;&#30830;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#30005;&#31454;&#39046;&#22495;&#20013;&#32988;&#29575;&#20272;&#35745;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#34913;&#20998;&#25968;&#30340;&#26032;&#25351;&#26631;&#65292;&#23427;&#22312;&#20845;&#20010;&#22909;&#23646;&#24615;&#26041;&#38754;&#26159;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probability estimation models play an important role in various fields, such as weather forecasting, recommendation systems, and sports analysis. Among several models estimating probabilities, it is difficult to evaluate which model gives reliable probabilities since the ground-truth probabilities are not available. The win probability estimation model for esports, which calculates the win probability under a certain game state, is also one of the fields being actively studied in probability estimation. However, most of the previous works evaluated their models using accuracy, a metric that only can measure the performance of discrimination. In this work, we firstly investigate the Brier score and the Expected Calibration Error (ECE) as a replacement of accuracy used as a performance evaluation metric for win probability estimation models in esports field. Based on the analysis, we propose a novel metric called Balance score which is a simple yet effective metric in terms of six good p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#22522;&#20110;&#26041;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24230;&#37327;&#30340;&#39564;&#35777;&#65292;&#21457;&#29616;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#26159;&#20114;&#34917;&#30340;&#39564;&#35777;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#39564;&#35777;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06240</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#26159;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#22522;&#20110;&#26041;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24230;&#37327;&#30340;&#20114;&#34917;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Consistency and adaptivity are complementary targets for the validation of variance-based uncertainty quantification metrics in machine learning regression tasks. (arXiv:2309.06240v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06240
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#22522;&#20110;&#26041;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24230;&#37327;&#30340;&#39564;&#35777;&#65292;&#21457;&#29616;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#26159;&#20114;&#34917;&#30340;&#39564;&#35777;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#39564;&#35777;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#26448;&#26009;&#21644;&#21270;&#23398;&#31185;&#23398;&#20013;&#35768;&#22810;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#30446;&#21069;&#24050;&#32463;&#35748;&#35782;&#21040;&#24179;&#22343;&#26657;&#20934;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#20351;&#29992;&#39069;&#22806;&#30340;&#26041;&#27861;&#26469;&#27979;&#35797;&#26465;&#20214;&#26657;&#20934;&#65292;&#21363;&#19968;&#33268;&#24615;&#12290;&#19968;&#33268;&#24615;&#20027;&#35201;&#36890;&#36807;&#21487;&#38752;&#24615;&#22270;&#26469;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#24179;&#22343;&#26657;&#20934;&#20043;&#22806;&#36824;&#23384;&#22312;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#30340;&#26465;&#20214;&#26657;&#20934;&#65292;&#20063;&#23601;&#26159;&#36866;&#24212;&#24615;&#12290;&#23454;&#38469;&#19978;&#65292;&#36866;&#24212;&#24615;&#26159;ML-UQ&#26041;&#27861;&#30340;&#26368;&#32456;&#29992;&#25143;&#20851;&#27880;&#30340;&#20027;&#35201;&#38382;&#39064;&#65292;&#20182;&#20204;&#23547;&#27714;&#23545;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20219;&#20309;&#28857;&#30340;&#39044;&#27979;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#23637;&#31034;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#26159;&#20114;&#34917;&#30340;&#39564;&#35777;&#30446;&#26631;&#65292;&#24182;&#19988;&#22909;&#30340;&#19968;&#33268;&#24615;&#24182;&#19981;&#24847;&#21619;&#30528;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;&#25991;&#31456;&#25552;&#20986;&#24182;&#22312;&#19968;&#20010;&#20856;&#22411;&#31034;&#20363;&#19978;&#36827;&#34892;&#20102;&#36866;&#24212;&#24615;&#39564;&#35777;&#26041;&#27861;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable uncertainty quantification (UQ) in machine learning (ML) regression tasks is becoming the focus of many studies in materials and chemical science. It is now well understood that average calibration is insufficient, and most studies implement additional methods testing the conditional calibration with respect to uncertainty, i.e. consistency. Consistency is assessed mostly by so-called reliability diagrams. There exists however another way beyond average calibration, which is conditional calibration with respect to input features, i.e. adaptivity. In practice, adaptivity is the main concern of the final users of a ML-UQ method, seeking for the reliability of predictions and uncertainties for any point in features space. This article aims to show that consistency and adaptivity are complementary validation targets, and that a good consistency does not imply a good adaptivity. Adapted validation methods are proposed and illustrated on a representative example.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#39118;&#38505;&#24863;&#30693;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#26368;&#22823;&#21270;&#26399;&#26395;&#22870;&#21169;&#30340;&#21516;&#26102;&#25429;&#25417;&#28508;&#22312;&#39118;&#38505;&#65292;&#25552;&#20379;&#20102;&#25968;&#23398;&#31934;&#30830;&#30340;&#26041;&#27861;&#26469;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20013;&#39118;&#38505;&#32771;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06239</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#23454;&#29616;&#39118;&#38505;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Risk-Aware Reinforcement Learning through Optimal Transport Theory. (arXiv:2309.06239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#39118;&#38505;&#24863;&#30693;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#26368;&#22823;&#21270;&#26399;&#26395;&#22870;&#21169;&#30340;&#21516;&#26102;&#25429;&#25417;&#28508;&#22312;&#39118;&#38505;&#65292;&#25552;&#20379;&#20102;&#25968;&#23398;&#31934;&#30830;&#30340;&#26041;&#27861;&#26469;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20013;&#39118;&#38505;&#32771;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25805;&#20316;&#30340;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#65292;&#39118;&#38505;&#31649;&#29702;&#25104;&#20026;&#30830;&#20445;&#21487;&#38752;&#20915;&#31574;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20256;&#32479;RL&#26041;&#27861;&#22312;&#22870;&#21169;&#20248;&#21270;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;&#24120;&#24120;&#24573;&#35270;&#28508;&#22312;&#39118;&#38505;&#30340;&#24773;&#20917;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#27425;&#23558;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#29702;&#35770;&#19982;RL&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#39118;&#38505;&#24863;&#30693;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20462;&#25913;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#30830;&#20445;&#24471;&#21040;&#30340;&#31574;&#30053;&#19981;&#20165;&#26368;&#22823;&#21270;&#26399;&#26395;&#22870;&#21169;&#65292;&#36824;&#36981;&#23432;OT&#36317;&#31163;&#25152;&#25351;&#31034;&#30340;&#29366;&#24577;&#35775;&#38382;&#20998;&#24067;&#21644;&#26399;&#26395;&#39118;&#38505;&#37197;&#32622;&#20043;&#38388;&#30340;&#39118;&#38505;&#32422;&#26463;&#12290;&#36890;&#36807;&#21033;&#29992;OT&#30340;&#25968;&#23398;&#31934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#23558;&#39118;&#38505;&#32771;&#37327;&#19982;&#20256;&#32479;RL&#30446;&#26631;&#24182;&#21015;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23450;&#29702;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#36129;&#29486;&#65292;&#25581;&#31034;&#20102;&#39118;&#38505;&#20998;&#24067;&#12289;&#26368;&#20248;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;OT&#30340;&#35270;&#35282;&#65292;&#36825;&#39033;&#24037;&#20316;&#25581;&#31034;&#20102;&#39118;&#38505;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the dynamic and uncertain environments where reinforcement learning (RL) operates, risk management becomes a crucial factor in ensuring reliable decision-making. Traditional RL approaches, while effective in reward optimization, often overlook the landscape of potential risks. In response, this paper pioneers the integration of Optimal Transport (OT) theory with RL to create a risk-aware framework. Our approach modifies the objective function, ensuring that the resulting policy not only maximizes expected rewards but also respects risk constraints dictated by OT distances between state visitation distributions and the desired risk profiles. By leveraging the mathematical precision of OT, we offer a formulation that elevates risk considerations alongside conventional RL objectives. Our contributions are substantiated with a series of theorems, mapping the relationships between risk distributions, optimal value functions, and policy behaviors. Through the lens of OT, this work illumin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#31034;&#21644;&#20998;&#35789;&#26102;&#38388;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#20351;&#29992;&#36731;&#37327;&#32423;&#23884;&#20837;&#23618;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#21644;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.06236</link><description>&lt;p&gt;
&#36367;&#20986;&#30340;&#31532;&#19968;&#27493;&#26368;&#22256;&#38590;&#65306;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#31034;&#21644;&#20998;&#35789;&#26102;&#38388;&#25968;&#25454;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. (arXiv:2309.06236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#31034;&#21644;&#20998;&#35789;&#26102;&#38388;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#20351;&#29992;&#36731;&#37327;&#32423;&#23884;&#20837;&#23618;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#21644;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#23558;&#23427;&#20204;&#29992;&#20316;&#20010;&#20154;&#21161;&#25163;&#21644;&#36890;&#29992;&#35745;&#31639;&#24341;&#25806;&#12290;&#28982;&#32780;&#65292;&#23558;&#25968;&#20540;/&#26102;&#38388;&#25968;&#25454;&#36755;&#20837;&#21040;&#36825;&#20123;&#27169;&#22411;&#20013;&#26102;&#65292;&#20250;&#20986;&#29616;&#19968;&#20010;&#26126;&#26174;&#30340;&#38556;&#30861;&#65292;&#27604;&#22914;&#20174;&#21487;&#31359;&#25140;&#35774;&#22791;&#25110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;LLMs&#22312;&#20854;&#36755;&#20837;&#20013;&#20351;&#29992;&#20998;&#35789;&#22120;&#23558;&#25991;&#26412;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#21333;&#20301;&#12290;&#28982;&#32780;&#65292;&#20998;&#35789;&#22120;&#24182;&#19981;&#35774;&#35745;&#29992;&#20110;&#34920;&#31034;&#25968;&#20540;&#65292;&#24182;&#21487;&#33021;&#38590;&#20197;&#29702;&#35299;&#37325;&#22797;&#27169;&#24335;&#21644;&#19978;&#19979;&#25991;&#65292;&#23558;&#36830;&#32493;&#30340;&#20540;&#35270;&#20026;&#21333;&#29420;&#30340;&#26631;&#35760;&#24182;&#24573;&#30053;&#23427;&#20204;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#36817;&#20351;&#29992;LLMs&#36827;&#34892;&#20197;&#20154;&#20026;&#20013;&#24515;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;LLMs&#38169;&#35823;&#22320;&#23545;&#26102;&#38388;&#25968;&#25454;&#36827;&#34892;&#20998;&#35789;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20363;&#22914;&#20351;&#29992;&#36731;&#37327;&#32423;&#23884;&#20837;&#23618;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#21644;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable generalization across diverse tasks, leading individuals to increasingly use them as personal assistants and universal computing engines. Nevertheless, a notable obstacle emerges when feeding numerical/temporal data into these models, such as data sourced from wearables or electronic health records. LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships. Here, we discuss recent works that employ LLMs for human-centric tasks such as in mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#32500;&#21333;&#25351;&#25968;&#27169;&#22411;&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#20449;&#24687;&#20934;&#21017;&#26469;&#30830;&#23450;&#25903;&#25345;&#30340;&#22238;&#24402;&#31995;&#25968;&#22823;&#23567;&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#36873;&#25321;&#30340;&#35843;&#20248;&#38656;&#27714;&#65292;&#24182;&#20855;&#26377;&#23376;&#38598;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#39640;&#27010;&#29575;&#19979;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06230</link><description>&lt;p&gt;
&#21333;&#25351;&#25968;&#27169;&#22411;&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#25193;&#23637;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models. (arXiv:2309.06230v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#32500;&#21333;&#25351;&#25968;&#27169;&#22411;&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#20449;&#24687;&#20934;&#21017;&#26469;&#30830;&#23450;&#25903;&#25345;&#30340;&#22238;&#24402;&#31995;&#25968;&#22823;&#23567;&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#36873;&#25321;&#30340;&#35843;&#20248;&#38656;&#27714;&#65292;&#24182;&#20855;&#26377;&#23376;&#38598;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#39640;&#27010;&#29575;&#19979;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#20998;&#26512;&#24341;&#21457;&#20102;&#23545;&#21333;&#25351;&#25968;&#27169;&#22411;&#65288;SIMs&#65289;&#21644;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#22686;&#21152;&#20852;&#36259;&#12290;SIMs&#20026;&#39640;&#32500;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#28789;&#27963;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#32780;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#26088;&#22312;&#20174;&#22823;&#37327;&#30340;&#39044;&#27979;&#22240;&#23376;&#20013;&#25214;&#21040;&#31232;&#30095;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#34987;&#35748;&#20026;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#25918;&#23485;&#36873;&#25321;&#65292;&#20294;&#19981;&#33021;&#24471;&#21040;&#26368;&#20339;&#23376;&#38598;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#31532;&#19968;&#20010;&#32463;&#36807;&#35777;&#26126;&#30340;&#38024;&#23545;&#39640;&#32500;SIMs&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#35745;&#31639;&#38590;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#35299;&#20855;&#26377;&#23376;&#38598;&#36873;&#25321;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#20960;&#20046;&#32943;&#23450;&#20855;&#26377;&#29992;&#20110;&#21442;&#25968;&#20272;&#35745;&#30340;&#34394;&#25311;&#23646;&#24615;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#19968;&#20010;&#24191;&#20041;&#20449;&#24687;&#20934;&#21017;&#26469;&#30830;&#23450;&#22238;&#24402;&#31995;&#25968;&#30340;&#25903;&#25345;&#22823;&#23567;&#65292;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20551;&#35774;&#35823;&#24046;&#20998;&#24067;&#25110;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of high-dimensional data has led to increased interest in both single index models (SIMs) and best subset selection. SIMs provide an interpretable and flexible modeling framework for high-dimensional data, while best subset selection aims to find a sparse model from a large set of predictors. However, best subset selection in high-dimensional models is known to be computationally intractable. Existing methods tend to relax the selection, but do not yield the best subset solution. In this paper, we directly tackle the intractability by proposing the first provably scalable algorithm for best subset selection in high-dimensional SIMs. Our algorithmic solution enjoys the subset selection consistency and has the oracle property with a high probability. The algorithm comprises a generalized information criterion to determine the support size of the regression coefficients, eliminating the model selection tuning. Moreover, our method does not assume an error distribution or a specif
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#12290;&#37319;&#29992;&#21367;&#31215;LSTM&#21644;Transformer&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06212</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Long-term drought prediction using deep neural networks based on geospatial weather data. (arXiv:2309.06212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06212
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#12290;&#37319;&#29992;&#21367;&#31215;LSTM&#21644;Transformer&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20892;&#19994;&#23454;&#36341;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#23545;&#20110;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#23588;&#20854;&#23545;&#20110;&#38271;&#26399;&#20915;&#31574;&#65292;&#25552;&#21069;&#19968;&#24180;&#36827;&#34892;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#21450;&#20854;&#30456;&#37051;&#21306;&#22495;&#20869;&#21508;&#31181;&#22240;&#32032;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#39044;&#27979;&#36825;&#19968;&#27010;&#29575;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21508;&#31181;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25152;&#32771;&#34385;&#30340;&#27169;&#22411;&#20027;&#35201;&#26159;&#26681;&#25454;Palmer&#24178;&#26097;&#20005;&#37325;&#25351;&#25968;&#65288;PDSI&#65289;&#39044;&#27979;&#24863;&#20852;&#36259;&#20122;&#21306;&#30340;&#24178;&#26097;&#24378;&#24230;&#65292;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#30340;&#20869;&#22312;&#22240;&#32032;&#21644;&#35265;&#35299;&#26469;&#25552;&#39640;&#24178;&#26097;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#27604;&#36739;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#26799;&#24230;&#25552;&#21319;&#21644;&#36923;&#36753;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#21367;&#31215;LSTM&#65288;ConvLSTM&#65289;&#21644;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#21069;&#20004;&#31181;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;ROC AUC&#20998;&#25968;&#65292;&#39640;&#36798;0.90
&lt;/p&gt;
&lt;p&gt;
The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.  Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24179;&#28369;&#36719;&#38408;&#20540;&#20989;&#25968;&#30340;&#23637;&#24320;ISTA&#21644;ADMM&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#21306;&#22495;&#20013;&#30340;&#20248;&#21270;&#20445;&#35777;&#65292;&#36890;&#36807;&#28385;&#36275;&#29305;&#23450;&#21306;&#22495;&#30340;PL$^*$&#26465;&#20214;&#23454;&#29616;&#25509;&#36817;&#38646;&#35757;&#32451;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2309.06195</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#28369;&#36719;&#38408;&#20540;&#20989;&#25968;&#30340;&#23637;&#24320;ISTA&#21644;ADMM&#32593;&#32476;&#30340;&#20248;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimization Guarantees of Unfolded ISTA and ADMM Networks With Smooth Soft-Thresholding. (arXiv:2309.06195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24179;&#28369;&#36719;&#38408;&#20540;&#20989;&#25968;&#30340;&#23637;&#24320;ISTA&#21644;ADMM&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#21306;&#22495;&#20013;&#30340;&#20248;&#21270;&#20445;&#35777;&#65292;&#36890;&#36807;&#28385;&#36275;&#29305;&#23450;&#21306;&#22495;&#30340;PL$^*$&#26465;&#20214;&#23454;&#29616;&#25509;&#36817;&#38646;&#35757;&#32451;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22522;&#20110;&#31639;&#27861;&#23637;&#24320;&#30340;&#65292;&#27169;&#22411;&#24863;&#30693;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23398;&#20064;&#30340;&#36845;&#20195;&#36719;&#38408;&#20540;&#31639;&#27861;&#65288;LISTA&#65289;&#21644;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#21387;&#32553;&#24863;&#30693;&#32593;&#32476;&#65288;ADMM-CSNet&#65289;&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36825;&#31181;&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;ISTA&#21644;ADMM&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#24179;&#28369;&#36719;&#38408;&#20540;&#22312;&#36807;&#21442;&#25968;&#21270;&#65288;OP&#65289;&#21306;&#22495;&#30340;&#26377;&#38480;&#23618;&#23637;&#24320;&#32593;&#32476;&#65288;&#22914;LISTA&#21644;ADMM-CSNet&#65289;&#30340;&#20248;&#21270;&#20445;&#35777;&#65292;&#21363;&#36890;&#36807;&#23398;&#20064;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#23454;&#29616;&#25509;&#36817;&#38646;&#35757;&#32451;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;Polyak-Lojasiewicz&#30340;&#19968;&#20010;&#20462;&#25913;&#29256;&#26412;&#65292;&#21363;PL$^*$&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#25439;&#22833;&#20989;&#25968;&#22270;&#26223;&#30340;&#29305;&#23450;&#21306;&#22495;&#20869;&#28385;&#36275;PL$^*$&#26465;&#20214;&#21487;&#20197;&#30830;&#20445;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#65292;&#24182;&#23454;&#29616;&#20174;&#21021;&#22987;&#21270;&#26102;&#30340;&#25351;&#25968;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving linear inverse problems plays a crucial role in numerous applications. Algorithm unfolding based, model-aware data-driven approaches have gained significant attention for effectively addressing these problems. Learned iterative soft-thresholding algorithm (LISTA) and alternating direction method of multipliers compressive sensing network (ADMM-CSNet) are two widely used such approaches, based on ISTA and ADMM algorithms, respectively. In this work, we study optimization guarantees, i.e., achieving near-zero training loss with the increase in the number of learning epochs, for finite-layer unfolded networks such as LISTA and ADMM-CSNet with smooth soft-thresholding in an over-parameterized (OP) regime. We achieve this by leveraging a modified version of the Polyak-Lojasiewicz, denoted PL$^*$, condition. Satisfying the PL$^*$ condition within a specific region of the loss landscape ensures the existence of a global minimum and exponential convergence from initialization using gra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#27867;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#22411;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#22312;&#22122;&#22768;&#21644;&#28151;&#21709;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;&#27979;&#35797;&#26465;&#20214;&#19979;&#35757;&#32451;&#30340;&#21442;&#32771;&#27169;&#22411;&#20316;&#20026;&#38590;&#24230;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.06183</link><description>&lt;p&gt;
&#35780;&#20272;&#23398;&#20064;&#22411;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#22312;&#22122;&#22768;&#21644;&#28151;&#21709;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Assessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments. (arXiv:2309.06183v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#27867;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#22411;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#22312;&#22122;&#22768;&#21644;&#28151;&#21709;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;&#27979;&#35797;&#26465;&#20214;&#19979;&#35757;&#32451;&#30340;&#21442;&#32771;&#27169;&#22411;&#20316;&#20026;&#38590;&#24230;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#21644;&#28151;&#21709;&#35821;&#38899;&#28151;&#21512;&#29289;&#30340;&#22768;&#23398;&#21464;&#24322;&#24615;&#21463;&#22810;&#20010;&#22240;&#32032;&#24433;&#21709;&#65292;&#20363;&#22914;&#30446;&#26631;&#35828;&#35805;&#32773;&#21644;&#24178;&#25200;&#22122;&#22768;&#30340;&#39057;&#35889;&#26102;&#22495;&#29305;&#24615;&#65292;&#20449;&#22122;&#27604;&#21644;&#25151;&#38388;&#29305;&#24615;&#12290;&#36825;&#31181;&#22823;&#30340;&#21464;&#24322;&#24615;&#32473;&#22522;&#20110;&#23398;&#20064;&#30340;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#26465;&#20214;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#20250;&#22823;&#22823;&#38477;&#20302;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#19982;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#19981;&#21516;&#30340;&#26032;&#35821;&#38899;&#12289;&#22122;&#22768;&#25110;&#21452;&#32819;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#65288;BRIR&#65289;&#25968;&#25454;&#24211;&#23545;&#31995;&#32479;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#23545;&#26410;&#30693;&#26465;&#20214;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#30340;&#38590;&#24230;&#21487;&#33021;&#20250;&#38543;&#30528;&#25968;&#25454;&#24211;&#30340;&#21464;&#21270;&#32780;&#25913;&#21464;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#27867;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22312;&#27979;&#35797;&#26465;&#20214;&#19979;&#35757;&#32451;&#30340;&#21442;&#32771;&#27169;&#22411;&#20316;&#20026;&#27979;&#35797;&#26465;&#20214;&#38590;&#24230;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The acoustic variability of noisy and reverberant speech mixtures is influenced by multiple factors, such as the spectro-temporal characteristics of the target speaker and the interfering noise, the signal-to-noise ratio (SNR) and the room characteristics. This large variability poses a major challenge for learning-based speech enhancement systems, since a mismatch between the training and testing conditions can substantially reduce the performance of the system. Generalization to unseen conditions is typically assessed by testing the system with a new speech, noise or binaural room impulse response (BRIR) database different from the one used during training. However, the difficulty of the speech enhancement task can change across databases, which can substantially influence the results. The present study introduces a generalization assessment framework that uses a reference model trained on the test condition, such that it can be used as a proxy for the difficulty of the test conditio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PagedAttention&#31639;&#27861;&#21644;vLLM&#31995;&#32479;&#65292;&#36890;&#36807;&#31867;&#20284;&#34394;&#25311;&#20869;&#23384;&#21644;&#20998;&#39029;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#38190;-&#20540;&#32531;&#23384;&#20869;&#23384;&#30340;&#39640;&#25928;&#31649;&#29702;&#21644;&#28789;&#27963;&#20849;&#20139;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#65288;&#22914;FasterTransformer&#21644;Orca&#65289;&#25913;&#36827;&#20102;2-4&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.06180</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#39640;&#25928;&#20869;&#23384;&#31649;&#29702;&#65306;&#22522;&#20110;&#20998;&#39029;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Memory Management for Large Language Model Serving with PagedAttention. (arXiv:2309.06180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PagedAttention&#31639;&#27861;&#21644;vLLM&#31995;&#32479;&#65292;&#36890;&#36807;&#31867;&#20284;&#34394;&#25311;&#20869;&#23384;&#21644;&#20998;&#39029;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#38190;-&#20540;&#32531;&#23384;&#20869;&#23384;&#30340;&#39640;&#25928;&#31649;&#29702;&#21644;&#28789;&#27963;&#20849;&#20139;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#65288;&#22914;FasterTransformer&#21644;Orca&#65289;&#25913;&#36827;&#20102;2-4&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#21534;&#21520;&#37327;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26381;&#21153;&#38656;&#35201;&#19968;&#27425;&#25209;&#22788;&#29702;&#36275;&#22815;&#22810;&#30340;&#35831;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#27599;&#20010;&#35831;&#27714;&#30340;&#38190;-&#20540;&#32531;&#23384;&#65288;KV&#32531;&#23384;&#65289;&#20869;&#23384;&#38750;&#24120;&#24222;&#22823;&#19988;&#21160;&#24577;&#22686;&#38271;&#21644;&#25910;&#32553;&#12290;&#24403;&#31649;&#29702;&#19981;&#24403;&#26102;&#65292;&#36825;&#20123;&#20869;&#23384;&#21487;&#33021;&#20250;&#22240;&#20026;&#30862;&#29255;&#21270;&#21644;&#20887;&#20313;&#22797;&#21046;&#32780;&#34987;&#22823;&#37327;&#28010;&#36153;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#25209;&#22788;&#29702;&#30340;&#35268;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32463;&#20856;&#34394;&#25311;&#20869;&#23384;&#21644;&#20998;&#39029;&#25216;&#26415;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;PagedAttention&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;vLLM&#65292;&#19968;&#20010;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#65288;1&#65289;KV&#32531;&#23384;&#20869;&#23384;&#20960;&#20046;&#38646;&#28010;&#36153;&#21644;&#65288;2&#65289;&#28789;&#27963;&#20849;&#20139;KV&#32531;&#23384;&#65292;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#65288;&#22914;FasterTransformer&#21644;Orca&#65289;&#30456;&#27604;&#65292;vLLM&#22312;&#20445;&#25345;&#30456;&#21516;&#24310;&#36831;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#27969;&#34892;&#30340;LLMs&#30340;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;2-4&#20493;&#12290;&#23545;&#20110;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#36825;&#31181;&#25913;&#36827;&#25928;&#26524;&#26356;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequence
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;ER SDE&#65289;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37319;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#31934;&#30830;&#35299;&#21644;&#39640;&#38454;&#36817;&#20284;&#35299;&#65292;&#24182;&#35299;&#37322;&#20102;&#22312;&#24555;&#36895;&#37319;&#26679;&#26041;&#38754;ODE&#27714;&#35299;&#22120;&#20248;&#20110;SDE&#27714;&#35299;&#22120;&#30340;&#25968;&#23398;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06169</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#35299;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Elucidating the solution space of extended reverse-time SDE for diffusion models. (arXiv:2309.06169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06169
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;ER SDE&#65289;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37319;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#31934;&#30830;&#35299;&#21644;&#39640;&#38454;&#36817;&#20284;&#35299;&#65292;&#24182;&#35299;&#37322;&#20102;&#22312;&#24555;&#36895;&#37319;&#26679;&#26041;&#38754;ODE&#27714;&#35299;&#22120;&#20248;&#20110;SDE&#27714;&#35299;&#22120;&#30340;&#25968;&#23398;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#37319;&#26679;&#36895;&#24230;&#36739;&#24930;&#65292;&#38656;&#35201;&#36890;&#36807;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#30334;&#25110;&#25968;&#21315;&#27425;&#36830;&#32493;&#20989;&#25968;&#35780;&#20272;&#25165;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#21487;&#20197;&#30475;&#20316;&#26159;&#35299;&#30456;&#24212;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#37319;&#26679;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388; SDE&#65288;ER SDE&#65289;&#65292;&#23558;&#20043;&#21069;&#23545;ODE&#21644;SDE&#30340;&#25506;&#32034;&#32479;&#19968;&#36215;&#26469;&#12290;&#21033;&#29992;ER SDE&#35299;&#30340;&#21322;&#32447;&#24615;&#32467;&#26500;&#65292;&#25105;&#20204;&#20026;VP SDE&#25552;&#20379;&#20102;&#31934;&#30830;&#35299;&#21644;&#20219;&#24847;&#39640;&#38454;&#36817;&#20284;&#35299;&#65292;&#20026;VE SDE&#25552;&#20379;&#20102;&#39640;&#38454;&#36817;&#20284;&#35299;&#12290;&#22522;&#20110;ER SDE&#30340;&#35299;&#31354;&#38388;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ODE&#27714;&#35299;&#22120;&#22312;&#24555;&#36895;&#37319;&#26679;&#26041;&#38754;&#20248;&#20110;SDE&#27714;&#35299;&#22120;&#30340;&#25968;&#23398;&#27934;&#23519;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;VP SDE&#27714;&#35299;&#22120;&#19982;&#20854;VE SDE&#27714;&#35299;&#22120;&#22312;&#24615;&#33021;&#19978;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) demonstrate potent image generation capabilities in various generative modeling tasks. Nevertheless, their primary limitation lies in slow sampling speed, requiring hundreds or thousands of sequential function evaluations through large neural networks to generate high-quality images. Sampling from DMs can be seen as solving corresponding stochastic differential equations (SDEs) or ordinary differential equations (ODEs). In this work, we formulate the sampling process as an extended reverse-time SDE (ER SDE), unifying prior explorations into ODEs and SDEs. Leveraging the semi-linear structure of ER SDE solutions, we offer exact solutions and arbitrarily high-order approximate solutions for VP SDE and VE SDE, respectively. Based on the solution space of the ER SDE, we yield mathematical insights elucidating the superior performance of ODE solvers over SDE solvers in terms of fast sampling. Additionally, we unveil that VP SDE solvers stand on par with their VE SDE c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26657;&#20934;&#30340;Lipschitz&#36793;&#30028;&#35823;&#24046;&#65288;CLL&#65289;&#26469;&#25552;&#39640;&#35748;&#35777;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#30028;&#35823;&#24046;&#19981;&#20250;&#26681;&#25454;&#25910;&#32553;&#30340;&#36755;&#20986;&#20998;&#24067;&#35843;&#25972;&#24809;&#32602;&#21644;&#26368;&#23567;&#21270;Lipschitz&#24120;&#25968;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06166</link><description>&lt;p&gt;
&#20855;&#26377;&#24377;&#24615;&#25511;&#21046;&#21644;&#36739;&#22823;Lipschitz&#24120;&#25968;&#30340;&#35748;&#35777;&#40065;&#26834;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Certified Robust Models with Slack Control and Large Lipschitz Constants. (arXiv:2309.06166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26657;&#20934;&#30340;Lipschitz&#36793;&#30028;&#35823;&#24046;&#65288;CLL&#65289;&#26469;&#25552;&#39640;&#35748;&#35777;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#30028;&#35823;&#24046;&#19981;&#20250;&#26681;&#25454;&#25910;&#32553;&#30340;&#36755;&#20986;&#20998;&#24067;&#35843;&#25972;&#24809;&#32602;&#21644;&#26368;&#23567;&#21270;Lipschitz&#24120;&#25968;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#20173;&#28982;&#23545;&#36755;&#20837;&#21464;&#21270;&#65292;&#22914;&#23545;&#25239;&#26679;&#26412;&#65292;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#20026;&#20102;&#33719;&#24471;&#23545;&#36825;&#31181;&#25200;&#21160;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#32771;&#34385;&#20102;&#22522;&#20110;Lipschitz&#30340;&#27491;&#21017;&#21270;&#22120;&#25110;&#32422;&#26463;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#39044;&#27979;&#36793;&#30028;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#26679;&#20570;&#20250;&#26174;&#33879;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#30340;Lipschitz&#36793;&#30028;&#35823;&#24046;&#65288;CLL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#20004;&#20010;&#38382;&#39064;&#26469;&#25552;&#39640;&#35748;&#35777;&#40065;&#26834;&#24615;&#65306;&#39318;&#20808;&#65292;&#24120;&#29992;&#30340;&#36793;&#30028;&#35823;&#24046;&#19981;&#20250;&#26681;&#25454;&#25910;&#32553;&#30340;&#36755;&#20986;&#20998;&#24067;&#35843;&#25972;&#24809;&#32602;&#65292;&#36825;&#26159;&#30001;&#20110;&#26368;&#23567;&#21270;Lipschitz&#24120;&#25968;K&#25152;&#36896;&#25104;&#30340;&#12290;&#20854;&#27425;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#23567;&#21270;K&#21487;&#20197;&#23548;&#33268;&#20915;&#31574;&#20989;&#25968;&#36807;&#24230;&#24179;&#28369;&#12290;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;CLL&#36890;&#36807;&#26126;&#30830;&#26657;&#20934;&#25439;&#22833;&#19982;&#36793;&#30028;&#21644;Lipschitz&#24120;&#25968;&#30340;&#20851;&#31995;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20174;&#32780;&#30830;&#20445;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent success, state-of-the-art learning-based models remain highly vulnerable to input changes such as adversarial examples. In order to obtain certifiable robustness against such perturbations, recent work considers Lipschitz-based regularizers or constraints while at the same time increasing prediction margin. Unfortunately, this comes at the cost of significantly decreased accuracy. In this paper, we propose a Calibrated Lipschitz-Margin Loss (CLL) that addresses this issue and improves certified robustness by tackling two problems: Firstly, commonly used margin losses do not adjust the penalties to the shrinking output distribution; caused by minimizing the Lipschitz constant $K$. Secondly, and most importantly, we observe that minimization of $K$ can lead to overly smooth decision functions. This limits the model's complexity and thus reduces accuracy. Our CLL addresses these issues by explicitly calibrating the loss w.r.t. margin and Lipschitz constant, thereby establis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#31561;&#32452;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#23427;&#22312;&#36724;&#25215;&#26426;&#22120;&#24212;&#29992;&#20013;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06157</link><description>&lt;p&gt;
Robust-MBDL:&#19968;&#31181;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#37492;&#21035;&#30340;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines. (arXiv:2309.06157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#31561;&#32452;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#23427;&#22312;&#36724;&#25215;&#26426;&#22120;&#24212;&#29992;&#20013;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#21097;&#20313;&#23551;&#21629;(RUL)&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;(CO)&#37492;&#21035;&#30340;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#21253;&#25324;&#20027;&#35201;&#32452;&#20214;&#65306;(1)&#37319;&#29992;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#65307;(2)&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#20174;&#21435;&#22122;&#25968;&#25454;&#20013;&#29983;&#25104;&#26102;&#22495;&#12289;&#39057;&#22495;&#21644;&#26102;&#39057;&#22495;&#29305;&#24449;&#65307;(3)&#37319;&#29992;&#26032;&#39062;&#32780;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#26469;&#21033;&#29992;&#22810;&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;XJTU-SY&#21644;PRONOSTIA&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#25105;&#20204;&#30340;&#31995;&#32479;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#20855;&#26377;&#22312;&#36724;&#25215;&#26426;&#22120;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a Robust Multi-branch Deep learning-based system for remaining useful life (RUL) prediction and condition operations (CO) identification of rotating machines is proposed. In particular, the proposed system comprises main components: (1) an LSTM-Autoencoder to denoise the vibration data; (2) a feature extraction to generate time-domain, frequency-domain, and time-frequency based features from the denoised data; (3) a novel and robust multi-branch deep learning network architecture to exploit the multiple features. The performance of our proposed system was evaluated and compared to the state-of-the-art systems on two benchmark datasets of XJTU-SY and PRONOSTIA. The experimental results prove that our proposed system outperforms the state-of-the-art systems and presents potential for real-life applications on bearing machines.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#20219;&#21153;&#65292;&#29992;&#20110;&#25163;&#20889;&#20013;&#25991;&#23383;&#31526;&#35782;&#21035;&#65292;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#20016;&#23500;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;18&#31181;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#29702;&#24819;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35774;&#35745;&#21160;&#24577;&#39046;&#22495;&#27867;&#21270;&#35774;&#32622;&#65292;&#25581;&#31034;&#20102;&#21482;&#26377;&#30041;&#20986;&#19968;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#25165;&#33021;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06142</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#38752;&#30340;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Reliable Domain Generalization: A New Dataset and Evaluations. (arXiv:2309.06142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06142
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#20219;&#21153;&#65292;&#29992;&#20110;&#25163;&#20889;&#20013;&#25991;&#23383;&#31526;&#35782;&#21035;&#65292;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#20016;&#23500;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;18&#31181;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#29702;&#24819;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35774;&#35745;&#21160;&#24577;&#39046;&#22495;&#27867;&#21270;&#35774;&#32622;&#65292;&#25581;&#31034;&#20102;&#21482;&#26377;&#30041;&#20986;&#19968;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#25165;&#33021;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#30528;&#26222;&#36941;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#24456;&#23481;&#26131;&#23545;&#35757;&#32451;&#38598;&#20135;&#29983;&#20559;&#35265;&#65292;&#36825;&#22312;&#25509;&#25910;&#21040;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#26102;&#20250;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#22312;&#39046;&#22495;&#27867;&#21270;(DG)&#30340;&#25991;&#29486;&#20013;&#65292;&#30740;&#31350;&#20102;&#35768;&#22810;&#26041;&#27861;&#20197;&#35757;&#32451;&#22312;&#19981;&#21516;&#20998;&#24067;&#20559;&#31227;&#19979;&#20855;&#26377;&#24191;&#27867;&#27867;&#21270;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;DomainBed&#21644;WILDS&#22522;&#20934;&#25361;&#25112;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#38024;&#23545;&#29616;&#26377;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#20219;&#21153;&#65292;&#29992;&#20110;&#20016;&#23500;DG&#26041;&#27861;&#30740;&#31350;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#21363;&#25163;&#20889;&#20013;&#25991;&#23383;&#31526;&#35782;&#21035;(HCCR)&#12290;&#25105;&#20204;&#22312;&#25552;&#20986;&#30340;PaHCC&#65288;&#21360;&#21047;&#21644;&#25163;&#20889;&#20013;&#25991;&#23383;&#31526;&#65289;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;18&#31181;DG&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#29616;&#26377;&#26041;&#27861;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#27492;&#22806;&#65292;&#22312;&#35774;&#35745;&#30340;&#21160;&#24577;DG&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;DG&#26041;&#27861;&#30340;&#26356;&#22810;&#23646;&#24615;&#65292;&#24182;&#35748;&#20026;&#21482;&#26377;&#30041;&#20986;&#19968;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are ubiquitous distribution shifts in the real world. However, deep neural networks (DNNs) are easily biased towards the training set, which causes severe performance degradation when they receive out-of-distribution data. Many methods are studied to train models that generalize under various distribution shifts in the literature of domain generalization (DG). However, the recent DomainBed and WILDS benchmarks challenged the effectiveness of these methods. Aiming at the problems in the existing research, we propose a new domain generalization task for handwritten Chinese character recognition (HCCR) to enrich the application scenarios of DG method research. We evaluate eighteen DG methods on the proposed PaHCC (Printed and Handwritten Chinese Characters) dataset and show that the performance of existing methods on this dataset is still unsatisfactory. Besides, under a designed dynamic DG setting, we reveal more properties of DG methods and argue that only the leave-one-domain-out
&lt;/p&gt;</description></item><item><title>&#24418;&#21464;&#22120;&#26159;&#19968;&#20010;&#38598;&#25104;&#30340;&#35774;&#35745;&#12289;&#32534;&#35793;&#21644;&#20223;&#30495;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#23427;&#33021;&#33258;&#21160;&#23558;AI&#24212;&#29992;&#20869;&#26680;&#32534;&#35793;&#21040;&#29992;&#25143;&#23450;&#20041;&#30340;CGRA&#26550;&#26500;&#19978;&#65292;&#24182;&#39564;&#35777;&#20854;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06127</link><description>&lt;p&gt;
&#20351;&#29992;&#24418;&#21464;&#22120;&#21152;&#36895;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#20010;&#38598;&#25104;&#30340;&#35774;&#35745;&#12289;&#32534;&#35793;&#21644;&#20223;&#30495;&#26694;&#26550;&#29992;&#20110;&#31895;&#31890;&#24230;&#21487;&#37325;&#26500;&#25968;&#32452;
&lt;/p&gt;
&lt;p&gt;
Accelerating Edge AI with Morpher: An Integrated Design, Compilation and Simulation Framework for CGRAs. (arXiv:2309.06127v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06127
&lt;/p&gt;
&lt;p&gt;
&#24418;&#21464;&#22120;&#26159;&#19968;&#20010;&#38598;&#25104;&#30340;&#35774;&#35745;&#12289;&#32534;&#35793;&#21644;&#20223;&#30495;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#23427;&#33021;&#33258;&#21160;&#23558;AI&#24212;&#29992;&#20869;&#26680;&#32534;&#35793;&#21040;&#29992;&#25143;&#23450;&#20041;&#30340;CGRA&#26550;&#26500;&#19978;&#65292;&#24182;&#39564;&#35777;&#20854;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31895;&#31890;&#24230;&#21487;&#37325;&#26500;&#25968;&#32452;&#65288;CGRAs&#65289;&#20316;&#20026;&#21151;&#32791;&#39640;&#25928;&#30340;&#36793;&#32536;&#21152;&#36895;&#22120;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#25552;&#20379;&#36229;&#36234;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#22810;&#26679;&#24615;&#12290;&#24418;&#21464;&#22120;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#26550;&#26500;&#33258;&#36866;&#24212;&#30340;CGRA&#35774;&#35745;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25506;&#32034;CGRAs&#30340;&#24191;&#38420;&#35774;&#35745;&#31354;&#38388;&#12290;&#24418;&#21464;&#22120;&#30340;&#20840;&#38754;&#29983;&#24577;&#31995;&#32479;&#21253;&#25324;&#23450;&#21046;&#30340;&#32534;&#35793;&#22120;&#12289;&#27169;&#25311;&#22120;&#12289;&#21152;&#36895;&#22120;&#32508;&#21512;&#21644;&#39564;&#35777;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#24418;&#21464;&#22120;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;&#22312;&#33258;&#21160;&#32534;&#35793;AI&#24212;&#29992;&#20869;&#26680;&#21040;&#29992;&#25143;&#23450;&#20041;&#30340;CGRA&#26550;&#26500;&#19978;&#24182;&#39564;&#35777;&#20854;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#24418;&#21464;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;CGRAs&#30340;&#22810;&#26679;&#24615;&#65292;&#23454;&#29616;&#20102;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#39640;&#25928;&#32534;&#35793;&#21644;&#39564;&#35777;&#65292;&#35206;&#30422;&#20102;&#24191;&#27867;&#23884;&#20837;&#24335;AI&#24037;&#20316;&#36127;&#36733;&#30340;&#37325;&#35201;&#20869;&#26680;&#12290;&#24418;&#21464;&#22120;&#21487;&#20197;&#22312;https://github.com/ecolab-nus/morpher-v2 &#19978;&#22312;&#32447;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coarse-Grained Reconfigurable Arrays (CGRAs) hold great promise as power-efficient edge accelerator, offering versatility beyond AI applications. Morpher, an open-source, architecture-adaptive CGRA design framework, is specifically designed to explore the vast design space of CGRAs. The comprehensive ecosystem of Morpher includes a tailored compiler, simulator, accelerator synthesis, and validation framework. This study provides an overview of Morpher, highlighting its capabilities in automatically compiling AI application kernels onto user-defined CGRA architectures and verifying their functionality. Through the Morpher framework, the versatility of CGRAs is harnessed to facilitate efficient compilation and verification of edge AI applications, covering important kernels representative of a wide range of embedded AI workloads. Morpher is available online at https://github.com/ecolab-nus/morpher-v2.
&lt;/p&gt;</description></item><item><title>AstroLLaMA&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#22825;&#25991;&#23398;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;arXiv&#20013;&#30340;&#22825;&#25991;&#23398;&#25688;&#35201;fine-tuned&#24471;&#21040;&#65292;&#20854;&#22312;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#23436;&#25104;&#21644;&#23884;&#20837;&#25552;&#21462;&#27604;&#20854;&#20182;&#22522;&#30784;&#27169;&#22411;&#26356;&#20855;&#27934;&#23519;&#21147;&#21644;&#31185;&#23398;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06126</link><description>&lt;p&gt;
AstroLLaMA: &#38754;&#21521;&#22825;&#25991;&#23398;&#30340;&#19987;&#19994;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AstroLLaMA: Towards Specialized Foundation Models in Astronomy. (arXiv:2309.06126v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06126
&lt;/p&gt;
&lt;p&gt;
AstroLLaMA&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#22825;&#25991;&#23398;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;arXiv&#20013;&#30340;&#22825;&#25991;&#23398;&#25688;&#35201;fine-tuned&#24471;&#21040;&#65292;&#20854;&#22312;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#23436;&#25104;&#21644;&#23884;&#20837;&#25552;&#21462;&#27604;&#20854;&#20182;&#22522;&#30784;&#27169;&#22411;&#26356;&#20855;&#27934;&#23519;&#21147;&#21644;&#31185;&#23398;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20154;&#31867;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#23398;&#26415;&#22825;&#25991;&#23398;&#31561;&#39640;&#24230;&#19987;&#19994;&#21270;&#39046;&#22495;&#24448;&#24448;&#38590;&#20197;&#32988;&#20219;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AstroLLaMA&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;arXiv&#19978;&#30340;&#36229;&#36807;300,000&#20010;&#22825;&#25991;&#23398;&#25688;&#35201;&#20013;&#20351;&#29992;LLaMA-2 fine-tuned&#24471;&#21040;&#30340;70&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;AstroLLaMA&#38024;&#23545;&#20256;&#32479;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20854;&#22256;&#24785;&#24230;&#27604;Llama-2&#20302;30&#65285;&#65292;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#39046;&#22495;&#36866;&#24212;&#24615;&#12290;&#23613;&#31649;&#21442;&#25968;&#26126;&#26174;&#36739;&#23569;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#23436;&#25104;&#21644;&#23884;&#20837;&#25552;&#21462;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#26356;&#20855;&#27934;&#23519;&#21147;&#21644;&#31185;&#23398;&#30456;&#20851;&#24615;&#12290;AstroLLaMA&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;fine-tuning&#28508;&#21147;&#12290;&#20854;&#20844;&#24320;&#21457;&#24067;&#26088;&#22312;&#25512;&#21160;&#22260;&#32469;&#22825;&#25991;&#23398;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#33258;&#21160;&#35770;&#25991;&#25688;&#35201;&#21644;&#23545;&#35805;&#20195;&#29702;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;HRTEM&#65289;&#30340;&#40065;&#26834;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;Construction Zone&#36719;&#20214;&#21253;&#21644;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#21033;&#29992;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#27169;&#25311;&#25968;&#25454;&#24211;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32435;&#31859;&#39063;&#31890;&#30340;&#20998;&#21106;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.06122</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;(HRTEM)&#30340;&#40065;&#26834;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A robust synthetic data generation framework for machine learning in High-Resolution Transmission Electron Microscopy (HRTEM). (arXiv:2309.06122v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;HRTEM&#65289;&#30340;&#40065;&#26834;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;Construction Zone&#36719;&#20214;&#21253;&#21644;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#21033;&#29992;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#27169;&#25311;&#25968;&#25454;&#24211;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32435;&#31859;&#39063;&#31890;&#30340;&#20998;&#21106;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#20110;&#24320;&#21457;&#39640;&#24230;&#20934;&#30830;&#30340;&#32435;&#31859;&#26448;&#26009;&#34920;&#24449;&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#21253;&#25324;&#39640;&#20998;&#36776;&#29575;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;HRTEM&#65289;&#65292;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#25104;&#21151;&#23454;&#26045;&#36825;&#31867;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#20174;&#23454;&#39564;&#20013;&#33719;&#21462;&#36275;&#22815;&#22823;&#12289;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Construction Zone&#65292;&#19968;&#20010;&#29992;&#20110;&#24555;&#36895;&#29983;&#25104;&#22797;&#26434;&#32435;&#31859;&#23610;&#24230;&#21407;&#23376;&#32467;&#26500;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#21019;&#24314;&#22823;&#22411;&#27169;&#25311;&#25968;&#25454;&#24211;&#20197;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;Construction Zone&#33021;&#22815;&#24555;&#36895;&#32780;&#31995;&#32479;&#22320;&#37319;&#26679;&#29616;&#23454;&#32435;&#31859;&#26448;&#26009;&#32467;&#26500;&#65292;&#24182;&#21487;&#29992;&#20316;&#27169;&#25311;&#25968;&#25454;&#24211;&#30340;&#38543;&#26426;&#32467;&#26500;&#29983;&#25104;&#22120;&#65292;&#36825;&#23545;&#20110;&#29983;&#25104;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;HRTEM&#26174;&#24494;&#38236;&#22270;&#20687;&#20026;&#20363;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#27169;&#25311;&#25968;&#25454;&#24211;&#30340;&#21508;&#20010;&#23376;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#31070;&#32463;&#32593;&#32476;&#20197;&#20998;&#21106;&#32435;&#31859;&#39063;&#31890;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques are attractive options for developing highly-accurate automated analysis tools for nanomaterials characterization, including high-resolution transmission electron microscopy (HRTEM). However, successfully implementing such machine learning tools can be difficult due to the challenges in procuring sufficiently large, high-quality training datasets from experiments. In this work, we introduce Construction Zone, a Python package for rapidly generating complex nanoscale atomic structures, and develop an end-to-end workflow for creating large simulated databases for training neural networks. Construction Zone enables fast, systematic sampling of realistic nanomaterial structures, and can be used as a random structure generator for simulated databases, which is important for generating large, diverse synthetic datasets. Using HRTEM imaging as an example, we train a series of neural networks on various subsets of our simulated databases to segment nanoparticles and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20445;&#30495;&#24230;&#35825;&#23548;&#30340;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#65288;FIPE&#65289;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#19981;&#36879;&#26126;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26143;&#38469;&#20105;&#38712; II &#36825;&#26679;&#30340;&#22797;&#26434;&#25511;&#21046;&#29615;&#22659;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06097</link><description>&lt;p&gt;
&#22522;&#20110;&#20445;&#30495;&#24230;&#35825;&#23548;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fidelity-Induced Interpretable Policy Extraction for Reinforcement Learning. (arXiv:2309.06097v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20445;&#30495;&#24230;&#35825;&#23548;&#30340;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#65288;FIPE&#65289;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#19981;&#36879;&#26126;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26143;&#38469;&#20105;&#38712; II &#36825;&#26679;&#30340;&#22797;&#26434;&#25511;&#21046;&#29615;&#22659;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20197;&#19981;&#36879;&#26126;&#30340;&#26041;&#24335;&#36827;&#34892;&#20915;&#31574;&#65292;&#38459;&#30861;&#20102;&#29992;&#25143;&#24314;&#31435;&#20449;&#20219;&#21644;&#23457;&#35270;&#20195;&#29702;&#30340;&#24369;&#28857;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20123;&#21487;&#35299;&#37322;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#26469;&#35299;&#37322;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20294;&#23427;&#20204;&#30340;&#35299;&#37322;&#24120;&#24120;&#19982;&#20195;&#29702;&#30340;&#34892;&#20026;&#19981;&#19968;&#33268;&#65292;&#22240;&#27492;&#32463;&#24120;&#26080;&#27861;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#20445;&#30495;&#24230;&#35825;&#23548;&#30340;&#31574;&#30053;&#25552;&#21462;&#65288;FIPE&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#29616;&#26377;&#21487;&#35299;&#37322;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#30340;&#20248;&#21270;&#26426;&#21046;&#65292;&#38416;&#36848;&#20102;&#22312;&#22686;&#21152;&#32047;&#31215;&#22870;&#21169;&#26102;&#24573;&#35270;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#20445;&#30495;&#24230;&#37327;&#38598;&#25104;&#21040;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20445;&#30495;&#24230;&#35825;&#23548;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;&#26143;&#38469;&#20105;&#38712; II &#30340;&#22797;&#26434;&#25511;&#21046;&#29615;&#22659;&#19979;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#26159;&#24403;&#21069;&#21487;&#35299;&#37322;&#31574;&#30053;&#25552;&#21462;&#26041;&#27861;&#36890;&#24120;&#36991;&#20813;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making problems. However, existing DRL agents make decisions in an opaque fashion, hindering the user from establishing trust and scrutinizing weaknesses of the agents. While recent research has developed Interpretable Policy Extraction (IPE) methods for explaining how an agent takes actions, their explanations are often inconsistent with the agent's behavior and thus, frequently fail to explain. To tackle this issue, we propose a novel method, Fidelity-Induced Policy Extraction (FIPE). Specifically, we start by analyzing the optimization mechanism of existing IPE methods, elaborating on the issue of ignoring consistency while increasing cumulative rewards. We then design a fidelity-induced mechanism by integrate a fidelity measurement into the reinforcement learning feedback. We conduct experiments in the complex control environment of StarCraft II, an arena typically avoided by current IPE method
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#26469;&#36890;&#36807;&#35777;&#20070;&#21512;&#25104;&#39564;&#35777;&#21160;&#24577;&#21644;&#25511;&#21046;&#27169;&#22411;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35774;&#35745;&#25511;&#21046;&#22120;&#24182;&#20998;&#26512;&#22797;&#26434;&#35268;&#33539;&#12290;&#36825;&#20010;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;SMT&#27714;&#35299;&#22120;&#26469;&#25552;&#20379;&#20505;&#36873;&#25511;&#21046;&#21644;&#35777;&#20070;&#20989;&#25968;&#65292;&#24182;&#20026;&#25511;&#21046;&#30340;&#23433;&#20840;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.06090</link><description>&lt;p&gt;
&#36890;&#36807;&#35777;&#20070;&#21512;&#25104;&#30340;&#21160;&#24577;&#19982;&#25511;&#21046;&#27169;&#22411;&#30340;&#36890;&#29992;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Verification Framework for Dynamical and Control Models via Certificate Synthesis. (arXiv:2309.06090v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06090
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#26469;&#36890;&#36807;&#35777;&#20070;&#21512;&#25104;&#39564;&#35777;&#21160;&#24577;&#21644;&#25511;&#21046;&#27169;&#22411;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35774;&#35745;&#25511;&#21046;&#22120;&#24182;&#20998;&#26512;&#22797;&#26434;&#35268;&#33539;&#12290;&#36825;&#20010;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;SMT&#27714;&#35299;&#22120;&#26469;&#25552;&#20379;&#20505;&#36873;&#25511;&#21046;&#21644;&#35777;&#20070;&#20989;&#25968;&#65292;&#24182;&#20026;&#25511;&#21046;&#30340;&#23433;&#20840;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#35770;&#30340;&#19968;&#20010;&#26032;&#20852;&#20998;&#25903;&#19987;&#38376;&#30740;&#31350;&#35777;&#20070;&#23398;&#20064;&#65292;&#28041;&#21450;&#23545;&#33258;&#20027;&#25110;&#25511;&#21046;&#27169;&#22411;&#30340;&#25152;&#38656;&#65288;&#21487;&#33021;&#26159;&#22797;&#26434;&#30340;&#65289;&#31995;&#32479;&#34892;&#20026;&#30340;&#35268;&#33539;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20989;&#25968;&#30340;&#35777;&#26126;&#36827;&#34892;&#20998;&#26512;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#28385;&#36275;&#36825;&#20123;&#22797;&#26434;&#35201;&#27714;&#30340;&#25511;&#21046;&#22120;&#30340;&#21512;&#25104;&#36890;&#24120;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#36229;&#20986;&#20102;&#22823;&#22810;&#25968;&#19987;&#23478;&#25511;&#21046;&#24037;&#31243;&#24072;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33258;&#21160;&#25216;&#26415;&#33021;&#22815;&#35774;&#35745;&#25511;&#21046;&#22120;&#24182;&#20998;&#26512;&#21508;&#31181;&#22797;&#26434;&#35268;&#33539;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#32534;&#30721;&#31995;&#32479;&#35268;&#33539;&#24182;&#23450;&#20041;&#30456;&#24212;&#30340;&#35777;&#20070;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#27491;&#24335;&#21512;&#25104;&#25511;&#21046;&#22120;&#21644;&#35777;&#20070;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#25511;&#21046;&#30340;&#23433;&#20840;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#25552;&#20379;&#20505;&#36873;&#30340;&#25511;&#21046;&#21644;&#35777;&#20070;&#20989;&#25968;&#65292;&#21516;&#26102;&#20351;&#29992;SMT&#27714;&#35299;&#22120;&#26469;&#25552;&#20379;&#24418;&#24335;&#21270;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging branch of control theory specialises in certificate learning, concerning the specification of a desired (possibly complex) system behaviour for an autonomous or control model, which is then analytically verified by means of a function-based proof. However, the synthesis of controllers abiding by these complex requirements is in general a non-trivial task and may elude the most expert control engineers. This results in a need for automatic techniques that are able to design controllers and to analyse a wide range of elaborate specifications. In this paper, we provide a general framework to encode system specifications and define corresponding certificates, and we present an automated approach to formally synthesise controllers and certificates. Our approach contributes to the broad field of safe learning for control, exploiting the flexibility of neural networks to provide candidate control and certificate functions, whilst using SMT-solvers to offer a formal guarantee of co
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#21644;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.06089</link><description>&lt;p&gt;
&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#33539;&#24335;&#20013;&#27979;&#37327;&#28798;&#38590;&#24615;&#36951;&#24536;&#65306;&#25506;&#32034;&#35843;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies. (arXiv:2309.06089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#21644;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#26159;&#19968;&#31181;&#35299;&#20915;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#20219;&#21153;&#30340;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#12290;&#22312;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19982;&#38646;&#23556;&#21644;&#20840;&#23556;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20316;&#20026;&#24494;&#35843;&#31574;&#30053;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21442;&#25968;&#25928;&#29575;&#36866;&#37197;&#22120;&#26041;&#27861;&#19982;&#25152;&#26377;&#21442;&#25968;&#24494;&#35843;&#12290;&#20316;&#20026;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#27599;&#20010;&#35821;&#35328;&#20381;&#27425;&#30340;&#20013;&#38388;&#35757;&#32451;&#65288;IT&#65289;&#21644;&#22312;&#24494;&#35843;&#30340;&#39564;&#35777;&#38454;&#27573;&#24050;&#32463;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#39564;&#35777;&#65288;CLV&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#20197;&#21450;&#28304;&#35821;&#35328;&#20013;&#30001;&#20110;&#36328;&#35821;&#35328;&#36716;&#31227;&#32780;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#65292;&#21363;&#22312;&#23398;&#20064;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#26032;&#20449;&#24687;&#26102;&#20043;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#25439;&#22833;&#20102;&#22810;&#23569;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#65292;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#20135;&#21697;&#35780;&#35770;&#65292;&#20998;&#21035;&#21253;&#21547;&#20102;&#22810;&#20010;&#35821;&#31181;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cross-lingual transfer is a promising technique to solve tasks in less-resourced languages. In this empirical study, we compare two fine-tuning approaches combined with zero-shot and full-shot learning approaches for large language models in a cross-lingual setting. As fine-tuning strategies, we compare parameter-efficient adapter methods with fine-tuning of all parameters. As cross-lingual transfer strategies, we compare the intermediate-training (\textit{IT}) that uses each language sequentially and cross-lingual validation (\textit{CLV}) that uses a target language already in the validation phase of fine-tuning. We assess the success of transfer and the extent of catastrophic forgetting in a source language due to cross-lingual transfer, i.e., how much previously acquired knowledge is lost when we learn new information in a different language. The results on two different classification problems, hate speech detection and product reviews, each containing datasets in several lang
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#22609;&#24615;&#30340;&#20114;&#34917;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#35299;&#38500;&#19987;&#23478;&#32593;&#32476;&#23545;&#20445;&#30041;&#20808;&#21069;&#30693;&#35782;&#30340;&#35201;&#27714;&#65292;&#24182;&#36890;&#36807;&#36866;&#24212;-&#22238;&#39038;&#38454;&#27573;&#19982;&#20043;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06086</link><description>&lt;p&gt;
&#20248;&#21270;&#22609;&#24615;&#30340;&#20114;&#34917;&#32593;&#32476;&#29992;&#20110;&#26080;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning. (arXiv:2309.06086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#22609;&#24615;&#30340;&#20114;&#34917;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#35299;&#38500;&#19987;&#23478;&#32593;&#32476;&#23545;&#20445;&#30041;&#20808;&#21069;&#30693;&#35782;&#30340;&#35201;&#27714;&#65292;&#24182;&#36890;&#36807;&#36866;&#24212;-&#22238;&#39038;&#38454;&#27573;&#19982;&#20043;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;CURL&#65289;&#30740;&#31350;&#21463;&#30410;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25216;&#26415;&#30340;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;SSL&#30340;&#29616;&#26377;CURL&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#65292;&#20294;&#22312;&#23398;&#20064;&#22810;&#20219;&#21153;&#25968;&#25454;&#27969;&#26102;&#24615;&#33021;&#19979;&#38477;&#26126;&#26174;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#30001;&#20110;&#20026;&#20102;&#38450;&#27490;&#36951;&#24536;&#32780;&#26045;&#21152;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#36896;&#25104;&#30340;&#65292;&#23548;&#33268;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#19981;&#22815;&#20248;&#21270;&#65306;&#23427;&#20204;&#35201;&#20040;&#19981;&#23436;&#20840;&#36866;&#24212;&#26032;&#30340;&#25968;&#25454;&#65288;&#22609;&#24615;&#20302;&#65289;&#65292;&#35201;&#20040;&#22312;&#23436;&#20840;&#36866;&#24212;&#26032;&#30340;SSL&#39044;&#35757;&#32451;&#20219;&#21153;&#26102;&#20135;&#29983;&#26174;&#33879;&#36951;&#24536;&#65288;&#31283;&#23450;&#24615;&#20302;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#19968;&#20010;&#19987;&#23478;&#32593;&#32476;&#65292;&#23427;&#19981;&#20877;&#38656;&#35201;&#20445;&#30041;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#32780;&#26159;&#21487;&#20197;&#19987;&#27880;&#20110;&#22312;&#26032;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65288;&#20248;&#21270;&#22609;&#24615;&#65289;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#30693;&#35782;&#19982;&#20808;&#21069;&#30340;&#32593;&#32476;&#32467;&#21512;&#22312;&#19968;&#20010;&#36866;&#24212;-&#22238;&#39038;&#38454;&#27573;&#65292;&#20197;&#36991;&#20813;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous unsupervised representation learning (CURL) research has greatly benefited from improvements in self-supervised learning (SSL) techniques. As a result, existing CURL methods using SSL can learn high-quality representations without any labels, but with a notable performance drop when learning on a many-tasks data stream. We hypothesize that this is caused by the regularization losses that are imposed to prevent forgetting, leading to a suboptimal plasticity-stability trade-off: they either do not adapt fully to the incoming data (low plasticity), or incur significant forgetting when allowed to fully adapt to a new SSL pretext-task (low stability). In this work, we propose to train an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity). In the second phase, we combine this new knowledge with the previous network in an adaptation-retrospection phase to avoid forgetting and i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#26512;&#29616;&#20195;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#21147;&#24066;&#22330;&#20013;&#20215;&#26684;&#39129;&#21319;&#20107;&#20214;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.06082</link><description>&lt;p&gt;
&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35299;&#26512;&#30005;&#21147;&#24066;&#22330;&#20215;&#26684;&#20107;&#20214;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Framework to Deconstruct the Primary Drivers for Electricity Market Price Events. (arXiv:2309.06082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#26512;&#29616;&#20195;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#21147;&#24066;&#22330;&#20013;&#20215;&#26684;&#39129;&#21319;&#20107;&#20214;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#32593;&#32476;&#27491;&#22312;&#21521;100%&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#22823;&#23481;&#37327;&#30005;&#21147;&#32593;&#32476;&#36716;&#21464;&#65292;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#21644;&#30005;&#21147;&#24066;&#22330;&#30340;&#25972;&#20307;&#21160;&#24577;&#20063;&#22312;&#21457;&#29983;&#21464;&#21270;&#12290;&#30005;&#21147;&#24066;&#22330;&#19981;&#20165;&#22312;&#32463;&#27982;&#19978;&#35843;&#24230;&#36164;&#28304;&#65292;&#36824;&#32771;&#34385;&#20102;&#21508;&#31181;&#21487;&#25511;&#34892;&#21160;&#65292;&#22914;&#21487;&#20877;&#29983;&#33021;&#28304;&#38480;&#21046;&#12289;&#36755;&#30005;&#38459;&#22622;&#32531;&#35299;&#21644;&#33021;&#37327;&#20648;&#23384;&#20248;&#21270;&#31561;&#65292;&#20197;&#30830;&#20445;&#30005;&#32593;&#21487;&#38752;&#24615;&#12290;&#22240;&#27492;&#65292;&#30005;&#21147;&#24066;&#22330;&#30340;&#20215;&#26684;&#24418;&#25104;&#21464;&#24471;&#38750;&#24120;&#22797;&#26434;&#12290;&#20256;&#32479;&#30340;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#21644;&#32479;&#35745;&#26041;&#27861;&#26080;&#27861;&#20998;&#26512;&#21644;&#25512;&#26029;&#29616;&#20195;&#30005;&#32593;&#21644;&#20855;&#26377;&#21487;&#21464;&#21487;&#20877;&#29983;&#33021;&#28304;&#65288;VRE&#65289;&#30340;&#24066;&#22330;&#20215;&#26684;&#24418;&#25104;&#32972;&#21518;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#26512;&#29616;&#20195;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#21147;&#24066;&#22330;&#20013;&#20215;&#26684;&#39129;&#21319;&#20107;&#20214;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#24212;&#29992;&#20110;&#24066;&#22330;&#35774;&#35745;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#35843;&#24230;&#31561;&#21508;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Power grids are moving towards 100% renewable energy source bulk power grids, and the overall dynamics of power system operations and electricity markets are changing. The electricity markets are not only dispatching resources economically but also taking into account various controllable actions like renewable curtailment, transmission congestion mitigation, and energy storage optimization to ensure grid reliability. As a result, price formations in electricity markets have become quite complex. Traditional root cause analysis and statistical approaches are rendered inapplicable to analyze and infer the main drivers behind price formation in the modern grid and markets with variable renewable energy (VRE). In this paper, we propose a machine learning-based analysis framework to deconstruct the primary drivers for price spike events in modern electricity markets with high renewable energy. The outcomes can be utilized for various critical aspects of market design, renewable dispatch an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20449;&#24687;&#27969;&#21160;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#26469;&#35299;&#32806;&#32593;&#32476;&#36830;&#25509;&#24615;&#19982;&#22270;&#25968;&#25454;&#36830;&#25509;&#24615;&#65292;&#24182;&#22312;&#20020;&#24202;&#20998;&#35786;&#24212;&#29992;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#32593;&#32476;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#32593;&#32476;&#36830;&#25509;&#24615;&#33021;&#22815;&#25552;&#21319;&#39044;&#27979;&#24615;&#33021;&#65292;&#36127;&#36793;&#23545;&#20110;&#33391;&#22909;&#30340;&#39044;&#27979;&#36215;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20351;&#29992;&#36807;&#22810;&#30340;&#32593;&#32476;&#23618;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06081</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#27969;&#21160;&#65306;&#20020;&#24202;&#20998;&#35786;&#24212;&#29992;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Information Flow in Graph Neural Networks: A Clinical Triage Use Case. (arXiv:2309.06081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20449;&#24687;&#27969;&#21160;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#26469;&#35299;&#32806;&#32593;&#32476;&#36830;&#25509;&#24615;&#19982;&#22270;&#25968;&#25454;&#36830;&#25509;&#24615;&#65292;&#24182;&#22312;&#20020;&#24202;&#20998;&#35786;&#24212;&#29992;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#32593;&#32476;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#32593;&#32476;&#36830;&#25509;&#24615;&#33021;&#22815;&#25552;&#21319;&#39044;&#27979;&#24615;&#33021;&#65292;&#36127;&#36793;&#23545;&#20110;&#33391;&#22909;&#30340;&#39044;&#27979;&#36215;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20351;&#29992;&#36807;&#22810;&#30340;&#32593;&#32476;&#23618;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30001;&#20110;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#21644;&#22810;&#20851;&#31995;&#22270;&#32780;&#22312;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#26377;&#25928;&#35757;&#32451;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23884;&#20837;&#20449;&#24687;&#22312;GNNs&#20869;&#37096;&#20256;&#25773;&#22914;&#20309;&#24433;&#21709;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#38142;&#25509;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#23558;GNN&#30340;&#36830;&#25509;&#24615;&#19982;&#22270;&#25968;&#25454;&#30340;&#36830;&#25509;&#24615;&#35299;&#32806;&#65292;&#24182;&#22312;&#20020;&#24202;&#20998;&#35786;&#24212;&#29992;&#26696;&#20363;&#20013;&#35780;&#20272;&#20102;GNNs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#21040;GNN&#30340;&#36830;&#25509;&#24615;&#20013;&#27604;&#20351;&#29992;&#19982;KG&#30456;&#21516;&#30340;&#36830;&#25509;&#24615;&#25110;&#20801;&#35768;&#26080;&#38480;&#21046;&#30340;&#23884;&#20837;&#20256;&#25773;&#23548;&#33268;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36127;&#36793;&#22312;&#23454;&#29616;&#33391;&#22909;&#39044;&#27979;&#20013;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#20351;&#29992;&#22826;&#22810;&#30340;GNN&#23618;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained popularity in healthcare and other domains due to their ability to process multi-modal and multi-relational graphs. However, efficient training of GNNs remains challenging, with several open research questions. In this paper, we investigate how the flow of embedding information within GNNs affects the prediction of links in Knowledge Graphs (KGs). Specifically, we propose a mathematical model that decouples the GNN connectivity from the connectivity of the graph data and evaluate the performance of GNNs in a clinical triage use case. Our results demonstrate that incorporating domain knowledge into the GNN connectivity leads to better performance than using the same connectivity as the KG or allowing unconstrained embedding propagation. Moreover, we show that negative edges play a crucial role in achieving good predictions, and that using too many GNN layers can degrade performance.
&lt;/p&gt;</description></item><item><title>A2V&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#23454;&#29616;&#33041;&#34880;&#31649;&#30340;&#36328;&#27169;&#24577;&#20998;&#21106;&#12290;&#23427;&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#28304;&#22495;&#21040;&#30446;&#26631;&#22495;&#30340;&#22270;&#20687;&#32423;&#33258;&#36866;&#24212;&#65292;&#24182;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06075</link><description>&lt;p&gt;
A2V: &#19968;&#31181;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#29992;&#20110;&#19981;&#21516;&#22270;&#20687;&#27169;&#24577;&#30340;&#33041;&#34880;&#31649;&#20998;&#21106;&#65292;&#36890;&#36807;&#20108;&#38454;&#27573;&#35757;&#32451;&#20174;&#34880;&#31649;&#36896;&#24433;&#21040;&#38745;&#33033;&#36896;&#24433;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel Segmentation via Two-Phase Training Angiography-to-Venography Translation. (arXiv:2309.06075v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06075
&lt;/p&gt;
&lt;p&gt;
A2V&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#23454;&#29616;&#33041;&#34880;&#31649;&#30340;&#36328;&#27169;&#24577;&#20998;&#21106;&#12290;&#23427;&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#28304;&#22495;&#21040;&#30446;&#26631;&#22495;&#30340;&#22270;&#20687;&#32423;&#33258;&#36866;&#24212;&#65292;&#24182;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#22270;&#20687;&#27169;&#24577;&#20013;&#20998;&#21106;&#33041;&#34880;&#31649;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#38598;&#20013;&#20110;&#21333;&#19968;&#27169;&#24577;&#65292;&#24573;&#35270;&#20102;&#24191;&#27867;&#21487;&#29992;&#30340;&#33041;&#34880;&#31649;&#25104;&#20687;&#25216;&#26415;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#26174;&#33879;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#23545;&#36328;&#27169;&#24577;&#30340;&#27867;&#21270;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#20381;&#36182;&#27880;&#37322;&#30340;&#34880;&#31649;&#36896;&#24433;&#21644;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#37322;&#30340;&#38745;&#33033;&#36896;&#24433;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#21033;&#29992;&#31163;&#25955;&#21270;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#34920;&#31034;&#24322;&#26500;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#28304;&#22495;&#21040;&#30446;&#26631;&#22495;&#30340;&#22270;&#20687;&#32423;&#33258;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#22522;&#20110;&#24490;&#29615;&#30340;&#26550;&#26500;&#30340;&#20856;&#22411;&#22797;&#26434;&#24615;&#65292;&#26368;&#23567;&#21270;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#20351;&#29992;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#31283;&#23450;&#35757;&#32451;&#30340;&#39640;&#25928;&#30452;&#35266;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#30913;&#20849;&#25391;&#34880;&#31649;&#36896;&#24433;&#21644;&#38745;&#33033;&#36896;&#24433;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#29616;&#26368;&#20808;&#36827;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#31283;&#23450;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a semi-supervised domain adaptation framework for brain vessel segmentation from different image modalities. Existing state-of-the-art methods focus on a single modality, despite the wide range of available cerebrovascular imaging techniques. This can lead to significant distribution shifts that negatively impact the generalization across modalities. By relying on annotated angiographies and a limited number of annotated venographies, our framework accomplishes image-to-image translation and semantic segmentation, leveraging a disentangled and semantically rich latent space to represent heterogeneous data and perform image-level adaptation from source to target domains. Moreover, we reduce the typical complexity of cycle-based architectures and minimize the use of adversarial training, which allows us to build an efficient and intuitive model with stable training. We evaluate our method on magnetic resonance angiographies and venographies. While achieving state-of-the-art pe
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#28369;&#22369;&#26131;&#21457;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#37325;&#35201;&#30340;&#36129;&#29486;&#22240;&#32032;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06062</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#28369;&#22369;&#26131;&#21457;&#24615;&#26102;&#30340;&#36129;&#29486;&#22240;&#32032;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Selection of contributing factors for predicting landslide susceptibility using machine learning and deep learning models. (arXiv:2309.06062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06062
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#28369;&#22369;&#26131;&#21457;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#37325;&#35201;&#30340;&#36129;&#29486;&#22240;&#32032;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28369;&#22369;&#26159;&#24120;&#35265;&#30340;&#33258;&#28982;&#28798;&#23475;&#65292;&#21487;&#33021;&#23548;&#33268;&#20154;&#21592;&#20260;&#20129;&#12289;&#36130;&#20135;&#23433;&#20840;&#23041;&#32961;&#21644;&#32463;&#27982;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#25110;&#39044;&#27979;&#28508;&#22312;&#39118;&#38505;&#22320;&#28857;&#30340;&#28369;&#22369;&#21457;&#29983;&#27010;&#29575;&#38750;&#24120;&#37325;&#35201;&#12290;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#28369;&#22369;&#28165;&#21333;&#21644;&#19968;&#32452;&#28369;&#22369;&#36129;&#29486;&#22240;&#32032;&#36827;&#34892;&#28369;&#22369;&#26131;&#21457;&#24615;&#35780;&#20272;&#65292;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#25110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#35760;&#24518;&#65289;&#26469;&#23454;&#29616;&#12290;&#20316;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#28369;&#22369;&#36129;&#29486;&#22240;&#32032;&#23545;&#28369;&#22369;&#21457;&#29983;&#26377;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26377;&#36923;&#36753;&#30340;&#36873;&#25321;&#26356;&#37325;&#35201;&#30340;&#36129;&#29486;&#22240;&#32032;&#24182;&#28040;&#38500;&#19981;&#30456;&#20851;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Landslides are a common natural disaster that can cause casualties, property safety threats and economic losses. Therefore, it is important to understand or predict the probability of landslide occurrence at potentially risky sites. A commonly used means is to carry out a landslide susceptibility assessment based on a landslide inventory and a set of landslide contributing factors. This can be readily achieved using machine learning (ML) models such as logistic regression (LR), support vector machine (SVM), random forest (RF), extreme gradient boosting (Xgboost), or deep learning (DL) models such as convolutional neural network (CNN) and long short time memory (LSTM). As the input data for these models, landslide contributing factors have varying influences on landslide occurrence. Therefore, it is logically feasible to select more important contributing factors and eliminate less relevant ones, with the aim of increasing the prediction accuracy of these models. However, selecting more
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21363;&#26381;&#21153;&#65288;FaaS&#65289;&#30340;&#23433;&#20840;&#12289;&#21487;&#39564;&#35777;&#21644;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#65292;&#29992;&#20110;&#35745;&#31639;&#21644;&#39564;&#35777;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;FaaS&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#20844;&#24179;&#24615;&#25351;&#26631;&#65307;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20316;&#23457;&#35745;&#20219;&#20309;ML&#27169;&#22411;&#30340;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.06061</link><description>&lt;p&gt;
&#39564;&#35777;&#20844;&#24179;&#24615;&#65306;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20844;&#24179;&#24615;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Verifiable Fairness: Privacy-preserving Computation of Fairness for Machine Learning Systems. (arXiv:2309.06061v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06061
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21363;&#26381;&#21153;&#65288;FaaS&#65289;&#30340;&#23433;&#20840;&#12289;&#21487;&#39564;&#35777;&#21644;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#65292;&#29992;&#20110;&#35745;&#31639;&#21644;&#39564;&#35777;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;FaaS&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#20844;&#24179;&#24615;&#25351;&#26631;&#65307;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20316;&#23457;&#35745;&#20219;&#20309;ML&#27169;&#22411;&#30340;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#34028;&#21187;&#21457;&#23637;&#19988;&#20805;&#28385;&#27963;&#21147;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21363;&#26381;&#21153;&#65288;FaaS&#65289;&#30340;&#23433;&#20840;&#12289;&#21487;&#39564;&#35777;&#21644;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#65292;&#29992;&#20110;&#35745;&#31639;&#21644;&#39564;&#35777;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;FaaS&#30340;&#35774;&#35745;&#20013;&#65292;&#36890;&#36807;&#23494;&#25991;&#34920;&#31034;&#25968;&#25454;&#21644;&#32467;&#26524;&#65292;&#20197;&#30830;&#20445;&#38544;&#31169;&#12290;&#27492;&#22806;&#65292;&#38646;&#30693;&#35782;&#35777;&#26126;&#20445;&#35777;&#20102;&#23494;&#25991;&#21644;&#24213;&#23618;&#25968;&#25454;&#30340;&#33391;&#22909;&#24615;&#12290;FaaS&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#20844;&#24179;&#24615;&#25351;&#26631;&#65307;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20316;&#23457;&#35745;&#20219;&#20309;ML&#27169;&#22411;&#30340;&#26381;&#21153;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#38656;&#35201;&#21487;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#25110;&#31169;&#23494;&#36890;&#36947;&#26469;&#35745;&#31639;&#20844;&#24179;&#24230;&#37327;&#12290;&#23433;&#20840;&#20445;&#35777;&#21644;&#25215;&#35834;&#20197;&#30830;&#20445;&#27599;&#20010;&#27493;&#39588;&#37117;&#26159;&#23433;&#20840;&#36879;&#26126;&#30340;&#65292;&#24182;&#19988;&#25972;&#20010;&#36807;&#31243;&#20174;&#24320;&#22987;&#21040;&#32467;&#26463;&#37117;&#21487;&#20197;&#36827;&#34892;&#39564;&#35777;&#12290;&#25152;&#26377;&#36755;&#20837;&#25968;&#25454;&#30340;&#23494;&#25991;&#23545;&#20110;&#27599;&#20010;&#20154;&#65292;&#20363;&#22914;&#23457;&#35745;&#21592;&#12289;&#31038;&#20250;&#27963;&#21160;&#23478;&#21644;&#19987;&#23478;&#26469;&#35828;&#37117;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#65292;&#20197;&#39564;&#35777;&#20854;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fair machine learning is a thriving and vibrant research topic. In this paper, we propose Fairness as a Service (FaaS), a secure, verifiable and privacy-preserving protocol to computes and verify the fairness of any machine learning (ML) model. In the deisgn of FaaS, the data and outcomes are represented through cryptograms to ensure privacy. Also, zero knowledge proofs guarantee the well-formedness of the cryptograms and underlying data. FaaS is model--agnostic and can support various fairness metrics; hence, it can be used as a service to audit the fairness of any ML model. Our solution requires no trusted third party or private channels for the computation of the fairness metric. The security guarantees and commitments are implemented in a way that every step is securely transparent and verifiable from the start to the end of the process. The cryptograms of all input data are publicly available for everyone, e.g., auditors, social activists and experts, to verify the correctness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06054</link><description>&lt;p&gt;
&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#23545;&#21512;&#25104;&#20219;&#21153;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21363;&#20174;&#19978;&#19979;&#25991;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#26159;Transformer&#30340;&#19968;&#39033;&#24341;&#20154;&#27880;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39537;&#21160;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#35282;&#24230;&#36827;&#34892;&#35843;&#26597;&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#34920;&#31034;&#26356;&#21152;&#22797;&#26434;&#65292;&#34920;&#31034;&#21487;&#20197;&#21463;&#21040;&#27169;&#22411;&#26435;&#37325;&#21644;&#19978;&#19979;&#25991;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#19978;&#36848;&#20004;&#20010;&#27010;&#24565;&#26041;&#38754;&#30340;&#34920;&#31034;&#20998;&#21035;&#31216;&#20026;&#26435;&#37325;&#20869;&#37096;&#25104;&#20998;&#21644;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20004;&#20010;&#25104;&#20998;&#22914;&#20309;&#24433;&#21709;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#20174;&#32780;&#21487;&#20197;&#35774;&#35745;&#20004;&#20010;&#25506;&#38024;&#65292;&#21363;&#26435;&#37325;&#20869;&#37096;&#25506;&#38024;&#21644;&#19978;&#19979;&#25991;&#25506;&#38024;&#65292;&#20998;&#21035;&#35780;&#20272;&#36825;&#20004;&#20010;&#25104;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#30340;&#22909;&#22351;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#39640;&#24230;&#30456;&#20851;&#65292;&#36825;&#34920;&#26126;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32416;&#32544;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#26426;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#31070;&#32463;&#20803;&#26435;&#37325;&#26469;&#25214;&#21040;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20998;&#31163;&#36229;&#24179;&#38754;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#22823;&#22411;&#25110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36866;&#24403;&#36716;&#25442;&#21021;&#22987;&#25968;&#25454;&#38598;&#26469;&#23558;&#20998;&#31163;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#31867;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06049</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#26426;&#30340;&#32447;&#24615;&#20998;&#31163;&#31934;&#32454;&#36924;&#36817;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Perceptron-based Fine Approximation Technique for Linear Separation. (arXiv:2309.06049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#26426;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#31070;&#32463;&#20803;&#26435;&#37325;&#26469;&#25214;&#21040;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20998;&#31163;&#36229;&#24179;&#38754;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#22823;&#22411;&#25110;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36866;&#24403;&#36716;&#25442;&#21021;&#22987;&#25968;&#25454;&#38598;&#26469;&#23558;&#20998;&#31163;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#31867;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040;&#26631;&#35760;&#20026;&#27491;&#25110;&#36127;&#30340;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20998;&#31163;&#36229;&#24179;&#38754;&#12290;&#30001;&#20110;&#20154;&#24037;&#31070;&#32463;&#20803;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#21487;&#20197;&#30452;&#25509;&#19982;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#36229;&#24179;&#38754;&#30456;&#20851;&#32852;&#65292;&#25152;&#20197;&#35813;&#25216;&#26415;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#35757;&#32451;&#22522;&#20110;&#24863;&#30693;&#26426;&#30340;&#20108;&#20998;&#31867;&#22120;&#12290;&#22312;&#22823;&#22411;&#25110;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#35299;&#26512;&#25110;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#21464;&#24471;&#31105;&#27490;&#21644;&#19981;&#23454;&#38469;&#65292;&#32780;&#21551;&#21457;&#24335;&#21644;&#36817;&#20284;&#25216;&#26415;&#20173;&#28982;&#36866;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#24863;&#30693;&#26426;&#31639;&#27861;&#65292;&#20294;&#22312;&#25628;&#32034;&#20998;&#31163;&#36229;&#24179;&#38754;&#26399;&#38388;&#21482;&#35843;&#25972;&#31070;&#32463;&#20803;&#26435;&#37325;&#30340;&#24517;&#35201;&#31243;&#24230;&#12290;&#36890;&#36807;&#36866;&#24403;&#36716;&#25442;&#21021;&#22987;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#32771;&#34385;&#25968;&#25454;&#26631;&#31614;&#65292;&#20063;&#19981;&#38656;&#35201;&#32771;&#34385;&#20559;&#32622;&#39033;&#65292;&#23558;&#21487;&#20998;&#24615;&#38477;&#20302;&#20026;&#19968;&#31867;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#25910;&#25947;&#65307;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;m
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel online learning method that aims at finding a separator hyperplane between data points labelled as either positive or negative. Since weights and biases of artificial neurons can directly be related to hyperplanes in high-dimensional spaces, the technique is applicable to train perceptron-based binary classifiers in machine learning. In case of large or imbalanced data sets, use of analytical or gradient-based solutions can become prohibitive and impractical, where heuristics and approximation techniques are still applicable. The proposed method is based on the Perceptron algorithm, however, it tunes neuron weights in just the necessary extent during searching the separator hyperplane. Due to an appropriate transformation of the initial data set we need not to consider data labels, neither the bias term. respectively, reducing separability to a one-class classification problem. The presented method has proven converge; empirical results show that it can be m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#22120;&#21463;&#26631;&#31614;&#22122;&#22768;&#24433;&#21709;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#21463;&#21040;&#26631;&#31614;&#22122;&#22768;&#24433;&#21709;&#30340;&#20803;&#35757;&#32451;&#20013;&#65292;Reptile&#12289;iMAML&#21644;foMAML&#22312;Omniglot&#21644;CifarFS&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#20102;&#26368;&#39640;&#36798;42%&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Man&#21644;BatMan&#20004;&#31181;&#37319;&#26679;&#25216;&#26415;&#65292;&#23558;&#26377;&#22122;&#22768;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#36716;&#21464;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.06046</link><description>&lt;p&gt;
BatMan-CLR: &#20351;&#24471;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#22120;&#23545;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise. (arXiv:2309.06046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#22120;&#21463;&#26631;&#31614;&#22122;&#22768;&#24433;&#21709;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#21463;&#21040;&#26631;&#31614;&#22122;&#22768;&#24433;&#21709;&#30340;&#20803;&#35757;&#32451;&#20013;&#65292;Reptile&#12289;iMAML&#21644;foMAML&#22312;Omniglot&#21644;CifarFS&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#20102;&#26368;&#39640;&#36798;42%&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Man&#21644;BatMan&#20004;&#31181;&#37319;&#26679;&#25216;&#26415;&#65292;&#23558;&#26377;&#22122;&#22768;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#36716;&#21464;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#23545;&#20110;&#32463;&#20856;&#30340;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#26377;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#26159;&#22312;&#20803;&#23398;&#20064;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20803;&#23398;&#20064;&#22120;&#26088;&#22312;&#36890;&#36807;&#22312;&#20803;&#35757;&#32451;&#20013;&#23398;&#20064;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#22987;&#27169;&#22411;&#65292;&#24182;&#22312;&#20803;&#27979;&#35797;&#26399;&#38388;&#26681;&#25454;&#26032;&#20219;&#21153;&#36827;&#34892;&#36830;&#32493;&#24494;&#35843;&#65292;&#20197;&#36866;&#24212;&#26410;&#30693;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#26368;&#20808;&#36827;&#30340;&#20803;&#23398;&#20064;&#22120;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;&#26799;&#24230;&#30340;N-way K-shot&#23398;&#20064;&#22120;&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20803;&#35757;&#32451;&#21463;&#21040;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#26102;&#65292;Reptile&#12289;iMAML&#21644;foMAML&#22312;Omniglot&#21644;CifarFS&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#20102;&#26368;&#39640;&#36798;42%&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#37319;&#26679;&#25216;&#26415;&#65292;&#21363;&#27969;&#24418;&#65288;Man&#65289;&#21644;&#25209;&#27425;&#27969;&#24418;&#65288;BatMan&#65289;&#65292;&#23558;&#26377;&#22122;&#22768;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#36716;&#21464;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#65292;&#20197;&#22686;&#21152;&#22122;&#22768;&#26631;&#31614;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;N-way 2-contrastiv&#30340;&#27969;&#24418;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
The negative impact of label noise is well studied in classical supervised learning yet remains an open research question in meta-learning. Meta-learners aim to adapt to unseen learning tasks by learning a good initial model in meta-training and consecutively fine-tuning it according to new tasks during meta-testing. In this paper, we present the first extensive analysis of the impact of varying levels of label noise on the performance of state-of-the-art meta-learners, specifically gradient-based $N$-way $K$-shot learners. We show that the accuracy of Reptile, iMAML, and foMAML drops by up to 42% on the Omniglot and CifarFS datasets when meta-training is affected by label noise. To strengthen the resilience against label noise, we propose two sampling techniques, namely manifold (Man) and batch manifold (BatMan), which transform the noisy supervised learners into semi-supervised ones to increase the utility of noisy labels. We first construct manifold samples of $N$-way $2$-contrastiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#24577;&#23398;&#20064;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;NLGAD&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#23545;&#27604;&#23398;&#20064;&#32593;&#32476;&#26469;&#22686;&#24378;&#23398;&#20064;&#27491;&#24120;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#20197;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06034</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#24577;&#23398;&#20064;&#30340;&#22810;&#23610;&#24230;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning. (arXiv:2309.06034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#24577;&#23398;&#20064;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;NLGAD&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#23545;&#27604;&#23398;&#20064;&#32593;&#32476;&#26469;&#22686;&#24378;&#23398;&#20064;&#27491;&#24120;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#20197;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#39046;&#22495;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#25429;&#25417;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;GAD&#20013;&#33410;&#28857;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#26816;&#27979;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20219;&#21153;&#30340;&#29305;&#24615;&#20173;&#28982;&#30456;&#23545;&#19981;&#36275;&#12290;GAD&#26088;&#22312;&#35782;&#21035;&#19982;&#22823;&#22810;&#25968;&#33410;&#28857;&#26377;&#25152;&#20559;&#31163;&#30340;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#23398;&#20064;&#21040;&#32452;&#25104;&#22823;&#22810;&#25968;&#26679;&#26412;&#30340;&#27491;&#24120;&#26679;&#26412;&#30340;&#27169;&#24335;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24403;&#24322;&#24120;&#34892;&#20026;&#19982;&#27491;&#24120;&#24615;&#19981;&#21516;&#30340;&#26102;&#20505;&#65292;&#24322;&#24120;&#21487;&#20197;&#24456;&#23481;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#27491;&#24120;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#24577;&#23398;&#20064;&#30340;GAD&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#23610;&#24230;&#23545;&#27604;&#23398;&#20064;&#32593;&#32476;&#65288;&#31616;&#31216;NLGAD&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#19981;&#21516;&#23610;&#24230;&#30340;&#23545;&#27604;&#32593;&#32476;&#21021;&#22987;&#21270;&#27169;&#22411;&#65292;&#20197;&#25552;&#20379;&#20805;&#36275;&#21487;&#38752;&#30340;&#27491;&#24120;&#33410;&#28857;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection (GAD) has attracted increasing attention in machine learning and data mining. Recent works have mainly focused on how to capture richer information to improve the quality of node embeddings for GAD. Despite their significant advances in detection performance, there is still a relative dearth of research on the properties of the task. GAD aims to discern the anomalies that deviate from most nodes. However, the model is prone to learn the pattern of normal samples which make up the majority of samples. Meanwhile, anomalies can be easily detected when their behaviors differ from normality. Therefore, the performance can be further improved by enhancing the ability to learn the normal pattern. To this end, we propose a normality learning-based GAD framework via multi-scale contrastive learning networks (NLGAD for abbreviation). Specifically, we first initialize the model with the contrastive networks on different scales. To provide sufficient and reliable normal nod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#33021;&#37327;&#25910;&#38598;&#35774;&#22791;&#19982;&#22810;&#20449;&#36947;ALOHA&#30456;&#32467;&#21512;&#30340;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#33021;&#37327;&#32791;&#23613;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#20219;&#21153;&#30340;&#39034;&#21033;&#25191;&#34892;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#20110;&#22522;&#20110;&#35268;&#33539;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.06033</link><description>&lt;p&gt;
&#20855;&#26377;&#20998;&#24067;&#24335;&#29992;&#25143;&#37319;&#26679;&#21644;&#22810;&#20449;&#36947;ALOHA&#30340;&#33021;&#28304;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Energy-Aware Federated Learning with Distributed User Sampling and Multichannel ALOHA. (arXiv:2309.06033v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#33021;&#37327;&#25910;&#38598;&#35774;&#22791;&#19982;&#22810;&#20449;&#36947;ALOHA&#30456;&#32467;&#21512;&#30340;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#33021;&#37327;&#32791;&#23613;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#20219;&#21153;&#30340;&#39034;&#21033;&#25191;&#34892;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#20110;&#22522;&#20110;&#35268;&#33539;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20986;&#29616;&#65292;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36793;&#32536;&#35774;&#22791;&#36890;&#24120;&#20855;&#26377;&#26377;&#38480;&#30340;&#30005;&#27744;&#21644;&#24322;&#26500;&#30340;&#33021;&#28304;&#21487;&#29992;&#24615;&#65292;&#32780;FL&#38656;&#35201;&#22810;&#36718;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#65292;&#21152;&#22823;&#20102;&#23545;&#33021;&#28304;&#25928;&#29575;&#30340;&#38656;&#27714;&#12290;&#33021;&#28304;&#32791;&#23613;&#21487;&#33021;&#20250;&#38459;&#30861;&#35757;&#32451;&#36807;&#31243;&#21644;&#35757;&#32451;&#27169;&#22411;&#30340;&#39640;&#25928;&#21033;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#32771;&#34385;&#23558;&#33021;&#37327;&#25910;&#38598;&#65288;EH&#65289;&#35774;&#22791;&#38598;&#25104;&#21040;&#20855;&#26377;&#22810;&#20449;&#36947;ALOHA&#30340;FL&#32593;&#32476;&#20013;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#20445;&#35777;&#20302;&#33021;&#28304;&#20013;&#26029;&#27010;&#29575;&#21644;&#25104;&#21151;&#25191;&#34892;&#26410;&#26469;&#20219;&#21153;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#24179;&#22343;&#33021;&#37327;&#25910;&#20837;&#26080;&#27861;&#35206;&#30422;&#36845;&#20195;&#25104;&#26412;&#30340;&#20851;&#38190;&#35774;&#32622;&#20013;&#12290;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#26102;&#38388;&#21644;&#30005;&#27744;&#30005;&#37327;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#35268;&#33539;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed learning on edge devices has attracted increased attention with the advent of federated learning (FL). Notably, edge devices often have limited battery and heterogeneous energy availability, while multiple rounds are required in FL for convergence, intensifying the need for energy efficiency. Energy depletion may hinder the training process and the efficient utilization of the trained model. To solve these problems, this letter considers the integration of energy harvesting (EH) devices into a FL network with multi-channel ALOHA, while proposing a method to ensure both low energy outage probability and successful execution of future tasks. Numerical results demonstrate the effectiveness of this method, particularly in critical setups where the average energy income fails to cover the iteration cost. The method outperforms a norm based solution in terms of convergence time and battery level.
&lt;/p&gt;</description></item><item><title>&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32039;&#24613;&#36890;&#20449;&#65288;EC-MARL&#65289;&#26159;&#35299;&#20915;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#20013;&#39640;&#32500;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#21512;&#20316;&#26041;&#24335;&#65292;&#23427;&#36171;&#20104;&#32593;&#32476;&#23454;&#20307;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.06021</link><description>&lt;p&gt;
&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32039;&#24613;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Emergent Communication in Multi-Agent Reinforcement Learning for Future Wireless Networks. (arXiv:2309.06021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06021
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32039;&#24613;&#36890;&#20449;&#65288;EC-MARL&#65289;&#26159;&#35299;&#20915;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#20013;&#39640;&#32500;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#21512;&#20316;&#26041;&#24335;&#65292;&#23427;&#36171;&#20104;&#32593;&#32476;&#23454;&#20307;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#30340;&#26080;&#32447;&#32593;&#32476;&#22330;&#26223;&#20013;&#65292;&#22810;&#20010;&#32593;&#32476;&#23454;&#20307;&#38656;&#35201;&#21512;&#20316;&#20197;&#23454;&#29616;&#20849;&#21516;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#24310;&#36831;&#21644;&#33021;&#28304;&#28040;&#32791;&#12290;&#26410;&#26469;&#30340;&#26080;&#32447;&#32593;&#32476;&#35201;&#27714;&#22312;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#20132;&#25442;&#39640;&#32500;&#25968;&#25454;&#65292;&#22240;&#27492;&#23454;&#26045;&#36890;&#20449;&#25511;&#21046;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#39640;&#24230;&#22797;&#26434;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#32039;&#24613;&#36890;&#20449;&#65288;EC-MARL&#65289;&#26159;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#27979;&#29366;&#24577;&#19979;&#30340;&#39640;&#32500;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#21512;&#20316;&#26041;&#24335;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#26500;&#24314;&#32039;&#24613;&#36890;&#20449;&#21327;&#35758;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;EC-MARL&#22312;&#26410;&#26469;6G&#26080;&#32447;&#32593;&#32476;&#32972;&#26223;&#19979;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#32593;&#32476;&#36171;&#20104;&#32593;&#32476;&#23454;&#20307;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#26469;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#12289;&#26426;&#22120;&#20154;&#23548;&#33322;&#12289;&#39134;&#34892;&#22522;&#31449;&#32593;&#32476;&#35268;&#21010;&#21644;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#31561;&#22797;&#26434;&#20219;&#21153;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;EC-MARL&#31639;&#27861;&#21644;
&lt;/p&gt;
&lt;p&gt;
In different wireless network scenarios, multiple network entities need to cooperate in order to achieve a common task with minimum delay and energy consumption. Future wireless networks mandate exchanging high dimensional data in dynamic and uncertain environments, therefore implementing communication control tasks becomes challenging and highly complex. Multi-agent reinforcement learning with emergent communication (EC-MARL) is a promising solution to address high dimensional continuous control problems with partially observable states in a cooperative fashion where agents build an emergent communication protocol to solve complex tasks. This paper articulates the importance of EC-MARL within the context of future 6G wireless networks, which imbues autonomous decision-making capabilities into network entities to solve complex tasks such as autonomous driving, robot navigation, flying base stations network planning, and smart city applications. An overview of EC-MARL algorithms and the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#12289;&#36924;&#36817;&#21644;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#25511;&#21046;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36890;&#29992;&#25554;&#20540;&#21644;&#36890;&#29992;&#36924;&#36817;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#24615;&#36136;&#19981;&#33021;&#20114;&#30456;&#25512;&#23548;&#65292;&#20294;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#20004;&#20010;&#24615;&#36136;&#20063;&#26159;&#31561;&#20215;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.06015</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#12289;&#36924;&#36817;&#21644;&#21487;&#25511;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Interpolation, Approximation and Controllability of Deep Neural Networks. (arXiv:2309.06015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#12289;&#36924;&#36817;&#21644;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#25511;&#21046;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36890;&#29992;&#25554;&#20540;&#21644;&#36890;&#29992;&#36924;&#36817;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#24615;&#36136;&#19981;&#33021;&#20114;&#30456;&#25512;&#23548;&#65292;&#20294;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#20004;&#20010;&#24615;&#36136;&#20063;&#26159;&#31561;&#20215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25511;&#21046;&#29702;&#35770;&#65292;&#30740;&#31350;&#20102;&#23558;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#29702;&#24819;&#21270;&#20026;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20004;&#20010;&#24615;&#36136;&#65292;&#21363;&#36890;&#29992;&#25554;&#20540; - &#33021;&#22815;&#21305;&#37197;&#20219;&#24847;&#36755;&#20837;&#21644;&#30446;&#26631;&#35757;&#32451;&#26679;&#26412; - &#20197;&#21450;&#32039;&#23494;&#30456;&#20851;&#30340;&#36890;&#29992;&#36924;&#36817; - &#33021;&#22815;&#36890;&#36807;&#27969;&#21160;&#22270;&#36817;&#20284;&#36755;&#20837;-&#30446;&#26631;&#20989;&#25968;&#20851;&#31995;&#12290;&#22312;&#25511;&#21046;&#23478;&#26063;&#20855;&#26377;&#20223;&#23556;&#19981;&#21464;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36890;&#29992;&#25554;&#20540;&#30340;&#21051;&#30011;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#38750;&#32447;&#24615;&#30340;&#20219;&#20309;&#26550;&#26500;&#65292;&#23427;&#37117;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#36890;&#29992;&#25554;&#20540;&#21644;&#36890;&#29992;&#36924;&#36817;&#22312;&#19968;&#33324;&#25511;&#21046;&#31995;&#32479;&#32972;&#26223;&#19979;&#30340;&#20851;&#31995;&#65292;&#34920;&#26126;&#36825;&#20004;&#20010;&#24615;&#36136;&#19981;&#33021;&#20174;&#24444;&#27492;&#25512;&#23548;&#20986;&#26469;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25511;&#21046;&#23478;&#26063;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#26465;&#20214;&#65292;&#30830;&#20445;&#20102;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the expressive power of deep residual neural networks idealized as continuous dynamical systems through control theory. Specifically, we consider two properties that arise from supervised learning, namely universal interpolation - the ability to match arbitrary input and target training samples - and the closely related notion of universal approximation the ability to approximate input-target functional relationships via flow maps. Under the assumption of affine invariance of the control family, we give a characterisation of universal interpolation, showing that it holds for essentially any architecture with non-linearity. Furthermore, we elucidate the relationship between universal interpolation and universal approximation in the context of general control systems, showing that the two properties cannot be deduced from each other. At the same time, we identify conditions on the control family and the target function that ensures the equivalence of the two notions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#26694;&#26550;&#26469;&#22788;&#29702;&#39046;&#22495;&#36716;&#31227;&#21644;&#35821;&#20041;&#36716;&#31227;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20840;&#23616;&#20302;&#32423;&#29305;&#24449;&#21644;&#23494;&#38598;&#39640;&#32423;&#29305;&#24449;&#22270;&#26469;&#36866;&#24212;&#27169;&#22411;&#21040;&#26410;&#35265;&#39046;&#22495;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#22312;&#26816;&#27979;&#26032;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05994</link><description>&lt;p&gt;
ATTA: &#19968;&#31181;&#38024;&#23545;&#20998;&#21106;&#20013;&#30340;&#21306;&#20998;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24322;&#24120;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation. (arXiv:2309.05994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#26694;&#26550;&#26469;&#22788;&#29702;&#39046;&#22495;&#36716;&#31227;&#21644;&#35821;&#20041;&#36716;&#31227;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20840;&#23616;&#20302;&#32423;&#29305;&#24449;&#21644;&#23494;&#38598;&#39640;&#32423;&#29305;&#24449;&#22270;&#26469;&#36866;&#24212;&#27169;&#22411;&#21040;&#26410;&#35265;&#39046;&#22495;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#22312;&#26816;&#27979;&#26032;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#31264;&#23494;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20855;&#26377;&#30456;&#20284;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#20551;&#35774;&#23427;&#20204;&#20043;&#38388;&#19981;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#19979;&#24120;&#24120;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#65292;&#24182;&#19988;&#26174;&#33879;&#24433;&#21709;&#29616;&#26377;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#22330;&#22806;&#20998;&#24067;&#26816;&#27979;&#26694;&#26550;&#65292;&#21516;&#26102;&#22788;&#29702;&#39046;&#22495;&#36716;&#31227;&#21644;&#35821;&#20041;&#36716;&#31227;&#12290;&#31532;&#19968;&#23618;&#21033;&#29992;&#20840;&#23616;&#20302;&#32423;&#29305;&#24449;&#21306;&#20998;&#22270;&#20687;&#20013;&#26159;&#21542;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#65292;&#32780;&#31532;&#20108;&#23618;&#21033;&#29992;&#23494;&#38598;&#39640;&#32423;&#29305;&#24449;&#22270;&#35782;&#21035;&#20855;&#26377;&#35821;&#20041;&#36716;&#31227;&#30340;&#20687;&#32032;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#36866;&#24212;&#27169;&#22411;&#21040;&#26410;&#35265;&#39046;&#22495;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#22312;&#26816;&#27979;&#26032;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22330;&#22806;&#20998;&#21106;&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324; those with significant domain shifts and those without&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in dense out-of-distribution (OOD) detection have primarily focused on scenarios where the training and testing datasets share a similar domain, with the assumption that no domain shift exists between them. However, in real-world situations, domain shift often exits and significantly affects the accuracy of existing out-of-distribution (OOD) detection models. In this work, we propose a dual-level OOD detection framework to handle domain shift and semantic shift jointly. The first level distinguishes whether domain shift exists in the image by leveraging global low-level features, while the second level identifies pixels with semantic shift by utilizing dense high-level feature maps. In this way, we can selectively adapt the model to unseen domains as well as enhance model's capacity in detecting novel classes. We validate the efficacy of our proposed method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, obse
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#23398;&#20064;&#26080;&#20559;&#35265;&#30340;&#26032;&#38395;&#25991;&#31456;&#34920;&#31034;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#26032;&#38395;&#25991;&#31456;&#30340;&#25919;&#27835;&#20542;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#23398;&#20064;&#27169;&#22411;&#21463;&#26032;&#38395;&#21457;&#24067;&#32773;&#25919;&#27835;&#20559;&#35265;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05981</link><description>&lt;p&gt;
&#23398;&#20064;&#26080;&#20559;&#35265;&#30340;&#26032;&#38395;&#25991;&#31456;&#34920;&#24449;&#65306;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Unbiased News Article Representations: A Knowledge-Infused Approach. (arXiv:2309.05981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05981
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#23398;&#20064;&#26080;&#20559;&#35265;&#30340;&#26032;&#38395;&#25991;&#31456;&#34920;&#31034;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#26032;&#38395;&#25991;&#31456;&#30340;&#25919;&#27835;&#20542;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#23398;&#20064;&#27169;&#22411;&#21463;&#26032;&#38395;&#21457;&#24067;&#32773;&#25919;&#27835;&#20559;&#35265;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35299;&#31038;&#20250;&#32676;&#20307;&#20013;&#25919;&#27835;&#24847;&#35782;&#24418;&#24577;&#30340;&#21160;&#24577;&#21644;&#20943;&#36731;&#20854;&#24433;&#21709;&#30340;&#25514;&#26045;&#26041;&#38754;&#65292;&#23545;&#22312;&#32447;&#26032;&#38395;&#25991;&#31456;&#30340;&#25919;&#27835;&#20542;&#21521;&#36827;&#34892;&#37327;&#21270;&#26377;&#21161;&#20110;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#26032;&#38395;&#25991;&#31456;&#30340;&#25919;&#27835;&#20542;&#21521;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#22240;&#20026;(i)&#26032;&#38395;&#25991;&#31456;&#30340;&#25919;&#27835;&#24847;&#35782;&#24418;&#24577;&#30001;&#22810;&#20010;&#22240;&#32032;&#23450;&#20041;&#65292;&#20197;&#21450;(ii)&#29616;&#26377;&#23398;&#20064;&#27169;&#22411;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21463;&#26032;&#38395;&#21457;&#24067;&#32773;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#24433;&#21709;&#32780;&#20559;&#20506;&#12290;&#30446;&#21069;&#21482;&#26377;&#26377;&#38480;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#26032;&#38395;&#25991;&#31456;&#30340;&#25919;&#27835;&#20542;&#21521;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#27809;&#26377;&#32771;&#34385;&#31639;&#27861;&#30340;&#25919;&#27835;&#20559;&#35265;&#65292;&#36825;&#38477;&#20302;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20219;&#20309;&#26032;&#38395;&#21457;&#24067;&#32773;&#21457;&#24067;&#30340;&#26032;&#38395;&#25991;&#31456;&#30340;&#25919;&#27835;&#20542;&#21521;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#30456;&#23545;&#21487;&#38752;&#30340;&#22806;&#37096;&#25968;&#25454;&#36164;&#28304;&#26469;&#23398;&#20064;&#26080;&#20559;&#35265;&#30340;&#26032;&#38395;&#25991;&#31456;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification of the political leaning of online news articles can aid in understanding the dynamics of political ideology in social groups and measures to mitigating them. However, predicting the accurate political leaning of a news article with machine learning models is a challenging task. This is due to (i) the political ideology of a news article is defined by several factors, and (ii) the innate nature of existing learning models to be biased with the political bias of the news publisher during the model training. There is only a limited number of methods to study the political leaning of news articles which also do not consider the algorithmic political bias which lowers the generalization of machine learning models to predict the political leaning of news articles published by any new news publishers. In this work, we propose a knowledge-infused deep learning model that utilizes relatively reliable external data resources to learn unbiased representations of news articles usin
&lt;/p&gt;</description></item><item><title>CleanUNet 2&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#27874;&#24418;&#21644;&#39057;&#35889;&#22270;&#30340;&#28151;&#21512;&#35821;&#38899;&#38477;&#22122;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#23427;&#26500;&#24314;&#22312;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27874;&#24418;&#38477;&#22122;&#22120;CleanUNet&#30340;&#22522;&#30784;&#19978;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#30340;&#39057;&#35889;&#22270;&#20316;&#20026;&#36755;&#20837;&#26469;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05975</link><description>&lt;p&gt;
CleanUNet 2&#65306;&#19968;&#31181;&#22522;&#20110;&#27874;&#24418;&#21644;&#39057;&#35889;&#22270;&#30340;&#28151;&#21512;&#35821;&#38899;&#38477;&#22122;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram. (arXiv:2309.05975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05975
&lt;/p&gt;
&lt;p&gt;
CleanUNet 2&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#27874;&#24418;&#21644;&#39057;&#35889;&#22270;&#30340;&#28151;&#21512;&#35821;&#38899;&#38477;&#22122;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#23427;&#26500;&#24314;&#22312;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27874;&#24418;&#38477;&#22122;&#22120;CleanUNet&#30340;&#22522;&#30784;&#19978;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#30340;&#39057;&#35889;&#22270;&#20316;&#20026;&#36755;&#20837;&#26469;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CleanUNet 2&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#27874;&#24418;&#38477;&#22122;&#22120;&#21644;&#39057;&#35889;&#22270;&#38477;&#22122;&#22120;&#20248;&#28857;&#24182;&#19988;&#21462;&#24471;&#20102;&#26368;&#20339;&#25928;&#26524;&#30340;&#35821;&#38899;&#38477;&#22122;&#27169;&#22411;&#12290;CleanUNet 2&#37319;&#29992;&#20102;&#19968;&#20010;&#21463;&#27969;&#34892;&#30340;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#21551;&#21457;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#27874;&#24418;&#27169;&#22411;&#21644;&#19968;&#20010;&#39057;&#35889;&#22270;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CleanUNet 2&#22312;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27874;&#24418;&#38477;&#22122;&#22120;CleanUNet&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#39057;&#35889;&#22270;&#38477;&#22122;&#22120;&#39044;&#27979;&#30340;&#39057;&#35889;&#22270;&#20316;&#20026;&#36755;&#20837;&#26469;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CleanUNet 2&#22312;&#21508;&#31181;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present CleanUNet 2, a speech denoising model that combines the advantages of waveform denoiser and spectrogram denoiser and achieves the best of both worlds. CleanUNet 2 uses a two-stage framework inspired by popular speech synthesis methods that consist of a waveform model and a spectrogram model. Specifically, CleanUNet 2 builds upon CleanUNet, the state-of-the-art waveform denoiser, and further boosts its performance by taking predicted spectrograms from a spectrogram denoiser as the input. We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#36335;&#24452;&#26469;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#33391;&#34892;&#20026;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#20165;&#28040;&#34701;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#24182;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.05973</link><description>&lt;p&gt;
&#20999;&#26029;&#30005;&#36335;: &#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#21435;&#38500;&#27169;&#22411;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Circuit Breaking: Removing Model Behaviors with Targeted Ablation. (arXiv:2309.05973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#36335;&#24452;&#26469;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#33391;&#34892;&#20026;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#20165;&#28040;&#34701;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#24182;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#34920;&#29616;&#20986;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#19978;&#25552;&#39640;&#24615;&#33021;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#38477;&#20302;&#24615;&#33021;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#22240;&#26524;&#36335;&#24452;&#65292;&#20197;&#31105;&#29992;&#19982;&#19981;&#33391;&#34892;&#20026;&#26377;&#20851;&#30340;&#35745;&#31639;&#30005;&#36335;&#65292;&#20174;&#32780;&#21435;&#38500;&#19981;&#33391;&#34892;&#20026;&#12290;&#22312;&#25317;&#26377;&#27169;&#22411;&#34920;&#29616;&#24046;&#30340;&#23567;&#22411;&#36755;&#20837;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23398;&#20250;&#20102;&#28040;&#34701;&#19968;&#23567;&#37096;&#20998;&#37325;&#35201;&#30340;&#22240;&#26524;&#36335;&#24452;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#28040;&#34701;&#20165;&#20165;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#65292;&#21487;&#20197;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. We propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. Given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. In the setting of reducing GPT-2 toxic language generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23618;&#30697;&#38453;&#20998;&#35299;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#23618;&#32534;&#30721;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#27969;&#24418;&#21644;&#25968;&#23398;&#36816;&#31639;&#30340;&#20960;&#20309;&#24615;&#36136;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#31361;&#30772;&#32500;&#24230;&#35781;&#21650;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.05968</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23618;&#30697;&#38453;&#20998;&#35299;&#25581;&#31034;&#28508;&#22312;&#27969;&#24418;&#32534;&#30721;&#21644;&#23384;&#20648;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Neural Network Layer Matrix Decomposition reveals Latent Manifold Encoding and Memory Capacity. (arXiv:2309.05968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05968
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23618;&#30697;&#38453;&#20998;&#35299;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#23618;&#32534;&#30721;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#27969;&#24418;&#21644;&#25968;&#23398;&#36816;&#31639;&#30340;&#20960;&#20309;&#24615;&#36136;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#31361;&#30772;&#32500;&#24230;&#35781;&#21650;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#30340;&#36870;&#23450;&#29702;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;(NN)&#32534;&#30721;&#23450;&#29702;&#65292;&#23427;&#34920;&#26126;&#23545;&#20110;&#27599;&#20010;&#31283;&#23450;&#25910;&#25947;&#30340;NN&#21644;&#36830;&#32493;&#28608;&#27963;&#20989;&#25968;&#65292;&#20854;&#26435;&#37325;&#30697;&#38453;&#23454;&#38469;&#19978;&#32534;&#30721;&#20102;&#19968;&#20010;&#36830;&#32493;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#22312;&#26377;&#30028;&#22495;&#20869;&#36817;&#20284;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#35823;&#24046;&#22312;&#26377;&#38480;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20351;&#29992;&#29305;&#24449;&#20540;&#20998;&#35299;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#23545;&#27599;&#20010;NN&#23618;&#30340;&#26435;&#37325;&#30697;&#38453;&#36827;&#34892;&#30697;&#38453;&#20998;&#35299;&#65292;&#21487;&#20197;&#25581;&#31034;&#35757;&#32451;&#25968;&#25454;&#38598;&#25152;&#32534;&#30721;&#21644;&#34920;&#31034;&#30340;&#28508;&#22312;&#31354;&#38388;&#27969;&#24418;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#27599;&#20010;NN&#23618;&#25191;&#34892;&#30340;&#20960;&#20309;&#25805;&#20316;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#29702;&#35299;NN&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#23384;&#20648;&#23481;&#37327;&#26469;&#31361;&#30772;&#32500;&#24230;&#35781;&#21650;&#20855;&#26377;&#24847;&#20041;&#65292;&#24182;&#19988;&#36825;&#20004;&#32773;&#26159;&#20114;&#34917;&#30340;&#12290;&#36825;&#31181;&#23618;&#30697;&#38453;&#20998;&#35299;(LMD)&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;NN&#23618;&#30340;&#29305;&#24449;&#20540;&#20998;&#35299;&#19982;&#26368;&#26032;&#30340;&#30740;&#31350;&#26377;&#23494;&#20999;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove the converse of the universal approximation theorem, i.e. a neural network (NN) encoding theorem which shows that for every stably converged NN of continuous activation functions, its weight matrix actually encodes a continuous function that approximates its training dataset to within a finite margin of error over a bounded domain. We further show that using the Eckart-Young theorem for truncated singular value decomposition of the weight matrix for every NN layer, we can illuminate the nature of the latent space manifold of the training dataset encoded and represented by every NN layer, and the geometric nature of the mathematical operations performed by each NN layer. Our results have implications for understanding how NNs break the curse of dimensionality by harnessing memory capacity for expressivity, and that the two are complementary. This Layer Matrix Decomposition (LMD) further suggests a close relationship between eigen-decomposition of NN layers and the latest advanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>GLAD&#26159;&#19968;&#20010;&#20869;&#23481;&#24863;&#30693;&#30340;&#21160;&#24577;&#22270;&#29992;&#20110;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#23427;&#32508;&#21512;&#20102;&#26085;&#24535;&#35821;&#20041;&#12289;&#20851;&#31995;&#27169;&#24335;&#21644;&#39034;&#24207;&#27169;&#24335;&#65292;&#36890;&#36807;&#35782;&#21035;&#20851;&#38190;&#23383;&#27573;&#21644;&#26500;&#24314;&#21160;&#24577;&#26085;&#24535;&#22270;&#26469;&#26816;&#27979;&#31995;&#32479;&#26085;&#24535;&#20013;&#30340;&#20851;&#32852;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2309.05953</link><description>&lt;p&gt;
GLAD: &#20869;&#23481;&#24863;&#30693;&#30340;&#21160;&#24577;&#22270;&#29992;&#20110;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection. (arXiv:2309.05953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05953
&lt;/p&gt;
&lt;p&gt;
GLAD&#26159;&#19968;&#20010;&#20869;&#23481;&#24863;&#30693;&#30340;&#21160;&#24577;&#22270;&#29992;&#20110;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#23427;&#32508;&#21512;&#20102;&#26085;&#24535;&#35821;&#20041;&#12289;&#20851;&#31995;&#27169;&#24335;&#21644;&#39034;&#24207;&#27169;&#24335;&#65292;&#36890;&#36807;&#35782;&#21035;&#20851;&#38190;&#23383;&#27573;&#21644;&#26500;&#24314;&#21160;&#24577;&#26085;&#24535;&#22270;&#26469;&#26816;&#27979;&#31995;&#32479;&#26085;&#24535;&#20013;&#30340;&#20851;&#32852;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24405;&#20102;&#26377;&#20851;&#31995;&#32479;&#20449;&#24687;&#30340;&#26085;&#24535;&#22312;&#31995;&#32479;&#30417;&#25511;&#21644;&#35843;&#35797;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#20107;&#20214;&#21644;&#29366;&#24577;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#26085;&#24535;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#32771;&#34385;&#31995;&#32479;&#32452;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#65292;&#20363;&#22914;&#26381;&#21153;&#21644;&#29992;&#25143;&#65292;&#36825;&#20123;&#21487;&#20197;&#20174;&#26085;&#24535;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#26469;&#12290;&#29702;&#35299;&#36825;&#20123;&#20851;&#31995;&#23545;&#20110;&#26816;&#27979;&#24322;&#24120;&#21450;&#20854;&#28508;&#22312;&#21407;&#22240;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GLAD&#65292;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#31995;&#32479;&#26085;&#24535;&#20013;&#30340;&#20851;&#32852;&#24322;&#24120;&#12290;GLAD&#23558;&#26085;&#24535;&#35821;&#20041;&#12289;&#20851;&#31995;&#27169;&#24335;&#21644;&#39034;&#24207;&#27169;&#24335;&#32435;&#20837;&#32479;&#19968;&#26694;&#26550;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GLAD&#39318;&#20808;&#24341;&#20837;&#19968;&#20010;&#23383;&#27573;&#25552;&#21462;&#27169;&#22359;&#65292;&#21033;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20174;&#26085;&#24535;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#20851;&#38190;&#23383;&#27573;&#12290;&#28982;&#21518;&#65292;GLAD&#36890;&#36807;&#20114;&#36830;&#25552;&#21462;&#30340;&#23383;&#27573;&#21644;&#26085;&#24535;&#20107;&#20214;&#26500;&#24314;&#28369;&#21160;&#31383;&#21475;&#30340;&#21160;&#24577;&#26085;&#24535;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logs play a crucial role in system monitoring and debugging by recording valuable system information, including events and states. Although various methods have been proposed to detect anomalies in log sequences, they often overlook the significance of considering relations among system components, such as services and users, which can be identified from log contents. Understanding these relations is vital for detecting anomalies and their underlying causes. To address this issue, we introduce GLAD, a Graph-based Log Anomaly Detection framework designed to detect relational anomalies in system logs. GLAD incorporates log semantics, relational patterns, and sequential patterns into a unified framework for anomaly detection. Specifically, GLAD first introduces a field extraction module that utilizes prompt-based few-shot learning to identify essential fields from log contents. Then GLAD constructs dynamic log graphs for sliding windows by interconnecting extracted fields and log events p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05950</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;VLMs &#30340;&#24494;&#35843;&#26041;&#27861;&#20027;&#35201;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#25805;&#20316;&#65292;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810; VLMs &#20381;&#36182;&#20110;&#19987;&#26377;&#25968;&#25454;&#19988;&#19981;&#24320;&#28304;&#65292;&#38480;&#21046;&#20102;&#20351;&#29992;&#30333;&#30418;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;&#37492;&#20110;&#20687; ChatGPT &#36825;&#26679;&#30340;&#21463;&#27426;&#36814;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20173;&#28982;&#25552;&#20379;&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340; VLMs &#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#29305;&#24449;&#23884;&#20837;&#25110;&#36755;&#20986; logits &#30340;&#38656;&#35201;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340; LLMs &#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#20197;&#22312;&#20351;&#29992; CLIP &#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#31034;&#20363;&#20219;&#21153;&#20013;&#23547;&#25214;&#26368;&#20339;&#25991;&#26412;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;"&#29228;&#23665;"&#31243;&#24207;&#65292;&#23427;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#30340;&#25552;&#31034;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{bio}$FAME&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#30340;&#39044;&#35757;&#32451;&#12290;&#20854;&#36890;&#36807;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#36827;&#34892;&#34920;&#31034;&#21442;&#25968;&#21270;&#65292;&#21033;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.05927</link><description>&lt;p&gt;
&#38024;&#23545;&#29983;&#29289;&#20449;&#21495;&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals. (arXiv:2309.05927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{bio}$FAME&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#30340;&#39044;&#35757;&#32451;&#12290;&#20854;&#36890;&#36807;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#36827;&#34892;&#34920;&#31034;&#21442;&#25968;&#21270;&#65292;&#21033;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26469;&#33258;&#29983;&#29289;&#20449;&#21495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#23545;&#20154;&#20204;&#30340;&#36523;&#24515;&#29366;&#24577;&#36827;&#34892;&#32508;&#21512;&#24314;&#27169;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#36890;&#24120;&#22312;&#39044;&#35757;&#32451;&#21644;&#25512;&#26029;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36825;&#28304;&#20110;&#20219;&#21153;&#35268;&#33539;&#30340;&#21464;&#21270;&#25110;&#32773;&#27169;&#24577;&#32452;&#21512;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#22312;&#28508;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39057;&#29575;&#24863;&#30693;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;$\texttt{bio}$FAME&#65289;&#65292;&#35813;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#30340;&#34920;&#31034;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;$\texttt{bio}$FAME&#21253;&#21547;&#19968;&#20010;&#39057;&#29575;&#24863;&#30693;&#21464;&#21387;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#22266;&#23450;&#22823;&#23567;&#30340;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#19982;&#36755;&#20837;&#30340;&#38271;&#24230;&#21644;&#37319;&#26679;&#29575;&#26080;&#20851;&#12290;&#20026;&#20102;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#28508;&#31354;&#38388;&#20013;&#25191;&#34892;&#25513;&#30721;&#33258;&#32534;&#30721;&#12290;&#26368;&#32456;&#30340;&#26550;&#26500;&#26377;&#25928;&#22320;&#25429;&#33719;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#39057;&#29575;&#29305;&#24449;&#21644;&#27169;&#24577;&#32452;&#21512;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder ($\texttt{bio}$FAME) that learns to parameterize the representation of biosignals in the frequency space. $\texttt{bio}$FAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. The resulting architecture effectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#12290;&#32463;&#39564;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.05925</link><description>&lt;p&gt;
&#20851;&#20110;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Regularized Sparse Logistic Regression. (arXiv:2309.05925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#12290;&#32463;&#39564;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#26088;&#22312;&#21516;&#26102;&#36827;&#34892;&#39640;&#32500;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#30740;&#31350;&#35299;&#20915;&#20102;$\ell_1$&#27491;&#21017;&#21270;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#19982;&#38750;&#20984;&#24809;&#32602;&#30456;&#20851;&#30340;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#24182;&#27809;&#26377;&#31561;&#37327;&#30340;&#25991;&#29486;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#19968;&#23450;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#31867;&#20284;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#22312;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19981;&#21516;&#30340;&#32447;&#25628;&#32034;&#20934;&#21017;&#26469;&#20445;&#35777;&#19981;&#21516;&#27491;&#21017;&#21270;&#39033;&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse logistic regression aims to perform classification and feature selection simultaneously for high-dimensional data. Although many studies have been done to solve $\ell_1$-regularized logistic regression, there is no equivalently abundant literature about solving sparse logistic regression associated with nonconvex penalties. In this paper, we propose to solve $\ell_1$-regularized sparse logistic regression and some nonconvex penalties-regularized sparse logistic regression, when the nonconvex penalties satisfy some prerequisites, with similar optimization frameworks. In the proposed optimization frameworks, we utilize different line search criteria to guarantee good convergence performance for different regularization terms. Empirical experiments on binary classification tasks with real-world datasets demonstrate our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#21160;&#24577;&#35268;&#21010;&#24212;&#29992;&#20110;&#20915;&#31574;Transformer&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#20010;&#27493;&#39588;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#35780;&#20272;&#21160;&#20316;&#36136;&#37327;&#65292;&#24182;&#35757;&#32451;ACT&#29983;&#25104;&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#21160;&#20316;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05915</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21183;&#35843;&#33410;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#22686;&#24378;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning. (arXiv:2309.05915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#21160;&#24577;&#35268;&#21010;&#24212;&#29992;&#20110;&#20915;&#31574;Transformer&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#20010;&#27493;&#39588;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#35780;&#20272;&#21160;&#20316;&#36136;&#37327;&#65292;&#24182;&#35757;&#32451;ACT&#29983;&#25104;&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#21160;&#20316;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;Transformer (DT) &#21033;&#29992;&#34920;&#36798;&#20016;&#23500;&#30340;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#26469;&#25191;&#34892;&#21160;&#20316;&#29983;&#25104;&#65292;&#24050;&#25104;&#20026;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;DT &#29983;&#25104;&#30340;&#21160;&#20316;&#26159;&#22522;&#20110;&#26399;&#26395;&#26410;&#26469;&#22238;&#25253;&#30340;&#26465;&#20214;&#65292;&#24050;&#30693;&#20855;&#26377;&#26576;&#20123;&#24369;&#28857;&#65292;&#27604;&#22914;&#26131;&#21463;&#29615;&#22659;&#38543;&#26426;&#24615;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;DT&#30340;&#24369;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;DT&#20013;&#22686;&#21152;&#21160;&#24577;&#35268;&#21010;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26679;&#26412;&#20869;&#20540;&#36845;&#20195;&#26469;&#33719;&#24471;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#36825;&#28041;&#21450;&#21040;MDP&#32467;&#26500;&#19978;&#30340;&#21160;&#24577;&#35268;&#21010;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#32467;&#21512;&#20272;&#35745;&#30340;&#20248;&#21183;&#26469;&#35780;&#20272;&#21160;&#20316;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#20248;&#21183;&#20272;&#35745;&#22120;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20197;&#20272;&#35745;&#30340;&#20248;&#21183;&#20026;&#26465;&#20214;&#29983;&#25104;&#21160;&#20316;&#30340;&#20248;&#21183;&#26465;&#20214;Transformer (ACT)&#12290;&#26368;&#21518;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;ACT&#26681;&#25454;&#25152;&#38656;&#30340;&#20248;&#21183;&#29983;&#25104;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Decision Transformer (DT), which employs expressive sequence modeling techniques to perform action generation, has emerged as a promising approach to offline policy optimization. However, DT generates actions conditioned on a desired future return, which is known to bear some weaknesses such as the susceptibility to environmental stochasticity. To overcome DT's weaknesses, we propose to empower DT with dynamic programming. Our method comprises three steps. First, we employ in-sample value iteration to obtain approximated value functions, which involves dynamic programming over the MDP structure. Second, we evaluate action quality in context with estimated advantages. We introduce two types of advantage estimators, IAE and GAE, which are suitable for different tasks. Third, we train an Advantage-Conditioned Transformer (ACT) to generate actions conditioned on the estimated advantages. Finally, during testing, ACT generates actions conditioned on a desired advantage. Our evaluation resul
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#36890;&#36807;&#31526;&#21495;&#23398;&#20064;&#30340;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.05900</link><description>&lt;p&gt;
&#36890;&#36807;&#31526;&#21495;&#23398;&#20064;&#35780;&#20272;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks Assessment of Salient Object Detection via Symbolic Learning. (arXiv:2309.05900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#36890;&#36807;&#31526;&#21495;&#23398;&#20064;&#30340;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#20027;&#27969;&#25216;&#26415;&#30340;&#26680;&#24515;&#65292;&#20248;&#20110;&#25163;&#24037;&#29305;&#24449;&#35774;&#35745;&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;&#38500;&#20102;&#23545;&#20154;&#24037;&#29305;&#24449;&#25552;&#21462;&#30340;&#23398;&#20064;&#36807;&#31243;&#22806;&#65292;&#23427;&#20855;&#26377;&#20174;&#36755;&#20837;&#21040;&#36755;&#20986;&#30340;&#31471;&#21040;&#31471;&#33539; Paradigm&#65292;&#21487;&#20197;&#23454;&#29616;&#38750;&#24120;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20854;&#23545;&#24694;&#24847;&#21644;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#30340;&#31283;&#20581;&#24615;&#30340;&#23433;&#20840;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#22240;&#20026;&#20854;&#39044;&#27979;&#21487;&#33021;&#20250;&#23436;&#20840;&#25913;&#21464;&#12290;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#24050;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20854;&#21487;&#20449;&#24615;&#20195;&#34920;&#20102;&#19968;&#20010;&#38656;&#35201;&#20998;&#26512;&#21644;&#35299;&#20915;&#40657;&#23458;&#25915;&#20987;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#33041;&#31243;&#24207;&#35774;&#35745;&#26159;&#19968;&#31181;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20256;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#31526;&#21495;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#35774;&#35745;&#21487;&#38752;&#30340;&#35270;&#35273;&#27880;&#24847;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#32463;&#21463;&#20303;&#26368;&#24378;&#28872;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#36825;&#31181;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is at the center of mainstream technology and outperforms classical approaches to handcrafted feature design. Aside from its learning process for artificial feature extraction, it has an end-to-end paradigm from input to output, reaching outstandingly accurate results. However, security concerns about its robustness to malicious and imperceptible perturbations have drawn attention since its prediction can be changed entirely. Salient object detection is a research area where deep convolutional neural networks have proven effective but whose trustworthiness represents a significant issue requiring analysis and solutions to hackers' attacks. Brain programming is a kind of symbolic learning in the vein of good old-fashioned artificial intelligence. This work provides evidence that symbolic learning robustness is crucial in designing reliable visual attention systems since it can withstand even the most intense perturbations. We test this evolutionary computation methodolo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20135;&#21697;&#31867;&#22411;&#19978;&#26657;&#27491;&#22810;&#20010;&#32570;&#38519;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#23618;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;&#32570;&#38519;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#27169;&#22411;&#22312;&#32570;&#38519;&#26657;&#27491;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.05883</link><description>&lt;p&gt;
&#20998;&#23618;&#26465;&#20214;&#21322;&#37197;&#23545;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#65306;&#38024;&#23545;&#36141;&#29289;&#32593;&#31449;&#30340;&#22810;&#20219;&#21153;&#22270;&#20687;&#32570;&#38519;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Conditional Semi-Paired Image-to-Image Translation For Multi-Task Image Defect Correction On Shopping Websites. (arXiv:2309.05883v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20135;&#21697;&#31867;&#22411;&#19978;&#26657;&#27491;&#22810;&#20010;&#32570;&#38519;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#23618;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;&#32570;&#38519;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#27169;&#22411;&#22312;&#32570;&#38519;&#26657;&#27491;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36141;&#29289;&#32593;&#31449;&#19978;&#65292;&#20302;&#36136;&#37327;&#30340;&#20135;&#21697;&#22270;&#29255;&#20250;&#23545;&#23458;&#25143;&#30340;&#20307;&#39564;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#23613;&#31649;&#22312;&#26816;&#27979;&#20855;&#26377;&#19981;&#21516;&#32570;&#38519;&#30340;&#22270;&#20687;&#26041;&#38754;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#33268;&#21147;&#20110;&#22312;&#35268;&#27169;&#19978;&#32416;&#27491;&#36825;&#20123;&#32570;&#38519;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#65292;&#26377;&#25104;&#21315;&#19978;&#19975;&#31181;&#20135;&#21697;&#31867;&#22411;&#65292;&#27599;&#31181;&#31867;&#22411;&#37117;&#26377;&#29305;&#23450;&#30340;&#32570;&#38519;&#65292;&#22240;&#27492;&#26500;&#24314;&#29305;&#23450;&#32570;&#38519;&#30340;&#27169;&#22411;&#26159;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#65288;I2I&#65289;&#36716;&#25442;&#27169;&#22411;&#65292;&#20197;&#32416;&#27491;&#19981;&#21516;&#20135;&#21697;&#31867;&#22411;&#19978;&#30340;&#22810;&#20010;&#32570;&#38519;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#20998;&#23618;&#26041;&#24335;&#23558;&#39640;&#32423;&#32570;&#38519;&#32452;&#21644;&#29305;&#23450;&#32570;&#38519;&#31867;&#22411;&#21512;&#24182;&#65292;&#24341;&#23548;&#32593;&#32476;&#23558;&#28966;&#28857;&#25918;&#22312;&#19982;&#32570;&#38519;&#30456;&#20851;&#30340;&#22270;&#20687;&#21306;&#22495;&#19978;&#12290;&#22312;&#20843;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;I2I&#26041;&#27861;MoNCE&#30456;&#27604;&#65292;&#24179;&#22343;&#23558;Frechet Inception Distance (FID)&#38477;&#20302;&#20102;24.6%&#12290;&#19982;&#20844;&#20849;&#25968;&#25454;&#19981;&#21516;&#65292;&#36141;&#29289;&#32593;&#31449;&#19978;&#30340;&#21478;&#19968;&#20010;&#23454;&#38469;&#25361;&#25112;&#26159;&#26377;&#20123;&#37197;&#23545;&#22270;&#20687;&#36136;&#37327;&#36739;&#20302;&#12290;&#22240;&#27492;&#65292;
&lt;/p&gt;
&lt;p&gt;
On shopping websites, product images of low quality negatively affect customer experience. Although there are plenty of work in detecting images with different defects, few efforts have been dedicated to correct those defects at scale. A major challenge is that there are thousands of product types and each has specific defects, therefore building defect specific models is unscalable. In this paper, we propose a unified Image-to-Image (I2I) translation model to correct multiple defects across different product types. Our model leverages an attention mechanism to hierarchically incorporate high-level defect groups and specific defect types to guide the network to focus on defect-related image regions. Evaluated on eight public datasets, our model reduces the Frechet Inception Distance (FID) by 24.6% in average compared with MoNCE, the state-of-the-art I2I method. Unlike public data, another practical challenge on shopping websites is that some paired images are of low quality. Therefore 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#30340;&#24191;&#20041;&#25915;&#20987;&#12290;&#36890;&#36807;&#24341;&#20837;DodgePersonation&#25915;&#20987;&#65292;&#21019;&#36896;&#20102;&#27169;&#20223;&#32473;&#23450;&#36523;&#20221;&#30340;&#38754;&#37096;&#22270;&#20687;&#65292;&#21516;&#26102;&#36991;&#20813;&#34987;&#35782;&#21035;&#20026;&#19981;&#21516;&#30340;&#36523;&#20221;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#32479;&#19968;&#24635;&#32467;&#12290;&#24182;&#25552;&#20986;&#20102;&#8220;&#19968;&#24352;&#38754;&#23380;&#32479;&#27835;&#25152;&#26377;&#8221;&#30340;&#25915;&#20987;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05879</link><description>&lt;p&gt;
&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#30340;&#24191;&#20041;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Generalized Attacks on Face Verification Systems. (arXiv:2309.05879v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05879
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#30340;&#24191;&#20041;&#25915;&#20987;&#12290;&#36890;&#36807;&#24341;&#20837;DodgePersonation&#25915;&#20987;&#65292;&#21019;&#36896;&#20102;&#27169;&#20223;&#32473;&#23450;&#36523;&#20221;&#30340;&#38754;&#37096;&#22270;&#20687;&#65292;&#21516;&#26102;&#36991;&#20813;&#34987;&#35782;&#21035;&#20026;&#19981;&#21516;&#30340;&#36523;&#20221;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#32479;&#19968;&#24635;&#32467;&#12290;&#24182;&#25552;&#20986;&#20102;&#8220;&#19968;&#24352;&#38754;&#23380;&#32479;&#27835;&#25152;&#26377;&#8221;&#30340;&#25915;&#20987;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20154;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#36793;&#22659;&#25511;&#21046;&#21644;&#26234;&#33021;&#25163;&#26426;&#35299;&#38145;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#24471;&#21040;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#20250;&#25805;&#32437;&#36755;&#20837;&#22270;&#20687;&#20197;&#27450;&#39575;&#36825;&#20123;&#31995;&#32479;&#65292;&#32780;&#36825;&#31181;&#25805;&#32437;&#23545;&#20154;&#31867;&#36890;&#24120;&#26159;&#38590;&#20197;&#23519;&#35273;&#30340;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#23545;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DodgePersonation&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#27169;&#20223;&#19968;&#32452;&#32473;&#23450;&#36523;&#20221;&#30340;&#38754;&#37096;&#22270;&#20687;&#65292;&#21516;&#26102;&#36991;&#20813;&#34987;&#35782;&#21035;&#20026;&#20219;&#20309;&#19968;&#20010;&#19981;&#21516;&#30340;&#36523;&#20221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#20197;&#25552;&#20379;&#23545;&#25239;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#30340;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#21253;&#25324;&#36530;&#36991;&#25915;&#20987;&#12289;&#20882;&#21517;&#25915;&#20987;&#21644;&#20027;&#35201;&#38754;&#37096;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#19968;&#24352;&#38754;&#23380;&#32479;&#27835;&#25152;&#26377;&#8221;&#30340;&#25915;&#20987;&#65292;&#23427;&#20351;&#29992;DodgePersonation&#25915;&#20987;&#24182;&#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#22330;&#26223;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face verification (FV) using deep neural network models has made tremendous progress in recent years, surpassing human accuracy and seeing deployment in various applications such as border control and smartphone unlocking. However, FV systems are vulnerable to Adversarial Attacks, which manipulate input images to deceive these systems in ways usually unnoticeable to humans. This paper provides an in-depth study of attacks on FV systems. We introduce the DodgePersonation Attack that formulates the creation of face images that impersonate a set of given identities while avoiding being identified as any of the identities in a separate, disjoint set. A taxonomy is proposed to provide a unified view of different types of Adversarial Attacks against FV systems, including Dodging Attacks, Impersonation Attacks, and Master Face Attacks. Finally, we propose the ''One Face to Rule Them All'' Attack which implements the DodgePersonation Attack with state-of-the-art performance on a well-known sce
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21453;&#24212;&#22352;&#26631;&#27969;&#65292;&#29992;&#20110;&#21457;&#29616;&#20998;&#23376;&#31995;&#32479;&#20302;&#32500;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#36830;&#32493;&#26102;&#38388;&#21644;&#31354;&#38388;&#20013;&#30340;&#21487;&#35757;&#32451;&#21644;&#21487;&#22788;&#29702;&#30340;&#26041;&#24335;&#36827;&#34892;&#27169;&#22411;&#31616;&#21270;&#65292;&#20135;&#29983;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.05878</link><description>&lt;p&gt;
&#21453;&#24212;&#22352;&#26631;&#27969;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#22411;&#31616;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Reaction coordinate flows for model reduction of molecular kinetics. (arXiv:2309.05878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21453;&#24212;&#22352;&#26631;&#27969;&#65292;&#29992;&#20110;&#21457;&#29616;&#20998;&#23376;&#31995;&#32479;&#20302;&#32500;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#36830;&#32493;&#26102;&#38388;&#21644;&#31354;&#38388;&#20013;&#30340;&#21487;&#35757;&#32451;&#21644;&#21487;&#22788;&#29702;&#30340;&#26041;&#24335;&#36827;&#34892;&#27169;&#22411;&#31616;&#21270;&#65292;&#20135;&#29983;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21453;&#24212;&#22352;&#26631;&#65288;RC&#65289;&#27969;&#65292;&#29992;&#20110;&#21457;&#29616;&#20998;&#23376;&#31995;&#32479;&#20302;&#32500;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;RC&#27969;&#21033;&#29992;&#24402;&#19968;&#21270;&#27969;&#35774;&#35745;&#22352;&#26631;&#21464;&#25442;&#65292;&#24182;&#20351;&#29992;&#24067;&#26391;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#36817;&#20284;RC&#30340;&#21160;&#21147;&#23398;&#65292;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#21487;&#20197;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#20272;&#35745;&#12290;&#19982;&#29616;&#26377;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;&#30001;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#21487;&#36870;&#24615;&#65292;RC&#27969;&#22312;&#36830;&#32493;&#26102;&#38388;&#21644;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#21487;&#35757;&#32451;&#21644;&#21487;&#22788;&#29702;&#30340;&#31616;&#21270;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#30740;&#31350;&#30340;&#22522;&#20110;&#24067;&#26391;&#21160;&#21147;&#23398;&#30340;&#31616;&#21270;&#21160;&#21147;&#23398;&#27169;&#22411;&#22312;&#20998;&#23376;&#31995;&#32479;&#30340;&#30456;&#31354;&#38388;&#20013;&#20135;&#29983;&#20102;&#26131;&#20110;&#36776;&#21035;&#30340;&#20122;&#31283;&#24577;&#34920;&#31034;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22914;&#20309;&#26377;&#25928;&#22320;&#21457;&#29616;&#32473;&#23450;&#30340;&#23436;&#25972;&#29366;&#24577;&#21160;&#21147;&#23398;&#30340;&#21487;&#35299;&#37322;&#21644;&#20934;&#30830;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a flow based machine learning approach, called reaction coordinate (RC) flow, for discovery of low-dimensional kinetic models of molecular systems. The RC flow utilizes a normalizing flow to design the coordinate transformation and a Brownian dynamics model to approximate the kinetics of RC, where all model parameters can be estimated in a data-driven manner. In contrast to existing model reduction methods for molecular kinetics, RC flow offers a trainable and tractable model of reduced kinetics in continuous time and space due to the invertibility of the normalizing flow. Furthermore, the Brownian dynamics-based reduced kinetic model investigated in this work yields a readily discernible representation of metastable states within the phase space of the molecular system. Numerical experiments demonstrate how effectively the proposed method discovers interpretable and accurate low-dimensional representations of given full-state kinetics from simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36339;&#36291;&#36317;&#31163;&#30340;&#21147;&#23548;&#21521;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#33258;&#23450;&#20041;&#30340;&#24341;&#21147;&#21644;&#26021;&#21147;&#20197;&#21450;&#20351;&#29992;&#29275;&#39039;&#31532;&#20108;&#23450;&#24459;&#65292;&#23884;&#20837;&#33410;&#28857;&#20197;&#20445;&#25345;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#20998;&#26512;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05865</link><description>&lt;p&gt;
&#22522;&#20110;&#36339;&#36291;&#36317;&#31163;&#30340;&#21147;&#23548;&#21521;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Force-directed graph embedding with hops distance. (arXiv:2309.05865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36339;&#36291;&#36317;&#31163;&#30340;&#21147;&#23548;&#21521;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#33258;&#23450;&#20041;&#30340;&#24341;&#21147;&#21644;&#26021;&#21147;&#20197;&#21450;&#20351;&#29992;&#29275;&#39039;&#31532;&#20108;&#23450;&#24459;&#65292;&#23884;&#20837;&#33410;&#28857;&#20197;&#20445;&#25345;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#20998;&#26512;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23884;&#20837;&#24050;&#25104;&#20026;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#19968;&#31181;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#22270;&#20013;&#33410;&#28857;&#34920;&#31034;&#20026;&#20302;&#32500;&#31354;&#38388;&#20013;&#30340;&#21521;&#37327;&#65292;&#22270;&#23884;&#20837;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#22270;&#22788;&#29702;&#21644;&#20998;&#26512;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#21487;&#35270;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21147;&#23548;&#21521;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#21033;&#29992;&#31283;&#23450;&#21152;&#36895;&#24230;&#21160;&#21147;&#23398;&#20844;&#24335;&#23558;&#33410;&#28857;&#23884;&#20837;&#20197;&#20445;&#25345;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27169;&#25311;&#20102;&#38024;&#23545;&#27599;&#23545;&#33410;&#28857;&#30340;&#33258;&#23450;&#20041;&#24341;&#21147;&#21644;&#26021;&#21147;&#65292;&#36825;&#20123;&#21147;&#26159;&#26681;&#25454;&#33410;&#28857;&#20043;&#38388;&#30340;&#36339;&#36291;&#36317;&#31163;&#35745;&#31639;&#30340;&#12290;&#28982;&#21518;&#20351;&#29992;&#29275;&#39039;&#31532;&#20108;&#23450;&#24459;&#26469;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#21152;&#36895;&#24230;&#12290;&#35813;&#26041;&#27861;&#30452;&#35266;&#12289;&#21487;&#24182;&#34892;&#21270;&#21644;&#39640;&#24230;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22270;&#20998;&#26512;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#23884;&#20837;&#25216;&#26415;&#30456;&#27604;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph embedding has become an increasingly important technique for analyzing graph-structured data. By representing nodes in a graph as vectors in a low-dimensional space, graph embedding enables efficient graph processing and analysis tasks like node classification, link prediction, and visualization. In this paper, we propose a novel force-directed graph embedding method that utilizes the steady acceleration kinetic formula to embed nodes in a way that preserves graph topology and structural features. Our method simulates a set of customized attractive and repulsive forces between all node pairs with respect to their hop distance. These forces are then used in Newton's second law to obtain the acceleration of each node. The method is intuitive, parallelizable, and highly scalable. We evaluate our method on several graph analysis tasks and show that it achieves competitive performance compared to state-of-the-art unsupervised embedding techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32908;&#32905;&#39592;&#39612;&#27169;&#25311;&#21644;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#21644;&#32908;&#32905;&#21147;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.05863</link><description>&lt;p&gt;
&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22806;&#37096;&#27169;&#25311;&#20154;&#31867;&#36816;&#21160;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The bionic neural network for external simulation of human locomotor system. (arXiv:2309.05863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32908;&#32905;&#39592;&#39612;&#27169;&#25311;&#21644;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#21644;&#32908;&#32905;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#32908;&#32905;&#39592;&#39612;&#27169;&#25311;&#25216;&#26415;&#20272;&#35745;&#30340;&#32908;&#32905;&#21147;&#37327;&#21644;&#20851;&#33410;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#25551;&#36848;&#36816;&#21160;&#36136;&#37327;&#30340;&#26377;&#29992;&#25351;&#26631;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#35745;&#31639;&#26426;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#39537;&#21160;&#32908;&#32905;&#12289;&#32908;&#32905;&#21160;&#21147;&#23398;&#12289;&#36523;&#20307;&#21644;&#20851;&#33410;&#21160;&#21147;&#23398;&#20197;&#21450;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#23384;&#22312;&#35745;&#31639;&#26102;&#38388;&#38271;&#21644;&#32908;&#32905;&#25307;&#21215;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#24182;&#19981;&#23481;&#26131;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32908;&#32905;&#39592;&#39612;&#27169;&#25311;&#30340;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#21644;&#32908;&#32905;&#21147;&#37327;&#12290;&#23558;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#20316;&#20026;&#19968;&#20010;&#24102;&#26377;&#29983;&#29702;&#21442;&#25968;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35782;&#21035;&#32908;&#32905;&#28608;&#27963;&#21160;&#21147;&#23398;&#21644;&#32908;&#32905;&#25910;&#32553;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Muscle forces and joint kinematics estimated with musculoskeletal (MSK) modeling techniques offer useful metrics describing movement quality. Model-based computational MSK models can interpret the dynamic interaction between the neural drive to muscles, muscle dynamics, body and joint kinematics, and kinetics. Still, such a set of solutions suffers from high computational time and muscle recruitment problems, especially in complex modeling. In recent years, data-driven methods have emerged as a promising alternative due to the benefits of flexibility and adaptability. However, a large amount of labeled training data is not easy to be acquired. This paper proposes a physics-informed deep learning method based on MSK modeling to predict joint motion and muscle forces. The MSK model is embedded into the neural network as an ordinary differential equation (ODE) loss function with physiological parameters of muscle activation dynamics and muscle contraction dynamics to be identified. These 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;mesa-optimization&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.05858</link><description>&lt;p&gt;
&#25581;&#31034;Transformer&#20013;&#30340;mesa-optimization&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Uncovering mesa-optimization algorithms in Transformers. (arXiv:2309.05858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;mesa-optimization&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#20027;&#23548;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#21331;&#36234;&#24615;&#33021;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20551;&#35774;Transformer&#30340;&#24378;&#22823;&#24615;&#33021;&#28304;&#20110;&#20854;&#26550;&#26500;&#20013;&#23545;mesa-optimization&#30340;&#20559;&#22909;&#65292;&#21363;&#19968;&#20010;&#23398;&#20064;&#36807;&#31243;&#22312;&#27169;&#22411;&#30340;&#21069;&#21521;&#20256;&#36882;&#20013;&#36816;&#34892;&#65292;&#30001;&#20197;&#19979;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65306;&#65288;i&#65289;&#26500;&#24314;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#65292;&#21644;&#65288;ii&#65289;&#36890;&#36807;&#20248;&#21270;&#25214;&#21040;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#22312;&#31616;&#21333;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;Transformer&#36827;&#34892;&#20102;&#36870;&#21521;&#24037;&#31243;&#65292;&#25581;&#31034;&#20102;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24213;&#23618;mesa-optimization&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#21069;&#21521;&#20256;&#36882;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#31435;&#21363;&#34987;&#37325;&#26032;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#36825;&#34920;&#26126;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#21457;&#29616;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.05855</link><description>&lt;p&gt;
&#38543;&#26426;&#28388;&#27874;&#22120;&#32452;&#30340;&#33021;&#37327;&#20445;&#25345;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Energy Preservation and Stability of Random Filterbanks. (arXiv:2309.05855v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#21457;&#29616;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#24418;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#20026;&#20160;&#20040;&#22914;&#27492;&#22256;&#38590;&#65311;&#23613;&#31649;&#26377;&#22810;&#27425;&#23581;&#35797;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(convnets)&#36827;&#34892;&#28388;&#27874;&#22120;&#35774;&#35745;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#36229;&#36234;&#25163;&#24037;&#21019;&#24314;&#30340;&#22522;&#32447;&#12290;&#36825;&#26356;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#36825;&#20123;&#22522;&#32447;&#26159;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65306;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#20256;&#36882;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#22823;&#24863;&#21463;&#37326;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#36825;&#22312;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#20013;&#26159;&#20856;&#22411;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38543;&#26426;&#28388;&#27874;&#22120;&#32452;&#30340;&#26399;&#26395;&#33021;&#37327;&#20445;&#25345;&#23545;&#20110;&#25968;&#20540;&#31283;&#23450;&#24615;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
What makes waveform-based deep learning so hard? Despite numerous attempts at training convolutional neural networks (convnets) for filterbank design, they often fail to outperform hand-crafted baselines. This is all the more surprising because these baselines are linear time-invariant systems: as such, their transfer functions could be accurately represented by a convnet with a large receptive field. In this article, we elaborate on the statistical properties of simple convnets from the mathematical perspective of random convolutional operators. We find that FIR filterbanks with random Gaussian weights are ill-conditioned for large filters and locally periodic input signals, which both are typical in audio signal processing applications. Furthermore, we observe that expected energy preservation of a random filterbank is not sufficient for numerical stability and derive theoretical bounds for its expected frame bounds.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#26679;&#26412;&#31354;&#38388;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#29983;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30446;&#26631;&#20998;&#23376;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#36890;&#36807;&#22312;&#21270;&#23398;&#31354;&#38388;&#20195;&#29702;&#20869;&#37096;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#29983;&#25104;&#20998;&#23376;&#19982;&#34507;&#30333;&#36136;&#38774;&#26631;&#20043;&#38388;&#21560;&#24341;&#21147;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05853</link><description>&lt;p&gt;
ChemSpaceAL:&#19968;&#20010;&#24212;&#29992;&#20110;&#29305;&#23450;&#34507;&#30333;&#36136;&#20998;&#23376;&#29983;&#25104;&#30340;&#39640;&#25928;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation. (arXiv:2309.05853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05853
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#26679;&#26412;&#31354;&#38388;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#29983;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30446;&#26631;&#20998;&#23376;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#36890;&#36807;&#22312;&#21270;&#23398;&#31354;&#38388;&#20195;&#29702;&#20869;&#37096;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#29983;&#25104;&#20998;&#23376;&#19982;&#34507;&#30333;&#36136;&#38774;&#26631;&#20043;&#38388;&#21560;&#24341;&#21147;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#19981;&#21487;&#24605;&#35758;&#33021;&#21147;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#23427;&#20204;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#33021;&#22815;&#22686;&#24378;&#36825;&#20123;&#24378;&#22823;&#24037;&#20855;&#30340;&#33021;&#21147;&#21644;&#36866;&#29992;&#24615;&#30340;&#26041;&#27861;&#35770;&#20855;&#26377;&#24040;&#22823;&#30340;&#20852;&#36259;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#26500;&#24314;&#30340;&#26679;&#26412;&#31354;&#38388;&#34920;&#31034;&#20013;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#65292;&#21487;&#20197;&#20351;&#29983;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30446;&#26631;&#20998;&#23376;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#33021;&#22815;&#30456;&#23545;&#20110;&#22522;&#20110;&#21560;&#24341;&#21147;&#30456;&#20114;&#20316;&#29992;&#35780;&#20998;&#20989;&#25968;&#36827;&#34892;&#24494;&#35843;&#30340;GPT&#22522;&#30784;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#21270;&#23398;&#31354;&#38388;&#20195;&#29702;&#20869;&#37096;&#31574;&#30053;&#24615;&#22320;&#25805;&#20316;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#29983;&#25104;&#30340;&#20998;&#23376;&#19982;&#34507;&#30333;&#36136;&#38774;&#26631;&#20043;&#38388;&#30340;&#21560;&#24341;&#21147;&#30456;&#20114;&#20316;&#29992;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#29992;&#20110;&#24494;&#35843;&#30340;&#25152;&#26377;&#25968;&#25454;&#28857;&#36827;&#34892;&#21333;&#29420;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The incredible capabilities of generative artificial intelligence models have inevitably led to their application in the domain of drug discovery. It is therefore of tremendous interest to develop methodologies that enhance the abilities and applicability of these powerful tools. In this work, we present a novel and efficient semi-supervised active learning methodology that allows for the fine-tuning of a generative model with respect to an objective function by strategically operating within a constructed representation of the sample space. In the context of targeted molecular generation, we demonstrate the ability to fine-tune a GPT-based molecular generator with respect to an attractive interaction-based scoring function by strategically operating within a chemical space proxy, thereby maximizing attractive interactions between the generated molecules and a protein target. Importantly, our approach does not require the individual evaluation of all data points that are used for fine-
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24322;&#24120;&#27963;&#21160;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;F1&#20998;&#25968;&#20026;0.839&#12290;</title><link>http://arxiv.org/abs/2309.05845</link><description>&lt;p&gt;
&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#25968;&#25454;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#24322;&#24120;&#27963;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data. (arXiv:2309.05845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05845
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24322;&#24120;&#27963;&#21160;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;F1&#20998;&#25968;&#20026;0.839&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#25968;&#25454;&#36890;&#36807;&#22810;&#20010;&#20256;&#24863;&#22120;&#25910;&#38598;&#65292;&#20026;&#26234;&#33021;&#21307;&#30103;&#22330;&#26223;&#20013;&#20934;&#30830;&#30340;&#24322;&#24120;&#27963;&#21160;&#26816;&#27979;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24322;&#24120;&#34920;&#29616;&#20986;&#22810;&#26679;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#22312;MTS&#25968;&#25454;&#20013;&#21464;&#24471;&#19981;&#23481;&#26131;&#23519;&#35273;&#12290;&#22240;&#27492;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;Rs-AD&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24322;&#24120;&#27963;&#21160;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#27493;&#24577;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;F1&#20998;&#25968;&#20026;0.839&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series (MTS) data collected from multiple sensors provide the potential for accurate abnormal activity detection in smart healthcare scenarios. However, anomalies exhibit diverse patterns and become unnoticeable in MTS data. Consequently, achieving accurate anomaly detection is challenging since we have to capture both temporal dependencies of time series and inter-relationships among variables. To address this problem, we propose a Residual-based Anomaly Detection approach, Rs-AD, for effective representation learning and abnormal activity detection. We evaluate our scheme on a real-world gait dataset and the experimental results demonstrate an F1 score of 0.839.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;SimCLR&#21644;Slowfast NFNet&#32972;&#39592;&#65292;&#20248;&#21270;&#20581;&#24247;&#22768;&#23398;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#38024;&#23545;&#35813;&#24212;&#29992;&#35782;&#21035;&#20102;&#26377;&#25928;&#30340;&#38899;&#39057;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#24403;&#36825;&#20123;&#22686;&#24378;&#26041;&#27861;&#30456;&#32467;&#21512;&#26102;&#33021;&#22815;&#20135;&#29983;&#36229;&#36807;&#21333;&#29420;&#24212;&#29992;&#27599;&#20010;&#22686;&#24378;&#30340; synergistic effects&#12290;</title><link>http://arxiv.org/abs/2309.05843</link><description>&lt;p&gt;
&#20248;&#21270;&#20581;&#24247;&#30456;&#20851;&#22768;&#38899;&#23545;&#27604;&#23398;&#20064;&#30340;&#38899;&#39057;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Optimizing Audio Augmentations for Contrastive Learning of Health-Related Acoustic Signals. (arXiv:2309.05843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;SimCLR&#21644;Slowfast NFNet&#32972;&#39592;&#65292;&#20248;&#21270;&#20581;&#24247;&#22768;&#23398;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#38024;&#23545;&#35813;&#24212;&#29992;&#35782;&#21035;&#20102;&#26377;&#25928;&#30340;&#38899;&#39057;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#24403;&#36825;&#20123;&#22686;&#24378;&#26041;&#27861;&#30456;&#32467;&#21512;&#26102;&#33021;&#22815;&#20135;&#29983;&#36229;&#36807;&#21333;&#29420;&#24212;&#29992;&#27599;&#20010;&#22686;&#24378;&#30340; synergistic effects&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#30456;&#20851;&#22768;&#38899;&#65288;&#22914;&#21683;&#22013;&#21644;&#21628;&#21560;&#22768;&#65289;&#23545;&#20110;&#21307;&#23398;&#35786;&#26029;&#21644;&#25345;&#32493;&#20581;&#24247;&#30417;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20581;&#24247;&#22768;&#23398;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;SimCLR&#21644;Slowfast NFNet&#32972;&#39592;&#36827;&#34892;&#20581;&#24247;&#22768;&#23398;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;&#20248;&#21270;Slowfast NFNet&#22312;&#36825;&#20010;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#22312;&#20110;&#35782;&#21035;&#26377;&#25928;&#30340;&#38899;&#39057;&#22686;&#24378;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#38899;&#39057;&#22686;&#24378;&#31574;&#30053;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#36866;&#24403;&#30340;&#22686;&#24378;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;Slowfast NFNet&#38899;&#39057;&#32534;&#30721;&#22120;&#22312;&#21508;&#31181;&#20581;&#24247;&#22768;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#22686;&#24378;&#26041;&#27861;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#20135;&#29983;&#36229;&#36807;&#21333;&#29420;&#24212;&#29992;&#27599;&#20010;&#22686;&#24378;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health-related acoustic signals, such as cough and breathing sounds, are relevant for medical diagnosis and continuous health monitoring. Most existing machine learning approaches for health acoustics are trained and evaluated on specific tasks, limiting their generalizability across various healthcare applications. In this paper, we leverage a self-supervised learning framework, SimCLR with a Slowfast NFNet backbone, for contrastive learning of health acoustics. A crucial aspect of optimizing Slowfast NFNet for this application lies in identifying effective audio augmentations. We conduct an in-depth analysis of various audio augmentation strategies and demonstrate that an appropriate augmentation strategy enhances the performance of the Slowfast NFNet audio encoder across a diverse set of health acoustic tasks. Our findings reveal that when augmentations are combined, they can produce synergistic effects that exceed the benefits seen when each is applied individually.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25552;&#20379;&#20102;&#23545;&#23433;&#20840;&#36807;&#28388;&#22120;&#26041;&#27861;&#30340;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25216;&#26415;&#26694;&#26550;&#26469;&#29702;&#35299;&#12289;&#27604;&#36739;&#21644;&#32467;&#21512;&#29616;&#26377;&#30340;&#25216;&#26415;&#65292;&#20026;&#25104;&#21151;&#37096;&#32626;&#19979;&#19968;&#20195;&#33258;&#20027;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.05837</link><description>&lt;p&gt;
&#23433;&#20840;&#36807;&#28388;&#22120;: &#33258;&#20027;&#31995;&#32479;&#20013;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#30340;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems. (arXiv:2309.05837v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25552;&#20379;&#20102;&#23545;&#23433;&#20840;&#36807;&#28388;&#22120;&#26041;&#27861;&#30340;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25216;&#26415;&#26694;&#26550;&#26469;&#29702;&#35299;&#12289;&#27604;&#36739;&#21644;&#32467;&#21512;&#29616;&#26377;&#30340;&#25216;&#26415;&#65292;&#20026;&#25104;&#21151;&#37096;&#32626;&#19979;&#19968;&#20195;&#33258;&#20027;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#20154;&#33258;&#20027;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#20276;&#38543;&#30528;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#19981;&#26029;&#25193;&#23637;&#12290;&#28982;&#32780;&#65292;&#26032;&#37096;&#32626;&#39046;&#22495;&#30340;&#20986;&#29616;&#32473;&#20445;&#35777;&#36825;&#20123;&#31995;&#32479;&#23433;&#20840;&#36816;&#34892;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#65292;&#36825;&#20173;&#28982;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23433;&#20840;&#25511;&#21046;&#26041;&#27861;&#22312;&#27867;&#21270;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26032;&#20852;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21017;&#24448;&#24448;&#32570;&#20047;&#26126;&#30830;&#30340;&#20445;&#35777;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;&#25104;&#21151;&#37096;&#32626;&#19979;&#19968;&#20195;&#33258;&#20027;&#26426;&#22120;&#20154;&#23558;&#38656;&#35201;&#25972;&#21512;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23433;&#20840;&#36807;&#28388;&#22120;&#26041;&#27861;&#30340;&#32508;&#36848;&#65292;&#31361;&#20986;&#20102;&#29616;&#26377;&#25216;&#26415;&#20043;&#38388;&#30340;&#37325;&#35201;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25216;&#26415;&#26694;&#26550;&#26469;&#29702;&#35299;&#12289;&#27604;&#36739;&#21644;&#32467;&#21512;&#23427;&#20204;&#12290;&#36825;&#20010;&#26032;&#30340;&#32479;&#19968;&#35270;&#22270;&#25581;&#31034;&#20102;&#19968;&#31995;&#21015;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#31867;&#20043;&#38388;&#30340;&#20849;&#20139;&#27169;&#22359;&#32467;&#26500;&#65292;&#24182;&#33258;&#28982;&#22320;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen significant progress in the realm of robot autonomy, accompanied by the expanding reach of robotic technologies. However, the emergence of new deployment domains brings unprecedented challenges in ensuring safe operation of these systems, which remains as crucial as ever. While traditional model-based safe control methods struggle with generalizability and scalability, emerging data-driven approaches tend to lack well-understood guarantees, which can result in unpredictable catastrophic failures. Successful deployment of the next generation of autonomous robots will require integrating the strengths of both paradigms. This article provides a review of safety filter approaches, highlighting important connections between existing techniques and proposing a unified technical framework to understand, compare, and combine them. The new unified view exposes a shared modular structure across a range of seemingly disparate safety filter classes and naturally suggests dir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05833</link><description>&lt;p&gt;
PACE: &#20351;&#29992;GPT-4&#36827;&#34892;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#20013;&#30340;&#25552;&#31034;&#21644;&#22686;&#21152;&#20197;&#36827;&#34892;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;IT&#34892;&#19994;&#21521;&#22522;&#20110;&#20113;&#30340;&#24179;&#21488;&#30340;&#36716;&#21464;&#24378;&#35843;&#20102;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#32500;&#25252;&#23458;&#25143;&#20449;&#20219;&#12290;&#26680;&#24515;&#38382;&#39064;&#26159;&#26377;&#25928;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#30001;&#20110;&#24403;&#20195;&#20113;&#22522;&#30784;&#35774;&#26045;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#35782;&#21035;&#30340;&#22522;&#20110;AI&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20173;&#21463;&#21040;&#20854;&#36755;&#20986;&#36136;&#37327;&#19981;&#19968;&#33268;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#26681;&#25454;&#21382;&#21490;&#20107;&#20214;&#25968;&#25454;&#35780;&#20272;&#33258;&#36523;&#30340;&#32622;&#20449;&#24230;&#65292;&#32771;&#34385;&#20854;&#23545;&#35777;&#25454;&#30340;&#35780;&#20272;&#24378;&#24230;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#23457;&#26680;&#30001;&#39044;&#27979;&#22120;&#29983;&#25104;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#20248;&#21270;&#27493;&#39588;&#23558;&#36825;&#20123;&#35780;&#20272;&#32467;&#21512;&#36215;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#26080;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20960;&#20309;&#20316;&#20026;&#20849;&#20139;&#34920;&#31034;&#65292;&#23558;&#35270;&#35273;&#19982;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#20174;RGBD&#35270;&#39057;&#20013;&#23398;&#20064;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21018;&#24615;&#21644;&#20984;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#65292;&#24182;&#25913;&#36827;&#20102;&#36319;&#36394;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.05832</link><description>&lt;p&gt;
&#23454;&#20363;&#26080;&#20851;&#30340;&#20960;&#20309;&#19982;&#25509;&#35302;&#21160;&#21147;&#23398;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instance-Agnostic Geometry and Contact Dynamics Learning. (arXiv:2309.05832v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#26080;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20960;&#20309;&#20316;&#20026;&#20849;&#20139;&#34920;&#31034;&#65292;&#23558;&#35270;&#35273;&#19982;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#20174;RGBD&#35270;&#39057;&#20013;&#23398;&#20064;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21018;&#24615;&#21644;&#20984;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#65292;&#24182;&#25913;&#36827;&#20102;&#36319;&#36394;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#26080;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20960;&#20309;&#20316;&#20026;&#20849;&#20139;&#34920;&#31034;&#65292;&#23558;&#35270;&#35273;&#19982;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#23398;&#20064;&#24418;&#29366;&#12289;&#23039;&#24577;&#36712;&#36857;&#21644;&#29289;&#29702;&#24615;&#36136;&#12290;&#19982;&#35768;&#22810;&#25509;&#35302;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20174;RGBD&#35270;&#39057;&#20013;&#23398;&#20064;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#23646;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#31867;&#21035;&#32423;&#21035;&#25110;&#23454;&#20363;&#32423;&#21035;&#30340;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#23558;&#35270;&#35273;&#31995;&#32479;BundleSDF&#19982;&#21160;&#21147;&#23398;&#31995;&#32479;ContactNets&#38598;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24490;&#29615;&#35757;&#32451;&#31649;&#36947;&#65292;&#20351;&#29992;&#21160;&#21147;&#23398;&#27169;&#22359;&#30340;&#36755;&#20986;&#36890;&#36807;&#36879;&#35270;&#37325;&#25237;&#24433;&#26469;&#25913;&#36827;&#35270;&#35273;&#27169;&#22359;&#30340;&#23039;&#24577;&#21644;&#20960;&#20309;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#23398;&#20064;&#21018;&#24615;&#21644;&#20984;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#65292;&#24182;&#25913;&#36827;&#20102;&#24403;&#21069;&#30340;&#36319;&#36394;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an instance-agnostic learning framework that fuses vision with dynamics to simultaneously learn shape, pose trajectories and physical properties via the use of geometry as a shared representation. Unlike many contact learning approaches that assume motion capture input and a known shape prior for the collision model, our proposed framework learns an object's geometric and dynamic properties from RGBD video, without requiring either category-level or instance-level shape priors. We integrate a vision system, BundleSDF, with a dynamics system, ContactNets and propose a cyclic training pipeline to use the output from the dynamics module to refine the poses and the geometry from the vision module, using perspective reprojection. Experiments demonstrate our framework's ability to learn the geometry and dynamics of rigid and convex objects and improve upon the current tracking framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#23454;&#39564;&#23460;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#26102;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.05831</link><description>&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#35299;&#20915;&#24037;&#20316;&#22330;&#25152;&#23433;&#20840;&#38382;&#39064;&#20013;&#65292;&#22522;&#20110;&#23454;&#39564;&#23460;&#25215;&#36733;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Studying Accuracy of Machine Learning Models Trained on Lab Lifting Data in Solving Real-World Problems Using Wearable Sensors for Workplace Safety. (arXiv:2309.05831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#23454;&#39564;&#23460;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#26102;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23454;&#39564;&#23460;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#23454;&#39564;&#23460;&#35757;&#32451;&#30340;&#20030;&#37325;&#35782;&#21035;&#27169;&#22411;&#31227;&#26893;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#12290;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#27604;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#35201;&#20302;&#24471;&#22810;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22833;&#36133;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Porting ML models trained on lab data to real-world situations has long been a challenge. This paper discusses porting a lab-trained lifting identification model to the real-world. With performance much lower than on training data, we explored causes of the failure and proposed four potential solutions to increase model performance
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#26102;&#38388;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22320;&#29702;&#32593;&#26684;&#20043;&#38388;&#30340;&#21160;&#24577;&#31354;&#38388;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.05828</link><description>&lt;p&gt;
&#25506;&#32034;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Geometric Deep Learning For Precipitation Nowcasting. (arXiv:2309.05828v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#26102;&#38388;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22320;&#29702;&#32593;&#26684;&#20043;&#38388;&#30340;&#21160;&#24577;&#31354;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#27700;&#39044;&#27979;&#65288;&#20960;&#23567;&#26102;&#20869;&#65289;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#38656;&#35201;&#20934;&#30830;&#25429;&#25417;&#22797;&#26434;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#20110;&#21367;&#31215;&#26680;&#19982;&#32593;&#26684;&#25968;&#25454;&#36827;&#34892;&#21367;&#31215;&#65292;&#24182;&#19988;&#25552;&#21462;&#30340;&#29305;&#24449;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#24863;&#30693;&#22495;&#65292;&#36890;&#24120;&#34920;&#29616;&#20026;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#27604;&#36807;&#24230;&#24179;&#28369;&#30340;&#36755;&#20986;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#32570;&#20047;&#23545;&#32593;&#26684;&#20043;&#38388;&#22797;&#26434;&#31354;&#38388;&#20851;&#31995;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26088;&#22312;&#23558;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25512;&#24191;&#21040;&#38750;&#27431;&#20960;&#37324;&#24503;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#23450;&#20041;&#33410;&#28857;&#21644;&#36793;&#32536;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22320;&#29702;&#32593;&#26684;&#20043;&#38388;&#30340;&#21160;&#24577;&#31354;&#38388;&#20851;&#31995;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24212;&#29992;&#20110;&#38477;&#27700;&#39044;&#27979;&#12290;&#33258;&#21160;&#23398;&#20064;&#25551;&#36848;&#32593;&#26684;&#21333;&#20803;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#20540;&#19982;&#30495;&#23454;&#20687;&#32032;&#20540;&#20043;&#38388;&#30340;L1&#25439;&#22833;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precipitation nowcasting (up to a few hours) remains a challenge due to the highly complex local interactions that need to be captured accurately. Convolutional Neural Networks rely on convolutional kernels convolving with grid data and the extracted features are trapped by limited receptive field, typically expressed in excessively smooth output compared to ground truth. Thus they lack the capacity to model complex spatial relationships among the grids. Geometric deep learning aims to generalize neural network models to non-Euclidean domains. Such models are more flexible in defining nodes and edges and can effectively capture dynamic spatial relationship among geographical grids. Motivated by this, we explore a geometric deep learning-based temporal Graph Convolutional Network (GCN) for precipitation nowcasting. The adjacency matrix that simulates the interactions among grid cells is learned automatically by minimizing the L1 loss between prediction and ground truth pixel value durin
&lt;/p&gt;</description></item><item><title>KD-FixMatch&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;FixMatch&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#30693;&#35782;&#33976;&#39311;&#65292;&#36890;&#36807;&#39034;&#24207;&#21644;&#24182;&#34892;&#35757;&#32451;SNNs&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#24182;&#38477;&#20302;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2309.05826</link><description>&lt;p&gt;
KD-FixMatch: &#30693;&#35782;&#33976;&#39311;&#30340;&#23402;&#29983;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
KD-FixMatch: Knowledge Distillation Siamese Neural Networks. (arXiv:2309.05826v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05826
&lt;/p&gt;
&lt;p&gt;
KD-FixMatch&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;FixMatch&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#30693;&#35782;&#33976;&#39311;&#65292;&#36890;&#36807;&#39034;&#24207;&#21644;&#24182;&#34892;&#35757;&#32451;SNNs&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#24182;&#38477;&#20302;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#35299;&#20915;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#26631;&#27880;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#26631;&#27880;&#30340;&#36807;&#31243;&#32791;&#26102;&#19988;&#19981;&#21487;&#25193;&#23637;&#65292;&#23548;&#33268;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#12290;SSL&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#39069;&#22806;&#30340;&#26410;&#26631;&#27880;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;FixMatch&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;SSL&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23402;&#29983;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#21516;&#26102;&#35757;&#32451;&#30456;&#21516;&#26435;&#37325;&#20849;&#20139;&#30340;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#22914;&#26524;&#20266;&#26631;&#31614;&#23384;&#22312;&#36739;&#22823;&#22122;&#22768;&#65292;&#35813;&#31639;&#27861;&#23481;&#26131;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KD-FixMatch&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;SSL&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#26469;&#35299;&#20915;FixMatch&#30340;&#23616;&#38480;&#24615;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#39034;&#24207;&#21644;&#24182;&#34892;&#35757;&#32451;SNNs&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#24182;&#38477;&#20302;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) has become a crucial approach in deep learning as a way to address the challenge of limited labeled data. The success of deep neural networks heavily relies on the availability of large-scale high-quality labeled data. However, the process of data labeling is time-consuming and unscalable, leading to shortages in labeled data. SSL aims to tackle this problem by leveraging additional unlabeled data in the training process. One of the popular SSL algorithms, FixMatch, trains identical weight-sharing teacher and student networks simultaneously using a siamese neural network (SNN). However, it is prone to performance degradation when the pseudo labels are heavily noisy in the early training stage. We present KD-FixMatch, a novel SSL algorithm that addresses the limitations of FixMatch by incorporating knowledge distillation. The algorithm utilizes a combination of sequential and simultaneous training of SNNs to enhance performance and reduce performance degra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22522;&#20110;&#38598;&#25104;&#30340;&#32452;&#20214;&#27169;&#22411;DEECo&#30340;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#20197;&#24314;&#31435;&#21644;&#37325;&#26032;&#37197;&#32622;&#33258;&#20027;&#32452;&#20214;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#29616;&#20195;&#26234;&#33021;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#65292;&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20248;&#21270;&#34892;&#20026;&#65292;&#24182;&#22312;&#36816;&#34892;&#26102;&#22788;&#29702;&#29615;&#22659;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.05823</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#27169;&#22411;&#30340;&#25277;&#35937;&#21270;&#26041;&#27861;&#29992;&#20110;&#29616;&#20195;&#33258;&#25105;&#20248;&#21270;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Ensemble-based modeling abstractions for modern self-optimizing systems. (arXiv:2309.05823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22522;&#20110;&#38598;&#25104;&#30340;&#32452;&#20214;&#27169;&#22411;DEECo&#30340;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#20197;&#24314;&#31435;&#21644;&#37325;&#26032;&#37197;&#32622;&#33258;&#20027;&#32452;&#20214;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#29616;&#20195;&#26234;&#33021;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#65292;&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20248;&#21270;&#34892;&#20026;&#65292;&#24182;&#22312;&#36816;&#34892;&#26102;&#22788;&#29702;&#29615;&#22659;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22522;&#20110;&#38598;&#25104;&#30340;&#32452;&#20214;&#27169;&#22411;DEECo&#65292;&#20351;&#20854;&#20855;&#26377;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#24314;&#31435;&#21644;&#37325;&#26032;&#37197;&#32622;&#33258;&#20027;&#32452;&#20214;&#38598;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27169;&#22411;&#23618;&#27425;&#19978;&#25429;&#25417;&#36825;&#20123;&#27010;&#24565;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#20102;&#22312;&#24037;&#19994;4.0&#29615;&#22659;&#20013;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#22914;&#20309;&#26377;&#30410;&#22320;&#29992;&#20110;&#24314;&#27169;&#35775;&#38382;&#25511;&#21046;&#30456;&#20851;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#32435;&#20837;&#29616;&#20195;&#26234;&#33021;&#31995;&#32479;&#26159;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#65292;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#24182;&#22312;&#36816;&#34892;&#26102;&#20248;&#21270;&#20854;&#34892;&#20026;&#65292;&#20197;&#24212;&#23545;&#29615;&#22659;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we extend our ensemble-based component model DEECo with the capability to use machine-learning and optimization heuristics in establishing and reconfiguration of autonomic component ensembles. We show how to capture these concepts on the model level and give an example of how such a model can be beneficially used for modeling access-control related problem in the Industry 4.0 settings. We argue that incorporating machine-learning and optimization heuristics is a key feature for modern smart systems which are to learn over the time and optimize their behavior at runtime to deal with uncertainty in their environment.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#23398;&#20064;&#26377;&#25928;&#21160;&#21147;&#23398;&#65288;iLED&#65289;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#24212;&#29992;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05812</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#22810;&#23610;&#24230;&#31995;&#32479;&#26377;&#25928;&#21160;&#21147;&#23398;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable learning of effective dynamics for multiscale systems. (arXiv:2309.05812v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#23398;&#20064;&#26377;&#25928;&#21160;&#21147;&#23398;&#65288;iLED&#65289;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#24212;&#29992;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#20223;&#30495;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#38754;&#20020;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#23613;&#31649;&#29616;&#20170;&#30340;&#35745;&#31639;&#26426;&#25216;&#26415;&#19981;&#26029;&#36827;&#27493;&#65292;&#35299;&#20915;&#30001;&#25511;&#21046;&#26041;&#31243;&#25551;&#36848;&#30340;&#25152;&#26377;&#26102;&#31354;&#23610;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#36965;&#19981;&#21487;&#21450;&#30340;&#30446;&#26631;&#12290;&#36825;&#31181;&#35748;&#35782;&#20419;&#20351;&#20154;&#20204;&#22823;&#21147;&#21457;&#23637;&#27169;&#22411;&#38477;&#38454;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#22312;&#22797;&#26434;&#26102;&#31354;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#20223;&#30495;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22411;&#24320;&#21457;&#30340;&#28789;&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#32467;&#21512;&#23454;&#39564;&#21644;&#35745;&#31639;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#23398;&#20064;&#26377;&#25928;&#21160;&#21147;&#23398;&#65288;iLED&#65289;&#26694;&#26550;&#65292;&#23427;&#20855;&#26377;&#19982;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modeling and simulation of high-dimensional multiscale systems is a critical challenge across all areas of science and engineering. It is broadly believed that even with today's computer advances resolving all spatiotemporal scales described by the governing equations remains a remote target. This realization has prompted intense efforts to develop model order reduction techniques. In recent years, techniques based on deep recurrent neural networks have produced promising results for the modeling and simulation of complex spatiotemporal systems and offer large flexibility in model development as they can incorporate experimental and computational data. However, neural networks lack interpretability, which limits their utility and generalizability across complex systems. Here we propose a novel framework of Interpretable Learning Effective Dynamics (iLED) that offers comparable accuracy to state-of-the-art recurrent neural network-based approaches while providing the added benefit o
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#39044;&#27979;&#20998;&#23376;&#20113;&#30340;&#36752;&#23556;&#22330;&#24378;&#24230;&#65292;&#36890;&#36807;&#21512;&#25104;&#23576;&#22467;&#21457;&#23556;&#22270;&#26469;&#20272;&#35745;&#26143;&#38469;&#36752;&#23556;&#22330;&#65288;ISRF&#65289;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#29289;&#29702;&#21442;&#25968;&#30340;&#26032;&#27169;&#25311;&#20013;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05811</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#39044;&#27979;&#20998;&#23376;&#20113;&#30340;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Predicting the Radiation Field of Molecular Clouds using Denoising Diffusion Probabilistic Models. (arXiv:2309.05811v1 [astro-ph.GA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05811
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#39044;&#27979;&#20998;&#23376;&#20113;&#30340;&#36752;&#23556;&#22330;&#24378;&#24230;&#65292;&#36890;&#36807;&#21512;&#25104;&#23576;&#22467;&#21457;&#23556;&#22270;&#26469;&#20272;&#35745;&#26143;&#38469;&#36752;&#23556;&#22330;&#65288;ISRF&#65289;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#29289;&#29702;&#21442;&#25968;&#30340;&#26032;&#27169;&#25311;&#20013;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#37327;&#21270;&#36752;&#23556;&#21453;&#39304;&#23545;&#24658;&#26143;&#24418;&#25104;&#30340;&#24433;&#21709;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21363;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#65292;&#26681;&#25454;4.5um&#12289;24um&#21644;250um&#19977;&#39057;&#27573;&#30340;&#23576;&#22467;&#21457;&#23556;&#26469;&#39044;&#27979;&#26143;&#38469;&#36752;&#23556;&#22330;&#65288;ISRF&#65289;&#24378;&#24230;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;STARFORGE&#65288;&#26143;&#38469;&#29615;&#22659;&#20013;&#30340;&#24658;&#26143;&#24418;&#25104;&#65289;&#39033;&#30446;&#20013;&#30340;&#30913;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#26469;&#27169;&#25311;&#24658;&#26143;&#24418;&#25104;&#21644;&#24040;&#22823;&#20998;&#23376;&#20113;&#65288;GMC&#65289;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19982;Monoceros R2&#65288;MonR2&#65289;GMC&#35266;&#27979;&#21040;&#30340;&#35889;&#33021;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#21512;&#25104;&#23576;&#22467;&#21457;&#23556;&#22270;&#12290;&#25105;&#20204;&#35757;&#32451;DDPMs&#26469;&#20351;&#29992;&#21512;&#25104;&#30340;&#19977;&#39057;&#27573;&#23576;&#22467;&#21457;&#23556;&#26469;&#20272;&#35745;ISRF&#12290;&#39044;&#27979;&#20540;&#19982;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#31163;&#25955;&#24230;&#22312;&#27979;&#35797;&#38598;&#20013;&#22312;0.1&#20493;&#20197;&#20869;&#12290;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#35780;&#20272;&#25193;&#23637;&#21040;&#21253;&#21547;&#19981;&#21516;&#29289;&#29702;&#21442;&#25968;&#30340;&#26032;&#27169;&#25311;&#20013;&#12290;&#22312;&#36825;&#20123;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#27169;&#25311;&#20013;&#35266;&#23519;&#21040;&#20102;&#19968;&#33268;&#30340;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately quantifying the impact of radiation feedback in star formation is challenging. To address this complex problem, we employ deep learning techniques, denoising diffusion probabilistic models (DDPMs), to predict the interstellar radiation field (ISRF) strength based on three-band dust emission at 4.5 \um, 24 \um, and 250 \um. We adopt magnetohydrodynamic simulations from the STARFORGE (STAR FORmation in Gaseous Environments) project that model star formation and giant molecular cloud (GMC) evolution. We generate synthetic dust emission maps matching observed spectral energy distributions in the Monoceros R2 (MonR2) GMC. We train DDPMs to estimate the ISRF using synthetic three-band dust emission. The dispersion between the predictions and true values is within a factor of 0.1 for the test set. We extended our assessment of the diffusion model to include new simulations with varying physical parameters. While there is a consistent offset observed in these out-of-distribution sim
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;SHIFT3D&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;&#20197;&#27450;&#39575;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#24182;&#19988;&#20855;&#26377;&#39044;&#38450;&#24615;&#21457;&#29616;&#28508;&#22312;&#23433;&#20840;&#39118;&#38505;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05810</link><description>&lt;p&gt;
SHIFT3D: &#21512;&#25104;&#22256;&#24785;&#19977;&#32500;&#26816;&#27979;&#22120;&#30340;&#24378;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors. (arXiv:2309.05810v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05810
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SHIFT3D&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;&#20197;&#27450;&#39575;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#24182;&#19988;&#20855;&#26377;&#39044;&#38450;&#24615;&#21457;&#29616;&#28508;&#22312;&#23433;&#20840;&#39118;&#38505;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SHIFT3D&#65292;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#29983;&#25104;&#22312;&#32467;&#26500;&#19978;&#21512;&#29702;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;&#65292;&#20197;&#27450;&#39575;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22120;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#21457;&#29616;&#36825;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29289;&#20307;&#21487;&#20197;&#25581;&#31034;&#19977;&#32500;&#26816;&#27979;&#22120;&#30340;&#26410;&#30693;&#28431;&#27934;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#26469;&#34920;&#31034;&#29289;&#20307;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26799;&#24230;&#35823;&#24046;&#20449;&#21495;&#20351;&#25105;&#20204;&#33021;&#22815;&#24179;&#28369;&#22320;&#25913;&#21464;&#19977;&#32500;&#29289;&#20307;&#30340;&#24418;&#29366;&#25110;&#23039;&#24577;&#65292;&#20197;&#22256;&#24785;&#19979;&#28216;&#30340;&#19977;&#32500;&#26816;&#27979;&#22120;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;SHIFT3D&#29983;&#25104;&#30340;&#29289;&#20307;&#22312;&#29289;&#29702;&#19978;&#19982;&#22522;&#20934;&#29289;&#20307;&#19981;&#21516;&#65292;&#20294;&#20445;&#30041;&#20102;&#35821;&#20041;&#21487;&#35782;&#21035;&#30340;&#24418;&#29366;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#29616;&#20195;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22120;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#36825;&#20123;&#39118;&#38505;&#21464;&#25104;&#20005;&#37325;&#25925;&#38556;&#20043;&#21069;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#24615;&#22320;&#21457;&#29616;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SHIFT3D, a differentiable pipeline for generating 3D shapes that are structurally plausible yet challenging to 3D object detectors. In safety-critical applications like autonomous driving, discovering such novel challenging objects can offer insight into unknown vulnerabilities of 3D detectors. By representing objects with a signed distanced function (SDF), we show that gradient error signals allow us to smoothly deform the shape or pose of a 3D object in order to confuse a downstream 3D detector. Importantly, the objects generated by SHIFT3D physically differ from the baseline object yet retain a semantically recognizable shape. Our approach provides interpretable failure modes for modern 3D object detectors, and can aid in preemptive discovery of potential safety risks within 3D perception systems before these risks become critical failures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#21644;&#20154;&#31867;&#30340;&#39068;&#33394;&#24863;&#30693;&#24046;&#24322;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;DNN&#32467;&#26500;&#22312;&#39068;&#33394;&#30456;&#20284;&#24615;&#21028;&#26029;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#23545;&#20110;&#28145;&#20837;&#29702;&#35299;DNN&#22312;&#20154;&#31867;&#35270;&#35273;&#26041;&#38754;&#30340;&#27169;&#25311;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.05809</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#22312;&#39068;&#33394;&#24863;&#30693;&#26041;&#38754;&#30340;&#24046;&#24322;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Divergences in Color Perception between Deep Neural Networks and Humans. (arXiv:2309.05809v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#21644;&#20154;&#31867;&#30340;&#39068;&#33394;&#24863;&#30693;&#24046;&#24322;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;DNN&#32467;&#26500;&#22312;&#39068;&#33394;&#30456;&#20284;&#24615;&#21028;&#26029;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#23545;&#20110;&#28145;&#20837;&#29702;&#35299;DNN&#22312;&#20154;&#31867;&#35270;&#35273;&#26041;&#38754;&#30340;&#27169;&#25311;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25552;&#20986;&#20316;&#20026;&#20154;&#31867;&#35270;&#35273;&#27169;&#22411;&#65292;&#36825;&#24471;&#30410;&#20110;&#23427;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;DNNs&#33021;&#21542;&#25429;&#25417;&#21040;&#20154;&#31867;&#35270;&#35273;&#20013;&#35832;&#22914;&#39068;&#33394;&#24863;&#30693;&#31561;&#22522;&#26412;&#26041;&#38754;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#26032;&#39062;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DNN&#20013;&#39068;&#33394;&#23884;&#20837;&#30340;&#24863;&#30693;&#19968;&#33268;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#23545;&#36890;&#36807;&#22312;&#32447;&#35843;&#26597;&#25910;&#38598;&#30340;&#20154;&#31867;&#39068;&#33394;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#20869;&#30340;&#26368;&#20808;&#36827;DNN&#32467;&#26500;&#22312;&#23545;&#25511;&#21046;&#39068;&#33394;&#29305;&#24615;&#30340;&#22270;&#20687;&#12289;&#22312;&#32447;&#25628;&#32034;&#29983;&#25104;&#30340;&#22270;&#20687;&#20197;&#21450;&#32463;&#20856;&#30340;CIFAR-10&#25968;&#25454;&#38598;&#20013;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#30340;&#39068;&#33394;&#30456;&#20284;&#24615;&#21028;&#26029;&#26041;&#38754;&#19982;&#20154;&#31867;&#39068;&#33394;&#21028;&#26029;&#26174;&#33879;&#19981;&#21516;&#12290;&#25105;&#20204;&#23558;DNN&#24615;&#33021;&#19982;&#22522;&#20110;&#23567;&#27874;&#20998;&#35299;&#30340;&#21487;&#35299;&#37322;&#19988;&#31526;&#21512;&#35748;&#30693;&#30340;&#39068;&#33394;&#24863;&#30693;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are increasingly proposed as models of human vision, bolstered by their impressive performance on image classification and object recognition tasks. Yet, the extent to which DNNs capture fundamental aspects of human vision such as color perception remains unclear. Here, we develop novel experiments for evaluating the perceptual coherence of color embeddings in DNNs, and we assess how well these algorithms predict human color similarity judgments collected via an online survey. We find that state-of-the-art DNN architectures $-$ including convolutional neural networks and vision transformers $-$ provide color similarity judgments that strikingly diverge from human color judgments of (i) images with controlled color properties, (ii) images generated from online searches, and (iii) real-world images from the canonical CIFAR-10 dataset. We compare DNN performance against an interpretable and cognitively plausible model of color perception based on wavelet decomp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25253;&#21578;&#20102;&#23558;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#36866;&#24212;&#20013;&#30340;&#32463;&#39564;&#65292;&#35752;&#35770;&#20102;&#28041;&#21450;&#35268;&#33539;&#12289;&#22312;&#32447;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#38519;&#38449;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#21487;&#20197;&#20026;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.05805</link><description>&lt;p&gt;
&#22312;&#38519;&#38449;&#38754;&#21069;&#30340;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Online ML Self-adaptation in Face of Traps. (arXiv:2309.05805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25253;&#21578;&#20102;&#23558;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#36866;&#24212;&#20013;&#30340;&#32463;&#39564;&#65292;&#35752;&#35770;&#20102;&#28041;&#21450;&#35268;&#33539;&#12289;&#22312;&#32447;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#38519;&#38449;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#21487;&#20197;&#20026;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#65292;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#32463;&#24120;&#34987;&#29992;&#26469;&#21152;&#24378;&#36866;&#24212;&#26426;&#21046;&#65292;&#25552;&#39640;&#31995;&#32479;&#25928;&#29992;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#22909;&#22788;&#65292;&#24212;&#29992;&#22312;&#32447;ML&#36827;&#34892;&#33258;&#36866;&#24212;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#19988;&#24456;&#23569;&#26377;&#35770;&#25991;&#25253;&#21578;&#20854;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#23581;&#35797;&#23558;&#22312;&#32447;ML&#24212;&#29992;&#20110;&#26234;&#33021;&#20892;&#19994;&#22330;&#26223;&#30340;&#33258;&#36866;&#24212;&#20013;&#65292;&#24182;&#19988;&#25105;&#20204;&#38754;&#20020;&#20102;&#19968;&#20123;&#24847;&#22806;&#30340;&#22256;&#38590; - &#38519;&#38449; - &#36825;&#22312;&#25105;&#20204;&#30340;&#20102;&#35299;&#20013;&#22312;&#31038;&#21306;&#20013;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#35752;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#25105;&#20204;&#22312;&#36825;&#20123;&#38519;&#38449;&#20013;&#30340;&#32463;&#39564;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#22522;&#20110;ML&#30340;&#20272;&#35745;&#22120;&#30340;&#35268;&#33539;&#21644;&#22312;&#32447;&#35757;&#32451;&#26377;&#20851;&#30340;&#20960;&#20010;&#38519;&#38449;&#65292;&#23427;&#20204;&#23545;&#33258;&#36866;&#24212;&#30340;&#24433;&#21709;&#20197;&#21450;&#29992;&#20110;&#35780;&#20272;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#38519;&#38449;&#30340;&#27010;&#36848;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#21487;&#20197;&#20026;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#22312;&#24212;&#29992;&#22312;&#32447;ML&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online machine learning (ML) is often used in self-adaptive systems to strengthen the adaptation mechanism and improve the system utility. Despite such benefits, applying online ML for self-adaptation can be challenging, and not many papers report its limitations. Recently, we experimented with applying online ML for self-adaptation of a smart farming scenario and we had faced several unexpected difficulties -- traps -- that, to our knowledge, are not discussed enough in the community. In this paper, we report our experience with these traps. Specifically, we discuss several traps that relate to the specification and online training of the ML-based estimators, their impact on self-adaptation, and the approach used to evaluate the estimators. Our overview of these traps provides a list of lessons learned, which can serve as guidance for other researchers and practitioners when applying online ML for self-adaptation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#33021;&#37327;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#26222;&#36941;&#20256;&#35328;&#26159;&#38169;&#35823;&#30340;&#12290;&#21516;&#26102;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#33021;&#22815;&#22312;&#39640;&#32500;&#36830;&#32493;&#31354;&#38388;&#20013;&#26377;&#25928;&#35757;&#32451;&#30340;&#25351;&#26631;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.05803</link><description>&lt;p&gt;
&#37325;&#35775;&#33021;&#37327;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#65306;&#35780;&#32423;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#21644;&#25554;&#20540;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Revisiting Energy Based Models as Policies: Ranking Noise Contrastive Estimation and Interpolating Energy Models. (arXiv:2309.05803v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05803
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#33021;&#37327;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#26222;&#36941;&#20256;&#35328;&#26159;&#38169;&#35823;&#30340;&#12290;&#21516;&#26102;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#33021;&#22815;&#22312;&#39640;&#32500;&#36830;&#32493;&#31354;&#38388;&#20013;&#26377;&#25928;&#35757;&#32451;&#30340;&#25351;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#26426;&#22120;&#20154;&#23398;&#20064;&#27969;&#31243;&#30340;&#20851;&#38190;&#35774;&#35745;&#20915;&#31574;&#26159;&#36873;&#25321;&#31574;&#30053;&#34920;&#31034;&#24418;&#24335;&#65306;&#24212;&#35813;&#20351;&#29992;&#20160;&#20040;&#31867;&#22411;&#30340;&#27169;&#22411;&#26469;&#29983;&#25104;&#19979;&#19968;&#27493;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#65311;&#30001;&#20110;&#35768;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#20855;&#26377;&#22266;&#26377;&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#24182;&#19988;&#32467;&#21512;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#36716;&#21521;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#22914;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#35775;&#20102;&#23558;&#33021;&#37327;&#27169;&#22411;&#65288;EBM&#65289;&#20316;&#20026;&#19968;&#31181;&#31574;&#30053;&#31867;&#21035;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#26222;&#36941;&#20256;&#35328;&#8212;&#8212;&#39640;&#32500;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#33021;&#37327;&#27169;&#22411;&#19981;&#23481;&#26131;&#35757;&#32451;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#33021;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#32467;&#21512;&#20102;&#20960;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;(i) &#35780;&#32423;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745; (R-NCE)&#65292;(ii) &#21487;&#23398;&#20064;&#30340;&#36127;&#37319;&#26679;&#22120;&#65292;&#20197;&#21450; (iii) &#38750;&#23545;&#25239;&#24615;&#32852;&#21512;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#26159;&#28176;&#36817;&#19968;&#33268;&#30340;&#65292;&#24182;&#37327;&#21270;&#20102;&#20854;&#26497;&#38480;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial design decision for any robot learning pipeline is the choice of policy representation: what type of model should be used to generate the next set of robot actions? Owing to the inherent multi-modal nature of many robotic tasks, combined with the recent successes in generative modeling, researchers have turned to state-of-the-art probabilistic models such as diffusion models for policy representation. In this work, we revisit the choice of energy-based models (EBM) as a policy class. We show that the prevailing folklore -- that energy models in high dimensional continuous spaces are impractical to train -is false. We develop a practical training objective and algorithm for energy models which combines several key ingredients: (i) ranking noise contrastive estimation (R-NCE), (ii) learnable negative samplers, and (iii) non-adversarial joint training. We prove that our proposed objective function is asymptotically consistent and quantify its limiting variance. On the other ha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#36229;&#36793;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33410;&#28857;&#32858;&#21512;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#36229;&#36793;&#39044;&#27979;&#20013;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#32531;&#35299;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05798</link><description>&lt;p&gt;
&#22686;&#24378;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36229;&#36793;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Hyperedge Prediction with Context-Aware Self-Supervised Learning. (arXiv:2309.05798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#36229;&#36793;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#33410;&#28857;&#32858;&#21512;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#36229;&#36793;&#39044;&#27979;&#20013;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#32531;&#35299;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#21487;&#20197;&#33258;&#28982;&#22320;&#24314;&#27169;&#32676;&#32452;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#19968;&#32452;&#20849;&#21516;&#36141;&#20080;&#29289;&#21697;&#30340;&#29992;&#25143;&#65289;&#65292;hyperedge&#39044;&#27979;&#26159;&#39044;&#27979;&#26410;&#26469;&#25110;&#26410;&#35266;&#23519;&#21040;&#30340;&#36229;&#36793;&#30340;&#20219;&#21153;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20013;&#24456;&#23569;&#25506;&#35752;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;C1&#65289;&#22914;&#20309;&#32858;&#21512;&#27599;&#20010;&#36229;&#36793;&#20505;&#36873;&#20013;&#30340;&#33410;&#28857;&#20197;&#20934;&#30830;&#39044;&#27979;&#36229;&#36793;&#65311;&#65288;C2&#65289;&#22914;&#20309;&#32531;&#35299;&#36229;&#36793;&#39044;&#27979;&#20013;&#22266;&#26377;&#30340;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#65311;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36229;&#36793;&#39044;&#27979;&#26694;&#26550;CASH&#65292;&#23427;&#37319;&#29992;&#20102;&#65288;1&#65289;&#19978;&#19979;&#25991;&#24863;&#30693;&#33410;&#28857;&#32858;&#21512;&#65292;&#31934;&#30830;&#25429;&#25417;&#27599;&#20010;&#36229;&#36793;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#29992;&#20110;&#35299;&#20915;&#25361;&#25112;&#65288;C1&#65289;&#65292;&#20197;&#21450;&#65288;2&#65289;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22312;&#36229;&#36793;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#22686;&#24378;&#36229;&#22270;&#34920;&#31034;&#65292;&#20197;&#24212;&#23545;&#25361;&#25112;&#65288;C2&#65289;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#25361;&#25112;&#65288;C2&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#36793;&#24863;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs can naturally model group-wise relations (e.g., a group of users who co-purchase an item) as hyperedges. Hyperedge prediction is to predict future or unobserved hyperedges, which is a fundamental task in many real-world applications (e.g., group recommendation). Despite the recent breakthrough of hyperedge prediction methods, the following challenges have been rarely studied: (C1) How to aggregate the nodes in each hyperedge candidate for accurate hyperedge prediction? and (C2) How to mitigate the inherent data sparsity problem in hyperedge prediction? To tackle both challenges together, in this paper, we propose a novel hyperedge prediction framework (CASH) that employs (1) context-aware node aggregation to precisely capture complex relations among nodes in each hyperedge for (C1) and (2) self-supervised contrastive learning in the context of hyperedge prediction to enhance hypergraph representations for (C2). Furthermore, as for (C2), we propose a hyperedge-aware augmenta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#21453;&#36716;&#29983;&#25104;&#27169;&#22411;&#30340;&#35745;&#31639;&#38590;&#24230;&#30340;&#32454;&#31890;&#24230;&#35270;&#22270;&#65292;&#24314;&#31435;&#20102;&#23545;&#31934;&#30830;&#21644;&#36817;&#20284;&#27169;&#22411;&#21453;&#36716;&#30340;&#26032;&#30340;&#38590;&#24230;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2309.05795</link><description>&lt;p&gt;
&#20851;&#20110;&#21453;&#36716;&#29983;&#25104;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
On the Fine-Grained Hardness of Inverting Generative Models. (arXiv:2309.05795v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#21453;&#36716;&#29983;&#25104;&#27169;&#22411;&#30340;&#35745;&#31639;&#38590;&#24230;&#30340;&#32454;&#31890;&#24230;&#35270;&#22270;&#65292;&#24314;&#31435;&#20102;&#23545;&#31934;&#30830;&#21644;&#36817;&#20284;&#27169;&#22411;&#21453;&#36716;&#30340;&#26032;&#30340;&#38590;&#24230;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#21453;&#36716;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#19968;&#20010;&#22823;&#23567;&#20026;$n$&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#35813;&#21521;&#37327;&#33021;&#22815;&#20135;&#29983;&#19982;&#32473;&#23450;&#30446;&#26631;&#23494;&#20999;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#28041;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35768;&#22810;&#29616;&#20195;&#24212;&#29992;&#20013;&#26159;&#26680;&#24515;&#30340;&#35745;&#31639;&#21407;&#35821;&#12290;&#28982;&#32780;&#65292;&#35813;&#38382;&#39064;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#34987;&#35748;&#20026;&#26159;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;NP&#38590;&#35299;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#35745;&#31639;&#38590;&#24230;&#30340;&#32454;&#31890;&#24230;&#35270;&#22270;&#12290;&#25105;&#20204;&#38024;&#23545;&#31934;&#30830;&#21644;&#36817;&#20284;&#27169;&#22411;&#21453;&#36716;&#24314;&#31435;&#20102;&#20960;&#20010;&#26032;&#30340;&#38590;&#24230;&#19979;&#30028;&#12290;&#22312;&#31934;&#30830;&#21453;&#36716;&#20013;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#19968;&#20010;&#30446;&#26631;&#26159;&#21542;&#21253;&#21547;&#22312;&#32473;&#23450;&#29983;&#25104;&#27169;&#22411;&#30340;&#33539;&#22260;&#20869;&#12290;&#22312;&#24378;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;&#65288;SETH&#65289;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;$k$-SAT&#30340;&#32422;&#31616;&#26469;&#35777;&#26126;&#65292;&#31934;&#30830;&#21453;&#36716;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#30028;&#20026;$\Omega(2^n)$&#65307;&#36825;&#26159;&#24050;&#30693;&#32467;&#26524;&#30340;&#21152;&#24378;&#29256;&#12290;&#23545;&#20110;&#26356;&#20855;&#23454;&#38469;&#24847;&#20041;&#30340;&#36817;&#20284;&#21453;&#36716;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23547;&#25214;&#19968;&#20010;&#28508;&#22312;&#21521;&#37327;&#65292;&#20351;&#24471;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#19982;&#32473;&#23450;&#30446;&#26631;&#23613;&#21487;&#33021;&#25509;&#36817;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#20010;&#24120;&#25968;$\gamma$&#65292;&#20351;&#24471;&#38500;&#38750;$\text{NP}\subseteq \text{BPTIME}(2^{O(n^\gamma)})$&#65292;&#21542;&#21017;&#36817;&#20284;&#21453;&#36716;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26080;&#27861;&#31361;&#30772;$\Omega(2^{n/2})$&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of generative model inversion is to identify a size-$n$ latent vector that produces a generative model output that closely matches a given target. This operation is a core computational primitive in numerous modern applications involving computer vision and NLP. However, the problem is known to be computationally challenging and NP-hard in the worst case. This paper aims to provide a fine-grained view of the landscape of computational hardness for this problem. We establish several new hardness lower bounds for both exact and approximate model inversion. In exact inversion, the goal is to determine whether a target is contained within the range of a given generative model. Under the strong exponential time hypothesis (SETH), we demonstrate that the computational complexity of exact inversion is lower bounded by $\Omega(2^n)$ via a reduction from $k$-SAT; this is a strengthening of known results. For the more practically relevant problem of approximate inversion, the goal 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29992;&#25143;&#20013;&#24515;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#22810;&#27169;&#24577;&#20132;&#20114;&#20013;&#33258;&#20027;&#31995;&#32479;&#23545;&#29289;&#20307;&#21644;&#29615;&#22659;&#30340;&#31526;&#21495;&#21270;&#29702;&#35299;&#33021;&#21147;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.05787</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#29992;&#25143;&#20013;&#24515;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#20132;&#20114;&#20013;&#30340;&#24212;&#29992;&#20110;&#33258;&#20027;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adaptive User-centered Neuro-symbolic Learning for Multimodal Interaction with Autonomous Systems. (arXiv:2309.05787v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29992;&#25143;&#20013;&#24515;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#22810;&#27169;&#24577;&#20132;&#20114;&#20013;&#33258;&#20027;&#31995;&#32479;&#23545;&#29289;&#20307;&#21644;&#29615;&#22659;&#30340;&#31526;&#21495;&#21270;&#29702;&#35299;&#33021;&#21147;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#20197;&#24863;&#30693;&#30340;&#38750;&#31526;&#21495;&#21270;&#26041;&#24335;&#35782;&#21035;&#21644;&#29702;&#35299;&#23545;&#35937;&#21450;&#20854;&#29615;&#22659;&#12290;&#36825;&#20123;&#31995;&#32479;&#29616;&#22312;&#21487;&#20197;&#25191;&#34892;&#29289;&#20307;&#26816;&#27979;&#12289;&#20256;&#24863;&#22120;&#25968;&#25454;&#34701;&#21512;&#21644;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#35201;&#25552;&#21319;&#36825;&#20123;&#31995;&#32479;&#23545;&#23545;&#35937;&#21450;&#20854;&#29615;&#22659;&#30340;&#27010;&#24565;&#21644;&#31526;&#21495;&#29702;&#35299;&#33021;&#21147;&#65292;&#38656;&#35201;&#32508;&#21512;&#32771;&#34385;&#20154;&#31867;&#25552;&#20379;&#30340;&#26174;&#24615;&#25945;&#23548;&#65288;&#20363;&#22914;&#25551;&#36848;&#24773;&#20917;&#25110;&#35299;&#37322;&#22914;&#20309;&#34892;&#21160;&#65289;&#21644;&#36890;&#36807;&#35266;&#23519;&#20154;&#31867;&#34892;&#20026;&#65288;&#36890;&#36807;&#31995;&#32479;&#30340;&#20256;&#24863;&#22120;&#65289;&#33719;&#24471;&#30340;&#38544;&#24615;&#25945;&#23548;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#24517;&#39035;&#35774;&#35745;&#20855;&#26377;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#33021;&#21147;&#65292;&#20197;&#25903;&#25345;&#38544;&#24615;&#21644;&#26174;&#24615;&#20132;&#20114;&#27169;&#22411;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20027;&#24352;&#21516;&#26102;&#32771;&#34385;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65292;&#20197;&#21450;&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#21442;&#19982;&#21644;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#25512;&#36827;&#36825;&#19968;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#27700;&#24179;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning, particularly deep learning, have enabled autonomous systems to perceive and comprehend objects and their environments in a perceptual subsymbolic manner. These systems can now perform object detection, sensor data fusion, and language understanding tasks. However, there is a growing need to enhance these systems to understand objects and their environments more conceptually and symbolically. It is essential to consider both the explicit teaching provided by humans (e.g., describing a situation or explaining how to act) and the implicit teaching obtained by observing human behavior (e.g., through the system's sensors) to achieve this level of powerful artificial intelligence. Thus, the system must be designed with multimodal input and output capabilities to support implicit and explicit interaction models. In this position paper, we argue for considering both types of inputs, as well as human-in-the-loop and incremental learning techniques, for advan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28784;&#30418;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20223;&#30495;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20851;&#20110;&#27963;&#21160;&#30340;&#31354;&#38388;&#20998;&#24067;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#65292;&#24182;&#22312;&#36300;&#20498;&#26816;&#27979;&#12289;&#23460;&#20869;&#23450;&#20301;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#20110;&#40657;&#30418;&#20248;&#21270;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05784</link><description>&lt;p&gt;
&#36741;&#21161;&#29983;&#27963;&#29615;&#22659;&#20013;&#20256;&#24863;&#22120;&#24067;&#32622;&#30340;&#28784;&#30418;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Grey-box Bayesian Optimization for Sensor Placement in Assisted Living Environments. (arXiv:2309.05784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28784;&#30418;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20223;&#30495;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20851;&#20110;&#27963;&#21160;&#30340;&#31354;&#38388;&#20998;&#24067;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#65292;&#24182;&#22312;&#36300;&#20498;&#26816;&#27979;&#12289;&#23460;&#20869;&#23450;&#20301;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#20110;&#40657;&#30418;&#20248;&#21270;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#20256;&#24863;&#22120;&#30340;&#37197;&#32622;&#21644;&#24067;&#32622;&#23545;&#20110;&#21487;&#38752;&#30340;&#36300;&#20498;&#26816;&#27979;&#12289;&#23460;&#20869;&#23450;&#20301;&#21644;&#27963;&#21160;&#35782;&#21035;&#22312;&#36741;&#21161;&#29983;&#27963;&#31354;&#38388;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28784;&#30418;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#22522;&#20110;&#20223;&#30495;&#30340;&#35780;&#20272;&#65292;&#22312;&#20219;&#24847;&#23460;&#20869;&#31354;&#38388;&#20013;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#22312;&#20110;&#25429;&#25417;&#20851;&#20110;&#27963;&#21160;&#30340;&#31354;&#38388;&#20998;&#24067;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36845;&#20195;&#26597;&#35810;&#28857;&#36873;&#25321;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20004;&#20010;&#27169;&#25311;&#23460;&#20869;&#29615;&#22659;&#21644;&#21253;&#21547;&#20154;&#31867;&#27963;&#21160;&#21644;&#20256;&#24863;&#22120;&#35302;&#21457;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#39640;&#36136;&#37327;&#20256;&#24863;&#22120;&#24067;&#32622;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#40657;&#30418;&#20248;&#21270;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#65292;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#27963;&#21160;&#35782;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#22823;&#24133;&#24230;&#20943;&#23569;(&#24179;&#22343;&#20943;&#23569;51.3%)&#26114;&#36149;&#30340;&#20989;&#25968;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing the configuration and placement of sensors is crucial for reliable fall detection, indoor localization, and activity recognition in assisted living spaces. We propose a novel, sample-efficient approach to find a high-quality sensor placement in an arbitrary indoor space based on grey-box Bayesian optimization and simulation-based evaluation. Our key technical contribution lies in capturing domain-specific knowledge about the spatial distribution of activities and incorporating it into the iterative selection of query points in Bayesian optimization. Considering two simulated indoor environments and a real-world dataset containing human activities and sensor triggers, we show that our proposed method performs better compared to state-of-the-art black-box optimization techniques in identifying high-quality sensor placements, leading to accurate activity recognition in terms of F1-score, while also requiring a significantly lower (51.3% on average) number of expensive function 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26234;&#33021;&#25163;&#34920;&#25910;&#38598;&#22768;&#23398;&#29305;&#24449;&#20316;&#20026;&#26816;&#27979;&#35748;&#30693;&#32570;&#38519;&#24341;&#36215;&#30340;&#26085;&#24120;&#21151;&#33021;&#32570;&#38519;&#30340;&#23458;&#35266;&#26631;&#35760;&#30340;&#21487;&#34892;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20351;&#29992;&#22768;&#23398;&#29305;&#24449;&#21487;&#20197;&#20197;&#39640;&#20934;&#30830;&#29575;&#26816;&#27979;&#21040;&#23384;&#22312;&#26085;&#24120;&#21151;&#33021;&#32570;&#38519;&#30340;&#20010;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.05777</link><description>&lt;p&gt;
&#26234;&#33021;&#25163;&#34920;&#27966;&#29983;&#30340;&#22768;&#23398;&#26631;&#35760;&#29992;&#20110;&#35748;&#30693;&#30456;&#20851;&#26085;&#24120;&#21151;&#33021;&#30340;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Smartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday Functioning. (arXiv:2309.05777v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05777
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26234;&#33021;&#25163;&#34920;&#25910;&#38598;&#22768;&#23398;&#29305;&#24449;&#20316;&#20026;&#26816;&#27979;&#35748;&#30693;&#32570;&#38519;&#24341;&#36215;&#30340;&#26085;&#24120;&#21151;&#33021;&#32570;&#38519;&#30340;&#23458;&#35266;&#26631;&#35760;&#30340;&#21487;&#34892;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20351;&#29992;&#22768;&#23398;&#29305;&#24449;&#21487;&#20197;&#20197;&#39640;&#20934;&#30830;&#29575;&#26816;&#27979;&#21040;&#23384;&#22312;&#26085;&#24120;&#21151;&#33021;&#32570;&#38519;&#30340;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#30001;&#35748;&#30693;&#32570;&#38519;&#24341;&#36215;&#30340;&#26085;&#24120;&#21151;&#33021;&#24494;&#22937;&#32570;&#38519;&#23545;&#26089;&#26399;&#21457;&#29616;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#23588;&#20854;&#26159;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26085;&#24120;&#21151;&#33021;&#35780;&#20272;&#30340;&#26631;&#20934;&#22522;&#20110;&#23450;&#24615;&#30340;&#20027;&#35266;&#35780;&#32423;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#35821;&#38899;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#23458;&#35266;&#26631;&#35760;&#26469;&#35780;&#20272;&#35748;&#30693;&#32570;&#38519;&#65292;&#20294;&#19982;&#35748;&#30693;&#30456;&#20851;&#30340;&#26085;&#24120;&#21151;&#33021;&#30340;&#20851;&#32852;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#22522;&#20110;&#26234;&#33021;&#25163;&#34920;&#30340;&#24212;&#29992;&#31243;&#24207;&#25910;&#38598;&#22768;&#23398;&#29305;&#24449;&#20316;&#20026;&#26816;&#27979;&#26085;&#24120;&#21151;&#33021;&#32570;&#38519;&#30340;&#23458;&#35266;&#26631;&#35760;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#20174;54&#21517;&#32769;&#24180;&#20154;&#20013;&#25910;&#38598;&#20102;&#22312;&#25191;&#34892;&#35748;&#30693;&#20219;&#21153;&#21644;&#26085;&#24120;&#23545;&#35805;&#20013;&#30340;&#35821;&#38899;&#25968;&#25454;&#65292;&#20316;&#20026;&#21487;&#33021;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#23545;&#26085;&#24120;&#21151;&#33021;&#36827;&#34892;&#20102;&#27979;&#37327;&#12290;&#20351;&#29992;&#22768;&#23398;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20197;&#39640;&#36798;77.8%&#30340;&#20934;&#30830;&#29575;&#26816;&#27979;&#21040;&#26085;&#24120;&#21151;&#33021;&#32570;&#38519;&#30340;&#20010;&#20307;&#65292;&#36825;&#27604;&#24403;&#21069;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#35201;&#39640;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of subtle deficits in everyday functioning due to cognitive impairment is important for early detection of neurodegenerative diseases, particularly Alzheimer's disease. However, current standards for assessment of everyday functioning are based on qualitative, subjective ratings. Speech has been shown to provide good objective markers for cognitive impairments, but the association with cognition-relevant everyday functioning remains uninvestigated. In this study, we demonstrate the feasibility of using a smartwatch-based application to collect acoustic features as objective markers for detecting deficits in everyday functioning. We collected voice data during the performance of cognitive tasks and daily conversation, as possible application scenarios, from 54 older adults, along with a measure of everyday functioning. Machine learning models using acoustic features could detect individuals with deficits in everyday functioning with up to 77.8% accuracy, which was higher than 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#32500;&#24230;&#23545;&#21387;&#32553;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#21387;&#32553;&#21518;&#22312;&#20302;&#32500;&#31354;&#38388;&#20869;&#35757;&#32451;&#20840;&#31209;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#29702;&#35770;&#20445;&#35777;&#20102;&#22312;&#19981;&#20381;&#36182;&#29615;&#22659;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#24230;&#37327;&#23398;&#20064;&#30340;&#35823;&#24046;&#21487;&#20197;&#34987;&#25511;&#21046;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#33391;&#24615;&#20960;&#20309;&#32467;&#26500;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.05751</link><description>&lt;p&gt;
&#20869;&#22312;&#32500;&#24230;&#23545;&#21387;&#32553;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Intrinsic Dimension on Metric Learning under Compression. (arXiv:2309.05751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#32500;&#24230;&#23545;&#21387;&#32553;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#21387;&#32553;&#21518;&#22312;&#20302;&#32500;&#31354;&#38388;&#20869;&#35757;&#32451;&#20840;&#31209;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#29702;&#35770;&#20445;&#35777;&#20102;&#22312;&#19981;&#20381;&#36182;&#29615;&#22659;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#24230;&#37327;&#23398;&#20064;&#30340;&#35823;&#24046;&#21487;&#20197;&#34987;&#25511;&#21046;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#33391;&#24615;&#20960;&#20309;&#32467;&#26500;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24230;&#37327;&#23398;&#20064;&#26088;&#22312;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#36866;&#24403;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#20197;&#25913;&#21892;&#22522;&#20110;&#36317;&#31163;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#65292;&#24230;&#37327;&#23398;&#20064;&#36824;&#21487;&#20197;&#20316;&#20026;&#38477;&#32500;&#30340;&#25163;&#27573;&#65292;&#36890;&#36807;&#23545;&#23398;&#20064;&#30340;&#24230;&#37327;&#26045;&#21152;&#19968;&#20010;&#20302;&#31209;&#32422;&#26463;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#30340;&#26159;&#23545;&#25968;&#25454;&#30340;&#19968;&#20010;&#38543;&#26426;&#21387;&#32553;&#29256;&#26412;&#65292;&#28982;&#21518;&#22312;&#20854;&#20013;&#35757;&#32451;&#19968;&#20010;&#20840;&#31209;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20851;&#20110;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#30340;&#35823;&#24046;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#19981;&#20381;&#36182;&#20110;&#29615;&#22659;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#36793;&#30028;&#38500;&#20102;&#23545;&#26469;&#33258;&#26377;&#30028;&#25903;&#25345;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#27809;&#26377;&#26174;&#24335;&#30340;&#20551;&#35774;&#20043;&#22806;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#33391;&#24615;&#20960;&#20309;&#32467;&#26500;&#26102;&#33258;&#21160;&#25910;&#25947;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metric learning aims at finding a suitable distance metric over the input space, to improve the performance of distance-based learning algorithms. In high-dimensional settings, metric learning can also play the role of dimensionality reduction, by imposing a low-rank restriction to the learnt metric. In this paper, instead of training a low-rank metric on high-dimensional data, we consider a randomly compressed version of the data, and train a full-rank metric there. We give theoretical guarantees on the error of distance-based metric learning, with respect to the random compression, which do not depend on the ambient dimension. Our bounds do not make any explicit assumptions, aside from i.i.d. data from a bounded support, and automatically tighten when benign geometrical structures are present. Experimental results on both synthetic and real data sets support our theoretical findings in high-dimensional settings.
&lt;/p&gt;</description></item><item><title>CaloClouds II&#26159;&#19968;&#20010;&#36229;&#24555;&#36895;&#20960;&#20309;&#29420;&#31435;&#30340;&#39640;&#20998;&#36776;&#29575;&#37327;&#33021;&#22120;&#27169;&#25311;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#24471;&#20998;&#24314;&#27169;&#65292;&#30456;&#27604;&#20256;&#32479;&#27169;&#25311;&#26356;&#24555;&#19988;&#20855;&#26377;&#21487;&#27604;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.05704</link><description>&lt;p&gt;
CaloClouds II: &#36229;&#24555;&#36895;&#20960;&#20309;&#29420;&#31435;&#30340;&#39640;&#20998;&#36776;&#29575;&#37327;&#33021;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation. (arXiv:2309.05704v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05704
&lt;/p&gt;
&lt;p&gt;
CaloClouds II&#26159;&#19968;&#20010;&#36229;&#24555;&#36895;&#20960;&#20309;&#29420;&#31435;&#30340;&#39640;&#20998;&#36776;&#29575;&#37327;&#33021;&#22120;&#27169;&#25311;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#24471;&#20998;&#24314;&#27169;&#65292;&#30456;&#27604;&#20256;&#32479;&#27169;&#25311;&#26356;&#24555;&#19988;&#20855;&#26377;&#21487;&#27604;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#30340;&#33021;&#37327;&#27785;&#31215;&#30340;&#24555;&#36895;&#27169;&#25311;&#23545;&#20110;&#26410;&#26469;&#20855;&#26377;&#19981;&#26029;&#22686;&#21152;&#20142;&#24230;&#30340;&#23545;&#25758;&#26426;&#23454;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#21152;&#24555;&#21644;&#22686;&#24378;&#29289;&#29702;&#20998;&#26512;&#20013;&#30340;&#20256;&#32479;&#27169;&#25311;&#38142;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#21162;&#21147;&#23616;&#38480;&#20110;&#20381;&#36182;&#20110;&#22266;&#23450;&#12289;&#35268;&#21017;&#30340;&#25506;&#27979;&#22120;&#35835;&#20986;&#20960;&#20309;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;CaloClouds&#27169;&#22411;&#26159;&#19968;&#20010;&#20960;&#20309;&#29420;&#31435;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20026;&#35774;&#24819;&#20013;&#30340;&#22269;&#38469;&#22823;&#22411;&#25506;&#27979;&#22120;&#65288;ILD&#65289;&#30340;&#30005;&#30913;&#37327;&#33021;&#22120;&#29983;&#25104;&#22359;&#29366;&#28857;&#20113;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CaloClouds II&#65292;&#23427;&#20855;&#26377;&#19968;&#20123;&#20851;&#38190;&#30340;&#25913;&#36827;&#12290;&#36825;&#21253;&#25324;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#24471;&#20998;&#30340;&#24314;&#27169;&#65292;&#23427;&#20801;&#35768;&#36827;&#34892;25&#27493;&#37319;&#26679;&#65292;&#19982;CaloClouds&#30340;&#20445;&#30495;&#24230;&#30456;&#24403;&#65292;&#21516;&#26102;&#22312;&#21333;&#20010;CPU&#19978;&#27604;Geant4&#24555;6&#20493;&#65288;&#27604;CaloClouds&#24555;5&#20493;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25193;&#25955;&#27169;&#22411;&#25552;&#28860;&#25104;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast simulation of the energy depositions in high-granular detectors is needed for future collider experiments with ever increasing luminosities. Generative machine learning (ML) models have been shown to speed up and augment the traditional simulation chain in physics analysis. However, the majority of previous efforts were limited to models relying on fixed, regular detector readout geometries. A major advancement is the recently introduced CaloClouds model, a geometry-independent diffusion model, which generates calorimeter showers as point clouds for the electromagnetic calorimeter of the envisioned International Large Detector (ILD).  In this work, we introduce CaloClouds II which features a number of key improvements. This includes continuous time score-based modelling, which allows for a 25 step sampling with comparable fidelity to CaloClouds while yielding a $6\times$ speed-up over Geant4 on a single CPU ($5\times$ over CaloClouds). We further distill the diffusion model into a
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#21033;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#37319;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644;t-&#20998;&#24067;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;t-SNE&#65289;&#31561;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#23558;&#26631;&#35760;&#30340;&#20849;&#24179;&#38754;&#31354;&#38388;&#25237;&#24433;&#21040;&#20855;&#26377;&#30456;&#30028;&#38480;&#30340;&#20302;&#32500;&#30456;&#31354;&#38388;&#20013;&#65292;&#20197;&#35782;&#21035;&#19982;&#30456;&#21516;&#27867;&#22411;Calabi-Yau 3&#25240;&#23545;&#24212;&#30340;4&#32500;N=1&#36229;&#23545;&#31216;&#35268;&#33539;&#29702;&#35770;&#30340;&#27867;&#22411;&#30456;&#12290;</title><link>http://arxiv.org/abs/2309.05702</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#25506;&#32034;&#28909;&#24102;&#20849;&#24179;&#38754;&#65292;&#33180;&#22270;&#21644;Seiberg&#23545;&#20598;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Machine Learning Techniques for Exploring Tropical Coamoeba, Brane Tilings and Seiberg Duality. (arXiv:2309.05702v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05702
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#37319;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644;t-&#20998;&#24067;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;t-SNE&#65289;&#31561;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#23558;&#26631;&#35760;&#30340;&#20849;&#24179;&#38754;&#31354;&#38388;&#25237;&#24433;&#21040;&#20855;&#26377;&#30456;&#30028;&#38480;&#30340;&#20302;&#32500;&#30456;&#31354;&#38388;&#20013;&#65292;&#20197;&#35782;&#21035;&#19982;&#30456;&#21516;&#27867;&#22411;Calabi-Yau 3&#25240;&#23545;&#24212;&#30340;4&#32500;N=1&#36229;&#23545;&#31216;&#35268;&#33539;&#29702;&#35770;&#30340;&#27867;&#22411;&#30456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#35782;&#21035;&#19982;&#30456;&#21516;&#30340;&#27867;&#22411;Calabi-Yau 3&#25240;&#23545;&#24212;&#30340;4&#32500;N=1&#36229;&#23545;&#31216;&#35268;&#33539;&#29702;&#35770;&#30340;&#27867;&#22411;&#30456;&#12290;&#36825;&#20123;4&#32500;N=1&#36229;&#23545;&#31216;&#35268;&#33539;&#29702;&#35770;&#26159;&#25506;&#27979;&#27867;&#22411;Calabi-Yau 3&#25240;&#30340;D3-&#33180;&#30340;&#19990;&#30028;&#20307;&#29702;&#35770;&#65292;&#24182;&#19988;&#20197;&#31216;&#20026;&#33180;&#22270;&#30340;Type IIB&#33180;&#37197;&#32622;&#24418;&#24335;&#23454;&#29616;&#12290;&#23427;&#23545;&#24212;&#20110;&#19982;&#27867;&#22411;Calabi-Yau 3&#25240;&#20851;&#32852;&#30340;&#38236;&#38754;&#26354;&#32447;&#30340;&#20849;&#24179;&#38754;&#25237;&#24433;&#30340;&#39592;&#26550;&#22270;&#12290;&#24403;&#25105;&#20204;&#25913;&#21464;&#38236;&#38754;Calabi-Yau 3&#25240;&#30340;&#22797;&#32467;&#26500;&#27169;&#37327;&#26102;&#65292;&#20849;&#24179;&#38754;&#21644;&#30456;&#24212;&#30340;&#33180;&#22270;&#20250;&#25913;&#21464;&#20854;&#24418;&#29366;&#65292;&#20135;&#29983;&#30001;Seiberg&#23545;&#20598;&#30456;&#32852;&#31995;&#30340;&#19981;&#21516;&#27867;&#22411;&#30456;&#12290;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644;t-&#20998;&#24067;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;t-SNE&#65289;&#31561;&#25216;&#26415;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#30001;&#22797;&#32467;&#26500;&#27169;&#37327;&#26631;&#35760;&#30340;&#20849;&#24179;&#38754;&#31354;&#38388;&#25237;&#24433;&#21040;&#20855;&#26377;&#30456;&#30028;&#38480;&#30340;&#20302;&#32500;&#30456;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce unsupervised machine learning techniques in order to identify toric phases of 4d N=1 supersymmetric gauge theories corresponding to the same toric Calabi-Yau 3-fold. These 4d N=1 supersymmetric gauge theories are worldvolume theories of a D3-brane probing a toric Calabi-Yau 3-fold and are realized in terms of a Type IIB brane configuration known as a brane tiling. It corresponds to the skeleton graph of the coamoeba projection of the mirror curve associated to the toric Calabi-Yau 3-fold. When we vary the complex structure moduli of the mirror Calabi-Yau 3-fold, the coamoeba and the corresponding brane tilings change their shape, giving rise to different toric phases related by Seiberg duality. We illustrate that by employing techniques such as principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE), we can project the space of coamoeba labelled by complex structure moduli down to a lower dimensional phase space with phase boundaries corr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27969;&#24335;&#38647;&#36798;&#25968;&#25454;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#22686;&#24378;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#25928;&#29575;&#30340;&#26032;&#25216;&#26415;&#12290;&#36890;&#36807;&#22312;&#32593;&#32476;&#20013;&#28155;&#21152;&#39069;&#22806;&#30340;&#20998;&#31867;&#22120;&#20998;&#25903;&#65292;&#23454;&#29616;&#26681;&#25454;&#36816;&#34892;&#26102;&#20915;&#31574;&#26426;&#21046;&#25552;&#21069;&#32456;&#27490;&#25512;&#29702;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#32780;&#20445;&#25345;&#26368;&#23567;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#27604;&#21333;&#36335;&#36864;&#20986;&#32593;&#32476;&#21644;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#26089;&#26399;&#36864;&#20986;&#29256;&#26412;&#33410;&#30465;&#39640;&#36798;26%&#21644;12%&#30340;&#25512;&#29702;&#25805;&#20316;&#12290;&#36825;&#20123;&#25216;&#26415;&#36866;&#29992;&#20110;&#36890;&#29992;&#30828;&#20214;&#24182;&#21487;&#20197;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.05686</link><description>&lt;p&gt;
&#26102;&#38388;&#32784;&#24515;&#65306;&#29992;&#20110;&#23884;&#20837;&#24335;&#38647;&#36798;&#25968;&#25454;&#22788;&#29702;&#30340;&#39640;&#25928;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Temporal Patience: Efficient Adaptive Deep Learning for Embedded Radar Data Processing. (arXiv:2309.05686v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05686
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27969;&#24335;&#38647;&#36798;&#25968;&#25454;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#22686;&#24378;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#25928;&#29575;&#30340;&#26032;&#25216;&#26415;&#12290;&#36890;&#36807;&#22312;&#32593;&#32476;&#20013;&#28155;&#21152;&#39069;&#22806;&#30340;&#20998;&#31867;&#22120;&#20998;&#25903;&#65292;&#23454;&#29616;&#26681;&#25454;&#36816;&#34892;&#26102;&#20915;&#31574;&#26426;&#21046;&#25552;&#21069;&#32456;&#27490;&#25512;&#29702;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#32780;&#20445;&#25345;&#26368;&#23567;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#27604;&#21333;&#36335;&#36864;&#20986;&#32593;&#32476;&#21644;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#26089;&#26399;&#36864;&#20986;&#29256;&#26412;&#33410;&#30465;&#39640;&#36798;26%&#21644;12%&#30340;&#25512;&#29702;&#25805;&#20316;&#12290;&#36825;&#20123;&#25216;&#26415;&#36866;&#29992;&#20110;&#36890;&#29992;&#30828;&#20214;&#24182;&#21487;&#20197;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38647;&#36798;&#20256;&#24863;&#22120;&#20026;&#24120;&#24320;&#26234;&#33021;&#35774;&#22791;&#25552;&#20379;&#20102;&#21151;&#32791;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#22788;&#29702;&#25968;&#25454;&#27969;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#27969;&#24335;&#38647;&#36798;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#22686;&#24378;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#30340;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#32593;&#32476;&#22312;&#26550;&#26500;&#30340;&#38544;&#34255;&#23618;&#20043;&#38388;&#28155;&#21152;&#20102;&#39069;&#22806;&#30340;&#20998;&#31867;&#22120;&#20998;&#25903;&#65292;&#20801;&#35768;&#26681;&#25454;&#36816;&#34892;&#26102;&#20915;&#31574;&#26426;&#21046;&#30340;&#32467;&#26524;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25552;&#21069;&#32456;&#27490;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#22312;&#20309;&#26102;&#32456;&#27490;&#25512;&#29702;&#26377;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#26368;&#23567;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21333;&#36335;&#36864;&#20986;&#32593;&#32476;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#27599;&#27425;&#25512;&#29702;&#33410;&#30465;&#20102;&#39640;&#36798;26%&#30340;&#25805;&#20316;&#65292;&#19982;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#26089;&#26399;&#36864;&#20986;&#29256;&#26412;&#30456;&#27604;&#33410;&#30465;&#20102;12%&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#36866;&#29992;&#20110;&#36890;&#29992;&#30828;&#20214;&#24182;&#21487;&#20197;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radar sensors offer power-efficient solutions for always-on smart devices, but processing the data streams on resource-constrained embedded platforms remains challenging. This paper presents novel techniques that leverage the temporal correlation present in streaming radar data to enhance the efficiency of Early Exit Neural Networks for Deep Learning inference on embedded devices. These networks add additional classifier branches between the architecture's hidden layers that allow for an early termination of the inference if their result is deemed sufficient enough by an at-runtime decision mechanism. Our methods enable more informed decisions on when to terminate the inference, reducing computational costs while maintaining a minimal loss of accuracy.  Our results demonstrate that our techniques save up to 26% of operations per inference over a Single Exit Network and 12% over a confidence-based Early Exit version. Our proposed techniques work on commodity hardware and can be combined
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#23478;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#22312;&#32447;&#36712;&#36857;&#39044;&#27979;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35843;&#25972;&#32593;&#32476;&#23618;&#30340;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#38382;&#39064;&#65292;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26032;&#22330;&#26223;&#30340;&#30693;&#35782;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.05683</link><description>&lt;p&gt;
EANet: &#19987;&#23478;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#22312;&#32447;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EANet: Expert Attention Network for Online Trajectory Prediction. (arXiv:2309.05683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#23478;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#22312;&#32447;&#36712;&#36857;&#39044;&#27979;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35843;&#25972;&#32593;&#32476;&#23618;&#30340;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#38382;&#39064;&#65292;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26032;&#22330;&#26223;&#30340;&#30693;&#35782;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#20027;&#27969;&#30740;&#31350;&#21644;&#22522;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#22312;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#23548;&#33268;&#22312;&#22330;&#26223;&#31361;&#28982;&#21464;&#21270;&#26102;&#39044;&#27979;&#20934;&#30830;&#24230;&#36739;&#20302;&#65292;&#26080;&#27861;&#21450;&#26102;&#21709;&#24212;&#21644;&#26356;&#26032;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#23454;&#26102;&#39044;&#27979;&#24182;&#20351;&#29992;&#25968;&#25454;&#23454;&#20363;&#31435;&#21363;&#26356;&#26032;&#27169;&#22411;&#65288;&#21363;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#65289;&#20173;&#28982;&#26159;&#20010;&#38382;&#39064;&#12290;&#36824;&#38656;&#35201;&#35299;&#20915;&#30001;&#25968;&#25454;&#23454;&#20363;&#27969;&#24341;&#36215;&#30340;&#26799;&#24230;&#29190;&#28856;&#25110;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;&#21463;&#21040;Hedge Propagation&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#19987;&#23478;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#19987;&#23478;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35843;&#25972;&#32593;&#32476;&#23618;&#30340;&#19981;&#21516;&#28145;&#24230;&#30340;&#26435;&#37325;&#65292;&#36991;&#20813;&#30001;&#20110;&#26799;&#24230;&#38382;&#39064;&#23548;&#33268;&#27169;&#22411;&#26356;&#26032;&#32531;&#24930;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26032;&#22330;&#26223;&#30340;&#30693;&#35782;&#20197;&#24674;&#22797;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#30701;&#26399;&#36816;&#21160;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trajectory prediction plays a crucial role in autonomous driving. Existing mainstream research and continuoual learning-based methods all require training on complete datasets, leading to poor prediction accuracy when sudden changes in scenarios occur and failing to promptly respond and update the model. Whether these methods can make a prediction in real-time and use data instances to update the model immediately(i.e., online learning settings) remains a question. The problem of gradient explosion or vanishing caused by data instance streams also needs to be addressed. Inspired by Hedge Propagation algorithm, we propose Expert Attention Network, a complete online learning framework for trajectory prediction. We introduce expert attention, which adjusts the weights of different depths of network layers, avoiding the model updated slowly due to gradient problem and enabling fast learning of new scenario's knowledge to restore prediction accuracy. Furthermore, we propose a short-term mot
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#28304;&#22823;&#20840;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#27861;&#24459;&#12289;&#29983;&#21629;&#31185;&#23398;&#12289;&#26032;&#38395;&#31038;&#20132;&#31561;&#65292;&#20197;&#28385;&#36275;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.05682</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#28304;&#22823;&#20840;
&lt;/p&gt;
&lt;p&gt;
A compendium of data sources for data science, machine learning, and artificial intelligence. (arXiv:2309.05682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05682
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#28304;&#22823;&#20840;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#27861;&#24459;&#12289;&#29983;&#21629;&#31185;&#23398;&#12289;&#26032;&#38395;&#31038;&#20132;&#31561;&#65292;&#20197;&#28385;&#36275;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23548;&#33268;&#23545;&#21487;&#20379;&#36825;&#20123;&#27169;&#22411;&#22788;&#29702;&#30340;&#25968;&#25454;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#34429;&#28982;&#25968;&#25454;&#28304;&#26159;&#24212;&#29992;&#29305;&#23450;&#30340;&#65292;&#26080;&#27861;&#21015;&#20986;&#35814;&#23613;&#26080;&#36951;&#30340;&#25968;&#25454;&#28304;&#21015;&#34920;&#65292;&#20294;&#19968;&#20010;&#20840;&#38754;&#32780;&#19981;&#23436;&#25972;&#30340;&#21015;&#34920;&#20173;&#28982;&#26377;&#21033;&#20110;&#21508;&#32423;&#21035;&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#36825;&#26679;&#19968;&#20010;&#65288;&#24517;&#28982;&#19981;&#23436;&#25972;&#30340;&#65289;&#21015;&#34920;&#65292;&#21363;&#36328;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#25968;&#25454;&#28304;&#22823;&#20840;&#25110;&#27010;&#35272;&#65292;&#21253;&#25324;&#37329;&#34701;&#21644;&#32463;&#27982;&#12289;&#27861;&#24459;&#65288;&#27861;&#24459;&#21644;&#27861;&#35268;&#65289;&#12289;&#29983;&#21629;&#31185;&#23398;&#65288;&#21307;&#23398;&#21644;&#33647;&#29289;&#21457;&#29616;&#65289;&#12289;&#26032;&#38395;&#24773;&#32490;&#21644;&#31038;&#20132;&#23186;&#20307;&#12289;&#38646;&#21806;&#21644;&#30005;&#23376;&#21830;&#21153;&#12289;&#21355;&#26143;&#22270;&#20687;&#20197;&#21450;&#33322;&#36816;&#21644;&#29289;&#27969;&#65292;&#20197;&#28385;&#36275;&#21508;&#31181;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in data science, machine learning, and artificial intelligence, such as the emergence of large language models, are leading to an increasing demand for data that can be processed by such models. While data sources are application-specific, and it is impossible to produce an exhaustive list of such data sources, it seems that a comprehensive, rather than complete, list would still benefit data scientists and machine learning experts of all levels of seniority. The goal of this publication is to provide just such an (inevitably incomplete) list -- or compendium -- of data sources across multiple areas of applications, including finance and economics, legal (laws and regulations), life sciences (medicine and drug discovery), news sentiment and social media, retail and ecommerce, satellite imagery, and shipping and logistics, and sports.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#26469;&#32454;&#21270;&#31185;&#23398;&#20986;&#29256;&#29289;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#23398;&#20064;&#27010;&#29575;&#36923;&#36753;&#27169;&#22411;&#24182;&#20351;&#29992;&#21151;&#33021;&#26799;&#24230;&#25552;&#21319;&#21644;&#20154;&#31867;&#30693;&#35782;&#24341;&#23548;&#26469;&#35782;&#21035;&#20316;&#32773;&#36523;&#20221;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20154;&#31867;&#30693;&#35782;&#22312;&#20316;&#32773;&#36523;&#20221;&#39046;&#22495;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.05681</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#31185;&#23398;&#20986;&#29256;&#29289;&#30693;&#35782;&#22270;&#35889;&#30340;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based Refinement of Scientific Publication Knowledge Graphs. (arXiv:2309.05681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#26469;&#32454;&#21270;&#31185;&#23398;&#20986;&#29256;&#29289;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#23398;&#20064;&#27010;&#29575;&#36923;&#36753;&#27169;&#22411;&#24182;&#20351;&#29992;&#21151;&#33021;&#26799;&#24230;&#25552;&#21319;&#21644;&#20154;&#31867;&#30693;&#35782;&#24341;&#23548;&#26469;&#35782;&#21035;&#20316;&#32773;&#36523;&#20221;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20154;&#31867;&#30693;&#35782;&#22312;&#20316;&#32773;&#36523;&#20221;&#39046;&#22495;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#35782;&#21035;&#20316;&#32773;&#36523;&#20221;&#30340;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#30693;&#35782;&#22270;&#26500;&#24314;&#21644;&#32454;&#21270;&#30340;&#38382;&#39064;&#26469;&#32771;&#34385;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#22312;&#20154;&#31867;&#25351;&#23548;&#65288;&#22522;&#20110;&#30693;&#35782;&#30340;&#23398;&#20064;&#65289;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27010;&#29575;&#36923;&#36753;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21151;&#33021;&#26799;&#24230;&#25552;&#21319;&#23398;&#20064;&#20851;&#31995;&#22238;&#24402;&#26641;&#65292;&#24182;&#36755;&#20986;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#12290;&#20026;&#20102;&#24341;&#20837;&#20154;&#31867;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#19968;&#38454;&#23376;&#21477;&#30340;&#24418;&#24335;&#30340;&#24314;&#35758;&#27880;&#20837;&#21040;&#26641;&#20013;&#36827;&#34892;&#32454;&#21270;&#12290;&#25105;&#20204;&#22312;&#19971;&#20010;&#20316;&#32773;&#36523;&#20221;&#39046;&#22495;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#20154;&#31867;&#30693;&#35782;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of identifying authorship by posing it as a knowledge graph construction and refinement. To this effect, we model this problem as learning a probabilistic logic model in the presence of human guidance (knowledge-based learning). Specifically, we learn relational regression trees using functional gradient boosting that outputs explainable rules. To incorporate human knowledge, advice in the form of first-order clauses is injected to refine the trees. We demonstrate the usefulness of human knowledge both quantitatively and qualitatively in seven authorship domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#36235;&#21183;&#30340;&#27979;&#35797;&#26041;&#27861;&#35780;&#20272;&#20102;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#30340;&#24544;&#35802;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#36235;&#21183;&#27979;&#35797;&#26041;&#27861;&#65292;&#20174;&#23454;&#35777;&#32467;&#26524;&#26469;&#30475;&#65292;&#36825;&#20123;&#26032;&#27979;&#35797;&#26041;&#27861;&#22312;&#22270;&#20687;&#12289;&#33258;&#28982;&#35821;&#35328;&#21644;&#23433;&#20840;&#20219;&#21153;&#20013;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#35299;&#37322;&#27169;&#22411;&#30340;&#24544;&#35802;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.05679</link><description>&lt;p&gt;
&#22806;&#34920;&#20248;&#32654;&#20294;&#32570;&#20047;&#24544;&#35802;&#24230;&#65306;&#36890;&#36807;&#22522;&#20110;&#36235;&#21183;&#30340;&#27979;&#35797;&#29702;&#35299;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing. (arXiv:2309.05679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#36235;&#21183;&#30340;&#27979;&#35797;&#26041;&#27861;&#35780;&#20272;&#20102;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#30340;&#24544;&#35802;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#36235;&#21183;&#27979;&#35797;&#26041;&#27861;&#65292;&#20174;&#23454;&#35777;&#32467;&#26524;&#26469;&#30475;&#65292;&#36825;&#20123;&#26032;&#27979;&#35797;&#26041;&#27861;&#22312;&#22270;&#20687;&#12289;&#33258;&#28982;&#35821;&#35328;&#21644;&#23433;&#20840;&#20219;&#21153;&#20013;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#35299;&#37322;&#27169;&#22411;&#30340;&#24544;&#35802;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25104;&#23601;&#65292;&#20294;&#20154;&#20204;&#20063;&#23545;DL&#27169;&#22411;&#30340;&#20915;&#31574;&#24863;&#21040;&#25285;&#24551;&#65292;&#22240;&#20026;DL&#27169;&#22411;&#30340;&#39640;&#24230;&#38750;&#32447;&#24615;&#20351;&#24471;&#20915;&#31574;&#26497;&#20854;&#38590;&#20197;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#25915;&#20987;&#65288;&#22914;&#23545;&#25239;&#25915;&#20987;&#65289;&#24456;&#23481;&#26131;&#36827;&#34892;&#65292;&#20294;&#24456;&#38590;&#26816;&#27979;&#21644;&#35299;&#37322;&#65292;&#36825;&#23548;&#33268;&#20102;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#30340;&#30740;&#31350;&#28608;&#22686;&#65292;&#20197;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35299;&#37322;&#26041;&#27861;&#30340;&#24544;&#35802;&#24230;&#65292;&#24182;&#21457;&#29616;&#20256;&#32479;&#30340;&#24544;&#35802;&#24230;&#27979;&#35797;&#36935;&#21040;&#20102;&#38543;&#26426;&#20248;&#21183;&#38382;&#39064;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#25928;&#26524;&#26368;&#22909;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22797;&#26434;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#24544;&#35802;&#24230;&#27979;&#35797;&#65292;&#24182;&#20174;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#26032;&#30340;&#36235;&#21183;&#27979;&#35797;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#22270;&#20687;&#12289;&#33258;&#28982;&#35821;&#35328;&#21644;&#23433;&#20840;&#20219;&#21153;&#30340;&#27979;&#35797;&#26356;&#22909;&#22320;&#35780;&#20272;&#24544;&#35802;&#24230;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#35780;&#20272;&#31995;&#32479;&#65292;&#24182;&#35780;&#20272;&#20102;&#21313;&#31181;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While enjoying the great achievements brought by deep learning (DL), people are also worried about the decision made by DL models, since the high degree of non-linearity of DL models makes the decision extremely difficult to understand. Consequently, attacks such as adversarial attacks are easy to carry out, but difficult to detect and explain, which has led to a boom in the research on local explanation methods for explaining model decisions. In this paper, we evaluate the faithfulness of explanation methods and find that traditional tests on faithfulness encounter the random dominance problem, \ie, the random selection performs the best, especially for complex data. To further solve this problem, we propose three trend-based faithfulness tests and empirically demonstrate that the new trend tests can better assess faithfulness than traditional tests on image, natural language and security tasks. We implement the assessment system and evaluate ten popular explanation methods. Benefitin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#31354;&#38388;&#30340;&#20135;&#21697;&#27969;&#24418;&#30340;Gromov-Hausdorff&#36317;&#31163;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#36317;&#31163;&#26469;&#25628;&#32034;&#26368;&#20339;&#30340;&#28508;&#22312;&#20960;&#20309;&#12290;&#36825;&#20026;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.05678</link><description>&lt;p&gt;
&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#31354;&#38388;&#30340;&#20135;&#21697;&#27969;&#24418;&#30340;Gromov-Hausdorff&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Gromov-Hausdorff Distances for Comparing Product Manifolds of Model Spaces. (arXiv:2309.05678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#31354;&#38388;&#30340;&#20135;&#21697;&#27969;&#24418;&#30340;Gromov-Hausdorff&#36317;&#31163;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#36317;&#31163;&#26469;&#25628;&#32034;&#26368;&#20339;&#30340;&#28508;&#22312;&#20960;&#20309;&#12290;&#36825;&#20026;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24449;&#19982;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#40784;&#26469;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#30740;&#31350;&#20154;&#21592;&#24314;&#35758;&#20351;&#29992;&#20855;&#26377;&#24120;&#26354;&#29575;&#30340;&#21452;&#26354;&#21644;&#29699;&#38754;&#31354;&#38388;&#65292;&#25110;&#23427;&#20204;&#30340;&#32452;&#21512;&#65288;&#31216;&#20026;&#20135;&#21697;&#27969;&#24418;&#65289;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21407;&#21017;&#24615;&#30340;&#25216;&#26415;&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#28508;&#22312;&#20135;&#21697;&#27969;&#24418;&#29305;&#24449;&#65292;&#21363;&#27969;&#24418;&#32452;&#20214;&#30340;&#36873;&#25321;&#21644;&#32500;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20505;&#36873;&#28508;&#22312;&#20960;&#20309;&#20043;&#38388;&#30340;&#36317;&#31163;&#27010;&#24565;&#65292;&#20351;&#29992;&#24230;&#37327;&#20960;&#20309;&#20013;&#30340;Gromov-Hausdorff&#36317;&#31163;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#19968;&#20010;&#22270;&#25628;&#32034;&#31354;&#38388;&#65292;&#21033;&#29992;&#20272;&#35745;&#30340;Gromov-Hausdorff&#36317;&#31163;&#26469;&#25628;&#32034;&#26368;&#20339;&#30340;&#28508;&#22312;&#20960;&#20309;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#27169;&#22411;&#31354;&#38388;&#20043;&#38388;&#30340;Gromov-Hausdorff&#36317;&#31163;&#21450;&#20854;&#35745;&#31639;&#23454;&#29616;&#30340;&#31639;&#27861;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies propose enhancing machine learning models by aligning the geometric characteristics of the latent space with the underlying data structure. Instead of relying solely on Euclidean space, researchers have suggested using hyperbolic and spherical spaces with constant curvature, or their combinations (known as product manifolds), to improve model performance. However, there exists no principled technique to determine the best latent product manifold signature, which refers to the choice and dimensionality of manifold components. To address this, we introduce a novel notion of distance between candidate latent geometries using the Gromov-Hausdorff distance from metric geometry. We propose using a graph search space that uses the estimated Gromov-Hausdorff distances to search for the optimal latent geometry. In this work we focus on providing a description of an algorithm to compute the Gromov-Hausdorff distance between model spaces and its computational implementation.
&lt;/p&gt;</description></item><item><title>MultiCaM-Vis&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#22312;&#22823;&#37327;&#31867;&#21035;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#20013;&#35782;&#21035;&#20986;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#25506;&#32034;&#21644;&#26816;&#26597;&#31867;&#21035;&#32423;&#21035;&#30340;&#23454;&#20363;&#38169;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.05676</link><description>&lt;p&gt;
MultiCaM-Vis: &#20855;&#26377;&#22823;&#37327;&#31867;&#21035;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
MultiCaM-Vis: Visual Exploration of Multi-Classification Model with High Number of Classes. (arXiv:2309.05676v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05676
&lt;/p&gt;
&lt;p&gt;
MultiCaM-Vis&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#22312;&#22823;&#37327;&#31867;&#21035;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#20013;&#35782;&#21035;&#20986;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#25506;&#32034;&#21644;&#26816;&#26597;&#31867;&#21035;&#32423;&#21035;&#30340;&#23454;&#20363;&#38169;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#37327;&#31867;&#21035;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#25506;&#32034;&#26377;&#21161;&#20110;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#22312;&#23398;&#20064;&#38454;&#27573;&#35782;&#21035;&#20986;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#22914;&#23454;&#20363;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;&#20197;&#24448;&#30340;&#22823;&#22810;&#25968;&#21487;&#35270;&#21270;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#21482;&#38024;&#23545;&#23569;&#25968;&#31867;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;MultiCaM-Vis&#65292;&#23427;&#25552;&#20379;&#20102;&#24635;&#35272;+&#35814;&#32454;&#20449;&#24687;&#30340;&#24179;&#34892;&#22352;&#26631;&#35270;&#22270;&#21644;&#24358;&#22270;&#65292;&#29992;&#20110;&#25506;&#32034;&#21644;&#26816;&#26597;&#31867;&#21035;&#32423;&#21035;&#30340;&#23454;&#20363;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#39033;&#21021;&#27493;&#29992;&#25143;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#20849;&#26377;12&#21517;&#21442;&#19982;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual exploration of multi-classification models with large number of classes would help machine learning experts in identifying the root cause of a problem that occurs during learning phase such as miss-classification of instances. Most of the previous visual analytics solutions targeted only a few classes. In this paper, we present our interactive visual analytics tool, called MultiCaM-Vis, that provides \Emph{overview+detail} style parallel coordinate views and a Chord diagram for exploration and inspection of class-level miss-classification of instances. We also present results of a preliminary user study with 12 participants.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAPE&#30340;&#36866;&#24212;&#26679;&#26412;&#30340;&#23618;&#27425;&#33647;&#29289;&#39044;&#27979;&#32593;&#32476;&#65292;&#26088;&#22312;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#20013;&#22797;&#26434;&#22810;&#21457;&#30149;&#26465;&#20214;&#19979;&#30340;&#33647;&#29289;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#32039;&#20945;&#30340;&#20869;&#37096;&#30149;&#24739;&#23601;&#35786;&#20107;&#20214;&#20851;&#31995;&#32534;&#30721;&#22120;&#21644;&#32437;&#21521;&#30149;&#21382;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#20934;&#30830;&#30340;&#30149;&#24739;&#34920;&#31034;&#21644;&#32437;&#21521;&#24207;&#21015;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.05675</link><description>&lt;p&gt;
SHAPE&#65306;&#19968;&#31181;&#36866;&#24212;&#26679;&#26412;&#30340;&#23618;&#27425;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
SHAPE: A Sample-adaptive Hierarchical Prediction Network for Medication Recommendation. (arXiv:2309.05675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAPE&#30340;&#36866;&#24212;&#26679;&#26412;&#30340;&#23618;&#27425;&#33647;&#29289;&#39044;&#27979;&#32593;&#32476;&#65292;&#26088;&#22312;&#35299;&#20915;&#21307;&#30103;&#20445;&#20581;&#20013;&#22797;&#26434;&#22810;&#21457;&#30149;&#26465;&#20214;&#19979;&#30340;&#33647;&#29289;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#32039;&#20945;&#30340;&#20869;&#37096;&#30149;&#24739;&#23601;&#35786;&#20107;&#20214;&#20851;&#31995;&#32534;&#30721;&#22120;&#21644;&#32437;&#21521;&#30149;&#21382;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#20934;&#30830;&#30340;&#30149;&#24739;&#34920;&#31034;&#21644;&#32437;&#21521;&#24207;&#21015;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#38024;&#23545;&#22797;&#26434;&#30340;&#22810;&#21457;&#30149;&#26465;&#20214;&#36827;&#34892;&#26377;&#25928;&#30340;&#33647;&#29289;&#25512;&#33616;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#22522;&#20110;&#32437;&#21521;&#35760;&#24405;&#39044;&#27979;&#33647;&#29289;&#65292;&#36825;&#31181;&#26041;&#27861;&#20551;&#35774;&#23398;&#20064;&#32437;&#21521;&#24207;&#21015;&#25968;&#25454;&#30340;&#20449;&#24687;&#20256;&#36755;&#27169;&#24335;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#30149;&#24739;&#22312;&#23601;&#35786;&#26399;&#38388;&#30340;&#21307;&#30103;&#20107;&#20214;&#26159;&#26377;&#24207;&#30340;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#35270;&#20102;&#20197;&#19979;&#26465;&#20214;&#65306;1&#65289;&#24613;&#38656;&#19968;&#31181;&#26356;&#32039;&#20945;&#30340;&#20869;&#37096;&#30149;&#24739;&#23601;&#35786;&#20107;&#20214;&#20851;&#31995;&#32534;&#30721;&#22120;&#65307;2&#65289;&#23398;&#20064;&#30149;&#20154;&#21487;&#21464;&#32437;&#21521;&#24207;&#21015;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#31574;&#30053;&#26159;&#19981;&#21516;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#26679;&#26412;&#30340;&#23618;&#27425;&#33647;&#29289;&#39044;&#27979;&#32593;&#32476;&#65292;&#31216;&#20026;SHAPE&#65292;&#26469;&#35299;&#20915;&#33647;&#29289;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#19978;&#36848;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#20869;&#37096;&#30149;&#24739;&#23601;&#35786;&#20107;&#20214;&#20851;&#31995;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#33719;&#24471;&#23601;&#35786;&#32423;&#21035;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21457;&#23637;&#20102;&#19968;&#20010;&#32437;&#21521;&#30149;&#21382;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;&#24739;&#32773;&#30340;&#32437;&#21521;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively medication recommendation with complex multimorbidity conditions is a critical task in healthcare. Most existing works predicted medications based on longitudinal records, which assumed the information transmitted patterns of learning longitudinal sequence data are stable and intra-visit medical events are serialized. However, the following conditions may have been ignored: 1) A more compact encoder for intra-relationship in the intra-visit medical event is urgent; 2) Strategies for learning accurate representations of the variable longitudinal sequences of patients are different. In this paper, we proposed a novel Sample-adaptive Hierarchical medicAtion Prediction nEtwork, termed SHAPE, to tackle the above challenges in the medication recommendation task. Specifically, we design a compact intra-visit set encoder to encode the relationship in the medical event for obtaining visit-level representation and then develop an inter-visit longitudinal encoder to learn the patient-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"Circles"&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#20998;&#26512;&#24037;&#20855;&#65292;&#20801;&#35768;&#22312;&#19968;&#20010;&#35270;&#22270;&#20013;&#36827;&#34892;&#23545;&#20855;&#26377;1K&#31867;&#21035;&#30340;&#22810;&#20010;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#38388;&#27604;&#36739;&#12290;&#20351;&#29992;&#21516;&#24515;&#24452;&#21521;&#32447;&#24067;&#23616;&#26469;&#35299;&#20915;&#35270;&#35273;&#28151;&#20081;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05672</link><description>&lt;p&gt;
&#22280;&#22280;&#65306;&#23545;&#20855;&#26377;&#22823;&#37327;&#31867;&#21035;&#30340;&#22810;&#20998;&#31867;&#38382;&#39064;&#30340;&#27169;&#22411;&#38388;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Circles: Inter-Model Comparison of Multi-Classification Problems with High Number of Classes. (arXiv:2309.05672v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05672
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"Circles"&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#20998;&#26512;&#24037;&#20855;&#65292;&#20801;&#35768;&#22312;&#19968;&#20010;&#35270;&#22270;&#20013;&#36827;&#34892;&#23545;&#20855;&#26377;1K&#31867;&#21035;&#30340;&#22810;&#20010;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#38388;&#27604;&#36739;&#12290;&#20351;&#29992;&#21516;&#24515;&#24452;&#21521;&#32447;&#24067;&#23616;&#26469;&#35299;&#20915;&#35270;&#35273;&#28151;&#20081;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#29983;&#25104;&#22788;&#29702;&#21253;&#21547;&#25968;&#30334;&#31867;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#27604;&#22914;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#23545;&#39640;&#25968;&#37327;&#31867;&#21035;&#30340;&#20998;&#31867;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#21644;&#36825;&#20123;&#20998;&#31867;&#38382;&#39064;&#30340;&#27169;&#22411;&#38388;&#27604;&#36739;&#36825;&#20004;&#20010;&#39046;&#22495;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#65292;&#23613;&#31649;&#20998;&#31867;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#26469;&#35299;&#20915;&#20855;&#26377;&#38750;&#24120;&#22823;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#20998;&#26512;&#24037;&#20855;&#65292;&#31216;&#20026;"Circles"&#65292;&#23427;&#20801;&#35768;&#22312;&#19968;&#20010;&#35270;&#22270;&#20013;&#23545;1K&#31867;&#21035;&#30340;&#22810;&#20010;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#35270;&#35273;&#27169;&#22411;&#38388;&#27604;&#36739;&#12290;&#20026;&#20102;&#20943;&#23569;&#35270;&#35273;&#28151;&#20081;&#30340;&#26840;&#25163;&#38382;&#39064;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#21516;&#24515;&#24452;&#21521;&#32447;&#24067;&#23616;&#20316;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#38388;&#27604;&#36739;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#21407;&#22411;&#23637;&#31034;&#20102;9&#20010;&#20855;&#26377;1K&#31867;&#21035;&#30340;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in machine learning have motivated researchers to generate classification models dealing with hundreds of classes such as in the case of image datasets. However, visualization of classification models with high number of classes and inter-model comparison in such classification problems are two areas that have not received much attention in the literature, despite the ever-increasing use of classification models to address problems with very large class categories. In this paper, we present our interactive visual analytics tool, called Circles, that allows a visual inter-model comparison of numerous classification models with 1K classes in one view. To mitigate the tricky issue of visual clutter, we chose concentric a radial line layout for our inter-model comparison task. Our prototype shows the results of 9 models with 1K classes
&lt;/p&gt;</description></item><item><title>tSPM+&#31639;&#27861;&#26159;&#19968;&#31181;&#39640;&#24615;&#33021;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#27169;&#24335;&#20013;&#21152;&#20837;&#25345;&#32493;&#26102;&#38388;&#32500;&#24230;&#65292;&#25552;&#20379;&#20102;&#39640;&#36895;&#36816;&#34892;&#21644;&#20869;&#23384;&#28040;&#32791;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.05671</link><description>&lt;p&gt;
tSPM+&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20020;&#24202;&#25968;&#25454;&#20013;&#25366;&#25496;&#20256;&#36882;&#39034;&#24207;&#27169;&#24335;&#30340;&#39640;&#24615;&#33021;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
tSPM+; a high-performance algorithm for mining transitive sequential patterns from clinical data. (arXiv:2309.05671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05671
&lt;/p&gt;
&lt;p&gt;
tSPM+&#31639;&#27861;&#26159;&#19968;&#31181;&#39640;&#24615;&#33021;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#27169;&#24335;&#20013;&#21152;&#20837;&#25345;&#32493;&#26102;&#38388;&#32500;&#24230;&#65292;&#25552;&#20379;&#20102;&#39640;&#36895;&#36816;&#34892;&#21644;&#20869;&#23384;&#28040;&#32791;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#20174;&#24739;&#32773;&#37027;&#37324;&#25910;&#38598;&#21040;&#30340;&#22823;&#22411;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#20351;&#24471;&#20351;&#29992;&#19981;&#21516;&#30340;&#20998;&#26512;&#31639;&#27861;&#23545;&#22797;&#26434;&#30142;&#30149;&#36827;&#34892;&#35745;&#31639;&#21270;&#29305;&#24449;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#19968;&#31181;&#20174;&#22823;&#22411;&#20020;&#24202;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#27861;&#26159;&#23558;&#26102;&#38388;&#27169;&#24335;&#25366;&#25496;&#19982;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#25366;&#25496;&#36825;&#20123;&#26102;&#38388;&#27169;&#24335;&#26159;&#19968;&#39033;&#35745;&#31639;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#24182;&#19988;&#20250;&#23545;&#23384;&#20648;&#22120;&#20135;&#29983;&#24433;&#21709;&#12290;&#30446;&#21069;&#30340;&#31639;&#27861;&#65292;&#22914;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#65288;tSPM&#65289;&#31639;&#27861;&#65292;&#24050;&#32463;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#26377;&#20248;&#21270;&#30340;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;tSPM+&#31639;&#27861;&#65292;&#36825;&#26159;tSPM&#31639;&#27861;&#30340;&#19968;&#31181;&#39640;&#24615;&#33021;&#23454;&#29616;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#27169;&#24335;&#20013;&#28155;&#21152;&#25345;&#32493;&#26102;&#38388;&#32500;&#24230;&#65292;&#22686;&#21152;&#20102;&#19968;&#31181;&#26032;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;tSPM+&#31639;&#27861;&#25552;&#20379;&#20102;&#39640;&#36798;980&#20493;&#30340;&#21152;&#36895;&#21644;&#39640;&#36798;48&#20493;&#30340;&#20869;&#23384;&#20351;&#29992;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;R&#21253;&#30340;docker&#23481;&#22120;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#23454;&#39564;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing availability of large clinical datasets collected from patients can enable new avenues for computational characterization of complex diseases using different analytic algorithms. One of the promising new methods for extracting knowledge from large clinical datasets involves temporal pattern mining integrated with machine learning workflows. However, mining these temporal patterns is a computational intensive task and has memory repercussions. Current algorithms, such as the temporal sequence pattern mining (tSPM) algorithm, are already providing promising outcomes, but still leave room for optimization. In this paper, we present the tSPM+ algorithm, a high-performance implementation of the tSPM algorithm, which adds a new dimension by adding the duration to the temporal patterns. We show that the tSPM+ algorithm provides a speed up to factor 980 and a up to 48 fold improvement in memory consumption. Moreover, we present a docker container with an R-package, We also provi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#22522;&#20110;&#35270;&#35273;&#30340;&#22810;&#26679;&#21270;&#20844;&#22253;our&#25216;&#33021;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#31616;&#21333;&#30340;&#22870;&#21169;&#32780;&#19981;&#20351;&#29992;&#21442;&#32771;&#21160;&#20316;&#25968;&#25454;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#19981;&#21516;&#30340;&#20844;&#22253;our&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#22235;&#36275;&#26426;&#22120;&#20154;&#20013;&#12290;</title><link>http://arxiv.org/abs/2309.05665</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20844;&#22253;our&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robot Parkour Learning. (arXiv:2309.05665v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#22522;&#20110;&#35270;&#35273;&#30340;&#22810;&#26679;&#21270;&#20844;&#22253;our&#25216;&#33021;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#31616;&#21333;&#30340;&#22870;&#21169;&#32780;&#19981;&#20351;&#29992;&#21442;&#32771;&#21160;&#20316;&#25968;&#25454;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#19981;&#21516;&#30340;&#20844;&#22253;our&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#22235;&#36275;&#26426;&#22120;&#20154;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#22253;our&#26159;&#19968;&#20010;&#23545;&#20110;&#26426;&#22120;&#20154;&#26469;&#35828;&#38656;&#35201;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36805;&#36895;&#20811;&#26381;&#21508;&#31181;&#38556;&#30861;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#21160;&#29289;&#25968;&#25454;&#25110;&#22797;&#26434;&#30340;&#22870;&#21169;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#20294;&#30450;&#30446;&#30340;&#36816;&#21160;&#25216;&#33021;&#25110;&#22522;&#20110;&#35270;&#35273;&#30340;&#29305;&#27530;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#33258;&#20027;&#20844;&#22253;our&#38656;&#35201;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20110;&#35270;&#35273;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21487;&#25512;&#24191;&#25216;&#33021;&#65292;&#20197;&#24863;&#30693;&#21644;&#24212;&#23545;&#21508;&#31181;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#22870;&#21169;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#21442;&#32771;&#21160;&#20316;&#25968;&#25454;&#65292;&#26469;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#12289;&#22522;&#20110;&#35270;&#35273;&#30340;&#20844;&#22253;our&#25216;&#33021;&#30340;&#21333;&#19968;&#31471;&#21040;&#31471;&#31574;&#30053;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21463;&#21040;&#30452;&#25509;&#21327;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20844;&#22253;our&#25216;&#33021;&#65292;&#21253;&#25324;&#25856;&#29228;&#39640;&#38556;&#30861;&#29289;&#12289;&#36328;&#36234;&#22823;&#36317;&#31163;&#38388;&#38553;&#12289;&#29228;&#34892;&#20302;&#22721;&#22418;&#12289;&#31359;&#36234;&#29421;&#31364;&#32541;&#38553;&#21644;&#22868;&#36305;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25216;&#33021;&#25552;&#28860;&#25104;&#19968;&#20010;&#21333;&#19968;&#22522;&#20110;&#35270;&#35273;&#30340;&#20844;&#22253;our&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#65292;&#21033;&#29992;&#20854;&#33258;&#25105;&#20013;&#24515;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric 
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#23545;&#31216;&#20449;&#24687;&#29942;&#39048;&#26159;&#19968;&#31181;&#21516;&#26102;&#21387;&#32553;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20197;&#20445;&#30041;&#20449;&#24687;&#30340;&#32500;&#24230;&#32422;&#31616;&#25216;&#26415;&#65292;&#30456;&#36739;&#20110;&#36880;&#20010;&#21387;&#32553;&#21464;&#37327;&#65292;&#23427;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#36798;&#21040;&#30456;&#21516;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.05649</link><description>&lt;p&gt;
&#25968;&#25454;&#25928;&#29575;&#12289;&#32500;&#24230;&#32422;&#31616;&#21644;&#24191;&#20041;&#23545;&#31216;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck. (arXiv:2309.05649v1 [cs.IT] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05649
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#23545;&#31216;&#20449;&#24687;&#29942;&#39048;&#26159;&#19968;&#31181;&#21516;&#26102;&#21387;&#32553;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20197;&#20445;&#30041;&#20449;&#24687;&#30340;&#32500;&#24230;&#32422;&#31616;&#25216;&#26415;&#65292;&#30456;&#36739;&#20110;&#36880;&#20010;&#21387;&#32553;&#21464;&#37327;&#65292;&#23427;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#36798;&#21040;&#30456;&#21516;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#20449;&#24687;&#29942;&#39048;&#65288;SIB&#65289;&#26159;&#19968;&#31181;&#32500;&#24230;&#32422;&#31616;&#25216;&#26415;&#65292;&#23427;&#26159;&#26356;&#24120;&#35265;&#30340;&#20449;&#24687;&#29942;&#39048;&#30340;&#25193;&#23637;&#65292;&#21516;&#26102;&#21387;&#32553;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20197;&#20445;&#30041;&#23427;&#20204;&#30340;&#21387;&#32553;&#29256;&#26412;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#20041;&#23545;&#31216;&#20449;&#24687;&#29942;&#39048;&#65288;GSIB&#65289;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#21151;&#33021;&#24418;&#24335;&#30340;&#21516;&#26102;&#32422;&#31616;&#25104;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21516;&#26102;&#21387;&#32553;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#28041;&#21450;&#25439;&#22833;&#20989;&#25968;&#30340;&#32479;&#35745;&#27874;&#21160;&#30340;&#30028;&#38480;&#21644;&#22343;&#26041;&#26681;&#20272;&#35745;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#20856;&#22411;&#24773;&#20917;&#19979;&#65292;&#19982;&#36880;&#20010;&#21387;&#32553;&#21464;&#37327;&#30456;&#27604;&#65292;&#21516;&#26102;&#30340;GSIB&#21387;&#32553;&#22312;&#36798;&#21040;&#30456;&#21516;&#35823;&#24046;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#21407;&#21017;&#30340;&#20363;&#23376;&#65292;&#21363;&#21516;&#26102;&#21387;&#32553;&#27604;&#29420;&#31435;&#21387;&#32553;&#36755;&#20837;&#21464;&#37327;&#26356;&#20855;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Symmetric Information Bottleneck (SIB), an extension of the more familiar Information Bottleneck, is a dimensionality reduction technique that simultaneously compresses two random variables to preserve information between their compressed versions. We introduce the Generalized Symmetric Information Bottleneck (GSIB), which explores different functional forms of the cost of such simultaneous reduction. We then explore the dataset size requirements of such simultaneous compression. We do this by deriving bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions. We show that, in typical situations, the simultaneous GSIB compression requires qualitatively less data to achieve the same errors compared to compressing variables one at a time. We suggest that this is an example of a more general principle that simultaneous compression is more data efficient than independent compression of each of the input variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#21333;&#20010;&#21644;&#32806;&#21512;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#35760;&#24518;&#38459;&#24615;&#65292;&#24182;&#21457;&#29616;&#26368;&#22823;&#21270;&#35760;&#24518;&#38459;&#24615;&#21487;&#20197;&#25552;&#39640;&#20004;&#20010;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#32416;&#32544;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#37327;&#23376;&#30456;&#20851;&#24615;&#19982;&#35760;&#24518;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#19968;&#21457;&#29616;&#22686;&#24378;&#20102;&#23558;&#37327;&#23376;&#24518;&#38459;&#22120;&#24212;&#29992;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#37327;&#23376;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05062</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#26368;&#22823;&#21270;&#21333;&#20010;&#21644;&#32806;&#21512;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#35760;&#24518;&#38459;&#24615;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for maximizing the memristivity of single and coupled quantum memristors. (arXiv:2309.05062v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#21333;&#20010;&#21644;&#32806;&#21512;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#35760;&#24518;&#38459;&#24615;&#65292;&#24182;&#21457;&#29616;&#26368;&#22823;&#21270;&#35760;&#24518;&#38459;&#24615;&#21487;&#20197;&#25552;&#39640;&#20004;&#20010;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#32416;&#32544;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#37327;&#23376;&#30456;&#20851;&#24615;&#19982;&#35760;&#24518;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#19968;&#21457;&#29616;&#22686;&#24378;&#20102;&#23558;&#37327;&#23376;&#24518;&#38459;&#22120;&#24212;&#29992;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#37327;&#23376;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#34920;&#24449;&#21333;&#20010;&#21644;&#32806;&#21512;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#35760;&#24518;&#38459;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#21270;&#35760;&#24518;&#38459;&#24615;&#20250;&#23548;&#33268;&#20004;&#20010;&#37327;&#23376;&#24518;&#38459;&#22120;&#30340;&#32416;&#32544;&#31243;&#24230;&#36739;&#22823;&#65292;&#25581;&#31034;&#20102;&#37327;&#23376;&#30456;&#20851;&#24615;&#19982;&#35760;&#24518;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22686;&#24378;&#20102;&#23558;&#37327;&#23376;&#24518;&#38459;&#22120;&#20316;&#20026;&#31070;&#32463;&#24418;&#24577;&#30340;&#37327;&#23376;&#35745;&#31639;&#30340;&#20851;&#38190;&#32452;&#20214;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose machine learning (ML) methods to characterize the memristive properties of single and coupled quantum memristors. We show that maximizing the memristivity leads to large values in the degree of entanglement of two quantum memristors, unveiling the close relationship between quantum correlations and memory. Our results strengthen the possibility of using quantum memristors as key components of neuromorphic quantum computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;KDDT&#65292;&#29992;&#20110;&#21015;&#36710;&#25511;&#21046;&#21644;&#31649;&#29702;&#31995;&#32479;&#65288;TCMS&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;KDDT&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20998;&#21035;&#25552;&#21462;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#39034;&#24207;&#29305;&#24449;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#20016;&#23500;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;KDDT&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#20998;&#25968;0.931&#21644;0.91&#12290;</title><link>http://arxiv.org/abs/2309.04616</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation-Empowered Digital Twin for Anomaly Detection. (arXiv:2309.04616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;KDDT&#65292;&#29992;&#20110;&#21015;&#36710;&#25511;&#21046;&#21644;&#31649;&#29702;&#31995;&#32479;&#65288;TCMS&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;KDDT&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20998;&#21035;&#25552;&#21462;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#39034;&#24207;&#29305;&#24449;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#20016;&#23500;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;KDDT&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#20998;&#25968;0.931&#21644;0.91&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#31995;&#32479;&#65292;&#22312;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#20013;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#22914;&#21015;&#36710;&#25511;&#21046;&#21644;&#31649;&#29702;&#31995;&#32479;&#65288;TCMS&#65289;&#12290;&#20316;&#20026;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#65292;&#30830;&#20445;&#20854;&#22312;&#25805;&#20316;&#36807;&#31243;&#20013;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25968;&#23383;&#23402;&#29983;&#65288;DTs&#65289;&#30001;&#20110;&#20855;&#26377;&#36816;&#34892;&#26102;&#30417;&#25511;&#21644;&#35686;&#21578;&#12289;&#24322;&#24120;&#39044;&#27979;&#21644;&#26816;&#27979;&#31561;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;TCMS&#20013;&#26500;&#24314;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#38656;&#35201;&#20805;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#25552;&#21462;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#26102;&#38388;&#39034;&#24207;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KDDT&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;TCMS&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;KDDT&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#20998;&#21035;&#25552;&#21462;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#39034;&#24207;&#29305;&#24449;&#12290;&#20026;&#20102;&#22686;&#21152;&#25968;&#25454;&#37327;&#65292;KDDT&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#39046;&#22495;&#22806;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#24037;&#19994;&#21512;&#20316;&#20249;&#20276;&#38463;&#23572;&#26031;&#36890;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;KDDT&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#33719;&#24471;&#20102;0.931&#21644;0.91&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming ubiquitous in critical infrastructures. As safety-critical systems, ensuring their dependability during operation is crucial. Digital twins (DTs) have been increasingly studied for this purpose owing to their capability of runtime monitoring and warning, prediction and detection of anomalies, etc. However, constructing a DT for anomaly detection in TCMS necessitates sufficient training data and extracting both chronological and context features with high quality. Hence, in this paper, we propose a novel method named KDDT for TCMS anomaly detection. KDDT harnesses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To enrich data volume, KDDT benefits from out-of-domain data with knowledge distillation (KD). We evaluated KDDT with two datasets from our industry partner Alstom and obtained the F1 scores of 0.931 and 0.91
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#29289;&#29702;&#20449;&#24687;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#21644;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#26469;&#33719;&#21462;&#26368;&#36866;&#24403;&#30340;&#21453;&#23545;&#35282;&#39033;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2309.04434</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26368;&#20248;&#21453;&#23545;&#35282;&#37327;&#23376;&#35745;&#31639;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation. (arXiv:2309.04434v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#29289;&#29702;&#20449;&#24687;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#21644;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#26469;&#33719;&#21462;&#26368;&#36866;&#24403;&#30340;&#21453;&#23545;&#35282;&#39033;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#20248;&#21183;&#26469;&#35299;&#20915;&#30001;$N_{Q}$&#27604;&#29305;&#31995;&#32479;&#32452;&#25104;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#30340;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#31934;&#30830;&#22320;&#35299;&#20915;&#37327;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#29289;&#29702;&#21487;&#35266;&#27979;&#37327;&#30340;&#26102;&#38388;&#28436;&#21270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#24517;&#35201;&#30340;&#29289;&#29702;&#20449;&#24687;&#23884;&#20837;&#21040;&#24213;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23545;&#25152;&#26377;&#29289;&#29702;&#21487;&#35266;&#27979;&#37327;&#26045;&#21152;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#65292;&#20445;&#35777;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26368;&#36866;&#24403;&#21453;&#23545;&#35282;&#39033;&#30340;&#33719;&#21462;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#36873;&#25321;&#26469;&#35299;&#20915;CD&#39537;&#21160;&#38382;&#39064;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel methodology that leverages the strength of Physics-Informed Neural Networks (PINNs) to address the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits. The primary objective is to utilize physics-inspired deep learning techniques to accurately solve the time evolution of the different physical observables within the quantum system. To accomplish this objective, we embed the necessary physical information into an underlying neural network to effectively tackle the problem. In particular, we impose the hermiticity condition on all physical observables and make use of the principle of least action, guaranteeing the acquisition of the most appropriate counterdiabatic terms based on the underlying physics. The proposed approach offers a dependable alternative to address the CD driving problem, free from the constraints typically encountered in previous methodologies relying on classical numerical approximations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04296</link><description>&lt;p&gt;
&#22312;COVID-19&#26399;&#38388;&#23548;&#33322;&#19981;&#22312;&#20998;&#24067;&#33539;&#22260;&#20869;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65306;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility. (arXiv:2309.04296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#25968;&#25454;&#20998;&#24067;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#38750;&#20998;&#24067;&#26399;&#38388;&#26102;&#65292;&#22914;COVID-19&#30340;&#23553;&#38145;&#26399;&#65292;&#25968;&#25454;&#20998;&#24067;&#19982;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#35265;&#30340;&#26126;&#26174;&#20559;&#31163;&#12290;&#26412;&#25991;&#37319;&#29992;&#21452;&#37325;&#31574;&#30053;&#65306;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#26356;&#26032;&#27169;&#22411;&#30340;&#26032;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#22312;&#24314;&#31569;&#29289;&#22806;&#37096;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#34892;&#20154;&#35745;&#25968;&#22120;&#25910;&#38598;&#30340;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#12290;&#19982;&#22312;&#32447;&#23398;&#20064;&#30456;&#27604;&#65292;&#21518;&#32773;&#24120;&#24120;&#20250;&#36973;&#21463;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#26032;&#33719;&#24471;&#30340;&#30693;&#35782;&#24120;&#24120;&#20250;&#25273;&#21435;&#20808;&#21069;&#30340;&#20449;&#24687;&#65292;&#25345;&#32493;&#23398;&#20064;&#21017;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#20307;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23558;FSNet&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#22696;&#23572;&#26412;&#24066;13&#20010;&#24314;&#31569;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20998;&#21106;&#26041;&#27861;MMSFormer&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MCubeS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.04001</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#29992;&#20110;&#26448;&#26009;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multimodal Transformer for Material Segmentation. (arXiv:2309.04001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20998;&#21106;&#26041;&#27861;MMSFormer&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MCubeS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22810;&#27169;&#24577;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#20010;&#27169;&#24577;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65306;RGB&#12289;&#32447;&#24615;&#20559;&#25391;&#35282;&#65288;AoLP&#65289;&#12289;&#32447;&#24615;&#20559;&#25391;&#24230;&#65288;DoLP&#65289;&#21644;&#36817;&#32418;&#22806;&#65288;NIR&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#20998;&#21106;&#21464;&#25442;&#22120;&#65288;MMSFormer&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#22810;&#27169;&#24577;&#26448;&#26009;&#20998;&#21106;&#12290;MMSFormer&#22312;&#22810;&#27169;&#24577;&#26448;&#26009;&#20998;&#21106;&#65288;MCubeS&#65289;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;52.05&#65285;&#30340;mIoU&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#30782;&#30707;&#65288;+10.4&#65285;&#65289;&#21644;&#20154;&#31867;&#65288;+9.1&#65285;&#65289;&#31867;&#19978;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#34701;&#21512;&#22359;&#20013;&#30340;&#19981;&#21516;&#27169;&#22359;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes. Ablation studies show that different modules in the fusion block are crucial for
&lt;/p&gt;</description></item><item><title>ImageBind-LLM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#32852;&#21512;&#23884;&#20837;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03905</link><description>&lt;p&gt;
ImageBind-LLM: &#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03905
&lt;/p&gt;
&lt;p&gt;
ImageBind-LLM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#32852;&#21512;&#23884;&#20837;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;ImageBind&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#21644;&#22270;&#20687;&#25351;&#20196;&#35843;&#20248;&#26041;&#38754;&#65292;&#19982;&#27492;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;ImageBind-LLM&#21487;&#20197;&#21709;&#24212;&#22810;&#27169;&#24577;&#26465;&#20214;&#65292;&#21253;&#25324;&#38899;&#39057;&#12289;3D&#28857;&#20113;&#12289;&#35270;&#39057;&#20197;&#21450;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#31639;&#26415;&#65292;&#21482;&#38656;&#36827;&#34892;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21487;&#23398;&#20064;&#30340;Bind&#32593;&#32476;&#26469;&#23545;&#40784;LLaMA&#21644;ImageBind&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;Bind&#32593;&#32476;&#36716;&#25442;&#30340;&#22270;&#20687;&#29305;&#24449;&#34987;&#28155;&#21152;&#21040;LLaMA&#30340;&#25152;&#26377;&#23618;&#30340;&#21333;&#35789;&#26631;&#35760;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#26080;&#27880;&#24847;&#21147;&#21644;&#38646;&#21021;&#22987;&#21270;&#30340;&#38376;&#25511;&#26426;&#21046;&#36880;&#27493;&#27880;&#20837;&#35270;&#35273;&#25351;&#20196;&#12290;&#22312;ImageBind&#30340;&#32852;&#21512;&#23884;&#20837;&#30340;&#24110;&#21161;&#19979;&#65292;&#31616;&#21333;&#30340;&#22270;&#20687;-&#25991;&#26412;&#35757;&#32451;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22810;&#27169;&#24577;&#36755;&#20837;&#34987;&#36865;&#20837;&#30456;&#24212;&#30340;ImageBind&#32534;&#30721;&#22120;&#65292;&#24182;&#34987;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03241</link><description>&lt;p&gt;
GPT&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25191;&#34892;&#31639;&#26415;&#36816;&#31639;&#65292;&#29305;&#21035;&#26159;&#36229;&#36807;8&#20301;&#25968;&#23383;&#30340;&#20056;&#27861;&#65292;&#20197;&#21450;&#28041;&#21450;&#23567;&#25968;&#21644;&#20998;&#25968;&#30340;&#36816;&#31639;&#12290;&#26412;&#25991;&#26088;&#22312;&#25361;&#25112;&#36825;&#31181;&#35823;&#35299;&#12290;&#36890;&#36807;&#20805;&#20998;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19968;&#20010;&#25317;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20197;&#36817;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#32780;&#19988;&#27809;&#26377;&#25968;&#25454;&#27844;&#38706;&#65292;&#26174;&#33879;&#36229;&#36807;&#20102;GPT-4&#65288;&#20854;&#22810;&#20301;&#25968;&#20056;&#27861;&#20934;&#30830;&#29575;&#20165;&#20026;4.3%&#65289;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;MathGLM&#65292;&#23427;&#26159;&#36890;&#36807;&#22312;&#21253;&#21547;&#20102;&#25991;&#26412;&#25551;&#36848;&#30340;&#38468;&#21152;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#20174;GLM-10B&#24494;&#35843;&#32780;&#25104;&#30340;&#65292;&#23427;&#22312;&#19968;&#20010;&#21253;&#21547;5000&#20010;&#26679;&#26412;&#30340;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#27979;&#35797;&#38598;&#19978;&#30340;&#34920;&#29616;&#19982;GPT-4&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of &gt;8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#26631;&#35760;&#19981;&#36275;&#12289;POI&#38388;&#26102;&#31354;&#20381;&#36182;&#24615;&#22797;&#26434;&#21644;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30456;&#20851;&#24615;&#22810;&#26679;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03239</link><description>&lt;p&gt;
POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference. (arXiv:2309.03239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#26631;&#35760;&#19981;&#36275;&#12289;POI&#38388;&#26102;&#31354;&#20381;&#36182;&#24615;&#22797;&#26434;&#21644;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30456;&#20851;&#24615;&#22810;&#26679;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#33719;&#21462;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#30340;&#20154;&#32676;&#27969;&#37327;&#23545;&#20110;&#26377;&#25928;&#30340;&#20132;&#36890;&#31649;&#29702;&#12289;&#20844;&#20849;&#26381;&#21153;&#21644;&#22478;&#24066;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22914;&#27492;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#22478;&#24066;&#24863;&#30693;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#20197;&#30417;&#27979;&#27599;&#20010;POI&#30340;&#20154;&#32676;&#27969;&#21160;&#12290;&#36825;&#20351;&#24471;&#20174;&#20302;&#36136;&#37327;&#25968;&#25454;&#20013;&#25512;&#26029;&#20934;&#30830;&#30340;&#20154;&#32676;&#27969;&#37327;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#19968;&#22797;&#26434;&#24615;&#20027;&#35201;&#30001;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#24341;&#36215;&#65306;1&#65289;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#32597;&#35265;&#24615;&#65307;2&#65289;POI&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65307;3&#65289;&#31934;&#30830;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30340;&#20247;&#22810;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#20154;&#32676;&#27969;&#25512;&#26029;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#25968;&#25454;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;model&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#26500;&#24314;&#19968;&#20010;&#31354;&#38388;&#22270;&#24320;&#22987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad correlations between precise crowd flow and GPS reports}.  To address these challenges, we recast the crowd flow inference problem as a self-supervised attributed graph representation learning task and introduce a novel \underline{C}ontrastive \underline{S}elf-learning framework for \underline{S}patio-\underline{T}emporal data (\model). Our approach initiates with the construction of a spati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.03224</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#20381;&#28982;&#33021;&#33719;&#30410;&#65306;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03224
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32972;&#26223;&#23398;&#20064;&#33021;&#21147;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36807;&#31243;&#30417;&#30563;&#65292;&#23558;PLMs&#24212;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#36890;&#24120;&#26080;&#27861;&#29983;&#25104;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#21644;&#26368;&#32456;&#31572;&#26696;&#65292;&#21363;&#20351;&#35299;&#20915;&#26041;&#26696;&#27010;&#29575;&#24456;&#39640;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#24494;&#35843;&#30340;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#21644;&#36731;&#37327;&#32423;&#33021;&#37327;&#20989;&#25968;&#20026;LLMs&#36171;&#20104;&#21363;&#26102;&#21453;&#24212;&#21644;&#31934;&#32454;&#25512;&#29702;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#24494;&#35843;&#30340;LLMs&#37325;&#26032;&#23450;&#20041;&#20026;&#22522;&#20110;&#27531;&#24046;&#30340;&#33021;&#37327;&#27169;&#22411;&#65288;Residual-EBM&#65289;&#65292;&#24182;&#24212;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#26469;&#20272;&#35745;&#33021;&#37327;&#20989;&#25968;&#30340;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#33021;&#37327;&#20989;&#25968;&#30340;MCTS&#20316;&#20026;&#36335;&#24452;&#39564;&#35777;&#22120;&#26469;&#25628;&#32034;&#36755;&#20986;&#31354;&#38388;&#24182;&#35780;&#20272;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit impressive language understanding and in-context learning abilities including natural language processing (NLP) tasks and challenging mathematical reasoning. However, due to the lack of process-supervision, applying PLMs to mathematical reasoning tasks often fail to generate correct reasoning steps and final answer even though solutions have high probabilities. To unleash the mathematical reasoning of finetuned-LLMs without any further fineutuning steps, we propose a method to endow LLMs with immediate reaction and delicate reasoning system via Monte Carlo Tree Search(MCTS) and a light energy function to rank the decision steps. In particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy Model~(Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function . Then we use MCTS with energy function as path verifier to search the output space and evaluating the reasoning path. Through extensive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#25191;&#34892;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;softmax&#20989;&#25968;&#24212;&#29992;&#20110;&#38463;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#23545;&#35937;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#26102;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#25193;&#23637;&#21040;&#21333;&#20301;&#31435;&#26041;&#20307;&#19978;&#65292;&#20174;&#32780;&#22312;&#26377;&#30028;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.02530</link><description>&lt;p&gt;
&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Diffusion on the Probability Simplex. (arXiv:2309.02530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#25191;&#34892;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;softmax&#20989;&#25968;&#24212;&#29992;&#20110;&#38463;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#23545;&#35937;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#26102;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#25193;&#23637;&#21040;&#21333;&#20301;&#31435;&#26041;&#20307;&#19978;&#65292;&#20174;&#32780;&#22312;&#26377;&#30028;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#36870;&#36716;&#25968;&#25454;&#20998;&#24067;&#30340;&#36880;&#28176;&#22122;&#22768;&#21270;&#26469;&#21019;&#24314;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#30340;&#22122;&#22768;&#21270;&#36807;&#31243;&#19982;&#31163;&#25955;&#25968;&#25454;&#20043;&#38388;&#30340;&#26399;&#26395;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#23545;&#35937;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#25191;&#34892;&#25193;&#25955;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#27010;&#29575;&#21333;&#32431;&#24418;&#33258;&#28982;&#22320;&#21019;&#24314;&#20102;&#19968;&#31181;&#35299;&#37322;&#65292;&#20854;&#20013;&#28857;&#23545;&#24212;&#20110;&#20998;&#31867;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23545;&#38463;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#20043;&#38388;&#36827;&#34892;softmax&#20989;&#25968;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#21253;&#25324;&#23545;&#21333;&#20301;&#31435;&#26041;&#20307;&#30340;&#25193;&#25955;&#65292;&#36825;&#23545;&#20110;&#26377;&#30028;&#22270;&#20687;&#29983;&#25104;&#24212;&#29992;&#20855;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models learn to reverse the progressive noising of a data distribution to create a generative model. However, the desired continuous nature of the noising process can be at odds with discrete data. To deal with this tension between continuous and discrete objects, we propose a method of performing diffusion on the probability simplex. Using the probability simplex naturally creates an interpretation where points correspond to categorical probability distributions. Our method uses the softmax function applied to an Ornstein-Unlenbeck Process, a well-known stochastic differential equation. We find that our methodology also naturally extends to include diffusion on the unit cube which has applications for bounded image generation.
&lt;/p&gt;</description></item><item><title>MedShapeNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#23545;&#20110;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.16139</link><description>&lt;p&gt;
MedShapeNet - &#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22823;&#35268;&#27169;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16139
&lt;/p&gt;
&lt;p&gt;
MedShapeNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#23545;&#20110;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MedShapeNet&#65292;&#19968;&#20010;&#21253;&#21547;&#20102;&#35299;&#21078;&#24418;&#29366;&#65288;&#22914;&#39592;&#39612;&#12289;&#22120;&#23448;&#12289;&#34880;&#31649;&#65289;&#21644;&#19977;&#32500;&#25163;&#26415;&#22120;&#26800;&#27169;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#20043;&#21069;&#65292;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#35777;&#26126;&#20102;&#24418;&#29366;&#24120;&#34987;&#29992;&#26469;&#25551;&#36848;&#21307;&#23398;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#20307;&#32032;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#24418;&#29366;&#65288;&#21253;&#25324;&#20307;&#32032;&#21344;&#25454;&#32593;&#26684;&#12289;&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#38544;&#24335;&#34920;&#38754;&#27169;&#22411;&#65289;&#26159;&#19977;&#32500;&#25968;&#25454;&#30340;&#39318;&#36873;&#34920;&#31034;&#26041;&#27861;&#65292;&#36825;&#19968;&#28857;&#21487;&#20197;&#20174;&#22823;&#37327;&#20851;&#20110;&#24418;&#29366;&#30340;&#25991;&#31456;&#21450;&#22312;&#39030;&#32423;&#35745;&#31639;&#26426;&#35270;&#35273;&#20250;&#35758;&#65288;&#22914;IEEE/CVF&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#27169;&#24335;&#35782;&#21035;&#20250;&#35758;&#65288;CVPR&#65289;&#65289;&#20013;&#35265;&#21040;&#65292;&#21516;&#26102;ShapeNet&#65288;&#32422;51300&#20010;&#27169;&#22411;&#65289;&#21644;&#26222;&#26519;&#26031;&#39039;ModelNet&#65288;127,915&#20010;&#27169;&#22411;&#65289;&#30340;&#27969;&#34892;&#24230;&#20063;&#22312;&#19981;&#26029;&#22686;&#21152;&#12290;MedShapeNet&#30340;&#21019;&#24314;&#26159;&#20026;&#20102;&#20316;&#20026;&#36825;&#20123;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to f
&lt;/p&gt;</description></item><item><title>LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.15930</link><description>&lt;p&gt;
LLaSM: &#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15930
&lt;/p&gt;
&lt;p&gt;
LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35270;&#35273;-&#35821;&#35328;&#22810;&#27169;&#24577;&#27169;&#22411;&#19978;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#26469;&#36981;&#24490;&#35270;&#35273;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#35821;&#38899;&#20063;&#26159;&#20154;&#31867;&#19982;&#19990;&#30028;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#21161;&#25163;&#26469;&#35828;&#65292;&#33021;&#22815;&#36981;&#24490;&#22810;&#27169;&#24577;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65288;LLaSM&#65289;&#12290;LLaSM&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#38899;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#33021;&#22815;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;LLaSM&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;&#20026;&#20102;&#25903;&#25345;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#38899;&#25351;&#20196;&#25968;&#25454;&#38598;LLaSM-Audio-Instructions&#12290;&#20195;&#30721;&#21644;&#28436;&#31034;&#21487;&#22312;https://github.com/LinkSoul-AI/LLaSM&#21644;ht&#19978;&#26597;&#30475;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and ht
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Johnson-Lindenstrauss&#24341;&#29702;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#30340;&#36716;&#25442;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13991</link><description>&lt;p&gt;
JL-&#24341;&#29702;&#25512;&#23548;&#30340;&#29992;&#20110;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#30340;&#26368;&#20248;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
JL-lemma derived Optimal Projections for Discriminative Dictionary Learning. (arXiv:2308.13991v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Johnson-Lindenstrauss&#24341;&#29702;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#30340;&#36716;&#25442;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20811;&#26381;&#22312;&#20855;&#26377;&#22823;&#37327;&#31867;&#21035;&#30340;&#22823;&#32500;&#24230;&#25968;&#25454;&#20998;&#31867;&#20013;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#21033;&#29992;Johnson-Lindenstrauss(JL)&#24341;&#29702;&#65292;&#22312;&#19968;&#20010;&#36716;&#25442;&#31354;&#38388;&#20013;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#65292;&#21487;&#20197;&#22312;&#20854;&#20013;&#23398;&#20064;&#29992;&#20110;&#20449;&#21495;&#20998;&#31867;&#30340;&#21028;&#21035;&#24335;&#23383;&#20856;&#12290;&#19982;&#36890;&#24120;&#20351;&#29992;JL&#36827;&#34892;&#38477;&#32500;&#30340;&#38543;&#26426;&#25237;&#24433;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;Modified Supervised PC Analysis (M-SPCA)&#25512;&#23548;&#24471;&#20986;&#30340;&#25237;&#24433;&#36716;&#25442;&#30697;&#38453;&#65292;&#20854;&#32500;&#25968;&#36981;&#24490;JL&#30340;&#35268;&#23450;&#12290;JLSPCADL&#25552;&#20379;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25512;&#26029;&#36866;&#24403;&#30340;&#22833;&#30495;&#27700;&#24179;&#21644;&#30456;&#24212;&#30340;&#23383;&#20856;&#21407;&#23376;&#30340;&#36866;&#24403;&#25551;&#36848;&#38271;&#24230;(SDL)&#65292;&#20197;&#25512;&#23548;&#20986;&#19968;&#20010;&#26368;&#20248;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#30340;&#23383;&#20856;&#21407;&#23376;&#30340;SDL&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#38477;&#32500;&#30340;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#20174;M-SPCA&#20013;&#19968;&#27493;&#24471;&#20986;&#30340;&#25237;&#24433;&#36716;&#25442;&#30697;&#38453;&#25552;&#20379;&#20102;&#26368;&#22823;&#30340;&#29305;&#24449;-&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To overcome difficulties in classifying large dimensionality data with a large number of classes, we propose a novel approach called JLSPCADL. This paper uses the Johnson-Lindenstrauss (JL) Lemma to select the dimensionality of a transformed space in which a discriminative dictionary can be learned for signal classification. Rather than reducing dimensionality via random projections, as is often done with JL, we use a projection transformation matrix derived from Modified Supervised PC Analysis (M-SPCA) with the JL-prescribed dimension.  JLSPCADL provides a heuristic to deduce suitable distortion levels and the corresponding Suitable Description Length (SDL) of dictionary atoms to derive an optimal feature space and thus the SDL of dictionary atoms for better classification. Unlike state-of-the-art dimensionality reduction-based dictionary learning methods, a projection transformation matrix derived in a single step from M-SPCA provides maximum feature-label consistency of the transfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#30340;&#20998;&#23376;&#29983;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#21464;&#24418;&#29366;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;ShapeMol&#25104;&#21151;&#29983;&#25104;&#20102;&#26032;&#39062;&#12289;&#22810;&#26679;&#19988;&#31867;&#20284;&#32473;&#23450;&#24418;&#29366;&#26465;&#20214;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2308.11890</link><description>&lt;p&gt;
&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#24418;&#29366;&#30340;3D&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models. (arXiv:2308.11890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#30340;&#20998;&#23376;&#29983;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#21464;&#24418;&#29366;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;ShapeMol&#25104;&#21151;&#29983;&#25104;&#20102;&#26032;&#39062;&#12289;&#22810;&#26679;&#19988;&#31867;&#20284;&#32473;&#23450;&#24418;&#29366;&#26465;&#20214;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37197;&#20307;&#22522;&#33647;&#29289;&#35774;&#35745;&#26088;&#22312;&#35782;&#21035;&#19982;&#24050;&#30693;&#27963;&#24615;&#20998;&#23376;&#24418;&#29366;&#30456;&#20284;&#30340;&#26032;&#22411;&#33647;&#29289;&#20505;&#36873;&#29289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#30340;&#20998;&#23376;&#29983;&#25104;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#20998;&#23376;&#30340;&#24418;&#29366;&#26465;&#20214;&#19979;&#29983;&#25104;3D&#20998;&#23376;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31561;&#21464;&#24418;&#29366;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;ShapeMol&#12290;ShapeMol&#30001;&#19968;&#20010;&#31561;&#21464;&#24418;&#29366;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;&#36825;&#20123;&#32534;&#30721;&#29983;&#25104;3D&#20998;&#23376;&#30340;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#32452;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ShapeMol&#33021;&#22815;&#29983;&#25104;&#26032;&#39062;&#12289;&#22810;&#26679;&#19988;&#31867;&#20284;&#32473;&#23450;&#24418;&#29366;&#26465;&#20214;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;ShapeMol&#22312;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;3D&#24418;&#29366;&#24182;&#19982;&#34507;&#30333;&#38774;&#28857;&#32467;&#21512;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#25945;&#24072;&#30340;&#26041;&#27861;&#65288;TA&#65289;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09544</link><description>&lt;p&gt;
&#36866;&#24212;&#24744;&#30340;&#25945;&#24072;: &#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning. (arXiv:2308.09544v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#25945;&#24072;&#30340;&#26041;&#27861;&#65288;TA&#65289;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20316;&#20026;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#26088;&#22312;&#38450;&#27490;&#36951;&#24536;&#12290;KD&#26041;&#27861;&#22312;CIL&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#22312;&#27809;&#26377;&#35775;&#38382;&#20808;&#21069;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24448;&#24448;&#24456;&#38590;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#36825;&#20010;&#38382;&#39064;&#28304;&#20110;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#26102;&#25945;&#24072;&#32593;&#32476;&#20013;&#30340;&#26174;&#33879;&#34920;&#31034;&#36716;&#25442;&#12290;&#36825;&#23548;&#33268;KD&#25439;&#22833;&#25104;&#20998;&#20013;&#20986;&#29616;&#36739;&#22823;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#23548;&#33268;CIL&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#21463;&#26368;&#36817;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25945;&#24072;&#36866;&#24212;&#65288;TA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#26356;&#26032;&#25945;&#24072;&#21644;&#20027;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;KD&#30340;CIL&#26041;&#27861;&#26080;&#32541;&#38598;&#25104;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#26080;&#26679;&#26412;CIL&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL models. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main models during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#21407;&#22240;&#65292;&#26377;&#25928;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#34987;&#39564;&#35777;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.09437</link><description>&lt;p&gt;
&#20174;&#26399;&#26395;&#21040;&#23433;&#20840;&#65306;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#27491;&#30830;&#30340;&#21407;&#22240;&#26469;&#28040;&#38500;&#28145;&#24230;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space. (arXiv:2308.09437v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#21407;&#22240;&#65292;&#26377;&#25928;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#29615;&#22659;&#20013;&#34987;&#39564;&#35777;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20013;&#28508;&#34255;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#21487;&#33021;&#26377;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#36825;&#22312;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#20110;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#65288;&#22914;&#21307;&#23398;&#24212;&#29992;&#65289;&#26102;&#23384;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#30340;&#21518;&#22788;&#29702;&#27169;&#22411;&#26657;&#27491;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#36755;&#20837;&#32423;&#21035;&#30340;&#27880;&#37322;&#65292;&#36825;&#21482;&#36866;&#29992;&#20110;&#23616;&#37096;&#21270;&#20559;&#35265;&#65292;&#35201;&#20040;&#36890;&#36807;&#25193;&#20805;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#65292;&#24076;&#26395;&#33021;&#23454;&#29616;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#20943;&#23567;&#27169;&#22411;&#23545;&#20559;&#35265;&#30340;&#25935;&#24863;&#24615;&#65292;&#20174;&#32780;&#22312;&#27010;&#24565;&#32423;&#21035;&#19978;&#30830;&#20445;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;&#24403;&#36890;&#36807;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#26469;&#24314;&#27169;&#20559;&#35265;&#26102;&#65292;&#25105;&#20204;&#24378;&#35843;&#36873;&#25321;&#31283;&#20581;&#30340;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#65288;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#24448;&#24448;&#20250;&#23548;&#33268;&#21457;&#25955;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#20351;&#29992;VGG&#12289;ResNet&#21644;EfficientN&#22312;ISIC&#12289;&#39592;&#40836;&#12289;ImageNet&#21644;CelebA&#25968;&#25454;&#38598;&#19978;&#22312;&#21463;&#25511;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#26377;&#25928;&#20943;&#36731;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientN
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.13704</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review. (arXiv:2307.13704v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#21307;&#30103;&#24212;&#29992;&#21644;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#29616;&#22312;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#33021;&#22815;&#35299;&#37322;&#22797;&#26434;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;XAI&#29305;&#21035;&#36866;&#29992;&#20110;&#21361;&#38505;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#20154;&#31867;&#30340;&#29983;&#21629;&#20381;&#36182;&#20110;AI&#31995;&#32479;&#30340;&#20915;&#31574;&#12290;&#21307;&#30103;&#30740;&#31350;&#30340;&#19968;&#20010;&#39046;&#22495;&#26159;&#24180;&#40836;&#39044;&#27979;&#21644;&#34928;&#32769;&#21450;&#19982;&#24180;&#40836;&#30456;&#20851;&#30142;&#30149;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#37492;&#23450;&#12290;&#28982;&#32780;&#65292;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;XAI&#30340;&#20316;&#29992;&#23578;&#26410;&#30452;&#25509;&#25506;&#35752;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#26041;&#27861;&#22312;&#24180;&#40836;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#22120;&#23448;&#31995;&#32479;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;XAI&#22312;&#21307;&#30103;&#24212;&#29992;&#20197;&#21450;&#29305;&#21035;&#26159;&#24180;&#40836;&#39044;&#27979;&#39046;&#22495;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;TwinLiteNet&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#25104;&#26412;&#20302;&#24265;&#19988;&#39640;&#25928;&#20934;&#30830;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#35745;&#31639;&#36164;&#28304;&#33410;&#32422;&#12290;</title><link>http://arxiv.org/abs/2307.10705</link><description>&lt;p&gt;
TwinLiteNet&#65306;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#30340;&#39640;&#25928;&#36731;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;TwinLiteNet&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#25104;&#26412;&#20302;&#24265;&#19988;&#39640;&#25928;&#20934;&#30830;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#35745;&#31639;&#36164;&#28304;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#12290;&#23545;&#20110;&#36947;&#36335;&#19978;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#23548;&#33322;&#26469;&#35828;&#65292;&#21487;&#39537;&#21160;&#21306;&#22495;&#20998;&#21106;&#21644;&#36710;&#36947;&#26816;&#27979;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#38656;&#35201;&#39640;&#31471;&#30828;&#20214;&#65292;&#36825;&#23545;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#32447;&#20998;&#21106;&#27169;&#22411;&#12290;TwinLiteNet&#35774;&#35745;&#25104;&#25104;&#26412;&#20302;&#24265;&#65292;&#20294;&#33021;&#22815;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;BDD100K&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;TwinLiteNet&#65292;&#24182;&#19982;&#29616;&#20195;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;TwinLiteNet&#19982;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#30456;&#20284;&#65292;&#20294;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#26174;&#33879;&#20943;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TwinLiteNet&#22312;&#21487;&#39537;&#21160;&#21306;&#22495;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;91.3%&#30340;mIoU&#35780;&#20998;&#65292;&#22312;&#36710;&#36947;&#26816;&#27979;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;31.08%&#30340;IoU&#35780;&#20998;&#65292;&#20165;&#20351;&#29992;&#20102;40&#19975;&#20010;&#21442;&#25968;&#65292;&#22312;GPU RTX&#19978;&#23454;&#29616;&#20102;415 FPS&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#34394;&#25311;&#30005;&#21147;&#21378;&#22312;&#26085;&#21069;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#31454;&#26631;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#31934;&#30830;&#30340;&#24066;&#22330;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#23398;&#20064;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31454;&#26631;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#23631;&#34109;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#24809;&#32602;&#26426;&#21046;&#26469;&#32771;&#34385;&#34394;&#25311;&#30005;&#21147;&#21378;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#23433;&#20840;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.05812</link><description>&lt;p&gt;
&#34394;&#25311;&#30005;&#21147;&#21378;&#22312;&#26085;&#21069;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#31454;&#26631;&#30340;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning for Strategic Bidding of Virtual Power Plants in Day-Ahead Markets. (arXiv:2307.05812v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#34394;&#25311;&#30005;&#21147;&#21378;&#22312;&#26085;&#21069;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#31454;&#26631;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#31934;&#30830;&#30340;&#24066;&#22330;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#23398;&#20064;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31454;&#26631;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#23631;&#34109;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#24809;&#32602;&#26426;&#21046;&#26469;&#32771;&#34385;&#34394;&#25311;&#30005;&#21147;&#21378;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#23433;&#20840;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#34394;&#25311;&#30005;&#21147;&#21378;&#22312;&#26085;&#21069;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#31454;&#26631;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#37319;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#26041;&#27861;&#65292;&#23398;&#20064;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31454;&#26631;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#31934;&#30830;&#30340;&#24066;&#22330;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32771;&#34385;&#34394;&#25311;&#30005;&#21147;&#21378;&#30340;&#22797;&#26434;&#20869;&#37096;&#29289;&#29702;&#32422;&#26463;&#65292;&#25105;&#20204;&#23545;DDPG&#26041;&#27861;&#36827;&#34892;&#20102;&#20004;&#20010;&#22686;&#24378;&#12290;&#39318;&#20808;&#65292;&#25512;&#23548;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#24433;&#30340;&#23433;&#20840;&#23631;&#34109;&#65292;&#23558;&#20195;&#29702;&#30340;&#34892;&#20026;&#38480;&#21046;&#22312;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#30340;&#38750;&#32447;&#24615;&#21151;&#29575;&#27969;&#26041;&#31243;&#21644;&#36816;&#34892;&#32422;&#26463;&#25152;&#23450;&#20041;&#30340;&#21487;&#34892;&#31354;&#38388;&#20869;&#12290;&#20854;&#27425;&#65292;&#24341;&#20837;&#20102;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#23631;&#34109;&#28608;&#27963;&#24809;&#32602;&#65292;&#40723;&#21169;&#20195;&#29702;&#23398;&#20064;&#26356;&#23433;&#20840;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;IEEE 13-bus&#32593;&#32476;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#20351;&#20195;&#29702;&#23398;&#20064;&#21040;&#39640;&#31454;&#20105;&#21147;&#65292;&#23433;&#20840;&#30340;&#25112;&#30053;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel safe reinforcement learning algorithm for strategic bidding of Virtual Power Plants (VPPs) in day-ahead electricity markets. The proposed algorithm utilizes the Deep Deterministic Policy Gradient (DDPG) method to learn competitive bidding policies without requiring an accurate market model. Furthermore, to account for the complex internal physical constraints of VPPs we introduce two enhancements to the DDPG method. Firstly, a projection-based safety shield that restricts the agent's actions to the feasible space defined by the non-linear power flow equations and operating constraints of distributed energy resources is derived. Secondly, a penalty for the shield activation in the reward function that incentivizes the agent to learn a safer policy is introduced. A case study based on the IEEE 13-bus network demonstrates the effectiveness of the proposed approach in enabling the agent to learn a highly competitive, safe strategic policy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#21453;&#24605;&#35780;&#20272;&#24180;&#40836;&#20272;&#35745;&#23454;&#36341;&#30340;&#21628;&#21505;&#65292;&#23545;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#35299;&#20915;&#23427;&#20204;&#12290;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#24494;&#19981;&#36275;&#36947;&#65292;&#32780;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#26356;&#22823;&#12290;&#30740;&#31350;&#21033;&#29992;&#24471;&#21040;&#30340;&#35265;&#35299;&#25552;&#20986;&#20351;&#29992;FaRL&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04570</link><description>&lt;p&gt;
&#35780;&#20272;&#24180;&#40836;&#20272;&#35745;&#23454;&#36341;&#30340;&#21453;&#24605;&#21628;&#21505;&#65306;&#29616;&#26377;&#25216;&#26415;&#30340;&#27604;&#36739;&#20998;&#26512;&#21644;&#32479;&#19968;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark. (arXiv:2307.04570v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#21453;&#24605;&#35780;&#20272;&#24180;&#40836;&#20272;&#35745;&#23454;&#36341;&#30340;&#21628;&#21505;&#65292;&#23545;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#35299;&#20915;&#23427;&#20204;&#12290;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#24494;&#19981;&#36275;&#36947;&#65292;&#32780;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#26356;&#22823;&#12290;&#30740;&#31350;&#21033;&#29992;&#24471;&#21040;&#30340;&#35265;&#35299;&#25552;&#20986;&#20351;&#29992;FaRL&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22522;&#20934;&#27969;&#31243;&#30340;&#19981;&#19968;&#33268;&#24615;&#23548;&#33268;&#30340;&#21457;&#24067;&#32467;&#26524;&#30340;&#19981;&#21487;&#38752;&#24615;&#65292;&#27604;&#36739;&#19981;&#21516;&#30340;&#24180;&#40836;&#20272;&#35745;&#26041;&#27861;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25253;&#21578;&#25351;&#20986;&#65292;&#20351;&#29992;&#19987;&#38376;&#30340;&#26041;&#27861;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#25345;&#32493;&#25913;&#21892;&#20102;&#24615;&#33021;&#65292;&#28982;&#32780;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#36825;&#20123;&#22768;&#26126;&#25552;&#20986;&#36136;&#30097;&#12290;&#26412;&#25991;&#35782;&#21035;&#20986;&#24403;&#21069;&#20351;&#29992;&#30340;&#35780;&#20272;&#21327;&#35758;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#29712;&#30862;&#20294;&#25345;&#32493;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#35299;&#20915;&#23427;&#20204;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#35813;&#21327;&#35758;&#30340;&#20855;&#20307;&#31034;&#20363;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#21327;&#35758;&#23545;&#26368;&#20808;&#36827;&#30340;&#38754;&#37096;&#24180;&#40836;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#19982;&#20854;&#20182;&#22240;&#32032;&#65288;&#22914;&#38754;&#37096;&#23545;&#40784;&#12289;&#38754;&#37096;&#35206;&#30422;&#12289;&#22270;&#20687;&#20998;&#36776;&#29575;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#37327;&#65289;&#30456;&#27604;&#24494;&#19981;&#36275;&#36947;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#25552;&#20986;&#20351;&#29992;FaRL&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparing different age estimation methods poses a challenge due to the unreliability of published results stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. This paper identifies two trivial, yet persistent issues with the currently used evaluation protocol and describes how to resolve them. We describe our evaluation protocol in detail and provide specific examples of how the protocol should be used. We utilize the protocol to offer an extensive comparative analysis for state-of-the-art facial age estimation methods. Surprisingly, we find that the performance differences between the methods are negligible compared to the effect of other factors, such as facial alignment, facial coverage, image resolution, model architecture, or the amount of data used for pretraining. We use the gained insights to propose using FaRL a
&lt;/p&gt;</description></item><item><title>ProbVLM&#26159;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22266;&#26377;&#30340;&#23884;&#20837;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.00398</link><description>&lt;p&gt;
ProbVLM: &#20923;&#32467;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models. (arXiv:2307.00398v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00398
&lt;/p&gt;
&lt;p&gt;
ProbVLM&#26159;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22266;&#26377;&#30340;&#23884;&#20837;&#27495;&#20041;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22914;CLIP&#25104;&#21151;&#22320;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#12290;&#36890;&#36807;&#26631;&#20934;&#30340;&#30830;&#23450;&#24615;&#26144;&#23556;&#36807;&#31243;&#65292;&#23558;&#22270;&#20687;&#25110;&#25991;&#26412;&#26679;&#26412;&#26144;&#23556;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#21521;&#37327;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65306;&#30001;&#20110;&#22810;&#20010;&#26679;&#26412;&#65288;&#22270;&#20687;&#25110;&#25991;&#26412;&#65289;&#21487;&#20197;&#25277;&#35937;&#20986;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#30456;&#21516;&#27010;&#24565;&#65292;&#30830;&#23450;&#24615;&#23884;&#20837;&#19981;&#21453;&#26144;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProbVLM&#65292;&#19968;&#31181;&#27010;&#29575;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20107;&#21518;&#26041;&#24335;&#22312;&#39044;&#35757;&#32451;&#30340;VLM&#20013;&#36890;&#36807;&#20869;&#37096;/&#22806;&#37096;&#27169;&#24577;&#23545;&#40784;&#20272;&#35745;&#23884;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#25110;&#35745;&#31639;&#12290;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#21363;COCO&#12289;Flickr&#12289;CUB&#21644;Oxford-flowers&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#20004;&#20010;VLM&#65288;CLIP&#21644;BLIP&#65289;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#37327;&#21270;&#20102;&#23884;&#20837;&#19981;&#30830;&#23450;&#24615;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#26657;&#20934;&#65292;&#24182;&#34920;&#26126;ProbVLM&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20027;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#19979;&#28216;&#30340;&#27700;&#20301;&#65292;&#24182;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.15907</link><description>&lt;p&gt;
&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#27700;&#20301;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Models for Water Stage Predictions in South Florida. (arXiv:2306.15907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#19979;&#28216;&#30340;&#27700;&#20301;&#65292;&#24182;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21644;&#39044;&#27979;&#27827;&#27969;&#31995;&#32479;&#30340;&#27700;&#20301;&#23545;&#20110;&#27946;&#27700;&#35686;&#25253;&#12289;&#27700;&#21147;&#25805;&#20316;&#21644;&#27946;&#27700;&#20943;&#36731;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;HEC-RAS&#12289;MIKE&#21644;SWMM&#31561;&#24037;&#20855;&#24314;&#31435;&#35814;&#32454;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27700;&#25991;&#21644;&#27700;&#21147;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#25972;&#20010;&#27969;&#22495;&#65292;&#20174;&#32780;&#39044;&#27979;&#31995;&#32479;&#20013;&#20219;&#24847;&#28857;&#30340;&#27700;&#20301;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#35745;&#31639;&#37327;&#22823;&#65292;&#23588;&#20854;&#23545;&#20110;&#22823;&#27969;&#22495;&#21644;&#38271;&#26102;&#38388;&#27169;&#25311;&#26469;&#35828;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20960;&#20010;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#27700;&#20301;&#12290;&#26412;&#25991;&#20197;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#30340;&#19979;&#28216;&#27700;&#20301;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#26469;&#33258;&#21335;&#20315;&#32599;&#37324;&#36798;&#27700;&#31649;&#29702;&#21306;&#65288;SFWMD&#65289;&#30340;DBHYDRO&#25968;&#25454;&#24211;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2010&#24180;1&#26376;1&#26085;&#33267;2020&#24180;12&#26376;31&#26085;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DL&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating and predicting water levels in river systems is essential for flood warnings, hydraulic operations, and flood mitigations. In the engineering field, tools such as HEC-RAS, MIKE, and SWMM are used to build detailed physics-based hydrological and hydraulic computational models to simulate the entire watershed, thereby predicting the water stage at any point in the system. However, these physics-based models are computationally intensive, especially for large watersheds and for longer simulations. To overcome this problem, we train several deep learning (DL) models for use as surrogate models to rapidly predict the water stage. The downstream stage of the Miami River in South Florida is chosen as a case study for this paper. The dataset is from January 1, 2010, to December 31, 2020, downloaded from the DBHYDRO database of the South Florida Water Management District (SFWMD). Extensive experiments show that the performance of the DL models is comparable to that of the physics-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2306.12774</link><description>&lt;p&gt;
&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#32431;&#25506;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pure Exploration in Bandits with Linear Constraints. (arXiv:2306.12774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#23384;&#22312;&#32447;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#19968;&#23450;&#32622;&#20449;&#24230;&#19979;&#30830;&#23450;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#19982;&#26631;&#20934;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#19981;&#21516;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31574;&#30053;&#21487;&#33021;&#19981;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#26159;&#21487;&#33021;&#22312;&#22810;&#20010;&#33218;&#20043;&#38388;&#36827;&#34892;&#28151;&#21512;&#12290;&#36825;&#31181;&#24773;&#20917;&#25913;&#21464;&#20102;&#38382;&#39064;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#19979;&#30028;&#36827;&#34892;&#20102;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#27492;&#35774;&#32622;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#22522;&#20110;&#8220;&#36319;&#36394;&#20572;&#27490;&#8221;&#26041;&#27861;&#65292;&#21478;&#19968;&#20010;&#22522;&#20110;&#21338;&#24328;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#32422;&#26463;&#22914;&#20309;&#25913;&#21464;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of identifying the optimal policy with a fixed confidence level in a multi-armed bandit setup, when \emph{the arms are subject to linear constraints}. Unlike the standard best-arm identification problem which is well studied, the optimal policy in this case may not be deterministic and could mix between several arms. This changes the geometry of the problem which we characterize via an information-theoretic lower bound. We introduce two asymptotically optimal algorithms for this setting, one based on the Track-and-Stop method and the other based on a game-theoretic approach. Both these algorithms try to track an optimal allocation based on the lower bound and computed by a weighted projection onto the boundary of a normal cone. Finally, we provide empirical results that validate our bounds and visualize how constraints change the hardness of the problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25171;&#30772;&#35774;&#22791;&#19978;&#35757;&#32451;&#20869;&#23384;&#22721;&#22418;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#35757;&#32451;&#26356;&#22823;&#26356;&#22797;&#26434;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35843;&#26597;&#20998;&#26512;&#20102;&#20869;&#23384;&#22721;&#22418;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#24635;&#32467;&#20102;&#35774;&#22791;&#19978;&#35757;&#32451;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10388</link><description>&lt;p&gt;
&#25171;&#30772;&#35774;&#22791;&#19978;&#35757;&#32451;&#20869;&#23384;&#22721;&#22418;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Breaking On-device Training Memory Wall: A Systematic Survey. (arXiv:2306.10388v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25171;&#30772;&#35774;&#22791;&#19978;&#35757;&#32451;&#20869;&#23384;&#22721;&#22418;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#35757;&#32451;&#26356;&#22823;&#26356;&#22797;&#26434;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35843;&#26597;&#20998;&#26512;&#20102;&#20869;&#23384;&#22721;&#22418;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#24635;&#32467;&#20102;&#35774;&#22791;&#19978;&#35757;&#32451;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#36825;&#20123;&#35774;&#22791;&#19978;&#26377;&#38480;&#30340;&#20869;&#23384;&#65292;&#36825;&#20250;&#20005;&#37325;&#38480;&#21046;&#21487;&#20197;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#20010;&#31995;&#32479;&#24615;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#25171;&#30772;&#35774;&#22791;&#19978;&#35757;&#32451;&#20869;&#23384;&#22721;&#22418;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#35757;&#32451;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#35774;&#22791;&#19978;&#35757;&#32451;&#20013;&#36935;&#21040;&#30340;&#20869;&#23384;&#22721;&#22418;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#35774;&#22791;&#19978;&#35757;&#32451;&#30340;&#32508;&#21512;&#25991;&#29486;&#32508;&#36848;&#65292;&#22320;&#22336;&#20102;&#20869;&#23384;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#35774;&#22791;&#19978;&#35757;&#32451;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20379;&#36825;&#20123;&#25216;&#26415;&#21450;&#20854;&#25928;&#26524;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#23545;&#20110;&#35299;&#20915;&#35774;&#22791;&#19978;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device training has become an increasingly popular approach to machine learning, enabling models to be trained directly on mobile and edge devices. However, a major challenge in this area is the limited memory available on these devices, which can severely restrict the size and complexity of the models that can be trained. In this systematic survey, we aim to explore the current state-of-the-art techniques for breaking on-device training memory walls, focusing on methods that can enable larger and more complex models to be trained on resource-constrained devices.  Specifically, we first analyze the key factors that contribute to the phenomenon of memory walls encountered during on-device training. Then, we present a comprehensive literature review of on-device training, which addresses the issue of memory limitations. Finally, we summarize on-device training and highlight the open problems for future research.  By providing a comprehensive overview of these techniques and their effe
&lt;/p&gt;</description></item><item><title>RescueSpeech&#26159;&#19968;&#20010;&#29992;&#20110;&#25628;&#25937;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#30340;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20173;&#26080;&#27861;&#20196;&#20154;&#28385;&#24847;&#12290;</title><link>http://arxiv.org/abs/2306.04054</link><description>&lt;p&gt;
RescueSpeech&#65306;&#29992;&#20110;&#25628;&#25937;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#30340;&#24503;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
RescueSpeech: A German Corpus for Speech Recognition in Search and Rescue Domain. (arXiv:2306.04054v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04054
&lt;/p&gt;
&lt;p&gt;
RescueSpeech&#26159;&#19968;&#20010;&#29992;&#20110;&#25628;&#25937;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#30340;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20173;&#26080;&#27861;&#20196;&#20154;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#38899;&#35782;&#21035;&#22312;&#26368;&#36817;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20294;&#22312;&#22024;&#26434;&#12289;&#22238;&#22768;&#30340;&#29615;&#22659;&#20013;&#20934;&#30830;&#36716;&#24405;&#23545;&#35805;&#21644;&#24773;&#24863;&#34920;&#36798;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38590;&#24230;&#12290;&#22312;&#25628;&#25937;&#39046;&#22495;&#23588;&#20854;&#22914;&#27492;&#65292;&#22240;&#20026;&#36716;&#24405;&#25937;&#25588;&#38431;&#25104;&#21592;&#20043;&#38388;&#30340;&#23545;&#35805;&#23545;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#25628;&#25937;&#22330;&#26223;&#20013;&#35821;&#38899;&#25968;&#25454;&#21644;&#30456;&#20851;&#32972;&#26223;&#22122;&#22768;&#30340;&#31232;&#32570;&#24615;&#20351;&#24471;&#37096;&#32626;&#20581;&#22766;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#24182;&#20844;&#24320;&#20102;&#19968;&#20010;&#21517;&#20026;RescueSpeech&#30340;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#27169;&#25311;&#25937;&#25588;&#28436;&#20064;&#30340;&#30495;&#23454;&#35821;&#38899;&#24405;&#38899;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#31454;&#20105;&#24615;&#35757;&#32451;&#37197;&#26041;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25152;&#36798;&#21040;&#30340;&#24615;&#33021;&#27700;&#24179;&#20173;&#36828;&#26410;&#33021;&#20196;&#20154;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advancements in speech recognition, there are still difficulties in accurately transcribing conversational and emotional speech in noisy and reverberant acoustic environments. This poses a particular challenge in the search and rescue (SAR) domain, where transcribing conversations among rescue team members is crucial to support real-time decision-making. The scarcity of speech data and associated background noise in SAR scenarios make it difficult to deploy robust speech recognition systems.  To address this issue, we have created and made publicly available a German speech dataset called RescueSpeech. This dataset includes real speech recordings from simulated rescue exercises. Additionally, we have released competitive training recipes and pre-trained models. Our study indicates that the current level of performance achieved by state-of-the-art methods is still far from being acceptable.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#26377;&#25928;&#23398;&#20064;&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#38543;&#30528;&#31283;&#23450;&#32500;&#25968;&#22686;&#22823;&#32780;&#23398;&#20064;&#25152;&#26377;&#29366;&#24577;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13409</link><description>&lt;p&gt;
&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#26377;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates. (arXiv:2305.13409v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#26377;&#25928;&#23398;&#20064;&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#38543;&#30528;&#31283;&#23450;&#32500;&#25968;&#22686;&#22823;&#32780;&#23398;&#20064;&#25152;&#26377;&#29366;&#24577;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36890;&#36807;&#20811;&#21033;&#31119;&#24503;&#38376;&#21644;$O(\log(n))$&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26368;&#22810;&#20351;&#29992;$t$&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;$n$&#37327;&#23376;&#27604;&#29305;&#29366;&#24577;$|\psi\rangle$&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#29992;$\mathsf{poly}(n,2^t,1/\epsilon)$&#26102;&#38388;&#21644;$|\psi\rangle$&#30340;&#22797;&#21046;&#26469;&#23398;&#20064;$|\psi\rangle$&#65292;&#20351;&#20854;&#36319;&#30495;&#23454;&#29366;&#24577;&#30340;&#36317;&#31163;&#19981;&#36229;&#36807;$\epsilon$&#12290;&#35813;&#32467;&#26524;&#26159;&#19968;&#20010;&#31283;&#23450;&#32500;&#25968;&#36739;&#22823;&#30340;&#29366;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#29305;&#20363;&#65292;&#24403;&#19968;&#20010;&#37327;&#23376;&#29366;&#24577;&#30340;&#31283;&#23450;&#23376;&#32500;&#25968;&#20026;$k$&#65292;&#34920;&#31034;&#23427;&#34987;&#19968;&#20010;&#30001;$2^k$&#20010;Pauli&#31639;&#23376;&#30340;Abel&#32676;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give an algorithm that efficiently learns a quantum state prepared by Clifford gates and $O(\log(n))$ non-Clifford gates. Specifically, for an $n$-qubit state $\lvert \psi \rangle$ prepared with at most $t$ non-Clifford gates, we show that $\mathsf{poly}(n,2^t,1/\epsilon)$ time and copies of $\lvert \psi \rangle$ suffice to learn $\lvert \psi \rangle$ to trace distance at most $\epsilon$. This result follows as a special case of an algorithm for learning states with large stabilizer dimension, where a quantum state has stabilizer dimension $k$ if it is stabilized by an abelian group of $2^k$ Pauli operators. We also develop an efficient property testing algorithm for stabilizer dimension, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#26469;&#23398;&#20064;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#25152;&#38656;&#30340;&#35266;&#23519;&#24207;&#21015;&#30340;&#26368;&#20248;&#39034;&#24207;&#12290;&#22240;&#20026;&#35786;&#26029;&#25351;&#21335;&#30340;&#32570;&#38519;&#65292;&#23588;&#20854;&#23545;&#32597;&#35265;&#30149;&#25110;&#24739;&#26377;&#22810;&#31181;&#30149;&#30340;&#24739;&#32773;&#65292;DRL&#31639;&#27861;&#20855;&#26377;&#37325;&#35201;&#29616;&#23454;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.06295</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20174;&#30005;&#23376;&#30149;&#21382;&#20013;&#25552;&#21462;&#35786;&#26029;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning. (arXiv:2305.06295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#26469;&#23398;&#20064;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#25152;&#38656;&#30340;&#35266;&#23519;&#24207;&#21015;&#30340;&#26368;&#20248;&#39034;&#24207;&#12290;&#22240;&#20026;&#35786;&#26029;&#25351;&#21335;&#30340;&#32570;&#38519;&#65292;&#23588;&#20854;&#23545;&#32597;&#35265;&#30149;&#25110;&#24739;&#26377;&#22810;&#31181;&#30149;&#30340;&#24739;&#32773;&#65292;DRL&#31639;&#27861;&#20855;&#26377;&#37325;&#35201;&#29616;&#23454;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35786;&#26029;&#25351;&#21335;&#26088;&#22312;&#35828;&#26126;&#21487;&#33021;&#23548;&#33268;&#35786;&#26029;&#30340;&#27493;&#39588;&#12290;&#25351;&#21335;&#33021;&#22815;&#29702;&#24615;&#22320;&#35268;&#33539;&#21270;&#20020;&#24202;&#20915;&#31574;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#30340;&#24314;&#31435;&#26159;&#20026;&#20102;&#35206;&#30422;&#22823;&#22810;&#25968;&#20154;&#32676;&#65292;&#22240;&#27492;&#22312;&#25351;&#23548;&#32597;&#35265;&#30149;&#25110;&#24739;&#26377;&#22810;&#31181;&#30149;&#30340;&#24739;&#32773;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#26041;&#38754;&#65292;&#23384;&#22312;&#32570;&#38519;&#12290;&#26412;&#25991;&#21463;&#25351;&#21335;&#21551;&#21457;&#65292;&#23558;&#35786;&#26029;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;(EHRs)&#35757;&#32451;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#33719;&#24471;&#27491;&#30830;&#35786;&#26029;&#25152;&#38656;&#30340;&#35266;&#23519;&#24207;&#21015;&#30340;&#26368;&#20248;&#39034;&#24207;&#12290;&#30001;&#20110;DRL&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#19978;&#19979;&#25991;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#26041;&#27861;&#21644;&#35774;&#32622;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#24444;&#27492;&#21644;&#32463;&#20856;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#20294;&#36924;&#30495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical diagnosis guidelines aim at specifying the steps that may lead to a diagnosis. Guidelines enable rationalizing and normalizing clinical decisions but suffer drawbacks as they are built to cover the majority of the population and may fail in guiding to the right diagnosis for patients with uncommon conditions or multiple pathologies. Moreover, their updates are long and expensive, making them unsuitable to emerging practices. Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms trained on Electronic Health Records (EHRs) to learn the optimal sequence of observations to perform in order to obtain a correct diagnosis. Because of the variety of DRL algorithms and of their sensitivity to the context, we considered several approaches and settings that we compared to each other, and to classical classifiers. We experimented on a synthetic but realistic dataset to differenti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;PyTorch&#30340;Fully Sharded Data Parallel&#65288;FSDP&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21487;&#25193;&#23637;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20248;&#21270;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11277</link><description>&lt;p&gt;
PyTorch FSDP&#65306;&#20840;&#38754;&#20998;&#29255;&#25968;&#25454;&#24182;&#34892;&#35268;&#27169;&#21270;&#30340;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. (arXiv:2304.11277v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;PyTorch&#30340;Fully Sharded Data Parallel&#65288;FSDP&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21487;&#25193;&#23637;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20248;&#21270;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#24191;&#27867;&#39046;&#22495;&#20869;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#28508;&#21147;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#24320;&#21457;&#21644;&#25506;&#32034;&#22823;&#22411;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#20173;&#21463;&#38480;&#20110;&#23569;&#25968;&#39640;&#32423;&#29992;&#25143;&#21644;&#34892;&#19994;&#39046;&#34966;&#65292;&#23548;&#33268;&#25216;&#26415;&#19978;&#30340;&#38544;&#21547;&#22721;&#22418;&#38459;&#30861;&#24191;&#27867;&#31038;&#21306;&#35775;&#38382;&#21644;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PyTorch Fully Sharded Data Parallel&#65288;FSDP&#65289;&#20316;&#20026;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;&#30340;&#20135;&#19994;&#32423;&#35299;&#20915;&#26041;&#26696;&#12290;FSDP&#24050;&#19982;&#20960;&#20010;&#20851;&#38190;PyTorch&#26680;&#24515;&#32452;&#20214;&#65288;&#21253;&#25324;&#24352;&#37327;&#23454;&#29616;&#12289;&#20998;&#21457;&#22120;&#31995;&#32479;&#21644;CUDA&#20869;&#23384;&#32531;&#23384;&#20998;&#37197;&#22120;&#65289;&#23494;&#20999;&#21327;&#20316;&#65292;&#20197;&#25552;&#20379;&#38750;&#20405;&#20837;&#24335;&#29992;&#25143;&#20307;&#39564;&#21644;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;FSDP&#26412;&#22320;&#38598;&#25104;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#21644;&#35774;&#32622;&#65292;&#20248;&#21270;&#20102;&#21508;&#31181;&#30828;&#20214;&#37197;&#32622;&#30340;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.05257</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#26102;&#38388;&#21464;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Multi-granulariy Time-based Transformer for Knowledge Tracing. (arXiv:2304.05257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#23398;&#29983;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#20026;&#27599;&#20010;&#23398;&#29983;&#21019;&#24314;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#39044;&#27979;&#23398;&#29983;&#22312;&#32473;&#23450;&#27979;&#35797;&#20013;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;RIIID&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20316;&#20026;&#35299;&#30721;&#22120;&#36755;&#20837;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;LightGBM&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a transformer architecture for predicting student performance on standardized tests. Specifically, we leverage students historical data, including their past test scores, study habits, and other relevant information, to create a personalized model for each student. We then use these models to predict their future performance on a given test. Applying this model to the RIIID dataset, we demonstrate that using multiple granularities for temporal features as the decoder input significantly improve model performance. Our results also show the effectiveness of our approach, with substantial improvements over the LightGBM method. Our work contributes to the growing field of AI in education, providing a scalable and accurate tool for predicting student outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#21644;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;&#23454;&#29616;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01347</link><description>&lt;p&gt;
&#26102;&#38388;&#21160;&#24577;&#21516;&#27493;&#21151;&#33021;&#33041;&#32593;&#32476;&#22312;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis. (arXiv:2304.01347v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#21644;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;&#23454;&#29616;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#21487;&#20197;&#25429;&#25417;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#20013;&#30340;&#33041;&#27963;&#21160;&#26102;&#21464;&#24322;&#24120;&#65292;&#24182;&#22312;&#25581;&#31034;&#31934;&#31070;&#20998;&#35010;&#30151;&#65288;SZ&#65289;&#24739;&#32773;&#24322;&#24120;&#33041;&#27963;&#21160;&#26426;&#21046;&#26041;&#38754;&#20855;&#26377;&#22825;&#28982;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#21160;&#24577;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#8212;&#8212;&#26102;&#24577;&#33041;&#31867;&#21035;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;temporal-BCGCN&#65289;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#29420;&#29305;&#30340;&#21160;&#24577;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22359;DSF-BrainNet&#65292;&#29992;&#20110;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;TemporalConv&#65292;&#22522;&#20110;&#29305;&#24449;&#30340;&#21516;&#27493;&#26102;&#38388;&#23646;&#24615;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#21270;&#24322;&#24120;&#21322;&#29699;&#20391;&#21270;&#26816;&#27979;&#24037;&#20855;&#65292;&#31216;&#20026;CategoryPool&#12290;&#35813;&#30740;&#31350;&#22312;COBRE&#21644;UCLA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#20998;&#21035;&#36798;&#21040;83.62&#65285;&#21644;89.71&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Available evidence suggests that dynamic functional connectivity (dFC) can capture time-varying abnormalities in brain activity in rs-fMRI data and has a natural advantage in uncovering mechanisms of abnormal brain activity in schizophrenia(SZ) patients. Hence, an advanced dynamic brain network analysis model called the temporal brain category graph convolutional network (temporal-BCGCN) was employed. Firstly, a unique dynamic brain network analysis module, DSF-BrainNet, was designed to construct dynamic synchronization features. Subsequently, a revolutionary graph convolution method, TemporalConv, was proposed, based on the synchronous temporal properties of feature. Finally, the first modular abnormal hemispherical lateralization test tool in deep learning based on rs-fMRI data, named CategoryPool, was proposed. This study was validated on COBRE and UCLA datasets and achieved 83.62% and 89.71% average accuracy, respectively, outperforming the baseline model and other State-of-the-Art
&lt;/p&gt;</description></item><item><title>BOLT&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#27169;&#22411;&#24182;&#25277;&#35937;&#25481;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2303.17727</link><description>&lt;p&gt;
BOLT&#65306;&#19968;&#31181;&#29992;&#20110;&#22312;&#26222;&#36890;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#21270;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware. (arXiv:2303.17727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17727
&lt;/p&gt;
&lt;p&gt;
BOLT&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#27169;&#22411;&#24182;&#25277;&#35937;&#25481;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#22312;&#26222;&#36890;CPU&#30828;&#20214;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#23545;&#20110;&#27665;&#20027;&#21270;&#28145;&#24230;&#23398;&#20064;&#33021;&#21147;&#20855;&#26377;&#24040;&#22823;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#30446;&#21069;&#65292;&#30001;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#32452;&#25104;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#24191;&#27867;&#20351;&#29992;&#19987;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;&#20363;&#22914;GPU&#65289;&#65292;&#36825;&#20123;&#21152;&#36895;&#22120;&#20165;&#38480;&#20110;&#23569;&#25968;&#20855;&#26377;&#30456;&#24403;&#36130;&#21153;&#36164;&#28304;&#30340;&#26426;&#26500;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#21644;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#24102;&#26469;&#24778;&#20154;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;BOLT&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;BOLT&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#29992;&#20110;&#26500;&#24314;&#27169;&#22411;&#65292;&#35813;API&#23545;&#20110;&#29616;&#26377;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#29992;&#25143;&#26469;&#35828;&#26159;&#29087;&#24713;&#30340;&#12290;&#36890;&#36807;&#33258;&#21160;&#35843;&#25972;&#19987;&#29992;&#36229;&#21442;&#25968;&#65292;BOLT&#20063;&#25277;&#35937;&#25481;&#20102;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient large-scale neural network training and inference on commodity CPU hardware is of immense practical significance in democratizing deep learning (DL) capabilities. Presently, the process of training massive models consisting of hundreds of millions to billions of parameters requires the extensive use of specialized hardware accelerators, such as GPUs, which are only accessible to a limited number of institutions with considerable financial resources. Moreover, there is often an alarming carbon footprint associated with training and deploying these models. In this paper, we address these challenges by introducing BOLT, a sparse deep learning library for training massive neural network models on standard CPU hardware. BOLT provides a flexible, high-level API for constructing models that will be familiar to users of existing popular DL frameworks. By automatically tuning specialized hyperparameters, BOLT also abstracts away the algorithmic details of sparse network training. We e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;R-CNN&#65289;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#30058;&#33540;&#26893;&#29289;&#30340;&#21494;&#30149;&#23475;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#25972;&#20010;&#30058;&#33540;&#26893;&#29289;&#22270;&#20687;&#30340;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.09063</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26893;&#29289;&#30149;&#23475;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Plant Disease Detection using Region-Based Convolutional Neural Network. (arXiv:2303.09063v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;R-CNN&#65289;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#30058;&#33540;&#26893;&#29289;&#30340;&#21494;&#30149;&#23475;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#25972;&#20010;&#30058;&#33540;&#26893;&#29289;&#22270;&#20687;&#30340;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#22312;&#23391;&#21152;&#25289;&#22269;&#30340;&#39135;&#21697;&#21644;&#32463;&#27982;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#32454;&#33740;&#65292;&#30149;&#27602;&#21644;&#30495;&#33740;&#30149;&#23475;&#38480;&#21046;&#20102;&#20316;&#29289;&#29983;&#20135;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#39044;&#27979;&#30058;&#33540;&#26893;&#29289;&#30340;&#21494;&#30149;&#23475;&#12290;&#36890;&#36807;&#20462;&#25913;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;R-CNN&#65289;&#30340;&#32467;&#26500;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#21033;&#29992;&#25972;&#20010;&#30058;&#33540;&#26893;&#29289;&#22270;&#20687;&#30340;&#29305;&#24449;&#26469;&#26816;&#27979;&#26893;&#29289;&#30149;&#23475;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26816;&#27979;&#22235;&#31181;&#24120;&#35265;&#30340;&#30058;&#33540;&#26893;&#29289;&#30149;&#23475;&#26041;&#38754;&#34920;&#29616;&#20986;94.3&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agriculture plays an important role in the food and economy of Bangladesh. The rapid growth of population over the years also has increased the demand for food production. One of the major reasons behind low crop production is numerous bacteria, virus and fungal plant diseases. Early detection of plant diseases and proper usage of pesticides and fertilizers are vital for preventing the diseases and boost the yield. Most of the farmers use generalized pesticides and fertilizers in the entire fields without specifically knowing the condition of the plants. Thus the production cost oftentimes increases, and, not only that, sometimes this becomes detrimental to the yield. Deep Learning models are found to be very effective to automatically detect plant diseases from images of plants, thereby reducing the need for human specialists. This paper aims at building a lightweight deep learning model for predicting leaf disease in tomato plants. By modifying the region-based convolutional neural n
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20010;&#26435;&#34913;&#65292;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#35823;&#24046;&#65292;&#20294;&#20250;&#22686;&#21152;&#25968;&#25454;&#35823;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2303.05718</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#27867;&#21270;&#38169;&#35823;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Tradeoff of generalization error in unsupervised learning. (arXiv:2303.05718v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05718
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20010;&#26435;&#34913;&#65292;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#35823;&#24046;&#65292;&#20294;&#20250;&#22686;&#21152;&#25968;&#25454;&#35823;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#26368;&#23567;&#21270;&#27867;&#21270;&#38169;&#35823;&#65288;GE&#65289;&#30340;&#26368;&#20339;&#27169;&#22411;&#22797;&#26434;&#24230;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#23545;&#20110;&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#36825;&#20010;&#20219;&#21153;&#36890;&#24120;&#28041;&#21450;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65306;&#36890;&#36807;&#20351;&#27169;&#22411;&#26356;&#22797;&#26434;&#26469;&#38477;&#20302;&#20559;&#24046;&#20250;&#23548;&#33268;&#26041;&#24046;&#30340;&#22686;&#21152;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20851;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#26159;&#21542;&#23384;&#22312;&#30456;&#21516;&#30340;&#26435;&#34913;&#38382;&#39064;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#26080;&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#34920;&#29616;&#20986;GE&#30340;&#20004;&#20010;&#32452;&#25104;&#35201;&#32032;&#30340;&#26435;&#34913;&#65292;&#21363;&#27169;&#22411;&#35823;&#24046;&#21644;&#25968;&#25454;&#35823;&#24046; - &#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#20943;&#23567;&#27169;&#22411;&#35823;&#24046;&#30340;&#21516;&#26102;&#20197;&#25968;&#25454;&#35823;&#24046;&#20026;&#20195;&#20215;&#65292;&#25968;&#25454;&#35823;&#24046;&#23545;&#20110;&#26356;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#36215;&#21040;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#35757;&#32451;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#29983;&#25104;&#32473;&#23450;&#28201;&#24230;&#19979;&#30340;&#20108;&#32500;&#20234;&#36763;&#27169;&#22411;&#30340;&#37197;&#32622;&#20197;&#21450;&#32473;&#23450;&#20837;&#21475;&#21644;&#20986;&#21475;&#36895;&#29575;&#30340;&#23436;&#20840;&#38750;&#23545;&#31216;&#31616;&#21333;&#25490;&#26021;&#36807;&#31243;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Finding the optimal model complexity that minimizes the generalization error (GE) is a key issue of machine learning. For the conventional supervised learning, this task typically involves the bias-variance tradeoff: lowering the bias by making the model more complex entails an increase in the variance. Meanwhile, little has been studied about whether the same tradeoff exists for unsupervised learning. In this study, we propose that unsupervised learning generally exhibits a two-component tradeoff of the GE, namely the model error and the data error -- using a more complex model reduces the model error at the cost of the data error, with the data error playing a more significant role for a smaller training dataset. This is corroborated by training the restricted Boltzmann machine to generate the configurations of the two-dimensional Ising model at a given temperature and the totally asymmetric simple exclusion process with given entry and exit rates. Our results also indicate that the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32593;&#32476;&#19978;&#30340;&#24322;&#26500;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05445</link><description>&lt;p&gt;
&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65306;&#22797;&#26434;&#32593;&#32476;&#19978;&#24322;&#26500;&#36172;&#21338;&#26426;&#30340;&#39640;&#25928;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks. (arXiv:2303.05445v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32593;&#32476;&#19978;&#30340;&#24322;&#26500;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#39034;&#24207;&#20915;&#31574;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#22914;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#21644;&#26080;&#32447;&#32593;&#32476;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22810;&#20195;&#29702;&#30340;&#22330;&#26223;&#65292;&#27599;&#20010;&#20195;&#29702;&#35299;&#20915;&#33258;&#24049;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#36172;&#21338;&#26426;&#25317;&#26377;&#19981;&#21516;&#30340;&#33218;&#12290;&#20182;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#36890;&#36807;&#32473;&#23450;&#32593;&#32476;&#30340;&#36890;&#20449;&#21327;&#35758;&#21327;&#20316;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#20182;&#20204;&#30340;&#38598;&#20307;&#36951;&#25022;&#12290;&#20808;&#21069;&#20851;&#20110;&#27492;&#38382;&#39064;&#30340;&#25991;&#29486;&#21482;&#32771;&#34385;&#20102;&#33218;&#30340;&#24322;&#36136;&#24615;&#21644;&#32593;&#32476;&#21270;&#20195;&#29702;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21516;&#26102;&#21253;&#21547;&#36825;&#20004;&#20010;&#29305;&#24615;&#30340;&#35774;&#32622;&#12290;&#38024;&#23545;&#36825;&#19968;&#26032;&#39062;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#26631;&#20934;&#27867;&#27946;&#21327;&#35758;&#32467;&#21512;&#32463;&#20856;&#30340;&#19978;&#32622;&#20449;&#30028;&#31574;&#30053;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20943;&#36731;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#27867;&#27946;&#36896;&#25104;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#35758;&#65292;&#31216;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#12290;&#25105;&#20204;&#23545;&#30001;&#27492;&#20135;&#29983;&#30340;&#36951;&#25022;&#19978;&#30028;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#21327;&#35758;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-armed bandits are extensively used to model sequential decision-making, making them ubiquitous in many real-life applications such as online recommender systems and wireless networking. We consider a multi-agent setting where each agent solves their own bandit instance endowed with a different set of arms. Their goal is to minimize their group regret while collaborating via some communication protocol over a given network. Previous literature on this problem only considered arm heterogeneity and networked agents separately. In this work, we introduce a setting that encompasses both features. For this novel setting, we first provide a rigorous regret analysis for a standard flooding protocol combined with the classic UCB policy. Then, to mitigate the issue of high communication costs incurred by flooding in complex networks, we propose a new protocol called Flooding with Absorption (FwA). We provide a theoretical analysis of the resulting regret bound and discuss the advantages of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HODL&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#30740;&#31350;&#20248;&#21270;&#23548;&#20986;&#27169;&#22411;&#26500;&#24314;&#21644;&#30456;&#24212;&#23398;&#20064;&#36807;&#31243;&#30340;&#20869;&#22312;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#32852;&#21512;&#25910;&#25947;&#24615;&#12290;&#36825;&#26159;&#23545;&#36825;&#20004;&#20010;&#20219;&#21153;&#25552;&#20379;&#30340;&#39318;&#20010;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.05587</link><description>&lt;p&gt;
&#20998;&#23618;&#20248;&#21270;&#23548;&#20986;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Optimization-Derived Learning. (arXiv:2302.05587v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HODL&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#30740;&#31350;&#20248;&#21270;&#23548;&#20986;&#27169;&#22411;&#26500;&#24314;&#21644;&#30456;&#24212;&#23398;&#20064;&#36807;&#31243;&#30340;&#20869;&#22312;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#32852;&#21512;&#25910;&#25947;&#24615;&#12290;&#36825;&#26159;&#23545;&#36825;&#20004;&#20010;&#20219;&#21153;&#25552;&#20379;&#30340;&#39318;&#20010;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#21033;&#29992;&#20248;&#21270;&#25216;&#26415;&#26469;&#35268;&#21010;&#28145;&#24230;&#27169;&#22411;&#30340;&#20256;&#25773;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#25152;&#35859;&#30340;&#20248;&#21270;&#23548;&#20986;&#23398;&#20064;&#65288;ODL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#21516;&#30340;&#23398;&#20064;&#21644;&#35270;&#35273;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#30340;ODL&#26041;&#27861;&#20173;&#23384;&#22312;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#24403;&#21069;&#30340;ODL&#26041;&#27861;&#20542;&#21521;&#20110;&#23558;&#27169;&#22411;&#26500;&#24314;&#21644;&#23398;&#20064;&#35270;&#20026;&#20004;&#20010;&#29420;&#31435;&#30340;&#38454;&#27573;&#65292;&#22240;&#27492;&#26410;&#33021;&#20934;&#30830;&#34920;&#36798;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;Hierarchical ODL&#65288;HODL&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#20248;&#21270;&#23548;&#20986;&#27169;&#22411;&#26500;&#24314;&#21644;&#30456;&#24212;&#23398;&#20064;&#36807;&#31243;&#30340;&#20869;&#22312;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#32852;&#21512;&#25910;&#25947;&#24615;&#65292;&#20174;&#36924;&#36817;&#36136;&#37327;&#21644;&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#35282;&#24230;&#23637;&#31034;&#20102;&#35777;&#26126;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23545;&#36825;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#20219;&#21153;&#25552;&#20379;&#30340;&#39318;&#20010;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, by utilizing optimization techniques to formulate the propagation of deep model, a variety of so-called Optimization-Derived Learning (ODL) approaches have been proposed to address diverse learning and vision tasks. Although having achieved relatively satisfying practical performance, there still exist fundamental issues in existing ODL methods. In particular, current ODL methods tend to consider model construction and learning as two separate phases, and thus fail to formulate their underlying coupling and depending relationship. In this work, we first establish a new framework, named Hierarchical ODL (HODL), to simultaneously investigate the intrinsic behaviors of optimization-derived model construction and its corresponding learning process. Then we rigorously prove the joint convergence of these two sub-tasks, from the perspectives of both approximation quality and stationary analysis. To our best knowledge, this is the first theoretical guarantee for these two cou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#20174;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#23433;&#20840;&#20219;&#21153;&#8212;&#8212;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;SVEN&#65292;&#23454;&#29616;&#29983;&#25104;&#26082;&#23433;&#20840;&#21448;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#23545;&#24403;&#21069;&#30340;LM&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05319</link><description>&lt;p&gt;
&#29992;&#20110;&#32534;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Code: Security Hardening and Adversarial Testing. (arXiv:2302.05319v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#20174;&#23433;&#20840;&#21152;&#22266;&#21644;&#23545;&#25239;&#27979;&#35797;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#23433;&#20840;&#20219;&#21153;&#8212;&#8212;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;SVEN&#65292;&#23454;&#29616;&#29983;&#25104;&#26082;&#23433;&#20840;&#21448;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#23545;&#24403;&#21069;&#30340;LM&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#39044;&#20808;&#22312;&#22823;&#35268;&#27169;&#20195;&#30721;&#24211;&#19978;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#29992;&#20110;&#29983;&#25104;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;LM&#32570;&#20047;&#23433;&#20840;&#24847;&#35782;&#65292;&#24182;&#32463;&#24120;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#12290;&#26412;&#30740;&#31350;&#27839;&#30528;&#20004;&#20010;&#37325;&#35201;&#26041;&#21521;&#30740;&#31350;&#20102;LM&#30340;&#23433;&#20840;&#24615;:(i)&#23433;&#20840;&#21152;&#22266;&#65292;&#26088;&#22312;&#22686;&#24378;LM&#22312;&#29983;&#25104;&#23433;&#20840;&#20195;&#30721;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;;(ii)&#23545;&#25239;&#27979;&#35797;&#65292;&#26088;&#22312;&#22312;&#23545;&#25239;&#24615;&#31435;&#22330;&#35780;&#20272;LM&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21046;&#23450;&#19968;&#39033;&#31216;&#20026;&#21463;&#25511;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#23433;&#20840;&#20219;&#21153;&#26469;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#26159;&#21442;&#25968;&#21270;&#30340;&#65292;&#23558;&#19968;&#20010;&#20108;&#36827;&#21046;&#23646;&#24615;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#25351;&#23548;LM&#29983;&#25104;&#23433;&#20840;&#25110;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#65292;&#21516;&#26102;&#20445;&#30041;LM&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SVEN&#30340;&#26032;&#22411;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;SVEN&#21033;&#29992;&#23646;&#24615;&#29305;&#23450;&#30340;&#36830;&#32493;&#21521;&#37327;&#26469;&#24341;&#23548;&#31243;&#24207;&#29983;&#25104;&#36798;&#21040;&#32473;&#23450;&#30340;&#23646;&#24615;&#65292;&#32780;&#19981;&#20462;&#25913;LM&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#25237;&#24433;&#25439;&#22833;&#26469;&#20248;&#21270;&#36825;&#20123;&#36830;&#32493;&#21521;&#37327;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;SVEN&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#65292;&#24182;&#34920;&#26126;&#24403;&#21069;&#30340;LM&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22312;&#27979;&#35797;&#26102;&#20462;&#25913;&#23427;&#20204;&#30340;&#36755;&#20837;&#32780;&#20445;&#30041;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#38656;&#35201;&#22312;LM&#30340;&#22521;&#35757;&#21644;&#35780;&#20272;&#20013;&#32771;&#34385;&#23433;&#20840;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are increasingly pretrained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous ve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21322;&#32676;&#20013;&#30340;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#23558;&#26410;&#30693;&#33258;&#20027;&#21160;&#21147;&#31995;&#32479;&#24314;&#27169;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28382;&#21518;&#19979;&#25910;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#21487;&#21464;&#26102;&#38388;&#27493;&#38271;&#30340;&#28436;&#21270;&#31639;&#31526;&#65292;&#26500;&#25104;&#33258;&#20027;&#31995;&#32479;&#30340;&#21322;&#32676;&#12290;</title><link>http://arxiv.org/abs/2302.03358</link><description>&lt;p&gt;
Deep-OSG&#65306;&#21322;&#32676;&#20013;&#30340;&#36816;&#31639;&#31526;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep-OSG: Deep Learning of Operators in Semigroup. (arXiv:2302.03358v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21322;&#32676;&#20013;&#30340;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#23558;&#26410;&#30693;&#33258;&#20027;&#21160;&#21147;&#31995;&#32479;&#24314;&#27169;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28382;&#21518;&#19979;&#25910;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#21487;&#21464;&#26102;&#38388;&#27493;&#38271;&#30340;&#28436;&#21270;&#31639;&#31526;&#65292;&#26500;&#25104;&#33258;&#20027;&#31995;&#32479;&#30340;&#21322;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21322;&#32676;&#20013;&#30340;&#36816;&#31639;&#31526;&#65292;&#24182;&#24212;&#29992;&#20110;&#20351;&#29992;&#22312;&#19981;&#21516;&#26102;&#38388;&#28382;&#21518;&#19979;&#25910;&#38598;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23545;&#26410;&#30693;&#33258;&#20027;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#26159;&#20043;&#21069;&#27969;&#22270;&#23398;&#20064;(FML)&#24037;&#20316;&#30340;&#32493;&#38598;&#65292;&#35813;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#20855;&#26377;&#22266;&#23450;&#26102;&#38388;&#27493;&#38271;&#30340;&#21333;&#19968;&#28436;&#21270;&#31639;&#31526;&#12290;&#26412;&#25991;&#26088;&#22312;&#23398;&#20064;&#19968;&#26063;&#20855;&#26377;&#21487;&#21464;&#26102;&#38388;&#27493;&#38271;&#30340;&#28436;&#21270;&#31639;&#31526;&#65292;&#26500;&#25104;&#33258;&#20027;&#31995;&#32479;&#30340;&#21322;&#32676;&#12290;&#21322;&#32676;&#24615;&#36136;&#23545;&#20110;&#32852;&#31995;&#31995;&#32479;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#28436;&#21270;&#34892;&#20026;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#23558;&#21322;&#32676;&#24615;&#36136;&#23884;&#20837;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel deep learning approach for learning operators in semigroup, with applications to modeling unknown autonomous dynamical systems using time series data collected at varied time lags. It is a sequel to the previous flow map learning (FML) works [T. Qin, K. Wu, and D. Xiu, J. Comput. Phys., 395:620--635, 2019], [K. Wu and D. Xiu, J. Comput. Phys., 408:109307, 2020], and [Z. Chen, V. Churchill, K. Wu, and D. Xiu, J. Comput. Phys., 449:110782, 2022], which focused on learning single evolution operator with a fixed time step. This paper aims to learn a family of evolution operators with variable time steps, which constitute a semigroup for an autonomous system. The semigroup property is very crucial and links the system's evolutionary behaviors across varying time scales, but it was not considered in the previous works. We propose for the first time a framework of embedding the semigroup property into the data-driven learning process, through a novel neural network
&lt;/p&gt;</description></item><item><title>SGID&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24179;&#34913;&#22270;&#20687;&#22810;&#26679;&#24615;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SGID&#22312;ResNet-5&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#26368;&#20339;&#22686;&#24378;&#22522;&#32447;1.72&#65285;&#12290;</title><link>http://arxiv.org/abs/2302.02070</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Semantic-Guided Generative Image Augmentation Method with Diffusion Models for Image Classification. (arXiv:2302.02070v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02070
&lt;/p&gt;
&lt;p&gt;
SGID&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24179;&#34913;&#22270;&#20687;&#22810;&#26679;&#24615;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SGID&#22312;ResNet-5&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#26368;&#20339;&#22686;&#24378;&#22522;&#32447;1.72&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#20998;&#20026;&#20004;&#31867;&#65306;&#25200;&#21160;&#26041;&#27861;&#21644;&#29983;&#25104;&#26041;&#27861;&#12290;&#25200;&#21160;&#26041;&#27861;&#23545;&#21407;&#22987;&#22270;&#20687;&#24212;&#29992;&#39044;&#23450;&#20041;&#30340;&#25200;&#21160;&#26469;&#22686;&#24378;&#22270;&#20687;&#65292;&#20294;&#21482;&#26159;&#23616;&#37096;&#21464;&#21270;&#22270;&#20687;&#65292;&#22240;&#27492;&#32570;&#20047;&#22270;&#20687;&#22810;&#26679;&#24615;&#12290;&#30456;&#21453;&#65292;&#29983;&#25104;&#26041;&#27861;&#22312;&#22686;&#24378;&#22270;&#20687;&#20013;&#24102;&#26469;&#26356;&#22810;&#30340;&#22270;&#20687;&#22810;&#26679;&#24615;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#38169;&#35823;&#22320;&#25913;&#21464;&#20102;&#21407;&#22987;&#22270;&#20687;&#30340;&#22522;&#26412;&#35821;&#20041;&#12290;&#20026;&#20102;&#22312;&#22686;&#24378;&#22270;&#20687;&#20013;&#24179;&#34913;&#22270;&#20687;&#22810;&#26679;&#24615;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SGID&#65292;&#19968;&#31181;&#35821;&#20041;&#24341;&#23548;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SGID&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#22270;&#20687;&#22810;&#26679;&#24615;&#30340;&#22686;&#24378;&#22270;&#20687;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;SGID&#20197;&#22270;&#20687;&#26631;&#31614;&#21644;&#26631;&#39064;&#20316;&#20026;&#25351;&#23548;&#65292;&#20197;&#20445;&#25345;&#22686;&#24378;&#21644;&#21407;&#22987;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SGID&#22312;ResNet-5&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20339;&#22686;&#24378;&#22522;&#32447;1.72&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing image augmentation methods consist of two categories: perturbation-based methods and generative methods. Perturbation-based methods apply pre-defined perturbations to augment an original image, but only locally vary the image, thus lacking image diversity. In contrast, generative methods bring more image diversity in the augmented images but may not preserve semantic consistency, thus incorrectly changing the essential semantics of the original image. To balance image diversity and semantic consistency in augmented images, we propose SGID, a Semantic-guided Generative Image augmentation method with Diffusion models for image classification. Specifically, SGID employs diffusion models to generate augmented images with good image diversity. More importantly, SGID takes image labels and captions as guidance to maintain semantic consistency between the augmented and original images. Experimental results show that SGID outperforms the best augmentation baseline by 1.72% on ResNet-5
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#27169;&#22411;&#20272;&#35745;&#30340;&#40065;&#26834;MDPs&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#38382;&#39064;&#36716;&#21270;&#20026;&#21478;&#19968;&#31181;&#24418;&#24335;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#27714;&#35299;&#65292;&#20174;&#32780;&#21435;&#38500;&#20102;&#23545;&#20248;&#21270;&#22120;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2302.01248</link><description>&lt;p&gt;
&#26080;&#38656;&#27169;&#22411;&#20272;&#35745;&#30340;&#40065;&#26834;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Robust Markov Decision Processes without Model Estimation. (arXiv:2302.01248v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#27169;&#22411;&#20272;&#35745;&#30340;&#40065;&#26834;MDPs&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#38382;&#39064;&#36716;&#21270;&#20026;&#21478;&#19968;&#31181;&#24418;&#24335;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#27714;&#35299;&#65292;&#20174;&#32780;&#21435;&#38500;&#20102;&#23545;&#20248;&#21270;&#22120;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#22312;&#23398;&#20064;&#19968;&#20010;&#23545;&#29615;&#22659;&#21464;&#21270;&#19981;&#25935;&#24863;&#30340;&#40065;&#26834;&#31574;&#30053;&#26041;&#38754;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#24037;&#20316;&#20998;&#26512;&#40065;&#26834;MDPs&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24212;&#29992;&#40065;&#26834;MDPs&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#39318;&#20808;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#26159;&#22312;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#40065;&#26834;MDPs&#65292;&#20854;&#20013;&#36716;&#31227;&#27010;&#29575;&#38656;&#35201;&#36827;&#34892;&#20272;&#35745;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35760;&#24518;&#65288;O(|S|&#178;|A|)&#65289;&#12290;&#20854;&#27425;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#24378;&#22823;&#30340;&#20248;&#21270;&#22120;&#26469;&#33719;&#24471;&#26368;&#20248;&#35299;&#65292;&#29992;&#20316;&#35299;&#20915;&#40065;&#26834;MDPs&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#24182;&#19981;&#23384;&#22312;&#36825;&#26679;&#30340;&#20248;&#21270;&#22120;&#12290;&#20026;&#20102;&#21435;&#38500;&#20248;&#21270;&#22120;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#30340;&#40065;&#26834;MDPs&#36716;&#21270;&#20026;&#21478;&#19968;&#31181;&#24418;&#24335;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#26469;&#27714;&#35299;&#40065;&#26834;MDPs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26367;&#20195;&#24418;&#24335;&#20173;&#28982;&#20855;&#26377;&#31867;&#20284;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#36825;&#31181;&#26032;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#37319;&#26679;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#40065;&#26834;MDPs&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Markov Decision Processes (MDPs) are receiving much attention in learning a robust policy which is less sensitive to environment changes. There are an increasing number of works analyzing sample-efficiency of robust MDPs. However, there are two major barriers to applying robust MDPs in practice. First, most works study robust MDPs in a model-based regime, where the transition probability needs to be estimated and requires a large amount of memories $\mathcal{O}(|\mathcal{S}|^2|\mathcal{A}|)$. Second, prior work typically assumes a strong oracle to obtain the optimal solution as an intermediate step to solve robust MDPs. However, in practice, such an oracle does not exist usually. To remove the oracle, we transform the original robust MDPs into an alternative form, which allows us to use stochastic gradient methods to solve the robust MDPs. Moreover, we prove the alternative form still plays a similar role as the original form. With this new formulation, we devise a sample-effici
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#20256;&#32479;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#22312;&#24314;&#27169;&#20986;&#34892;&#26041;&#24335;&#36873;&#25321;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#29305;&#28857;&#65292;&#24182;&#30830;&#23450;&#21738;&#20123;&#27169;&#22411;&#26368;&#36866;&#21512;&#20110;&#24314;&#27169;&#65292;&#20986;&#34892;&#34892;&#20026;&#21487;&#35299;&#37322;&#24615;&#21644;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25968;&#25454;&#25928;&#29575;&#31561;&#22240;&#32032;&#38656;&#35201;&#36827;&#34892;&#25972;&#20307;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2301.04404</link><description>&lt;p&gt;
&#23545;&#20986;&#34892;&#26041;&#24335;&#36873;&#25321;&#24314;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#39044;&#27979;&#21644;&#34892;&#20026;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A prediction and behavioural analysis of machine learning methods for modelling travel mode choice. (arXiv:2301.04404v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#20256;&#32479;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#22312;&#24314;&#27169;&#20986;&#34892;&#26041;&#24335;&#36873;&#25321;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#29305;&#28857;&#65292;&#24182;&#30830;&#23450;&#21738;&#20123;&#27169;&#22411;&#26368;&#36866;&#21512;&#20110;&#24314;&#27169;&#65292;&#20986;&#34892;&#34892;&#20026;&#21487;&#35299;&#37322;&#24615;&#21644;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25968;&#25454;&#25928;&#29575;&#31561;&#22240;&#32032;&#38656;&#35201;&#36827;&#34892;&#25972;&#20307;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20986;&#34892;&#26041;&#24335;&#36873;&#25321;&#38382;&#39064;&#65292;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20986;&#29616;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#21738;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#21738;&#20123;&#24212;&#29992;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#19981;&#20165;&#20165;&#22312;&#20110;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#28041;&#21450;&#34892;&#20026;&#21487;&#35299;&#37322;&#24615;&#21644;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25968;&#25454;&#25928;&#29575;&#31561;&#22810;&#26041;&#38754;&#30340;&#24179;&#34913;&#12290;&#35768;&#22810;&#30740;&#31350;&#35797;&#22270;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#19982;&#20256;&#32479;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#21482;&#20998;&#26512;&#31163;&#25955;&#39044;&#27979;&#24615;&#33021;&#65292;&#24573;&#30053;&#20854;&#20182;&#24433;&#21709;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#30740;&#31350;&#21463;&#25216;&#26415;&#38480;&#21046;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#20351;&#29992;&#19981;&#24688;&#24403;&#30340;&#39564;&#35777;&#26041;&#26696;&#12289;&#20998;&#23618;&#25968;&#25454;&#30340;&#38169;&#35823;&#25277;&#26679;&#12289;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#20197;&#21450;&#20165;&#20351;&#29992;&#31163;&#25955;&#24230;&#37327;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#34892;&#20026;&#20998;&#26512;&#65292;&#26088;&#22312;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#21644;&#29305;&#28857;&#65292;&#20197;&#20415;&#30830;&#23450;&#26368;&#36866;&#21512;&#24314;&#27169;&#20986;&#34892;&#36873;&#25321;&#30340;&#27169;&#22411;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of a variety of Machine Learning (ML) approaches for travel mode choice prediction poses an interesting question to transport modellers: which models should be used for which applications? The answer to this question goes beyond simple predictive performance, and is instead a balance of many factors, including behavioural interpretability and explainability, computational complexity, and data efficiency. There is a growing body of research which attempts to compare the predictive performance of different ML classifiers with classical random utility models. However, existing studies typically analyse only the disaggregate predictive performance, ignoring other aspects affecting model choice. Furthermore, many studies are affected by technical limitations, such as the use of inappropriate validation schemes, incorrect sampling for hierarchical data, lack of external validation, and the exclusive use of discrete metrics. We address these limitations by conducting a systemati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GAN&#30340;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#24182;&#25552;&#20986;&#20102;StyleGAN&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#26032;&#30340;&#39640;&#25928;&#36731;&#37327;&#32423;&#21442;&#25968;&#21270;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2212.10229</link><description>&lt;p&gt;
StyleDomain&#65306;&#29992;&#20110;&#21333;&#27425;&#21644;&#23569;&#27425;&#39046;&#22495;&#36866;&#24212;&#30340;StyleGAN&#39640;&#25928;&#36731;&#37327;&#21270;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation. (arXiv:2212.10229v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GAN&#30340;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#24182;&#25552;&#20986;&#20102;StyleGAN&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#26032;&#30340;&#39640;&#25928;&#36731;&#37327;&#32423;&#21442;&#25968;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GAN&#30340;&#39046;&#22495;&#36866;&#24212;&#26159;fine-tuning&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#20808;&#36827;GAN&#27169;&#22411;&#65288;&#20363;&#22914;StyleGAN&#65289;&#20197;&#36866;&#24212;&#20855;&#26377;&#23569;&#37327;&#26679;&#26412;&#30340;&#29305;&#23450;&#39046;&#22495;&#65288;&#20363;&#22914;&#32472;&#30011;&#38754;&#23380;&#12289;&#32032;&#25551;&#31561;&#65289;&#12290;&#26412;&#25991;&#23545;GAN&#30340;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#65288;&#29305;&#21035;&#26159;StyleGAN&#27169;&#22411;&#65289;&#36827;&#34892;&#31995;&#32479;&#21644;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;StyleGAN&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#26032;&#30340;&#39640;&#25928;&#36731;&#37327;&#32423;&#21442;&#25968;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation of GANs is a problem of fine-tuning the state-of-the-art GAN models (e.g. StyleGAN) pretrained on a large dataset to a specific domain with few samples (e.g. painting faces, sketches, etc.). While there are a great number of methods that tackle this problem in different ways, there are still many important questions that remain unanswered.  In this paper, we provide a systematic and in-depth analysis of the domain adaptation problem of GANs, focusing on the StyleGAN model. First, we perform a detailed exploration of the most important parts of StyleGAN that are responsible for adapting the generator to a new domain depending on the similarity between the source and target domains. As a result of this in-depth study, we propose new efficient and lightweight parameterizations of StyleGAN for domain adaptation. Particularly, we show there exist directions in StyleSpace (StyleDomain directions) that are sufficient for adapting to similar domains and they can be reduced fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31454;&#36187;&#30340;&#29616;&#29366;&#21644;&#21442;&#19982;&#23454;&#36341;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#65292;&#21442;&#19982;&#32773;&#20027;&#35201;&#21160;&#26426;&#26159;&#30693;&#35782;&#20132;&#27969;&#65292;&#32780;&#22870;&#37329;&#30340;&#33719;&#24471;&#21482;&#36215;&#21040;&#27425;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.08568</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31454;&#36187;&#65306;&#24403;&#21069;&#21442;&#19982;&#23454;&#36341;&#30340;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
Biomedical image analysis competitions: The state of current participation practice. (arXiv:2212.08568v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31454;&#36187;&#30340;&#29616;&#29366;&#21644;&#21442;&#19982;&#23454;&#36341;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#65292;&#21442;&#19982;&#32773;&#20027;&#35201;&#21160;&#26426;&#26159;&#30693;&#35782;&#20132;&#27969;&#65292;&#32780;&#22870;&#37329;&#30340;&#33719;&#24471;&#21482;&#36215;&#21040;&#27425;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30740;&#31350;&#21644;&#23454;&#36341;&#39046;&#22495;&#65292;&#22269;&#38469;&#22522;&#20934;&#31454;&#36187;&#30340;&#25968;&#37327;&#22312;&#31283;&#27493;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#23545;&#20110;&#31038;&#21306;&#22312;&#24212;&#23545;&#25552;&#20986;&#30340;&#30740;&#31350;&#38382;&#39064;&#26102;&#30340;&#24120;&#35265;&#23454;&#36341;&#21644;&#36935;&#21040;&#30340;&#29942;&#39048;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#25581;&#31034;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#31639;&#27861;&#24320;&#21457;&#30340;&#29616;&#29366;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#39033;&#22269;&#38469;&#35843;&#26597;&#65292;&#35813;&#35843;&#26597;&#38024;&#23545;&#19982;IEEE ISBI 2021&#21644;MICCAI 2021&#20250;&#35758;&#65288;&#24635;&#20849;80&#20010;&#31454;&#36187;&#65289;&#30456;&#20851;&#30340;&#25361;&#25112;&#30340;&#25152;&#26377;&#21442;&#19982;&#32773;&#36827;&#34892;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;&#21442;&#19982;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#24037;&#20316;&#29615;&#22659;&#12289;&#20182;&#20204;&#36873;&#25321;&#30340;&#31574;&#30053;&#20197;&#21450;&#31639;&#27861;&#29305;&#24615;&#12290;&#20854;&#20013;&#26377;&#20013;&#20301;&#25968;&#20026;72%&#30340;&#31454;&#36187;&#21442;&#19982;&#32773;&#21442;&#19982;&#20102;&#36825;&#39033;&#35843;&#26597;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#30693;&#35782;&#20132;&#27969;&#26159;&#21442;&#19982;&#30340;&#20027;&#35201;&#21160;&#26426;&#65288;70%&#65289;&#65292;&#32780;&#22870;&#37329;&#30340;&#33719;&#24471;&#21482;&#36215;&#21040;&#27425;&#35201;&#20316;&#29992;&#65288;16%&#65289;&#12290;&#21442;&#19982;&#32773;&#24179;&#22343;&#33457;&#36153;80&#20010;&#24037;&#20316;&#23567;&#26102;&#36827;&#34892;&#31639;&#27861;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of international benchmarking competitions is steadily increasing in various fields of machine learning (ML) research and practice. So far, however, little is known about the common practice as well as bottlenecks faced by the community in tackling the research questions posed. To shed light on the status quo of algorithm development in the specific field of biomedical imaging analysis, we designed an international survey that was issued to all participants of challenges conducted in conjunction with the IEEE ISBI 2021 and MICCAI 2021 conferences (80 competitions in total). The survey covered participants' expertise and working environments, their chosen strategies, as well as algorithm characteristics. A median of 72% challenge participants took part in the survey. According to our results, knowledge exchange was the primary incentive (70%) for participation, while the reception of prize money played only a minor role (16%). While a median of 80 working hours was spent on m
&lt;/p&gt;</description></item><item><title>ROSCOE&#26159;&#19968;&#22871;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20998;&#36880;&#27493;&#25512;&#29702;&#30340;&#27491;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;&#23427;&#21487;&#20197;&#34913;&#37327;&#35821;&#20041;&#19968;&#33268;&#24615;&#12289;&#36923;&#36753;&#24615;&#12289;&#20449;&#24687;&#37327;&#12289;&#27969;&#30021;&#24230;&#21644;&#20107;&#23454;&#31561;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.07919</link><description>&lt;p&gt;
ROSCOE: &#29992;&#20110;&#35780;&#20998;&#36880;&#27493;&#25512;&#29702;&#30340;&#19968;&#22871;&#24230;&#37327;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. (arXiv:2212.07919v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07919
&lt;/p&gt;
&lt;p&gt;
ROSCOE&#26159;&#19968;&#22871;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20998;&#36880;&#27493;&#25512;&#29702;&#30340;&#27491;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;&#23427;&#21487;&#20197;&#34913;&#37327;&#35821;&#20041;&#19968;&#33268;&#24615;&#12289;&#36923;&#36753;&#24615;&#12289;&#20449;&#24687;&#37327;&#12289;&#27969;&#30021;&#24230;&#21644;&#20107;&#23454;&#31561;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35201;&#27714;&#29983;&#25104;&#36880;&#27493;&#25512;&#29702;&#26469;&#35299;&#37322;&#20854;&#26368;&#32456;&#31572;&#26696;&#26102;&#65292;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19978;&#26174;&#31034;&#20986;&#20102;&#25913;&#36827;&#12290;&#36825;&#20123;&#25512;&#29702;&#27493;&#39588;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39564;&#35777;&#24615;&#65292;&#20294;&#22312;&#27809;&#26377;&#21487;&#38752;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#29420;&#31435;&#20110;&#26368;&#32456;&#31572;&#26696;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#27491;&#30830;&#24615;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#24182;&#19981;&#30693;&#36947;&#25152;&#36848;&#30340;&#25512;&#29702;&#27493;&#39588;&#23454;&#38469;&#19978;&#26377;&#22810;&#23569;&#25903;&#25345;&#26368;&#32456;&#20219;&#21153;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROSCOE&#65292;&#36825;&#26159;&#19968;&#22871;&#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#35780;&#20998;&#25351;&#26631;&#65292;&#23427;&#25913;&#36827;&#24182;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#12290;&#20026;&#20102;&#35780;&#20272;ROSCOE&#19982;&#22522;&#32447;&#25351;&#26631;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25512;&#29702;&#38169;&#35823;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#24120;&#29992;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#25910;&#38598;&#20102;&#21512;&#25104;&#21644;&#20154;&#24037;&#35780;&#20272;&#24471;&#20998;&#12290;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#27604;&#65292;ROSCOE&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36880;&#27493;&#25512;&#29702;&#30340;&#29305;&#24615;&#26469;&#34913;&#37327;&#35821;&#20041;&#19968;&#33268;&#24615;&#12289;&#36923;&#36753;&#24615;&#12289;&#20449;&#24687;&#37327;&#12289;&#27969;&#30021;&#24230;&#21644;&#20107;&#23454;&#31561;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#26469;&#26356;&#24555;&#22320;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2211.16691</link><description>&lt;p&gt;
&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules. (arXiv:2211.16691v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#26469;&#26356;&#24555;&#22320;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#31351;&#20030;&#25506;&#32034;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20197;&#25214;&#21040;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#32780;&#23548;&#33268;&#26679;&#26412;&#22797;&#26434;&#24230;&#19981;&#22826;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#31995;&#32479;&#30340;&#19987;&#23478;&#30693;&#35782;&#36890;&#24120;&#20801;&#35768;&#25105;&#20204;&#35774;&#35745;&#31616;&#21333;&#35268;&#21017;&#65292;&#25105;&#20204;&#26399;&#26395;&#33391;&#22909;&#30340;&#31574;&#30053;&#22987;&#32456;&#36981;&#24490;&#36825;&#20123;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36830;&#32493;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#20197;&#32435;&#20837;&#36825;&#20123;&#35268;&#21017;&#24182;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#65292;&#20174;&#32780;&#26174;&#30528;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25913;&#36827;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#20195;&#29702;&#31243;&#24207;&#36873;&#25321;&#30340;&#21160;&#20316;&#19981;&#31526;&#21512;&#25105;&#20204;&#30340;&#30452;&#35273;&#65292;&#25105;&#20204;&#20250;&#39281;&#21644;&#36825;&#20123;&#21160;&#20316;&#65292;&#20851;&#38190;&#26159;&#20462;&#25913;&#31574;&#30053;&#30340;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#27969;&#31243;&#19981;&#21463;&#39281;&#21644;&#27493;&#39588;&#30340;&#24433;&#21709;&#12290;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#23427;&#20351;&#20195;&#29702;&#31243;&#24207;&#20197;&#27604;&#20256;&#32479;&#20195;&#29702;&#31243;&#24207;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#21040;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#28040;&#32791;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) generally suffers from poor sample complexity, mostly due to the need to exhaustively explore the state-action space to find well-performing policies. On the other hand, we postulate that expert knowledge of the system often allows us to design simple rules we expect good policies to follow at all times. In this work, we hence propose a simple yet effective modification of continuous actor-critic frameworks to incorporate such rules and avoid regions of the state-action space that are known to be suboptimal, thereby significantly accelerating the convergence of RL agents. Concretely, we saturate the actions chosen by the agent if they do not comply with our intuition and, critically, modify the gradient update step of the policy to ensure the learning process is not affected by the saturation step. On a room temperature control case study, it allows agents to converge to well-performing policies up to 6-7x faster than classical agents without computational o
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23545;&#28145;&#24230;&#22270;&#32858;&#31867;&#36827;&#34892;&#20102;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#65292;&#24182;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2211.12875</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#32858;&#31867;&#32508;&#36848;&#65306;&#20998;&#31867;&#12289;&#25361;&#25112;&#12289;&#24212;&#29992;&#21644;&#24320;&#25918;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Graph Clustering: Taxonomy, Challenge, Application, and Open Resource. (arXiv:2211.12875v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12875
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23545;&#28145;&#24230;&#22270;&#32858;&#31867;&#36827;&#34892;&#20102;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#65292;&#24182;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26088;&#22312;&#23558;&#22270;&#20013;&#30340;&#33410;&#28857;&#21010;&#20998;&#20026;&#33509;&#24178;&#19981;&#21516;&#30340;&#31751;&#65292;&#26159;&#19968;&#39033;&#22522;&#30784;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#20511;&#21161;&#28145;&#24230;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#32508;&#36848;&#35770;&#25991;&#30456;&#23545;&#36739;&#23569;&#65292;&#32508;&#36848;&#35813;&#39046;&#22495;&#30340;&#24037;&#20316;&#21183;&#22312;&#24517;&#34892;&#12290;&#22522;&#20110;&#27492;&#21160;&#26426;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#30340;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#20844;&#24335;&#21270;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#22270;&#31867;&#22411;&#12289;&#32593;&#32476;&#26550;&#26500;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#32858;&#31867;&#26041;&#27861;&#31561;&#22235;&#20010;&#19981;&#21516;&#30340;&#26631;&#20934;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#20174;&#22270;&#25968;&#25454;&#36136;&#37327;&#12289;&#31283;&#23450;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#36776;&#21035;&#33021;&#21147;&#21644;&#26410;&#30693;&#31751;&#25968;&#31561;&#20116;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering, which aims to divide nodes in the graph into several distinct clusters, is a fundamental yet challenging task. Benefiting from the powerful representation capability of deep learning, deep graph clustering methods have achieved great success in recent years. However, the corresponding survey paper is relatively scarce, and it is imminent to make a summary of this field. From this motivation, we conduct a comprehensive survey of deep graph clustering. Firstly, we introduce formulaic definition, evaluation, and development in this field. Secondly, the taxonomy of deep graph clustering methods is presented based on four different criteria, including graph type, network architecture, learning paradigm, and clustering method. Thirdly, we carefully analyze the existing methods via extensive experiments and summarize the challenges and opportunities from five perspectives, including graph data quality, stability, scalability, discriminative capability, and unknown cluster nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20840;&#26032;&#30340;K-FACs&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20998;&#35299;&#26356;&#26032;&#26469;&#21152;&#36895;K-FAC&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#36825;&#31181;&#31639;&#27861;&#27604;&#20256;&#32479;&#30340;K-FAC&#21644;RS-KFAC&#26356;&#24265;&#20215;&#65292;&#34429;&#28982;&#31934;&#24230;&#36739;&#20302;&#65292;&#20294;&#22312;&#39044;&#35843;&#33410;&#37096;&#20998;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#21482;&#26377;&#32447;&#24615;&#32553;&#25918;&#12290;</title><link>http://arxiv.org/abs/2210.08494</link><description>&lt;p&gt;
&#20840;&#26032;&#30340;K-FACs&#65306;&#21033;&#29992;&#22312;&#32447;&#20998;&#35299;&#26356;&#26032;&#21152;&#36895;K-FAC
&lt;/p&gt;
&lt;p&gt;
Brand New K-FACs: Speeding up K-FAC with Online Decomposition Updates. (arXiv:2210.08494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20840;&#26032;&#30340;K-FACs&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20998;&#35299;&#26356;&#26032;&#26469;&#21152;&#36895;K-FAC&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#36825;&#31181;&#31639;&#27861;&#27604;&#20256;&#32479;&#30340;K-FAC&#21644;RS-KFAC&#26356;&#24265;&#20215;&#65292;&#34429;&#28982;&#31934;&#24230;&#36739;&#20302;&#65292;&#20294;&#22312;&#39044;&#35843;&#33410;&#37096;&#20998;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#21482;&#26377;&#32447;&#24615;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
K-FAC&#26159;Deep Learning&#20013;&#21487;&#34892;&#30340;Natural Gradient&#30340;&#23454;&#29616;&#65292;&#20854;&#29942;&#39048;&#22312;&#20110;&#35745;&#31639;&#25152;&#35859;&#30340;&#8220;Kronecker-Factors&#8221;&#65288;K-factors&#65289;&#30340;&#36870;&#12290;RS-KFAC&#26159;K-FAC&#30340;&#25913;&#36827;&#65292;&#25552;&#20379;&#20102;&#20272;&#35745;K-factors&#36870;&#30340;&#19968;&#31181;&#24265;&#20215;&#26041;&#27861;&#12290;&#26412;&#25991;&#21033;&#29992;K-factors&#30340;&#25351;&#25968;&#24179;&#22343;&#26500;&#36896;&#33539;&#24335;&#65292;&#24182;&#20351;&#29992;&#22312;&#32447;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#24265;&#20215;&#65288;&#20294;&#31934;&#24230;&#36739;&#20302;&#65289;&#30340;&#20272;&#35745;K-factors&#36870;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#23618;&#22823;&#23567;&#19978;&#32447;&#24615;&#32553;&#25918;&#30340;K-factor&#36870;&#26356;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#36870;&#24212;&#29992;&#36807;&#31243;&#20013;&#20063;&#32447;&#24615;&#32553;&#25918;&#30340;&#26041;&#27861;&#65288;K-FAC&#30340;&#36870;&#24212;&#29992;&#36807;&#31243;&#26159;&#19977;&#27425;&#26041;&#32553;&#25918;&#30340;&#65292;RS-KFAC&#30340;&#26159;&#20108;&#27425;&#32553;&#25918;&#30340;&#65289;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#36817;&#20284;&#30340;K-FAC&#23454;&#29616;&#65292;&#20854;&#39044;&#35843;&#33410;&#37096;&#20998;&#22312;&#23618;&#22823;&#23567;&#19978;&#32447;&#24615;&#32553;&#25918;&#65288;&#19982;K-FAC&#30340;&#19977;&#27425;&#26041;&#21644;RS-KFAC&#30340;&#20108;&#27425;&#26041;&#30456;&#27604;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
K-FAC (arXiv:1503.05671, arXiv:1602.01407) is a tractable implementation of Natural Gradient (NG) for Deep Learning (DL), whose bottleneck is computing the inverses of the so-called ``Kronecker-Factors'' (K-factors). RS-KFAC (arXiv:2206.15397) is a K-FAC improvement which provides a cheap way of estimating the K-factors inverses.  In this paper, we exploit the exponential-average construction paradigm of the K-factors, and use online numerical linear algebra techniques to propose an even cheaper (but less accurate) way of estimating the K-factors inverses. In particular, we propose a K-factor inverse update which scales linearly in layer size. We also propose an inverse application procedure which scales linearly as well (the one of K-FAC scales cubically and the one of RS-KFAC scales quadratically). Overall, our proposed algorithm gives an approximate K-FAC implementation whose preconditioning part scales linearly in layer size (compare to cubic for K-FAC and quadratic for RS-KFAC). I
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;&#21453;&#22240;&#26524;&#39044;&#27979;&#20013;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#20998;&#31163;&#20934;&#21017;&#24212;&#29992;&#20110;&#21453;&#22240;&#26524;&#35774;&#32622;&#30340;&#26032;&#21160;&#26426;&#12290;&#35813;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20197;&#40065;&#26834;&#24615;&#20026;&#39537;&#21160;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#23454;&#29616;&#20998;&#31163;&#65292;&#19988;&#36890;&#24120;&#27604;&#30452;&#25509;&#35774;&#35745;&#20844;&#24179;&#24615;&#26041;&#27861;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2209.09423</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#22312;&#21453;&#22240;&#26524;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fairness and robustness in anti-causal prediction. (arXiv:2209.09423v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09423
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;&#21453;&#22240;&#26524;&#39044;&#27979;&#20013;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#20998;&#31163;&#20934;&#21017;&#24212;&#29992;&#20110;&#21453;&#22240;&#26524;&#35774;&#32622;&#30340;&#26032;&#21160;&#26426;&#12290;&#35813;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20197;&#40065;&#26834;&#24615;&#20026;&#39537;&#21160;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#23454;&#29616;&#20998;&#31163;&#65292;&#19988;&#36890;&#24120;&#27604;&#30452;&#25509;&#35774;&#35745;&#20844;&#24179;&#24615;&#26041;&#27861;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#20316;&#20026;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#35201;&#27714;&#65292;&#29420;&#31435;&#22320;&#20986;&#29616;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#35201;&#27714;&#20284;&#20046;&#30456;&#20851;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#23427;&#20204;&#20043;&#38388;&#30340;&#36830;&#25509;&#24120;&#24120;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#30340;&#35270;&#35282;&#35752;&#35770;&#36825;&#20123;&#36830;&#25509;&#65292;&#37325;&#28857;&#20851;&#27880;&#21453;&#22240;&#26524;&#39044;&#27979;&#20219;&#21153;&#65292;&#20854;&#20013;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#34987;&#20551;&#23450;&#20026;&#30001;&#30446;&#26631;&#26631;&#31614;&#21644;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20989;&#25968;&#29983;&#25104;&#12290;&#36890;&#36807;&#37319;&#21462;&#36825;&#31181;&#35270;&#35282;&#65292;&#25105;&#20204;&#26126;&#30830;&#20102;&#19968;&#20010;&#24120;&#35265;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017; - &#20998;&#31163;&#24615; - &#19982;&#19968;&#31181;&#24120;&#35265;&#30340;&#40065;&#26834;&#24615;&#27010;&#24565; - &#39118;&#38505;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#36825;&#20123;&#36830;&#25509;&#20026;&#22312;&#21453;&#22240;&#26524;&#35774;&#32622;&#20013;&#24212;&#29992;&#20998;&#31163;&#20934;&#21017;&#25552;&#20379;&#20102;&#26032;&#30340;&#21160;&#26426;&#65292;&#24182;&#20026;&#20851;&#20110;&#20844;&#24179;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26087;&#35752;&#35770;&#25552;&#20379;&#20102;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#40065;&#26834;&#24615;&#39537;&#21160;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#23454;&#29616;&#20998;&#31163;&#65292;&#24182;&#19988;&#36890;&#24120;&#27604;&#30452;&#25509;&#35774;&#35745;&#20844;&#24179;&#24615;&#26041;&#27861;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness to distribution shift and fairness have independently emerged as two important desiderata required of modern machine learning models. While these two desiderata seem related, the connection between them is often unclear in practice. Here, we discuss these connections through a causal lens, focusing on anti-causal prediction tasks, where the input to a classifier (e.g., an image) is assumed to be generated as a function of the target label and the protected attribute. By taking this perspective, we draw explicit connections between a common fairness criterion - separation - and a common notion of robustness - risk invariance. These connections provide new motivation for applying the separation criterion in anticausal settings, and inform old discussions regarding fairness-performance tradeoffs. In addition, our findings suggest that robustness-motivated approaches can be used to enforce separation, and that they often work better in practice than methods designed to directly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#32422;&#26463;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38480;&#21046;&#28508;&#22312;&#34920;&#31034;&#36981;&#24490;&#29305;&#23450;&#21160;&#24577;&#35268;&#24459;&#20197;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24847;&#20041;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;DNA&#33639;&#20809;&#30005;&#24433;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#65292;&#34920;&#26126;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#23398;&#20064;&#21160;&#24577;&#35268;&#24459;&#65292;&#24182;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2209.00905</link><description>&lt;p&gt;
&#20174;&#28508;&#22312;&#21160;&#21147;&#23398;&#21040;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
From latent dynamics to meaningful representations. (arXiv:2209.00905v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#32422;&#26463;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38480;&#21046;&#28508;&#22312;&#34920;&#31034;&#36981;&#24490;&#29305;&#23450;&#21160;&#24577;&#35268;&#24459;&#20197;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24847;&#20041;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;DNA&#33639;&#20809;&#30005;&#24433;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#65292;&#34920;&#26126;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#23398;&#20064;&#21160;&#24577;&#35268;&#24459;&#65292;&#24182;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#34920;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#23835;&#36215;&#30340;&#26680;&#24515;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#26159;&#20351;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24847;&#20041;&#12290;&#20026;&#27492;&#65292;&#20856;&#22411;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20808;&#39564;&#27010;&#29575;&#20998;&#24067;&#26469;&#35268;&#33539;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20808;&#39564;&#36890;&#24120;&#26159;&#19981;&#21487;&#29992;&#25110;&#20020;&#26102;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#32422;&#26463;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#19981;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#27010;&#29575;&#65292;&#32780;&#26159;&#38480;&#21046;&#28508;&#22312;&#34920;&#31034;&#36981;&#24490;&#29305;&#23450;&#30340;&#21160;&#24577;&#35268;&#24459;&#65292;&#36825;&#26159;&#21160;&#24577;&#31995;&#32479;&#34920;&#31034;&#23398;&#20064;&#26356;&#33258;&#28982;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#20449;&#20208;&#28304;&#20110;&#29289;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#35266;&#23519;&#65292;&#21363;&#34429;&#28982;&#19981;&#21516;&#30340;&#31995;&#32479;&#21487;&#20197;&#26377;&#19981;&#21516;&#30340;&#36793;&#38469;&#27010;&#29575;&#20998;&#24067;&#65292;&#20294;&#36890;&#24120;&#36981;&#24490;&#30456;&#21516;&#30340;&#21160;&#24577;&#35268;&#24459;&#65292;&#20363;&#22914;&#29275;&#39039;&#21644;&#34203;&#23450;&#35860;&#26041;&#31243;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#31995;&#32479;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#33639;&#20809;DNA&#30005;&#24433;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#23398;&#20064;&#21160;&#24577;&#35268;&#24459;&#65292;&#24182;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
While representation learning has been central to the rise of machine learning and artificial intelligence, a key problem remains in making the learnt representations meaningful. For this the typical approach is to regularize the learned representation through prior probability distributions. However such priors are usually unavailable or ad hoc. To deal with this, we propose a dynamics-constrained representation learning framework. Instead of using predefined probabilities, we restrict the latent representation to follow specific dynamics, which is a more natural constraint for representation learning in dynamical systems. Our belief stems from a fundamental observation in physics that though different systems can have different marginalized probability distributions, they typically obey the same dynamics, such as Newton's and Schrodinger's equations. We validate our framework for different systems including a real-world fluorescent DNA movie dataset. We show that our algorithm can un
&lt;/p&gt;</description></item><item><title>GEDI&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#31471;&#21040;&#31471;&#25968;&#25454;&#25554;&#34917;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#32593;&#32476;&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#26469;&#25913;&#36827;&#29305;&#24449;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#21644;&#35266;&#27979;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#36873;&#25321;&#23545;&#19979;&#28216;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#26377;&#24433;&#21709;&#21147;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#22522;&#20934;&#26041;&#27861;&#19978;&#37117;&#33021;&#25552;&#39640;&#25554;&#34917;&#21644;&#26631;&#31614;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.06573</link><description>&lt;p&gt;
GEDI:&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#31471;&#21040;&#31471;&#25968;&#25454;&#25554;&#34917;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GEDI: A Graph-based End-to-end Data Imputation Framework. (arXiv:2208.06573v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06573
&lt;/p&gt;
&lt;p&gt;
GEDI&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#31471;&#21040;&#31471;&#25968;&#25454;&#25554;&#34917;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#32593;&#32476;&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#26469;&#25913;&#36827;&#29305;&#24449;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#21644;&#35266;&#27979;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#36873;&#25321;&#23545;&#19979;&#28216;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#26377;&#24433;&#21709;&#21147;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#22522;&#20934;&#26041;&#27861;&#19978;&#37117;&#33021;&#25552;&#39640;&#25554;&#34917;&#21644;&#26631;&#31614;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25554;&#34917;&#26159;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#25554;&#34917;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#37325;&#35201;&#30446;&#26631;&#65306;&#65288;1&#65289;&#20445;&#25345;&#29305;&#24449;&#30697;&#38453;&#20013;&#35266;&#27979;&#20043;&#38388;&#30340;&#34892;&#30456;&#20284;&#24615;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#21015;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#65288;2&#65289;&#23558;&#25554;&#34917;&#36807;&#31243;&#35843;&#25972;&#20026;&#29305;&#23450;&#30340;&#19979;&#28216;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#25554;&#34917;&#36807;&#31243;&#20351;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#29305;&#24449;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#21644;&#35266;&#27979;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#29992;&#20803;&#23398;&#20064;&#26694;&#26550;&#26469;&#36873;&#25321;&#23545;&#19979;&#28216;&#24863;&#20852;&#36259;&#30340;&#39044;&#27979;&#20219;&#21153;&#26377;&#24433;&#21709;&#21147;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25554;&#34917;&#36807;&#31243;&#22312;&#21508;&#31181;&#22522;&#20934;&#26041;&#27861;&#19978;&#22987;&#32456;&#25552;&#39640;&#20102;&#25554;&#34917;&#21644;&#26631;&#31614;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data imputation is an effective way to handle missing data, which is common in practical applications. In this study, we propose and test a novel data imputation process that achieve two important goals: (1) preserve the row-wise similarities among observations and column-wise contextual relationships among features in the feature matrix, and (2) tailor the imputation process to specific downstream label prediction task. The proposed imputation process uses Transformer network and graph structure learning to iteratively refine the contextual relationships among features and similarities among observations. Moreover, it uses a meta-learning framework to select features that are influential to the downstream prediction task of interest. We conduct experiments on real-world large data sets, and show that the proposed imputation process consistently improves imputation and label prediction performance over a variety of benchmark methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.00979</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs. (arXiv:2206.00979v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00979
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65288;SP&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#26680;&#24515;&#20043;&#19968;&#12290;&#23427;&#23558;&#22270;&#20998;&#35299;&#20026;&#26368;&#30701;&#36335;&#24452;&#65292;&#24182;&#35745;&#31639;&#27599;&#20010;&#22270;&#20013;&#26368;&#30701;&#36335;&#24452;&#30340;&#39057;&#29575;&#12290;&#28982;&#32780;&#65292;SP&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#26368;&#30701;&#36335;&#24452;&#30340;&#19977;&#20803;&#34920;&#31034;&#22833;&#21435;&#20102;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;SP&#27604;&#36739;&#22270;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#22270;&#32467;&#26500;&#30340;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#24456;&#24120;&#35265;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38142;&#29366;&#32467;&#26500;&#12289;&#29615;&#29366;&#32467;&#26500;&#21644;&#26143;&#29366;&#32467;&#26500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#12290;&#23427;&#20351;&#29992;&#20197;&#27599;&#20010;&#39030;&#28857;&#20026;&#26681;&#30340;&#26576;&#20010;&#28145;&#24230;&#30340;BFS&#26641;&#26469;&#38480;&#21046;&#32771;&#34385;&#26368;&#30701;&#36335;&#24452;&#30340;&#26368;&#22823;&#38271;&#24230;&#65292;&#32771;&#34385;&#21040;&#23567;&#19990;&#30028;&#29305;&#24615;&#12290;&#23427;&#32771;&#34385;&#20102;&#26368;&#30701;&#36335;&#24452;&#20013;&#25152;&#26377;&#39030;&#28857;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#26041;&#20415;&#22312;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#19978;&#27604;&#36739;&#22270;&#65292;&#23427;&#20174;&#39030;&#28857;&#21644;
&lt;/p&gt;
&lt;p&gt;
The traditional shortest-path graph kernel (SP) is one of the most popular graph kernels. It decomposes graphs into shortest paths and computes their frequencies in each graph. However, SP has two main challenges: Firstly, the triplet representation of the shortest path loses information. Secondly, SP compares graphs without considering the multiple different scales of the graph structure which is common in real-world graphs, e.g., the chain-, ring-, and star-structures in social networks. To overcome these two challenges, we develop a novel shortest-path graph kernel called the Multi-scale Wasserstein Shortest-Path Filtration graph kernel (MWSPF). It uses a BFS tree of a certain depth rooted at each vertex to restrict the maximum length of the shortest path considering the small world property. It considers the labels of all the vertices in the shortest path. To facilitate the comparison of graphs at multiple different scales, it augments graphs from both the aspects of the vertex and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#32676;&#20248;&#21270;&#30340;PSO&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#19981;&#21516;&#38454;&#27573;&#23454;&#29616;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2205.10456</link><description>&lt;p&gt;
&#20855;&#26377;&#24322;&#26500;&#23398;&#20064;&#29575;&#30340;PSO&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PSO-Convolutional Neural Networks with Heterogeneous Learning Rate. (arXiv:2205.10456v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#32676;&#20248;&#21270;&#30340;PSO&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#19981;&#21516;&#38454;&#27573;&#23454;&#29616;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;ConvNets&#25110;CNN&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#21160;&#24577;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65306;&#35757;&#32451;&#23427;&#20204;&#26159;&#22256;&#38590;&#19988;&#35745;&#31639;&#24320;&#38144;&#22823;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#24182;&#35299;&#20915;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#22914;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#21160;&#20316;&#35782;&#21035;&#20197;&#21450;&#29289;&#20307;&#26816;&#27979;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#30340;ConvNets&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#26694;&#26550;&#20013;&#65292;&#27599;&#20010;ConvNet&#30340;&#26435;&#37325;&#21521;&#37327;&#36890;&#24120;&#34987;&#35270;&#20026;&#30456;&#20301;&#31354;&#38388;&#20013;&#31890;&#23376;&#30340;&#20301;&#32622;&#65292;&#20854;&#20013;PSO&#21327;&#21516;&#21160;&#21147;&#23398;&#19982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#19979;&#65306;i) [&#24120;&#35268;&#38454;&#27573;]&#27599;&#20010;ConvNet&#36890;&#36807;SGD&#29420;&#31435;&#35757;&#32451;&#65307;ii) [&#21327;&#21516;&#38454;&#27573;]ConvNets&#20043;&#38388;&#20849;&#20139;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (ConvNets or CNNs) have been candidly deployed in the scope of computer vision and related fields. Nevertheless, the dynamics of training of these neural networks lie still elusive: it is hard and computationally expensive to train them. A myriad of architectures and training strategies have been proposed to overcome this challenge and address several problems in image processing such as speech, image and action recognition as well as object detection. In this article, we propose a novel Particle Swarm Optimization (PSO) based training for ConvNets. In such framework, the vector of weights of each ConvNet is typically cast as the position of a particle in phase space whereby PSO collaborative dynamics intertwines with Stochastic Gradient Descent (SGD) in order to boost training performance and generalization. Our approach goes as follows: i) [regular phase] each ConvNet is trained independently via SGD; ii) [collaborative phase] ConvNets share among themse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#31181;&#24120;&#35265;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25200;&#21160;&#20266;&#24433;&#23545;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#22312;&#35780;&#20272;&#20013;&#38656;&#35201;&#32771;&#34385;&#20266;&#24433;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2203.02928</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#25200;&#21160;&#20266;&#24433;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks. (arXiv:2203.02928v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20960;&#31181;&#24120;&#35265;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#25200;&#21160;&#20266;&#24433;&#23545;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#22312;&#35780;&#20272;&#20013;&#38656;&#35201;&#32771;&#34385;&#20266;&#24433;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22914;&#20309;&#35299;&#37322;&#20854;&#20915;&#31574;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#27492;&#20986;&#29616;&#20102;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#29305;&#24449;&#26469;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20294;&#26159;&#25200;&#21160;&#26412;&#36523;&#21487;&#33021;&#20250;&#24341;&#20837;&#20266;&#24433;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#20266;&#24433;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27492;&#26041;&#27861;&#35780;&#20272;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#25200;&#21160;&#20266;&#24433;&#23545;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#22312;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26102;&#32771;&#34385;&#20266;&#24433;&#23384;&#22312;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite excellent performance of deep neural networks (DNNs) in image classification, detection, and prediction, characterizing how DNNs make a given decision remains an open problem, resulting in a number of interpretability methods. Post-hoc interpretability methods primarily aim to quantify the importance of input features with respect to the class probabilities. However, due to the lack of ground truth and the existence of interpretability methods with diverse operating characteristics, evaluating these methods is a crucial challenge. A popular approach to evaluate interpretability methods is to perturb input features deemed important for a given prediction and observe the decrease in accuracy. However, perturbation itself may introduce artifacts. We propose a method for estimating the impact of such artifacts on the fidelity estimation by utilizing model accuracy curves from perturbing input features according to the Most Import First (MIF) and Least Import First (LIF) orders. Usi
&lt;/p&gt;</description></item><item><title>&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2203.01881</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#20998;&#29305;&#24449;&#24230;&#37327;&#19979;&#28216;&#20998;&#31867;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features. (arXiv:2203.01881v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01881
&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#22833;&#36133;&#27169;&#24335;&#21644;&#23398;&#20064;&#34920;&#31034;&#30340;&#35299;&#37322;&#65292;&#23384;&#22312;&#30528;&#26377;&#38480;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102; SimCLR&#12289;SwaV&#12289;MoCo&#12289;BYOL&#12289;DINO&#12289;SimSiam&#12289;VICReg &#21644; Barlow Twins &#31561;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#22312;&#19981;&#20351;&#29992;&#31867;&#26631;&#31614;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23545;&#24212;&#20110;&#22270;&#20687;&#20013;&#29420;&#29305;&#29289;&#29702;&#23646;&#24615;&#30340;&#21306;&#20998;&#29305;&#24449;&#65292;&#36825;&#20123;&#21306;&#20998;&#29305;&#24449;&#20027;&#35201;&#23384;&#22312;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#34920;&#31034;&#20013;&#12290;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#34920;&#31034;&#31354;&#38388;&#21387;&#32553;&#22810;&#36798; 40%&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#32447;&#24615;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65288;&#25110; Q-Score&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#12289;&#26080;&#30417;&#30563;&#30340;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#19968;&#20010;&#32473;&#23450;&#26679;&#26412;&#22312;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#26159;&#21542;&#21487;&#33021;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#22312; ImageNet-100 &#21644; ImageNet-1K &#19978;&#23454;&#29616;&#20102; AUPRC &#20998;&#21035;&#20026; 91.45 &#21644; 78.78&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Neyman-Scott&#36807;&#31243;&#65288;NSPs&#65289;&#21644;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#28151;&#21512;&#27169;&#22411;&#65288;DPMM&#65289;&#20043;&#38388;&#30340;&#26032;&#39062;&#32852;&#31995;&#65292;&#24182;&#25506;&#35752;&#20102;NSP&#22312;&#26102;&#31354;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2201.05044</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#28151;&#21512;&#27169;&#22411;&#30340;&#32852;&#31995;&#65292;&#20351;&#29992;Neyman-Scott&#36807;&#31243;&#36827;&#34892;&#26102;&#31354;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal Clustering with Neyman-Scott Processes via Connections to Bayesian Nonparametric Mixture Models. (arXiv:2201.05044v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Neyman-Scott&#36807;&#31243;&#65288;NSPs&#65289;&#21644;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#28151;&#21512;&#27169;&#22411;&#65288;DPMM&#65289;&#20043;&#38388;&#30340;&#26032;&#39062;&#32852;&#31995;&#65292;&#24182;&#25506;&#35752;&#20102;NSP&#22312;&#26102;&#31354;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neyman-Scott&#36807;&#31243;&#65288;NSPs&#65289;&#26159;&#29983;&#25104;&#26102;&#38388;&#25110;&#31354;&#38388;&#20013;&#28857;&#31751;&#30340;&#28857;&#36807;&#31243;&#27169;&#22411;&#12290;&#23427;&#20204;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#24191;&#27867;&#29616;&#35937;&#30340;&#33258;&#28982;&#27169;&#22411;&#65292;&#20174;&#31070;&#32463;&#33033;&#20914;&#24207;&#21015;&#21040;&#25991;&#26723;&#27969;&#12290;&#32858;&#31867;&#23646;&#24615;&#26159;&#36890;&#36807;&#21452;&#37325;&#38543;&#26426;&#20844;&#24335;&#23454;&#29616;&#30340;&#65306;&#39318;&#20808;&#65292;&#20174;&#27850;&#26494;&#36807;&#31243;&#20013;&#32472;&#21046;&#19968;&#32452;&#28508;&#22312;&#20107;&#20214;&#65307;&#28982;&#21518;&#65292;&#27599;&#20010;&#28508;&#22312;&#20107;&#20214;&#26681;&#25454;&#21478;&#19968;&#20010;&#27850;&#26494;&#36807;&#31243;&#29983;&#25104;&#19968;&#32452;&#35266;&#27979;&#25968;&#25454;&#28857;&#12290;&#36825;&#20010;&#32467;&#26500;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#28151;&#21512;&#27169;&#22411;&#65292;&#22914;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#65288;DPMM&#65289;&#65292;&#20854;&#20013;&#28508;&#22312;&#20107;&#20214;&#65288;&#21363;&#31751;&#65289;&#30340;&#25968;&#37327;&#26159;&#19968;&#20010;&#38543;&#26426;&#21464;&#37327;&#65292;&#20294;&#28857;&#36807;&#31243;&#30340;&#26500;&#36896;&#20351;&#24471;NSP&#29305;&#21035;&#36866;&#21512;&#20110;&#24314;&#27169;&#26102;&#31354;&#25968;&#25454;&#12290;&#34429;&#28982;&#20026;DPMM&#24320;&#21457;&#20102;&#35768;&#22810;&#19987;&#38376;&#31639;&#27861;&#65292;&#20294;&#30456;&#23545;&#36739;&#23569;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;NSP&#30340;&#25512;&#26029;&#19978;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NSP&#19982;DPMM&#20043;&#38388;&#30340;&#26032;&#39062;&#32852;&#31995;&#65292;&#20851;&#38190;&#30340;&#36830;&#25509;&#26159;&#31532;&#19977;&#31867;&#36125;&#21494;&#26031;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neyman-Scott processes (NSPs) are point process models that generate clusters of points in time or space. They are natural models for a wide range of phenomena, ranging from neural spike trains to document streams. The clustering property is achieved via a doubly stochastic formulation: first, a set of latent events is drawn from a Poisson process; then, each latent event generates a set of observed data points according to another Poisson process. This construction is similar to Bayesian nonparametric mixture models like the Dirichlet process mixture model (DPMM) in that the number of latent events (i.e. clusters) is a random variable, but the point process formulation makes the NSP especially well suited to modeling spatiotemporal data. While many specialized algorithms have been developed for DPMMs, comparatively fewer works have focused on inference in NSPs. Here, we present novel connections between NSPs and DPMMs, with the key link being a third class of Bayesian mixture models c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#23558;&#21508;&#31181;&#21487;&#33021;&#30340;&#25512;&#26029;&#35299;&#37322;&#32858;&#21512;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#35299;&#37322;&#20013;&#22810;&#20010;&#26377;&#25928;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#21644;&#24050;&#30693;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2109.03890</link><description>&lt;p&gt;
&#12298;&#25512;&#26029;&#35299;&#37322;&#30340;&#20844;&#29702;&#32858;&#21512;&#12299;
&lt;/p&gt;
&lt;p&gt;
Axiomatic Aggregations of Abductive Explanations. (arXiv:2109.03890v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#23558;&#21508;&#31181;&#21487;&#33021;&#30340;&#25512;&#26029;&#35299;&#37322;&#32858;&#21512;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#35299;&#37322;&#20013;&#22810;&#20010;&#26377;&#25928;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#21644;&#24050;&#30693;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#21518;&#32493;&#27169;&#22411;&#36924;&#36817;&#35299;&#37322;&#26041;&#27861;&#65288;&#22914;LIME&#21644;SHAP&#65289;&#30340;&#40065;&#26834;&#24615;&#30340;&#36817;&#26399;&#25209;&#35780;&#23548;&#33268;&#20102;&#27169;&#22411;&#31934;&#30830;&#30340;&#25512;&#26029;&#35299;&#37322;&#30340;&#20852;&#36215;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#28857;&#65292;&#25512;&#26029;&#35299;&#37322;&#25552;&#20379;&#20102;&#19968;&#20010;&#36275;&#20197;&#29983;&#25104;&#32467;&#26524;&#30340;&#26368;&#23567;&#23376;&#38598;&#29305;&#24449;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#26159;&#20005;&#26684;&#21644;&#21487;&#38752;&#30340;&#65292;&#20294;&#25512;&#26029;&#35299;&#37322;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#21516;&#19968;&#25968;&#25454;&#28857;&#21487;&#20197;&#26377;&#22810;&#20010;&#26377;&#25928;&#30340;&#25512;&#26029;&#35299;&#37322;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#19968;&#20010;&#21333;&#19968;&#30340;&#25512;&#26029;&#35299;&#37322;&#21487;&#33021;&#26159;&#19981;&#36275;&#22815;&#30340;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#25552;&#20379;&#25152;&#26377;&#26377;&#25928;&#30340;&#25512;&#26029;&#35299;&#37322;&#21487;&#33021;&#30001;&#20110;&#20854;&#35268;&#27169;&#32780;&#38590;&#20197;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#21508;&#31181;&#21487;&#33021;&#30340;&#25512;&#26029;&#35299;&#37322;&#32858;&#21512;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65306;&#20004;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#33879;&#21517;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#20844;&#29702;&#19978;&#23545;&#36825;&#19977;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#35777;&#26126;&#27599;&#20010;&#26041;&#27861;&#37117;&#26159;&#33391;&#23450;&#20041;&#30340;&#19988;&#31526;&#21512;&#20844;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35774;&#22791;&#21512;&#20316;&#36793;&#32536;&#25512;&#29702;&#30340;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#65292;&#36890;&#36807;&#20248;&#21270;&#26412;&#22320;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#24067;&#24335;&#29305;&#24449;&#32534;&#30721;&#65292;&#23454;&#29616;&#20302;&#24310;&#36831;&#30340;&#21512;&#20316;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2109.00172</link><description>&lt;p&gt;
&#22810;&#35774;&#22791;&#21512;&#20316;&#36793;&#32536;&#25512;&#29702;&#30340;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented Communication for Multi-Device Cooperative Edge Inference. (arXiv:2109.00172v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.00172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35774;&#22791;&#21512;&#20316;&#36793;&#32536;&#25512;&#29702;&#30340;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#65292;&#36890;&#36807;&#20248;&#21270;&#26412;&#22320;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#24067;&#24335;&#29305;&#24449;&#32534;&#30721;&#65292;&#23454;&#29616;&#20302;&#24310;&#36831;&#30340;&#21512;&#20316;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35774;&#22791;&#21512;&#20316;&#36793;&#32536;&#25512;&#29702;&#30340;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#65292;&#22312;&#36825;&#31181;&#22330;&#26223;&#19979;&#65292;&#19968;&#32452;&#20998;&#24067;&#24335;&#20302;&#31471;&#36793;&#32536;&#35774;&#22791;&#23558;&#26412;&#22320;&#26679;&#26412;&#25552;&#21462;&#30340;&#29305;&#24449;&#20256;&#36755;&#21040;&#24378;&#22823;&#30340;&#36793;&#32536;&#26381;&#21153;&#22120;&#36827;&#34892;&#25512;&#29702;&#12290;&#23613;&#31649;&#21512;&#20316;&#36793;&#32536;&#25512;&#29702;&#21487;&#20197;&#20811;&#26381;&#21333;&#20010;&#35774;&#22791;&#30340;&#26377;&#38480;&#24863;&#30693;&#33021;&#21147;&#65292;&#20294;&#23427;&#22823;&#22823;&#22686;&#21152;&#20102;&#36890;&#20449;&#24320;&#38144;&#24182;&#21487;&#33021;&#24341;&#36215;&#36807;&#22823;&#30340;&#24310;&#36831;&#12290;&#20026;&#20102;&#23454;&#29616;&#20302;&#24310;&#36831;&#30340;&#21512;&#20316;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#20219;&#21153;&#23548;&#21521;&#30340;&#26041;&#24335;&#20248;&#21270;&#26412;&#22320;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#24067;&#24335;&#29305;&#24449;&#32534;&#30721;&#65292;&#21363;&#21435;&#38500;&#25968;&#25454;&#20887;&#20313;&#24182;&#20256;&#36755;&#23545;&#19979;&#28216;&#25512;&#29702;&#20219;&#21153;&#32780;&#35328;&#33267;&#20851;&#37325;&#35201;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#37325;&#26032;&#26500;&#24314;&#25968;&#25454;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048; (IB) &#21407;&#29702;&#22312;&#27599;&#20010;&#36793;&#32536;&#35774;&#22791;&#19978;&#25552;&#21462;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#20998;&#24067;&#24335;&#20449;&#24687;&#29942;&#39048; (DIB) &#26694;&#26550;&#23545;&#21333;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#29305;&#24449;&#36827;&#34892;&#24418;&#24335;&#21270;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates task-oriented communication for multi-device cooperative edge inference, where a group of distributed low-end edge devices transmit the extracted features of local samples to a powerful edge server for inference. While cooperative edge inference can overcome the limited sensing capability of a single device, it substantially increases the communication overhead and may incur excessive latency. To enable low-latency cooperative inference, we propose a learning-based communication scheme that optimizes local feature extraction and distributed feature encoding in a task-oriented manner, i.e., to remove data redundancy and transmit information that is essential for the downstream inference task rather than reconstructing the data samples at the edge server. Specifically, we leverage an information bottleneck (IB) principle to extract the task-relevant feature at each edge device and adopt a distributed information bottleneck (DIB) framework to formalize a single-let
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23450;&#20041;&#20102;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#23427;&#20204;&#22312;&#27969;&#24418;&#21464;&#24418;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#25512;&#24191;&#20102;&#22270;&#28388;&#27874;&#22120;&#21644;&#26631;&#20934;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24050;&#30693;&#31283;&#23450;&#24615;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2106.03725</link><description>&lt;p&gt;
&#12298;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#24418;&#31283;&#23450;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
Stability to Deformations of Manifold Filters and Manifold Neural Networks. (arXiv:2106.03725v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#23427;&#20204;&#22312;&#27969;&#24418;&#21464;&#24418;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#25512;&#24191;&#20102;&#22270;&#28388;&#27874;&#22120;&#21644;&#26631;&#20934;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24050;&#30693;&#31283;&#23450;&#24615;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#24182;&#30740;&#31350;&#20102;&#27969;&#24418;&#65288;M&#65289;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#12290;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;MNN&#26159;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#31639;&#23376;&#25351;&#25968;&#23450;&#20041;&#30340;&#65292;&#24182;&#19988;&#22312;&#27969;&#24418;&#34987;&#37319;&#26679;&#26102;&#65292;&#21487;&#20197;&#24674;&#22797;&#20026;&#22270;&#65288;G&#65289;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#31163;&#25955;&#36817;&#20284;&#12290;&#36825;&#20123;&#28388;&#27874;&#22120;&#20855;&#26377;&#35889;&#34920;&#31034;&#65292;&#23427;&#26159;&#22270;&#28388;&#27874;&#22120;&#30340;&#35889;&#34920;&#31034;&#21644;&#36830;&#32493;&#26102;&#38388;&#20013;&#26631;&#20934;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#39057;&#29575;&#21709;&#24212;&#30340;&#25512;&#24191;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#20998;&#26512;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;MNN&#22312;&#27969;&#24418;&#20809;&#28369;&#21464;&#24418;&#19979;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#20998;&#26512;&#25512;&#24191;&#20102;&#22270;&#28388;&#27874;&#22120;&#21644;GNN&#30340;&#24050;&#30693;&#31283;&#23450;&#24615;&#24615;&#36136;&#65292;&#24182;&#19988;&#20063;&#26159;&#36830;&#32493;&#26102;&#38388;&#20013;&#26631;&#20934;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#24050;&#30693;&#31283;&#23450;&#24615;&#24615;&#36136;&#30340;&#25512;&#24191;&#12290;&#20174;&#36825;&#31181;&#20998;&#26512;&#20013;&#24471;&#20986;&#30340;&#26368;&#37325;&#35201;&#30340;&#35266;&#23519;&#26159;&#65292;&#27969;&#24418;&#28388;&#27874;&#22120;&#21644;&#22270;&#28388;&#27874;&#22120;&#19968;&#26679;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper defines and studies manifold (M) convolutional filters and neural networks (NNs). \emph{Manifold} filters and MNNs are defined in terms of the Laplace-Beltrami operator exponential and are such that \emph{graph} (G) filters and neural networks (NNs) are recovered as discrete approximations when the manifold is sampled. These filters admit a spectral representation which is a generalization of both the spectral representation of graph filters and the frequency response of standard convolutional filters in continuous time. The main technical contribution of the paper is to analyze the stability of manifold filters and MNNs to smooth deformations of the manifold. This analysis generalizes known stability properties of graph filters and GNNs and it is also a generalization of known stability properties of standard convolutional filters and neural networks in continuous time. The most important observation that follows from this analysis is that manifold filters, same as graph fil
&lt;/p&gt;</description></item><item><title>&#22270;&#24418;Barlow Twins&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#20132;&#21449;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23398;&#20064;&#22270;&#24418;&#34920;&#31034;&#65292;&#20811;&#26381;&#20102;&#36127;&#26679;&#26412;&#23450;&#20041;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#65292;&#23427;&#19981;&#20381;&#36182;&#38750;&#23545;&#31216;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#38656;&#35201;&#36739;&#23569;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2106.02466</link><description>&lt;p&gt;
&#22270;&#24418;Barlow Twins&#65306;&#19968;&#31181;&#29992;&#20110;&#22270;&#24418;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Graph Barlow Twins: A self-supervised representation learning framework for graphs. (arXiv:2106.02466v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02466
&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;Barlow Twins&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#20132;&#21449;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23398;&#20064;&#22270;&#24418;&#34920;&#31034;&#65292;&#20811;&#26381;&#20102;&#36127;&#26679;&#26412;&#23450;&#20041;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#65292;&#23427;&#19981;&#20381;&#36182;&#38750;&#23545;&#31216;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#38656;&#35201;&#36739;&#23569;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#33539;&#24335;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25506;&#32034;&#39046;&#22495;&#65292;&#26088;&#22312;&#28040;&#38500;&#26114;&#36149;&#30340;&#25968;&#25454;&#26631;&#27880;&#38656;&#27714;&#12290;&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#38656;&#35201;&#38590;&#20197;&#23450;&#20041;&#30340;&#36127;&#26679;&#26412;&#12290;&#22312;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#26159;&#23454;&#29616;&#31283;&#20581;&#34920;&#31034;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550; - &#22270;&#24418;Barlow Twins&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#20132;&#21449;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#36127;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;BGRL&#30456;&#27604;&#65292;&#23427;&#19981;&#20381;&#36182;&#38750;&#23545;&#31216;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38656;&#35201;&#36739;&#23569;&#36229;&#21442;&#25968;&#21644;&#23454;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#19982;&#26368;&#22909;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#21644;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-supervised learning (SSL) paradigm is an essential exploration area, which tries to eliminate the need for expensive data labeling. Despite the great success of SSL methods in computer vision and natural language processing, most of them employ contrastive learning objectives that require negative samples, which are hard to define. This becomes even more challenging in the case of graphs and is a bottleneck for achieving robust representations. To overcome such limitations, we propose a framework for self-supervised graph representation learning - Graph Barlow Twins, which utilizes a cross-correlation-based loss function instead of negative samples. Moreover, it does not rely on non-symmetric neural network architectures - in contrast to state-of-the-art self-supervised graph representation learning method BGRL. We show that our method achieves as competitive results as the best self-supervised methods and fully supervised ones while requiring fewer hyperparameters and substan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#32422;&#26463;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#20449;&#24687;&#32858;&#21512;&#36807;&#31243;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#23548;&#33268;&#30340;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2105.08511</link><description>&lt;p&gt;
&#38754;&#21521;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#38544;&#31169;&#20445;&#25252;&#32422;&#26463;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Constrained Domain Generalization for Medical Image Classification. (arXiv:2105.08511v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.08511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#32422;&#26463;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#20449;&#24687;&#32858;&#21512;&#36807;&#31243;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#23548;&#33268;&#30340;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#24433;&#20687;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#26377;&#38480;&#21644;&#23545;&#24739;&#32773;&#38544;&#31169;&#20445;&#25252;&#30340;&#20005;&#26684;&#27861;&#24459;&#21644;&#20262;&#29702;&#35201;&#27714;&#65292;&#30001;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#24456;&#22823;&#38459;&#30861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#38544;&#31169;&#20445;&#25252;&#32422;&#26463;&#22495;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#22312;&#38544;&#31169;&#20445;&#25252;&#26465;&#20214;&#19979;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#23545;&#40784;&#25439;&#22833;&#65292;&#20197;&#25913;&#21892;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#31471;&#30340;&#20449;&#24687;&#32858;&#21512;&#36807;&#31243;&#65292;&#26399;&#26395;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#8220;&#26410;&#35265;&#36807;&#8221;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have demonstrated unprecedented success for medical imaging applications. However, due to the issue of limited dataset availability and the strict legal and ethical requirements for patient privacy protection, the broad applications of medical imaging classification driven by DNN with large-scale training data have been largely hindered. For example, when training the DNN from one domain (e.g., with data only from one hospital), the generalization capability to another domain (e.g., data from another hospital) could be largely lacking. In this paper, we aim to tackle this problem by developing the privacy-preserving constrained domain generalization method, aiming to improve the generalization capability under the privacy-preserving condition. In particular, We propose to improve the information aggregation process on the centralized server-side with a novel gradient alignment loss, expecting that the trained model can be better generalized to the "unseen" bu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;&#36890;&#36807;&#23398;&#20064;&#22810;&#26679;&#30340;&#39044;&#27979;&#22120;&#21487;&#20197;&#20272;&#35745;&#26032;&#35266;&#27979;&#23454;&#20363;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#24182;&#19981;&#19968;&#23450;&#33021;&#36716;&#21270;&#20026;&#39044;&#27979;&#22120;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2010.12995</link><description>&lt;p&gt;
&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#65306;&#21442;&#25968;&#19982;&#39044;&#27979;&#22120;&#29109;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution detection for regression tasks: parameter versus predictor entropy. (arXiv:2010.12995v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.12995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;&#36890;&#36807;&#23398;&#20064;&#22810;&#26679;&#30340;&#39044;&#27979;&#22120;&#21487;&#20197;&#20272;&#35745;&#26032;&#35266;&#27979;&#23454;&#20363;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#24182;&#19981;&#19968;&#23450;&#33021;&#36716;&#21270;&#20026;&#39044;&#27979;&#22120;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#26816;&#27979;&#26679;&#26412;&#19982;&#35757;&#32451;&#26679;&#26412;&#30456;&#36317;&#22826;&#36828;&#26102;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#34987;&#31216;&#20026;&#31163;&#32676;&#26679;&#26412;&#65288;OOD&#65289;&#26816;&#27979;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#32780;&#35328;&#65292;&#19968;&#31181;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#30340;&#26041;&#27861;&#26159;&#23398;&#20064;&#22810;&#26679;&#30340;&#39044;&#27979;&#22120;&#65292;&#36825;&#20123;&#39044;&#27979;&#22120;&#37117;&#33021;&#35299;&#37322;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#20272;&#35745;&#26032;&#35266;&#27979;&#23454;&#20363;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#39044;&#27979;&#32467;&#26524;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#34913;&#37327;&#12290;&#35780;&#20272;&#21644;&#35748;&#35777;&#26041;&#27861;&#26816;&#27979;OOD&#30340;&#33021;&#21147;&#38656;&#35201;&#25351;&#23450;&#22312;&#37096;&#32626;&#20013;&#21487;&#33021;&#21457;&#29983;&#20294;&#27809;&#26377;&#21487;&#29992;&#39044;&#27979;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#36873;&#25321;&#22238;&#24402;&#20219;&#21153;&#20316;&#20026;&#30740;&#31350;&#37325;&#28857;&#65292;&#22312;&#27492;&#20219;&#21153;&#20013;&#36873;&#25321;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#27934;&#23519;&#21147;&#30340;&#27169;&#22411;&#26469;&#34920;&#31034;OOD&#20998;&#24067;&#65292;&#24182;&#23545;&#21508;&#31181;&#26041;&#27861;&#22312;&#21306;&#20998;OOD&#26679;&#26412;&#21644;&#25968;&#25454;&#20013;&#30340;&#33021;&#21147;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#21487;&#33021;&#26080;&#27861;&#36716;&#21270;&#20026;&#39044;&#27979;&#22120;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is crucial to detect when an instance lies downright too far from the training samples for the machine learning model to be trusted, a challenge known as out-of-distribution (OOD) detection. For neural networks, one approach to this task consists of learning a diversity of predictors that all can explain the training data. This information can be used to estimate the epistemic uncertainty at a given newly observed instance in terms of a measure of the disagreement of the predictions. Evaluation and certification of the ability of a method to detect OOD require specifying instances which are likely to occur in deployment yet on which no prediction is available. Focusing on regression tasks, we choose a simple yet insightful model for this OOD distribution and conduct an empirical evaluation of the ability of various methods to discriminate OOD samples from the data. Moreover, we exhibit evidence that a diversity of parameters may fail to translate to a diversity of predictors. Based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GTAdam&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#36319;&#36394;&#21644;&#33258;&#36866;&#24212;&#21160;&#37327;&#20272;&#35745;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#24378;&#20984;&#25104;&#26412;&#20989;&#25968;&#30340;&#22312;&#32447;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#21160;&#24577;&#36951;&#25022;&#19978;&#30028;&#21644;&#38745;&#24577;&#35774;&#32622;&#19979;&#30340;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2009.01745</link><description>&lt;p&gt;
GTAdam&#65306;&#24102;&#26377;&#33258;&#36866;&#24212;&#21160;&#37327;&#30340;&#26799;&#24230;&#36319;&#36394;&#29992;&#20110;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GTAdam: Gradient Tracking with Adaptive Momentum for Distributed Online Optimization. (arXiv:2009.01745v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.01745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GTAdam&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#36319;&#36394;&#21644;&#33258;&#36866;&#24212;&#21160;&#37327;&#20272;&#35745;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#24378;&#20984;&#25104;&#26412;&#20989;&#25968;&#30340;&#22312;&#32447;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#21160;&#24577;&#36951;&#25022;&#19978;&#30028;&#21644;&#38745;&#24577;&#35774;&#32622;&#19979;&#30340;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28041;&#21450;&#19968;&#31181;&#35745;&#31639;&#20195;&#29702;&#32593;&#32476;&#65292;&#26088;&#22312;&#36890;&#36807;&#26412;&#22320;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#26041;&#24335;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#24067;&#24335;&#22320;&#35299;&#20915;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#36319;&#36394;&#19982;&#33258;&#36866;&#24212;&#21160;&#37327;&#20272;&#35745;&#65288;GTAdam&#65289;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#26799;&#24230;&#36319;&#36394;&#26426;&#21046;&#19982;&#26799;&#24230;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#21160;&#37327;&#20272;&#35745;&#30456;&#32467;&#21512;&#12290;&#31639;&#27861;&#22312;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25104;&#26412;&#20989;&#25968;&#30340;&#22312;&#32447;&#35774;&#32622;&#20013;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19982;&#21021;&#22987;&#26465;&#20214;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#26102;&#38388;&#21464;&#21270;&#30456;&#20851;&#30340;&#21160;&#24577;&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#20013;&#20445;&#35777;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;&#35813;&#31639;&#27861;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#20998;&#31867;&#38382;&#39064;&#12289;&#65288;&#31227;&#21160;&#65289;&#30446;&#26631;&#23450;&#20301;&#38382;&#39064;&#20197;&#21450;&#22270;&#20687;&#20998;&#31867;&#30340;&#38543;&#26426;&#20248;&#21270;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper deals with a network of computing agents aiming to solve an online optimization problem in a distributed fashion, i.e., by means of local computation and communication, without any central coordinator. We propose the gradient tracking with adaptive momentum estimation (GTAdam) distributed algorithm, which combines a gradient tracking mechanism with first and second order momentum estimates of the gradient. The algorithm is analyzed in the online setting for strongly convex cost functions with Lipschitz continuous gradients. We provide an upper bound for the dynamic regret given by a term related to the initial conditions and another term related to the temporal variations of the objective functions. Moreover, a linear convergence rate is guaranteed in the static setup. The algorithm is tested on a time-varying classification problem, on a (moving) target localization problem, and in a stochastic optimization setup from image classification. In these numerical experiments fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#19981;&#23436;&#25972;&#30340;&#35266;&#23519;&#25968;&#25454;&#19979;&#23398;&#20064;&#20998;&#24067;&#40065;&#26834;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#31574;&#30053;&#35780;&#20272;&#36807;&#31243;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#31867;&#22411;&#30340;&#20445;&#35777;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29615;&#22659;&#36716;&#21464;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2006.05630</link><description>&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#30340;&#25209;&#27425;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Batch Contextual Bandits. (arXiv:2006.05630v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.05630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#19981;&#23436;&#25972;&#30340;&#35266;&#23519;&#25968;&#25454;&#19979;&#23398;&#20064;&#20998;&#24067;&#40065;&#26834;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#31574;&#30053;&#35780;&#20272;&#36807;&#31243;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#31867;&#22411;&#30340;&#20445;&#35777;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29615;&#22659;&#36716;&#21464;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21382;&#21490;&#35266;&#23519;&#25968;&#25454;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#36873;&#25321;&#21521;&#23458;&#25143;&#21457;&#36865;&#30340;&#20248;&#24800;&#12289;&#20215;&#26684;&#12289;&#24191;&#21578;&#65292;&#20197;&#21450;&#36873;&#25321;&#32473;&#24739;&#32773;&#24320;&#20855;&#21738;&#31181;&#33647;&#29289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#29486;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#65292;&#21363;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#23558;&#34987;&#37096;&#32626;&#21040;&#30340;&#26410;&#26469;&#29615;&#22659;&#19982;&#29983;&#25104;&#25968;&#25454;&#30340;&#36807;&#21435;&#29615;&#22659;&#30456;&#21516;&#65292;&#32780;&#36825;&#20010;&#20551;&#35774;&#24448;&#24448;&#26159;&#38169;&#35823;&#30340;&#25110;&#32773;&#36807;&#20110;&#31895;&#30053;&#30340;&#36817;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#19981;&#23436;&#25972;&#35266;&#23519;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#35780;&#20272;&#36807;&#31243;&#65292;&#20197;&#35780;&#20272;&#31574;&#30053;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29615;&#22659;&#36716;&#21464;&#19979;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20010;&#25552;&#20986;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#26696;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#31867;&#22411;&#30340;&#20445;&#35777;&#12290;&#21033;&#29992;&#36825;&#20010;&#35780;&#20272;&#26041;&#26696;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy learning using historical observational data is an important problem that has found widespread applications. Examples include selecting offers, prices, advertisements to send to customers, as well as selecting which medication to prescribe to a patient. However, existing literature rests on the crucial assumption that the future environment where the learned policy will be deployed is the same as the past environment that has generated the data -- an assumption that is often false or too coarse an approximation. In this paper, we lift this assumption and aim to learn a distributionally robust policy with incomplete observational data. We first present a policy evaluation procedure that allows us to assess how well the policy does under the worst-case environment shift. We then establish a central limit theorem type guarantee for this proposed policy evaluation scheme. Leveraging this evaluation scheme, we further propose a novel learning algorithm that is able to learn a policy 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#35777;&#21644;&#23454;&#20363;&#20381;&#36182;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#28151;&#21512;&#26102;&#38388;&#12290;&#25105;&#20204;&#22522;&#20110;&#25910;&#32553;&#31995;&#25968;&#26469;&#20272;&#35745;&#28151;&#21512;&#26102;&#38388;&#65292;&#35813;&#31995;&#25968;&#33021;&#22815;&#25511;&#21046;&#28151;&#21512;&#26102;&#38388;&#30452;&#21040;&#24378;&#30340;&#26222;&#36941;&#24120;&#25968;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38750;&#21487;&#36870;&#38142;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#26356;&#23481;&#26131;&#19988;&#32622;&#20449;&#21306;&#38388;&#26356;&#31934;&#30830;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#32771;&#34385;&#36716;&#31227;&#30697;&#38453;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/1912.06845</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#38142;&#21644;&#28151;&#21512;&#26102;&#38388;&#30340;&#23454;&#35777;&#21644;&#23454;&#20363;&#20381;&#36182;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Empirical and Instance-Dependent Estimation of Markov Chain and Mixing Time. (arXiv:1912.06845v4 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.06845
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#35777;&#21644;&#23454;&#20363;&#20381;&#36182;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#28151;&#21512;&#26102;&#38388;&#12290;&#25105;&#20204;&#22522;&#20110;&#25910;&#32553;&#31995;&#25968;&#26469;&#20272;&#35745;&#28151;&#21512;&#26102;&#38388;&#65292;&#35813;&#31995;&#25968;&#33021;&#22815;&#25511;&#21046;&#28151;&#21512;&#26102;&#38388;&#30452;&#21040;&#24378;&#30340;&#26222;&#36941;&#24120;&#25968;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38750;&#21487;&#36870;&#38142;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#26356;&#23481;&#26131;&#19988;&#32622;&#20449;&#21306;&#38388;&#26356;&#31934;&#30830;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#32771;&#34385;&#36716;&#31227;&#30697;&#38453;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#21333;&#20010;&#35266;&#27979;&#36712;&#36857;&#20272;&#35745;&#39532;&#23572;&#21487;&#22827;&#38142;&#28151;&#21512;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#20351;&#29992;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26041;&#27861;&#20272;&#35745;&#35889;&#32570;&#21475;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#36873;&#25321;&#37319;&#29992;&#22522;&#20110;&#24635;&#21464;&#24046;&#25910;&#32553;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;Wolfer [2020]&#20013;&#24341;&#20837;&#30340;&#25910;&#32553;&#31995;&#25968;&#65292;&#21463;&#21040;Dobrushin&#30340;&#21551;&#21457;&#12290;&#19982;&#35889;&#32570;&#21475;&#19981;&#21516;&#65292;&#36825;&#20010;&#25968;&#37327;&#25511;&#21046;&#30528;&#28151;&#21512;&#26102;&#38388;&#30452;&#21040;&#24378;&#30340;&#26222;&#36941;&#24120;&#25968;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38750;&#21487;&#36870;&#38142;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#23436;&#20840;&#20381;&#36182;&#25968;&#25454;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36825;&#20123;&#21306;&#38388;&#27604;&#35889;&#30456;&#20851;&#25968;&#37327;&#26356;&#23481;&#26131;&#35745;&#31639;&#19988;&#26356;&#34180;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20851;&#20110;&#36716;&#31227;&#30697;&#38453;&#30340;&#38468;&#21152;&#20449;&#24687;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#36229;&#36807;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#38024;&#23545;&#35825;&#23548;&#22343;&#21248;&#33539;&#25968;&#21644;&#19968;&#20123;&#28151;&#21512;&#23646;&#24615;&#65292;&#23548;&#20986;&#19982;&#30697;&#38453;&#20272;&#35745;&#26377;&#20851;&#30340;&#23454;&#20363;&#20381;&#36182;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of estimating the mixing time of a Markov chain from a single trajectory of observations. Unlike most previous works which employed Hilbert space methods to estimate spectral gaps, we opt for an approach based on contraction with respect to total variation. Specifically, we estimate the contraction coefficient introduced in Wolfer [2020], inspired from Dobrushin's. This quantity, unlike the spectral gap, controls the mixing time up to strong universal constants and remains applicable to non-reversible chains. We improve existing fully data-dependent confidence intervals around this contraction coefficient, which are both easier to compute and thinner than spectral counterparts. Furthermore, we introduce a novel analysis beyond the worst-case scenario by leveraging additional information about the transition matrix. This allows us to derive instance-dependent rates for estimating the matrix with respect to the induced uniform norm, and some of its mixing propertie
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#36172;&#21338;&#23398;&#20064;&#20013;&#31454;&#20105;&#21644;&#21512;&#20316;&#23545;&#25506;&#32034;&#21644;&#21033;&#29992;&#26435;&#34913;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#32771;&#34385;&#20102;&#19981;&#21516;&#21512;&#20316;&#21442;&#25968;&#19979;&#29609;&#23478;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;Gittins&#25351;&#25968;&#31616;&#21270;&#20102;&#21333;&#20154;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/1908.01135</link><description>&lt;p&gt;
&#22810;&#20154;&#36172;&#21338;&#23398;&#20064;&#65292;&#20174;&#31454;&#20105;&#21040;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Multiplayer Bandit Learning, from Competition to Cooperation. (arXiv:1908.01135v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1908.01135
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#36172;&#21338;&#23398;&#20064;&#20013;&#31454;&#20105;&#21644;&#21512;&#20316;&#23545;&#25506;&#32034;&#21644;&#21033;&#29992;&#26435;&#34913;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#32771;&#34385;&#20102;&#19981;&#21516;&#21512;&#20316;&#21442;&#25968;&#19979;&#29609;&#23478;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;Gittins&#25351;&#25968;&#31616;&#21270;&#20102;&#21333;&#20154;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31454;&#20105;&#21644;&#21512;&#20316;&#23545;&#36825;&#31181;&#26435;&#34913;&#30340;&#24433;&#21709;&#12290;&#20551;&#35774;&#26377;k&#20010;&#33218;&#21644;&#20004;&#21517;&#29609;&#23478;&#65292;Alice&#21644;Bob&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#27599;&#20010;&#29609;&#23478;&#25289;&#21160;&#19968;&#20010;&#33218;&#65292;&#25509;&#25910;&#21040;&#30456;&#24212;&#30340;&#22870;&#21169;&#65292;&#24182;&#35266;&#23519;&#21040;&#23545;&#26041;&#30340;&#36873;&#25321;&#20294;&#19981;&#30693;&#36947;&#20182;&#20204;&#30340;&#22870;&#21169;&#12290;Alice&#30340;&#25928;&#29992;&#20989;&#25968;&#20026;$\Gamma_A + \lambda \Gamma_B$&#65288;Bob&#30340;&#25928;&#29992;&#20989;&#25968;&#31867;&#20284;&#65289;&#65292;&#20854;&#20013;$\Gamma_A$&#26159;Alice&#30340;&#24635;&#22870;&#21169;&#65292;$\lambda \in [-1, 1]$&#26159;&#21512;&#20316;&#21442;&#25968;&#12290;&#24403;$\lambda = -1$&#26102;&#65292;&#29609;&#23478;&#22312;&#19968;&#20010;&#38646;&#21644;&#28216;&#25103;&#20013;&#31454;&#20105;&#65307;&#24403;$\lambda = 1$&#26102;&#65292;&#20182;&#20204;&#23436;&#20840;&#21512;&#20316;&#65307;&#24403;$\lambda = 0$&#26102;&#65292;&#20182;&#20204;&#26159;&#20013;&#31435;&#30340;&#65306;&#27599;&#20010;&#29609;&#23478;&#30340;&#25928;&#29992;&#20989;&#25968;&#20026;&#20182;&#20204;&#33258;&#24049;&#30340;&#22870;&#21169;&#12290;&#35813;&#27169;&#22411;&#19982;&#25112;&#30053;&#23454;&#39564;&#32463;&#27982;&#23398;&#25991;&#29486;&#20013;&#20851;&#20110;&#35266;&#23519;&#23545;&#26041;&#22870;&#21169;&#30340;&#30740;&#31350;&#30456;&#20851;&#12290;&#20351;&#29992;&#25240;&#25187;&#22240;&#23376;$\beta$&#65292;Gittins&#25351;&#25968;&#23558;&#21333;&#20154;&#38382;&#39064;&#31616;&#21270;&#20026;&#23545;&#19968;&#20010;&#24102;&#26377;&#20808;&#39564;$\mu$&#30340;&#26377;&#39118;&#38505;&#33218;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic multi-armed bandit model captures the tradeoff between exploration and exploitation. We study the effects of competition and cooperation on this tradeoff. Suppose there are $k$ arms and two players, Alice and Bob. In every round, each player pulls an arm, receives the resulting reward, and observes the choice of the other player but not their reward. Alice's utility is $\Gamma_A + \lambda \Gamma_B$ (and similarly for Bob), where $\Gamma_A$ is Alice's total reward and $\lambda \in [-1, 1]$ is a cooperation parameter. At $\lambda = -1$ the players are competing in a zero-sum game, at $\lambda = 1$, they are fully cooperating, and at $\lambda = 0$, they are neutral: each player's utility is their own reward. The model is related to the economics literature on strategic experimentation, where usually players observe each other's rewards.  With discount factor $\beta$, the Gittins index reduces the one-player problem to the comparison between a risky arm, with a prior $\mu$, 
&lt;/p&gt;</description></item></channel></rss>