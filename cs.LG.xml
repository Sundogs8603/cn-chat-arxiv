<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#39640;&#32467;&#26524;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#21161;&#20110;&#30830;&#23450;&#26368;&#36866;&#21512;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20294;&#22312;&#23384;&#22312;&#22810;&#31181;&#21487;&#33021;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20165;&#20381;&#36182;&#32467;&#26524;&#39044;&#27979;&#36827;&#34892;&#24178;&#39044;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04470</link><description>&lt;p&gt;
&#20851;&#20110;&#32467;&#26524;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Actionability of Outcome Prediction. (arXiv:2309.04470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04470
&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#32467;&#26524;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#21161;&#20110;&#30830;&#23450;&#26368;&#36866;&#21512;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20294;&#22312;&#23384;&#22312;&#22810;&#31181;&#21487;&#33021;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20165;&#20381;&#36182;&#32467;&#26524;&#39044;&#27979;&#36827;&#34892;&#24178;&#39044;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#24433;&#21709;&#39046;&#22495;&#65292;&#39044;&#27979;&#26410;&#26469;&#32467;&#26524;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#24120;&#35265;&#24212;&#29992;&#12290;&#20363;&#23376;&#20174;&#39044;&#27979;&#25945;&#32946;&#20013;&#23398;&#29983;&#30340;&#25104;&#21151;&#21040;&#39044;&#27979;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#30142;&#30149;&#39118;&#38505;&#12290;&#20174;&#23454;&#36341;&#32773;&#26469;&#30475;&#65292;&#26368;&#32456;&#30446;&#26631;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#65292;&#32780;&#26159;&#26377;&#25928;&#22320;&#34892;&#21160;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#20165;&#20165;&#20381;&#38752;&#32467;&#26524;&#39044;&#27979;&#36827;&#34892;&#19979;&#28216;&#24178;&#39044;&#21487;&#33021;&#19981;&#20250;&#20135;&#29983;&#39044;&#26399;&#32467;&#26524;&#12290;&#22312;&#22823;&#22810;&#25968;&#39046;&#22495;&#20013;&#65292;&#27599;&#20010;&#20010;&#20307;&#23384;&#22312;&#22810;&#31181;&#21487;&#33021;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#36825;&#20351;&#24471;&#37319;&#21462;&#26377;&#25928;&#34892;&#21160;&#30340;&#25361;&#25112;&#26356;&#21152;&#20005;&#23803;&#12290;&#21363;&#20351;&#36830;&#25509;&#20010;&#20307;&#28508;&#22312;&#29366;&#24577;&#19982;&#32467;&#26524;&#30340;&#22240;&#26524;&#26426;&#21046;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#65292;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#23454;&#20363;&#65288;&#29305;&#23450;&#30340;&#23398;&#29983;&#25110;&#24739;&#32773;&#65289;&#20013;&#65292;&#23454;&#36341;&#32773;&#20173;&#28982;&#38656;&#35201;&#20174;&#39044;&#31639;&#27979;&#37327;&#30340;&#28508;&#22312;&#29366;&#24577;&#20013;&#25512;&#26029;&#20986;&#21738;&#31181;&#21487;&#33021;&#30340;&#24178;&#39044;&#25514;&#26045;&#23545;&#36825;&#20010;&#20010;&#20307;&#26368;&#26377;&#25928;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24819;&#38382;&#65306;&#20934;&#30830;&#30340;&#32467;&#26524;&#39044;&#27979;&#20309;&#26102;&#26377;&#21161;&#20110;&#35782;&#21035;&#26368;&#21512;&#36866;&#30340;&#24178;&#39044;&#25514;&#26045;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting future outcomes is a prevalent application of machine learning in social impact domains. Examples range from predicting student success in education to predicting disease risk in healthcare. Practitioners recognize that the ultimate goal is not just to predict but to act effectively. Increasing evidence suggests that relying on outcome predictions for downstream interventions may not have desired results.  In most domains there exists a multitude of possible interventions for each individual, making the challenge of taking effective action more acute. Even when causal mechanisms connecting the individual's latent states to outcomes is well understood, in any given instance (a specific student or patient), practitioners still need to infer -- from budgeted measurements of latent states -- which of many possible interventions will be most effective for this individual. With this in mind, we ask: when are accurate predictors of outcomes helpful for identifying the most suitable
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20154;&#31867;&#25512;&#29702;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#36890;&#36807;&#19968;&#20010;&#27969;&#27700;&#32447;&#21644;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27979;&#37327;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04461</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models. (arXiv:2309.04461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20154;&#31867;&#25512;&#29702;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#36890;&#36807;&#19968;&#20010;&#27969;&#27700;&#32447;&#21644;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27979;&#37327;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#33021;&#35299;&#26512;&#20851;&#20110;&#35270;&#35273;&#20869;&#23481;&#30340;&#33258;&#28982;&#26597;&#35810;&#24182;&#29983;&#25104;&#31867;&#20154;&#36755;&#20986;&#30340;&#35270;&#35273;&#21161;&#25163;&#65292;&#23637;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#21151;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#22522;&#20110;&#25152;&#24863;&#30693;&#20449;&#24687;&#30340;&#31867;&#20154;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#20851;&#20110;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#21040;&#24213;&#26377;&#22810;&#19968;&#33268;&#21644;&#26377;&#22810;&#22522;&#20110;&#23454;&#38469;&#30340;&#19968;&#20010;&#37325;&#35201;&#30097;&#34385;&#65292;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35780;&#20272;&#38656;&#35201;&#28085;&#30422;&#39640;&#23618;&#27425;&#25512;&#29702;&#21644;&#32454;&#33410;&#25512;&#29702;&#38142;&#30340;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;LLM-Human-in-the-Loop&#27969;&#27700;&#32447;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#27969;&#27700;&#32447;&#26174;&#33879;&#38477;&#20302;&#20102;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20010;&#27969;&#27700;&#32447;&#21644;&#29616;&#26377;&#30340;&#31895;&#31890;&#24230;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;CURE&#22522;&#20934;&#26469;&#21516;&#26102;&#27979;&#37327;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing a LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#24182;&#37319;&#29992;&#20998;&#35789;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#25216;&#24039;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#25506;&#32034;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04459</link><description>&lt;p&gt;
&#23376;&#35789;&#20316;&#20026;&#25216;&#24039;&#65306;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#35789;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning. (arXiv:2309.04459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#24182;&#37319;&#29992;&#20998;&#35789;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#25216;&#24039;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#25506;&#32034;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#20855;&#26377;&#22256;&#38590;&#65292;&#22240;&#20026;&#38656;&#35201;&#36890;&#36807;&#38271;&#26399;&#30340;&#12289;&#21327;&#35843;&#30340;&#34892;&#21160;&#24207;&#21015;&#25165;&#33021;&#33719;&#24471;&#20219;&#20309;&#22870;&#21169;&#12290;&#32780;&#19988;&#65292;&#22312;&#36830;&#32493;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65292;&#21487;&#33021;&#30340;&#34892;&#21160;&#25968;&#37327;&#26159;&#26080;&#31351;&#22810;&#30340;&#65292;&#36825;&#21482;&#20250;&#22686;&#21152;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#19968;&#31867;&#26041;&#27861;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#25910;&#38598;&#30340;&#20132;&#20114;&#25968;&#25454;&#20013;&#24418;&#25104;&#26102;&#38388;&#19978;&#24310;&#20280;&#30340;&#34892;&#21160;&#65292;&#36890;&#24120;&#31216;&#20026;&#25216;&#24039;&#65292;&#24182;&#22312;&#36825;&#20010;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#30340;&#20248;&#21270;&#12290;&#36890;&#24120;&#36825;&#26679;&#30340;&#26041;&#27861;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#38656;&#35201;&#19968;&#20010;&#28459;&#38271;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#24320;&#22987;&#20043;&#21069;&#24418;&#25104;&#25216;&#24039;&#12290;&#37492;&#20110;&#20808;&#21069;&#30340;&#35777;&#25454;&#34920;&#26126;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24182;&#19981;&#38656;&#35201;&#23436;&#25972;&#30340;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#24039;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#65292;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20511;&#37492;&#26469;&#30340;&#20998;&#35789;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#38598;&#21512;&#22825;&#27668;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#19981;&#21516;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#35813;&#32593;&#32476;&#23558;&#39044;&#27979;&#38598;&#21512;&#35270;&#20026;&#19968;&#32452;&#26080;&#24207;&#30340;&#25104;&#21592;&#39044;&#27979;&#65292;&#24182;&#23398;&#20064;&#23545;&#25104;&#21592;&#39034;&#24207;&#30340;&#25490;&#21015;&#32622;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#38142;&#25509;&#20989;&#25968;&#12290;&#22312;&#22320;&#34920;&#28201;&#24230;&#21644;&#39118;&#36895;&#39044;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.04452</link><description>&lt;p&gt;
&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#38598;&#21512;&#22825;&#27668;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks. (arXiv:2309.04452v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;&#38598;&#21512;&#22825;&#27668;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#19981;&#21516;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#35813;&#32593;&#32476;&#23558;&#39044;&#27979;&#38598;&#21512;&#35270;&#20026;&#19968;&#32452;&#26080;&#24207;&#30340;&#25104;&#21592;&#39044;&#27979;&#65292;&#24182;&#23398;&#20064;&#23545;&#25104;&#21592;&#39034;&#24207;&#30340;&#25490;&#21015;&#32622;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#38142;&#25509;&#20989;&#25968;&#12290;&#22312;&#22320;&#34920;&#28201;&#24230;&#21644;&#39118;&#36895;&#39044;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#21518;&#22788;&#29702;&#29992;&#20110;&#23558;&#21407;&#22987;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#30340;&#38598;&#21512;&#36716;&#21270;&#20026;&#21487;&#38752;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36825;&#19968;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36890;&#24120;&#22522;&#20110;&#38598;&#21512;&#27010;&#35201;&#32479;&#35745;&#20449;&#24687;&#24182;&#24573;&#30053;&#38598;&#21512;&#20998;&#24067;&#30340;&#32454;&#33410;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#23558;&#39044;&#27979;&#38598;&#21512;&#35270;&#20026;&#19968;&#32452;&#26080;&#24207;&#30340;&#25104;&#21592;&#39044;&#27979;&#65292;&#24182;&#23398;&#20064;&#23545;&#25104;&#21592;&#39034;&#24207;&#30340;&#25490;&#21015;&#32622;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#38142;&#25509;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#26657;&#20934;&#24230;&#21644;&#38160;&#24230;&#35780;&#20272;&#25152;&#33719;&#24471;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#36136;&#37327;&#65292;&#24182;&#23558;&#27169;&#22411;&#19982;&#32463;&#20856;&#30340;&#22522;&#20934;&#26041;&#27861;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#22788;&#29702;&#22320;&#34920;&#28201;&#24230;&#21644;&#39118;&#36895;&#39044;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;&#20026;&#20102;&#21152;&#28145;&#23545;&#23398;&#20064;&#25512;&#29702;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#32622;&#25442;&#30340;&#37325;&#35201;&#24615;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical postprocessing is used to translate ensembles of raw numerical weather forecasts into reliable probabilistic forecast distributions. In this study, we examine the use of permutation-invariant neural networks for this task. In contrast to previous approaches, which often operate on ensemble summary statistics and dismiss details of the ensemble distribution, we propose networks which treat forecast ensembles as a set of unordered member forecasts and learn link functions that are by design invariant to permutations of the member ordering. We evaluate the quality of the obtained forecast distributions in terms of calibration and sharpness, and compare the models against classical and neural network-based benchmark methods. In case studies addressing the postprocessing of surface temperature and wind gust forecasts, we demonstrate state-of-the-art prediction quality. To deepen the understanding of the learned inference process, we further propose a permutation-based importance
&lt;/p&gt;</description></item><item><title>&#27491;&#35268;&#21270;&#27969;&#26159;&#19968;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#19968;&#31995;&#21015;&#21452;&#23556;&#21464;&#25442;&#23558;&#22797;&#26434;&#30446;&#26631;&#20998;&#24067;&#34920;&#31034;&#20026;&#31616;&#21333;&#22522;&#30784;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#24494;&#20998;&#21516;&#32986;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#26377;&#25928;&#34920;&#31034;&#22797;&#26434;&#25299;&#25169;&#30446;&#26631;&#20998;&#24067;&#21644;&#22788;&#29702;&#20808;&#39564;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#19981;&#21516;&#32986;&#30340;&#24773;&#20917;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#30456;&#20851;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#25918;&#23485;&#20102;&#27491;&#35268;&#21270;&#27969;&#30340;&#32422;&#26463;&#24615;&#65292;&#20197;&#24179;&#34913;&#32422;&#26463;&#21644;&#28789;&#27963;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;</title><link>http://arxiv.org/abs/2309.04433</link><description>&lt;p&gt;
&#27491;&#35268;&#21270;&#27969;&#30340;&#21464;&#21270;&#21644;&#25918;&#26494;
&lt;/p&gt;
&lt;p&gt;
Variations and Relaxations of Normalizing Flows. (arXiv:2309.04433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04433
&lt;/p&gt;
&lt;p&gt;
&#27491;&#35268;&#21270;&#27969;&#26159;&#19968;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#19968;&#31995;&#21015;&#21452;&#23556;&#21464;&#25442;&#23558;&#22797;&#26434;&#30446;&#26631;&#20998;&#24067;&#34920;&#31034;&#20026;&#31616;&#21333;&#22522;&#30784;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#24494;&#20998;&#21516;&#32986;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#26377;&#25928;&#34920;&#31034;&#22797;&#26434;&#25299;&#25169;&#30446;&#26631;&#20998;&#24067;&#21644;&#22788;&#29702;&#20808;&#39564;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#19981;&#21516;&#32986;&#30340;&#24773;&#20917;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#30456;&#20851;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#25918;&#23485;&#20102;&#27491;&#35268;&#21270;&#27969;&#30340;&#32422;&#26463;&#24615;&#65292;&#20197;&#24179;&#34913;&#32422;&#26463;&#21644;&#28789;&#27963;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#35268;&#21270;&#27969;&#65288;NFs&#65289;&#25551;&#36848;&#20102;&#19968;&#31867;&#23558;&#22797;&#26434;&#30446;&#26631;&#20998;&#24067;&#34920;&#31034;&#20026;&#31616;&#21333;&#22522;&#30784;&#20998;&#24067;&#30340;&#19968;&#31995;&#21015;&#21452;&#23556;&#21464;&#25442;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#20505;&#36873;&#21464;&#25442;&#31354;&#38388;&#38480;&#21046;&#20026;&#24494;&#20998;&#21516;&#32986;&#65292;NFs&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#31934;&#30830;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#20351;&#20854;&#26082;&#33021;&#28789;&#27963;&#22320;&#20316;&#20026;&#21028;&#21035;&#27169;&#22411;&#65292;&#21448;&#33021;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#24494;&#20998;&#21516;&#32986;&#30340;&#38480;&#21046;&#24378;&#21046;&#36755;&#20837;&#12289;&#36755;&#20986;&#21644;&#25152;&#26377;&#20013;&#38388;&#31354;&#38388;&#20855;&#26377;&#30456;&#21516;&#30340;&#32500;&#25968;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#26377;&#25928;&#34920;&#31034;&#20855;&#26377;&#22797;&#26434;&#25299;&#25169;&#30340;&#30446;&#26631;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#39564;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#19981;&#21516;&#32986;&#30340;&#24773;&#20917;&#19979;&#65292;&#27491;&#35268;&#21270;&#27969;&#21487;&#33021;&#20250;&#23558;&#36136;&#37327;&#27844;&#28431;&#21040;&#30446;&#26631;&#25903;&#25345;&#20043;&#22806;&#12290;&#26412;&#32508;&#36848;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#23558;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65288;&#22914;VAEs&#21644;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#65289;&#30340;&#26041;&#38754;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#25918;&#23485;&#20102;NFs&#30340;&#20005;&#26684;&#21452;&#23556;&#32422;&#26463;&#65292;&#20197;&#23454;&#29616;&#24179;&#34913;&#32422;&#26463;&#19982;&#28789;&#27963;&#24615;&#20043;&#38388;&#30340;&#25240;&#20013;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows (NFs) describe a class of models that express a complex target distribution as the composition of a series of bijective transformations over a simpler base distribution. By limiting the space of candidate transformations to diffeomorphisms, NFs enjoy efficient, exact sampling and density evaluation, enabling NFs to flexibly behave as both discriminative and generative models. Their restriction to diffeomorphisms, however, enforces that input, output and all intermediary spaces share the same dimension, limiting their ability to effectively represent target distributions with complex topologies. Additionally, in cases where the prior and target distributions are not homeomorphic, Normalizing Flows can leak mass outside of the support of the target. This survey covers a selection of recent works that combine aspects of other generative model classes, such as VAEs and score-based diffusion, and in doing so loosen the strict bijectivity constraints of NFs to achieve a bal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;&#21644;&#36719;&#26368;&#23567;&#20989;&#25968;&#26469;&#35299;&#20915;&#37327;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#35843;&#33410;&#20248;&#21270;&#38382;&#39064;&#38590;&#24230;&#30340;&#25511;&#21046;&#21442;&#25968;&#65292;&#21487;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#38382;&#39064;&#26102;&#25552;&#20379;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.04428</link><description>&lt;p&gt;
&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;&#30340;&#36719;&#37327;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Soft Quantization using Entropic Regularization. (arXiv:2309.04428v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;&#21644;&#36719;&#26368;&#23567;&#20989;&#25968;&#26469;&#35299;&#20915;&#37327;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#35843;&#33410;&#20248;&#21270;&#38382;&#39064;&#38590;&#24230;&#30340;&#25511;&#21046;&#21442;&#25968;&#65292;&#21487;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#38382;&#39064;&#26102;&#25552;&#20379;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#38382;&#39064;&#26088;&#22312;&#36890;&#36807;&#26377;&#38480;&#30340;&#31163;&#25955;&#27979;&#24230;&#25214;&#21040;&#22312;${\mathbb{R}}^d$&#19978;&#27010;&#29575;&#27979;&#24230;&#30340;&#26368;&#20339;&#36817;&#20284;&#12290;Wasserstein&#36317;&#31163;&#26159;&#34913;&#37327;&#36817;&#20284;&#36136;&#37327;&#30340;&#20856;&#22411;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29109;&#27491;&#21017;&#21270;&#37327;&#21270;&#38382;&#39064;&#30340;&#24615;&#36136;&#21644;&#40065;&#26834;&#24615;&#65292;&#35813;&#26041;&#27861;&#25918;&#26494;&#20102;&#26631;&#20934;&#30340;&#37327;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;&#36817;&#20284;&#25216;&#26415;&#33258;&#28982;&#22320;&#37319;&#29992;&#20102;&#36719;&#26368;&#23567;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#22240;&#20854;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#32780;&#38395;&#21517;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;&#30340;Wasserstein&#36317;&#31163;&#26469;&#35780;&#20272;&#36719;&#37327;&#21270;&#38382;&#39064;&#36817;&#20284;&#30340;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#26469;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#30340;&#25511;&#21046;&#21442;&#25968;&#21487;&#20197;&#35843;&#25972;&#20248;&#21270;&#38382;&#39064;&#30340;&#38590;&#24230;&#32423;&#21035;&#65292;&#22312;&#22788;&#29702;&#24322;&#24120;&#25361;&#25112;&#24615;&#38382;&#39064;&#26102;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantization problem aims to find the best possible approximation of probability measures on ${\mathbb{R}}^d$ using finite, discrete measures. The Wasserstein distance is a typical choice to measure the quality of the approximation. This contribution investigates the properties and robustness of the entropy-regularized quantization problem, which relaxes the standard quantization problem. The proposed approximation technique naturally adopts the softmin function, which is well known for its robustness in terms of theoretical and practicability standpoints. Moreover, we use the entropy-regularized Wasserstein distance to evaluate the quality of the soft quantization problem's approximation, and we implement a stochastic gradient approach to achieve the optimal solutions. The control parameter in our proposed method allows for the adjustment of the optimization problem's difficulty level, providing significant advantages when dealing with exceptionally challenging problems of interes
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#40065;&#26834;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#32534;&#30721;&#25968;&#25454;&#20197;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#25216;&#26415;&#20013;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04427</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#31181;&#22810;&#30446;&#26631;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Representation Learning for Privacy-Preserving Machine Learning: A Multi-Objective Autoencoder Approach. (arXiv:2309.04427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#40065;&#26834;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#32534;&#30721;&#25968;&#25454;&#20197;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#25216;&#26415;&#20013;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23545;&#25968;&#25454;&#30340;&#37325;&#24230;&#20381;&#36182;&#23548;&#33268;&#20102;&#21508;&#31181;&#25968;&#25454;&#20262;&#29702;&#21644;&#38544;&#31169;&#27861;&#35268;&#30340;&#20986;&#29616;&#65292;&#24182;&#19988;&#23545;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;ppML&#65289;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#24403;&#21069;&#30340;ppML&#25216;&#26415;&#20027;&#35201;&#37319;&#29992;&#22522;&#20110;&#23494;&#30721;&#23398;&#30340;&#26041;&#27861;&#65292;&#22914;&#21516;&#24577;&#21152;&#23494;&#65292;&#25110;&#32773;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#24341;&#20837;&#22122;&#22768;&#65292;&#22914;&#24046;&#20998;&#38544;&#31169;&#12290;&#23545;&#36825;&#20123;&#25216;&#26415;&#30340;&#20027;&#35201;&#25209;&#35780;&#26159;&#23427;&#20204;&#35201;&#20040;&#36807;&#20110;&#32531;&#24930;&#65292;&#35201;&#20040;&#22312;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#26426;&#23494;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#40065;&#26834;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#32534;&#30721;&#25968;&#25454;&#65292;&#21516;&#26102;&#20248;&#21270;&#38544;&#31169;&#25928;&#29992;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#36890;&#36807;&#20197;&#22810;&#30446;&#26631;&#30340;&#26041;&#24335;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#65292;&#28982;&#21518;&#23558;&#32534;&#30721;&#37096;&#20998;&#30340;&#28508;&#22312;&#29305;&#24449;&#21644;&#23398;&#20064;&#29305;&#24449;&#36830;&#25509;&#36215;&#26469;&#65292;&#20316;&#20026;&#25105;&#20204;&#25968;&#25454;&#30340;&#32534;&#30721;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several domains increasingly rely on machine learning in their applications. The resulting heavy dependence on data has led to the emergence of various laws and regulations around data ethics and privacy and growing awareness of the need for privacy-preserving machine learning (ppML). Current ppML techniques utilize methods that are either purely based on cryptography, such as homomorphic encryption, or that introduce noise into the input, such as differential privacy. The main criticism given to those techniques is the fact that they either are too slow or they trade off a model s performance for improved confidentiality. To address this performance reduction, we aim to leverage robust representation learning as a way of encoding our data while optimizing the privacy-utility trade-off. Our method centers on training autoencoders in a multi-objective manner and then concatenating the latent and learned features from the encoding part as the encoded form of our data. Such a deep learnin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#21464;&#20998;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;SVDKL&#65289;&#30340;&#22768;&#38899;&#36716;&#25442;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#26377;&#38480;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#22797;&#26434;&#20989;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.04420</link><description>&lt;p&gt;
&#24182;&#34892;&#19988;&#26377;&#38480;&#25968;&#25454;&#30340;&#22768;&#38899;&#36716;&#25442;&#20351;&#29992;&#38543;&#26426;&#21464;&#20998;&#28145;&#24230;&#26680;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Parallel and Limited Data Voice Conversion Using Stochastic Variational Deep Kernel Learning. (arXiv:2309.04420v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04420
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#21464;&#20998;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;SVDKL&#65289;&#30340;&#22768;&#38899;&#36716;&#25442;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#26377;&#38480;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#22797;&#26434;&#20989;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22768;&#38899;&#36716;&#25442;&#34987;&#35270;&#20026;&#19968;&#20010;&#25317;&#26377;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#24037;&#31243;&#38382;&#39064;&#12290;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#26368;&#36817;&#20960;&#24180;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32479;&#35745;&#26041;&#27861;&#22312;&#26377;&#38480;&#25968;&#25454;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#24314;&#27169;&#22797;&#26434;&#26144;&#23556;&#20989;&#25968;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#21464;&#20998;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;SVDKL&#65289;&#30340;&#33021;&#22815;&#36866;&#24212;&#26377;&#38480;&#25968;&#25454;&#30340;&#22768;&#38899;&#36716;&#25442;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;SVDKL&#26082;&#33021;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21448;&#33021;&#21033;&#29992;&#39640;&#28789;&#27963;&#24615;&#30340;&#39640;&#26031;&#36807;&#31243;&#20316;&#20026;&#36125;&#21494;&#26031;&#21644;&#38750;&#21442;&#25968;&#26041;&#27861;&#12290;&#24403;&#23558;&#20256;&#32479;&#26680;&#20989;&#25968;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#26102;&#65292;&#21487;&#20197;&#20272;&#35745;&#38750;&#20809;&#28369;&#19988;&#26356;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#19982;&#31934;&#30830;&#39640;&#26031;&#36807;&#31243;&#19981;&#21516;&#65292;&#21487;&#20197;&#36827;&#34892;&#36817;&#20284;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typically, voice conversion is regarded as an engineering problem with limited training data. The reliance on massive amounts of data hinders the practical applicability of deep learning approaches, which have been extensively researched in recent years. On the other hand, statistical methods are effective with limited data but have difficulties in modelling complex mapping functions. This paper proposes a voice conversion method that works with limited data and is based on stochastic variational deep kernel learning (SVDKL). At the same time, SVDKL enables the use of deep neural networks' expressive capability as well as the high flexibility of the Gaussian process as a Bayesian and non-parametric method. When the conventional kernel is combined with the deep neural network, it is possible to estimate non-smooth and more complex functions. Furthermore, the model's sparse variational Gaussian process solves the scalability problem and, unlike the exact Gaussian process, allows for the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#32447;&#24615;&#29289;&#29702;&#32593;&#32476;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#23398;&#20064;&#34892;&#20026;&#19982;&#26080;&#24207;&#21644;&#29627;&#29827;&#29366;&#31995;&#32479;&#20013;&#30340;&#32769;&#21270;&#21644;&#35760;&#24518;&#24418;&#25104;&#36807;&#31243;&#30456;&#20284;&#12290;&#23398;&#20064;&#21160;&#24577;&#31867;&#20284;&#20110;&#32769;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#37325;&#22797;&#26045;&#21152;&#21453;&#39304;&#36793;&#30028;&#21147;&#26469;&#25918;&#26494;&#24182;&#32534;&#30721;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#30340;&#35760;&#24518;&#12290;</title><link>http://arxiv.org/abs/2309.04382</link><description>&lt;p&gt;
&#22312;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#32039;&#24613;&#23398;&#20064;&#20316;&#20026;&#22522;&#20110;&#21453;&#39304;&#30340;&#29627;&#29827;&#26223;&#35266;&#20013;&#30340;&#32769;&#21270;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Emergent learning in physical systems as feedback-based aging in a glassy landscape. (arXiv:2309.04382v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04382
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#32447;&#24615;&#29289;&#29702;&#32593;&#32476;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#23398;&#20064;&#34892;&#20026;&#19982;&#26080;&#24207;&#21644;&#29627;&#29827;&#29366;&#31995;&#32479;&#20013;&#30340;&#32769;&#21270;&#21644;&#35760;&#24518;&#24418;&#25104;&#36807;&#31243;&#30456;&#20284;&#12290;&#23398;&#20064;&#21160;&#24577;&#31867;&#20284;&#20110;&#32769;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#37325;&#22797;&#26045;&#21152;&#21453;&#39304;&#36793;&#30028;&#21147;&#26469;&#25918;&#26494;&#24182;&#32534;&#30721;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#32447;&#24615;&#29289;&#29702;&#32593;&#32476;&#26469;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#29289;&#29702;&#29305;&#24615;&#26159;&#30001;&#26435;&#37325;&#26356;&#26032;&#35268;&#21017;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#36825;&#26679;&#30340;&#32593;&#32476;&#23398;&#20064;&#34892;&#20026;&#19982;&#26080;&#24207;&#21644;&#29627;&#29827;&#29366;&#31995;&#32479;&#20013;&#32769;&#21270;&#21644;&#35760;&#24518;&#24418;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;&#24778;&#20154;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#21160;&#24577;&#31867;&#20284;&#20110;&#32769;&#21270;&#36807;&#31243;&#65292;&#31995;&#32479;&#22312;&#38754;&#23545;&#36755;&#20837;&#21147;&#30340;&#21453;&#39304;&#36793;&#30028;&#21147;&#30340;&#37325;&#22797;&#26045;&#21152;&#26102;&#25918;&#26494;&#24182;&#32534;&#30721;&#20102;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#30340;&#35760;&#24518;&#12290;&#38543;&#30528;&#36825;&#31181;&#25918;&#26494;&#65292;&#30456;&#20851;&#38271;&#24230;&#22686;&#21152;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#32593;&#32476;&#30340;&#20998;&#37327;&#30340;&#20004;&#28857;&#30456;&#20851;&#20989;&#25968;&#26469;&#25351;&#31034;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#35823;&#24046;&#22343;&#26041;&#26681;&#30340;&#24179;&#26041;&#26681;&#20316;&#20026;&#26102;&#38388;&#30340;&#20989;&#25968;&#21576;&#38750;&#25351;&#25968;&#24418;&#24335;&#65292;&#36825;&#26159;&#29627;&#29827;&#31995;&#32479;&#30340;&#20856;&#22411;&#29305;&#24449;&#12290;&#36825;&#31181;&#29289;&#29702;&#35299;&#37322;&#34920;&#26126;&#36890;&#36807;&#23558;&#26356;&#35814;&#32454;&#30340;&#20449;&#24687;&#32534;&#30721;&#21040;&#36755;&#20837;&#21644;&#21453;&#39304;&#36793;&#30028;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
By training linear physical networks to learn linear transformations, we discern how their physical properties evolve due to weight update rules. Our findings highlight a striking similarity between the learning behaviors of such networks and the processes of aging and memory formation in disordered and glassy systems. We show that the learning dynamics resembles an aging process, where the system relaxes in response to repeated application of the feedback boundary forces in presence of an input force, thus encoding a memory of the input-output relationship. With this relaxation comes an increase in the correlation length, which is indicated by the two-point correlation function for the components of the network. We also observe that the square root of the mean-squared error as a function of epoch takes on a non-exponential form, which is a typical feature of glassy systems. This physical interpretation suggests that by encoding more detailed information into input and feedback boundar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#33324;&#21270;&#30028;&#38480;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#20449;&#24687;&#35770;&#21644;PAC-Bayesian&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20849;&#21516;&#28857;&#12290;&#36825;&#23545;&#20110;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#21644;&#26032;&#31639;&#27861;&#30340;&#35774;&#35745;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.04381</link><description>&lt;p&gt;
&#19968;&#33324;&#21270;&#30028;&#38480;&#65306;&#20449;&#24687;&#35770;&#21644;PAC-Bayesian&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds: Perspectives from Information Theory and PAC-Bayes. (arXiv:2309.04381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#33324;&#21270;&#30028;&#38480;&#30340;&#20004;&#20010;&#35270;&#35282;&#65306;&#20449;&#24687;&#35770;&#21644;PAC-Bayesian&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20849;&#21516;&#28857;&#12290;&#36825;&#23545;&#20110;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#21644;&#26032;&#31639;&#27861;&#30340;&#35774;&#35745;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#19968;&#33324;&#21270;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;PAC-Bayesian&#26041;&#27861;&#24050;&#32463;&#34987;&#30830;&#23450;&#20026;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#19968;&#33324;&#21270;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#23545;&#22810;&#31181;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#28508;&#22312;&#36866;&#29992;&#24615;&#65292;&#23427;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36824;&#21457;&#23637;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#65292;&#20854;&#20013;&#24314;&#31435;&#20102;&#19968;&#33324;&#21270;&#19982;&#21508;&#31181;&#20449;&#24687;&#24230;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20010;&#26694;&#26550;&#19982;PAC-Bayesian&#26041;&#27861;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#26377;&#29420;&#31435;&#21457;&#29616;&#30340;&#24456;&#22810;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#36825;&#31181;&#24378;&#36830;&#25509;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#19968;&#33324;&#21270;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#35270;&#35282;&#20849;&#21516;&#25317;&#26377;&#30340;&#25216;&#26415;&#21644;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#21644;&#35299;&#37322;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#36830;&#25509;&#22914;&#20309;&#20135;&#29983;&#26032;&#30340;&#27934;&#35265;&#21644;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#20132;&#21449;&#24212;&#29992;&#21644;&#28508;&#22312;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of generalization. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21147;&#21709;&#24212;&#24335;&#36816;&#21160;&#25511;&#21046;&#30340;&#23548;&#30450;&#22235;&#36275;&#26426;&#22120;&#20154;&#23548;&#33322;&#31995;&#32479;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#36816;&#21160;&#25511;&#21046;&#22120;&#21644;&#22806;&#21147;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#23545;&#22806;&#37096;&#25289;&#25199;&#21147;&#30340;&#31283;&#20581;&#24863;&#30693;&#21644;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#23548;&#33322;&#24182;&#32469;&#36807;&#38556;&#30861;&#29289;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04370</link><description>&lt;p&gt;
&#24102;&#26377;&#21147;&#21709;&#24212;&#24335;&#36816;&#21160;&#25511;&#21046;&#30340;&#23548;&#30450;&#22235;&#36275;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control. (arXiv:2309.04370v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21147;&#21709;&#24212;&#24335;&#36816;&#21160;&#25511;&#21046;&#30340;&#23548;&#30450;&#22235;&#36275;&#26426;&#22120;&#20154;&#23548;&#33322;&#31995;&#32479;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#36816;&#21160;&#25511;&#21046;&#22120;&#21644;&#22806;&#21147;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#23545;&#22806;&#37096;&#25289;&#25199;&#21147;&#30340;&#31283;&#20581;&#24863;&#30693;&#21644;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#23548;&#33322;&#24182;&#32469;&#36807;&#38556;&#30861;&#29289;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23548;&#30450;&#26426;&#22120;&#20154;&#26159;&#20026;&#23548;&#24341;&#35270;&#38556;&#20154;&#22763;&#32780;&#35774;&#35745;&#30340;&#38750;&#24120;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#37492;&#20110;&#30495;&#27491;&#23548;&#30450;&#29356;&#30340;&#21487;&#33719;&#24471;&#24615;&#20302;&#19988;&#39640;&#25104;&#26412;&#65292;&#20854;&#21487;&#33021;&#20135;&#29983;&#24040;&#22823;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;&#23613;&#31649;&#24050;&#32463;&#28436;&#31034;&#20102;&#20960;&#20010;&#23548;&#30450;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#21040;&#23548;&#30450;&#29356;&#23454;&#38469;&#29615;&#22659;&#20013;&#32463;&#24120;&#21457;&#29983;&#30340;&#26469;&#33258;&#20154;&#31867;&#30340;&#22806;&#37096;&#25289;&#25199;&#12290;&#26412;&#25991;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21516;&#26102;&#35757;&#32451;&#20102;&#19968;&#20010;&#23545;&#22806;&#37096;&#25289;&#25199;&#21147;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#21644;&#19968;&#20010;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30340;&#22806;&#21147;&#20272;&#35745;&#22120;&#12290;&#25511;&#21046;&#22120;&#30830;&#20445;&#20102;&#31283;&#23450;&#30340;&#34892;&#36208;&#65292;&#22806;&#21147;&#20272;&#35745;&#22120;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23545;&#20154;&#31867;&#26045;&#21152;&#30340;&#22806;&#37096;&#21147;&#20570;&#20986;&#21709;&#24212;&#12290;&#36825;&#20123;&#21147;&#29992;&#20110;&#23558;&#26426;&#22120;&#20154;&#24341;&#23548;&#21040;&#26410;&#30693;&#30340;&#20840;&#23616;&#30446;&#26631;&#65292;&#21516;&#26102;&#26426;&#22120;&#20154;&#36890;&#36807;&#23616;&#37096;&#35268;&#21010;&#22120;&#23558;&#20154;&#31867;&#24341;&#23548;&#32469;&#36807;&#38468;&#36817;&#30340;&#38556;&#30861;&#29289;&#12290;&#22312;&#27169;&#25311;&#21644;&#30828;&#20214;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25511;&#21046;&#22120;&#23545;&#22806;&#37096;&#21147;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#23548;&#30450;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;...
&lt;/p&gt;
&lt;p&gt;
Seeing-eye robots are very useful tools for guiding visually impaired people, potentially producing a huge societal impact given the low availability and high cost of real guide dogs. Although a few seeing-eye robot systems have already been demonstrated, none considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;2D&#22522;&#20110;&#32593;&#26684;&#30340;&#20851;&#21345;&#21487;&#23436;&#25104;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;Super Mario Bros.&#12289;Kid Icarus&#21644;&#19968;&#20010;&#31867;&#20284;Zelda&#30340;&#28216;&#25103;&#29983;&#25104;&#30340;&#20851;&#21345;&#36827;&#34892;&#26631;&#35760;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04367</link><description>&lt;p&gt;
&#20026;&#20998;&#31867;2D&#22522;&#20110;&#32593;&#26684;&#30340;&#20851;&#21345;&#21487;&#23436;&#25104;&#24615;&#25552;&#20986;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Classifying 2D Grid-Based Level Completability. (arXiv:2309.04367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;2D&#22522;&#20110;&#32593;&#26684;&#30340;&#20851;&#21345;&#21487;&#23436;&#25104;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;Super Mario Bros.&#12289;Kid Icarus&#21644;&#19968;&#20010;&#31867;&#20284;Zelda&#30340;&#28216;&#25103;&#29983;&#25104;&#30340;&#20851;&#21345;&#36827;&#34892;&#26631;&#35760;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#30001;&#31243;&#24207;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#20851;&#21345;&#30340;&#21487;&#23436;&#25104;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#28041;&#21450;&#20351;&#29992;&#27714;&#35299;&#22120;&#20195;&#29702;&#26469;&#20998;&#26512;&#21644;&#35299;&#20915;&#20851;&#21345;&#65292;&#32780;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#12290;&#23613;&#31649;&#20027;&#21160;&#23398;&#20064;&#24050;&#32463;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#20687;&#21644;&#35821;&#38899;&#35782;&#21035;&#20197;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#20294;&#22312;&#28216;&#25103;&#35780;&#20272;&#20013;&#65292;&#23427;&#36824;&#27809;&#26377;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#25110;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#23398;&#20064;&#20851;&#21345;&#21487;&#23436;&#25104;&#24615;&#20998;&#31867;&#12290;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;Super Mario Bros.&#12289;Kid Icarus&#21644;&#19968;&#20010;&#31867;&#20284;Zelda&#30340;&#28216;&#25103;&#20013;&#29983;&#25104;&#30340;&#20851;&#21345;&#36827;&#34892;&#21487;&#23436;&#25104;&#24615;&#20998;&#31867;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26631;&#35760;&#20851;&#21345;&#19982;&#20351;&#29992;&#38543;&#26426;&#26597;&#35810;&#26631;&#35760;&#20851;&#21345;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#26631;&#35760;&#20851;&#21345;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#32780;&#26597;&#35810;&#26041;&#27861;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the completability of levels generated by procedural generators such as machine learning models can be challenging, as it can involve the use of solver agents that often require a significant amount of time to analyze and solve levels. Active learning is not yet widely adopted in game evaluations, although it has been used successfully in natural language processing, image and speech recognition, and computer vision, where the availability of labeled data is limited or expensive. In this paper, we propose the use of active learning for learning level completability classification. Through an active learning approach, we train deep-learning models to classify the completability of generated levels for Super Mario Bros., Kid Icarus, and a Zelda-like game. We compare active learning for querying levels to label with completability against random queries. Our results show using an active learning approach to label levels results in better classifier performance with the same am
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#26512;&#26469;&#35782;&#21035;&#30005;&#21147;&#20256;&#36755;&#31995;&#32479;&#20869;&#35760;&#24405;&#30340;&#24178;&#25200;&#20107;&#20214;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;&#22312;160&#20010;&#20449;&#21495;&#25991;&#20214;&#19978;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;99&#65285;&#12290;</title><link>http://arxiv.org/abs/2309.04361</link><description>&lt;p&gt;
&#20174;&#21151;&#29575;&#20449;&#21495;&#20013;&#23398;&#20064;&#65306;&#30005;&#21147;&#20256;&#36755;&#31995;&#32479;&#20013;&#30005;&#21147;&#24178;&#25200;&#35782;&#21035;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning from Power Signals: An Automated Approach to Electrical Disturbance Identification Within a Power Transmission System. (arXiv:2309.04361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#26512;&#26469;&#35782;&#21035;&#30005;&#21147;&#20256;&#36755;&#31995;&#32479;&#20869;&#35760;&#24405;&#30340;&#24178;&#25200;&#20107;&#20214;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;&#22312;160&#20010;&#20449;&#21495;&#25991;&#20214;&#19978;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;99&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#21147;&#36136;&#37327;&#22312;&#30005;&#21147;&#24037;&#19994;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#25552;&#39640;&#65292;&#24178;&#25200;&#20107;&#20214;&#25968;&#25454;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#12290;&#30005;&#21147;&#20844;&#21496;&#27809;&#26377;&#36275;&#22815;&#30340;&#20154;&#21592;&#25163;&#21160;&#20998;&#26512;&#27599;&#20010;&#20107;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#22312;&#30005;&#21147;&#20256;&#36755;&#31995;&#32479;&#20869;&#30001;&#25968;&#23383;&#25925;&#38556;&#24405;&#27874;&#22120;&#21644;&#30005;&#21147;&#36136;&#37327;&#30417;&#27979;&#20202;&#35760;&#24405;&#30340;&#30005;&#21147;&#36136;&#37327;&#20107;&#20214;&#12290;&#35813;&#33258;&#21160;&#21270;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#26816;&#26597;&#30005;&#21387;&#21644;&#30005;&#27969;&#20449;&#21495;&#30340;&#26102;&#22495;&#21644;&#39057;&#22495;&#29305;&#24449;&#12290;&#21487;&#23450;&#21046;&#30340;&#38408;&#20540;&#34987;&#35774;&#32622;&#29992;&#26469;&#23545;&#27599;&#20010;&#24178;&#25200;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#20998;&#26512;&#30340;&#20107;&#20214;&#21253;&#25324;&#21508;&#31181;&#25925;&#38556;&#12289;&#30005;&#21160;&#26426;&#36215;&#21160;&#21644;&#20202;&#22120;&#20114;&#24863;&#22120;&#28508;&#22312;&#25925;&#38556;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;14&#31181;&#19981;&#21516;&#20107;&#20214;&#31867;&#22411;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#23545;160&#20010;&#20449;&#21495;&#25991;&#20214;&#36827;&#34892;&#27979;&#35797;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;99&#65285;&#12290;&#37319;&#29992;&#19968;&#31181;&#31216;&#20026;&#24490;&#29615;&#30452;&#26041;&#22270;&#30340;&#26041;&#27861;&#23545;&#36830;&#32493;&#30340;&#12289;&#27491;&#24120;&#30340;&#20449;&#21495;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As power quality becomes a higher priority in the electric utility industry, the amount of disturbance event data continues to grow. Utilities do not have the required personnel to analyze each event by hand. This work presents an automated approach for analyzing power quality events recorded by digital fault recorders and power quality monitors operating within a power transmission system. The automated approach leverages rule-based analytics to examine the time and frequency domain characteristics of the voltage and current signals. Customizable thresholds are set to categorize each disturbance event. The events analyzed within this work include various faults, motor starting, and incipient instrument transformer failure. Analytics for fourteen different event types have been developed. The analytics were tested on 160 signal files and yielded an accuracy of ninety-nine percent. Continuous, nominal signal data analysis is performed using an approach coined as the cyclic histogram. Th
&lt;/p&gt;</description></item><item><title>&#20540;&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;VCSC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#30697;&#38453;&#23384;&#20648;&#26684;&#24335;&#65292;&#33021;&#22815;&#21033;&#29992;&#39640;&#20887;&#20313;&#24615;&#23558;&#25968;&#25454;&#36827;&#19968;&#27493;&#21387;&#32553;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#27809;&#26377;&#26174;&#33879;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#22686;&#37327;&#32534;&#30721;&#21644;&#23383;&#33410;&#25171;&#21253;&#21387;&#32553;&#32034;&#24341;&#25968;&#32452;&#65292;IVCSC&#23454;&#29616;&#20102;&#26356;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2309.04355</link><description>&lt;p&gt;
&#20540;&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;VCSC&#65289;&#65306;&#20887;&#20313;&#25968;&#25454;&#30340;&#31232;&#30095;&#30697;&#38453;&#23384;&#20648;
&lt;/p&gt;
&lt;p&gt;
Value-Compressed Sparse Column (VCSC): Sparse Matrix Storage for Redundant Data. (arXiv:2309.04355v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04355
&lt;/p&gt;
&lt;p&gt;
&#20540;&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;VCSC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#30697;&#38453;&#23384;&#20648;&#26684;&#24335;&#65292;&#33021;&#22815;&#21033;&#29992;&#39640;&#20887;&#20313;&#24615;&#23558;&#25968;&#25454;&#36827;&#19968;&#27493;&#21387;&#32553;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#27809;&#26377;&#26174;&#33879;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#22686;&#37327;&#32534;&#30721;&#21644;&#23383;&#33410;&#25171;&#21253;&#21387;&#32553;&#32034;&#24341;&#25968;&#32452;&#65292;IVCSC&#23454;&#29616;&#20102;&#26356;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;CSC&#65289;&#21644;&#22352;&#26631;&#65288;COO&#65289;&#26159;&#31232;&#30095;&#30697;&#38453;&#30340;&#24120;&#29992;&#21387;&#32553;&#26684;&#24335;&#12290;&#28982;&#32780;&#65292;CSC&#21644;COO&#37117;&#26159;&#36890;&#29992;&#26684;&#24335;&#65292;&#19981;&#33021;&#21033;&#29992;&#38500;&#31232;&#30095;&#24615;&#20197;&#22806;&#30340;&#25968;&#25454;&#29305;&#24615;&#65292;&#22914;&#25968;&#25454;&#20887;&#20313;&#24615;&#12290;&#39640;&#24230;&#20887;&#20313;&#30340;&#31232;&#30095;&#25968;&#25454;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#20363;&#22914;&#22522;&#22240;&#32452;&#23398;&#65292;&#22312;&#20256;&#32479;&#30340;&#31232;&#30095;&#23384;&#20648;&#26684;&#24335;&#19979;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#22826;&#22823;&#26080;&#27861;&#36827;&#34892;&#20869;&#23384;&#35745;&#31639;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25193;&#23637;&#30340;CSC&#26684;&#24335;&#65306;&#20540;&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;VCSC&#65289;&#21644;&#32034;&#24341;&#21644;&#20540;&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;IVCSC&#65289;&#12290;VCSC&#21033;&#29992;&#21015;&#20869;&#30340;&#39640;&#20887;&#20313;&#24615;&#65292;&#23558;&#25968;&#25454;&#36827;&#19968;&#27493;&#21387;&#32553;&#20102;3&#20493;&#20197;&#19978;&#65292;&#30456;&#27604;COO&#21387;&#32553;&#20102;2.25&#20493;&#65292;&#32780;&#24615;&#33021;&#29305;&#24449;&#27809;&#26377;&#26174;&#33879;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;IVCSC&#36890;&#36807;&#22686;&#37327;&#32534;&#30721;&#21644;&#23383;&#33410;&#25171;&#21253;&#21387;&#32553;&#32034;&#24341;&#25968;&#32452;&#65292;&#20351;&#20869;&#23384;&#20351;&#29992;&#37327;&#27604;COO&#20943;&#23569;&#20102;10&#20493;&#65292;&#27604;CSC&#20943;&#23569;&#20102;7.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressed Sparse Column (CSC) and Coordinate (COO) are popular compression formats for sparse matrices. However, both CSC and COO are general purpose and cannot take advantage of any of the properties of the data other than sparsity, such as data redundancy. Highly redundant sparse data is common in many machine learning applications, such as genomics, and is often too large for in-core computation using conventional sparse storage formats. In this paper, we present two extensions to CSC: (1) Value-Compressed Sparse Column (VCSC) and (2) Index- and Value-Compressed Sparse Column (IVCSC). VCSC takes advantage of high redundancy within a column to further compress data up to 3-fold over COO and 2.25-fold over CSC, without significant negative impact to performance characteristics. IVCSC extends VCSC by compressing index arrays through delta encoding and byte-packing, achieving a 10-fold decrease in memory usage over COO and 7.5-fold decrease over CSC. Our benchmarks on simulated and rea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;MoEs&#26469;&#32553;&#23567;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#30340;&#35268;&#27169;&#65292;&#20197;&#25552;&#39640;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35270;&#35273;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.04354</link><description>&lt;p&gt;
&#31227;&#21160;V-MoEs&#65306;&#36890;&#36807;&#31232;&#30095;MoEs&#32553;&#23567;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts. (arXiv:2309.04354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;MoEs&#26469;&#32553;&#23567;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#30340;&#35268;&#27169;&#65292;&#20197;&#25552;&#39640;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35270;&#35273;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoEs&#65289;&#22240;&#20854;&#33021;&#22815;&#36890;&#36807;&#20165;&#28608;&#27963;&#32473;&#23450;&#36755;&#20837;&#20196;&#29260;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#32780;&#23558;&#27169;&#22411;&#35268;&#27169;&#19982;&#25512;&#29702;&#25928;&#29575;&#20998;&#31163;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#31232;&#30095;&#30340;MoEs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#31232;&#30095;MoEs&#26469;&#32553;&#23567;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#30340;&#35268;&#27169;&#65292;&#20197;&#20351;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35270;&#35273;&#24212;&#29992;&#20013;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#21644;&#36866;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;MoE&#35774;&#35745;&#65292;&#20854;&#20013;&#25972;&#20010;&#22270;&#20687;&#32780;&#19981;&#26159;&#21333;&#20010;&#34917;&#19969;&#34987;&#36335;&#30001;&#21040;&#19987;&#23478;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;MoE&#35757;&#32451;&#36807;&#31243;&#65292;&#20351;&#29992;&#36229;&#31867;&#20449;&#24687;&#26469;&#24341;&#23548;&#36335;&#30001;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31232;&#30095;&#31227;&#21160;&#35270;&#35273;MoEs&#65288;V-MoEs&#65289;&#21487;&#20197;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#36798;&#21040;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due to their ability to decouple model size from inference efficiency by only activating a small subset of the model parameters for any given input token. As such, sparse MoEs have enabled unprecedented scalability, resulting in tremendous successes across domains such as natural language processing and computer vision. In this work, we instead explore the use of sparse MoEs to scale-down Vision Transformers (ViTs) to make them more attractive for resource-constrained vision applications. To this end, we propose a simplified and mobile-friendly MoE design where entire images rather than individual patches are routed to the experts. We also propose a stable MoE training procedure that uses super-class information to guide the router. We empirically show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off between performance and efficiency than the corresponding dense ViTs. For example, for the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24378;&#21270;&#26041;&#27861;RoboShot&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#20013;&#20197;&#21435;&#38500;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04344</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#23545;&#38646;&#26679;&#26412;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#24378;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Robustification of Zero-Shot Models With Foundation Models. (arXiv:2309.04344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04344
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24378;&#21270;&#26041;&#27861;RoboShot&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#20013;&#20197;&#21435;&#38500;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#25512;&#26029;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#36827;&#34892;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#32487;&#25215;&#30340;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24494;&#35843;&#65292;&#20294;&#36825;&#21066;&#24369;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20027;&#35201;&#20248;&#21183;&#65292;&#21363;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RoboShot&#65292;&#19968;&#31181;&#23436;&#20840;&#38646;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#36825;&#20123;&#35265;&#35299;&#34987;&#23884;&#20837;&#24182;&#29992;&#20110;&#21435;&#38500;&#23884;&#20837;&#20013;&#30340;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;--&#32780;&#26080;&#38656;&#20219;&#20309;&#30417;&#30563;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#21487;&#35745;&#31639;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#38646;&#26679;&#26412;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#22312;&#20061;&#20010;&#22270;&#20687;&#21644;NLP&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;RoboShot&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04339</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20984;&#20248;&#21270;&#23454;&#29616;&#22312;&#32447;&#23376;&#27169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#22312;&#19968;&#33324;&#24615;&#27169;&#24615;&#32422;&#26463;&#19979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#20248;&#21270;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#65292;&#21363;&#21152;&#26435;&#38408;&#20540;&#21183;&#20989;&#25968;&#65292;&#21487;&#20197;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;(OCO)&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20010;&#31867;&#21035;&#30340;&#20989;&#25968;&#21487;&#20197;&#36827;&#34892;&#20985;&#26494;&#24347;;&#22240;&#27492;&#65292;&#32467;&#21512;&#36866;&#24403;&#30340;&#33293;&#20837;&#26041;&#26696;&#65292;OCO&#31574;&#30053;&#21487;&#20197;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31616;&#21270;&#26041;&#24335;&#21487;&#20197;&#24212;&#29992;&#22312;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#21160;&#24577;&#36951;&#25022;&#12289;&#24378;&#30423;&#21644;&#20048;&#35266;&#23398;&#20064;&#31561;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Multi2SPE&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;CLS&#26631;&#35760;&#26469;&#32534;&#30721;&#22810;&#39046;&#22495;&#31185;&#23398;&#35770;&#25991;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#31185;&#23398;&#39046;&#22495;&#65292;&#24182;&#22312;&#24341;&#25991;&#39044;&#27979;&#20219;&#21153;&#20013;&#20943;&#23569;&#20102;&#22810;&#36798;25&#65285;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.04333</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20010;CLS&#26631;&#35760;&#38598;&#21512;&#23545;&#22810;&#39046;&#22495;&#31185;&#23398;&#35770;&#25991;&#36827;&#34892;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens. (arXiv:2309.04333v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Multi2SPE&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;CLS&#26631;&#35760;&#26469;&#32534;&#30721;&#22810;&#39046;&#22495;&#31185;&#23398;&#35770;&#25991;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#31185;&#23398;&#39046;&#22495;&#65292;&#24182;&#22312;&#24341;&#25991;&#39044;&#27979;&#20219;&#21153;&#20013;&#20943;&#23569;&#20102;&#22810;&#36798;25&#65285;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#25991;&#26723;&#19978;&#30340;&#26377;&#29992;&#20219;&#21153;&#65292;&#22914;&#20027;&#39064;&#20998;&#31867;&#21644;&#24341;&#25991;&#39044;&#27979;&#65292;&#28041;&#21450;&#36328;&#36234;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;Transformer&#30340;&#21333;&#20010;CLS&#26631;&#35760;&#30340;&#21521;&#37327;&#23884;&#20837;&#26469;&#23436;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20351;&#29992;&#22810;&#20010;CLS&#26631;&#35760;&#21487;&#20197;&#20351;Transformer&#26356;&#22909;&#22320;&#19987;&#27880;&#20110;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Multi2SPE&#65306;&#23427;&#40723;&#21169;&#22810;&#20010;CLS&#26631;&#35760;&#23398;&#20064;&#32858;&#21512;&#26631;&#35760;&#23884;&#20837;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30456;&#21152;&#20197;&#21019;&#24314;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26032;&#30340;&#22810;&#39046;&#22495;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;Multi-SciDocs&#65292;&#20197;&#27979;&#35797;&#22810;&#39046;&#22495;&#35774;&#32622;&#19979;&#30340;&#31185;&#23398;&#35770;&#25991;&#21521;&#37327;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;Multi2SPE&#22312;&#22810;&#39046;&#22495;&#24341;&#25991;&#39044;&#27979;&#20013;&#20943;&#23569;&#20102;&#22810;&#36798;25&#65285;&#30340;&#35823;&#24046;&#65292;&#21516;&#26102;&#38500;&#20102;&#19968;&#27425;BERT&#21069;&#21521;&#20256;&#36882;&#20043;&#22806;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#37327;&#30340;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many useful tasks on scientific documents, such as topic classification and citation prediction, involve corpora that span multiple scientific domains. Typically, such tasks are accomplished by representing the text with a vector embedding obtained from a Transformer's single CLS token. In this paper, we argue that using multiple CLS tokens could make a Transformer better specialize to multiple scientific domains. We present Multi2SPE: it encourages each of multiple CLS tokens to learn diverse ways of aggregating token embeddings, then sums them up together to create a single vector representation. We also propose our new multi-domain benchmark, Multi-SciDocs, to test scientific paper vector encoders under multi-domain settings. We show that Multi2SPE reduces error by up to 25 percent in multi-domain citation prediction, while requiring only a negligible amount of computation in addition to one BERT forward pass.
&lt;/p&gt;</description></item><item><title>&#22312;&#22270;&#24418;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#20351;&#22312;&#24573;&#30053;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24120;&#35268;&#22270;&#23545;&#20110;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04332</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#38656;&#35201;&#30340;&#26102;&#20505;&#20173;&#28982;&#20351;&#29992;&#22270;&#24418;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks Use Graphs When They Shouldn't. (arXiv:2309.04332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04332
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#24418;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#20351;&#22312;&#24573;&#30053;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24120;&#35268;&#22270;&#23545;&#20110;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#12289;&#20998;&#23376;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#31561;&#65292;&#23545;&#22270;&#24418;&#36827;&#34892;&#39044;&#27979;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#25104;&#20026;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22270;&#24418;&#26631;&#27880;&#38382;&#39064;&#30340;&#23454;&#20363;&#21253;&#25324;&#22270;&#32467;&#26500;(&#21363;&#37051;&#25509;&#30697;&#38453;)&#21644;&#33410;&#28857;&#29305;&#23450;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#22270;&#32467;&#26500;&#23545;&#20110;&#39044;&#27979;&#20219;&#21153;&#26469;&#35828;&#24182;&#19981;&#20855;&#26377;&#20449;&#24687;&#37327;&#12290;&#20363;&#22914;&#65292;&#20998;&#23376;&#24615;&#36136;&#22914;&#25705;&#23572;&#36136;&#37327;&#20165;&#20381;&#36182;&#20110;&#32452;&#25104;&#21407;&#23376;(&#33410;&#28857;&#29305;&#24449;)&#65292;&#32780;&#19982;&#20998;&#23376;&#32467;&#26500;&#26080;&#20851;&#12290;&#23613;&#31649;GNNs&#26377;&#33021;&#21147;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24573;&#30053;&#22270;&#32467;&#26500;&#65292;&#20294;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#20250;&#36825;&#26679;&#20570;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GNNs&#23454;&#38469;&#19978;&#20542;&#21521;&#20110;&#22312;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#22312;&#24573;&#30053;&#23427;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#20173;&#22312;&#20351;&#29992;&#12290;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#22270;&#20998;&#24067;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#21457;&#29616;&#24120;&#35268;&#22270;&#23545;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictions over graphs play a crucial role in various domains, including social networks, molecular biology, medicine, and more. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Instances of graph labeling problems consist of the graph-structure (i.e., the adjacency matrix), along with node-specific feature vectors. In some cases, this graph-structure is non-informative for the predictive task. For instance, molecular properties such as molar mass depend solely on the constituent atoms (node features), and not on the molecular structure. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SYNLABEL&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#26080;&#22122;&#22768;&#25968;&#25454;&#38598;&#65292;&#24182;&#21487;&#20197;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#19968;&#20010;&#36719;&#26631;&#31614;&#25110;&#26631;&#31614;&#20998;&#24067;&#65292;&#29992;&#20110;&#25913;&#36827;&#26631;&#31614;&#22122;&#22768;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.04318</link><description>&lt;p&gt;
&#29983;&#25104;&#30495;&#23454;&#26631;&#31614;: &#29992;&#20110;&#26631;&#31614;&#22122;&#22768;&#30740;&#31350;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating the Ground Truth: Synthetic Data for Label Noise Research. (arXiv:2309.04318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SYNLABEL&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#26080;&#22122;&#22768;&#25968;&#25454;&#38598;&#65292;&#24182;&#21487;&#20197;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#19968;&#20010;&#36719;&#26631;&#31614;&#25110;&#26631;&#31614;&#20998;&#24067;&#65292;&#29992;&#20110;&#25913;&#36827;&#26631;&#31614;&#22122;&#22768;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#20219;&#21153;&#37117;&#23384;&#22312;&#30528;&#19968;&#23450;&#31243;&#24230;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#36825;&#31181;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#23545;&#20110;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#20351;&#24471;&#22122;&#22768;&#22788;&#29702;&#26041;&#27861;&#30340;&#35780;&#20272;&#21464;&#24471;&#22797;&#26434;&#65292;&#22240;&#20026;&#27809;&#26377;&#28165;&#26224;&#30340;&#26631;&#31614;&#65292;&#26080;&#27861;&#20934;&#30830;&#34913;&#37327;&#20854;&#24615;&#33021;&#12290;&#22312;&#26631;&#31614;&#22122;&#22768;&#30740;&#31350;&#20013;&#65292;&#36890;&#24120;&#25509;&#21463;&#26377;&#22122;&#22768;&#25110;&#31616;&#21333;&#30340;&#27169;&#25311;&#25968;&#25454;&#20316;&#20026;&#22522;&#32447;&#65292;&#28982;&#21518;&#27880;&#20837;&#20855;&#26377;&#24050;&#30693;&#23646;&#24615;&#30340;&#39069;&#22806;&#22122;&#22768;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SYNLABEL&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25913;&#36827;&#19978;&#36848;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;&#23427;&#20801;&#35768;&#36890;&#36807;&#39044;&#20808;&#25351;&#23450;&#25110;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#23450;&#20041;&#20026;&#29983;&#25104;&#26631;&#31614;&#30340;&#22522;&#26412;&#30495;&#20540;&#20989;&#25968;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#26080;&#22122;&#22768;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#20989;&#25968;&#22495;&#20013;&#30340;&#36873;&#23450;&#29305;&#24449;&#19978;&#37325;&#26032;&#37319;&#26679;&#19968;&#20123;&#20540;&#65292;&#35780;&#20272;&#20989;&#25968;&#24182;&#27719;&#24635;&#32467;&#26524;&#26631;&#31614;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#19968;&#20010;&#36719;&#26631;&#31614;&#25110;&#26631;&#31614;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most real-world classification tasks suffer from label noise to some extent. Such noise in the data adversely affects the generalization error of learned models and complicates the evaluation of noise-handling methods, as their performance cannot be accurately measured without clean labels. In label noise research, typically either noisy or incomplex simulated data are accepted as a baseline, into which additional noise with known properties is injected. In this paper, we propose SYNLABEL, a framework that aims to improve upon the aforementioned methodologies. It allows for creating a noiseless dataset informed by real data, by either pre-specifying or learning a function and defining it as the ground truth function from which labels are generated. Furthermore, by resampling a number of values for selected features in the function domain, evaluating the function and aggregating the resulting labels, each data point can be assigned a soft label or label distribution. Such distributions 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#30697;&#31070;&#32463;&#32593;&#32476;&#30340;&#28436;&#21592;-&#35780;&#35770;&#21592;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24179;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20215;&#20540;&#20989;&#25968;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;&#20998;&#24067;&#30340;&#36712;&#36857;&#26469;&#23454;&#29616;&#23398;&#20064;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#32500;&#21644;&#38750;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#31561;&#19981;&#21516;&#24773;&#22659;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04317</link><description>&lt;p&gt;
&#20351;&#29992;&#30697;&#31070;&#32463;&#32593;&#32476;&#30340;&#24179;&#22343;&#22330;&#25511;&#21046;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#21592;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Actor critic learning algorithms for mean-field control with moment neural networks. (arXiv:2309.04317v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04317
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#30697;&#31070;&#32463;&#32593;&#32476;&#30340;&#28436;&#21592;-&#35780;&#35770;&#21592;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24179;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20215;&#20540;&#20989;&#25968;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;&#20998;&#24067;&#30340;&#36712;&#36857;&#26469;&#23454;&#29616;&#23398;&#20064;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#32500;&#21644;&#38750;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#31561;&#19981;&#21516;&#24773;&#22659;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#35299;&#20915;&#24179;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#30340;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#21644;&#28436;&#21592;-&#35780;&#35770;&#21592;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20215;&#20540;&#20989;&#25968;&#34920;&#31034;&#65292;&#37319;&#29992;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#31574;&#30053;&#12290;&#28436;&#21592;&#65288;&#31574;&#30053;&#65289;&#21644;&#35780;&#35770;&#21592;&#65288;&#20215;&#20540;&#20989;&#25968;&#65289;&#30340;&#23398;&#20064;&#26159;&#36890;&#36807;&#22312;&#27010;&#29575;&#27979;&#24230;&#30340;Wasserstein&#31354;&#38388;&#19978;&#30340;&#19968;&#31867;&#30697;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#26469;&#23454;&#29616;&#30340;&#65292;&#20854;&#20851;&#38190;&#29305;&#24449;&#26159;&#30452;&#25509;&#37319;&#26679;&#20998;&#24067;&#30340;&#36712;&#36857;&#12290;&#36825;&#39033;&#30740;&#31350;&#20013;&#35299;&#20915;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#28041;&#21450;&#21040;&#23545;&#20110;&#24179;&#22343;&#22330;&#26694;&#26550;&#29305;&#26377;&#30340;&#36816;&#31639;&#31526;&#30340;&#35745;&#31639;&#22788;&#29702;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#28085;&#30422;&#20102;&#22810;&#32500;&#35774;&#32622;&#21644;&#20855;&#26377;&#21463;&#25511;&#27874;&#21160;&#24615;&#30340;&#38750;&#32447;&#24615;&#20108;&#27425;&#24179;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#31561;&#19981;&#21516;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new policy gradient and actor-critic algorithm for solving mean-field control problems within a continuous time reinforcement learning setting. Our approach leverages a gradient-based representation of the value function, employing parametrized randomized policies. The learning for both the actor (policy) and critic (value function) is facilitated by a class of moment neural network functions on the Wasserstein space of probability measures, and the key feature is to sample directly trajectories of distributions. A central challenge addressed in this study pertains to the computational treatment of an operator specific to the mean-field framework. To illustrate the effectiveness of our methods, we provide a comprehensive set of numerical results. These encompass diverse examples, including multi-dimensional settings and nonlinear quadratic mean-field control problems with controlled volatility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20581;&#24247;&#32769;&#40836;&#24212;&#29992;&#20013;&#30340;&#26089;&#26399;&#36864;&#23398;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32852;&#21512;&#20010;&#20307;&#21644;&#32452;&#32455;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.04311</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20581;&#24247;&#32769;&#40836;&#24212;&#29992;&#26089;&#26399;&#36864;&#23398;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Early Dropout Prediction on Healthy Ageing Applications. (arXiv:2309.04311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20581;&#24247;&#32769;&#40836;&#24212;&#29992;&#20013;&#30340;&#26089;&#26399;&#36864;&#23398;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32852;&#21512;&#20010;&#20307;&#21644;&#32452;&#32455;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#25252;&#29702;&#24212;&#29992;&#30340;&#25552;&#20379;&#23545;&#20110;&#25913;&#21892;&#32769;&#24180;&#20154;&#30340;&#29983;&#27963;&#36136;&#37327;&#21644;&#20026;&#36816;&#33829;&#21830;&#25552;&#20379;&#26089;&#26399;&#24178;&#39044;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#20581;&#24247;&#32769;&#40836;&#24212;&#29992;&#20013;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#36864;&#23398;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#30452;&#25509;&#19982;&#20010;&#20307;&#20581;&#24247;&#29366;&#20917;&#30456;&#20851;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#23454;&#29616;&#20102;&#39640;&#24230;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#20010;&#20307;&#27169;&#24335;&#26102;&#38750;&#24120;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#22312;&#23384;&#22312;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;(PII)&#21644;&#21463;&#21040;&#27861;&#35268;&#30862;&#29255;&#21270;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;(FML)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#20256;&#36755;&#20010;&#20307;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;FML&#19979;&#30340;&#20010;&#20307;&#21644;&#32452;&#32455;&#26469;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#36825;&#26679;&#21487;&#20197;&#23545;&#36328;&#35774;&#22791;&#21644;&#36328;&#24179;&#21488;&#23398;&#20064;&#22330;&#26223;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The provision of social care applications is crucial for elderly people to improve their quality of life and enables operators to provide early interventions. Accurate predictions of user dropouts in healthy ageing applications are essential since they are directly related to individual health statuses. Machine Learning (ML) algorithms have enabled highly accurate predictions, outperforming traditional statistical methods that struggle to cope with individual patterns. However, ML requires a substantial amount of data for training, which is challenging due to the presence of personal identifiable information (PII) and the fragmentation posed by regulations. In this paper, we present a federated machine learning (FML) approach that minimizes privacy concerns and enables distributed training, without transferring individual data. We employ collaborative training by considering individuals and organizations under FML, which models both cross-device and cross-silo learning scenarios. Our a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04296</link><description>&lt;p&gt;
&#22312;COVID-19&#26399;&#38388;&#23548;&#33322;&#19981;&#22312;&#20998;&#24067;&#33539;&#22260;&#20869;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65306;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility. (arXiv:2309.04296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#25968;&#25454;&#20998;&#24067;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#38750;&#20998;&#24067;&#26399;&#38388;&#26102;&#65292;&#22914;COVID-19&#30340;&#23553;&#38145;&#26399;&#65292;&#25968;&#25454;&#20998;&#24067;&#19982;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#35265;&#30340;&#26126;&#26174;&#20559;&#31163;&#12290;&#26412;&#25991;&#37319;&#29992;&#21452;&#37325;&#31574;&#30053;&#65306;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#26356;&#26032;&#27169;&#22411;&#30340;&#26032;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#22312;&#24314;&#31569;&#29289;&#22806;&#37096;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#34892;&#20154;&#35745;&#25968;&#22120;&#25910;&#38598;&#30340;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#12290;&#19982;&#22312;&#32447;&#23398;&#20064;&#30456;&#27604;&#65292;&#21518;&#32773;&#24120;&#24120;&#20250;&#36973;&#21463;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#26032;&#33719;&#24471;&#30340;&#30693;&#35782;&#24120;&#24120;&#20250;&#25273;&#21435;&#20808;&#21069;&#30340;&#20449;&#24687;&#65292;&#25345;&#32493;&#23398;&#20064;&#21017;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#20307;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23558;FSNet&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#22696;&#23572;&#26412;&#24066;13&#20010;&#24314;&#31569;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the s
&lt;/p&gt;</description></item><item><title>&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#65292;&#23637;&#31034;&#20854;&#26377;&#36259;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04284</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304; - &#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Viewing the process of generating counterfactuals as a source of knowledge -- Application to the Naive Bayes classifier. (arXiv:2309.04284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04284
&lt;/p&gt;
&lt;p&gt;
&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#65292;&#23637;&#31034;&#20854;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#35768;&#22810;&#29702;&#35299;&#31639;&#27861;&#21487;&#20197;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20915;&#31574;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#31034;&#20363;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#36825;&#20010;&#29983;&#25104;&#36807;&#31243;&#35270;&#20026;&#19968;&#31181;&#21019;&#36896;&#19968;&#23450;&#37327;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#30693;&#35782;&#21487;&#20197;&#23384;&#20648;&#24182;&#22312;&#20197;&#21518;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#26412;&#25991;&#22312;&#21152;&#27861;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#22312;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#27492;&#30446;&#30340;&#19978;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are now many comprehension algorithms for understanding the decisions of a machine learning algorithm. Among these are those based on the generation of counterfactual examples. This article proposes to view this generation process as a source of creating a certain amount of knowledge that can be stored to be used, later, in different ways. This process is illustrated in the additive model and, more specifically, in the case of the naive Bayes classifier, whose interesting properties for this purpose are shown.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#32500;&#24230;&#25968;&#25454;&#30340;&#26680;&#22238;&#24402;&#30340;&#26368;&#20248;&#27604;&#29575;&#65292;&#36890;&#36807;&#20351;&#29992;Mendelson&#22797;&#26434;&#24615;&#21644;&#24230;&#37327;&#29109;&#26469;&#21051;&#30011;&#20854;&#19978;&#30028;&#21644;&#26368;&#23567;&#21270;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#26368;&#20248;&#27604;&#29575;&#38543;&#30528;&#32500;&#24230;&#19982;&#26679;&#26412;&#22823;&#23567;&#20851;&#31995;&#30340;&#21464;&#21270;&#21576;&#29616;&#20986;&#22810;&#27425;&#19979;&#38477;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.04268</link><description>&lt;p&gt;
&#22823;&#32500;&#24230;&#24773;&#20917;&#19979;&#26680;&#22238;&#24402;&#30340;&#26368;&#20248;&#27604;&#29575;
&lt;/p&gt;
&lt;p&gt;
Optimal Rate of Kernel Regression in Large Dimensions. (arXiv:2309.04268v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#32500;&#24230;&#25968;&#25454;&#30340;&#26680;&#22238;&#24402;&#30340;&#26368;&#20248;&#27604;&#29575;&#65292;&#36890;&#36807;&#20351;&#29992;Mendelson&#22797;&#26434;&#24615;&#21644;&#24230;&#37327;&#29109;&#26469;&#21051;&#30011;&#20854;&#19978;&#30028;&#21644;&#26368;&#23567;&#21270;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#26368;&#20248;&#27604;&#29575;&#38543;&#30528;&#32500;&#24230;&#19982;&#26679;&#26412;&#22823;&#23567;&#20851;&#31995;&#30340;&#21464;&#21270;&#21576;&#29616;&#20986;&#22810;&#27425;&#19979;&#38477;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22823;&#32500;&#24230;&#25968;&#25454;&#65288;&#26679;&#26412;&#22823;&#23567;$n$&#19982;&#26679;&#26412;&#32500;&#24230;$d$&#30340;&#20851;&#31995;&#20026;&#22810;&#39033;&#24335;&#65292;&#21363;$n\asymp d^{\gamma}$&#65292;&#20854;&#20013;$\gamma&gt;0$&#65289;&#30340;&#26680;&#22238;&#24402;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;Mendelson&#22797;&#26434;&#24615;$\varepsilon_{n}^{2}$&#21644;&#24230;&#37327;&#29109;$\bar{\varepsilon}_{n}^{2}$&#26469;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#24037;&#20855;&#65292;&#29992;&#20110;&#21051;&#30011;&#22823;&#32500;&#24230;&#25968;&#25454;&#30340;&#26680;&#22238;&#24402;&#30340;&#19978;&#30028;&#21644;&#26368;&#23567;&#21270;&#19979;&#30028;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#23646;&#20110;&#19982;$\mathbb{S}^{d}$&#19978;&#23450;&#20041;&#30340;&#65288;&#19968;&#33324;&#65289;&#20869;&#31215;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;RKHS&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26032;&#24037;&#20855;&#26469;&#23637;&#31034;&#26680;&#22238;&#24402;&#30340;&#36807;&#37327;&#39118;&#38505;&#30340;&#26368;&#23567;&#21270;&#29575;&#26159;$n^{-1/2}$&#65292;&#24403;$n\asymp d^{\gamma}$&#65292;&#20854;&#20013;$\gamma=2, 4, 6, 8, \cdots$&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#23545;&#20110;&#25152;&#26377;$\gamma&gt;0$&#65292;&#26680;&#22238;&#24402;&#36807;&#37327;&#39118;&#38505;&#30340;&#26368;&#20248;&#27604;&#29575;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;$\gamma$&#30340;&#21464;&#21270;&#65292;&#26368;&#20248;&#27604;&#29575;&#30340;&#26354;&#32447;&#23637;&#29616;&#20986;&#20960;&#20010;&#26032;&#29616;&#35937;&#65292;&#21253;&#25324;&#22810;&#27425;&#19979;&#38477;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We perform a study on kernel regression for large-dimensional data (where the sample size $n$ is polynomially depending on the dimension $d$ of the samples, i.e., $n\asymp d^{\gamma}$ for some $\gamma &gt;0$ ). We first build a general tool to characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively. When the target function falls into the RKHS associated with a (general) inner product model defined on $\mathbb{S}^{d}$, we utilize the new tool to show that the minimax rate of the excess risk of kernel regression is $n^{-1/2}$ when $n\asymp d^{\gamma}$ for $\gamma =2, 4, 6, 8, \cdots$. We then further determine the optimal rate of the excess risk of kernel regression for all the $\gamma&gt;0$ and find that the curve of optimal rate varying along $\gamma$ exhibits several new phenomena including the {\it multiple descent behavior
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20998;&#24067;&#24335;&#26680;&#23725;&#22238;&#24402;&#65288;AdaDKRR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#33258;&#27835;&#24615;&#12289;&#38544;&#31169;&#24615;&#21644;&#21512;&#20316;&#24615;&#35299;&#20915;&#20102;&#25968;&#25454;&#23396;&#31435;&#30340;&#38382;&#39064;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#19982;&#25972;&#20307;&#25968;&#25454;&#36816;&#34892;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#31867;&#20284;&#12290;</title><link>http://arxiv.org/abs/2309.04236</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20998;&#24067;&#24335;&#26680;&#23725;&#22238;&#24402;&#65306;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#25968;&#25454;&#23396;&#31435;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos. (arXiv:2309.04236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20998;&#24067;&#24335;&#26680;&#23725;&#22238;&#24402;&#65288;AdaDKRR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#33258;&#27835;&#24615;&#12289;&#38544;&#31169;&#24615;&#21644;&#21512;&#20316;&#24615;&#35299;&#20915;&#20102;&#25968;&#25454;&#23396;&#31435;&#30340;&#38382;&#39064;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#19982;&#25972;&#20307;&#25968;&#25454;&#36816;&#34892;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#23396;&#31435;&#20027;&#35201;&#30001;&#38544;&#31169;&#21644;&#20114;&#25805;&#20316;&#24615;&#24341;&#36215;&#65292;&#26174;&#33879;&#38480;&#21046;&#20102;&#19981;&#21516;&#32452;&#32455;&#22312;&#30456;&#21516;&#30446;&#30340;&#19979;&#20855;&#26377;&#30456;&#20284;&#25968;&#25454;&#30340;&#21512;&#20316;&#12290;&#22522;&#20110;&#20998;&#32780;&#27835;&#20043;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20026;&#35299;&#20915;&#25968;&#25454;&#23396;&#31435;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#20854;&#38754;&#20020;&#30528;&#33258;&#27835;&#24615;&#12289;&#38544;&#31169;&#20445;&#35777;&#21644;&#21512;&#20316;&#30340;&#24517;&#35201;&#24615;&#31561;&#35832;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#24320;&#21457;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#24067;&#24335;&#26680;&#23725;&#22238;&#24402;&#65288;AdaDKRR&#65289;&#26041;&#27861;&#65292;&#32771;&#34385;&#21442;&#25968;&#36873;&#25321;&#30340;&#33258;&#27835;&#24615;&#12289;&#20256;&#36882;&#38750;&#25935;&#24863;&#20449;&#24687;&#30340;&#38544;&#31169;&#24615;&#21644;&#24615;&#33021;&#25913;&#36827;&#30340;&#21512;&#20316;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;AdaDKRR&#30340;&#22362;&#23454;&#29702;&#35770;&#39564;&#35777;&#21644;&#20840;&#38754;&#23454;&#39564;&#26469;&#35777;&#26126;&#20854;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#19968;&#20123;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;AdaDKRR&#22312;&#25972;&#20010;&#25968;&#25454;&#19978;&#36816;&#34892;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#31867;&#20284;&#65292;&#39564;&#35777;&#20102;&#21512;&#20316;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#27809;&#26377;&#20854;&#20182;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data silos, mainly caused by privacy and interoperability, significantly constrain collaborations among different organizations with similar data for the same purpose. Distributed learning based on divide-and-conquer provides a promising way to settle the data silos, but it suffers from several challenges, including autonomy, privacy guarantees, and the necessity of collaborations. This paper focuses on developing an adaptive distributed kernel ridge regression (AdaDKRR) by taking autonomy in parameter selection, privacy in communicating non-sensitive information, and the necessity of collaborations in performance improvement into account. We provide both solid theoretical verification and comprehensive experiments for AdaDKRR to demonstrate its feasibility and effectiveness. Theoretically, we prove that under some mild conditions, AdaDKRR performs similarly to running the optimal learning algorithms on the whole data, verifying the necessity of collaborations and showing that no other
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31163;&#32447;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#25512;&#33616;&#31995;&#32479;&#29992;&#20363;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;&#31574;&#30053;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#30001;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#32479;&#35745;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.04222</link><description>&lt;p&gt;
&#26410;&#35266;&#23519;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#19979;&#30340;&#31163;&#32447;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Offline Recommender System Evaluation under Unobserved Confounding. (arXiv:2309.04222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31163;&#32447;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#25512;&#33616;&#31995;&#32479;&#29992;&#20363;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;&#31574;&#30053;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#30001;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#32479;&#35745;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#25919;&#31574;&#20272;&#35745;&#26041;&#27861;(OPE)&#20801;&#35768;&#25105;&#20204;&#20174;&#35760;&#24405;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#35780;&#20272;&#20915;&#31574;&#31574;&#30053;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#31163;&#32447;&#35780;&#20272;&#25512;&#33616;&#31995;&#32479;&#30340;&#21560;&#24341;&#20154;&#36873;&#25321;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#20316;&#21697;&#25253;&#36947;&#20102;&#25104;&#21151;&#37319;&#29992;OPE&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#19968;&#20010;&#37325;&#35201;&#20551;&#35774;&#26159;&#19981;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#65306;&#22312;&#25968;&#25454;&#25910;&#38598;&#26102;&#24433;&#21709;&#34892;&#21160;&#21644;&#22870;&#21169;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#36890;&#24120;&#22312;&#20174;&#19994;&#32773;&#30340;&#25511;&#21046;&#20043;&#19979;&#65292;&#22240;&#27492;&#24456;&#23569;&#26126;&#30830;&#22320;&#25552;&#21450;&#26080;&#28151;&#28102;&#20551;&#35774;&#65292;&#24182;&#19988;&#29616;&#26377;&#25991;&#29486;&#20013;&#24456;&#23569;&#22788;&#29702;&#20854;&#36829;&#35268;&#38382;&#39064;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#24378;&#35843;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#20272;&#35745;&#26102;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#20363;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;&#20272;&#35745;&#22120;&#65292;&#20854;&#20013;&#26085;&#24535;&#20542;&#21521;&#26159;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#23545;&#30001;&#20110;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#32479;&#35745;&#20559;&#24046;&#36827;&#34892;&#20102;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Estimation (OPE) methods allow us to learn and evaluate decision-making policies from logged data. This makes them an attractive choice for the offline evaluation of recommender systems, and several recent works have reported successful adoption of OPE methods to this end. An important assumption that makes this work is the absence of unobserved confounders: random variables that influence both actions and rewards at data collection time. Because the data collection policy is typically under the practitioner's control, the unconfoundedness assumption is often left implicit, and its violations are rarely dealt with in the existing literature.  This work aims to highlight the problems that arise when performing off-policy estimation in the presence of unobserved confounders, specifically focusing on a recommendation use-case. We focus on policy-based estimators, where the logging propensities are learned from logged data. We characterise the statistical bias that arises due to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#27979;&#35797;&#38382;&#39064;&#65292;&#21363;&#21516;&#26102;&#36827;&#34892;&#30340;&#20998;&#32452;&#27979;&#35797;(ConcGT)&#12290;&#35813;&#38382;&#39064;&#38656;&#35201;&#36890;&#36807;&#23613;&#21487;&#33021;&#23569;&#30340;&#27979;&#35797;&#26469;&#21487;&#38752;&#22320;&#35782;&#21035;&#22810;&#20010;&#19981;&#30456;&#20132;&#30340;&#21322;&#32570;&#38519;&#38598;&#21512;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#21508;&#31181;&#31639;&#27861;&#65292;&#24182;&#22312;&#20004;&#20010;&#21322;&#32570;&#38519;&#38598;&#21512;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#37325;&#28857;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2309.04221</link><description>&lt;p&gt;
&#21516;&#26102;&#36827;&#34892;&#30340;&#20998;&#32452;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Concomitant Group Testing. (arXiv:2309.04221v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#27979;&#35797;&#38382;&#39064;&#65292;&#21363;&#21516;&#26102;&#36827;&#34892;&#30340;&#20998;&#32452;&#27979;&#35797;(ConcGT)&#12290;&#35813;&#38382;&#39064;&#38656;&#35201;&#36890;&#36807;&#23613;&#21487;&#33021;&#23569;&#30340;&#27979;&#35797;&#26469;&#21487;&#38752;&#22320;&#35782;&#21035;&#22810;&#20010;&#19981;&#30456;&#20132;&#30340;&#21322;&#32570;&#38519;&#38598;&#21512;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#21508;&#31181;&#31639;&#27861;&#65292;&#24182;&#22312;&#20004;&#20010;&#21322;&#32570;&#38519;&#38598;&#21512;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#37325;&#28857;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25429;&#25417;&#21040;&#20102;&#19968;&#20010;&#27491;&#27979;&#35797;&#38656;&#35201;&#22810;&#20010;&#8220;&#31867;&#22411;&#8221;&#39033;&#30446;&#30340;&#32452;&#21512;&#27979;&#35797;&#38382;&#39064;&#30340;&#21464;&#31181;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20551;&#35774;&#23384;&#22312;&#22810;&#20010;&#19981;&#30456;&#20132;&#30340;&#8220;&#21322;&#32570;&#38519;&#38598;&#21512;&#8221;&#65292;&#24182;&#19988;&#21482;&#26377;&#21253;&#21547;&#27599;&#20010;&#38598;&#21512;&#20013;&#33267;&#23569;&#19968;&#20010;&#39033;&#30446;&#30340;&#27979;&#35797;&#32467;&#26524;&#25165;&#26159;&#27491;&#30340;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#27979;&#35797;&#26469;&#21487;&#38752;&#22320;&#35782;&#21035;&#25152;&#26377;&#30340;&#21322;&#32570;&#38519;&#38598;&#21512;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#8220;&#21516;&#26102;&#36827;&#34892;&#30340;&#20998;&#32452;&#27979;&#35797;&#8221;(ConcGT)&#12290;&#25105;&#20204;&#38024;&#23545;&#36825;&#20010;&#20219;&#21153;&#25512;&#23548;&#20986;&#20102;&#21508;&#31181;&#31639;&#27861;&#65292;&#20027;&#35201;&#38598;&#20013;&#20110;&#23384;&#22312;&#20004;&#20010;&#21322;&#32570;&#38519;&#38598;&#21512;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;(i)&#23427;&#20204;&#26159;&#30830;&#23450;&#24615;&#30340;(&#38646;&#38169;&#35823;)&#36824;&#26159;&#38543;&#26426;&#30340;(&#23567;&#38169;&#35823;)&#65292;(ii)&#23427;&#20204;&#26159;&#38750;&#33258;&#36866;&#24212;&#30340;&#12289;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#65292;&#36824;&#26159;&#26377;&#38480;&#33258;&#36866;&#24212;&#24615;&#30340;(&#20363;&#22914;&#65292;2&#25110;3&#20010;&#38454;&#27573;)&#12290;&#25105;&#20204;&#30340;&#30830;&#23450;&#24615;&#33258;&#36866;&#24212;&#31639;&#27861;&#21644;&#25105;&#20204;&#30340;&#38543;&#26426;&#31639;&#27861;(&#38750;&#33258;&#36866;&#24212;&#25110;&#26377;&#38480;&#33258;&#36866;&#24212;&#24615;)&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#19978;&#37117;&#26159;&#26368;&#20248;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a variation of the group testing problem capturing the idea that a positive test requires a combination of multiple ``types'' of item. Specifically, we assume that there are multiple disjoint \emph{semi-defective sets}, and a test is positive if and only if it contains at least one item from each of these sets. The goal is to reliably identify all of the semi-defective sets using as few tests as possible, and we refer to this problem as \textit{Concomitant Group Testing} (ConcGT). We derive a variety of algorithms for this task, focusing primarily on the case that there are two semi-defective sets. Our algorithms are distinguished by (i) whether they are deterministic (zero-error) or randomized (small-error), and (ii) whether they are non-adaptive, fully adaptive, or have limited adaptivity (e.g., 2 or 3 stages). Both our deterministic adaptive algorithm and our randomized algorithms (non-adaptive or limited adaptivity) are order-optimal in broad scaling reg
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23616;&#37096;&#24341;&#23548;&#30340;&#39034;&#24207;&#31639;&#27861;&#22238;&#28335;&#65292;&#23454;&#29616;&#20102;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#23450;&#20301;&#19968;&#20010;&#21453;&#20107;&#23454;&#21450;&#20854;&#22238;&#28335;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.04211</link><description>&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#24341;&#23548;&#30340;&#39034;&#24207;&#31639;&#27861;&#22238;&#28335;&#23454;&#29616;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse. (arXiv:2309.04211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04211
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#24341;&#23548;&#30340;&#39034;&#24207;&#31639;&#27861;&#22238;&#28335;&#65292;&#23454;&#29616;&#20102;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#23450;&#20301;&#19968;&#20010;&#21453;&#20107;&#23454;&#21450;&#20854;&#22238;&#28335;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31639;&#27861;&#22238;&#28335;&#23454;&#29616;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#24050;&#25104;&#20026;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#35299;&#37322;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#27010;&#24565;&#19978;&#65292;&#32473;&#23450;&#19968;&#20010;&#34987;&#20998;&#31867;&#20026;y&#30340;&#20010;&#20307;&#65288;&#20107;&#23454;&#65289;&#65292;&#25105;&#20204;&#23547;&#25214;&#19968;&#20123;&#34892;&#21160;&#65292;&#20351;&#20854;&#39044;&#27979;&#25104;&#20026;&#25152;&#26399;&#26395;&#30340;&#31867;&#21035;y'&#65288;&#21453;&#20107;&#23454;&#65289;&#12290;&#36825;&#20010;&#36807;&#31243;&#25552;&#20379;&#20102;&#26131;&#20110;&#23450;&#21046;&#21644;&#35299;&#37322;&#30340;&#31639;&#27861;&#22238;&#28335;&#65292;&#24182;&#19982;&#27599;&#20010;&#20010;&#20307;&#30340;&#30446;&#26631;&#30452;&#25509;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23545;&#19968;&#20010;&#8220;&#22909;&#8221;&#30340;&#21453;&#20107;&#23454;&#30340;&#24615;&#36136;&#20173;&#28982;&#23384;&#22312;&#24191;&#27867;&#30340;&#20105;&#35758;&#65307;&#22914;&#20309;&#26377;&#25928;&#22320;&#23450;&#20301;&#19968;&#20010;&#21453;&#20107;&#23454;&#21450;&#20854;&#30456;&#24212;&#30340;&#22238;&#28335;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#19968;&#20123;&#31574;&#30053;&#20351;&#29992;&#26799;&#24230;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23545;&#22238;&#28335;&#30340;&#21487;&#34892;&#24615;&#27809;&#26377;&#20219;&#20309;&#20445;&#35777;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#23545;&#31934;&#24515;&#21019;&#24314;&#30340;&#26354;&#38754;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#21644;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#20854;&#20182;&#26041;&#27861;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#36825;&#20027;&#35201;&#35299;&#20915;&#20102;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20294;&#20197;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20445;&#23494;&#20026;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactuals operationalised through algorithmic recourse have become a powerful tool to make artificial intelligence systems explainable. Conceptually, given an individual classified as y -- the factual -- we seek actions such that their prediction becomes the desired class y' -- the counterfactual. This process offers algorithmic recourse that is (1) easy to customise and interpret, and (2) directly aligned with the goals of each individual. However, the properties of a "good" counterfactual are still largely debated; it remains an open challenge to effectively locate a counterfactual along with its corresponding recourse. Some strategies use gradient-driven methods, but these offer no guarantees on the feasibility of the recourse and are open to adversarial attacks on carefully created manifolds. This can lead to unfairness and lack of robustness. Other methods are data-driven, which mostly addresses the feasibility problem at the expense of privacy, security and secrecy as they 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#30340;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04195</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#32531;&#35299;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Mitigating Architecture Overfitting in Dataset Distillation. (arXiv:2309.04195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#30340;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#22312;&#20351;&#29992;&#26497;&#23569;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#65306;&#30001;&#29305;&#23450;&#32593;&#32476;&#26550;&#26500;&#65288;&#21363;&#35757;&#32451;&#32593;&#32476;&#65289;&#21512;&#25104;&#30340;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#22312;&#20854;&#20182;&#32593;&#32476;&#26550;&#26500;&#65288;&#21363;&#27979;&#35797;&#32593;&#32476;&#65289;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26550;&#26500;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20849;&#21516;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#33976;&#39311;&#25968;&#25454;&#28041;&#21450;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#23481;&#37327;&#26356;&#22823;&#30340;&#32593;&#32476;&#23545;&#33976;&#39311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation methods have demonstrated remarkable performance for neural networks trained with very limited training data. However, a significant challenge arises in the form of architecture overfitting: the distilled training data synthesized by a specific network architecture (i.e., training network) generates poor performance when trained by other network architectures (i.e., test networks). This paper addresses this issue and proposes a series of approaches in both architecture designs and training schemes which can be adopted together to boost the generalization performance across different network architectures on the distilled training data. We conduct extensive experiments to demonstrate the effectiveness and generality of our methods. Particularly, across various scenarios involving different sizes of distilled data, our approaches achieve comparable or superior performance to existing methods when training on the distilled data using networks with larger capacities.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04160</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#26469;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#32463;&#24120;&#21576;&#29616;&#31232;&#30095;&#29305;&#24449;&#65292;&#32473;&#39044;&#27979;&#24314;&#27169;&#24102;&#26469;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30452;&#25509;&#25554;&#34917;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#25554;&#34917;&#26041;&#27861;&#65289;&#20381;&#36182;&#20110;&#21442;&#32771;&#31867;&#20284;&#34892;&#25110;&#21015;&#26469;&#23436;&#25104;&#21407;&#22987;&#32570;&#22833;&#25968;&#25454;&#65292;&#19981;&#21306;&#20998;&#25554;&#34917;&#21644;&#23454;&#38469;&#20540;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19982;&#39044;&#27979;&#30446;&#26631;&#26080;&#20851;&#30340;&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#22312;&#30452;&#25509;&#25554;&#34917;&#21518;&#37325;&#26032;&#26657;&#20934;&#25110;&#22686;&#24378;EHR&#23884;&#20837;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38169;&#35823;&#22320;&#20248;&#20808;&#32771;&#34385;&#25554;&#34917;&#29305;&#24449;&#12290;&#36825;&#31181;&#20248;&#20808;&#38169;&#35823;&#21487;&#33021;&#20250;&#32473;&#27169;&#22411;&#24341;&#20837;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37319;&#29992;&#38388;&#25509;&#25554;&#34917;&#65292;&#21033;&#29992;&#31867;&#20284;&#24739;&#32773;&#30340;&#21407;&#22411;&#34920;&#31034;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#12290;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#26102;&#36890;&#24120;&#23558;&#32570;&#22833;&#29305;&#24449;&#19982;&#23384;&#22312;&#29305;&#24449;&#30456;&#21516;&#30340;&#38480;&#21046;&#26102;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;DMI&#25935;&#24863;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#20302;&#20449;&#22122;&#27604;&#21644;&#22833;&#30495;&#30340;DMI FID&#30340;&#20195;&#35874;&#29289;&#27987;&#24230;&#65292;&#24182;&#36890;&#36807;MRI&#30340;&#36793;&#32536;&#20445;&#25252;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#21442;&#25968;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04100</link><description>&lt;p&gt;
DMI (Deuterium Metabolic Imaging) &#30340;&#25935;&#24863;&#24615;&#22686;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Method for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI). (arXiv:2309.04100v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;DMI&#25935;&#24863;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#20302;&#20449;&#22122;&#27604;&#21644;&#22833;&#30495;&#30340;DMI FID&#30340;&#20195;&#35874;&#29289;&#27987;&#24230;&#65292;&#24182;&#36890;&#36807;MRI&#30340;&#36793;&#32536;&#20445;&#25252;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#21442;&#25968;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;DMI&#65288;Deuterium Metabolic Imaging&#65289;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#21644;&#26368;&#23567;&#25195;&#25551;&#26102;&#38388;&#21463;&#21040;&#21487;&#36798;&#21040;&#30340;&#20449;&#22122;&#27604;&#30340;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;DMI&#25935;&#24863;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#35774;&#35745;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#20272;&#35745;&#20302;&#20449;&#22122;&#27604;&#21644;&#22833;&#30495;&#30340;DMI FID&#30340;2H&#26631;&#35760;&#20195;&#35874;&#29289;&#27987;&#24230;&#12290;CNN&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25968;&#25454;&#20195;&#34920;&#36890;&#24120;&#22312;&#20307;&#20869;&#36935;&#21040;&#30340;&#19968;&#31995;&#21015;&#20449;&#22122;&#27604;&#27700;&#24179;&#12290;&#36890;&#36807;&#23545;&#27599;&#20010;DMI&#25968;&#25454;&#38598;&#20351;&#29992;&#22522;&#20110;MRI&#30340;&#36793;&#32536;&#20445;&#25252;&#27491;&#21017;&#21270;&#36827;&#34892;CNN&#30340;&#24494;&#35843;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20272;&#35745;&#31934;&#24230;&#12290;&#25552;&#20986;&#30340;&#22788;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;PRECISE-DMI&#65288;PReserved Edge ConvolutIonal neural network for Sensitivity Enhanced DMI&#65289;&#65292;&#24212;&#29992;&#20110;&#27169;&#25311;&#30740;&#31350;&#21644;&#20307;&#20869;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#22312;SNR&#19978;&#39044;&#26399;&#30340;&#25913;&#36827;&#65292;&#24182;&#30740;&#31350;&#20102;&#21487;&#33021;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#65306;PRECISE-DMI&#22312;&#20302;&#20449;&#22122;&#27604;&#25968;&#25454;&#38598;&#30340;&#20195;&#35874;&#22270;&#20687;&#19978;&#26377;&#30528;&#26126;&#26174;&#30340;&#25913;&#21892;&#65292;&#24182;&#25552;&#39640;&#20102;&#21442;&#25968;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Common to most MRSI techniques, the spatial resolution and the minimal scan duration of Deuterium Metabolic Imaging (DMI) are limited by the achievable SNR. This work presents a deep learning method for sensitivity enhancement of DMI.  Methods: A convolutional neural network (CNN) was designed to estimate the 2H-labeled metabolite concentrations from low SNR and distorted DMI FIDs. The CNN was trained with synthetic data that represent a range of SNR levels typically encountered in vivo. The estimation precision was further improved by fine-tuning the CNN with MRI-based edge-preserving regularization for each DMI dataset. The proposed processing method, PReserved Edge ConvolutIonal neural network for Sensitivity Enhanced DMI (PRECISE-DMI), was applied to simulation studies and in vivo experiments to evaluate the anticipated improvements in SNR and investigate the potential for inaccuracies.  Results: PRECISE-DMI visually improved the metabolic maps of low SNR datasets, and qua
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#20849;&#35774;&#35745;&#30340;&#26679;&#26412;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35774;&#35745;&#31354;&#38388;&#20013;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#36890;&#36807;&#29305;&#23450;&#26041;&#24335;&#36941;&#21382;&#35774;&#35745;&#30697;&#38453;&#65292;&#21487;&#20197;&#25552;&#39640;&#35774;&#35745;&#35780;&#20272;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.04085</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#22312;&#36890;&#29992;&#31574;&#30053;&#32593;&#32476;&#19978;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#20849;&#35774;&#35745;&#30340;&#26679;&#26412;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Co-Design of Robotic Agents Using Multi-fidelity Training on Universal Policy Network. (arXiv:2309.04085v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04085
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#20849;&#35774;&#35745;&#30340;&#26679;&#26412;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35774;&#35745;&#31354;&#38388;&#20013;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#36890;&#36807;&#29305;&#23450;&#26041;&#24335;&#36941;&#21382;&#35774;&#35745;&#30697;&#38453;&#65292;&#21487;&#20197;&#25552;&#39640;&#35774;&#35745;&#35780;&#20272;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#35774;&#35745;&#28041;&#21450;&#21516;&#26102;&#20248;&#21270;&#25511;&#21046;&#22120;&#21644;&#20195;&#29702;&#29289;&#29702;&#35774;&#35745;&#12290;&#20854;&#22266;&#26377;&#30340;&#21452;&#23618;&#20248;&#21270;&#24418;&#24335;&#35201;&#27714;&#36890;&#36807;&#20869;&#23618;&#25511;&#21046;&#20248;&#21270;&#26469;&#39537;&#21160;&#22806;&#23618;&#35774;&#35745;&#20248;&#21270;&#12290;&#24403;&#35774;&#35745;&#31354;&#38388;&#36739;&#22823;&#19988;&#27599;&#20010;&#35774;&#35745;&#35780;&#20272;&#37117;&#28041;&#21450;&#25968;&#25454;&#23494;&#38598;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#26102;&#65292;&#36825;&#21487;&#33021;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hyperband&#30340;&#22810;&#20445;&#30495;&#24230;&#35774;&#35745;&#25506;&#32034;&#31574;&#30053;&#65292;&#22312;&#35774;&#35745;&#31354;&#38388;&#20013;&#36890;&#36807;&#36890;&#29992;&#31574;&#30053;&#23398;&#20064;&#32773;&#23558;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#36827;&#34892;&#20851;&#32852;&#65292;&#20197;&#21551;&#21160;&#21518;&#32493;&#25511;&#21046;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#33616;&#19968;&#31181;&#29305;&#23450;&#30340;&#36941;&#21382;Hyperband&#29983;&#25104;&#30340;&#35774;&#35745;&#30697;&#38453;&#30340;&#26041;&#24335;&#65292;&#20197;&#30830;&#20445;&#38543;&#30528;&#27599;&#20010;&#26032;&#30340;&#35774;&#35745;&#35780;&#20272;&#65292;&#36890;&#29992;&#31574;&#30053;&#23398;&#20064;&#32773;&#30340;&#22686;&#24378;&#25928;&#26524;&#36234;&#26469;&#36234;&#24378;&#65292;&#20174;&#32780;&#38477;&#20302;Hyperband&#30340;&#38543;&#26426;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24180;&#40836;&#33539;&#22260;&#20869;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Co-design involves simultaneously optimizing the controller and agents physical design. Its inherent bi-level optimization formulation necessitates an outer loop design optimization driven by an inner loop control optimization. This can be challenging when the design space is large and each design evaluation involves data-intensive reinforcement learning process for control optimization. To improve the sample-efficiency we propose a multi-fidelity-based design exploration strategy based on Hyperband where we tie the controllers learnt across the design spaces through a universal policy learner for warm-starting the subsequent controller learning problems. Further, we recommend a particular way of traversing the Hyperband generated design matrix that ensures that the stochasticity of the Hyperband is reduced the most with the increasing warm starting effect of the universal policy learner as it is strengthened with each new design evaluation. Experiments performed on a wide range of age
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26354;&#29575;Transformer&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#23436;&#20840;&#20056;&#31215;&#31435;&#20307;&#21464;&#25442;&#22120;&#19982;&#26631;&#35760;&#21270;&#30340;&#22270;Transformer&#30456;&#32467;&#21512;&#65292;&#27169;&#22411;&#33021;&#22815;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#36866;&#24212;&#36755;&#20837;&#22270;&#30340;&#26354;&#29575;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23884;&#20837;&#22270;&#20013;&#30340;&#20998;&#23618;&#25110;&#24490;&#29615;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.04082</link><description>&lt;p&gt;
&#23545;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28151;&#21512;&#26354;&#29575;Transformer: &#25296;&#24367;&#20320;&#30340;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning. (arXiv:2309.04082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26354;&#29575;Transformer&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#23436;&#20840;&#20056;&#31215;&#31435;&#20307;&#21464;&#25442;&#22120;&#19982;&#26631;&#35760;&#21270;&#30340;&#22270;Transformer&#30456;&#32467;&#21512;&#65292;&#27169;&#22411;&#33021;&#22815;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#36866;&#24212;&#36755;&#20837;&#22270;&#30340;&#26354;&#29575;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23884;&#20837;&#22270;&#20013;&#30340;&#20998;&#23618;&#25110;&#24490;&#29615;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#24448;&#24448;&#20855;&#26377;&#19981;&#36866;&#21512;&#20856;&#22411;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#20998;&#23618;&#25110;&#24490;&#29615;&#32467;&#26500;&#12290;&#34429;&#28982;&#23384;&#22312;&#33021;&#22815;&#21033;&#29992;&#21452;&#26354;&#25110;&#29699;&#38754;&#31354;&#38388;&#23398;&#20064;&#26356;&#20934;&#30830;&#23884;&#20837;&#36825;&#20123;&#32467;&#26500;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23616;&#38480;&#20110;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#20351;&#24471;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#31561;&#21103;&#20316;&#29992;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#22270;Transformer&#65292;&#21487;&#20197;&#36731;&#26494;&#24314;&#27169;&#38271;&#31243;&#20132;&#20114;&#65292;&#20294;&#23545;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#30340;&#25299;&#23637;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23436;&#20840;&#20056;&#31215;&#31435;&#20307;&#21464;&#25442;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24120;&#26354;&#29575;&#31354;&#38388;&#30340;&#21464;&#25442;&#22120;&#30340;&#25512;&#24191;&#12290;&#24403;&#19982;&#26631;&#35760;&#21270;&#30340;&#22270;Transformer&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#36866;&#21512;&#36755;&#20837;&#22270;&#30340;&#26354;&#29575;&#65292;&#26080;&#38656;&#22312;&#19981;&#21516;&#26354;&#29575;&#19978;&#36827;&#34892;&#39069;&#22806;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world graphs naturally exhibit hierarchical or cyclical structures that are unfit for the typical Euclidean space. While there exist graph neural networks that leverage hyperbolic or spherical spaces to learn representations that embed such structures more accurately, these methods are confined under the message-passing paradigm, making the models vulnerable against side-effects such as oversmoothing and oversquashing. More recent work have proposed global attention-based graph Transformers that can easily model long-range interactions, but their extensions towards non-Euclidean geometry are yet unexplored. To bridge this gap, we propose Fully Product-Stereographic Transformer, a generalization of Transformers towards operating entirely on the product of constant curvature spaces. When combined with tokenized graph Transformers, our model can learn the curvature appropriate for the input graph in an end-to-end fashion, without the need of additional tuning on different curvature i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#20559;&#24046;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28857;&#31215;logits&#20998;&#35299;&#20026;&#35282;&#24230;&#22240;&#23376;&#21644;&#33539;&#25968;&#22240;&#23376;&#65292;&#35299;&#20915;&#20102;&#28857;&#31215;logits&#20559;&#24046;&#38382;&#39064;&#12290;&#35282;&#24230;&#22240;&#23376;&#29992;&#20110;&#23398;&#20064;&#26032;&#30340;&#30693;&#35782;&#65292;&#32780;&#33539;&#25968;&#22240;&#23376;&#26377;&#21161;&#20110;&#35760;&#20303;&#21382;&#21490;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.04081</link><description>&lt;p&gt;
UER: &#19968;&#31181;&#38024;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#20559;&#24046;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UER: A Heuristic Bias Addressing Approach for Online Continual Learning. (arXiv:2309.04081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#20559;&#24046;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28857;&#31215;logits&#20998;&#35299;&#20026;&#35282;&#24230;&#22240;&#23376;&#21644;&#33539;&#25968;&#22240;&#23376;&#65292;&#35299;&#20915;&#20102;&#28857;&#31215;logits&#20559;&#24046;&#38382;&#39064;&#12290;&#35282;&#24230;&#22240;&#23376;&#29992;&#20110;&#23398;&#20064;&#26032;&#30340;&#30693;&#35782;&#65292;&#32780;&#33539;&#25968;&#22240;&#23376;&#26377;&#21161;&#20110;&#35760;&#20303;&#21382;&#21490;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21333;&#27425;&#36941;&#21382;&#25968;&#25454;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#12290;&#20316;&#20026;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#22238;&#25918;&#30340;&#26041;&#27861;&#20250;&#37325;&#26032;&#25773;&#25918;&#37096;&#20998;&#20808;&#21069;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#24120;&#35265;&#39044;&#27979;&#22120;&#20542;&#21521;&#20110;&#29983;&#25104;&#20559;&#21521;&#24403;&#21069;&#25968;&#25454;&#31867;&#21035;&#30340;&#26377;&#20559;&#24046;&#30340;&#28857;&#31215;logits&#65292;&#36825;&#34987;&#31216;&#20026;&#20559;&#24046;&#38382;&#39064;&#21644;&#36951;&#24536;&#29616;&#35937;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#36890;&#36807;&#32416;&#27491;&#20559;&#24046;&#26469;&#20811;&#26381;&#36951;&#24536;&#38382;&#39064;&#65292;&#20294;&#26159;&#23427;&#20204;&#36824;&#38656;&#35201;&#22312;&#22312;&#32447;&#26041;&#24335;&#19979;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#19968;&#31181;&#26356;&#30452;&#25509;&#21644;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#28857;&#31215;logits&#20998;&#35299;&#20026;&#35282;&#24230;&#22240;&#23376;&#21644;&#33539;&#25968;&#22240;&#23376;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#20559;&#24046;&#38382;&#39064;&#20027;&#35201;&#21457;&#29983;&#22312;&#35282;&#24230;&#22240;&#23376;&#20013;&#65292;&#21487;&#20197;&#29992;&#26469;&#23398;&#20064;&#26032;&#30340;&#30693;&#35782;&#20316;&#20026;&#20313;&#24358;logits&#12290;&#30456;&#21453;&#65292;&#34987;&#29616;&#26377;&#26041;&#27861;&#25243;&#24323;&#30340;&#33539;&#25968;&#22240;&#23376;&#26377;&#21161;&#20110;&#35760;&#20303;&#21382;&#21490;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online continual learning aims to continuously train neural networks from a continuous data stream with a single pass-through data. As the most effective approach, the rehearsal-based methods replay part of previous data. Commonly used predictors in existing methods tend to generate biased dot-product logits that prefer to the classes of current data, which is known as a bias issue and a phenomenon of forgetting. Many approaches have been proposed to overcome the forgetting problem by correcting the bias; however, they still need to be improved in online fashion. In this paper, we try to address the bias issue by a more straightforward and more efficient method. By decomposing the dot-product logits into an angle factor and a norm factor, we empirically find that the bias problem mainly occurs in the angle factor, which can be used to learn novel knowledge as cosine logits. On the contrary, the norm factor abandoned by existing methods helps remember historical knowledge. Based on this
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36710;&#36742;&#21160;&#21147;&#23398;&#35780;&#20272;&#39550;&#39542;&#21592;&#29983;&#29702;&#29366;&#20917;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25972;&#21512;&#20102;&#21830;&#19994;&#20256;&#24863;&#22120;&#21644;&#39550;&#39542;&#21592;&#36755;&#20837;&#65292;&#21487;&#20197;&#25552;&#20379;&#39550;&#39542;&#34892;&#20026;&#21644;&#29983;&#29702;&#21453;&#24212;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#20855;&#26377;&#25552;&#21319;&#36947;&#36335;&#23433;&#20840;&#21644;&#26089;&#26399;&#26816;&#27979;&#20581;&#24247;&#30456;&#20851;&#24182;&#21457;&#30151;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04078</link><description>&lt;p&gt;
&#36890;&#36807;&#36710;&#36742;&#21160;&#21147;&#23398;&#35780;&#20272;&#39550;&#39542;&#21592;&#29983;&#29702;&#29366;&#20917;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Enabling the Evaluation of Driver Physiology Via Vehicle Dynamics. (arXiv:2309.04078v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36710;&#36742;&#21160;&#21147;&#23398;&#35780;&#20272;&#39550;&#39542;&#21592;&#29983;&#29702;&#29366;&#20917;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25972;&#21512;&#20102;&#21830;&#19994;&#20256;&#24863;&#22120;&#21644;&#39550;&#39542;&#21592;&#36755;&#20837;&#65292;&#21487;&#20197;&#25552;&#20379;&#39550;&#39542;&#34892;&#20026;&#21644;&#29983;&#29702;&#21453;&#24212;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#20855;&#26377;&#25552;&#21319;&#36947;&#36335;&#23433;&#20840;&#21644;&#26089;&#26399;&#26816;&#27979;&#20581;&#24247;&#30456;&#20851;&#24182;&#21457;&#30151;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39550;&#39542;&#23545;&#20840;&#29699;&#24456;&#22810;&#20154;&#26469;&#35828;&#26159;&#26085;&#24120;&#24037;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#36710;&#36742;&#36716;&#21464;&#20026;&#19968;&#20010;&#36830;&#25509;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#37197;&#32622;&#21644;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#35780;&#20272;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#29366;&#20917;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#27773;&#36710;&#21644;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#30340;&#21830;&#19994;&#20256;&#24863;&#22120;&#20197;&#21450;&#36710;&#36742;&#26412;&#36523;&#30340;&#39550;&#39542;&#21592;&#36755;&#20837;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#30340;&#32452;&#21512;&#33021;&#22815;&#31934;&#30830;&#35760;&#24405;&#22806;&#37096;&#26465;&#20214;&#21644;&#39550;&#39542;&#21160;&#20316;&#12290;&#36825;&#20123;&#25968;&#25454;&#27969;&#32463;&#36807;&#22788;&#29702;&#21518;&#25552;&#21462;&#20851;&#38190;&#21442;&#25968;&#65292;&#21487;&#20197;&#27934;&#23519;&#39550;&#39542;&#21592;&#22312;&#22806;&#37096;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#37325;&#35201;&#30340;&#29983;&#29702;&#21453;&#24212;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#39550;&#39542;&#21592;&#35780;&#20272;&#31995;&#32479;&#26377;&#21487;&#33021;&#25552;&#21319;&#36947;&#36335;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#24403;&#19982;&#20256;&#32479;&#20581;&#24247;&#35774;&#32622;&#30340;&#25968;&#25454;&#37197;&#23545;&#20351;&#29992;&#26102;&#65292;&#23427;&#21487;&#33021;&#22686;&#24378;&#23545;&#19982;&#20581;&#24247;&#30456;&#20851;&#24182;&#21457;&#30151;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driving is a daily routine for many individuals across the globe. This paper presents the configuration and methodologies used to transform a vehicle into a connected ecosystem capable of assessing driver physiology. We integrated an array of commercial sensors from the automotive and digital health sectors along with driver inputs from the vehicle itself. This amalgamation of sensors allows for meticulous recording of the external conditions and driving maneuvers. These data streams are processed to extract key parameters, providing insights into driver behavior in relation to their external environment and illuminating vital physiological responses. This innovative driver evaluation system holds the potential to amplify road safety. Moreover, when paired with data from conventional health settings, it may enhance early detection of health-related complications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;Riemannian Langevin Monte Carlo&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#20855;&#26377;&#22266;&#23450;&#31209;&#30340;PSD&#30697;&#38453;&#20013;&#37319;&#26679;&#12290;&#36825;&#20123;&#26041;&#26696;&#36890;&#36807;&#22312;&#27969;&#24418;&#19978;&#20351;&#29992;&#24067;&#26391;&#36816;&#21160;&#30340;Riemannian Langevin&#26041;&#31243;&#30340;Euler-Maruyama&#31163;&#25955;&#21270;&#26469;&#23454;&#29616;&#37319;&#26679;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.04072</link><description>&lt;p&gt;
Riemannian Langevin Monte Carlo&#26041;&#26696;&#29992;&#20110;&#20174;&#20855;&#26377;&#22266;&#23450;&#31209;&#30340;PSD&#30697;&#38453;&#20013;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Riemannian Langevin Monte Carlo schemes for sampling PSD matrices with fixed rank. (arXiv:2309.04072v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;Riemannian Langevin Monte Carlo&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#20855;&#26377;&#22266;&#23450;&#31209;&#30340;PSD&#30697;&#38453;&#20013;&#37319;&#26679;&#12290;&#36825;&#20123;&#26041;&#26696;&#36890;&#36807;&#22312;&#27969;&#24418;&#19978;&#20351;&#29992;&#24067;&#26391;&#36816;&#21160;&#30340;Riemannian Langevin&#26041;&#31243;&#30340;Euler-Maruyama&#31163;&#25955;&#21270;&#26469;&#23454;&#29616;&#37319;&#26679;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26174;&#24335;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174; $\mathcal S^{n,p}_+$ &#20013;&#30340;Gibbs&#20998;&#24067;&#20013;&#37319;&#26679;&#30697;&#38453;&#65292;&#20854;&#20013; $\mathcal S^{n,p}_+$ &#26159;&#23610;&#23544;&#20026;$n\times n$&#65292;&#31209;&#20026;$p$&#30340;&#23454;&#27491;&#21322;&#23450;&#65288;PSD&#65289;&#30697;&#38453;&#27969;&#24418;&#12290;&#32473;&#23450;&#19968;&#20010;&#33021;&#37327;&#20989;&#25968; $\mathcal E:\mathcal S^{n,p}_+\to \mathbb{R}$ &#21644;&#26576;&#20123;&#22312; $\mathcal S^{n,p}_+$ &#19978;&#30340;Riemannian&#24230;&#37327; $g$&#65292;&#36825;&#20123;&#26041;&#26696;&#20381;&#36182;&#20110;&#22312;&#27969;&#24418;&#19978;&#20351;&#29992;&#24067;&#26391;&#36816;&#21160;&#30340;Riemannian Langevin&#26041;&#31243;&#65288;RLE&#65289;&#30340;Euler-Maruyama&#31163;&#25955;&#21270;&#12290;&#25105;&#20204;&#38024;&#23545; $\mathcal S^{n,p}_+$ &#19978;&#30340;&#20004;&#20010;&#22522;&#26412;&#24230;&#37327;&#65288;a&#65289;&#20174;&#23884;&#20837; $\mathcal S^{n,p}_+ \subset \mathbb{R}^{n\times n} $ &#33719;&#24471;&#30340;&#24230;&#37327;&#65307;&#20197;&#21450;&#65288;b&#65289;&#23545;&#24212;&#20110;&#21830;&#27969;&#24418;&#20960;&#20309;&#30340;Bures-Wasserstein&#24230;&#37327;&#65292;&#25552;&#20379;&#20102;RLE&#30340;&#25968;&#20540;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20855;&#26377;&#26126;&#30830;Gibbs&#20998;&#24067;&#30340;&#33021;&#37327;&#20989;&#25968;&#30340;&#31034;&#20363;&#65292;&#20197;&#20415;&#23545;&#36825;&#20123;&#26041;&#26696;&#36827;&#34892;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces two explicit schemes to sample matrices from Gibbs distributions on $\mathcal S^{n,p}_+$, the manifold of real positive semi-definite (PSD) matrices of size $n\times n$ and rank $p$. Given an energy function $\mathcal E:\mathcal S^{n,p}_+\to \mathbb{R}$ and certain Riemannian metrics $g$ on $\mathcal S^{n,p}_+$, these schemes rely on an Euler-Maruyama discretization of the Riemannian Langevin equation (RLE) with Brownian motion on the manifold. We present numerical schemes for RLE under two fundamental metrics on $\mathcal S^{n,p}_+$: (a) the metric obtained from the embedding of $\mathcal S^{n,p}_+ \subset \mathbb{R}^{n\times n} $; and (b) the Bures-Wasserstein metric corresponding to quotient geometry. We also provide examples of energy functions with explicit Gibbs distributions that allow numerical validation of these schemes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;D&amp;D&#65292;&#36890;&#36807;&#23558;3D&#21435;&#22122;&#22120;&#30340;&#34920;&#31034;&#33976;&#39311;&#21040;2D&#22270;&#24418;&#32534;&#30721;&#22120;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#20174;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04062</link><description>&lt;p&gt;
3D&#21435;&#22122;&#22120;&#26159;&#22909;&#30340;2D&#25945;&#24072;&#65306;&#36890;&#36807;&#21435;&#22122;&#21644;&#36328;&#27169;&#24577;&#33976;&#39311;&#36827;&#34892;&#20998;&#23376;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation. (arXiv:2309.04062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;D&amp;D&#65292;&#36890;&#36807;&#23558;3D&#21435;&#22122;&#22120;&#30340;&#34920;&#31034;&#33976;&#39311;&#21040;2D&#22270;&#24418;&#32534;&#30721;&#22120;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#20174;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#23545;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#33719;&#21462;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;&#34429;&#28982;&#23384;&#22312;&#21508;&#31181;&#22522;&#20110;2D&#22270;&#24418;&#30340;&#20998;&#23376;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38590;&#20197;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#26174;&#31034;&#20986;&#32479;&#35745;&#23398;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#21435;&#22122;&#20219;&#21153;&#19979;&#22522;&#20110;3D&#26500;&#35937;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#19979;&#28216;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;3D&#26500;&#35937;&#35757;&#32451;&#30340;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#30340;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#20998;&#23376;&#21407;&#23376;&#22352;&#26631;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22522;&#20110;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D&amp;D&#65292;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;3D&#21435;&#22122;&#22120;&#30340;&#34920;&#31034;&#33976;&#39311;&#21040;2D&#22270;&#24418;&#32534;&#30721;&#22120;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#21435;&#22122;&#21644;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21033;&#29992;&#20102;&#20174;&#21435;&#22122;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#32780;&#19988;&#24212;&#29992;&#36215;&#26469;&#24456;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, which led to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In light of this limitation, we propose D&amp;D, a self-supervised molecular representation learning framework that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless applica
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;SRN-SZ&#65292;&#29992;&#20110;&#25913;&#21892;&#38590;&#20197;&#21387;&#32553;&#30340;&#25968;&#25454;&#38598;&#30340;&#21387;&#32553;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.04037</link><description>&lt;p&gt;
SRN-SZ: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#19982;&#36229;&#20998;&#36776;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks. (arXiv:2309.04037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;SRN-SZ&#65292;&#29992;&#20110;&#25913;&#21892;&#38590;&#20197;&#21387;&#32553;&#30340;&#25968;&#25454;&#38598;&#30340;&#21387;&#32553;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36229;&#32423;&#35745;&#31639;&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#35268;&#27169;&#30340;&#24555;&#36895;&#22686;&#38271;&#32473;&#36229;&#32423;&#35745;&#31639;&#31995;&#32479;&#30340;&#31649;&#29702;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20445;&#25345;&#31185;&#23398;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#25552;&#20986;&#24182;&#21457;&#23637;&#20102;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#20316;&#20026;&#31185;&#23398;&#25968;&#25454;&#23610;&#23544;&#32553;&#20943;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#20197;&#38480;&#21046;&#25968;&#25454;&#22833;&#30495;&#12290;&#22312;&#21508;&#31181;&#31185;&#23398;&#27169;&#25311;&#29983;&#25104;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#20013;&#65292;&#26576;&#20123;&#25968;&#25454;&#38598;&#26080;&#27861;&#36890;&#36807;&#29616;&#26377;&#30340;&#20256;&#32479;&#25216;&#26415;&#30340;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;&#26377;&#25928;&#21387;&#32553;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#20102;&#22810;&#20301;&#30740;&#31350;&#20154;&#21592;&#23558;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#20173;&#28982;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#21387;&#32553;&#27604;&#21644;/&#25110;&#26497;&#20302;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#25913;&#21892;&#38590;&#20197;&#21387;&#32553;&#30340;&#25968;&#25454;&#38598;&#30340;&#21387;&#32553;&#24773;&#20917;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SRN-SZ&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fast growth of computational power and scales of modern super-computing systems have raised great challenges for the management of exascale scientific data. To maintain the usability of scientific data, error-bound lossy compression is proposed and developed as an essential technique for the size reduction of scientific data with constrained data distortion. Among the diverse datasets generated by various scientific simulations, certain datasets cannot be effectively compressed by existing error-bounded lossy compressors with traditional techniques. The recent success of Artificial Intelligence has inspired several researchers to integrate neural networks into error-bounded lossy compressors. However, those works still suffer from limited compression ratios and/or extremely low efficiencies. To address those issues and improve the compression on the hard-to-compress datasets, in this paper, we propose SRN-SZ, which is a deep learning-based scientific error-bounded lossy compressor 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#28608;&#27963;&#19982;&#27963;&#21160;&#21160;&#21147;&#23398;&#32447;&#24615;&#21270;&#36807;&#31243;&#20013;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#32447;&#24615;&#21270;&#27963;&#21160;&#21160;&#21147;&#23398;&#19979;&#19968;&#20123;&#19978;&#19979;&#25991;&#30456;&#20851;&#25928;&#24212;&#30340;&#26174;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.04030</link><description>&lt;p&gt;
&#22312;&#36880;&#28857;&#38750;&#32447;&#24615;&#20043;&#21069;&#19982;&#20043;&#21518;&#32447;&#24615;&#21270;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476; (RNNs) &#30340;&#31616;&#35201;&#25216;&#26415;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
Brief technical note on linearizing recurrent neural networks (RNNs) before vs after the pointwise nonlinearity. (arXiv:2309.04030v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04030
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#28608;&#27963;&#19982;&#27963;&#21160;&#21160;&#21147;&#23398;&#32447;&#24615;&#21270;&#36807;&#31243;&#20013;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#32447;&#24615;&#21270;&#27963;&#21160;&#21160;&#21147;&#23398;&#19979;&#19968;&#20123;&#19978;&#19979;&#25991;&#30456;&#20851;&#25928;&#24212;&#30340;&#26174;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#24120;&#20351;&#29992;&#32447;&#24615;&#21270;&#30740;&#31350;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476; (RNNs) &#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#12290;&#30456;&#21516;&#30340; RNN &#21160;&#21147;&#23398;&#21487;&#20197;&#29992;&#8220;&#28608;&#27963;&#8221;&#65288;&#27599;&#20010;&#21333;&#20803;&#30340;&#20928;&#36755;&#20837;&#65292;&#22312;&#36880;&#28857;&#38750;&#32447;&#24615;&#20043;&#21069;&#65289;&#25110;&#8220;&#27963;&#21160;&#8221;&#65288;&#27599;&#20010;&#21333;&#20803;&#30340;&#36755;&#20986;&#65292;&#22312;&#36880;&#28857;&#38750;&#32447;&#24615;&#20043;&#21518;&#65289;&#26469;&#34920;&#31034;&#65307;&#20004;&#31181;&#23545;&#24212;&#30340;&#32447;&#24615;&#21270;&#26041;&#27861;&#24444;&#27492;&#19981;&#21516;&#12290;&#36825;&#31687;&#31616;&#30701;&#30340;&#38750;&#27491;&#24335;&#25216;&#26415;&#35828;&#26126;&#25551;&#36848;&#20102;&#20004;&#31181;&#32447;&#24615;&#21270;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23427;&#20204;&#21160;&#21147;&#23398;&#30697;&#38453;&#30340;&#24038;&#21491;&#29305;&#24449;&#21521;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#34920;&#26126;&#19968;&#20123;&#19978;&#19979;&#25991;&#30456;&#20851;&#25928;&#24212;&#22312;&#27963;&#21160;&#21160;&#21147;&#23398;&#30340;&#32447;&#24615;&#21270;&#19979;&#26174;&#32780;&#26131;&#35265;&#65292;&#32780;&#22312;&#28608;&#27963;&#21160;&#21147;&#23398;&#30340;&#32447;&#24615;&#21270;&#19979;&#21017;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linearization of the dynamics of recurrent neural networks (RNNs) is often used to study their properties. The same RNN dynamics can be written in terms of the ``activations" (the net inputs to each unit, before its pointwise nonlinearity) or in terms of the ``activities" (the output of each unit, after its pointwise nonlinearity); the two corresponding linearizations are different from each other. This brief and informal technical note describes the relationship between the two linearizations, between the left and right eigenvectors of their dynamics matrices, and shows that some context-dependent effects are readily apparent under linearization of activity dynamics but not linearization of activation dynamics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;TIDE&#65288;Textual Identity Detection&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#36523;&#20221;&#35789;&#27719;&#21644;&#35821;&#22659;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#24320;&#21457;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04027</link><description>&lt;p&gt;
TIDE: &#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36523;&#20221;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models. (arXiv:2309.04027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TIDE&#65288;Textual Identity Detection&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#36523;&#20221;&#35789;&#27719;&#21644;&#35821;&#22659;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#24320;&#21457;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#19981;&#20844;&#27491;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#24847;&#22806;&#20559;&#35265;&#12290;&#22312;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#65292;&#35780;&#20272;&#21644;&#21435;&#20559;&#36825;&#20123;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#23588;&#20854;&#22256;&#38590;&#65292;&#22240;&#20026;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#24615;&#21462;&#21521;&#31561;&#25935;&#24863;&#23646;&#24615;&#21487;&#33021;&#19981;&#21487;&#29992;&#12290;&#24403;&#36825;&#20123;&#27169;&#22411;&#25237;&#25918;&#21040;&#31038;&#20250;&#20013;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#23545;&#21382;&#21490;&#19978;&#24369;&#21183;&#32676;&#20307;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#36523;&#20221;&#35789;&#27719;&#34920;TIDAL&#65292;&#21253;&#25324;15,123&#20010;&#36523;&#20221;&#26415;&#35821;&#21644;&#30456;&#20851;&#30340;&#35821;&#22659;&#65292;&#28085;&#30422;&#20102;&#19977;&#20010;&#20154;&#21475;&#32479;&#35745;&#31867;&#21035;&#12290;&#25105;&#20204;&#21033;&#29992;TIDAL&#24320;&#21457;&#20102;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#36523;&#20221;&#35821;&#22659;&#30340;&#21487;&#29992;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#36129;&#29486;&#32773;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21435;&#20559;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models can perpetuate unintended biases from unfair and imbalanced datasets. Evaluating and debiasing these datasets and models is especially hard in text datasets where sensitive attributes such as race, gender, and sexual orientation may not be available. When these models are deployed into society, they can lead to unfair outcomes for historically underrepresented groups. In this paper, we present a dataset coupled with an approach to improve text fairness in classifiers and language models. We create a new, more comprehensive identity lexicon, TIDAL, which includes 15,123 identity terms and associated sense context across three demographic categories. We leverage TIDAL to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of ML fairness techniques. We evaluate our approaches using human contributors, and additionally run experiments focused on dataset and model debiasing. Resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#24191;&#20102;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#21487;&#25511;&#30340;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04015</link><description>&lt;p&gt;
&#20351;&#29992;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport with Tempered Exponential Measures. (arXiv:2309.04015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#24191;&#20102;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#21487;&#25511;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#20013;&#65292;&#20004;&#20010;&#37325;&#35201;&#30340;&#23376;&#39046;&#22495;&#30456;&#20114;&#23545;&#31435;&#65306;&#65288;i&#65289;&#38750;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#65292;&#8220;&#21345;&#25176;&#35834;&#32500;&#22855;&#26041;&#24335;&#8221;&#65292;&#23548;&#33268;&#20102;&#38750;&#24120;&#31232;&#30095;&#30340;&#35268;&#21010;&#65292;&#20294;&#31639;&#27861;&#25928;&#29575;&#36739;&#20302;&#65307;&#65288;ii&#65289;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#65292;&#8220;&#36763;&#20811;&#38669;&#24681;-&#24211;&#37117;&#37324;&#26041;&#24335;&#8221;&#65292;&#33719;&#24471;&#20102;&#36817;&#20284;&#32447;&#24615;&#31639;&#27861;&#65292;&#20294;&#26368;&#22823;&#31243;&#24230;&#19978;&#26080;&#27861;&#31232;&#30095;&#35268;&#21010;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21518;&#19968;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#65292;&#21363;&#20855;&#26377;&#38388;&#25509;&#27979;&#24230;&#24402;&#19968;&#21270;&#30340;&#25351;&#25968;&#26063;&#27867;&#21270;&#65292;&#21462;&#24471;&#20102;&#38750;&#24120;&#26041;&#20415;&#30340;&#25240;&#20013;&#25928;&#26524;&#65292;&#20855;&#26377;&#38750;&#24120;&#24555;&#30340;&#36817;&#20284;&#31639;&#27861;&#21644;&#21487;&#25511;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20063;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of optimal transport, two prominent subfields face each other: (i) unregularized optimal transport, ``\`a-la-Kantorovich'', which leads to extremely sparse plans but with algorithms that scale poorly, and (ii) entropic-regularized optimal transport, ``\`a-la-Sinkhorn-Cuturi'', which gets near-linear approximation algorithms but leads to maximally un-sparse plans. In this paper, we show that a generalization of the latter to tempered exponential measures, a generalization of exponential families with indirect measure normalization, gets to a very convenient middle ground, with both very fast approximation algorithms and sparsity which is under control up to sparsity patterns. In addition, it fits naturally in the unbalanced optimal transport problem setting as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#28385;&#36275;&#26080;&#26465;&#20214;&#33021;&#37327;&#32791;&#25955;&#23450;&#24459;&#12289;&#22312;&#20984;&#35774;&#32622;&#20013;&#35777;&#26126;&#32447;&#24615;&#25910;&#25947;&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;E-RSAV&#65292;&#24182;&#19988;&#22312;&#21333;&#21464;&#37327;&#24773;&#20917;&#19979;&#25913;&#36827;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#20026;&#36229;&#32447;&#24615;&#65292;&#36824;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;E-RSAV&#31639;&#27861;&#21152;&#20197;&#23454;&#29616;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.04013</link><description>&lt;p&gt;
&#19968;&#31181;&#36880;&#20803;&#24179;&#26041;&#21644;&#25918;&#26494;&#30340;&#36741;&#21161;&#21464;&#37327;&#31639;&#27861;&#29992;&#20110;&#26080;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An Element-wise RSAV Algorithm for Unconstrained Optimization Problems. (arXiv:2309.04013v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#28385;&#36275;&#26080;&#26465;&#20214;&#33021;&#37327;&#32791;&#25955;&#23450;&#24459;&#12289;&#22312;&#20984;&#35774;&#32622;&#20013;&#35777;&#26126;&#32447;&#24615;&#25910;&#25947;&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;E-RSAV&#65292;&#24182;&#19988;&#22312;&#21333;&#21464;&#37327;&#24773;&#20917;&#19979;&#25913;&#36827;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#20026;&#36229;&#32447;&#24615;&#65292;&#36824;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;E-RSAV&#31639;&#27861;&#21152;&#20197;&#23454;&#29616;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#21363;&#36880;&#20803;&#25918;&#26494;&#30340;&#36741;&#21161;&#26631;&#37327;&#21464;&#37327;&#65288;E-RSAV&#65289;&#65292;&#23427;&#28385;&#36275;&#26080;&#26465;&#20214;&#30340;&#33021;&#37327;&#32791;&#25955;&#23450;&#24459;&#65292;&#24182;&#19988;&#25913;&#36827;&#20102;&#20462;&#25913;&#21518;&#19982;&#21407;&#33021;&#37327;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20984;&#35774;&#32622;&#20013;&#35777;&#26126;&#20102;&#32447;&#24615;&#25910;&#25947;&#30340;&#20005;&#26684;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21152;&#36895;&#31639;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#21333;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#20026;&#36229;&#32447;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;E-RSAV&#31639;&#27861;&#65292;&#37319;&#29992;Steffensen&#27493;&#38271;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel optimization algorithm, element-wise relaxed scalar auxiliary variable (E-RSAV), that satisfies an unconditional energy dissipation law and exhibits improved alignment between the modified and the original energy. Our algorithm features rigorous proofs of linear convergence in the convex setting. Furthermore, we present a simple accelerated algorithm that improves the linear convergence rate to super-linear in the univariate case. We also propose an adaptive version of E-RSAV with Steffensen step size. We validate the robustness and fast convergence of our algorithm through ample numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20998;&#21106;&#26041;&#27861;MMSFormer&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MCubeS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.04001</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#29992;&#20110;&#26448;&#26009;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multimodal Transformer for Material Segmentation. (arXiv:2309.04001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20998;&#21106;&#26041;&#27861;MMSFormer&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MCubeS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22810;&#27169;&#24577;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#20010;&#27169;&#24577;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65306;RGB&#12289;&#32447;&#24615;&#20559;&#25391;&#35282;&#65288;AoLP&#65289;&#12289;&#32447;&#24615;&#20559;&#25391;&#24230;&#65288;DoLP&#65289;&#21644;&#36817;&#32418;&#22806;&#65288;NIR&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#20998;&#21106;&#21464;&#25442;&#22120;&#65288;MMSFormer&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#22810;&#27169;&#24577;&#26448;&#26009;&#20998;&#21106;&#12290;MMSFormer&#22312;&#22810;&#27169;&#24577;&#26448;&#26009;&#20998;&#21106;&#65288;MCubeS&#65289;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;52.05&#65285;&#30340;mIoU&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#30782;&#30707;&#65288;+10.4&#65285;&#65289;&#21644;&#20154;&#31867;&#65288;+9.1&#65285;&#65289;&#31867;&#19978;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#34701;&#21512;&#22359;&#20013;&#30340;&#19981;&#21516;&#27169;&#22359;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes. Ablation studies show that different modules in the fusion block are crucial for
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#39046;&#22495;&#35299;&#32544;&#27169;&#22359;&#65288;DDM&#65289;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.03999</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#30417;&#30563;&#34920;&#31034;&#21040;&#22810;&#39046;&#22495;&#35774;&#32622;&#20013;
&lt;/p&gt;
&lt;p&gt;
Adapting Self-Supervised Representations to Multi-Domain Setups. (arXiv:2309.03999v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#39046;&#22495;&#35299;&#32544;&#27169;&#22359;&#65288;DDM&#65289;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22312;&#21333;&#20010;&#39046;&#22495;&#19978;&#35757;&#32451;&#26102;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#22312;&#28151;&#21512;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20063;&#24456;&#24046;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#37096;&#32626;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#12289;&#36731;&#37327;&#32423;&#30340;&#39046;&#22495;&#35299;&#32544;&#27169;&#22359;&#65288;DDM&#65289;&#65292;&#21487;&#20197;&#25554;&#20837;&#21040;&#20219;&#20309;&#33258;&#30417;&#30563;&#32534;&#30721;&#22120;&#20013;&#65292;&#22312;&#20855;&#26377;&#25110;&#19981;&#20855;&#26377;&#20849;&#20139;&#31867;&#30340;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#19978;&#26377;&#25928;&#22320;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#33258;&#30417;&#30563;&#25439;&#22833;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;DDM&#36890;&#36807;&#20998;&#21106;&#34920;&#31034;&#31354;&#38388;&#20026;&#39046;&#22495;&#21464;&#20307;&#21644;&#39046;&#22495;&#19981;&#21464;&#37096;&#20998;&#26469;&#24378;&#21046;&#23454;&#29616;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#35299;&#32544;&#12290;&#24403;&#27809;&#26377;&#39046;&#22495;&#26631;&#31614;&#21487;&#29992;&#26102;&#65292;DDM&#20351;&#29992;&#24378;&#20581;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#20266;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;DDM&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#19978;&#26174;&#31034;&#20986;&#39640;&#36798;3.5%&#30340;&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including 
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.03992</link><description>&lt;p&gt;
ConDA: &#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03992
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#21508;&#31181;&#29992;&#36884;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#26032;&#38395;&#25253;&#36947;&#12290;&#37492;&#20110;&#36825;&#20123;LLMs&#21487;&#33021;&#34987;&#24694;&#24847;&#20351;&#29992;&#26469;&#22823;&#35268;&#27169;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#26032;&#30340;LLMs&#19981;&#26029;&#34987;&#24320;&#21457;&#65292;&#33719;&#21462;&#29992;&#20110;&#30417;&#30563;&#24335;&#26816;&#27979;&#22120;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#27809;&#26377;&#20851;&#20110;&#20854;&#29983;&#25104;&#22120;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27492;&#25968;&#25454;&#38382;&#39064;&#65292;&#21363;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#65292;&#24182;&#23558;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#20219;&#21153;&#12290;&#36825;&#37324;&#30340;&#22495;&#26159;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#22120;&#65292;&#21363;LLMs&#65292;&#25105;&#20204;&#20551;&#35774;&#21482;&#33021;&#35775;&#38382;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;ConDA&#30340;&#23545;&#27604;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#26631;&#20934;&#30340;&#22495;&#36866;&#24212;&#25216;&#26415;&#19982;&#34920;&#31034;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#20013;&#23548;&#20986;&#20102;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#19982;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#30340;&#21463;&#25511;&#32791;&#25955;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.03990</link><description>&lt;p&gt;
&#20174;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#23548;&#20986;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Derivation of Coordinate Descent Algorithms from Optimal Control Theory. (arXiv:2309.03990v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#20013;&#23548;&#20986;&#20102;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#19982;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#30340;&#21463;&#25511;&#32791;&#25955;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20154;&#25552;&#20986;&#65292;&#21487;&#20197;&#29992;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#30340;&#19968;&#20010;&#26680;&#24515;&#28304;&#26469;&#32479;&#19968;&#19981;&#21516;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#36825;&#19968;&#21629;&#39064;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#36825;&#19968;&#26032;&#20852;&#21407;&#29702;&#20013;&#23548;&#20986;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#26412;&#30340;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21407;&#29702;&#21644;&#19968;&#31995;&#21015;&#26368;&#22823;&#20989;&#25968;&#20316;&#20026;&#8220;&#25511;&#21046;&#8221;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#26469;&#23548;&#20986;&#12290;&#22240;&#27492;&#65292;&#24471;&#21040;&#30340;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#19982;&#30456;&#24212;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#30340;&#21463;&#25511;&#32791;&#25955;&#26377;&#20851;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#25628;&#32034;&#21521;&#37327;&#30340;&#25805;&#20316;&#24230;&#37327;&#30001;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it was posited that disparate optimization algorithms may be coalesced in terms of a central source emanating from optimal control theory. Here we further this proposition by showing how coordinate descent algorithms may be derived from this emerging new principle. In particular, we show that basic coordinate descent algorithms can be derived using a maximum principle and a collection of max functions as "control" Lyapunov functions. The convergence of the resulting coordinate descent algorithms is thus connected to the controlled dissipation of their corresponding Lyapunov functions. The operational metric for the search vector in all cases is given by the Hessian of the convex objective function.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;&#22122;&#22768;&#26597;&#35810;&#35745;&#31639;$\mathsf{OR}$&#21644;$\mathsf{MAX}$&#20989;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38169;&#35823;&#27010;&#29575;&#25910;&#25947;&#26102;&#65292;&#35745;&#31639;&#36825;&#20004;&#20010;&#20989;&#25968;&#25152;&#38656;&#30340;&#26399;&#26395;&#26597;&#35810;&#25968;&#37327;&#19982;Kullback-Leibler&#24046;&#24322;&#20043;&#38388;&#26377;&#23494;&#20999;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.03986</link><description>&lt;p&gt;
&#22122;&#22768;&#35745;&#31639;$\mathsf{OR}$&#21644;$\mathsf{MAX}$&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Noisy Computing of the $\mathsf{OR}$ and $\mathsf{MAX}$ Functions. (arXiv:2309.03986v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;&#22122;&#22768;&#26597;&#35810;&#35745;&#31639;$\mathsf{OR}$&#21644;$\mathsf{MAX}$&#20989;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38169;&#35823;&#27010;&#29575;&#25910;&#25947;&#26102;&#65292;&#35745;&#31639;&#36825;&#20004;&#20010;&#20989;&#25968;&#25152;&#38656;&#30340;&#26399;&#26395;&#26597;&#35810;&#25968;&#37327;&#19982;Kullback-Leibler&#24046;&#24322;&#20043;&#38388;&#26377;&#23494;&#20999;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#21547;&#26377;&#22122;&#22768;&#30340;&#26597;&#35810;&#26469;&#35745;&#31639;&#19968;&#20010;&#21253;&#21547;$n$&#20010;&#21464;&#37327;&#30340;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26597;&#35810;&#22312;&#26576;&#20010;&#22266;&#23450;&#30340;&#24050;&#30693;&#27010;&#29575;$p \in (0,1/2)$&#19979;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#35745;&#31639;$n$&#20010;&#20301;&#30340;$\mathsf{OR}$&#20989;&#25968;&#65288;&#20854;&#20013;&#26597;&#35810;&#23545;&#24212;&#20110;&#20301;&#30340;&#22122;&#22768;&#35835;&#25968;&#65289;&#21644;$n$&#20010;&#23454;&#25968;&#30340;$\mathsf{MAX}$&#20989;&#25968;&#65288;&#20854;&#20013;&#26597;&#35810;&#23545;&#24212;&#20110;&#26377;&#22122;&#22768;&#30340;&#20004;&#20004;&#27604;&#36739;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#35823;&#24046;&#27010;&#29575;$\delta = o(1)$&#36805;&#36895;&#25910;&#25947;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#36825;&#20004;&#20010;&#20989;&#25968;&#25152;&#38656;&#30340;&#26399;&#26395;&#26597;&#35810;&#25968;&#37327;&#20026;\[ (1 \pm o(1)) \frac{n\log \frac{1}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)} \] &#65292;&#20854;&#20013; $D_{\mathsf{KL}}(p \| 1-p)$&#34920;&#31034;$\mathsf{Bern}(p)$&#21644;$\mathsf{Bern}(1-p)$&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#24046;&#24322;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#20004;&#20010;&#20989;&#25968;&#30340;&#19978;&#19979;&#30028;&#20013;&#37117;&#21152;&#24378;&#20102;&#23545;$p$&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of computing a function of $n$ variables using noisy queries, where each query is incorrect with some fixed and known probability $p \in (0,1/2)$. Specifically, we consider the computation of the $\mathsf{OR}$ function of $n$ bits (where queries correspond to noisy readings of the bits) and the $\mathsf{MAX}$ function of $n$ real numbers (where queries correspond to noisy pairwise comparisons). We show that an expected number of queries of \[ (1 \pm o(1)) \frac{n\log \frac{1}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)} \] is both sufficient and necessary to compute both functions with a vanishing error probability $\delta = o(1)$, where $D_{\mathsf{KL}}(p \| 1-p)$ denotes the Kullback-Leibler divergence between $\mathsf{Bern}(p)$ and $\mathsf{Bern}(1-p)$ distributions. Compared to previous work, our results tighten the dependence on $p$ in both the upper and lower bounds for the two functions.
&lt;/p&gt;</description></item><item><title>LanSER&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#24369;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26631;&#20934;SER&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#27169;&#25311;&#35821;&#38899;&#30340;&#38901;&#24459;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2309.03978</link><description>&lt;p&gt;
LanSER: &#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LanSER: Language-Model Supported Speech Emotion Recognition. (arXiv:2309.03978v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03978
&lt;/p&gt;
&lt;p&gt;
LanSER&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#24369;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26631;&#20934;SER&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#27169;&#25311;&#35821;&#38899;&#30340;&#38901;&#24459;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#65288;SER&#65289;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;&#23545;&#22823;&#22411;&#35821;&#38899;&#25968;&#25454;&#38598;&#21644;&#24494;&#22937;&#24773;&#32490;&#20998;&#31867;&#30340;&#25193;&#23637;&#26041;&#27861;&#22256;&#38590;&#37325;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LanSER&#65292;&#19968;&#31181;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24369;&#30417;&#30563;&#23398;&#20064;&#26469;&#25512;&#27979;&#24369;&#24773;&#32490;&#26631;&#31614;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#12290;&#23545;&#20110;&#21463;&#21040;&#20998;&#31867;&#32422;&#26463;&#30340;&#24369;&#26631;&#31614;&#25512;&#27979;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#25991;&#26412;&#34164;&#28085;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25552;&#21462;&#30340;&#35821;&#38899;&#36716;&#24405;&#20013;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#26368;&#39640;&#34164;&#28085;&#24471;&#20998;&#30340;&#24773;&#32490;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#24369;&#30417;&#30563;&#35757;&#32451;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26631;&#20934;SER&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#20986;&#25913;&#36827;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;&#23613;&#31649;&#21482;&#22312;&#25991;&#26412;&#19978;&#25512;&#27979;&#20986;&#30340;&#26631;&#31614;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#24471;&#21040;&#30340;&#34920;&#31034;&#20284;&#20046;&#33021;&#22815;&#27169;&#25311;&#35821;&#38899;&#30340;&#38901;&#24459;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition (SER) models typically rely on costly human-labeled data for training, making scaling methods to large speech datasets and nuanced emotion taxonomies difficult. We present LanSER, a method that enables the use of unlabeled data by inferring weak emotion labels via pre-trained large language models through weakly-supervised learning. For inferring weak labels constrained to a taxonomy, we use a textual entailment approach that selects an emotion label with the highest entailment score for a speech transcript extracted via automatic speech recognition. Our experimental results show that models pre-trained on large datasets with this weak supervision outperform other baseline models on standard SER datasets when fine-tuned, and show improved label efficiency. Despite being pre-trained on labels derived only from text, we show that the resulting representations appear to model the prosodic content of speech.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65288;ACEM&#65289;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#27010;&#24565;&#27880;&#37322;&#23545;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#26114;&#36149;&#21644;&#19981;&#21487;&#34892;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03970</link><description>&lt;p&gt;
&#33258;&#21160;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65288;ACEM&#65289;&#65306;&#26080;&#38656;&#35757;&#32451;&#26102;&#30340;&#27010;&#24565;&#65292;&#26080;&#38656;&#25285;&#24515;!
&lt;/p&gt;
&lt;p&gt;
Automatic Concept Embedding Model (ACEM): No train-time concepts, No issue!. (arXiv:2309.03970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03970
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65288;ACEM&#65289;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#27010;&#24565;&#27880;&#37322;&#23545;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#26114;&#36149;&#21644;&#19981;&#21487;&#34892;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#21644;&#25552;&#20379;&#31038;&#20250;&#35299;&#37322;&#26435;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#19982;&#20154;&#31867;&#25512;&#29702;&#30456;&#21563;&#21512;&#65292;&#35777;&#26126;&#26159;&#19968;&#31181;&#24456;&#22909;&#30340;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65288;CEM&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26550;&#26500;&#12290;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20811;&#26381;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046; - &#23427;&#20204;&#38656;&#35201;&#20026;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#27010;&#24565;&#27880;&#37322;&#12290;&#23545;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#36825;&#21487;&#33021;&#26159;&#26114;&#36149;&#32780;&#19981;&#21487;&#34892;&#30340;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65288;ACEM&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#27010;&#24565;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability and explainability of neural networks is continuously increasing in importance, especially within safety-critical domains and to provide the social right to explanation. Concept based explanations align well with how humans reason, proving to be a good way to explain models. Concept Embedding Models (CEMs) are one such concept based explanation architectures. These have shown to overcome the trade-off between explainability and performance. However, they have a key limitation -- they require concept annotations for all their training data. For large datasets, this can be expensive and infeasible. Motivated by this, we propose Automatic Concept Embedding Models (ACEMs), which learn the concept annotations automatically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#25913;&#36827;ResNet-9&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#20854;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#19981;&#21040;10&#20998;&#38047;&#30340;&#26102;&#38388;&#20869;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#30340;10%&#23376;&#38598;&#19978;&#36798;&#21040;&#20102;88%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.03965</link><description>&lt;p&gt;
&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#25913;&#36827;ResNet-9&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Resnet-9 Generalization Trained on Small Datasets. (arXiv:2309.03965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#25913;&#36827;ResNet-9&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#20854;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#19981;&#21040;10&#20998;&#38047;&#30340;&#26102;&#38388;&#20869;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#30340;10%&#23376;&#38598;&#19978;&#36798;&#21040;&#20102;88%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;ICLR&#30828;&#20214;&#24863;&#30693;&#39640;&#25928;&#35757;&#32451;&#31454;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;&#25361;&#25112;&#26159;&#22312;&#19981;&#21040;10&#20998;&#38047;&#30340;&#26102;&#38388;&#20869;&#65292;&#22312;&#19968;&#20010;&#23567;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#23613;&#21487;&#33021;&#39640;&#30340;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;&#35757;&#32451;&#20351;&#29992;&#30340;&#23567;&#25968;&#25454;&#38598;&#26159;&#20174;CIFAR-10&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#25361;&#36873;&#30340;5000&#24133;&#22270;&#20687;&#12290;&#35780;&#20272;&#30001;&#31454;&#36187;&#32452;&#32455;&#32773;&#22312;&#19968;&#20010;&#21253;&#21547;1000&#24133;&#30456;&#21516;&#22823;&#23567;&#22270;&#20687;&#30340;&#31192;&#23494;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#24212;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;ResNet-9&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21253;&#25324;&#65306;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#12289;&#26631;&#31614;&#24179;&#28369;&#12289;&#26799;&#24230;&#23621;&#20013;&#21270;&#12289;&#36755;&#20837;&#22270;&#20687;&#34917;&#19969;&#30333;&#21270;&#20197;&#21450;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21040;10&#20998;&#38047;&#30340;&#26102;&#38388;&#20869;&#65292;ResNet-9&#21487;&#20197;&#22312;&#20165;&#35757;&#32451;CIFAR-10&#25968;&#25454;&#38598;&#30340;10%&#23376;&#38598;&#19978;&#36798;&#21040;88%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our proposed approach that won the first prize at the ICLR competition on Hardware Aware Efficient Training. The challenge is to achieve the highest possible accuracy in an image classification task in less than 10 minutes. The training is done on a small dataset of 5000 images picked randomly from CIFAR-10 dataset. The evaluation is performed by the competition organizers on a secret dataset with 1000 images of the same size. Our approach includes applying a series of technique for improving the generalization of ResNet-9 including: sharpness aware optimization, label smoothing, gradient centralization, input patch whitening as well as metalearning based training. Our experiments show that the ResNet-9 can achieve the accuracy of 88% while trained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#21333;&#26679;&#26412;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;F-TTA&#20013;&#22024;&#26434;&#26679;&#26412;&#23548;&#33268;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03964</link><description>&lt;p&gt;
REALM: &#40065;&#26834;&#30340;&#29109;&#33258;&#36866;&#24212;&#25439;&#22833;&#26368;&#23567;&#21270;&#20197;&#25552;&#39640;&#21333;&#26679;&#26412;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation. (arXiv:2309.03964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#21333;&#26679;&#26412;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;F-TTA&#20013;&#22024;&#26434;&#26679;&#26412;&#23548;&#33268;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;F-TTA&#65289;&#21487;&#20197;&#20943;&#36731;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#65288;1&#65289;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#65288;2&#65289;&#26080;&#38656;&#20102;&#35299;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#22312;&#32447;F-TTA&#20013;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#65288;&#20363;&#22914;&#29109;&#26368;&#23567;&#21270;&#65289;&#26469;&#36866;&#24212;&#20351;&#29992;&#27979;&#35797;&#26679;&#26412;&#27969;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#29109;&#26368;&#23567;&#21270;&#22312;&#32447;&#36827;&#34892;&#36866;&#24212;&#30340;&#27169;&#22411;&#22312;&#21333;&#26679;&#26412;&#35774;&#32622;&#20013;&#19981;&#31283;&#23450;&#65292;&#23548;&#33268;&#36864;&#21270;&#35299;&#65292;&#24182;&#38480;&#21046;&#20102;TTA&#25512;&#29702;&#31574;&#30053;&#30340;&#37319;&#29992;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#30830;&#23450;&#20102;&#22024;&#26434;&#25110;&#19981;&#21487;&#38752;&#30340;&#26679;&#26412;&#26159;&#22312;&#32447;F-TTA&#22833;&#36133;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#24573;&#30053;&#36825;&#20123;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26356;&#26032;&#36807;&#31243;&#20013;&#30340;&#20559;&#24046;&#65292;&#36866;&#24212;&#32531;&#24930;&#21644;&#27867;&#21270;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#21892;F-TTA&#23545;&#36825;&#20123;&#22024;&#26434;&#26679;&#26412;&#40065;&#26834;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21463;&#21040;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully-test-time adaptation (F-TTA) can mitigate performance loss due to distribution shifts between train and test data (1) without access to the training data, and (2) without knowledge of the model training procedure. In online F-TTA, a pre-trained model is adapted using a stream of test samples by minimizing a self-supervised objective, such as entropy minimization. However, models adapted with online using entropy minimization, are unstable especially in single sample settings, leading to degenerate solutions, and limiting the adoption of TTA inference strategies. Prior works identify noisy, or unreliable, samples as a cause of failure in online F-TTA. One solution is to ignore these samples, which can lead to bias in the update procedure, slow adaptation, and poor generalization. In this work, we present a general framework for improving robustness of F-TTA to these noisy samples, inspired by self-paced learning and robust loss functions. Our proposed approach, Robust Entropy Adap
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#26377;&#22768;&#35835;&#29289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#25216;&#26415;&#65292;&#20174;&#22312;&#32447;&#30005;&#23376;&#20070;&#20013;&#21019;&#24314;&#25968;&#21315;&#26412;&#24320;&#25918;&#35768;&#21487;&#30340;&#26377;&#22768;&#35835;&#29289;&#12290;&#35813;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#26391;&#35835;&#36895;&#24230;&#21644;&#39118;&#26684;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#21305;&#37197;&#25152;&#38656;&#22768;&#38899;&#30340;&#21151;&#33021;&#12290;&#35813;&#24037;&#20316;&#36129;&#29486;&#20102;5000&#22810;&#26412;&#26377;&#22768;&#35835;&#29289;&#21644;&#19968;&#20010;&#20132;&#20114;&#24335;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03926</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#33258;&#21160;&#21019;&#24314;&#26377;&#22768;&#35835;&#29289;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Automatic Audiobook Creation. (arXiv:2309.03926v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03926
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#26377;&#22768;&#35835;&#29289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#25216;&#26415;&#65292;&#20174;&#22312;&#32447;&#30005;&#23376;&#20070;&#20013;&#21019;&#24314;&#25968;&#21315;&#26412;&#24320;&#25918;&#35768;&#21487;&#30340;&#26377;&#22768;&#35835;&#29289;&#12290;&#35813;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#26391;&#35835;&#36895;&#24230;&#21644;&#39118;&#26684;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#21305;&#37197;&#25152;&#38656;&#22768;&#38899;&#30340;&#21151;&#33021;&#12290;&#35813;&#24037;&#20316;&#36129;&#29486;&#20102;5000&#22810;&#26412;&#26377;&#22768;&#35835;&#29289;&#21644;&#19968;&#20010;&#20132;&#20114;&#24335;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#22768;&#35835;&#29289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#23398;&#20316;&#21697;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#35835;&#32773;&#21442;&#19982;&#24230;&#12290;&#28982;&#32780;&#65292;&#21046;&#20316;&#12289;&#32534;&#36753;&#21644;&#21457;&#24067;&#26377;&#22768;&#35835;&#29289;&#21487;&#33021;&#38656;&#35201;&#25968;&#30334;&#23567;&#26102;&#30340;&#20154;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20174;&#22312;&#32447;&#30005;&#23376;&#20070;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#26377;&#22768;&#35835;&#29289;&#30340;&#31995;&#32479;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#22312;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20174;&#39033;&#30446;&#21476;&#33150;&#22561;&#30005;&#23376;&#20070;&#25910;&#34255;&#20013;&#21019;&#24314;&#24182;&#21457;&#24067;&#20102;&#25968;&#21315;&#26412;&#39640;&#36136;&#37327;&#12289;&#24320;&#25918;&#35768;&#21487;&#30340;&#26377;&#22768;&#35835;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#21508;&#31181;&#32467;&#26500;&#22810;&#26679;&#30340;&#22270;&#20070;&#65292;&#35782;&#21035;&#20986;&#36866;&#21512;&#26391;&#35835;&#30340;&#30005;&#23376;&#20070;&#20869;&#23481;&#30340;&#21512;&#36866;&#23376;&#38598;&#65292;&#24182;&#21487;&#20197;&#24182;&#34892;&#22788;&#29702;&#25968;&#30334;&#26412;&#20070;&#31821;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#26377;&#22768;&#35835;&#29289;&#30340;&#26391;&#35835;&#36895;&#24230;&#21644;&#39118;&#26684;&#12289;&#24773;&#24863;&#35821;&#35843;&#65292;&#29978;&#33267;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#38899;&#39057;&#26469;&#21305;&#37197;&#25152;&#38656;&#30340;&#22768;&#38899;&#12290;&#36825;&#39033;&#24037;&#20316;&#36129;&#29486;&#20102;5000&#22810;&#26412;&#24320;&#25918;&#35768;&#21487;&#30340;&#26377;&#22768;&#35835;&#29289;&#20197;&#21450;&#19968;&#20010;&#20132;&#20114;&#24335;&#28436;&#31034;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#21019;&#24314;&#33258;&#24049;&#23450;&#21046;&#30340;&#26377;&#22768;&#35835;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
An audiobook can dramatically improve a work of literature's accessibility and improve reader engagement. However, audiobooks can take hundreds of hours of human effort to create, edit, and publish. In this work, we present a system that can automatically generate high-quality audiobooks from online e-books. In particular, we leverage recent advances in neural text-to-speech to create and release thousands of human-quality, open-license audiobooks from the Project Gutenberg e-book collection. Our method can identify the proper subset of e-book content to read for a wide collection of diversely structured books and can operate on hundreds of books in parallel. Our system allows users to customize an audiobook's speaking speed and style, emotional intonation, and can even match a desired voice using a small amount of sample audio. This work contributed over five thousand open-license audiobooks and an interactive demo that allows users to quickly create their own customized audiobooks. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#26469;&#20998;&#26512;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#32454;&#21270;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#29926;&#29255;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21644;&#39044;&#27979;&#24471;&#20998;&#65292;&#20197;&#21450;&#19982;&#32454;&#32990;&#26680;&#20998;&#21106;&#25513;&#27169;&#38598;&#25104;&#30340;&#39044;&#27979;-&#27880;&#24847;&#21147;&#21152;&#26435;&#65288;PAW&#65289;&#22320;&#22270;&#65292;&#23454;&#29616;&#20102;MIL&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2309.03925</link><description>&lt;p&gt;
&#36229;&#36234;&#27880;&#24847;&#21147;&#65306;&#20174;&#24369;&#30417;&#30563;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#20013;&#24471;&#20986;&#21487;&#35299;&#37322;&#30340;&#29983;&#29289;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Beyond attention: deriving biologically interpretable insights from weakly-supervised multiple-instance learning models. (arXiv:2309.03925v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#26469;&#20998;&#26512;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#32454;&#21270;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#29926;&#29255;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21644;&#39044;&#27979;&#24471;&#20998;&#65292;&#20197;&#21450;&#19982;&#32454;&#32990;&#26680;&#20998;&#21106;&#25513;&#27169;&#38598;&#25104;&#30340;&#39044;&#27979;-&#27880;&#24847;&#21147;&#21152;&#26435;&#65288;PAW&#65289;&#22320;&#22270;&#65292;&#23454;&#29616;&#20102;MIL&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#30340;&#36827;&#23637;&#25552;&#39640;&#20102;&#25105;&#20204;&#23545;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#27169;&#22411;&#20381;&#36182;&#30340;&#32452;&#32455;&#21306;&#22495;&#36827;&#34892;&#39044;&#27979;&#30340;&#27934;&#23519;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#20204;&#27809;&#26377;&#25253;&#21578;&#39640;&#27880;&#24847;&#21147;&#21306;&#22495;&#19982;&#31867;&#26631;&#31614;&#30340;&#27491;&#21521;&#25110;&#36127;&#21521;&#20851;&#32852;&#65292;&#20197;&#21450;&#36825;&#20123;&#21306;&#22495;&#19982;&#20808;&#21069;&#24314;&#31435;&#30340;&#20020;&#24202;&#21644;&#29983;&#29289;&#23398;&#30693;&#35782;&#30340;&#23545;&#24212;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#26469;&#20998;&#26512;MIL&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#32454;&#21270;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#29926;&#29255;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21644;&#39044;&#27979;&#24471;&#20998;&#65292;&#24341;&#20837;&#20102;&#39044;&#27979;-&#27880;&#24847;&#21147;&#21152;&#26435;&#65288;PAW&#65289;&#22320;&#22270;&#65292;&#20174;&#32780;&#33021;&#22815;&#37327;&#21270;&#39640;&#27880;&#24847;&#21147;&#21306;&#22495;&#30340;&#39044;&#27979;&#36129;&#29486;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;PAW&#22320;&#22270;&#19982;&#32454;&#32990;&#26680;&#20998;&#21106;&#25513;&#27169;&#38598;&#25104;&#36215;&#26469;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#29983;&#29289;&#29305;&#24449;&#23454;&#20363;&#21270;&#25216;&#26415;&#12290;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20379;&#20102;&#19982;&#29983;&#29289;&#24847;&#20041;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in attention-based multiple instance learning (MIL) have improved our insights into the tissue regions that models rely on to make predictions in digital pathology. However, the interpretability of these approaches is still limited. In particular, they do not report whether high-attention regions are positively or negatively associated with the class labels or how well these regions correspond to previously established clinical and biological knowledge. We address this by introducing a post-training methodology to analyse MIL models. Firstly, we introduce prediction-attention-weighted (PAW) maps by combining tile-level attention and prediction scores produced by a refined encoder, allowing us to quantify the predictive contribution of high-attention regions. Secondly, we introduce a biological feature instantiation technique by integrating PAW maps with nuclei segmentation masks. This further improves interpretability by providing biologically meaningful features relate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#30340;&#37327;&#23376;&#26550;&#26500;&#23558;3D&#21644;&#31354;&#38388;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#20114;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03919</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#20197;&#25552;&#39640;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
A hybrid quantum-classical fusion neural network to improve protein-ligand binding affinity predictions for drug discovery. (arXiv:2309.03919v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#30340;&#37327;&#23376;&#26550;&#26500;&#23558;3D&#21644;&#31354;&#38388;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#20114;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#20851;&#38190;&#22312;&#20110;&#20934;&#30830;&#39044;&#27979;&#28508;&#22312;&#33647;&#29289;&#20998;&#23376;&#19982;&#38774;&#34507;&#30333;&#20043;&#38388;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#34507;&#30333;&#30452;&#25509;&#24433;&#21709;&#30142;&#30149;&#30340;&#36827;&#23637;&#26102;&#12290;&#28982;&#32780;&#65292;&#20272;&#35745;&#32467;&#21512;&#20146;&#21644;&#21147;&#38656;&#35201;&#26174;&#33879;&#30340;&#36130;&#21153;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26032;&#20852;&#30340;&#28151;&#21512;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#24402;&#21151;&#20110;&#23427;&#20204;&#22266;&#26377;&#30340;&#24182;&#34892;&#24615;&#21644;&#31649;&#29702;&#25968;&#25454;&#32500;&#24230;&#25351;&#25968;&#32423;&#22686;&#21152;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#36827;&#23637;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#25910;&#25947;&#31283;&#23450;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of drug discovery hinges on the accurate prediction of binding affinity between prospective drug molecules and target proteins, especially when such proteins directly influence disease progression. However, estimating binding affinity demands significant financial and computational resources. While state-of-the-art methodologies employ classical machine learning (ML) techniques, emerging hybrid quantum machine learning (QML) models have shown promise for enhanced performance, owing to their inherent parallelism and capacity to manage exponential increases in data dimensionality. Despite these advances, existing models encounter issues related to convergence stability and prediction accuracy. This paper introduces a novel hybrid quantum-classical deep learning model tailored for binding affinity prediction in drug discovery. Specifically, the proposed model synergistically integrates 3D and spatial graph convolutional neural networks within an optimized quantum architecture. S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#31649;&#29702;&#36827;&#34892;SCS&#30340;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#12290;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26041;&#27861;&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#20026;&#24739;&#32773;&#25512;&#33616;SCS&#35774;&#32622;&#65292;&#26088;&#22312;&#25913;&#21892;&#20854;&#29366;&#20917;&#12290;&#36825;&#20123;&#25512;&#33616;&#36890;&#36807;&#25968;&#23383;&#20581;&#24247;&#29983;&#24577;&#31995;&#32479;&#30452;&#25509;&#21457;&#36865;&#32473;&#24739;&#32773;&#65292;&#24182;&#19982;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#32467;&#21512;&#65292;&#20026;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#30340;&#25972;&#20010;&#27835;&#30103;&#36807;&#31243;&#25552;&#20379;&#38381;&#29615;&#20851;&#24576;&#12290;</title><link>http://arxiv.org/abs/2309.03918</link><description>&lt;p&gt;
&#31649;&#29702;&#33034;&#39635;&#21050;&#28608;&#26415;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A recommender for the management of chronic pain in patients undergoing spinal cord stimulation. (arXiv:2309.03918v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#31649;&#29702;&#36827;&#34892;SCS&#30340;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#12290;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26041;&#27861;&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#20026;&#24739;&#32773;&#25512;&#33616;SCS&#35774;&#32622;&#65292;&#26088;&#22312;&#25913;&#21892;&#20854;&#29366;&#20917;&#12290;&#36825;&#20123;&#25512;&#33616;&#36890;&#36807;&#25968;&#23383;&#20581;&#24247;&#29983;&#24577;&#31995;&#32479;&#30452;&#25509;&#21457;&#36865;&#32473;&#24739;&#32773;&#65292;&#24182;&#19982;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#32467;&#21512;&#65292;&#20026;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#30340;&#25972;&#20010;&#27835;&#30103;&#36807;&#31243;&#25552;&#20379;&#38381;&#29615;&#20851;&#24576;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33034;&#39635;&#21050;&#28608;&#26415;&#65288;Spinal cord stimulation&#65292;SCS&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#27835;&#30103;&#24930;&#24615;&#30140;&#30171;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#20854;&#36890;&#36807;&#26893;&#20837;&#35013;&#32622;&#21521;&#33034;&#39635;&#20256;&#36882;&#30005;&#33033;&#20914;&#65292;&#22312;&#32473;&#20104;&#36866;&#24403;&#30340;&#21050;&#28608;&#21442;&#25968;&#26102;&#65292;&#21487;&#20197;&#25513;&#30422;&#25110;&#38459;&#26029;&#30140;&#30171;&#20449;&#21495;&#12290;&#20248;&#21270;&#21050;&#28608;&#21442;&#25968;&#30340;&#36873;&#25321;&#36890;&#24120;&#22312;&#20020;&#24202;&#30001;&#21307;&#29983;&#36127;&#36131;&#65292;&#32780;&#22312;&#23478;&#24237;&#20013;&#30340;SCS&#20248;&#21270;&#21017;&#30001;&#24739;&#32773;&#33258;&#24049;&#31649;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31649;&#29702;&#36827;&#34892;SCS&#30340;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#65288;CMAB&#65289;&#26041;&#27861;&#26469;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#20026;&#24739;&#32773;&#25512;&#33616;SCS&#35774;&#32622;&#65292;&#26088;&#22312;&#25913;&#21892;&#20854;&#29366;&#20917;&#12290;&#36825;&#20123;&#25512;&#33616;&#36890;&#36807;&#25968;&#23383;&#20581;&#24247;&#29983;&#24577;&#31995;&#32479;&#30452;&#25509;&#21457;&#36865;&#32473;&#24739;&#32773;&#65292;&#24182;&#19982;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#32467;&#21512;&#65292;&#20026;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#30340;&#25972;&#20010;&#27835;&#30103;&#36807;&#31243;&#25552;&#20379;&#38381;&#29615;&#20851;&#24576;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#25509;&#21463;SCS&#26893;&#20837;&#30340;ENVISION&#24739;&#32773;&#20013;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spinal cord stimulation (SCS) is a therapeutic approach used for the management of chronic pain. It involves the delivery of electrical impulses to the spinal cord via an implanted device, which when given suitable stimulus parameters can mask or block pain signals. Selection of optimal stimulation parameters usually happens in the clinic under the care of a provider whereas at-home SCS optimization is managed by the patient. In this paper, we propose a recommender system for the management of pain in chronic pain patients undergoing SCS. In particular, we use a contextual multi-armed bandit (CMAB) approach to develop a system that recommends SCS settings to patients with the aim of improving their condition. These recommendations, sent directly to patients though a digital health ecosystem, combined with a patient monitoring system closes the therapeutic loop around a chronic pain patient over their entire patient journey. We evaluated the system in a cohort of SCS-implanted ENVISION 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#32431;&#36793;&#32536;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#32534;&#25490;&#65288;R-AdWOrch&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20248;&#20808;&#32423;&#23450;&#20041;&#21644;&#37325;&#26032;&#20998;&#37197;&#31574;&#30053;&#65292;&#26368;&#23567;&#21270;&#25130;&#27490;&#26102;&#38388;&#30340;&#26410;&#36798;&#25104;&#20197;&#21450;&#20302;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#25968;&#25454;&#20002;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;R-AdWOrch&#33021;&#22815;&#22312;&#25152;&#26377;&#26465;&#20214;&#19979;&#23613;&#37327;&#20943;&#23569;&#32039;&#24613;&#20219;&#21153;&#30340;&#25130;&#27490;&#26102;&#38388;&#26410;&#36798;&#25104;&#21644;&#20302;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#25968;&#25454;&#20002;&#22833;&#12290;</title><link>http://arxiv.org/abs/2309.03913</link><description>&lt;p&gt;
&#19968;&#31181;&#31283;&#20581;&#30340;&#32431;&#36793;&#32536;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
A Robust Adaptive Workload Orchestration in Pure Edge Computing. (arXiv:2309.03913v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#32431;&#36793;&#32536;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#32534;&#25490;&#65288;R-AdWOrch&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20248;&#20808;&#32423;&#23450;&#20041;&#21644;&#37325;&#26032;&#20998;&#37197;&#31574;&#30053;&#65292;&#26368;&#23567;&#21270;&#25130;&#27490;&#26102;&#38388;&#30340;&#26410;&#36798;&#25104;&#20197;&#21450;&#20302;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#25968;&#25454;&#20002;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;R-AdWOrch&#33021;&#22815;&#22312;&#25152;&#26377;&#26465;&#20214;&#19979;&#23613;&#37327;&#20943;&#23569;&#32039;&#24613;&#20219;&#21153;&#30340;&#25130;&#27490;&#26102;&#38388;&#26410;&#36798;&#25104;&#21644;&#20302;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#25968;&#25454;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32431;&#36793;&#32536;&#35745;&#31639;&#26088;&#22312;&#23558;&#20113;&#24212;&#29992;&#21644;&#26381;&#21153;&#24102;&#21040;&#32593;&#32476;&#36793;&#32536;&#65292;&#20197;&#25903;&#25345;&#19981;&#26029;&#22686;&#38271;&#30340;&#29992;&#25143;&#23545;&#26102;&#24577;&#24212;&#29992;&#21644;&#25968;&#25454;&#39537;&#21160;&#35745;&#31639;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#35774;&#22791;&#30340;&#31227;&#21160;&#24615;&#21644;&#26377;&#38480;&#30340;&#35745;&#31639;&#33021;&#21147;&#23545;&#20110;&#25903;&#25345;&#19968;&#20123;&#32039;&#24613;&#21644;&#35745;&#31639;&#23494;&#38598;&#22411;&#20219;&#21153;&#19988;&#20855;&#26377;&#20005;&#26684;&#21709;&#24212;&#26102;&#38388;&#35201;&#27714;&#30340;&#20219;&#21153;&#24102;&#26469;&#25361;&#25112;&#12290;&#22914;&#26524;&#36825;&#20123;&#20219;&#21153;&#30340;&#25191;&#34892;&#32467;&#26524;&#36229;&#36807;&#20102;&#25130;&#27490;&#26102;&#38388;&#65292;&#23427;&#20204;&#23601;&#21464;&#24471;&#27627;&#26080;&#20215;&#20540;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#36793;&#32536;&#33410;&#28857;&#23613;&#21487;&#33021;&#23436;&#25104;&#23613;&#21487;&#33021;&#22810;&#30340;&#20302;&#24310;&#36831;&#20219;&#21153;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pure Edge computing (PEC) aims to bring cloud applications and services to the edge of the network to support the growing user demand for time-sensitive applications and data-driven computing. However, mobility and limited computational capacity of edge devices pose challenges in supporting some urgent and computationally intensive tasks with strict response time demands. If the execution results of these tasks exceed the deadline, they become worthless and can cause severe safety issues. Therefore, it is essential to ensure that edge nodes complete as many latency-sensitive tasks as possible. \\In this paper, we propose a Robust Adaptive Workload Orchestration (R-AdWOrch) model to minimize deadline misses and data loss by using priority definition and a reallocation strategy. The results show that R-AdWOrch can minimize deadline misses of urgent tasks while minimizing the data loss of lower priority tasks under all conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;DrugChat&#30340;&#33647;&#29289;&#20998;&#23376;&#22270;&#32842;&#22825;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23454;&#29616;&#20102;&#31867;&#20284;ChatGPT&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#19978;&#20256;&#21270;&#21512;&#29289;&#20998;&#23376;&#22270;&#24182;&#36827;&#34892;&#22810;&#36718;&#20114;&#21160;&#65292;&#29992;&#25143;&#21487;&#20197;&#21521;DrugChat&#25552;&#38382;&#24182;&#33719;&#24471;&#22238;&#31572;&#65292;&#36825;&#21487;&#20197;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#12289;&#20248;&#21270;&#32467;&#26500;-&#27963;&#24615;&#20851;&#31995;&#12289;&#25351;&#23548;&#21069;&#23548;&#21270;&#21512;&#29289;&#20248;&#21270;&#31561;&#12290;&#31995;&#32479;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#32452;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.03907</link><description>&lt;p&gt;
DrugChat&#65306;&#23454;&#29616;&#32842;&#22825;GPT&#26679;&#33021;&#21147;&#20110;&#33647;&#29289;&#20998;&#23376;&#22270;&#19978;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs. (arXiv:2309.03907v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;DrugChat&#30340;&#33647;&#29289;&#20998;&#23376;&#22270;&#32842;&#22825;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23454;&#29616;&#20102;&#31867;&#20284;ChatGPT&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#19978;&#20256;&#21270;&#21512;&#29289;&#20998;&#23376;&#22270;&#24182;&#36827;&#34892;&#22810;&#36718;&#20114;&#21160;&#65292;&#29992;&#25143;&#21487;&#20197;&#21521;DrugChat&#25552;&#38382;&#24182;&#33719;&#24471;&#22238;&#31572;&#65292;&#36825;&#21487;&#20197;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#12289;&#20248;&#21270;&#32467;&#26500;-&#27963;&#24615;&#20851;&#31995;&#12289;&#25351;&#23548;&#21069;&#23548;&#21270;&#21512;&#29289;&#20248;&#21270;&#31561;&#12290;&#31995;&#32479;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#30740;&#31350;&#20013;&#65292;&#19968;&#31181;&#31867;&#20284;ChatGPT&#30340;&#33647;&#29289;&#21270;&#21512;&#29289;&#31995;&#32479;&#21487;&#33021;&#26159;&#19968;&#20010;&#25913;&#21464;&#28216;&#25103;&#35268;&#21017;&#30340;&#22240;&#32032;&#65292;&#21152;&#24555;&#33647;&#29289;&#21457;&#29616;&#65292;&#22686;&#24378;&#25105;&#20204;&#23545;&#32467;&#26500;-&#27963;&#24615;&#20851;&#31995;&#30340;&#29702;&#35299;&#65292;&#25351;&#23548;&#21069;&#23548;&#21270;&#21512;&#29289;&#20248;&#21270;&#65292;&#24110;&#21161;&#33647;&#29289;&#20877;&#21033;&#29992;&#65292;&#20943;&#23569;&#22833;&#36133;&#29575;&#24182;&#31616;&#21270;&#20020;&#24202;&#35797;&#39564;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21517;&#20026;DrugChat&#30340;&#21407;&#22411;&#31995;&#32479;&#65292;&#21021;&#27493;&#23581;&#35797;&#22312;&#33647;&#29289;&#20998;&#23376;&#22270;&#19978;&#23454;&#29616;&#31867;&#20284;ChatGPT&#30340;&#33021;&#21147;&#12290;DrugChat&#30340;&#24037;&#20316;&#26041;&#24335;&#31867;&#20284;&#20110;ChatGPT&#12290;&#29992;&#25143;&#19978;&#20256;&#19968;&#20010;&#21270;&#21512;&#29289;&#20998;&#23376;&#22270;&#24182;&#25552;&#20986;&#21508;&#31181;&#20851;&#20110;&#35813;&#21270;&#21512;&#29289;&#30340;&#38382;&#39064;&#12290;DrugChat&#23558;&#20197;&#22810;&#36718;&#20114;&#21160;&#30340;&#26041;&#24335;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;DrugChat&#31995;&#32479;&#30001;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12289;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#19968;&#20010;&#36866;&#37197;&#22120;&#32452;&#25104;&#12290;GNN&#23558;&#21270;&#21512;&#29289;&#20998;&#23376;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#23398;&#20064;&#35813;&#22270;&#30340;&#34920;&#31034;&#12290;&#36866;&#37197;&#22120;&#23558;GNN&#29983;&#25104;&#30340;&#22270;&#34920;&#31034;&#36716;&#25442;&#20026;LLM&#21487;&#25509;&#21463;&#30340;&#21478;&#19968;&#20010;&#34920;&#31034;&#12290;LLM&#25509;&#25910;&#35813;&#34920;&#31034;&#24182;&#22238;&#31572;&#29992;&#25143;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A ChatGPT-like system for drug compounds could be a game-changer in pharmaceutical research, accelerating drug discovery, enhancing our understanding of structure-activity relationships, guiding lead optimization, aiding drug repurposing, reducing the failure rate, and streamlining clinical trials. In this work, we make an initial attempt towards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a prototype system DrugChat. DrugChat works in a similar way as ChatGPT. Users upload a compound molecule graph and ask various questions about this compound. DrugChat will answer these questions in a multi-turn, interactive manner. The DrugChat system consists of a graph neural network (GNN), a large language model (LLM), and an adaptor. The GNN takes a compound molecule graph as input and learns a representation for this graph. The adaptor transforms the graph representation produced by the GNN into another representation that is acceptable to the LLM. The LLM takes th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#37096;&#20998;&#39046;&#22495;&#36866;&#24212;&#65288;PDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#40065;&#26834;&#30340;&#30446;&#26631;&#30417;&#30563;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#39046;&#22495;&#26080;&#20851;&#30340;&#26041;&#24335;&#19979;&#20248;&#21270;&#20102;&#31867;&#20869;&#32039;&#20945;&#24615;&#21644;&#31867;&#38388;&#20998;&#31163;&#24615;&#12290;&#36890;&#36807;&#39044;&#20808;&#25512;&#26029;&#28304;&#21407;&#22411;&#65292;&#30830;&#20445;&#20102;&#28304;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03531</link><description>&lt;p&gt;
&#20351;&#29992;&#28304;&#21407;&#22411;&#30340;&#24378;&#22823;&#36127;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#37096;&#20998;&#39046;&#22495;&#36866;&#24212;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes. (arXiv:2309.03531v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#37096;&#20998;&#39046;&#22495;&#36866;&#24212;&#65288;PDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#40065;&#26834;&#30340;&#30446;&#26631;&#30417;&#30563;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#39046;&#22495;&#26080;&#20851;&#30340;&#26041;&#24335;&#19979;&#20248;&#21270;&#20102;&#31867;&#20869;&#32039;&#20945;&#24615;&#21644;&#31867;&#38388;&#20998;&#31163;&#24615;&#12290;&#36890;&#36807;&#39044;&#20808;&#25512;&#26029;&#28304;&#21407;&#22411;&#65292;&#30830;&#20445;&#20102;&#28304;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#37096;&#20998;&#39046;&#22495;&#36866;&#24212;&#65288;PDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#40065;&#26834;&#30340;&#30446;&#26631;&#30417;&#30563;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#38598;&#25104;&#23398;&#20064;&#21644;&#21253;&#21547;&#22810;&#26679;&#21270;&#12289;&#20114;&#34917;&#30340;&#26631;&#31614;&#21453;&#39304;&#65292;&#20943;&#36731;&#20102;&#38169;&#35823;&#21453;&#39304;&#30340;&#24433;&#21709;&#65292;&#24182;&#20419;&#36827;&#20266;&#26631;&#31614;&#30340;&#25913;&#36827;&#12290;&#19982;&#20165;&#20381;&#36182;&#20998;&#24067;&#23545;&#40784;&#30340;&#19968;&#38454;&#30697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#28304;&#21407;&#22411;&#21644;&#39640;&#21487;&#20449;&#30340;&#30446;&#26631;&#26679;&#26412;&#65292;&#22312;&#39046;&#22495;&#26080;&#20851;&#30340;&#26041;&#24335;&#19979;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#31867;&#20869;&#32039;&#20945;&#24615;&#21644;&#31867;&#38388;&#20998;&#31163;&#24615;&#30340;&#26126;&#30830;&#30446;&#26631;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#39044;&#20808;&#25512;&#26029;&#28304;&#21407;&#22411;&#65292;&#30830;&#20445;&#20102;&#28304;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#65292;&#22312;&#36866;&#24212;&#38454;&#27573;&#26080;&#38656;&#35775;&#38382;&#28304;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#28040;&#34701;&#20998;&#26512;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#37096;&#20998;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20840;&#38754;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a robust Partial Domain Adaptation (PDA) framework that mitigates the negative transfer problem by incorporating a robust target-supervision strategy. It leverages ensemble learning and includes diverse, complementary label feedback, alleviating the effect of incorrect feedback and promoting pseudo-label refinement. Rather than relying exclusively on first-order moments for distribution alignment, our approach offers explicit objectives to optimize intra-class compactness and inter-class separation with the inferred source prototypes and highly-confident target samples in a domain-invariant fashion. Notably, we ensure source data privacy by eliminating the need to access the source data during the adaptation phase through a priori inference of source prototypes. We conducted a series of comprehensive experiments, including an ablation analysis, covering a range of partial domain adaptation tasks. Comprehensive evaluations on benchmark datasets corroborate our framewo
&lt;/p&gt;</description></item><item><title>R2D2&#26159;&#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#20013;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#65292;&#37319;&#29992;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#26356;&#26032;&#65292;&#37325;&#24314;&#20026;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#24378;&#24230;&#25104;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.03291</link><description>&lt;p&gt;
R2D2: &#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#36817;&#23454;&#26102;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
R2D2: Deep neural network series for near real-time high-dynamic range imaging in radio astronomy. (arXiv:2309.03291v1 [astro-ph.IM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03291
&lt;/p&gt;
&lt;p&gt;
R2D2&#26159;&#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#20013;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#65292;&#37319;&#29992;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#26356;&#26032;&#65292;&#37325;&#24314;&#20026;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#24378;&#24230;&#25104;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#23556;&#30005;&#24178;&#28041;&#27979;&#37327;&#65288;RI&#65289;&#22312;&#22825;&#25991;&#23398;&#20013;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#21512;&#25104;&#25104;&#20687;&#12290;R2D2&#20195;&#34920;&#8220;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#27531;&#24046;&#21040;&#27531;&#24046;DNN&#31995;&#21015;&#8221;&#65292;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#28151;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#26356;&#26032;&#12290;&#23427;&#30340;&#37325;&#24314;&#26159;&#30001;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#32452;&#25104;&#30340;&#65292;&#36825;&#20123;&#27531;&#24046;&#22270;&#20687;&#34987;&#20272;&#35745;&#20026;DNN&#30340;&#36755;&#20986;&#65292;&#27599;&#20010;DNN&#37117;&#20197;&#19978;&#19968;&#27425;&#36845;&#20195;&#30340;&#27531;&#24046;&#33039;&#22270;&#29255;&#20316;&#20026;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#20026;&#21305;&#37197;&#36861;&#36394;&#26041;&#27861;&#30340;&#23398;&#20064;&#29256;&#26412;&#65292;&#20854;&#20013;&#27169;&#22411;&#32452;&#20214;&#20174;&#27531;&#24046;&#33039;&#22270;&#29255;&#20013;&#36845;&#20195;&#22320;&#35782;&#21035;&#20986;&#26469;&#65292;CLEAN&#23601;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;R2D2&#27169;&#22411;&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#20998;&#21035;&#22522;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;DNN&#26550;&#26500;&#65306;&#26631;&#20934;&#30340;U-Net&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#23637;&#24320;&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;S&#27874;&#27573;&#23545;&#23556;&#30005;&#26143;&#31995;Cygnus~A&#30340;&#39640;&#28789;&#25935;&#24230;&#35266;&#27979;&#20013;&#29992;&#20110;&#21333;&#33394;&#24378;&#24230;&#25104;&#20687;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel AI approach for high-resolution high-dynamic range synthesis imaging by radio interferometry (RI) in astronomy. R2D2, standing for "{R}esidual-to-{R}esidual {D}NN series for high-{D}ynamic range imaging", is a model-based data-driven approach relying on hybrid deep neural networks (DNNs) and data-consistency updates. Its reconstruction is built as a series of residual images estimated as the outputs of DNNs, each taking the residual dirty image of the previous iteration as an input. The approach can be interpreted as a learned version of a matching pursuit approach, whereby model components are iteratively identified from residual dirty images, and of which CLEAN is a well-known example. We propose two variants of the R2D2 model, built upon two distinctive DNN architectures: a standard U-Net, and a novel unrolled architecture. We demonstrate their use for monochromatic intensity imaging on highly-sensitive observations of the radio galaxy Cygnus~A at S band, from the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#20805;&#30005;&#26469;&#35299;&#20915;&#26080;&#20154;&#26426;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#22320;&#22270;&#30340;&#35266;&#27979;&#20449;&#24687;&#65292;&#22312;&#25972;&#20010;&#20219;&#21153;&#21608;&#26399;&#20869;&#20248;&#21270;&#35206;&#30422;&#36712;&#36857;&#65292;&#24182;&#37319;&#29992;&#21160;&#20316;&#23631;&#34109;&#21644;&#25240;&#25187;&#22240;&#23376;&#35843;&#24230;&#31561;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#30446;&#26631;&#21306;&#22495;&#21644;&#22320;&#22270;&#19978;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03157</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#20805;&#30005;&#65306;&#26080;&#20154;&#26426;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning. (arXiv:2309.03157v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#20805;&#30005;&#26469;&#35299;&#20915;&#26080;&#20154;&#26426;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#22320;&#22270;&#30340;&#35266;&#27979;&#20449;&#24687;&#65292;&#22312;&#25972;&#20010;&#20219;&#21153;&#21608;&#26399;&#20869;&#20248;&#21270;&#35206;&#30422;&#36712;&#36857;&#65292;&#24182;&#37319;&#29992;&#21160;&#20316;&#23631;&#34109;&#21644;&#25240;&#25187;&#22240;&#23376;&#35843;&#24230;&#31561;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#30446;&#26631;&#21306;&#22495;&#21644;&#22320;&#22270;&#19978;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#65288;CPP&#65289;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#26377;&#25928;&#30340;&#36335;&#24452;&#65292;&#35206;&#30422;&#20852;&#36259;&#21306;&#22495;&#20013;&#30340;&#27599;&#19968;&#20010;&#28857;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20805;&#30005;&#26377;&#38480;&#30340;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#30340;&#30005;&#21147;&#38480;&#21046;CPP&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#23558;&#20805;&#30005;&#26053;&#31243;&#25972;&#21512;&#21040;&#25972;&#20307;&#35206;&#30422;&#31574;&#30053;&#20013;&#24102;&#26469;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#31361;&#20986;&#20102;&#21046;&#23450;&#25112;&#30053;&#24615;&#12289;&#38271;&#26399;&#24615;&#20915;&#31574;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#22320;&#22270;&#30340;&#35266;&#27979;&#20449;&#24687;&#65292;&#36816;&#29992;&#21160;&#20316;&#23631;&#34109;&#21644;&#25240;&#25187;&#22240;&#23376;&#35843;&#24230;&#26469;&#20248;&#21270;&#25972;&#20010;&#20219;&#21153;&#21608;&#26399;&#20869;&#30340;&#35206;&#30422;&#36712;&#36857;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20301;&#32622;&#21382;&#21490;&#35760;&#24405;&#32473;&#26234;&#33021;&#20307;&#65292;&#20197;&#22788;&#29702;&#20805;&#30005;&#33021;&#21147;&#24341;&#36215;&#30340;&#26032;&#20986;&#29616;&#30340;&#29366;&#24577;&#24490;&#29615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#30446;&#26631;&#21306;&#22495;&#21644;&#22320;&#22270;&#19978;&#20855;&#26377;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#23545;&#26410;&#30693;&#22320;&#22270;&#30340;&#27867;&#21270;&#24615;&#33021;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coverage path planning (CPP) is a critical problem in robotics, where the goal is to find an efficient path that covers every point in an area of interest. This work addresses the power-constrained CPP problem with recharge for battery-limited unmanned aerial vehicles (UAVs). In this problem, a notable challenge emerges from integrating recharge journeys into the overall coverage strategy, highlighting the intricate task of making strategic, long-term decisions. We propose a novel proximal policy optimization (PPO)-based deep reinforcement learning (DRL) approach with map-based observations, utilizing action masking and discount factor scheduling to optimize coverage trajectories over the entire mission horizon. We further provide the agent with a position history to handle emergent state loops caused by the recharge capability. Our approach outperforms a baseline heuristic, generalizes to different target zones and maps, with limited generalization to unseen maps. We offer valuable in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24674;&#22797;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#36890;&#36807;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#33719;&#21462;&#21160;&#24577;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.02571</link><description>&lt;p&gt;
&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#24674;&#22797;&#65306;&#22522;&#20110;FFT&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach. (arXiv:2309.02571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24674;&#22797;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#36890;&#36807;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#33719;&#21462;&#21160;&#24577;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#25928;&#24212;&#26159;&#31185;&#23398;&#20013;&#19968;&#20010;&#22522;&#30784;&#19988;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24403;&#22240;&#26524;&#20851;&#31995;&#26159;&#38745;&#24577;&#30340;&#26102;&#20505;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#36328;&#26102;&#38388;&#28857;&#23454;&#20307;&#20043;&#38388;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#21160;&#24577;&#22240;&#26524;&#25928;&#24212;&#30340;&#35782;&#21035;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#19982;&#38745;&#24577;&#24773;&#20917;&#30456;&#27604;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#35266;&#27979;&#20013;&#33719;&#21462;&#21160;&#24577;&#22240;&#26524;&#25928;&#24212;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#21521;&#37327;&#33258;&#22238;&#24402; (VAR) &#27169;&#22411;&#24674;&#22797;&#22240;&#26524;&#32467;&#26500;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026; $O(Tn^3N^2)$&#65292;&#20854;&#20013; $n$ &#26159;&#33410;&#28857;&#25968;&#65292;$T$ &#26159;&#26679;&#26412;&#25968;&#65292;$N$ &#26159;&#23454;&#20307;&#20043;&#38388;&#30340;&#26368;&#22823;&#26102;&#38388;&#28382;&#21518;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#20026; $O(Tn^3\log N)$ &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24674;&#22797;&#22240;&#26524;&#32467;&#26500;&#20197;&#33719;&#24471;&#39057;&#22495; (FD) &#34920;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#30001;&#20110;FFT&#23558;&#25152;&#26377;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#31215;&#32047;&#22312;&#27599;&#20010;&#39057;&#29575;&#19978;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning causal effects from data is a fundamental and well-studied problem across science, especially when the cause-effect relationship is static in nature. However, causal effect is less explored when there are dynamical dependencies, i.e., when dependencies exist between entities across time. Identifying dynamic causal effects from time-series observations is computationally expensive when compared to the static scenario. We demonstrate that the computational complexity of recovering the causation structure for the vector auto-regressive (VAR) model is $O(Tn^3N^2)$, where $n$ is the number of nodes, $T$ is the number of samples, and $N$ is the largest time-lag in the dependency between entities. We report a method, with a reduced complexity of $O(Tn^3 \log N)$, to recover the causation structure to obtain frequency-domain (FD) representations of time-series. Since FFT accumulates all the time dependencies on every frequency, causal inference can be performed efficiently by consider
&lt;/p&gt;</description></item><item><title>TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.02094</link><description>&lt;p&gt;
TensorBank: &#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#29992;&#20110;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TensorBank:Tensor Lakehouse for Foundation Model Training. (arXiv:2309.02094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02094
&lt;/p&gt;
&lt;p&gt;
TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#20043;&#22806;&#30340;&#39046;&#22495;&#30340;&#20852;&#36215;&#65292;&#23384;&#20648;&#21644;&#27969;&#24335;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#25104;&#20026;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TensorBank&#65292;&#19968;&#20010;&#33021;&#22815;&#22522;&#20110;&#22797;&#26434;&#20851;&#31995;&#26597;&#35810;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#65288;COS&#65289;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#30340;&#30334;&#20159;&#32423;&#24352;&#37327;&#28246;&#20179;&#24211;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#65288;HSI&#65289;&#26469;&#21152;&#36895;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#20801;&#35768;&#20351;&#29992;HTTP&#33539;&#22260;&#35835;&#21462;&#26469;&#30452;&#25509;&#35775;&#38382;&#22359;&#32423;&#21035;&#30340;&#24352;&#37327;&#12290;&#19968;&#26086;&#22312;GPU&#20869;&#23384;&#20013;&#65292;&#25968;&#25454;&#21487;&#20197;&#20351;&#29992;PyTorch&#36716;&#25442;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;PyTorch&#25968;&#25454;&#38598;&#31867;&#22411;&#65292;&#37197;&#26377;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#24037;&#21378;&#65292;&#29992;&#20110;&#23558;&#20851;&#31995;&#26597;&#35810;&#21644;&#35831;&#27714;&#30340;&#36716;&#25442;&#20316;&#20026;&#19968;&#20010;&#23454;&#20363;&#36827;&#34892;&#32763;&#35793;&#12290;&#36890;&#36807;&#20351;&#29992;HSI&#65292;&#21487;&#20197;&#36339;&#36807;&#19981;&#30456;&#20851;&#30340;&#22359;&#65292;&#32780;&#26080;&#38656;&#35835;&#21462;&#23427;&#20204;&#65292;&#22240;&#20026;&#36825;&#20123;&#32034;&#24341;&#21253;&#21547;&#19981;&#21516;&#23618;&#27425;&#20998;&#36776;&#29575;&#32423;&#21035;&#19978;&#20869;&#23481;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#26631;&#20934;&#30340;&#26377;&#20027;&#35266;&#35266;&#28857;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Storing and streaming high dimensional data for foundation model training became a critical requirement with the rise of foundation models beyond natural language. In this paper we introduce TensorBank, a petabyte scale tensor lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU memory at wire speed based on complex relational queries. We use Hierarchical Statistical Indices (HSI) for query acceleration. Our architecture allows to directly address tensors on block level using HTTP range reads. Once in GPU memory, data can be transformed using PyTorch transforms. We provide a generic PyTorch dataset type with a corresponding dataset factory translating relational queries and requested transformations as an instance. By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels. This is an opinionated architecture powered by open standards and making h
&lt;/p&gt;</description></item><item><title>LoopTune&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#24352;&#37327;&#35745;&#31639;&#30340;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#20248;&#21270;&#24352;&#37327;&#36941;&#21382;&#39034;&#24207;&#21644;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#22120;LoopNest&#25191;&#34892;&#30828;&#20214;&#29305;&#23450;&#20248;&#21270;&#65292;LoopTune&#33021;&#22815;&#29983;&#25104;&#27604;&#20854;&#20182;&#32534;&#35793;&#22120;&#26356;&#24555;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#37319;&#29992;&#26032;&#30340;&#22270;&#24418;&#34920;&#31034;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;LoopTune&#27604;TVM&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#27604;MetaSchedule&#24555;2.8&#20493;&#65292;&#27604;AutoTVM&#24555;1.08&#20493;&#65292;&#24182;&#25345;&#32493;&#22312;&#19982;&#25163;&#24037;&#35843;&#20248;&#30340;&#24211;Numpy&#30456;&#24403;&#30340;&#27700;&#24179;&#19978;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;LoopTune&#20248;&#21270;&#20195;&#30721;&#30340;&#26102;&#38388;&#21482;&#38656;&#20960;&#31186;&#38047;&#12290;</title><link>http://arxiv.org/abs/2309.01825</link><description>&lt;p&gt;
LoopTune: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#24352;&#37327;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
LoopTune: Optimizing Tensor Computations with Reinforcement Learning. (arXiv:2309.01825v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01825
&lt;/p&gt;
&lt;p&gt;
LoopTune&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#24352;&#37327;&#35745;&#31639;&#30340;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#20248;&#21270;&#24352;&#37327;&#36941;&#21382;&#39034;&#24207;&#21644;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#22120;LoopNest&#25191;&#34892;&#30828;&#20214;&#29305;&#23450;&#20248;&#21270;&#65292;LoopTune&#33021;&#22815;&#29983;&#25104;&#27604;&#20854;&#20182;&#32534;&#35793;&#22120;&#26356;&#24555;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#37319;&#29992;&#26032;&#30340;&#22270;&#24418;&#34920;&#31034;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;LoopTune&#27604;TVM&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#27604;MetaSchedule&#24555;2.8&#20493;&#65292;&#27604;AutoTVM&#24555;1.08&#20493;&#65292;&#24182;&#25345;&#32493;&#22312;&#19982;&#25163;&#24037;&#35843;&#20248;&#30340;&#24211;Numpy&#30456;&#24403;&#30340;&#27700;&#24179;&#19978;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;LoopTune&#20248;&#21270;&#20195;&#30721;&#30340;&#26102;&#38388;&#21482;&#38656;&#20960;&#31186;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#32534;&#35793;&#22120;&#25216;&#26415;&#23545;&#20110;&#20351;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22312;&#26032;&#22411;&#30828;&#20214;&#19978;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20256;&#32479;&#32534;&#35793;&#22120;&#26080;&#27861;&#25552;&#20379;&#24615;&#33021;&#65292;&#26222;&#36890;&#30340;&#33258;&#21160;&#35843;&#33410;&#22120;&#25628;&#32034;&#26102;&#38388;&#38271;&#65292;&#32463;&#19987;&#23478;&#20248;&#21270;&#30340;&#24211;&#23548;&#33268;&#19981;&#21487;&#25345;&#32493;&#30340;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;LoopTune&#65292;&#36825;&#26159;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32534;&#35793;&#22120;&#65292;&#29992;&#20110;&#20248;&#21270;CPU&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24352;&#37327;&#35745;&#31639;&#12290;LoopTune&#22312;&#20351;&#29992;&#36229;&#24555;&#36731;&#37327;&#32423;&#20195;&#30721;&#29983;&#25104;&#22120;LoopNest&#25191;&#34892;&#30828;&#20214;&#29305;&#23450;&#20248;&#21270;&#30340;&#21516;&#26102;&#65292;&#20248;&#21270;&#24352;&#37327;&#36941;&#21382;&#39034;&#24207;&#12290;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;LoopTune&#23558;LoopNest&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;3.2&#20493;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#27604;TVM&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#27604;MetaSchedule&#24555;2.8&#20493;&#65292;&#27604;AutoTVM&#24555;1.08&#20493;&#65292;&#24182;&#25345;&#32493;&#22312;&#25163;&#24037;&#35843;&#25972;&#30340;&#24211;Numpy&#30340;&#27700;&#24179;&#19978;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;LoopTune&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#20248;&#21270;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced compiler technology is crucial for enabling machine learning applications to run on novel hardware, but traditional compilers fail to deliver performance, popular auto-tuners have long search times and expert-optimized libraries introduce unsustainable costs. To address this, we developed LoopTune, a deep reinforcement learning compiler that optimizes tensor computations in deep learning models for the CPU. LoopTune optimizes tensor traversal order while using the ultra-fast lightweight code generator LoopNest to perform hardware-specific optimizations. With a novel graph-based representation and action space, LoopTune speeds up LoopNest by 3.2x, generating an order of magnitude faster code than TVM, 2.8x faster than MetaSchedule, and 1.08x faster than AutoTVM, consistently performing at the level of the hand-tuned library Numpy. Moreover, LoopTune tunes code in order of seconds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#22312;&#38754;&#23545;&#26631;&#31614;&#22122;&#22768;&#26102;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#26159;&#36731;&#24494;&#30340;&#26631;&#31614;&#22122;&#22768;&#20063;&#20250;&#20005;&#37325;&#24433;&#21709;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.01706</link><description>&lt;p&gt;
&#20851;&#20110;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Post-hoc GNN Explainers to Label Noise. (arXiv:2309.01706v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#22312;&#38754;&#23545;&#26631;&#31614;&#22122;&#22768;&#26102;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#26159;&#36731;&#24494;&#30340;&#26631;&#31614;&#22122;&#22768;&#20063;&#20250;&#20005;&#37325;&#24433;&#21709;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22266;&#26377;&#40657;&#30418;&#38480;&#21046;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#35757;&#32451;&#21518;&#30340;GNN&#34920;&#29616;&#34892;&#20026;&#30340;&#31934;&#30830;&#21644;&#28145;&#21051;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#29615;&#22659;&#20013;&#26368;&#36817;&#26377;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#22312;&#38754;&#23545;&#26631;&#31614;&#22122;&#22768;&#26102;&#30340;&#40065;&#26834;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#31243;&#24230;&#26631;&#31614;&#22122;&#22768;&#19979;&#21508;&#31181;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#20960;&#20010;&#20851;&#38190;&#35265;&#35299;&#65306;&#39318;&#20808;&#65292;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#23481;&#26131;&#21463;&#21040;&#26631;&#31614;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#21363;&#20351;&#26159;&#23545;GNN&#34920;&#29616;&#26080;&#20851;&#32039;&#35201;&#30340;&#36731;&#24494;&#26631;&#31614;&#22122;&#22768;&#65292;&#20063;&#20250;&#20005;&#37325;&#25439;&#23475;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23601;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#21319;&#39640;&#36880;&#28176;&#24674;&#22797;&#35299;&#37322;&#25928;&#26524;&#23637;&#24320;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proposed as a solution to the inherent black-box limitations of graph neural networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful explanations of the behaviours exhibited by trained GNNs. Despite their recent notable advancements in academic and industrial contexts, the robustness of post-hoc GNN explainers remains unexplored when confronted with label noise. To bridge this gap, we conduct a systematic empirical investigation to evaluate the efficacy of diverse post-hoc GNN explainers under varying degrees of label noise. Our results reveal several key insights: Firstly, post-hoc GNN explainers are susceptible to label perturbations. Secondly, even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially. Lastly, we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#33258;&#30417;&#30563;&#30340;&#24207;&#21015;&#27169;&#22411;&#20013;&#26032;&#22411;&#32447;&#24615;&#34920;&#31034;&#30340;&#35777;&#25454;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#25506;&#27979;&#26827;&#30424;&#30340;"&#25105;&#30340;&#39068;&#33394;"&#19982;"&#23545;&#25163;&#30340;&#39068;&#33394;"&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#36825;&#31181;&#20934;&#30830;&#30340;&#20869;&#37096;&#34920;&#31034;&#29702;&#35299;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31616;&#21333;&#30340;&#21521;&#37327;&#36816;&#31639;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#26174;&#33879;&#25512;&#36827;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.00941</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#26032;&#22411;&#32447;&#24615;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Emergent Linear Representations in World Models of Self-Supervised Sequence Models. (arXiv:2309.00941v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#33258;&#30417;&#30563;&#30340;&#24207;&#21015;&#27169;&#22411;&#20013;&#26032;&#22411;&#32447;&#24615;&#34920;&#31034;&#30340;&#35777;&#25454;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#25506;&#27979;&#26827;&#30424;&#30340;"&#25105;&#30340;&#39068;&#33394;"&#19982;"&#23545;&#25163;&#30340;&#39068;&#33394;"&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#36825;&#31181;&#20934;&#30830;&#30340;&#20869;&#37096;&#34920;&#31034;&#29702;&#35299;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31616;&#21333;&#30340;&#21521;&#37327;&#36816;&#31639;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#26174;&#33879;&#25512;&#36827;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#27169;&#22411;&#22914;&#20309;&#34920;&#31034;&#20854;&#20915;&#31574;&#36807;&#31243;&#65311;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#40657;&#30333;&#26827;&#21338;&#24328;&#31070;&#32463;&#32593;&#32476;&#23398;&#21040;&#20102;&#38750;&#32447;&#24615;&#30340;&#26827;&#30424;&#29366;&#24577;&#27169;&#22411;&#65288;Li&#31561;&#65292;2023&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#23384;&#22312;&#19968;&#20010;&#23494;&#20999;&#30456;&#20851;&#30340;&#32447;&#24615;&#26827;&#30424;&#34920;&#31034;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25506;&#27979;"&#25105;&#30340;&#39068;&#33394;"&#19982;"&#23545;&#25163;&#30340;&#39068;&#33394;"&#36825;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#23545;&#20869;&#37096;&#34920;&#31034;&#30340;&#20934;&#30830;&#29702;&#35299;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31616;&#21333;&#30340;&#21521;&#37327;&#36816;&#31639;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#32447;&#24615;&#34920;&#31034;&#20351;&#24471;&#37325;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#23637;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#36890;&#36807;&#36827;&#19968;&#27493;&#25506;&#32034;&#19990;&#30028;&#27169;&#22411;&#30340;&#35745;&#31639;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;</title><link>http://arxiv.org/abs/2308.16375</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#35843;&#26597;&#65306;&#25915;&#20987;&#12289;&#20445;&#25252;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16375
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#33021;&#21147;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#25913;&#21892;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#39640;&#25928;&#33021;&#34920;&#29616;&#65292;&#22914;&#20934;&#30830;&#24615;&#65292;&#32780;&#32570;&#20047;&#38544;&#31169;&#32771;&#34385;&#65292;&#36825;&#26159;&#29616;&#20195;&#31038;&#20250;&#38544;&#31169;&#25915;&#20987;&#30427;&#34892;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#24320;&#21457;&#20445;&#25252;&#38544;&#31169;&#30340;GNNs&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22270;&#39046;&#22495;&#32570;&#20047;&#23545;&#25915;&#20987;&#21644;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24635;&#32467;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#25915;&#20987;&#12289;&#23545;GNNs&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#20197;&#21450;&#23457;&#26597;&#21487;&#29992;&#20110;&#20998;&#26512;/&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#31243;&#24207;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#65292;&#20197;&#24314;&#31435;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#22235;&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#27979;&#37327;&#32467;&#26524;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.15605</link><description>&lt;p&gt;
&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#22235;&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#27979;&#37327;&#32467;&#26524;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26469;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#25552;&#20379;&#23545;&#20248;&#21270;&#20855;&#26377;&#31283;&#20581;&#24615;&#30340;&#35757;&#32451;&#20449;&#21495;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#27979;&#37327;&#31713;&#25913;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#22810;&#20010;&#27979;&#37327;&#32467;&#26524;&#65292;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#20551;&#35937;&#65292;&#32780;&#19981;&#26159;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#19968;&#32452;&#25991;&#26412;&#36755;&#20837;&#21644;&#27979;&#37327;&#32467;&#26524;&#65292;&#26088;&#22312;&#30830;&#23450;&#26576;&#20010;&#32467;&#26524;&#26159;&#21542;&#21457;&#29983;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27979;&#37327;&#32467;&#26524;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#25152;&#26377;&#27979;&#37327;&#32467;&#26524;&#37117;&#34920;&#26126;&#32467;&#26524;&#21457;&#29983;&#30340;&#31034;&#20363;&#26159;&#21542;&#30830;&#23454;&#21457;&#29983;&#20102;&#32467;&#26524;&#65292;&#25110;&#32773;&#36825;&#26159;&#30001;&#20110;&#27979;&#37327;&#31713;&#25913;&#24341;&#36215;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#31616;&#21333;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#27809;&#26377;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#22312;&#25216;&#26415;&#21644;&#25968;&#25454;&#38598;&#26041;&#38754;&#37117;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#25105;&#20204;&#24863;&#21040;&#20852;&#22859;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15363</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#25991;&#26412;&#21040;SQL&#30340;&#30740;&#31350;&#65306;&#19968;&#20010;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#19968;&#31181;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#22522;&#20934;&#38459;&#30861;&#20102;&#35774;&#35745;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#21644;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#21253;&#25324;&#38382;&#39064;&#34920;&#31034;&#12289;&#31034;&#20363;&#36873;&#25321;&#21644;&#31034;&#20363;&#32452;&#32455;&#65292;&#24182;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#35814;&#32454;&#38416;&#36848;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#65292;&#36798;&#21040;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#26438;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#65292;&#24182;&#22312;&#27492;&#24230;&#37327;&#19979;&#27604;&#36739;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24320;&#28304;LLMs&#65292;&#24182;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#30417;&#30563;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborates their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. Additionally, we investigate open-source LLMs in in-context learning, and further enhance their performance with task-specific superv
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.13352</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#23436;&#20840;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#27745;&#26579;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13352
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#35299;&#20915;&#20102;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#20219;&#21153;&#12290;&#36825;&#20123;&#31639;&#27861;&#20013;&#22823;&#22810;&#25968;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#23545;&#19968;&#20010;&#22522;&#20110;&#27531;&#24046;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#26681;&#25454;&#26410;&#35265;&#26679;&#26412;&#19982;&#23398;&#20064;&#21040;&#30340;&#27491;&#24120;&#33539;&#22260;&#30340;&#19981;&#30456;&#20284;&#24615;&#26469;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#21487;&#20197;&#29992;&#26080;&#24322;&#24120;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#20250;&#19982;&#19968;&#23450;&#27604;&#20363;&#30340;&#24322;&#24120;&#26679;&#26412;&#28151;&#21512;&#12290;&#32780;&#21033;&#29992;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#24517;&#28982;&#20250;&#23548;&#33268;&#22522;&#20110;&#27531;&#24046;&#30340;&#31639;&#27861;&#30340;AD&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#29992;&#20110;AD&#20219;&#21153;&#30340;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#30340;&#25913;&#36827;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#27531;&#24046;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#22810;&#20803;&#26102;&#38388;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;QiML&#65289;&#26159;&#19968;&#20010;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#21407;&#29702;&#22312;&#32463;&#20856;&#35745;&#31639;&#26694;&#26550;&#20013;&#30340;&#26032;&#39046;&#22495;&#65292;&#22312;&#26412;&#35843;&#26597;&#20013;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;QiML&#30340;&#32508;&#21512;&#21644;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#20854;&#22810;&#26679;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#20026;QiML&#24314;&#31435;&#20102;&#26126;&#30830;&#30340;&#23450;&#20041;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#23558;&#20174;&#37327;&#23376;&#21147;&#23398;&#12289;&#37327;&#23376;&#35745;&#31639;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#20016;&#23500;QiML&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.11269</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Quantum-Inspired Machine Learning: a Survey. (arXiv:2308.11269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11269
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;QiML&#65289;&#26159;&#19968;&#20010;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#21407;&#29702;&#22312;&#32463;&#20856;&#35745;&#31639;&#26694;&#26550;&#20013;&#30340;&#26032;&#39046;&#22495;&#65292;&#22312;&#26412;&#35843;&#26597;&#20013;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;QiML&#30340;&#32508;&#21512;&#21644;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#20854;&#22810;&#26679;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#20026;QiML&#24314;&#31435;&#20102;&#26126;&#30830;&#30340;&#23450;&#20041;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#23558;&#20174;&#37327;&#23376;&#21147;&#23398;&#12289;&#37327;&#23376;&#35745;&#31639;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#20016;&#23500;QiML&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;QiML&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#22240;&#20854;&#22312;&#32463;&#20856;&#35745;&#31639;&#26694;&#26550;&#20013;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#21407;&#29702;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#32508;&#36848;&#25991;&#29486;&#32463;&#24120;&#21482;&#23545;QiML&#36827;&#34892;&#34920;&#38754;&#25506;&#32034;&#65292;&#32780;&#26356;&#22810;&#20851;&#27880;&#24191;&#20041;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#39046;&#22495;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;QiML&#30340;&#32508;&#21512;&#21644;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;QiML&#30340;&#22810;&#26679;&#21270;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#24352;&#37327;&#32593;&#32476;&#27169;&#25311;&#12289;&#38750;&#37327;&#23376;&#21270;&#31639;&#27861;&#31561;&#65292;&#23637;&#31034;&#20102;&#26368;&#26032;&#30340;&#36827;&#23637;&#12289;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#20808;&#21069;&#23545;QiML&#30340;&#21508;&#31181;&#35299;&#37322;&#21450;&#20854;&#20869;&#22312;&#30340;&#27169;&#31946;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;QiML&#23450;&#20041;&#12290;&#38543;&#30528;QiML&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#25105;&#20204;&#39044;&#35745;&#23558;&#26377;&#22823;&#37327;&#30340;&#26410;&#26469;&#21457;&#23637;&#20174;&#37327;&#23376;&#21147;&#23398;&#12289;&#37327;&#23376;&#35745;&#31639;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#20016;&#23500;QiML&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum-inspired Machine Learning (QiML) is a burgeoning field, receiving global attention from researchers for its potential to leverage principles of quantum mechanics within classical computational frameworks. However, current review literature often presents a superficial exploration of QiML, focusing instead on the broader Quantum Machine Learning (QML) field. In response to this gap, this survey provides an integrated and comprehensive examination of QiML, exploring QiML's diverse research domains including tensor network simulations, dequantized algorithms, and others, showcasing recent advancements, practical applications, and illuminating potential future research avenues. Further, a concrete definition of QiML is established by analyzing various prior interpretations of the term and their inherent ambiguities. As QiML continues to evolve, we anticipate a wealth of future developments drawing from quantum mechanics, quantum computing, and classical machine learning, enriching 
&lt;/p&gt;</description></item><item><title>LadleNet&#26159;&#19968;&#31181;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#20004;&#38454;&#27573;U-Net&#23558;&#28909;&#32418;&#22806;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#22270;&#20687;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#21644;&#31934;&#32454;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;Handle&#27169;&#22359;&#26500;&#24314;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;Bowl&#27169;&#22359;&#35299;&#30721;&#35813;&#31354;&#38388;&#29983;&#25104;&#26144;&#23556;&#30340;VI&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.06603</link><description>&lt;p&gt;
LadleNet: &#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#20004;&#38454;&#27573;U-Net&#23558;&#28909;&#32418;&#22806;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net. (arXiv:2308.06603v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06603
&lt;/p&gt;
&lt;p&gt;
LadleNet&#26159;&#19968;&#31181;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#20004;&#38454;&#27573;U-Net&#23558;&#28909;&#32418;&#22806;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#22270;&#20687;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#21644;&#31934;&#32454;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;Handle&#27169;&#22359;&#26500;&#24314;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;Bowl&#27169;&#22359;&#35299;&#30721;&#35813;&#31354;&#38388;&#29983;&#25104;&#26144;&#23556;&#30340;VI&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28909;&#32418;&#22806;&#65288;TIR&#65289;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#65288;VI&#65289;&#22270;&#20687;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;TIR-VI&#22270;&#20687;&#37197;&#20934;&#21644;&#34701;&#21512;&#12290;&#21033;&#29992;&#20174;TIR&#22270;&#20687;&#36716;&#25442;&#20013;&#24471;&#21040;&#30340;&#34917;&#20805;&#20449;&#24687;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24212;&#29992;&#31243;&#24207;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#23384;&#22312;&#30340;&#20027;&#35201;&#38382;&#39064;&#21253;&#25324;&#22270;&#20687;&#20445;&#30495;&#24230;&#19981;&#39640;&#21644;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#26550;&#26500;&#30340;&#31639;&#27861;LadleNet&#12290; LadleNet&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;U-Net&#20018;&#32852;&#32467;&#26500;&#65292;&#22686;&#21152;&#20102;&#36339;&#36291;&#36830;&#25509;&#21644;&#31934;&#32454;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;LadleNet&#30001;&#8220;Handle&#8221;&#21644;&#8220;Bowl&#8221;&#27169;&#22359;&#32452;&#25104;&#65292;Handle&#27169;&#22359;&#29992;&#20110;&#26500;&#24314;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;&#32780;Bowl&#27169;&#22359;&#21017;&#35299;&#30721;&#36825;&#20010;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;&#29983;&#25104;&#26144;&#23556;&#30340;VI&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibilit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29289;&#32852;&#32593;&#36710;&#36742;&#20013;&#35745;&#31639;&#21368;&#36733;&#30340;&#24310;&#36831;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#20219;&#21153;&#21368;&#36733;&#21040;&#36793;&#32536;&#21333;&#20803;&#65292;&#21487;&#20197;&#20943;&#36731;&#36710;&#36733;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#36873;&#25321;&#27599;&#36742;&#36710;&#30340;&#26368;&#20339;&#21368;&#36733;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2308.02603</link><description>&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#29289;&#32852;&#32593;&#36710;&#36742;&#20013;&#30340;&#35745;&#31639;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles. (arXiv:2308.02603v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02603
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29289;&#32852;&#32593;&#36710;&#36742;&#20013;&#35745;&#31639;&#21368;&#36733;&#30340;&#24310;&#36831;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#20219;&#21153;&#21368;&#36733;&#21040;&#36793;&#32536;&#21333;&#20803;&#65292;&#21487;&#20197;&#20943;&#36731;&#36710;&#36733;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#36873;&#25321;&#27599;&#36742;&#36710;&#30340;&#26368;&#20339;&#21368;&#36733;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36710;&#36742;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#20219;&#21153;&#21368;&#36733;&#21040;&#36947;&#36335;&#36793;&#30340;&#21333;&#20803;(RSU)&#65292;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#22312;&#29289;&#32852;&#32593;&#36710;&#36742;(IoV)&#20013;&#21487;&#20197;&#20943;&#36731;&#36710;&#36733;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
By offloading computation-intensive tasks of vehicles to roadside units (RSUs), mobile edge computing (MEC) in the Internet of Vehicles (IoV) can relieve the onboard computation burden. However, existing model-based task offloading methods suffer from heavy computational complexity with the increase of vehicles and data-driven methods lack interpretability. To address these challenges, in this paper, we propose a knowledge-driven multi-agent reinforcement learning (KMARL) approach to reduce the latency of task offloading in cybertwin-enabled IoV. Specifically, in the considered scenario, the cybertwin serves as a communication agent for each vehicle to exchange information and make offloading decisions in the virtual space. To reduce the latency of task offloading, a KMARL approach is proposed to select the optimal offloading option for each vehicle, where graph neural networks are employed by leveraging domain knowledge concerning graph-structure communication topology and permutation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#39640;&#31232;&#30095;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20351;&#29992;&#20256;&#32479;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#25928;&#26524;&#19981;&#20339;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02060</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#31232;&#30095;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accurate Neural Network Pruning Requires Rethinking Sparse Optimization. (arXiv:2308.02060v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02060
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#39640;&#31232;&#30095;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20351;&#29992;&#20256;&#32479;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#25928;&#26524;&#19981;&#20339;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#65292;&#33719;&#24471;&#26082;&#39640;&#31934;&#30830;&#21448;&#39640;&#31232;&#30095;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29256;&#26412;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#31038;&#21306;&#24050;&#32463;&#23545;&#20960;&#31181;&#39640;&#24615;&#33021;&#30340;&#21098;&#26525;&#25216;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#31232;&#30095;&#24615;&#21644;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#30340;&#26631;&#20934;&#38543;&#26426;&#20248;&#21270;&#25216;&#26415;&#30340;&#20132;&#20114;&#20102;&#35299;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20351;&#29992;&#26631;&#20934;&#30340;&#23494;&#38598;&#35757;&#32451;&#35745;&#21010;&#21644;&#36229;&#21442;&#25968;&#26469;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31232;&#30095;&#22522;&#20934;&#26469;&#30740;&#31350;&#39640;&#31232;&#30095;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20351;&#29992;&#26631;&#20934;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#26159;&#27425;&#20248;&#30340;&#65292;&#23548;&#33268;&#27424;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26082;&#21487;&#20197;&#29992;&#20110;&#35270;&#35273;&#27169;&#22411;&#65288;&#22914;ResNet50/ImageNet&#65289;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT/GLUE&#65289;&#30340;&#31232;&#30095;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#24402;&#19968;&#21270;&#27969;&#30340;&#26694;&#26550;&#20013;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#20135;&#29983;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14839</link><description>&lt;p&gt;
&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#24402;&#19968;&#21270;&#27969;&#30340;&#26694;&#26550;&#20013;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#20135;&#29983;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#20197;&#20854;&#21487;&#36870;&#30340;&#26550;&#26500;&#32780;&#34987;&#25551;&#36848;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21487;&#36870;&#24615;&#35201;&#27714;&#23545;&#20854;&#34920;&#36798;&#33021;&#21147;&#26045;&#21152;&#38480;&#21046;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#21644;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#26469;&#36798;&#21040;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36716;&#25442;&#26469;&#23454;&#29616;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#26367;&#20195;&#30340;&#36716;&#25442;&#26041;&#27861;&#21364;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#20135;&#29983;&#26377;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;&#26680;&#21270;&#27969;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;Temporal Graph Benchmark&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#21040;TGB&#65292;&#24182;&#20351;&#29992;&#21313;&#19968;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#12290;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;DyGLib&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.12510</link><description>&lt;p&gt;
Temporal Graph Benchmark&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Temporal Graph Benchmark. (arXiv:2307.12510v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Temporal Graph Benchmark&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#21040;TGB&#65292;&#24182;&#20351;&#29992;&#21313;&#19968;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#12290;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;DyGLib&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#25193;&#23637;&#21040;Temporal Graph Benchmark (TGB)&#65292;&#23545;TGB&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#19982;TGB&#30456;&#27604;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#21313;&#19968;&#31181;&#27969;&#34892;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#36825;&#19982;&#20043;&#21069;&#30340;&#35266;&#23519;&#19968;&#33268;&#65307;&#65288;2&#65289;&#20351;&#29992;DyGLib&#26102;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#12290;&#26412;&#24037;&#20316;&#26088;&#22312;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#22312;TGB&#19978;&#35780;&#20272;&#21508;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35797;&#22270;&#25552;&#20379;&#21487;&#30452;&#25509;&#21442;&#32771;&#30340;&#32467;&#26524;&#20379;&#21518;&#32493;&#30740;&#31350;&#20351;&#29992;&#12290;&#26412;&#39033;&#30446;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#36164;&#28304;&#22343;&#21487;&#22312;https://github.com/yule-BUAA/DyGLib_TGB&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;&#26412;&#24037;&#20316;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#27426;&#36814;&#31038;&#21306;&#25552;&#20379;&#21453;&#39304;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#28041;&#21450;&#21040;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.10616</link><description>&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65306;&#29616;&#29366;&#19982;&#30740;&#31350;&#25361;&#25112;&#30340;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Federated Learning: State-of-the-art and Research Challenges. (arXiv:2307.10616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10616
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#28041;&#21450;&#21040;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#30001;&#20110;&#22312;&#22823;&#35268;&#27169;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#27169;&#22411;&#21516;&#36136;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#32852;&#37030;&#23398;&#20064;&#36890;&#24120;&#38754;&#20020;&#21442;&#19982;&#26041;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#12290;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064; (HFL) &#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#22810;&#26679;&#19988;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20010;&#20027;&#39064;&#36827;&#34892;&#20851;&#20110;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#30340;&#31995;&#32479;&#35843;&#26597;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102; HFL &#20013;&#26469;&#33258;&#20116;&#20010;&#26041;&#38754;&#30340;&#21508;&#31181;&#30740;&#31350;&#25361;&#25112;&#65306;&#32479;&#35745;&#24322;&#36136;&#24615;&#12289;&#27169;&#22411;&#24322;&#36136;&#24615;&#12289;&#36890;&#20449;&#24322;&#36136;&#24615;&#12289;&#35774;&#22791;&#24322;&#36136;&#24615;&#21644;&#39069;&#22806;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102; HFL &#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377; HFL &#26041;&#27861;&#30340;&#26032;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#20854;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify
&lt;/p&gt;</description></item><item><title>FALCON&#26159;&#19968;&#20010;&#35299;&#37322;&#22270;&#20687;&#34920;&#31034;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#23383;&#24149;&#25968;&#25454;&#38598;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#35299;&#37322;&#28040;&#38500;&#34394;&#20551;&#27010;&#24565;&#12290;&#22312;&#36739;&#22823;&#30340;&#31354;&#38388;&#20013;&#65292;&#29305;&#24449;&#36890;&#36807;&#30740;&#31350;&#32452;&#21512;&#21487;&#20197;&#26356;&#26131;&#35299;&#37322;&#21644;&#39640;&#38454;&#35780;&#20998;&#27010;&#24565;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.10504</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#34920;&#31034;&#20013;&#35782;&#21035;&#21487;&#35299;&#37322;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Identifying Interpretable Subspaces in Image Representations. (arXiv:2307.10504v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10504
&lt;/p&gt;
&lt;p&gt;
FALCON&#26159;&#19968;&#20010;&#35299;&#37322;&#22270;&#20687;&#34920;&#31034;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#23383;&#24149;&#25968;&#25454;&#38598;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#35299;&#37322;&#28040;&#38500;&#34394;&#20551;&#27010;&#24565;&#12290;&#22312;&#36739;&#22823;&#30340;&#31354;&#38388;&#20013;&#65292;&#29305;&#24449;&#36890;&#36807;&#30740;&#31350;&#32452;&#21512;&#21487;&#20197;&#26356;&#26131;&#35299;&#37322;&#21644;&#39640;&#38454;&#35780;&#20998;&#27010;&#24565;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#22270;&#20687;&#34920;&#31034;&#29305;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#8212;&#8212;FALCON&#65288;Automatic Feature Explanation using Contrasting Concepts&#65289;&#12290;&#23545;&#20110;&#30446;&#26631;&#29305;&#24449;&#65292;FALCON&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#30340;&#23383;&#24149;&#25968;&#25454;&#38598;&#65288;&#22914;LAION-400m&#65289;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#23545;&#20854;&#39640;&#24230;&#28608;&#27963;&#30340;&#35009;&#21098;&#22270;&#20687;&#36827;&#34892;&#23383;&#24149;&#29983;&#25104;&#12290;&#27599;&#20010;&#23383;&#24149;&#20013;&#30340;&#21333;&#35789;&#37117;&#32463;&#36807;&#35780;&#20998;&#21644;&#25490;&#24207;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19982;&#30446;&#26631;&#29305;&#24449;&#23494;&#20999;&#30456;&#20851;&#30340;&#23569;&#25968;&#20849;&#20139;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;FALCON&#36824;&#20351;&#29992;&#20302;&#28608;&#27963;&#30340;&#65288;&#23545;&#31435;&#30340;&#65289;&#22270;&#20687;&#24212;&#29992;&#23545;&#27604;&#35299;&#37322;&#65292;&#20197;&#28040;&#38500;&#34394;&#20551;&#27010;&#24565;&#12290;&#23613;&#31649;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#35299;&#37322;&#29305;&#24449;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#27169;&#22411;&#20013;&#65292;&#19981;&#21040;20%&#30340;&#34920;&#31034;&#31354;&#38388;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#29305;&#24449;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#19968;&#32452;&#29305;&#24449;&#19968;&#36215;&#30740;&#31350;&#26102;&#65292;&#26356;&#22823;&#30340;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#21464;&#24471;&#26356;&#26131;&#35299;&#37322;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;FALCON&#24471;&#21040;&#39640;&#38454;&#35780;&#20998;&#27010;&#24565;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#20840;&#23616;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05551</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization. (arXiv:2307.05551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#20840;&#23616;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#31859;&#25216;&#26415;&#21644;&#20808;&#36827;&#26448;&#26009;&#30340;&#31185;&#23398;&#36827;&#23637;&#20026;&#20307;&#20869;&#31934;&#20934;&#21307;&#23398;&#30340;&#32435;&#31859;&#23610;&#24230;&#35013;&#32622;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20854;&#20013;&#21253;&#25324;&#38598;&#25104;&#24863;&#24212;&#12289;&#35745;&#31639;&#12289;&#36890;&#20449;&#12289;&#25968;&#25454;&#21644;&#33021;&#37327;&#23384;&#20648;&#33021;&#21147;&#12290;&#22312;&#20154;&#20307;&#24515;&#34880;&#31649;&#31995;&#32479;&#20013;&#65292;&#36825;&#20123;&#35013;&#32622;&#34987;&#35774;&#24819;&#20026;&#34987;&#21160;&#27969;&#21160;&#24182;&#25345;&#32493;&#24863;&#30693;&#20197;&#20415;&#26816;&#27979;&#35786;&#26029;&#24863;&#20852;&#36259;&#30340;&#20107;&#20214;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20107;&#20214;&#30340;&#29289;&#29702;&#20301;&#32622;&#65288;&#22914;&#36523;&#20307;&#21306;&#22495;&#65289;&#20998;&#37197;&#32473;&#23427;&#20204;&#65292;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#21040;&#36825;&#20123;&#20107;&#20214;&#30340;&#35786;&#26029;&#20215;&#20540;&#65292;&#36825;&#26159;&#27969;&#23548;&#21521;&#23450;&#20301;&#30340;&#20027;&#35201;&#21629;&#39064;&#12290;&#24403;&#21069;&#30340;&#27969;&#23548;&#21521;&#23450;&#20301;&#26041;&#27861;&#23384;&#22312;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#22312;&#25972;&#20010;&#24515;&#34880;&#31649;&#31995;&#32479;&#20869;&#26412;&#22320;&#21270;&#20107;&#20214;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#36827;&#34892;&#23450;&#20301;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific advancements in nanotechnology and advanced materials are paving the way toward nanoscale devices for in-body precision medicine; comprising integrated sensing, computing, communication, data and energy storage capabilities. In the human cardiovascular system, such devices are envisioned to be passively flowing and continuously sensing for detecting events of diagnostic interest. The diagnostic value of detecting such events can be enhanced by assigning to them their physical locations (e.g., body region), which is the main proposition of flow-guided localization. Current flow-guided localization approaches suffer from low localization accuracy and they are by-design unable to localize events within the entire cardiovascular system. Toward addressing this issue, we propose the utilization of Graph Neural Networks (GNNs) for this purpose, and demonstrate localization accuracy and coverage enhancements of our proposal over the existing State of the Art (SotA) approaches. Based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#24377;&#24615;&#20840;&#27874;&#24418;&#21453;&#28436;&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;$\mathbf{\mathbb{E}^{FWI}}$&#65292;&#20854;&#21253;&#25324;8&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12386</link><description>&lt;p&gt;
$\mathbf{\mathbb{E}^{FWI}}$:&#22810;&#21442;&#25968;&#22320;&#29699;&#29289;&#29702;&#23646;&#24615;&#24377;&#24615;&#20840;&#27874;&#24418;&#21453;&#28436;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
$\mathbf{\mathbb{E}^{FWI}}$: Multi-parameter Benchmark Datasets for Elastic Full Waveform Inversion of Geophysical Properties. (arXiv:2306.12386v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#24377;&#24615;&#20840;&#27874;&#24418;&#21453;&#28436;&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;$\mathbf{\mathbb{E}^{FWI}}$&#65292;&#20854;&#21253;&#25324;8&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#22320;&#29699;&#29289;&#29702;&#23646;&#24615;&#65288;&#22914;P&#21644;S&#27874;&#36895;&#24230;&#65289;&#22312;CO$_2$&#23553;&#23384;&#21644;&#33021;&#28304;&#21208;&#25506;&#65288;&#22914;&#27682;&#21644;&#22320;&#28909;&#33021;&#65289;&#31561;&#21508;&#31181;&#22320;&#19979;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#24377;&#24615;&#20840;&#27874;&#24418;&#21453;&#28436;(FWI)&#24191;&#27867;&#24212;&#29992;&#20110;&#34920;&#24449;&#20648;&#23618;&#23646;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;$\mathbf{\mathbb{E}^{FWI}}$&#65292;&#19968;&#20010;&#19987;&#38376;&#20026;&#24377;&#24615;FWI&#35774;&#35745;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;$\mathbf{\mathbb{E}^{FWI}}$&#21253;&#25324;8&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#21508;&#31181;&#19981;&#21516;&#30340;&#22320;&#19979;&#22320;&#36136;&#32467;&#26500;&#65288;&#24179;&#22374;&#12289;&#26354;&#32447;&#12289;&#26029;&#23618;&#31561;&#65289;&#12290;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;&#19982;&#25105;&#20204;&#20808;&#21069;&#25552;&#20379;&#30340;&#22768;&#23398;FWI&#21387;&#21147;&#35760;&#24405;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;OpenFWI&#65289;&#30456;&#27604;&#65292;$\mathbf{\mathbb{E}^{FWI}}$&#30340;&#22320;&#38663;&#25968;&#25454;&#38598;&#20855;&#26377;&#22402;&#30452;&#21644;&#27700;&#24179;&#20998;&#37327;&#12290;&#27492;&#22806;&#65292;$\mathbf{\mathbb{E}^{FWI}}$&#20013;&#30340;&#36895;&#24230;&#22270;&#21253;&#21547;&#20102;P&#21644;S&#27874;&#36895;&#24230;&#12290;&#34429;&#28982;&#22810;&#20998;&#37327;FWI&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#20294;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#24377;&#24615;FWI&#30340;&#22810;&#21442;&#25968;&#21453;&#28436;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35838;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Elastic geophysical properties (such as P- and S-wave velocities) are of great importance to various subsurface applications like CO$_2$ sequestration and energy exploration (e.g., hydrogen and geothermal). Elastic full waveform inversion (FWI) is widely applied for characterizing reservoir properties. In this paper, we introduce $\mathbf{\mathbb{E}^{FWI}}$, a comprehensive benchmark dataset that is specifically designed for elastic FWI. $\mathbf{\mathbb{E}^{FWI}}$ encompasses 8 distinct datasets that cover diverse subsurface geologic structures (flat, curve, faults, etc). The benchmark results produced by three different deep learning methods are provided. In contrast to our previously presented dataset (pressure recordings) for acoustic FWI (referred to as OpenFWI), the seismic dataset in $\mathbf{\mathbb{E}^{FWI}}$ has both vertical and horizontal components. Moreover, the velocity maps in $\mathbf{\mathbb{E}^{FWI}}$ incorporate both Pand S-wave velocities. While the multicomponen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;GPU&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39564;&#35777;&#22312;&#30005;&#21147;&#31995;&#32479;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#38556;&#30861;&#65292;&#36890;&#36807;&#21516;&#26102;&#39564;&#35777;&#22810;&#20010;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#31934;&#30830;&#36716;&#25442;&#26041;&#27861;&#65292;&#23558;&#19968;&#32452;&#28508;&#22312;&#36829;&#35268;&#36716;&#21270;&#20026;&#22686;&#24378;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#33021;&#21147;&#30340;&#23618;&#12290;</title><link>http://arxiv.org/abs/2306.10617</link><description>&lt;p&gt;
&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#30340;GPU&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
GPU-Accelerated Verification of Machine Learning Models for Power Systems. (arXiv:2306.10617v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;GPU&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39564;&#35777;&#22312;&#30005;&#21147;&#31995;&#32479;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#38556;&#30861;&#65292;&#36890;&#36807;&#21516;&#26102;&#39564;&#35777;&#22810;&#20010;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#31934;&#30830;&#36716;&#25442;&#26041;&#27861;&#65292;&#23558;&#19968;&#32452;&#28508;&#22312;&#36829;&#35268;&#36716;&#21270;&#20026;&#22686;&#24378;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#33021;&#21147;&#30340;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#29992;&#20110;&#20005;&#26684;&#39564;&#35777;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#35745;&#31639;&#24037;&#20855;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26368;&#25104;&#21151;&#30340;&#27714;&#35299;&#22120;&#37319;&#29992;&#20102;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#12289;GPU&#21152;&#36895;&#30340;&#20998;&#25903;&#21644;&#30028;&#38480;&#31639;&#27861;&#12290;&#36825;&#26679;&#30340;&#24037;&#20855;&#23545;&#20110;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#25104;&#21151;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#30005;&#21147;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20173;&#28982;&#23384;&#22312;&#38556;&#30861;&#38459;&#27490;&#20102;&#23558;&#36825;&#20123;&#27714;&#35299;&#22120;&#30452;&#25509;&#24212;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#31181;&#20851;&#38190;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#39318;&#27425;&#21551;&#29992;&#20102;&#22810;&#20010;&#39564;&#35777;&#38382;&#39064;&#30340;&#21516;&#26102;&#39564;&#35777;&#65288;&#20363;&#22914;&#65292;&#21516;&#26102;&#26816;&#26597;&#25152;&#26377;&#32447;&#36335;&#27969;&#37327;&#32422;&#26463;&#30340;&#36829;&#35268;&#65292;&#24182;&#38750;&#36890;&#36807;&#35299;&#20915;&#21333;&#20010;&#39564;&#35777;&#38382;&#39064;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#36716;&#25442;&#65292;&#23558;&#19968;&#32452;&#28508;&#22312;&#36829;&#35268;&#20013;&#30340;&#26368;&#22351;&#24773;&#20917;&#36829;&#35268;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#22522;&#20110;ReLU&#30340;&#23618;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational tools for rigorously verifying the performance of large-scale machine learning (ML) models have progressed significantly in recent years. The most successful solvers employ highly specialized, GPU-accelerated branch and bound routines. Such tools are crucial for the successful deployment of machine learning applications in safety-critical systems, such as power systems. Despite their successes, however, barriers prevent out-of-the-box application of these routines to power system problems. This paper addresses this issue in two key ways. First, for the first time to our knowledge, we enable the simultaneous verification of multiple verification problems (e.g., checking for the violation of all line flow constraints simultaneously and not by solving individual verification problems). For that, we introduce an exact transformation that converts the "worst-case" violation across a set of potential violations to a series of ReLU-based layers that augment the original neural n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; RecFusion&#65292;&#19968;&#31181;&#29305;&#23450;&#38024;&#23545;1D&#21644;/&#25110;&#20108;&#36827;&#21046;&#35774;&#32622;&#30340;&#25512;&#33616;&#27169;&#22411;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#20108;&#39033;&#24335;&#25193;&#25955;&#36807;&#31243;&#23545;&#20108;&#20803;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#26174;&#24335;&#24314;&#27169;&#65292;&#24182;&#22312;&#26680;&#24515;&#25512;&#33616;&#35774;&#32622;&#21644;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#25509;&#36817;&#22797;&#26434;&#30340;VAE&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.08947</link><description>&lt;p&gt;
RecFusion&#65306;&#22522;&#20110;&#20108;&#39033;&#24335;&#25193;&#25955;&#36807;&#31243;&#30340;1D&#25968;&#25454;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation. (arXiv:2306.08947v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; RecFusion&#65292;&#19968;&#31181;&#29305;&#23450;&#38024;&#23545;1D&#21644;/&#25110;&#20108;&#36827;&#21046;&#35774;&#32622;&#30340;&#25512;&#33616;&#27169;&#22411;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#20108;&#39033;&#24335;&#25193;&#25955;&#36807;&#31243;&#23545;&#20108;&#20803;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#26174;&#24335;&#24314;&#27169;&#65292;&#24182;&#22312;&#26680;&#24515;&#25512;&#33616;&#35774;&#32622;&#21644;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#25509;&#36817;&#22797;&#26434;&#30340;VAE&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RecFusion&#65292;&#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#25512;&#33616;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#19981;&#21516;&#20110;&#21253;&#21547;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#24120;&#29992;&#20110;&#25512;&#33616;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#32570;&#20047;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#19968;&#32500;&#21521;&#37327;&#19978;&#21046;&#23450;&#20102;&#25193;&#25955;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20108;&#39033;&#24335;&#25193;&#25955;&#65292;&#36825;&#20010;&#26041;&#27861;&#21033;&#29992;&#20102;&#20271;&#21162;&#21033;&#36807;&#31243;&#26174;&#24335;&#22320;&#23545;&#20108;&#20803;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RecFusion&#22312;&#26680;&#24515;&#25512;&#33616;&#35774;&#32622;&#65288;&#38024;&#23545;&#20108;&#36827;&#21046;&#38750;&#39034;&#24207;&#21453;&#39304;&#30340;&#21069;n&#39033;&#25512;&#33616;&#65289;&#21644;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#65288;MovieLens&#21644;Netflix&#65289;&#19978;&#25509;&#36817;&#20110;&#22797;&#26434;&#30340;VAE&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19987;&#38376;&#38024;&#23545;1D&#21644;/&#25110;&#20108;&#36827;&#21046;&#35774;&#32622;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24847;&#20041;&#36229;&#20986;&#20102;&#25512;&#33616;&#31995;&#32479;&#65292;&#20363;&#22914;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;MRI&#21644;CT&#25195;&#25551;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose RecFusion, which comprise a set of diffusion models for recommendation. Unlike image data which contain spatial correlations, a user-item interaction matrix, commonly utilized in recommendation, lacks spatial relationships between users and items. We formulate diffusion on a 1D vector and propose binomial diffusion, which explicitly models binary user-item interactions with a Bernoulli process. We show that RecFusion approaches the performance of complex VAE baselines on the core recommendation setting (top-n recommendation for binary non-sequential feedback) and the most common datasets (MovieLens and Netflix). Our proposed diffusion models that are specialized for 1D and/or binary setups have implications beyond recommendation systems, such as in the medical domain with MRI and CT scans.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#21452;&#38750;&#22343;&#21248;&#29615;&#22659;&#19979;&#30740;&#31350;&#20102;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;(OPE)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#22870;&#21169;&#21644;&#35266;&#27979;&#36716;&#31227;&#20989;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;OPE&#26694;&#26550;&#12290;&#35813;&#30740;&#31350;&#23545;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#20013;&#30340;OPE&#20570;&#20986;&#20102;&#29702;&#35770;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08719</link><description>&lt;p&gt;
&#21452;&#38750;&#22343;&#21248;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-policy Evaluation in Doubly Inhomogeneous Environments. (arXiv:2306.08719v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#21452;&#38750;&#22343;&#21248;&#29615;&#22659;&#19979;&#30740;&#31350;&#20102;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;(OPE)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#22870;&#21169;&#21644;&#35266;&#27979;&#36716;&#31227;&#20989;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;OPE&#26694;&#26550;&#12290;&#35813;&#30740;&#31350;&#23545;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#20013;&#30340;OPE&#20570;&#20986;&#20102;&#29702;&#35770;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#22312;&#20004;&#20010;&#20851;&#38190;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20551;&#35774;&#8212;&#8212;&#26102;&#38388;&#31283;&#23450;&#24615;&#21644;&#20010;&#20307;&#22343;&#21248;&#24615;&#22343;&#34987;&#30772;&#22351;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#22788;&#29702;&#8220;&#21452;&#38750;&#22343;&#21248;&#24615;&#8221;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#22870;&#21169;&#21644;&#35266;&#27979;&#36716;&#31227;&#20989;&#25968;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#27169;&#22411;&#39537;&#21160;&#21644;&#27169;&#22411;&#33258;&#30001;&#26041;&#27861;&#30340;&#36890;&#29992;OPE&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#31163;&#32447;RL&#20013;&#24320;&#21457;&#32479;&#35745;&#19978;&#21487;&#38752;&#30340;OPE&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#24182;&#19988;&#28041;&#21450;&#20102;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#12290;&#35813;&#30740;&#31350;&#28145;&#20837;&#29702;&#35299;&#20102;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#20013;&#30340;OPE&#65292;&#24182;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#20960;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25152;&#25552;&#20986;&#30340;&#20215;&#20540;&#20272;&#35745;&#22120;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24573;&#35270;&#26102;&#38388;&#38750;&#31283;&#23450;&#24615;&#25110;&#20010;&#20307;&#24322;&#36136;&#24615;&#30340;&#31454;&#20105;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to study off-policy evaluation (OPE) under scenarios where two key reinforcement learning (RL) assumptions -- temporal stationarity and individual homogeneity are both violated. To handle the ``double inhomogeneities", we propose a class of latent factor models for the reward and observation transition functions, under which we develop a general OPE framework that consists of both model-based and model-free approaches. To our knowledge, this is the first paper that develops statistically sound OPE methods in offline RL with double inhomogeneities. It contributes to a deeper understanding of OPE in environments, where standard RL assumptions are not met, and provides several practical approaches in these settings. We establish the theoretical properties of the proposed value estimators and empirically show that our approach outperforms competing methods that ignore either temporal nonstationarity or individual heterogeneity. Finally, we illustrate our method on a data set
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#8212;&#8212;&#24378;&#36866;&#24212;&#23545;&#25163;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23545;&#25163;&#21644;&#25968;&#25454;&#20998;&#24067;&#38382;&#39064;&#12290;&#20316;&#32773;&#20351;&#29992;&#22810;&#24230;&#37327;&#35843;&#26597;&#26469;&#22686;&#24378; FL &#23545;&#36825;&#20123;&#23545;&#25163;&#30340;&#25269;&#25239;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03600</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#24230;&#37327;&#35843;&#26597;&#36991;&#20813;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Avoid Adversarial Adaption in Federated Learning by Multi-Metric Investigations. (arXiv:2306.03600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#8212;&#8212;&#24378;&#36866;&#24212;&#23545;&#25163;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23545;&#25163;&#21644;&#25968;&#25454;&#20998;&#24067;&#38382;&#39064;&#12290;&#20316;&#32773;&#20351;&#29992;&#22810;&#24230;&#37327;&#35843;&#26597;&#26469;&#22686;&#24378; FL &#23545;&#36825;&#20123;&#23545;&#25163;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#21487;&#20197;&#22312;&#20998;&#24067;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36991;&#20813;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#20301;&#32622;&#12290;&#36825;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;FL &#23481;&#26131;&#21463;&#21040;&#27745;&#26579;&#25915;&#20987;&#65292;&#21487;&#20197;&#26159;&#38750;&#23450;&#21521;&#30340;&#65292;&#26088;&#22312;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#20063;&#21487;&#20197;&#26159;&#26377;&#30446;&#30340;&#30340;&#65292;&#21363;&#25152;&#35859;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#20250;&#28155;&#21152;&#23545;&#25239;&#24615;&#34892;&#20026;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#21046;&#20316;&#30340;&#36755;&#20837;&#35302;&#21457;&#12290;&#20026;&#20102;&#36861;&#27714;&#38544;&#34109;&#24615;&#65292;&#21518;&#38376;&#25915;&#20987;&#26356;&#38590;&#24212;&#23545;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#32531;&#35299;&#25216;&#26415;&#20381;&#36182;&#20110;&#30417;&#35270;&#26576;&#20123;&#24230;&#37327;&#26631;&#20934;&#21644;&#36807;&#28388;&#24694;&#24847;&#27169;&#22411;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#27809;&#26377;&#32771;&#34385;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#23545;&#25163;&#21644;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#28857;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#8212;&#8212;&#24378;&#36866;&#24212;&#23545;&#25163;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#36866;&#24212;&#22810;&#20010;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#27979;&#35797;&#35777;&#26126;&#65292;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#22312;&#36825;&#31181;&#23545;&#25163;&#27169;&#22411;&#20013;&#21487;&#20197;&#34987;&#32469;&#36807;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#21487;&#20197;&#20351;&#29992;&#22810;&#24230;&#37327;&#26631;&#20934;&#35843;&#26597;&#26469;&#26174;&#33879;&#25552;&#39640; FL &#23545;&#24378;&#36866;&#24212;&#23545;&#25163;&#30340;&#38887;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#25110;&#27169;&#22411;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) trains machine learning models on data distributed across multiple devices, avoiding data transfer to a central location. This improves privacy, reduces communication costs, and enhances model performance. However, FL is prone to poisoning attacks, which can be untargeted aiming to reduce the model performance, or targeted, so-called backdoors, which add adversarial behavior that can be triggered with appropriately crafted inputs. Striving for stealthiness, backdoor attacks are harder to deal with.  Mitigation techniques against poisoning attacks rely on monitoring certain metrics and filtering malicious model updates. However, previous works didn't consider real-world adversaries and data distributions. To support our statement, we define a new notion of strong adaptive adversaries that can simultaneously adapt to multiple objectives and demonstrate through extensive tests, that existing defense methods can be circumvented in this adversary model. We also demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20195;&#25968;&#35780;&#20272;&#22120;&#26469;&#20272;&#35745;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#26377;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;&#31532;&#20108;&#31181;&#35780;&#20272;&#22120;&#30340;&#27491;&#30830;&#24615;&#34987;&#20445;&#35777;&#12290;&#20316;&#32773;&#36890;&#36807;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#22120;&#26080;&#27861;&#36820;&#22238;&#21512;&#29702;&#20272;&#35745;&#30340;&#22833;&#36133;&#65292;&#32531;&#35299;&#20102;&#22996;&#25176;/&#20195;&#29702;&#30417;&#25511;&#24726;&#35770;&#65292;&#24182;&#36890;&#36807;&#25628;&#32034;&#26469;&#23547;&#25214;&#20960;&#20046;&#26080;&#35823;&#24046;&#30340;&#19977;&#20803;&#32452;&#12290;</title><link>http://arxiv.org/abs/2306.01726</link><description>&lt;p&gt;
&#35780;&#20272;&#26377;&#22122;&#22768;&#21028;&#21035;&#22120;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#27969;&#24335;&#31639;&#27861; -- &#20108;&#20803;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification. (arXiv:2306.01726v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20195;&#25968;&#35780;&#20272;&#22120;&#26469;&#20272;&#35745;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#26377;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;&#31532;&#20108;&#31181;&#35780;&#20272;&#22120;&#30340;&#27491;&#30830;&#24615;&#34987;&#20445;&#35777;&#12290;&#20316;&#32773;&#36890;&#36807;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#22120;&#26080;&#27861;&#36820;&#22238;&#21512;&#29702;&#20272;&#35745;&#30340;&#22833;&#36133;&#65292;&#32531;&#35299;&#20102;&#22996;&#25176;/&#20195;&#29702;&#30417;&#25511;&#24726;&#35770;&#65292;&#24182;&#36890;&#36807;&#25628;&#32034;&#26469;&#23547;&#25214;&#20960;&#20046;&#26080;&#35823;&#24046;&#30340;&#19977;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#26377;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#35780;&#20272;&#20316;&#20026;&#27969;&#24335;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;: &#32473;&#23450;&#19968;&#20010;&#20998;&#31867;&#22120;&#20915;&#31574;&#30340;&#25968;&#25454;&#33609;&#22270;&#65292;&#20272;&#35745;&#26631;&#31614;&#30340;&#30495;&#23454;&#27969;&#34892;&#24230;&#20197;&#21450;&#27599;&#20010;&#20998;&#31867;&#22120;&#23545;&#23427;&#20204;&#30340;&#20934;&#30830;&#24230;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#20004;&#31181;&#23436;&#20840;&#20195;&#25968;&#21270;&#30340;&#35780;&#20272;&#22120;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20004;&#31181;&#35780;&#20272;&#22120;&#37117;&#22522;&#20110;&#20998;&#31867;&#22120;&#20135;&#29983;&#29420;&#31435;&#38169;&#35823;&#30340;&#20551;&#35774;&#12290;&#31532;&#19968;&#31181;&#26159;&#22522;&#20110;&#22810;&#25968;&#25237;&#31080;&#30340;&#12290;&#32780;&#31532;&#20108;&#31181;&#21017;&#26159;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#24182;&#34987;&#20445;&#35777;&#26159;&#27491;&#30830;&#30340;&#12290;&#20294;&#26159;&#22914;&#20309;&#30830;&#20445;&#20998;&#31867;&#22120;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#27979;&#35797;&#20013;&#26159;&#29420;&#31435;&#30340;&#21602;&#65311;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#29420;&#31435;&#35780;&#20272;&#22120;&#26080;&#27861;&#36820;&#22238;&#21512;&#29702;&#20272;&#35745;&#30340;&#22833;&#36133;&#26469;&#32531;&#35299;&#36825;&#20010;&#22996;&#25176;/&#20195;&#29702;&#30417;&#25511;&#24726;&#35770;&#12290;&#36890;&#36807;&#21033;&#29992;&#20195;&#25968;&#25925;&#38556;&#27169;&#24335;&#26469;&#25298;&#32477;&#22826;&#30456;&#20851;&#30340;&#35780;&#20272;&#38598;&#21512;&#65292;&#20351;&#29992; \texttt{adult}&#65292;\texttt{mushroom} &#21644; \texttt{two-norm} &#25968;&#25454;&#38598;&#23545;&#19968;&#32452;&#20960;&#20046;&#26080;&#35823;&#24046;&#19977;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#35777;&#25628;&#32034;&#12290;&#36825;&#20123;&#25628;&#32034;&#36890;&#36807;&#26500;&#24314;&#35780;&#20272;&#31354;&#38388;&#20013;&#30340;&#34920;&#38754;&#26469;&#36827;&#34892;&#31934;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of noisy binary classifiers on unlabeled data is treated as a streaming task: given a data sketch of the decisions by an ensemble, estimate the true prevalence of the labels as well as each classifier's accuracy on them. Two fully algebraic evaluators are constructed to do this. Both are based on the assumption that the classifiers make independent errors. The first is based on majority voting. The second, the main contribution of the paper, is guaranteed to be correct. But how do we know the classifiers are independent on any given test? This principal/agent monitoring paradox is ameliorated by exploiting the failures of the independent evaluator to return sensible estimates. A search for nearly error independent trios is empirically carried out on the \texttt{adult}, \texttt{mushroom}, and \texttt{two-norm} datasets by using the algebraic failure modes to reject evaluation ensembles as too correlated. The searches are refined by constructing a surface in evaluation spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#32500;&#25968;&#23398;&#27169;&#22411;&#36827;&#34892;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#38081;&#38034;&#28082;&#27969;&#30005;&#27744;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#27491;&#30830;&#39044;&#27979;&#21333;&#20803;&#30005;&#21387;&#12290;</title><link>http://arxiv.org/abs/2306.01010</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#32500;&#21333;&#20803;&#27169;&#22411;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#38081;&#38034;&#28082;&#27969;&#30005;&#27744;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Physics-informed machine learning of redox flow battery based on a two-dimensional unit cell model. (arXiv:2306.01010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#32500;&#25968;&#23398;&#27169;&#22411;&#36827;&#34892;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#38081;&#38034;&#28082;&#27969;&#30005;&#27744;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#27491;&#30830;&#39044;&#27979;&#21333;&#20803;&#30005;&#21387;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;(PINN)&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#20108;&#32500;&#25968;&#23398;&#27169;&#22411;&#26469;&#39044;&#27979;&#20840;&#38034;&#28082;&#27969;&#30005;&#27744;&#30340;&#24615;&#33021;&#65292;&#24182;&#24378;&#21046;&#25191;&#34892;&#20854;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;&#35813;&#20108;&#32500;&#27169;&#22411;&#21253;&#25324;6&#20010;&#25511;&#21046;&#26041;&#31243;&#21644;24&#20010;&#36793;&#30028;&#26465;&#20214;&#65292;&#35814;&#32454;&#25551;&#36848;&#20102;&#38081;&#38034;&#28082;&#27969;&#30005;&#27744;&#20869;&#21457;&#29983;&#30340;&#30005;&#21270;&#23398;&#21453;&#24212;&#12289;&#36136;&#37327;&#20256;&#36755;&#21644;&#27969;&#20307;&#21147;&#23398;&#36807;&#31243;&#12290;&#20026;&#20102;&#20351;&#29992;PINN&#26041;&#27861;&#35299;&#20915;2D&#27169;&#22411;&#65292;&#37319;&#29992;&#22797;&#21512;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#29289;&#31181;&#27987;&#24230;&#21644;&#30005;&#20301;&#65307;&#26681;&#25454;&#30005;&#27744;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#23545;&#36755;&#20837;&#21644;&#36755;&#20986;&#36827;&#34892;&#35268;&#33539;&#21270;&#22788;&#29702;&#65307;&#23558;&#22788;&#29702;&#21518;&#30340;&#25511;&#21046;&#26041;&#31243;&#21644;&#36793;&#30028;&#26465;&#20214;&#20808;&#32553;&#25918;&#21040;1&#30340;&#25968;&#37327;&#32423;&#65292;&#28982;&#21518;&#37319;&#29992;&#33258;&#37325;&#27861;&#36827;&#19968;&#27493;&#24179;&#34913;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;PINN&#33021;&#22815;&#27491;&#30830;&#22320;&#39044;&#27979;&#21333;&#20803;&#30005;&#21387;&#65292;&#20294;&#30005;&#20301;&#30340;&#39044;&#27979;&#20855;&#26377;&#31867;&#20284;&#20110;&#24120;&#25968;&#30340;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a physics-informed neural network (PINN) approach for predicting the performance of an all-vanadium redox flow battery, with its physics constraints enforced by a two-dimensional (2D) mathematical model. The 2D model, which includes 6 governing equations and 24 boundary conditions, provides a detailed representation of the electrochemical reactions, mass transport and hydrodynamics occurring inside the redox flow battery. To solve the 2D model with the PINN approach, a composite neural network is employed to approximate species concentration and potentials; the input and output are normalized according to prior knowledge of the battery system; the governing equations and boundary conditions are first scaled to an order of magnitude around 1, and then further balanced with a self-weighting method. Our numerical results show that the PINN is able to predict cell voltage correctly, but the prediction of potentials shows a constant-like shift. To fix the shift, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#24182;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#65292;&#20063;&#32473;&#20986;&#20102;&#19968;&#20123;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#26356;&#39640;&#30340;$k-$WL&#21464;&#20307;&#21644;&#20998;&#35010;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.19659</link><description>&lt;p&gt;
&#21033;&#29992;&#23616;&#37096;&#21270;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Expressivity of Graph Neural Networks using Localization. (arXiv:2305.19659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#24182;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#65292;&#20063;&#32473;&#20986;&#20102;&#19968;&#20123;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#26356;&#39640;&#30340;$k-$WL&#21464;&#20307;&#21644;&#20998;&#35010;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#26088;&#22312;&#22686;&#21152;&#34920;&#36798;&#33021;&#21147;&#24182;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#65292;&#24182;&#20026;&#20219;&#24847;$k$&#32473;&#20986;$k-$WL&#30340;&#23616;&#37096;&#29256;&#26412;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;Local $k-$WL&#30340;&#20316;&#29992;&#65292;&#24182;&#35777;&#26126;&#20854;&#27604;$k-$WL&#26356;&#20855;&#34920;&#29616;&#21147;&#65292;&#24182;&#19988;&#33267;&#22810;&#19982;$(k+1)-$WL&#19968;&#26679;&#20855;&#26377;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20123;&#27169;&#24335;&#30340;&#29305;&#24449;&#65292;&#22914;&#26524;&#20004;&#20010;&#22270;&#26159;Local $k-$WL&#31561;&#20215;&#30340;&#65292;&#21017;&#23427;&#20204;&#30340;&#23376;&#22270;&#21644;&#35825;&#23548;&#23376;&#22270;&#30340;&#35745;&#25968;&#26159;&#19981;&#21464;&#30340;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;$k-$WL&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#23618;$k-$WL&#21644;&#36882;&#24402;$k-$WL&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#27604;&#22312;&#25972;&#20010;&#22270;&#19978;&#24212;&#29992;$k-$WL&#26356;&#39640;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35010;&#25216;&#26415;&#65292;&#20351;&#29992;$1-$WL&#21363;&#21487;&#20445;&#35777;&#25152;&#26377;&#22823;&#23567;&#19981;&#36229;&#36807;4&#30340;&#35825;&#23548;&#23376;&#22270;&#30340;&#20934;&#30830;&#35745;&#25968;&#12290;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;$k&gt;1$&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#36824;&#23558;Local $k-$WL&#30340;&#34920;&#29616;&#21147;&#19982;&#20854;&#20182;GNN&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose localized versions of Weisfeiler-Leman (WL) algorithms in an effort to both increase the expressivity, as well as decrease the computational overhead. We focus on the specific problem of subgraph counting and give localized versions of $k-$WL for any $k$. We analyze the power of Local $k-$WL and prove that it is more expressive than $k-$WL and at most as expressive as $(k+1)-$WL. We give a characterization of patterns whose count as a subgraph and induced subgraph are invariant if two graphs are Local $k-$WL equivalent. We also introduce two variants of $k-$WL: Layer $k-$WL and recursive $k-$WL. These methods are more time and space efficient than applying $k-$WL on the whole graph. We also propose a fragmentation technique that guarantees the exact count of all induced subgraphs of size at most 4 using just $1-$WL. The same idea can be extended further for larger patterns using $k&gt;1$. We also compare the expressive power of Local $k-$WL with other GNN hierarc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;"Photo-zSNthesis"&#65292;&#29992;&#20110;&#20174;&#22810;&#27874;&#27573;&#36229;&#26032;&#26143;&#20809;&#21464;&#26354;&#32447;&#39044;&#27979;&#23436;&#25972;&#30340;&#32418;&#31227;&#27010;&#29575;&#20998;&#24067;&#65292;&#26080;&#38656;&#20809;&#35889;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#35266;&#27979;&#20013;&#37117;&#21462;&#24471;&#20102;&#37325;&#22823;&#25552;&#21319;&#65292;&#33021;&#26497;&#22823;&#22320;&#32422;&#26463;&#23431;&#23449;&#23398;&#12290;</title><link>http://arxiv.org/abs/2305.11869</link><description>&lt;p&gt;
Photo-zSNthesis: &#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;Ia&#22411;&#36229;&#26032;&#26143;&#20809;&#21464;&#26354;&#32447;&#36716;&#21270;&#20026;&#32418;&#31227;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Photo-zSNthesis: Converting Type Ia Supernova Lightcurves to Redshift Estimates via Deep Learning. (arXiv:2305.11869v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;"Photo-zSNthesis"&#65292;&#29992;&#20110;&#20174;&#22810;&#27874;&#27573;&#36229;&#26032;&#26143;&#20809;&#21464;&#26354;&#32447;&#39044;&#27979;&#23436;&#25972;&#30340;&#32418;&#31227;&#27010;&#29575;&#20998;&#24067;&#65292;&#26080;&#38656;&#20809;&#35889;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#35266;&#27979;&#20013;&#37117;&#21462;&#24471;&#20102;&#37325;&#22823;&#25552;&#21319;&#65292;&#33021;&#26497;&#22823;&#22320;&#32422;&#26463;&#23431;&#23449;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#23558;&#21040;&#26469;&#30340;&#20809;&#24230;&#24033;&#22825;&#23558;&#20250;&#21457;&#29616;&#25968;&#20197;&#19975;&#35745;&#30340;Ia&#22411;&#36229;&#26032;&#26143;(SNe Ia)&#65292;&#36828;&#36828;&#36229;&#36807;&#20102;&#25105;&#20204;&#30340;&#20809;&#35889;&#36164;&#28304;&#23481;&#37327;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#20809;&#35889;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#35266;&#27979;&#25968;&#25454;&#30340;&#31185;&#23398;&#22238;&#25253;&#65292;&#25105;&#20204;&#24517;&#39035;&#21482;&#21033;&#29992;&#20809;&#24230;&#20449;&#24687;&#20934;&#30830;&#22320;&#25552;&#21462;&#20851;&#38190;&#21442;&#25968;&#65292;&#20363;&#22914;SN&#32418;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;Photo-zSNthesis&#65292;&#29992;&#20110;&#20174;&#22810;&#27874;&#27573;&#36229;&#26032;&#26143;&#20809;&#21464;&#26354;&#32447;&#39044;&#27979;&#23436;&#25972;&#30340;&#32418;&#31227;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#22312;&#27169;&#25311;&#30340;SDSS&#21644;LSST&#25968;&#25454;&#20197;&#21450;&#35266;&#27979;&#21040;&#30340;SDSS SNe&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#30340;&#39044;&#27979;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#26080;&#35770;&#26159;&#22312;&#27169;&#25311;&#36824;&#26159;&#23454;&#38469;&#35266;&#27979;&#20013;&#65292;&#19988;&#23384;&#22312;&#26497;&#23569;&#30340;&#32418;&#31227;&#30456;&#20851;&#20559;&#24046;&#65292;&#36825;&#26159;&#30001;&#20110;&#36873;&#25321;&#25928;&#24212;(&#20363;&#22914;Malmquist&#20559;&#24046;)&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#33021;&#22815;&#24471;&#21040;&#24456;&#22909;&#30340;&#38480;&#21046;&#65292;&#24182;&#23558;&#26368;&#22823;&#31243;&#24230;&#22320;&#32422;&#26463;&#23431;&#23449;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Upcoming photometric surveys will discover tens of thousands of Type Ia supernovae (SNe Ia), vastly outpacing the capacity of our spectroscopic resources. In order to maximize the science return of these observations in the absence of spectroscopic information, we must accurately extract key parameters, such as SN redshifts, with photometric information alone. We present Photo-zSNthesis, a convolutional neural network-based method for predicting full redshift probability distributions from multi-band supernova lightcurves, tested on both simulated Sloan Digital Sky Survey (SDSS) and Vera C. Rubin Legacy Survey of Space and Time (LSST) data as well as observed SDSS SNe. We show major improvements over predictions from existing methods on both simulations and real observations as well as minimal redshift-dependent bias, which is a challenge due to selection effects, e.g. Malmquist bias. The PDFs produced by this method are well-constrained and will maximize the cosmological constraining 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10502</link><description>&lt;p&gt;
EENED&#65306;&#22522;&#20110;&#21367;&#31215;&#21464;&#21387;&#22120;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
EENED: End-to-End Neural Epilepsy Detection based on Convolutional Transformer. (arXiv:2305.10502v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;EEG&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#65288;Transformer&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#65292;&#32780;CNN&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#22914;&#38191;&#40831;&#27874;&#20043;&#31867;&#30340;&#23616;&#37096;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#22312;Transformer&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#21367;&#31215;&#27169;&#22359;&#65292;EENED&#21487;&#20197;&#23398;&#20064;&#24739;&#32773;EEG&#20449;&#21495;&#29305;&#24449;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#27880;&#24847;&#21040;&#19982;&#30315;&#30187;&#23494;&#20999;&#30456;&#20851;&#30340;&#23616;&#37096;EEG&#24322;&#24120;&#31361;&#21464;&#65292;&#22914;&#23574;&#38160;&#27874;&#30340;&#20986;&#29616;&#21644;&#32531;&#24930;&#27874;&#30340;&#25955;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;Transformer&#21644;CNN&#25429;&#25417;EEG&#20449;&#21495;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#23558;&#24456;&#24555;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Transformer and Convolution neural network (CNN) based models have shown promising results in EEG signal processing. Transformer models can capture the global dependencies in EEG signals through a self-attention mechanism, while CNN models can capture local features such as sawtooth waves. In this work, we propose an end-to-end neural epilepsy detection model, EENED, that combines CNN and Transformer. Specifically, by introducing the convolution module into the Transformer encoder, EENED can learn the time-dependent relationship of the patient's EEG signal features and notice local EEG abnormal mutations closely related to epilepsy, such as the appearance of spikes and the sprinkling of sharp and slow waves. Our proposed framework combines the ability of Transformer and CNN to capture different scale features of EEG signals and holds promise for improving the accuracy and reliability of epilepsy detection. Our source code will be released soon on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39564;&#35777;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#26131;&#20110;&#39564;&#35777;&#30340;&#38480;&#21046;&#27169;&#22411;&#31867;&#26469;&#35299;&#20915;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340; NP-hard &#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#35774;&#35745;&#20986;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#20197;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#65292;&#32780;&#19988;&#20173;&#20445;&#25345;&#30528;&#35813;&#39046;&#22495;&#26368;&#22909;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03626</link><description>&lt;p&gt;
&#40065;&#26834;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340;&#21487;&#39564;&#35777;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Verifiable Learning for Robust Tree Ensembles. (arXiv:2305.03626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39564;&#35777;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#26131;&#20110;&#39564;&#35777;&#30340;&#38480;&#21046;&#27169;&#22411;&#31867;&#26469;&#35299;&#20915;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340; NP-hard &#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#35774;&#35745;&#20986;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#20197;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#65292;&#32780;&#19988;&#20173;&#20445;&#25345;&#30528;&#35813;&#39046;&#22495;&#26368;&#22909;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#30830;&#23450;&#65292;&#23545;&#20110;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159; NP-hard &#65292;&#22240;&#27492;&#23545;&#20110;&#29305;&#23450;&#30340;&#36755;&#20837;&#26469;&#35828;&#26159;&#19981;&#21487;&#35299;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31867;&#21463;&#38480;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#31216;&#20026; large-spread &#38598;&#25104;&#65292;&#20854;&#20801;&#35768;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#23433;&#20840;&#39564;&#35777;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#39564;&#35777;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20513;&#23548;&#35757;&#32451;&#36825;&#31181;&#26131;&#20110;&#39564;&#35777;&#30340;&#21463;&#38480;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064; large-spread &#20915;&#31574;&#26641;&#38598;&#25104;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#30410;&#22788;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#12290;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#31639;&#27861;&#35757;&#32451;&#30340; large-spread &#38598;&#25104;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#20351;&#29992;&#26631;&#20934;&#21322;&#23450;&#32534;&#31243;&#27714;&#35299;&#22120;&#36827;&#34892;&#39564;&#35777;&#65292;&#21516;&#26102;&#23545;&#25239;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verifying the robustness of machine learning models against evasion attacks at test time is an important research problem. Unfortunately, prior work established that this problem is NP-hard for decision tree ensembles, hence bound to be intractable for specific inputs. In this paper, we identify a restricted class of decision tree ensembles, called large-spread ensembles, which admit a security verification algorithm running in polynomial time. We then propose a new approach called verifiable learning, which advocates the training of such restricted model classes which are amenable for efficient verification. We show the benefits of this idea by designing a new training algorithm that automatically learns a large-spread decision tree ensemble from labelled data, thus enabling its security verification in polynomial time. Experimental results on publicly available datasets confirm that large-spread ensembles trained using our algorithm can be verified in a matter of seconds, using stand
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#23637;&#24179;&#27969;&#24418;&#30340;&#31639;&#27861;FlatNet&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01777</link><description>&lt;p&gt;
&#36890;&#36807;&#27969;&#24418;&#23637;&#24179;&#21644;&#37325;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning via Manifold Flattening and Reconstruction. (arXiv:2305.01777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#23637;&#24179;&#27969;&#24418;&#30340;&#31639;&#27861;FlatNet&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#27969;&#24418;&#30340;&#26377;&#38480;&#26679;&#26412;&#20013;&#26174;&#24335;&#26500;&#24314;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#32447;&#24615;&#21270;&#21644;&#37325;&#26500;&#23884;&#20837;&#23376;&#27969;&#24418;&#12290;&#25105;&#20204;&#25152;&#29983;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#31216;&#20026;&#23637;&#24179;&#32593;&#32476;&#65288;FlatNet&#65289;&#65292;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#35745;&#31639;&#19978;&#21487;&#25193;&#23637;&#24615;&#24378;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36825;&#31181;&#24179;&#34913;&#36890;&#24120;&#22312;&#22522;&#20110;&#27969;&#24418;&#30340;&#23398;&#20064;&#26041;&#27861;&#20013;&#38590;&#20197;&#23454;&#29616;&#12290;&#25105;&#20204;&#22522;&#20110;&#21512;&#25104;&#30340;&#39640;&#32500;&#27969;&#24418;&#25968;&#25454;&#21644;2D&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#65292;&#24182;&#19982;&#20854;&#20182;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes an algorithm for explicitly constructing a pair of neural networks that linearize and reconstruct an embedded submanifold, from finite samples of this manifold. Our such-generated neural networks, called flattening networks (FlatNet), are theoretically interpretable, computationally feasible at scale, and generalize well to test data, a balance not typically found in manifold-based learning methods. We present empirical results and comparisons to other models on synthetic high-dimensional manifold data and 2D image data. Our code is publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#20840;&#23616;&#23884;&#20837;&#19982;&#23616;&#37096;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#65292;&#20445;&#35777;&#20102;&#36739;&#39640;&#30340;&#21534;&#21520;&#37327;&#21644;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13800</link><description>&lt;p&gt;
&#28508;&#25351;&#32441;&#35782;&#21035;&#65306;&#23616;&#37096;&#21644;&#20840;&#23616;&#23884;&#20837;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Latent Fingerprint Recognition: Fusion of Local and Global Embeddings. (arXiv:2304.13800v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#20840;&#23616;&#23884;&#20837;&#19982;&#23616;&#37096;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#65292;&#20445;&#35777;&#20102;&#36739;&#39640;&#30340;&#21534;&#21520;&#37327;&#21644;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25351;&#32441;&#35782;&#21035;&#20013;&#65292;&#19968;&#20010;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#30830;&#23450;&#19982;&#29359;&#32618;&#29616;&#22330;&#30041;&#19979;&#30340;&#37096;&#20998;&#21644;&#27169;&#31946;&#30340;&#25351;&#32441;&#65288;&#21363;&#28508;&#25351;&#32441;&#25110;&#25351;&#32441;&#30165;&#36857;&#65289;&#30456;&#20851;&#32852;&#30340;&#23244;&#30097;&#20154;&#36523;&#20221;&#12290;&#26412;&#25991;&#23558;&#20840;&#23616;&#23884;&#20837;&#19982;&#23616;&#37096;&#23884;&#20837;&#30456;&#32467;&#21512;&#65292;&#20026;&#28508;&#22312;&#21367;&#31215;&#21644;&#25293;&#25171;&#25351;&#32441;&#21305;&#37197;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;&#36825;&#31181;&#23616;&#37096;&#21644;&#20840;&#23616;&#34920;&#31034;&#30340;&#32452;&#21512;&#20351;&#35782;&#21035;&#20934;&#30830;&#29575;&#22312;NIST SD 27&#12289;NIST SD 302&#12289;MSP&#12289;MOLF DB1/DB4&#21644;MOLF DB2/DB4&#28508;&#25351;&#32441;&#25968;&#25454;&#38598;&#19978;&#37117;&#24471;&#21040;&#20102;&#26174;&#30528;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most challenging problems in fingerprint recognition continues to be establishing the identity of a suspect associated with partial and smudgy fingerprints left at a crime scene (i.e., latent prints or fingermarks). Despite the success of fixed-length embeddings for rolled and slap fingerprint recognition, the features learned for latent fingerprint matching have mostly been limited to local minutiae-based embeddings and have not directly leveraged global representations for matching. In this paper, we combine global embeddings with local embeddings for state-of-the-art latent to rolled matching accuracy with high throughput. The combination of both local and global representations leads to improved recognition accuracy across NIST SD 27, NIST SD 302, MSP, MOLF DB1/DB4, and MOLF DB2/DB4 latent fingerprint datasets for both closed-set (84.11%, 54.36%, 84.35%, 70.43%, 62.86% rank-1 retrieval rate, respectively) and open-set (0.50, 0.74, 0.44, 0.60, 0.68 FNIR at FPIR=0.02, resp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21028;&#26029;&#36136;&#25968;&#25972;&#38500;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20851;&#38190;&#22312;&#20110;&#25552;&#20379;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#21830;&#19994;&#21487;&#29992;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26080;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#38656;&#35201;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.01333</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21028;&#26029;&#36136;&#25968;&#25972;&#38500;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Prime Number Divisibility by Deep Learning. (arXiv:2304.01333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21028;&#26029;&#36136;&#25968;&#25972;&#38500;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20851;&#38190;&#22312;&#20110;&#25552;&#20379;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#21830;&#19994;&#21487;&#29992;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26080;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#38656;&#35201;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30830;&#23450;&#19968;&#20010;&#25972;&#25968;&#26159;&#21542;&#33021;&#22815;&#34987;2&#12289;3&#25110;&#20854;&#20182;&#36136;&#25968;&#25972;&#38500;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#21487;&#33021;&#24456;&#31616;&#21333;&#65292;&#20294;&#22312;&#27809;&#26377;&#39044;&#20808;&#25351;&#23450;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#23545;&#20110;&#35745;&#31639;&#26426;&#26469;&#35828;&#21487;&#33021;&#24182;&#19981;&#23481;&#26131;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#21644;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#30830;&#23450;&#22823;&#26377;&#38480;&#25972;&#25968;&#65288;&#39640;&#36798;$2^{32}$&#65289;&#26159;&#21542;&#33021;&#22815;&#34987;&#23567;&#36136;&#25968;&#25972;&#38500;&#30340;&#24773;&#20917;&#19979;&#65292;&#21508;&#31181;&#26694;&#26550;&#21644;&#32593;&#32476;&#32467;&#26500;&#65288;CNN&#12289;RNN&#12289;Transformer&#31561;&#65289;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#27979;&#36136;&#25968;&#25972;&#38500;&#24615;&#30340;&#33021;&#21147;&#26497;&#22823;&#22320;&#21462;&#20915;&#20110;&#25552;&#20379;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#32593;&#32476;&#26694;&#26550;&#25110;&#32593;&#32476;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65288;CNN&#12289;RNN&#12289;Transformer&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#26469;&#33258;&#20122;&#39532;&#36874;&#12289;&#35895;&#27468;&#21644;&#24494;&#36719;&#30340;&#21830;&#19994;&#21487;&#29992;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#31649;&#36947;&#65292;&#24182;&#35777;&#26126;&#38500;&#38750;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#24037;&#31243;&#65292;&#21542;&#21017;&#23427;&#20204;&#26080;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certain tasks such as determining whether a given integer can be divided by 2, 3, or other prime numbers may be trivial for human beings, but can be less straightforward for computers in the absence of pre-specified algorithms. In this paper, we tested multiple deep learning architectures and feature engineering approaches, and evaluated the scenario of determining divisibility of large finite integers (up to $2^{32}$) by small prime numbers. It turns out that, regardless of the network frameworks or the complexity of the network structures (CNN, RNN, Transformer, etc.), the ability to predict the prime number divisibility critically depends on the feature space fed into the deep learning models. We also evaluated commercially available Automated Machine Learning (AutoML) pipelines from Amazon, Google and Microsoft, and demonstrated that they failed to address this issue unless appropriately engineered features were provided. We further proposed a closed form solution to the problem us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#8212;&#8212;&#36719;Bellman&#24179;&#34913;&#65292;&#35299;&#20915;&#20102;&#20223;&#23556;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#30340;&#22810;&#20010;&#29609;&#23478;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#26469;&#35745;&#31639;&#27492;&#24179;&#34913;&#65292;&#21516;&#26102;&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#35299;&#20915;&#25512;&#26029;&#29609;&#23478;&#22870;&#21169;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00163</link><description>&lt;p&gt;
&#20223;&#23556;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#30340;&#36719;Bellman&#24179;&#34913;&#65306;&#21069;&#21521;&#35299;&#19982;&#36870;&#21521;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Soft-Bellman Equilibrium in Affine Markov Games: Forward Solutions and Inverse Learning. (arXiv:2304.00163v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#8212;&#8212;&#36719;Bellman&#24179;&#34913;&#65292;&#35299;&#20915;&#20102;&#20223;&#23556;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#30340;&#22810;&#20010;&#29609;&#23478;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#26469;&#35745;&#31639;&#27492;&#24179;&#34913;&#65292;&#21516;&#26102;&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#35299;&#20915;&#25512;&#26029;&#29609;&#23478;&#22870;&#21169;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#22312;&#38543;&#26426;&#21160;&#24577;&#29615;&#22659;&#20013;&#27169;&#25311;&#22810;&#20010;&#29609;&#23478;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#27599;&#20010;&#29609;&#23478;&#22312;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#26368;&#22823;&#21270;&#20854;&#26399;&#26395;&#30340;&#24635;&#25240;&#29616;&#22870;&#21169;&#65292;&#35813;&#22870;&#21169;&#21462;&#20915;&#20110;&#20854;&#20182;&#29609;&#23478;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#31216;&#20026;&#20223;&#23556;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#22312;&#20854;&#20013;&#65292;&#20223;&#23556;&#22870;&#21169;&#20989;&#25968;&#32806;&#21512;&#20102;&#29609;&#23478;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#21363;&#36719;Bellman&#24179;&#34913;&#65292;&#22312;&#20854;&#20013;&#65292;&#27599;&#20010;&#29609;&#23478;&#37117;&#26159;&#26377;&#38480;&#29702;&#24615;&#30340;&#65292;&#24182;&#36873;&#25321;&#36719;Bellman&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#20687;&#33879;&#21517;&#30340;Nash&#24179;&#34913;&#27010;&#24565;&#20013;&#37027;&#26679;&#36873;&#25321;&#32431;&#29702;&#24615;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36719;Bellman&#24179;&#34913;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#26469;&#35745;&#31639;&#21069;&#21521;&#38382;&#39064;&#20013;&#30340;&#36825;&#31181;&#24179;&#34913;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#35299;&#20915;&#20102;&#25512;&#26029;&#29609;&#23478;&#22870;&#21169;&#21442;&#25968;&#30340;&#36870;&#21521;&#21338;&#24328;&#38382;&#39064;&#12290;&#22312;&#25504;&#39135;&#32773;-&#29454;&#29289;OpenAI Gym&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#36719;Bellman&#31574;&#30053;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25511;&#21046;&#25504;&#39135;&#32773;&#21644;&#29454;&#29289;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov games model interactions among multiple players in a stochastic, dynamic environment. Each player in a Markov game maximizes its expected total discounted reward, which depends upon the policies of the other players. We formulate a class of Markov games, termed affine Markov games, where an affine reward function couples the players' actions. We introduce a novel solution concept, the soft-Bellman equilibrium, where each player is boundedly rational and chooses a soft-Bellman policy rather than a purely rational policy as in the well-known Nash equilibrium concept. We provide conditions for the existence and uniqueness of the soft-Bellman equilibrium and propose a nonlinear least squares algorithm to compute such an equilibrium in the forward problem. We then solve the inverse game problem of inferring the players' reward parameters from observed state-action trajectories via a projected gradient algorithm. Experiments in a predator-prey OpenAI Gym environment show that the rewa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#28982;&#20027;&#20041;&#21608;&#36793;&#24863;&#30693;&#30740;&#31350;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39550;&#39542;&#21592;&#29305;&#24449;&#35782;&#21035;&#21644;&#36125;&#21494;&#26031;&#24037;&#20316;&#36127;&#33655;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39550;&#39542;&#24615;&#33021;&#25968;&#25454;&#36827;&#34892;&#30417;&#27979;&#21644;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#39550;&#39542;&#21592;&#24037;&#20316;&#36127;&#33655;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.14720</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#20027;&#20041;&#21608;&#36793;&#24863;&#30693;&#30740;&#31350;&#25968;&#25454;&#30340;&#39550;&#39542;&#21592;&#29305;&#24449;&#35782;&#21035;&#19982;&#36125;&#21494;&#26031;&#24037;&#20316;&#36127;&#33655;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Driver Profiling and Bayesian Workload Estimation Using Naturalistic Peripheral Detection Study Data. (arXiv:2303.14720v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#28982;&#20027;&#20041;&#21608;&#36793;&#24863;&#30693;&#30740;&#31350;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39550;&#39542;&#21592;&#29305;&#24449;&#35782;&#21035;&#21644;&#36125;&#21494;&#26031;&#24037;&#20316;&#36127;&#33655;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39550;&#39542;&#24615;&#33021;&#25968;&#25454;&#36827;&#34892;&#30417;&#27979;&#21644;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#39550;&#39542;&#21592;&#24037;&#20316;&#36127;&#33655;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#24037;&#20316;&#36127;&#33655;&#26377;&#21161;&#20110;&#19982;&#36710;&#20869;&#20449;&#24687;&#31995;&#32479;&#36827;&#34892;&#23433;&#20840;&#20114;&#21160;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#20943;&#23569;&#23545;&#39550;&#39542;&#20027;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#36890;&#36807;&#39550;&#39542;&#24615;&#33021;&#25968;&#25454;&#23454;&#29616;&#20102;&#24037;&#20316;&#36127;&#33655;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#36890;&#36807;&#25913;&#36827;&#30340;&#21608;&#36793;&#24863;&#30693;&#20219;&#21153;&#25910;&#38598;&#20027;&#35266;&#24037;&#20316;&#36127;&#33655;&#25968;&#25454;&#30340;&#26032;&#22411;&#23454;&#22320;&#30740;&#31350;&#12290;&#36890;&#36807;&#35270;&#39057;&#20998;&#26512;&#30830;&#23450;&#35825;&#21457;&#39640;&#24515;&#29702;&#24037;&#20316;&#36127;&#33655;&#30340;&#20851;&#38190;&#29615;&#22659;&#22240;&#32032;&#65292;&#22914;&#20132;&#21449;&#36335;&#21475;&#21644;&#21069;&#26041;&#36710;&#36742;&#34892;&#20026;&#12290;&#20854;&#27425;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#25442;&#25216;&#26415;&#65289;&#30340;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#39550;&#39542;&#21592;&#22312;&#34892;&#31243;&#20013;&#24179;&#22343;&#32463;&#21382;&#30340;&#24037;&#20316;&#36127;&#33655;&#29305;&#24449;&#26469;&#36827;&#34892;&#39550;&#39542;&#21592;&#29305;&#24449;&#35782;&#21035;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#28388;&#27874;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#25110;&#25509;&#36817;&#23454;&#26102;&#22320;&#39034;&#24207;&#20272;&#35745;&#39550;&#39542;&#21592;&#30340;&#30636;&#26102;&#24037;&#20316;&#36127;&#33655;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring drivers' mental workload facilitates initiating and maintaining safe interactions with in-vehicle information systems, and thus delivers adaptive human machine interaction with reduced impact on the primary task of driving. In this paper, we tackle the problem of workload estimation from driving performance data. First, we present a novel on-road study for collecting subjective workload data via a modified peripheral detection task in naturalistic settings. Key environmental factors that induce a high mental workload are identified via video analysis, e.g. junctions and behaviour of vehicle in front. Second, a supervised learning framework using state-of-the-art time series classifiers (e.g. convolutional neural network and transform techniques) is introduced to profile drivers based on the average workload they experience during a journey. A Bayesian filtering approach is then proposed for sequentially estimating, in (near) real-time, the driver's instantaneous workload. Th
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#23558;&#24515;&#34880;&#31649;&#23454;&#39564;&#23460;&#25351;&#26631;&#30340;&#24739;&#32773;&#36827;&#23637;&#36235;&#21183;&#20174;&#24120;&#35265;&#24773;&#20917;&#36716;&#31227;&#21040;&#32597;&#35265;&#25110;&#29305;&#23450;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#20013;&#65292;&#20197;&#21327;&#21161;&#26816;&#27979;&#32463;&#30382;&#20896;&#29366;&#21160;&#33033;&#20171;&#20837;&#27835;&#30103;&#24739;&#32773;&#30340;&#38774;&#34880;&#31649;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.06980</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#36890;&#29992;&#23454;&#39564;&#23460;&#36827;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-supervised based general laboratory progress pretrained model for cardiovascular event detection. (arXiv:2303.06980v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06980
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#23558;&#24515;&#34880;&#31649;&#23454;&#39564;&#23460;&#25351;&#26631;&#30340;&#24739;&#32773;&#36827;&#23637;&#36235;&#21183;&#20174;&#24120;&#35265;&#24773;&#20917;&#36716;&#31227;&#21040;&#32597;&#35265;&#25110;&#29305;&#23450;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#20013;&#65292;&#20197;&#21327;&#21161;&#26816;&#27979;&#32463;&#30382;&#20896;&#29366;&#21160;&#33033;&#20171;&#20837;&#27835;&#30103;&#24739;&#32773;&#30340;&#38774;&#34880;&#31649;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#26399;&#30417;&#27979;&#26159;&#31649;&#29702;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#24517;&#35201;&#26041;&#38754;&#12290;&#30001;&#20110;&#32597;&#35265;&#25110;&#29305;&#23450;&#30142;&#30149;&#30340;&#24739;&#32773;&#35268;&#27169;&#36739;&#23567;&#65292;&#35266;&#23519;&#20063;&#26159;&#38388;&#27463;&#24615;&#30340;&#65292;&#22240;&#27492;&#20854;&#25307;&#21215;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#65292;&#32780;&#24120;&#35265;&#24773;&#20917;&#30001;&#20110;&#23450;&#26399;&#38543;&#35775;&#32780;&#26356;&#23481;&#26131;&#32047;&#31215;&#32437;&#21521;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#20197;&#20854;&#26080;&#35268;&#24459;&#24615;&#12289;&#26102;&#38388;&#24615;&#12289;&#32570;&#24109;&#24615;&#21644;&#31232;&#30095;&#24615;&#32780;&#38395;&#21517;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#20811;&#26381;&#19978;&#36848;&#38556;&#30861;&#65292;&#23558;&#24515;&#34880;&#31649;&#23454;&#39564;&#23460;&#25351;&#26631;&#30340;&#24739;&#32773;&#36827;&#23637;&#36235;&#21183;&#20174;&#24120;&#35265;&#24773;&#20917;&#36716;&#31227;&#21040;&#32597;&#35265;&#25110;&#29305;&#23450;&#24515;&#34880;&#31649;&#20107;&#20214;&#26816;&#27979;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#34880;&#21387;&#24739;&#32773;&#65288;&#23578;&#26410;&#24739;&#31958;&#23615;&#30149;&#65289;&#36827;&#34892;&#20102;&#19968;&#33324;&#23454;&#39564;&#23460;&#36827;&#23637;&#65288;GLP&#65289;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#23558;&#20854;&#23454;&#39564;&#23460;&#36827;&#23637;&#36235;&#21183;&#36716;&#31227;&#65292;&#20197;&#21327;&#21161;&#26816;&#27979;&#32463;&#30382;&#20896;&#29366;&#21160;&#33033;&#20171;&#20837;&#27835;&#30103;&#24739;&#32773;&#30340;&#38774;&#34880;&#31649;&#37325;&#24314;&#65288;TVR&#65289;&#12290;GLP&#37319;&#29992;&#20102;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular surveillance is an indispensable aspect of managing cardiovascular disorders. Patient recruitment for rare or specific diseases is often limited due to their small patient size and episodic observations, whereas prevalent cases accumulate longitudinal data easily due to regular follow-ups. These data, however, are notorious for their irregularity, temporality, absenteeism, and sparsity. In this study, we leveraged self-supervised learning (SSL) and transfer learning to overcome the above-mentioned barriers, transferring patient progress trends in cardiovascular laboratory parameters from prevalent cases to rare or specific cardiovascular events detection. We pretrained a general laboratory progress (GLP) pretrain model using hypertension patients (who were yet to be diabetic), and transferred their laboratory progress trend to assist in detecting target vessel revascularization (TVR) in percutaneous coronary intervention patients. GLP adopted a two-stage training process that u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;AIRI&#31639;&#27861;&#22312;ASKAP&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27491;&#21017;&#21270;&#27493;&#39588;&#20013;&#30340;&#21435;&#22122;&#22788;&#29702;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22270;&#20687;&#21160;&#24577;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2302.14149</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#31934;&#23494;&#23485;&#22330;&#23556;&#30005;&#24178;&#28041;&#25104;&#20687;&#65306;II. &#22312;ASKAP&#25968;&#25454;&#19978;&#23545;AIRI&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Scalable precision wide-field imaging in radio interferometry: II. AIRI validated on ASKAP data. (arXiv:2302.14149v2 [astro-ph.IM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;AIRI&#31639;&#27861;&#22312;ASKAP&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27491;&#21017;&#21270;&#27493;&#39588;&#20013;&#30340;&#21435;&#22122;&#22788;&#29702;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22270;&#20687;&#21160;&#24577;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#37096;&#20998;&#30340;&#32493;&#31687;&#65292;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;AI&#29992;&#20110;&#23556;&#30005;&#24178;&#28041;&#25104;&#20687;&#65288;AIRI&#65289;&#31639;&#27861;&#22312;&#28595;&#22823;&#21033;&#20122;&#26041;&#38453;&#21315;&#24179;&#26041;&#21315;&#31859;&#38453;&#21015;&#65288;ASKAP&#65289;&#35266;&#27979;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#30456;&#21516;&#24182;&#34892;&#21270;&#21644;&#33258;&#21160;&#21270;&#25104;&#20687;&#26694;&#26550;&#29983;&#25104;&#30340;&#21333;&#33394;AIRI-ASKAP&#22270;&#20687;&#65292;&#35813;&#26694;&#26550;&#22312;&#31532;&#19968;&#37096;&#20998;&#20013;&#25551;&#36848;&#20026;&#8220;&#22312;ASKAP&#25968;&#25454;&#19978;&#32463;&#36807;&#39564;&#35777;&#30340;uSARA&#8221;&#12290;AIRI&#37319;&#29992;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21435;&#21367;&#31215;&#30340;&#27491;&#21521;&#21453;&#21521;&#31639;&#27861;&#20013;&#65292;&#29992;&#32463;&#36807;&#35757;&#32451;&#30340;&#21435;&#22122;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26367;&#20195;&#27491;&#21017;&#21270;&#27493;&#39588;&#20013;&#30340;&#36817;&#20284;&#31639;&#23376;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#32452;&#32463;&#36807;&#35757;&#32451;&#30340;DNN&#21435;&#22122;&#22120;&#65292;&#38024;&#23545;&#25152;&#36873;&#25968;&#25454;&#30340;&#39044;&#20272;&#22270;&#20687;&#21160;&#24577;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#22312;&#36873;&#25321;&#26368;&#25509;&#36817;&#30340;DNN&#19982;&#20351;&#29992;&#20855;&#26377;&#26368;&#39640;&#21160;&#24577;&#33539;&#22260;&#30340;&#36890;&#29992;DNN&#26102;&#65292;AIRI&#37325;&#24314;&#30340;&#21464;&#21270;&#65292;&#20026;&#19968;&#20010;&#26356;&#23436;&#25972;&#30340;&#26694;&#26550;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#35813;&#26694;&#26550;&#19981;&#20165;&#21487;&#20197;&#25552;&#20379;&#22270;&#20687;&#31934;&#30830;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#20379;&#36739;&#39640;&#30340;&#21160;&#24577;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accompanying Part I, this sequel delineates a validation of the recently proposed AI for Regularisation in radio-interferometric Imaging (AIRI) algorithm on observations from the Australian Square Kilometre Array Pathfinder (ASKAP). The monochromatic AIRI-ASKAP images showcased in this work are formed using the same parallelised and automated imaging framework described in Part I: ``uSARA validated on ASKAP data''. Using a Plug-and-Play approach, AIRI differs from uSARA by substituting a trained denoising deep neural network (DNN) for the proximal operator in the regularisation step of the forward-backward algorithm during deconvolution. We build a trained shelf of DNN denoisers which target the estimated image-dynamic-ranges of our selected data. Furthermore, we quantify variations of AIRI reconstructions when selecting the nearest DNN on the shelf versus using a universal DNN with the highest dynamic range, opening the door to a more complete framework that not only delivers image es
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#26469;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#65292;&#21253;&#21547;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#21644;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.10798</link><description>&lt;p&gt;
&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#20197;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;: &#19968;&#31181;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach. (arXiv:2302.10798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#26469;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#65292;&#21253;&#21547;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#21644;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#22823;&#19988;&#26356;&#21152;&#22797;&#26434;&#65292;&#32511;&#33394;AI&#24050;&#32463;&#24341;&#36215;&#20102;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#23545;&#32593;&#32476;&#21442;&#25968;&#36827;&#34892;&#35009;&#21098;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#25512;&#26029;&#26102;&#30340;&#35745;&#31639;&#36127;&#33655;&#12290;&#28982;&#32780;&#65292;&#35009;&#21098;&#26041;&#26696;&#36890;&#24120;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#22240;&#20026;&#38656;&#35201;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#21644;&#24494;&#35843;&#25110;&#37325;&#22797;&#35745;&#31639;&#21160;&#24577;&#35009;&#21098;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#36731;&#37327;&#32423;&#23376;&#32593;&#32476;&#65292;&#26082;&#33021;&#26368;&#23567;&#21270;&#33021;&#32791;&#65292;&#21448;&#33021;&#22312;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#19982;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340;&#32593;&#32476;&#20445;&#25345;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35009;&#21098;&#26041;&#26696;&#20197;&#32511;&#33394;&#20026;&#23548;&#21521;&#65292;&#22240;&#20026;&#23427;&#20165;&#38656;&#35201;&#21160;&#24577;&#35009;&#21098;&#26041;&#27861;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;&#26469;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#12290;&#35813;&#26041;&#26696;&#30001;&#19968;&#20010;&#20108;&#36827;&#21046;&#38376;&#25511;&#27169;&#22359;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#32452;&#25104;&#65292;&#20197;&#21457;&#29616;&#20855;&#26377;&#29992;&#25143;&#23450;&#20041;&#31232;&#30095;&#24230;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#31561;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35009;&#21098;&#21644;&#36716;&#25442;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#33021;&#37327;&#28040;&#32791;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a novel loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and tr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;HPO&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#19978;&#36827;&#34892;&#39044;&#35780;&#20272;&#65292;&#28982;&#21518;&#22312;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#30446;&#30340;</title><link>http://arxiv.org/abs/2302.03845</link><description>&lt;p&gt;
&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#21152;&#36895;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#20004;&#27493;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Two-step hyperparameter optimization method: Accelerating hyperparameter search by using a fraction of a training dataset. (arXiv:2302.03845v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03845
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;HPO&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#19978;&#36827;&#34892;&#39044;&#35780;&#20272;&#65292;&#28982;&#21518;&#22312;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#30446;&#30340;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#26159;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#20294;&#24120;&#35265;&#20570;&#27861;&#36807;&#26102;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#25163;&#21160;&#25110;&#32593;&#26684;&#25628;&#32034;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#37319;&#29992;&#20808;&#36827;&#30340;HPO&#31639;&#27861;&#20250;&#24341;&#20837;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#65292;&#23548;&#33268;&#35745;&#31639;&#26102;&#38388;&#26356;&#38271;&#12290;&#36825;&#23545;ML&#24212;&#29992;&#26500;&#25104;&#20102;&#26174;&#33879;&#25361;&#25112;&#65292;&#22240;&#20026;&#27425;&#20248;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#38480;&#21046;&#20102;ML&#27169;&#22411;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#26368;&#32456;&#38459;&#30861;&#20102;&#23545;ML&#25216;&#26415;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;HPO&#26041;&#27861;&#20316;&#20026;&#35299;&#20915;&#35745;&#31639;&#38656;&#27714;&#21644;&#31561;&#24453;&#26102;&#38388;&#30340;&#25112;&#30053;&#24615;&#26041;&#26696;&#65292;&#36825;&#26159;&#20174;&#24212;&#29992;ML&#21442;&#25968;&#21270;&#24037;&#20316;&#30340;&#23454;&#36341;&#32463;&#39564;&#20013;&#24471;&#21040;&#30340;&#12290;&#21021;&#22987;&#38454;&#27573;&#28041;&#21450;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#19978;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#21021;&#27493;&#35780;&#20272;&#65292;&#28982;&#21518;&#22312;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#24182;&#37325;&#26032;&#35780;&#20272;&#26368;&#20339;&#20505;&#36873;&#27169;&#22411;&#12290;&#36825;&#31181;&#20004;&#27493;HPO&#26041;&#27861;&#26159;&#26222;&#36866;&#30340;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is an important step in machine learning (ML) model development, but common practices are archaic -- primarily relying on manual or grid searches. This is partly because adopting advanced HPO algorithms introduces added complexity to the workflow, leading to longer computation times. This poses a notable challenge to ML applications, as suboptimal hyperparameter selections curtail the potential of ML model performance, ultimately obstructing the full exploitation of ML techniques. In this article, we present a two-step HPO method as a strategic solution to curbing computational demands and wait times, gleaned from practical experiences in applied ML parameterization work. The initial phase involves a preliminary evaluation of hyperparameters on a small subset of the training dataset, followed by a re-evaluation of the top-performing candidate models post-retraining with the entire training dataset. This two-step HPO method is universally applicable acr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#22270;&#20013;&#30340;&#29305;&#23450;&#33410;&#28857;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#36890;&#36807;&#20266;&#35013;&#25104;&#33391;&#24615;&#33410;&#28857;&#30340;&#26041;&#24335;&#36827;&#34892;&#27880;&#20837;&#12290;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#30772;&#22351;&#22522;&#20110;GNN&#30340;&#33410;&#28857;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12277</link><description>&lt;p&gt;
&#31867;&#29305;&#23450;&#32593;&#32476;&#20013;&#33410;&#28857;&#27880;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Node Injection for Class-specific Network Poisoning. (arXiv:2301.12277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#22270;&#20013;&#30340;&#29305;&#23450;&#33410;&#28857;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#36890;&#36807;&#20266;&#35013;&#25104;&#33391;&#24615;&#33410;&#28857;&#30340;&#26041;&#24335;&#36827;&#34892;&#27880;&#20837;&#12290;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#30772;&#22351;&#22522;&#20110;GNN&#30340;&#33410;&#28857;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23398;&#20064;&#20016;&#23500;&#30340;&#32593;&#32476;&#34920;&#31034;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26377;&#21161;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;GNNs&#23481;&#26131;&#21463;&#21040;&#33410;&#28857;&#27880;&#20837;&#21644;&#32593;&#32476;&#25200;&#21160;&#31561;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20854;&#20013;&#65292;&#33410;&#28857;&#27880;&#20837;&#25915;&#20987;&#26356;&#21152;&#23454;&#38469;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#38656;&#35201;&#23545;&#29616;&#26377;&#32593;&#32476;&#36827;&#34892;&#25805;&#32437;&#65292;&#21487;&#20197;&#26356;&#30495;&#23454;&#22320;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#38472;&#36848; - &#22312;&#22270;&#20013;&#36827;&#34892;&#31867;&#29305;&#23450;&#30340;&#27602;&#32032;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#26088;&#22312;&#23558;&#30446;&#26631;&#31867;&#20013;&#30340;&#29305;&#23450;&#33410;&#28857;&#35823;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#65292;&#20351;&#29992;&#33410;&#28857;&#27880;&#20837;&#12290;&#27492;&#22806;&#65292;&#33410;&#28857;&#20197;&#19968;&#31181;&#20266;&#35013;&#25104;&#33391;&#24615;&#33410;&#28857;&#30340;&#26041;&#24335;&#27880;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;NICKI&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#30772;&#22351;&#22522;&#20110;GNN&#30340;&#33410;&#28857;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;NICKI&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#24037;&#20316;&#65292;&#39318;&#20808;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#65292;&#28982;&#21518;&#29983;&#25104;&#27880;&#20837;&#33410;&#28857;&#30340;&#29305;&#24449;&#21644;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are powerful in learning rich network representations that aid the performance of downstream tasks. However, recent studies showed that GNNs are vulnerable to adversarial attacks involving node injection and network perturbation. Among these, node injection attacks are more practical as they don't require manipulation in the existing network and can be performed more realistically. In this paper, we propose a novel problem statement - a class-specific poison attack on graphs in which the attacker aims to misclassify specific nodes in the target class into a different class using node injection. Additionally, nodes are injected in such a way that they camouflage as benign nodes. We propose NICKI, a novel attacking strategy that utilizes an optimization-based approach to sabotage the performance of GNN-based node classifiers. NICKI works in two phases - it first learns the node representation and then generates the features and edges of the injected nodes. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;FiN-1&#21644;FiN-2&#65292;&#20849;&#25910;&#38598;&#20102;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#24212;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#30005;&#21147;&#32593;&#22312;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#36807;&#28193;&#21644;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2209.12693</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#25968;&#25454;&#22312;&#30005;&#21147;&#32593;&#36890;&#35759;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids. (arXiv:2209.12693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;FiN-1&#21644;FiN-2&#65292;&#20849;&#25910;&#38598;&#20102;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#24212;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#30005;&#21147;&#32593;&#22312;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#36807;&#28193;&#21644;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#32593;&#24050;&#25104;&#20026;&#26085;&#24120;&#29983;&#27963;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#23613;&#31649;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#24448;&#24448;&#19981;&#20250;&#27880;&#24847;&#21040;&#12290;&#21482;&#26377;&#24403;&#30005;&#21147;&#32593;&#19981;&#20877;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#36890;&#24120;&#25165;&#20250;&#29305;&#21035;&#24847;&#35782;&#21040;&#36825;&#31181;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#37325;&#22823;&#21464;&#21270;&#65292;&#22914;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#65288;&#20809;&#20239;&#12289;&#39118;&#21147;&#28065;&#36718;&#26426;&#31561;&#65289;&#30340;&#36807;&#28193;&#20197;&#21450;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#65288;&#30005;&#21160;&#27773;&#36710;&#12289;&#23478;&#24237;&#30005;&#27744;&#31995;&#32479;&#31561;&#65289;&#30340;&#33021;&#28304;&#28040;&#36153;&#32773;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#32473;&#30005;&#21147;&#32593;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#65288;PLC&#65289;&#22522;&#30784;&#35774;&#26045;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#12290;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598; FiN-1 &#21644; FiN-2 &#22312;&#24503;&#22269;&#20302;&#21387;&#30005;&#32593;&#30340;&#19968;&#37096;&#20998;&#23454;&#38469;&#20351;&#29992;&#20013;&#25910;&#38598;&#65292;&#21521;&#22823;&#32422;440&#19975;&#20154;&#25552;&#20379;&#26381;&#21153;&#65292;&#24182;&#26174;&#31034;5100&#22810;&#20010;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#36229;&#36807;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#29992;&#20363;&#65292;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electricity grids have become an essential part of daily life, even if they are often not noticed in everyday life. We usually only become particularly aware of this dependence by the time the electricity grid is no longer available. However, significant changes, such as the transition to renewable energy (photovoltaic, wind turbines, etc.) and an increasing number of energy consumers with complex load profiles (electric vehicles, home battery systems, etc.), pose new challenges for the electricity grid. To address these challenges, we propose two first-of-its-kind datasets based on measurements in a broadband powerline communications (PLC) infrastructure. Both datasets FiN-1 and FiN-2, were collected during real practical use in a part of the German low-voltage grid that supplies around 4.4 million people and show more than 13 billion datapoints collected by more than 5100 sensors. In addition, we present different use cases in asset management, grid state visualization, forecasting, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#20004;&#31867;&#26032;&#30340;&#20449;&#24687;&#24230;&#37327;&#26041;&#27861;&#65292;&#25193;&#23637;&#21644;&#32479;&#19968;&#20102;&#19981;&#21516;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#36890;&#36807;&#20449;&#24687;&#24230;&#37327;&#21644;&#36125;&#21494;&#26031;&#39118;&#38505;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#20197;&#21450;&#20449;&#24687;&#22788;&#29702;&#31561;&#24335;&#65292;&#25581;&#31034;&#20102;&#22312;&#32463;&#20856;&#39118;&#38505;&#26368;&#23567;&#21270;&#20013;&#36873;&#25321;&#20551;&#35774;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.11987</link><description>&lt;p&gt;
&#20449;&#24687;&#22788;&#29702;&#30456;&#31561;&#24615;&#21644;&#20449;&#24687;&#39118;&#38505;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Information Processing Equalities and the Information-Risk Bridge. (arXiv:2207.11987v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#20004;&#31867;&#26032;&#30340;&#20449;&#24687;&#24230;&#37327;&#26041;&#27861;&#65292;&#25193;&#23637;&#21644;&#32479;&#19968;&#20102;&#19981;&#21516;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#36890;&#36807;&#20449;&#24687;&#24230;&#37327;&#21644;&#36125;&#21494;&#26031;&#39118;&#38505;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#20197;&#21450;&#20449;&#24687;&#22788;&#29702;&#31561;&#24335;&#65292;&#25581;&#31034;&#20102;&#22312;&#32463;&#20856;&#39118;&#38505;&#26368;&#23567;&#21270;&#20013;&#36873;&#25321;&#20551;&#35774;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31867;&#26032;&#30340;&#20449;&#24687;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#35745;&#23454;&#39564;&#65292;&#25512;&#24191;&#20102;&#21644;&#21253;&#21547;&#20102;$\phi$-&#25955;&#24230;&#12289;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#12289;$\mathfrak{N}$-&#36317;&#31163;&#65288;MMD&#65289;&#21644;&#20004;&#20010;&#25110;&#22810;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;$(f,\Gamma)$-&#25955;&#24230;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#20449;&#24687;&#24230;&#37327;&#21644;&#32479;&#35745;&#20915;&#31574;&#38382;&#39064;&#30340;&#36125;&#21494;&#26031;&#39118;&#38505;&#20043;&#38388;&#30340;&#31616;&#21333;&#20960;&#20309;&#20851;&#31995;&#65292;&#20174;&#32780;&#20197;&#23436;&#20840;&#23545;&#31216;&#30340;&#26041;&#24335;&#23558;&#21464;&#20998;$\phi$-&#25955;&#24230;&#34920;&#31034;&#25193;&#23637;&#21040;&#22810;&#20010;&#20998;&#24067;&#20013;&#12290;&#26032;&#30340;&#25955;&#24230;&#26063;&#22312;&#39532;&#23572;&#21487;&#22827;&#31639;&#23376;&#30340;&#20316;&#29992;&#19979;&#20445;&#25345;&#19981;&#21464;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#20449;&#24687;&#22788;&#29702;&#31561;&#24335;&#65292;&#23427;&#26159;&#32463;&#20856;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#30340;&#32454;&#21270;&#21644;&#25512;&#24191;&#12290;&#36825;&#20010;&#31561;&#24335;&#25581;&#31034;&#20102;&#22312;&#32463;&#20856;&#39118;&#38505;&#26368;&#23567;&#21270;&#20013;&#36873;&#25321;&#20551;&#35774;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce two new classes of measures of information for statistical experiments which generalise and subsume $\phi$-divergences, integral probability metrics, $\mathfrak{N}$-distances (MMD), and $(f,\Gamma)$ divergences between two or more distributions. This enables us to derive a simple geometrical relationship between measures of information and the Bayes risk of a statistical decision problem, thus extending the variational $\phi$-divergence representation to multiple distributions in an entirely symmetric manner. The new families of divergence are closed under the action of Markov operators which yields an information processing equality which is a refinement and generalisation of the classical data processing inequality. This equality gives insight into the significance of the choice of the hypothesis class in classical risk minimization.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#39318;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#28145;&#24230;&#36229;&#20998;&#36776;&#29575;&#24191;&#22330;&#23556;&#30005;&#24178;&#28041;&#25104;&#20687;&#26694;&#26550;&#65292;&#24182;&#22312;ESO 137-006&#23556;&#30005;&#26143;&#31995;&#30340;&#35266;&#27979;&#20013;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#8220;&#25554;&#20837;&#21644;&#25773;&#25918;&#8221;&#26041;&#26696;&#35299;&#20915;&#22270;&#20687;&#37325;&#24314;&#30340;&#21453;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#24674;&#22797;&#21644;&#33258;&#36866;&#24212;&#21435;&#22122;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#25968;&#25454;&#21644;&#22270;&#20687;&#23610;&#23544;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#24182;&#34892;&#31639;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.11336</link><description>&lt;p&gt;
&#39318;&#20010;&#29992;&#20110;&#28145;&#24230;&#36229;&#20998;&#36776;&#29575;&#24191;&#22330;&#30005;&#27874;&#22825;&#25991;&#23398;&#25104;&#20687;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#25581;&#31034;ESO 137--006&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
First AI for deep super-resolution wide-field imaging in radio astronomy: unveiling structure in ESO 137--006. (arXiv:2207.11336v2 [astro-ph.IM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11336
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#39318;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#28145;&#24230;&#36229;&#20998;&#36776;&#29575;&#24191;&#22330;&#23556;&#30005;&#24178;&#28041;&#25104;&#20687;&#26694;&#26550;&#65292;&#24182;&#22312;ESO 137-006&#23556;&#30005;&#26143;&#31995;&#30340;&#35266;&#27979;&#20013;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#8220;&#25554;&#20837;&#21644;&#25773;&#25918;&#8221;&#26041;&#26696;&#35299;&#20915;&#22270;&#20687;&#37325;&#24314;&#30340;&#21453;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#24674;&#22797;&#21644;&#33258;&#36866;&#24212;&#21435;&#22122;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#25968;&#25454;&#21644;&#22270;&#20687;&#23610;&#23544;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#24182;&#34892;&#31639;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#28145;&#24230;&#36229;&#20998;&#36776;&#29575;&#24191;&#22330;&#23556;&#30005;&#24178;&#28041;&#25104;&#20687;&#26694;&#26550;&#65292;&#24182;&#22312;ESO 137-006&#23556;&#30005;&#26143;&#31995;&#30340;&#35266;&#27979;&#20013;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#35813;&#31639;&#27861;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#22270;&#20687;&#37325;&#24314;&#30340;&#21453;&#38382;&#39064;&#65292;&#22522;&#20110;&#26368;&#36817;&#30340;&#8220;&#25554;&#20837;&#21644;&#25773;&#25918;&#8221;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#31639;&#27861;&#20013;&#23558;&#21435;&#22122;&#25805;&#20316;&#22120;&#27880;&#20837;&#20026;&#22270;&#20687;&#27491;&#21017;&#21270;&#22120;&#65292;&#23454;&#29616;&#21435;&#22122;&#27493;&#39588;&#21644;&#26799;&#24230;&#19979;&#38477;&#25968;&#25454;&#20445;&#30495;&#24230;&#27493;&#39588;&#20132;&#26367;&#36827;&#34892;&#65292;&#30452;&#33267;&#25910;&#25947;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#21435;&#22122;&#22120;&#30340;&#25163;&#24037;&#35774;&#35745;&#21644;&#23398;&#20064;&#25913;&#36827;&#29256;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#31639;&#27861;&#23454;&#29616;&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#20998;&#35299;&#20026;&#38754;&#21644;&#27979;&#37327;&#31639;&#23376;&#20026;&#31232;&#30095;&#20302;&#32500;&#22359;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#23545;&#22823;&#25968;&#25454;&#21644;&#22270;&#20687;&#23610;&#23544;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;1053&#21644;1399 MHz&#30340;19 GB MeerKAT&#25968;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#22270;&#20687;&#24418;&#25104;&#26694;&#26550;&#65292;&#22312;&#21253;&#21547;ESO 137-006&#30340;&#24191;&#22495;&#35270;&#37326;&#19978;&#36827;&#34892;&#20102;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the first AI-based framework for deep, super-resolution, wide-field radio-interferometric imaging, and demonstrate it on observations of the ESO~137-006 radio galaxy. The algorithmic framework to solve the inverse problem for image reconstruction builds on a recent ``plug-and-play'' scheme whereby a denoising operator is injected as an image regulariser in an optimisation algorithm, which alternates until convergence between denoising steps and gradient-descent data-fidelity steps. We investigate handcrafted and learned variants of high-resolution high-dynamic range denoisers. We propose a parallel algorithm implementation relying on automated decompositions of the image into facets and the measurement operator into sparse low-dimensional blocks, enabling scalability to large data and image dimensions. We validate our framework for image formation at a wide field of view containing ESO~137-006, from 19 gigabytes of MeerKAT data at 1053 and 1399 MHz. The recovered maps exhi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20215;&#26684;&#30340;&#32593;&#32476;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#38656;&#27714;&#23398;&#20064;&#21644;&#20844;&#24179;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#38656;&#27714;&#23398;&#20064;&#26041;&#27861;&#30340;&#21407;&#22987;&#23545;&#20598;&#22312;&#32447;&#31574;&#30053;&#26469;&#26368;&#22823;&#21270;&#35268;&#33539;&#21270;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2207.11159</link><description>&lt;p&gt;
&#22522;&#20110;&#38656;&#27714;&#23398;&#20064;&#21644;&#20844;&#24179;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#30340;&#32593;&#32476;&#25910;&#30410;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Network Revenue Management with Demand Learning and Fair Resource-Consumption Balancing. (arXiv:2207.11159v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20215;&#26684;&#30340;&#32593;&#32476;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#38656;&#27714;&#23398;&#20064;&#21644;&#20844;&#24179;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#38656;&#27714;&#23398;&#20064;&#26041;&#27861;&#30340;&#21407;&#22987;&#23545;&#20598;&#22312;&#32447;&#31574;&#30053;&#26469;&#26368;&#22823;&#21270;&#35268;&#33539;&#21270;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#26368;&#22823;&#21270;&#24635;&#25910;&#30410;&#22806;&#65292;&#24456;&#22810;&#34892;&#19994;&#30340;&#20915;&#31574;&#32773;&#36824;&#24076;&#26395;&#30830;&#20445;&#19981;&#21516;&#36164;&#28304;&#20043;&#38388;&#28040;&#32791;&#30340;&#24179;&#34913;&#12290;&#20363;&#22914;&#65292;&#22312;&#38646;&#21806;&#34892;&#19994;&#20013;&#65292;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#36164;&#28304;&#24179;&#34913;&#28040;&#32791;&#26377;&#21161;&#20110;&#25552;&#39640;&#20844;&#24179;&#24615;&#24182;&#32500;&#25345;&#33391;&#22909;&#30340;&#28192;&#36947;&#20851;&#31995;&#65307;&#22312;&#20113;&#35745;&#31639;&#34892;&#19994;&#20013;&#65292;&#36164;&#28304;&#28040;&#32791;&#30340;&#24179;&#34913;&#26377;&#21161;&#20110;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#24182;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#12290;&#38024;&#23545;&#36825;&#20123;&#23454;&#38469;&#38656;&#27714;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20215;&#26684;&#30340;&#32593;&#32476;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#38656;&#27714;&#23398;&#20064;&#21644;&#20844;&#24179;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35268;&#33539;&#21270;&#25910;&#30410;&#30340;&#27010;&#24565;&#65292;&#21363;&#36890;&#36807;&#24179;&#34913;&#27491;&#21017;&#21270;&#23558;&#20844;&#24179;&#30340;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#32435;&#20837;&#21040;&#25910;&#30410;&#26368;&#22823;&#21270;&#30340;&#30446;&#26631;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#32622;&#20449;&#30028;&#38480;&#65288;UCB&#65289;&#38656;&#27714;&#23398;&#20064;&#26041;&#27861;&#30340;&#21407;&#22987;&#23545;&#20598;&#22312;&#32447;&#31574;&#30053;&#26469;&#26368;&#22823;&#21270;&#35268;&#33539;&#21270;&#25910;&#30410;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#20010;&#21019;&#26032;&#26041;&#27861;&#26469;&#24212;&#23545;&#38656;&#27714;&#23398;&#20064;&#21644;&#36164;&#28304;&#28040;&#32791;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In addition to maximizing the total revenue, decision-makers in lots of industries would like to guarantee balanced consumption across different resources. For instance, in the retailing industry, ensuring a balanced consumption of resources from different suppliers enhances fairness and helps main a good channel relationship; in the cloud computing industry, resource-consumption balance helps increase customer satisfaction and reduce operational costs. Motivated by these practical needs, this paper studies the price-based network revenue management (NRM) problem with both demand learning and fair resource-consumption balancing. We introduce the regularized revenue, i.e., the total revenue with a balancing regularization, as our objective to incorporate fair resource-consumption balancing into the revenue maximization goal. We propose a primal-dual-type online policy with the Upper-Confidence-Bound (UCB) demand learning method to maximize the regularized revenue. We adopt several innov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#21644;&#26816;&#27979;&#30495;&#27491;&#27495;&#20041;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27979;&#35797;&#29983;&#25104;&#22120;&#21482;&#33021;&#29983;&#25104;&#36229;&#20986;&#20998;&#24067;&#36755;&#20837;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.10495</link><description>&lt;p&gt;
&#29983;&#25104;&#21644;&#26816;&#27979;&#30495;&#27491;&#30340;&#27495;&#20041;&#65306;DNN&#30417;&#30563;&#27979;&#35797;&#20013;&#30340;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#21361;&#38505;
&lt;/p&gt;
&lt;p&gt;
Generating and Detecting True Ambiguity: A Forgotten Danger in DNN Supervision Testing. (arXiv:2207.10495v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#21644;&#26816;&#27979;&#30495;&#27491;&#27495;&#20041;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27979;&#35797;&#29983;&#25104;&#22120;&#21482;&#33021;&#29983;&#25104;&#36229;&#20986;&#20998;&#24067;&#36755;&#20837;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#27491;&#22312;&#25104;&#20026;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#22312;&#19982;&#35757;&#32451;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#26465;&#20214;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65288;&#36229;&#20986;&#20998;&#24067;&#30340;&#36755;&#20837;&#65289;&#25110;&#22312;&#30495;&#27491;&#27169;&#31946;&#30340;&#36755;&#20837;&#19978;&#65288;&#21363;&#65292;&#22312;&#26631;&#31614;&#20013;&#23384;&#22312;&#22810;&#20010;&#31867;&#21035;&#19988;&#20854;&#27010;&#29575;&#19981;&#20026;&#38646;&#30340;&#36755;&#20837;&#65289;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;DNN&#30417;&#30563;&#22120;&#65292;&#20197;&#22312;&#21487;&#33021;&#23548;&#33268;&#20219;&#20309;&#25439;&#23475;&#20043;&#21069;&#26816;&#27979;&#21040;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#36755;&#20837;&#12290;&#20026;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;DNN&#30417;&#30563;&#22120;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#27979;&#35797;&#29983;&#25104;&#25216;&#26415;&#65292;&#20197;&#23558;&#27979;&#35797;&#24037;&#20316;&#37325;&#28857;&#25918;&#22312;&#37027;&#20123;&#30417;&#30563;&#22120;&#24212;&#35813;&#23558;&#20854;&#35782;&#21035;&#20026;&#24322;&#24120;&#30340;&#39640;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#19978;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27979;&#35797;&#29983;&#25104;&#22120;&#26088;&#22312;&#29983;&#25104;&#36229;&#20986;&#20998;&#24067;&#30340;&#36755;&#20837;&#12290;&#27809;&#26377;&#29616;&#26377;&#30340;&#27169;&#22411;&#21644;&#30417;&#30563;&#22120;&#26080;&#20851;&#30340;&#25216;&#26415;&#38024;&#23545;&#29983;&#25104;&#30495;&#27491;&#27169;&#31946;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#21363;&#65292;&#26681;&#25454;&#19987;&#23478;&#20154;&#21592;&#21028;&#26029;&#65292;&#36755;&#20837;&#21487;&#20197;&#23545;&#24212;&#22810;&#20010;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#21644;&#26816;&#27979;&#30495;&#27491;&#27495;&#20041;&#30340;&#27979;&#35797;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are becoming a crucial component of modern software systems, but they are prone to fail under conditions that are different from the ones observed during training (out-of-distribution inputs) or on inputs that are truly ambiguous, i.e., inputs that admit multiple classes with nonzero probability in their labels. Recent work proposed DNN supervisors to detect high-uncertainty inputs before their possible misclassification leads to any harm. To test and compare the capabilities of DNN supervisors, researchers proposed test generation techniques, to focus the testing effort on high-uncertainty inputs that should be recognized as anomalous by supervisors. However, existing test generators aim to produce out-of-distribution inputs. No existing model- and supervisor independent technique targets the generation of truly ambiguous test inputs, i.e., inputs that admit multiple classes according to expert human judgment.  In this paper, we propose a novel way to gener
&lt;/p&gt;</description></item><item><title>TREE-G&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#20462;&#25913;&#20915;&#31574;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20998;&#35010;&#20989;&#25968;&#21644;&#25351;&#38024;&#26426;&#21046;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#20110;&#24102;&#26377;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.02760</link><description>&lt;p&gt;
TREE-G:&#20915;&#31574;&#26641;&#23545;&#25239;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TREE-G: Decision Trees Contesting Graph Neural Networks. (arXiv:2207.02760v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02760
&lt;/p&gt;
&lt;p&gt;
TREE-G&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#20462;&#25913;&#20915;&#31574;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20998;&#35010;&#20989;&#25968;&#21644;&#25351;&#38024;&#26426;&#21046;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#20110;&#24102;&#26377;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#19978;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12289;&#26131;&#20110;&#24212;&#29992;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#24212;&#29992;&#20915;&#31574;&#26641;&#24182;&#23558;&#25299;&#25169;&#20449;&#24687;&#19982;&#22270;&#30340;&#39030;&#28857;&#19978;&#30340;&#34920;&#26684;&#25968;&#25454;&#30456;&#32467;&#21512;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TREE-G&#12290;TREE-G&#20462;&#25913;&#20102;&#26631;&#20934;&#20915;&#31574;&#26641;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#26032;&#22411;&#20998;&#35010;&#20989;&#25968;&#12290;&#36825;&#20010;&#20998;&#35010;&#20989;&#25968;&#19981;&#20165;&#21253;&#25324;&#33410;&#28857;&#29305;&#24449;&#21644;&#25299;&#25169;&#20449;&#24687;&#65292;&#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25351;&#38024;&#26426;&#21046;&#65292;&#20801;&#35768;&#20998;&#35010;&#33410;&#28857;&#20351;&#29992;&#22312;&#20808;&#21069;&#20998;&#35010;&#20013;&#35745;&#31639;&#24471;&#21040;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20998;&#35010;&#20989;&#25968;&#33021;&#22815;&#36866;&#24212;&#39044;&#27979;&#20219;&#21153;&#21644;&#24403;&#21069;&#30340;&#22270;&#12290;&#25105;&#20204;&#23545;TREE-G&#30340;&#29702;&#35770;&#24615;&#36136;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#21644;&#39030;&#28857;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;&#20102;&#23427;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
When dealing with tabular data, models based on decision trees are a popular choice due to their high accuracy on these data types, their ease of application, and explainability properties. However, when it comes to graph-structured data, it is not clear how to apply them effectively, in a way that incorporates the topological information with the tabular data available on the vertices of the graph. To address this challenge, we introduce TREE-G. TREE-G modifies standard decision trees, by introducing a novel split function that is specialized for graph data. Not only does this split function incorporate the node features and the topological information, but it also uses a novel pointer mechanism that allows split nodes to use information computed in previous splits. Therefore, the split function adapts to the predictive task and the graph at hand. We analyze the theoretical properties of TREE-G and demonstrate its benefits empirically on multiple graph and vertex prediction benchmarks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20960;&#31181;&#39044;&#35757;&#32451;&#21464;&#20307;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#39046;&#22495;&#65292;&#20351;&#29992;Transformer&#26550;&#26500;&#33021;&#22815;&#22312;&#27809;&#26377;&#20351;&#29992;&#26174;&#24335;&#35821;&#35328;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26368;&#20339;&#30340;&#20215;&#20540;&#39044;&#27979;&#24615;&#33021;&#65292;&#30456;&#20851;&#24615;&#31995;&#25968;&#20026;0.638&#12290;</title><link>http://arxiv.org/abs/2203.07378</link><description>&lt;p&gt;
&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;Transformer&#26102;&#20195;&#30340;&#40654;&#26126;&#65306;&#24357;&#21512;&#24773;&#24863;&#20215;&#20540;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Dawn of the transformer era in speech emotion recognition: closing the valence gap. (arXiv:2203.07378v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20960;&#31181;&#39044;&#35757;&#32451;&#21464;&#20307;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#39046;&#22495;&#65292;&#20351;&#29992;Transformer&#26550;&#26500;&#33021;&#22815;&#22312;&#27809;&#26377;&#20351;&#29992;&#26174;&#24335;&#35821;&#35328;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26368;&#20339;&#30340;&#20215;&#20540;&#39044;&#27979;&#24615;&#33021;&#65292;&#30456;&#20851;&#24615;&#31995;&#25968;&#20026;0.638&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;Transformer&#26550;&#26500;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24182;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#38899;&#39057;&#39046;&#22495;&#65292;&#36825;&#31181;&#26550;&#26500;&#20063;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;(SER)&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#36824;&#27809;&#26377;&#35780;&#20272;&#27169;&#22411;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#19979;&#28216;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#23545;&#27867;&#21270;&#33021;&#21147;&#12289;&#31283;&#20581;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#26412;&#25991;&#22312;&#20960;&#31181;&#39044;&#35757;&#32451;&#21464;&#20307;&#30340;wav2vec 2.0&#21644;HuBERT&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#22312;MSP-Podcast&#30340;&#21796;&#36215;&#12289;&#25511;&#21046;&#21644;&#20215;&#20540;&#32500;&#24230;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#21516;&#26102;&#20351;&#29992;IEMOCAP&#21644;MOSI&#36827;&#34892;&#36328;&#35821;&#26009;&#24211;&#27867;&#21270;&#27979;&#35797;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#22312;&#19981;&#20351;&#29992;&#26174;&#24335;&#35821;&#35328;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;MSP-Podcast&#19978;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#20215;&#20540;&#39044;&#27979;&#24615;&#33021;&#65292;&#30456;&#20851;&#24615;&#31995;&#25968;&#20026;0.638&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in transformer-based architectures which are pre-trained in self-supervised manner have shown great promise in several machine learning tasks. In the audio domain, such architectures have also been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of .638 on MSP-Podcast. Further
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#39550;&#39542;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;UNet&#26550;&#26500;&#21644;&#25511;&#21046;&#31639;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#36710;&#36947;&#20998;&#21106;&#12289;&#36335;&#24452;&#39044;&#27979;&#21644;&#36710;&#36742;&#25511;&#21046;&#12290;&#19982;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#24367;&#26354;&#36947;&#36335;&#19978;&#30340;&#24615;&#33021;&#30456;&#24403;&#65292;&#20294;&#20855;&#26377;&#31471;&#21040;&#31471;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2112.08967</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;UNet&#26550;&#26500;&#29992;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Multi-task UNet architecture for end-to-end autonomous driving. (arXiv:2112.08967v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08967
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#39550;&#39542;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;UNet&#26550;&#26500;&#21644;&#25511;&#21046;&#31639;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#36710;&#36947;&#20998;&#21106;&#12289;&#36335;&#24452;&#39044;&#27979;&#21644;&#36710;&#36742;&#25511;&#21046;&#12290;&#19982;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#24367;&#26354;&#36947;&#36335;&#19978;&#30340;&#24615;&#33021;&#30456;&#24403;&#65292;&#20294;&#20855;&#26377;&#31471;&#21040;&#31471;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#39550;&#39542;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#22810;&#20219;&#21153;UNet&#65288;MTUNet&#65289;&#26550;&#26500;&#21644;&#25511;&#21046;&#31639;&#27861;&#25972;&#21512;&#22312;&#20174;&#21069;&#32622;&#25668;&#20687;&#22836;&#21040;&#39550;&#39542;&#20915;&#31574;&#30340;&#25968;&#25454;&#27969;&#31649;&#36947;&#20013;&#12290;&#23427;&#25552;&#20379;&#20102;&#23450;&#37327;&#25351;&#26631;&#26469;&#35780;&#20272;&#31471;&#21040;&#31471;&#39550;&#39542;&#31995;&#32479;&#30340;&#25972;&#20307;&#12289;&#21160;&#24577;&#21644;&#23454;&#26102;&#24615;&#33021;&#65292;&#20174;&#32780;&#35780;&#20272;MTUNet&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;&#19968;&#20010;&#20998;&#21106;&#20219;&#21153;&#12289;&#19968;&#20010;&#22238;&#24402;&#20219;&#21153;&#21644;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#65292;&#20998;&#21035;&#29992;&#20110;&#36710;&#36947;&#20998;&#21106;&#12289;&#36335;&#24452;&#39044;&#27979;&#21644;&#36710;&#36742;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#26550;&#26500;&#21464;&#20307;&#65292;&#24182;&#22312;&#22235;&#20010;&#38745;&#24577;&#24230;&#37327;&#30340;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#65292;&#28982;&#21518;&#36890;&#36807;&#20004;&#20010;&#38468;&#21152;&#30340;&#21160;&#24577;&#24230;&#37327;&#22312;&#23454;&#26102;&#20223;&#30495;&#20013;&#30830;&#23450;&#26368;&#20339;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#30456;&#21516;&#20219;&#21153;&#30340;&#24367;&#26354;&#36947;&#36335;&#19978;&#30340;&#24615;&#33021;&#19982;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#65292;&#32780;&#21518;&#32773;&#19981;&#26159;&#31471;&#21040;&#31471;&#30340;&#32780;&#26159;&#22810;&#20219;&#21153;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an end-to-end driving model that integrates a multi-task UNet (MTUNet) architecture and control algorithms in a pipeline of data flow from a front camera through this model to driving decisions. It provides quantitative measures to evaluate the holistic, dynamic, and real-time performance of end-to-end driving systems and thus the safety and interpretability of MTUNet. The architecture consists of one segmentation, one regression, and two classification tasks for lane segmentation, path prediction, and vehicle controls. We present three variants of the architecture having different complexities, compare them on different tasks in four static measures for both single and multiple tasks, and then identify the best one by two additional dynamic measures in real-time simulation. Our results show that the performance of the proposed supervised learning model is comparable to that of a reinforcement learning model on curvy roads for the same task, which is not end-to-end but multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;&#38543;&#26426;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#65288;RLSVI&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#34892;&#21160;&#20540;&#20989;&#25968;&#30340;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#36827;&#34892;&#25200;&#21160;&#65292;&#35825;&#23548;&#20986;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20855;&#26377;&#20302;&#31209;&#36716;&#31227;&#21160;&#24577;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLSVI&#30340;&#39057;&#29575;&#21518;&#24724;&#19978;&#30028;&#20026;$\widetilde O(d^2 H^2 \sqrt{T})$&#12290;&#36825;&#26159;&#23545;&#20110;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#38543;&#26426;&#25506;&#32034;&#30340;&#39318;&#20010;&#39057;&#29575;&#21518;&#24724;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/1911.00567</link><description>&lt;p&gt;
&#38543;&#26426;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Frequentist Regret Bounds for Randomized Least-Squares Value Iteration. (arXiv:1911.00567v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.00567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;&#38543;&#26426;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#65288;RLSVI&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#34892;&#21160;&#20540;&#20989;&#25968;&#30340;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#36827;&#34892;&#25200;&#21160;&#65292;&#35825;&#23548;&#20986;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20855;&#26377;&#20302;&#31209;&#36716;&#31227;&#21160;&#24577;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLSVI&#30340;&#39057;&#29575;&#21518;&#24724;&#19978;&#30028;&#20026;$\widetilde O(d^2 H^2 \sqrt{T})$&#12290;&#36825;&#26159;&#23545;&#20110;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#38543;&#26426;&#25506;&#32034;&#30340;&#39318;&#20010;&#39057;&#29575;&#21518;&#24724;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26377;&#38480;&#26102;&#38388;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;-&#21033;&#29992;&#22256;&#22659;&#12290;&#24403;&#29366;&#24577;&#31354;&#38388;&#24456;&#22823;&#25110;&#36830;&#32493;&#26102;&#65292;&#20256;&#32479;&#30340;&#34920;&#26684;&#26041;&#27861;&#19981;&#21487;&#34892;&#65292;&#24517;&#39035;&#37319;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;&#24418;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20048;&#35266;&#21021;&#22987;&#21270;&#30340;&#25913;&#36827;&#29256;&#26412;&#30340;&#38543;&#26426;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#65288;RLSVI&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#20854;&#20013;&#25506;&#32034;&#26159;&#36890;&#36807;&#25200;&#21160;&#34892;&#21160;&#20540;&#20989;&#25968;&#30340;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#26469;&#35825;&#23548;&#30340;&#12290;&#22312;&#20551;&#35774;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20855;&#26377;&#20302;&#31209;&#36716;&#31227;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLSVI&#30340;&#39057;&#29575;&#21518;&#24724;&#23558;&#19978;&#30028;&#20026;$\widetilde O(d^2 H^2 \sqrt{T})$&#65292;&#20854;&#20013;$ d $&#26159;&#29305;&#24449;&#32500;&#24230;&#65292;$ H $&#26159;&#26102;&#38388;&#38480;&#21046;&#65292;$ T $&#26159;&#24635;&#27493;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23545;&#20110;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#38543;&#26426;&#25506;&#32034;&#30340;&#31532;&#19968;&#20010;&#39057;&#29575;&#21518;&#24724;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning (RL). When the state space is large or continuous, traditional tabular approaches are unfeasible and some form of function approximation is mandatory. In this paper, we introduce an optimistically-initialized variant of the popular randomized least-squares value iteration (RLSVI), a model-free algorithm where exploration is induced by perturbing the least-squares approximation of the action-value function. Under the assumption that the Markov decision process has low-rank transition dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by $\widetilde O(d^2 H^2 \sqrt{T})$ where $ d $ are the feature dimension, $ H $ is the horizon, and $ T $ is the total number of steps. To the best of our knowledge, this is the first frequentist regret analysis for randomized exploration with function approximation.
&lt;/p&gt;</description></item></channel></rss>