<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21512;&#24182;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#8220;ZipIt&#65281;&#8221;&#12290;</title><link>http://arxiv.org/abs/2305.03053</link><description>&lt;p&gt;
ZipIt&#65281;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#21512;&#24182;&#19981;&#21516;&#20219;&#21153;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZipIt! Merging Models from Different Tasks without Training. (arXiv:2305.03053v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21512;&#24182;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#8220;ZipIt&#65281;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30340;&#28145;&#24230;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;&#23427;&#20204;&#32463;&#36807;&#35757;&#32451;&#30340;&#21333;&#19968;&#20219;&#21153;&#12290;&#26412;&#25991;&#35299;&#20915;&#23558;&#23436;&#20840;&#19981;&#21516;&#30340;&#12289;&#27599;&#20010;&#35299;&#20915;&#19968;&#20010;&#29420;&#31435;&#20219;&#21153;&#30340;&#27169;&#22411;&#21512;&#24182;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#26497;&#20854;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#20197;&#21069;&#30340;&#27169;&#22411;&#21512;&#24182;&#24037;&#20316;&#23558;&#19968;&#20010;&#27169;&#22411;&#32622;&#25442;&#21040;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#31354;&#38388;&#20013;&#65292;&#20877;&#23558;&#23427;&#20204;&#30456;&#21152;&#12290;&#34429;&#28982;&#36825;&#23545;&#20110;&#22312;&#21516;&#19968;&#20010;&#20219;&#21153;&#19978;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#36215;&#20316;&#29992;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#26410;&#33021;&#32771;&#34385;&#21040;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;ZipIt&#65281;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#24182;&#30456;&#21516;&#32467;&#26500;&#30340;&#20004;&#20010;&#20219;&#24847;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#32771;&#34385;&#21040;&#22312;&#27169;&#22411;&#20043;&#38388;&#27809;&#26377;&#20849;&#20139;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#21512;&#24182;&#38382;&#39064;&#25193;&#23637;&#21040;&#36824;&#20801;&#35768;&#21512;&#24182;&#27599;&#20010;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#65292;&#23450;&#20041;&#19968;&#20010;&#36890;&#29992;&#30340;&#8220;zip&#8221;&#25805;&#20316;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#25903;&#25345;&#37096;&#20998;&#21387;&#32553;&#27169;&#22411;&#30340;&#21151;&#33021;&#65292;&#30452;&#21040;&#29305;&#23450;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining completely distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then adds them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce "ZipIt!", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to additionally allow for merging features within each model by defining a general "zip" operation. Second, we add support for partially zipping the models up until a specified layer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411; $\textbf{TCOW}$&#65292;&#29992;&#20110;&#22312;&#37325;&#24230;&#36974;&#25377;&#21644;&#23481;&#22120;&#20013;&#36827;&#34892;&#35270;&#35273;&#36319;&#36394;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#28151;&#21512;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20986;&#20154;&#24847;&#26009;&#22320;&#36319;&#36394;&#30446;&#26631;&#65292;&#20294;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24517;&#39035;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.03052</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#36319;&#36394;&#21547;&#23481;&#22120;&#21644;&#36974;&#25377;&#29289;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Tracking through Containers and Occluders in the Wild. (arXiv:2305.03052v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411; $\textbf{TCOW}$&#65292;&#29992;&#20110;&#22312;&#37325;&#24230;&#36974;&#25377;&#21644;&#23481;&#22120;&#20013;&#36827;&#34892;&#35270;&#35273;&#36319;&#36394;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#28151;&#21512;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20986;&#20154;&#24847;&#26009;&#22320;&#36319;&#36394;&#30446;&#26631;&#65292;&#20294;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24517;&#39035;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#20081;&#19988;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#36319;&#36394;&#20855;&#26377;&#25345;&#20037;&#24615;&#30340;&#30446;&#26631;&#20173;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#38754;&#20020;&#30340;&#38590;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411; $\textbf{TCOW}$&#65292;&#29992;&#20110;&#22312;&#37325;&#24230;&#36974;&#25377;&#21644;&#23481;&#22120;&#20013;&#36827;&#34892;&#35270;&#35273;&#36319;&#36394;&#12290;&#25105;&#20204;&#35774;&#23450;&#20102;&#19968;&#20010;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#35270;&#39057;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#21106;&#20986;&#30446;&#26631;&#29289;&#20307;&#30340;&#25237;&#24433;&#33539;&#22260;&#20197;&#21450;&#21608;&#22260;&#30340;&#23481;&#22120;&#25110;&#36974;&#25377;&#29289;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#28151;&#21512;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#21464;&#21270;&#24418;&#24335;&#19979;&#30340;&#30417;&#30563;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#35780;&#20272;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#23427;&#20204;&#22312;&#26576;&#20123;&#20219;&#21153;&#21464;&#21270;&#30340;&#35774;&#32622;&#19979;&#33021;&#22815;&#20986;&#20154;&#24847;&#26009;&#22320;&#36319;&#36394;&#30446;&#26631;&#65292;&#20294;&#22312;&#25105;&#20204;&#23459;&#31216;&#19968;&#20010;&#36319;&#36394;&#27169;&#22411;&#24050;&#32463;&#33719;&#24471;&#20102;&#30495;&#27491;&#30340;&#23545;&#35937;&#24658;&#24120;&#24615;&#27010;&#24565;&#20043;&#21069;&#65292;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tracking objects with persistence in cluttered and dynamic environments remains a difficult challenge for computer vision systems. In this paper, we introduce $\textbf{TCOW}$, a new benchmark and model for visual tracking through heavy occlusion and containment. We set up a task where the goal is to, given a video sequence, segment both the projected extent of the target object, as well as the surrounding container or occluder whenever one exists. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment. We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of tracking targets under certain settings of task variation, there remains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#35270;&#35273;-&#35302;&#35273;&#30340;&#20132;&#20114;&#21512;&#25104;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35302;&#24863;&#34920;&#38754;&#35302;&#25720;&#21644;&#30475;&#21040;&#21512;&#25104;&#29289;&#20307;&#65292;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#35302;&#24863;&#25968;&#25454;&#38598;&#21644;&#24320;&#21457;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.03051</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#35270;&#35273;-&#35302;&#35273;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controllable Visual-Tactile Synthesis. (arXiv:2305.03051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#35270;&#35273;-&#35302;&#35273;&#30340;&#20132;&#20114;&#21512;&#25104;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35302;&#24863;&#34920;&#38754;&#35302;&#25720;&#21644;&#30475;&#21040;&#21512;&#25104;&#29289;&#20307;&#65292;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#35302;&#24863;&#25968;&#25454;&#38598;&#21644;&#24320;&#21457;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#24418;&#35774;&#35745;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#34394;&#25311;&#35797;&#31359;&#31561;&#20869;&#23481;&#21019;&#20316;&#24212;&#29992;&#26041;&#38754;&#20855;&#26377;&#21508;&#31181;&#21487;&#33021;&#24615;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21512;&#25104;&#36924;&#30495;&#30340;&#35270;&#35273;&#36755;&#20986;&#19978;&#65292;&#24448;&#24448;&#24573;&#30053;&#20854;&#20182;&#24863;&#23448;&#27169;&#24577;&#65292;&#22914;&#35302;&#35273;&#65292;&#36825;&#38480;&#21046;&#20102;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#29289;&#29702;&#20132;&#20114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#31181;&#22810;&#24863;&#23448;&#20307;&#39564;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#35302;&#35273;&#34920;&#38754;&#19978;&#28369;&#21160;&#25163;&#25351;&#26102;&#35302;&#25720;&#21644;&#30475;&#21040;&#21512;&#25104;&#23545;&#35937;&#12290;&#20027;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#35270;&#35273;&#21644;&#35302;&#35273;&#24863;&#30693;&#20043;&#38388;&#30340;&#24040;&#22823;&#23610;&#24230;&#24046;&#24322;&#20197;&#21450;&#35302;&#35273;&#20256;&#24863;&#25968;&#25454;&#21040;&#35302;&#35273;&#28210;&#26579;&#35774;&#22791;&#30340;&#26174;&#24335;&#26144;&#23556;&#30340;&#32570;&#20047;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20351;&#29992;GelSight&#20256;&#24863;&#22120;&#25910;&#38598;&#39640;&#20998;&#36776;&#29575;&#30340;&#35302;&#24863;&#25968;&#25454;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#35302;&#35273;&#26381;&#35013;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#21333;&#20010;&#33609;&#22270;&#20013;&#21512;&#25104;&#35270;&#35273;&#21644;&#35302;&#35273;&#36755;&#20986;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#20687;&#36136;&#37327;&#21644;&#35302;&#35273;&#28210;&#26579;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have various content creation applications such as graphic design, e-commerce, and virtual Try-on. However, current works mainly focus on synthesizing realistic visual outputs, often ignoring other sensory modalities, such as touch, which limits physical interaction with users. In this work, we leverage deep generative models to create a multi-sensory experience where users can touch and see the synthesized object when sliding their fingers on a haptic surface. The main challenges lie in the significant scale discrepancy between vision and touch sensing and the lack of explicit mapping from touch sensing data to a haptic rendering device. To bridge this gap, we collect high-resolution tactile data with a GelSight sensor and create a new visuotactile clothing dataset. We then develop a conditional generative model that synthesizes both visual and tactile outputs from a single sketch. We evaluate our method regarding image quality and tactile rendering accuracy. Fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03048</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#19968;&#27425;&#24615;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#25512;&#21160;&#19979;&#65292;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#27169;&#22411;&#65288;SAM&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#24378;&#22823;&#19988;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#38761;&#26032;&#20102;&#20998;&#21106;&#27169;&#22411;&#39046;&#22495;&#12290;&#23613;&#31649;SAM&#38750;&#24120;&#36890;&#29992;&#65292;&#20294;&#33258;&#21160;&#20026;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#23450;&#21046;SAM&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#25552;&#31034;&#65292;&#22914;&#22312;&#19981;&#21516;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#20320;&#30340;&#23456;&#29289;&#29399;&#31561;&#65292; &#36824;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;PerSAM&#12290;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#65292;PerSAM&#39318;&#20808;&#36890;&#36807;&#20301;&#32622;&#20808;&#39564;&#23450;&#20301;&#30446;&#26631;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#26469;&#22312;&#20854;&#20182;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#20998;&#21106;&#23427;&#65306;&#30446;&#26631;&#24341;&#23548;&#27880;&#24847;&#21147;&#65292;&#30446;&#26631;&#35821;&#20041;&#25552;&#31034;&#21644;&#32423;&#32852;&#21518;&#22788;&#29702;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;SAM&#30340;&#31169;&#20154;&#20351;&#29992;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32531;&#35299;&#25513;&#27169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;&#65292;&#21363;PerSAM-F&#12290;&#20923;&#32467;&#25972;&#20010;SAM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21487;&#23398;&#20064;&#26435;&#37325;&#29992;&#20110;&#22810;&#23610;&#24230;&#25513;&#27169;&#65292;&#20165;&#35757;&#32451;2&#20010;&#21442;&#25968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters wit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SELF-ALIGN&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.03047</link><description>&lt;p&gt;
&#21407;&#21017;&#39537;&#21160;&#33258;&#25105;&#23545;&#40784;&#30340;&#26368;&#23567;&#20154;&#21147;&#30417;&#30563;&#30340;&#35821;&#35328;&#27169;&#22411;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03047
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SELF-ALIGN&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;AI&#21161;&#25163;&#20195;&#29702;&#65292;&#22914;ChatGPT&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#24847;&#22270;&#65292;&#30830;&#20445;&#23427;&#20204;&#26159;&#26377;&#29992;&#30340;&#12289;&#36947;&#24503;&#30340;&#12289;&#21487;&#38752;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20381;&#36182;&#24615;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#38480;&#21046;AI&#21161;&#25163;&#20195;&#29702;&#30340;&#30495;&#27491;&#28508;&#21147;&#65292;&#22240;&#20026;&#33719;&#24471;&#20154;&#31867;&#30417;&#30563;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#30456;&#20851;&#38382;&#39064;&#26377;&#36136;&#37327;&#12289;&#21487;&#38752;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#33258;&#19968;&#33268;&#24615;&#21644;&#19981;&#33391;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; SELF-ALIGN&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;&#26041;&#27861;&#21253;&#25324;&#22235;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21512;&#25104;&#25552;&#31034;&#65292;&#20351;&#29992;&#20027;&#39064;&#24341;&#23548;&#26041;&#27861;&#22686;&#21152;&#25552;&#31034;&#22810;&#26679;&#24615;&#65307;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#23567;&#32452;&#20154;&#24037;&#32534;&#20889;&#30340;AI&#27169;&#22411;&#21407;&#21017;&#65292;&#24182;&#25351;&#23548;AI&#27169;&#22411;&#36981;&#24490;&#65307;
&lt;/p&gt;
&lt;p&gt;
Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and gu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#12289;&#21270;&#23398;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#26500;&#24615;&#33021;&#65292;&#21457;&#29616;&#37325;&#26500;&#20934;&#30830;&#24615;&#24778;&#20154;&#22320;&#20302;&#12290;&#28982;&#32780;&#65292;&#25913;&#21892;&#37325;&#26500;&#24182;&#19981;&#19968;&#23450;&#20250;&#24102;&#26469;&#26356;&#22909;&#30340;&#37319;&#26679;&#25110;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03041</link><description>&lt;p&gt;
VAE&#22312;&#37325;&#26500;&#20998;&#23376;&#32467;&#26500;&#26041;&#38754;&#25928;&#26524;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are VAEs Bad at Reconstructing Molecular Graphs?. (arXiv:2305.03041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#12289;&#21270;&#23398;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#26500;&#24615;&#33021;&#65292;&#21457;&#29616;&#37325;&#26500;&#20934;&#30830;&#24615;&#24778;&#20154;&#22320;&#20302;&#12290;&#28982;&#32780;&#65292;&#25913;&#21892;&#37325;&#26500;&#24182;&#19981;&#19968;&#23450;&#20250;&#24102;&#26469;&#26356;&#22909;&#30340;&#37319;&#26679;&#25110;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35768;&#22810;&#20998;&#23376;&#30340;&#29983;&#25104;&#27169;&#22411;&#20026;&#20998;&#23376;&#22270;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#19988;&#21270;&#23398;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#26500;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#23427;&#20204;&#30340;&#37325;&#26500;&#20934;&#30830;&#24615;&#24778;&#20154;&#30340;&#20302;&#65292;&#29978;&#33267;&#19981;&#22914;&#20043;&#21069;&#30475;&#20284;&#26356;&#38590;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25552;&#39640;&#37325;&#26500;&#24182;&#19981;&#30452;&#25509;&#23548;&#33268;&#26356;&#22909;&#30340;&#37319;&#26679;&#25110;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many contemporary generative models of molecules are variational auto-encoders of molecular graphs. One term in their training loss pertains to reconstructing the input, yet reconstruction capabilities of state-of-the-art models have not yet been thoroughly compared on a large and chemically diverse dataset. In this work, we show that when several state-of-the-art generative models are evaluated under the same conditions, their reconstruction accuracy is surprisingly low, worse than what was previously reported on seemingly harder datasets. However, we show that improving reconstruction does not directly lead to better sampling or optimization performance. Failed reconstructions from the MoLeR model are usually similar to the inputs, assembling the same motifs in a different way, and possess similar chemical properties such as solubility. Finally, we show that the input molecule and its failed reconstruction are usually mapped by the different encoders to statistically distinguishable 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;159&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#21450;&#20854;&#29992;&#25143;&#21453;&#39304;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#35745;&#31639;&#31508;&#35760;&#26412;&#20013;&#35774;&#35745;&#21487;&#35270;&#20998;&#26512;&#24037;&#20855;&#30340;&#29420;&#29305;&#26426;&#20250;&#21644;&#32771;&#34385;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.03039</link><description>&lt;p&gt;
SuperNOVA: &#35745;&#31639;&#31508;&#35760;&#26412;&#20013;&#20114;&#21160;&#21487;&#35270;&#21270;&#30340;&#35774;&#35745;&#31574;&#30053;&#19982;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks. (arXiv:2305.03039v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03039
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;159&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#21450;&#20854;&#29992;&#25143;&#21453;&#39304;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#35745;&#31639;&#31508;&#35760;&#26412;&#20013;&#35774;&#35745;&#21487;&#35270;&#20998;&#26512;&#24037;&#20855;&#30340;&#29420;&#29305;&#26426;&#20250;&#21644;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#31508;&#35760;&#26412;&#65292;&#22914; Jupyter Notebook&#65292;&#24050;&#25104;&#20026;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20107;&#23454;&#32534;&#31243;&#29615;&#22659;&#12290;&#35768;&#22810;&#21487;&#35270;&#21270;&#30740;&#31350;&#32773;&#21644;&#23454;&#36341;&#32773;&#24050;&#24320;&#21457;&#20986;&#25903;&#25345;&#31508;&#35760;&#26412;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#22312;&#31508;&#35760;&#26412;&#20013;&#35774;&#35745; VA &#24037;&#20855;&#30340;&#36866;&#24403;&#26041;&#27861;&#30340;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#20851;&#38190;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512; 159 &#20010;&#31508;&#35760;&#26412; VA &#24037;&#20855;&#21450;&#20854;&#29992;&#25143;&#21453;&#39304;&#26469;&#30740;&#31350;&#36825;&#19968;&#39046;&#22495;&#30340;&#35774;&#35745;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;&#26469;&#33258;&#23398;&#26415;&#35770;&#25991;&#30340; 62 &#20010;&#31995;&#32479;&#21644;&#26469;&#33258;&#36890;&#36807;&#22312; GitHub &#19978;&#25235;&#21462; 860 &#19975;&#20010;&#31508;&#35760;&#26412;&#32780;&#33719;&#24471;&#30340;&#21253;&#21547;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#30340; 55k &#20010;&#31508;&#35760;&#26412;&#27744;&#20013;&#30340; 103 &#20010;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102; 15 &#20010;&#29992;&#25143;&#30740;&#31350;&#30340;&#21457;&#29616;&#21644; 379 &#20010; GitHub &#38382;&#39064;&#20013;&#30340;&#29992;&#25143;&#21453;&#39304;&#12290;&#36890;&#36807;&#36825;&#39033;&#24037;&#20316;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26410;&#26469;&#31508;&#35760;&#26412; VA &#24037;&#20855;&#30340;&#29420;&#29305;&#35774;&#35745;&#26426;&#20250;&#21644;&#32771;&#34385;&#22240;&#32032;&#65292;&#20363;&#22914;&#22312;&#31508;&#35760;&#26412;&#20013;&#20351;&#29992;&#21644;&#25805;&#20316;&#22810;&#27169;&#24577;&#25968;&#25454;&#20197;&#21450;&#24179;&#34913;&#21487;&#35270;&#21270;&#31508;&#35760;&#26412;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational notebooks such as Jupyter Notebook have become data scientists' de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. However, little is known about the appropriate design of visual analytics (VA) tools in notebooks. To bridge this critical research gap, we investigate the design strategies in this space by analyzing 159 notebook VA tools and their users' feedback. Our analysis encompasses 62 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. We also examine findings from 15 user studies and user feedback in 379 GitHub issues. Through this work, we identify unique design opportunities and considerations for future notebook VA tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-noteb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#21644;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#31561;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26410;&#30693;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#36890;&#36807;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#30340;&#21344;&#25454;&#32593;&#32476;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03036</link><description>&lt;p&gt;
&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#23398;&#20064;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Learning Hand-Held Object Reconstruction from In-The-Wild Videos. (arXiv:2305.03036v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#21644;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#31561;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26410;&#30693;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#36890;&#36807;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#30340;&#21344;&#25454;&#32593;&#32476;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#21333;&#24433;&#20687;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#26041;&#27861;&#20381;&#36182;&#20110;&#38590;&#20197;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35268;&#27169;&#21270;&#25910;&#38598;&#30340;&#30452;&#25509;3D&#24418;&#29366;&#30417;&#30563;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#22312;&#37326;&#22806;&#29615;&#22659;&#19979;&#38754;&#23545;&#26032;&#39062;&#29289;&#20307;&#26102;&#38590;&#20197;&#25512;&#24191;&#12290;&#26412;&#25991;&#20174;&#29983;&#21160;&#30340;&#37326;&#22806;&#21407;&#22987;&#35270;&#39057;&#25968;&#25454;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#22810;&#35270;&#35282;&#20108;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#36825;&#38656;&#35201;&#24212;&#23545;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#26410;&#30693;&#30340;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#12290;&#23545;&#20110;&#21069;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;ObMan&#25968;&#25454;&#38598;&#20013;&#21512;&#25104;&#30340;&#29289;&#20307;&#26469;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#38388;&#25509;&#30340;&#19977;&#32500;&#32447;&#32034;&#26469;&#35757;&#32451;&#21344;&#25454;&#32593;&#32476;&#65292;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#30340;&#19977;&#32500;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works for reconstructing hand-held objects from a single image rely on direct 3D shape supervision which is challenging to gather in real world at scale. Consequently, these approaches do not generalize well when presented with novel objects in in-the-wild settings. While 3D supervision is a major bottleneck, there is an abundance of in-the-wild raw video data showing hand-object interactions. In this paper, we automatically extract 3D supervision (via multiview 2D supervision) from such raw video data to scale up the learning of models for hand-held object reconstruction. This requires tackling two key challenges: unknown camera pose and occlusion. For the former, we use hand pose (predicted from existing techniques, e.g. FrankMocap) as a proxy for object pose. For the latter, we learn data-driven 3D shape priors using synthetic objects from the ObMan dataset. We use these indirect 3D cues to train occupancy networks that predict the 3D shape of objects from a single RGB image. 
&lt;/p&gt;</description></item><item><title>FastAMI&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#27604;&#36739;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26469;&#23454;&#29616;&#20598;&#28982;&#24615;&#35843;&#25972;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25490;&#21015;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.03022</link><description>&lt;p&gt;
FastAMI -- &#19968;&#31181;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#29992;&#20110;&#32858;&#31867;&#27604;&#36739;&#24230;&#37327;&#20013;&#30340;&#20598;&#28982;&#24615;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
FastAMI -- a Monte Carlo Approach to the Adjustment for Chance in Clustering Comparison Metrics. (arXiv:2305.03022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03022
&lt;/p&gt;
&lt;p&gt;
FastAMI&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#27604;&#36739;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26469;&#23454;&#29616;&#20598;&#28982;&#24615;&#35843;&#25972;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25490;&#21015;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#26680;&#24515;&#65292;&#38543;&#30528;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#22686;&#21152;&#65292;&#20854;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#30340;&#22686;&#38271;&#65292;&#24102;&#26377;&#20598;&#28982;&#24615;&#35843;&#25972;&#30340;&#32858;&#31867;&#27604;&#36739;&#21464;&#24471;&#35745;&#31639;&#22256;&#38590;&#65292;&#23548;&#33268;&#27809;&#26377;&#20559;&#35265;&#30340;&#30495;&#23454;&#27604;&#36739;&#21644;&#35299;&#20915;&#26041;&#26696;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FastAMI&#65292;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#20272;&#35745;&#32463;&#36807;&#35843;&#25972;&#30340;&#20114;&#20449;&#24687;&#65288;AMI&#65289;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#26631;&#20934;&#21270;&#20114;&#20449;&#24687;&#65288;SMI&#65289;&#12290;&#19982;&#20934;&#30830;&#35745;&#31639;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36275;&#22815;&#24555;&#65292;&#21487;&#20197;&#20026;&#22823;&#22411;&#25968;&#25454;&#38598;&#21551;&#29992;&#36825;&#20123;&#24102;&#26377;&#35843;&#25972;&#30340;&#20449;&#24687;&#35770;&#27604;&#36739;&#65292;&#21516;&#26102;&#20445;&#25345;&#27604;&#37197;&#23545;&#26041;&#27861;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is at the very core of machine learning, and its applications proliferate with the increasing availability of data. However, as datasets grow, comparing clusterings with an adjustment for chance becomes computationally difficult, preventing unbiased ground-truth comparisons and solution selection. We propose FastAMI, a Monte Carlo-based method to efficiently approximate the Adjusted Mutual Information (AMI) and extend it to the Standardized Mutual Information (SMI). The approach is compared with the exact calculation and a recently developed variant of the AMI based on pairwise permutations, using both synthetic and real data. In contrast to the exact calculation our method is fast enough to enable these adjusted information-theoretic comparisons for large datasets while maintaining considerably more accurate results than the pairwise approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.03017</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21644;&#26368;&#36817;&#19968;&#30452;&#22312;&#36827;&#34892;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#23436;&#25104;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#12290;&#30001;&#20110;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#22312;&#20114;&#32852;&#32593;&#19978;&#23547;&#25214;&#30456;&#20851;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#21033;&#29992;&#24320;&#28304;&#39033;&#30446;&#21644;&#38750;&#27491;&#24335;&#25991;&#26723;&#12290;&#20026;&#20102;&#25214;&#21040;&#26377;&#29992;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#38750;&#27491;&#24335;&#25991;&#26723;&#65288;&#22914;Stack Overflow&#35752;&#35770;&#21644;&#35770;&#22363;&#65289;&#21487;&#20197;&#38750;&#24120;&#23453;&#36149;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;Stack Overflow&#65292;&#23427;&#26159;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#35752;&#35770;&#19981;&#21516;&#20027;&#39064;&#30340;&#27969;&#34892;&#36164;&#28304;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#33616;&#20195;&#30721;&#31034;&#20363;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#25512;&#33616;&#20102;Java&#32534;&#31243;&#35821;&#35328;&#20013;&#26368;&#20339;&#30340;&#20195;&#30721;&#31034;&#20363;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;BERT&#26469;&#36827;&#34892;&#22788;&#29702;&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.02997</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#32988;&#36807;&#22686;&#24378;&#26641;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02997
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#20043;&#19968;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#20154;&#20204;&#20173;&#22312;&#31215;&#26497;&#35752;&#35770;NN&#26159;&#21542;&#36890;&#24120;&#20248;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#35201;&#20040;&#35748;&#20026;GBDT&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19968;&#36143;&#20248;&#20110;NN&#65292;&#35201;&#20040;&#35748;&#20026;NN&#20248;&#20110;GBDT&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#38382;&#65306;'&#36825;&#37325;&#35201;&#21527;&#65311;'&#25105;&#20204;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#27604;&#36739;19&#31181;&#31639;&#27861;&#65292;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;'NN vs. GBDT'&#20105;&#35770;&#34987;&#36807;&#20998;&#24378;&#35843;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#30456;&#24403;&#22810;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#35201;&#20040;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#35201;&#20040;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;965&#20010;&#20803;&#29305;&#24449;&#65292;&#20197;&#30830;&#23450;&#25968;&#25454;&#38598;&#30340;&#21738;&#20123;&#29305;&#24615;&#20351;NN&#25110;GBDT&#26356;&#36866;&#21512;&#34920;&#29616;&#33391;&#22909;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GBDT&#35201;&#27604;NN&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02996</link><description>&lt;p&gt;
&#24102;&#26377;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;CUR k-NN&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#38170;&#23450;&#39033;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders. (arXiv:2305.02996v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;ANNCUR&#27169;&#22411;&#20013;&#39640;&#21069;k&#39033;&#30340;&#36924;&#36817;&#35823;&#24046;&#21644;&#21484;&#22238;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-encoder models, which jointly encode and score a query-item pair, are typically prohibitively expensive for k-nearest neighbor search. Consequently, k-NN search is performed not with a cross-encoder, but with a heuristic retrieve (e.g., using BM25 or dual-encoder) and re-rank approach. Recent work proposes ANNCUR (Yadav et al., 2022) which uses CUR matrix factorization to produce an embedding space for efficient vector-based search that directly approximates the cross-encoder without the need for dual-encoders. ANNCUR defines this shared query-item embedding space by scoring the test query against anchor items which are sampled uniformly at random. While this minimizes average approximation error over all items, unsuitably high approximation error on top-k items remains and leads to poor recall of top-k (and especially top-1) items. Increasing the number of anchor items is a straightforward way of improving the approximation error and hence k-NN recall of ANNCUR but at the cost o
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02995</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the nonlinear correlation of ML performance between data subpopulations. (arXiv:2305.02995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02995
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#23545;&#20110;&#21487;&#38752;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#35748;&#20026;&#35757;&#32451;&#25968;&#25454;&#20869;&#37096;&#30340;&#20934;&#30830;&#24615;&#21644;&#26032;&#25968;&#25454;&#22806;&#37096;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#36817;&#20046;&#23436;&#32654;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35757;&#32451;&#26102;&#26399;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#65292;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#20026;&#24494;&#22937;&#65292;&#24182;&#19988;&#22312;&#19978;&#21319;&#38454;&#27573;&#23384;&#22312;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#65288;&#25243;&#29289;&#32447;&#19978;&#21319;&#26354;&#32447;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the performance of machine learning (ML) models across diverse data distributions is critically important for reliable applications. Despite recent empirical studies positing a near-perfect linear correlation between in-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically demonstrate that this correlation is more nuanced under subpopulation shifts. Through rigorous experimentation and analysis across a variety of datasets, models, and training epochs, we demonstrate that OOD performance often has a nonlinear correlation with ID performance in subpopulation shifts. Our findings, which contrast previous studies that have posited a linear correlation in model performance during distribution shifts, reveal a "moon shape" correlation (parabolic uptrend curve) between the test performance on the majority subpopulation and the minority subpopulation. This non-trivial nonlinear correlation holds across model architectures, hyperparameters, training durations
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.02993</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;7: &#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data. (arXiv:2305.02993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#20219;&#21153;7&#30340;&#32467;&#26524;&#65292;&#35813;&#20219;&#21153;&#20027;&#35201;&#28041;&#21450;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#20013;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI4CT&#65289;&#65292;&#30001;&#20004;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#65306;&#19968;&#20010;&#26159;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#65292;&#21478;&#19968;&#20010;&#26159;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#21307;&#23398;&#21644;&#25968;&#23383;&#25512;&#29702;&#65292;&#36825;&#23545;&#20110;&#24320;&#21457;&#33021;&#22815;&#36827;&#34892;&#22823;&#35268;&#27169;&#21307;&#30103;&#35777;&#25454;&#35299;&#37322;&#21644;&#26816;&#32034;&#12289;&#25552;&#20379;&#20010;&#24615;&#21270;&#22522;&#20110;&#35777;&#25454;&#30340;&#20445;&#20581;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#31532;1&#20010;&#23376;&#20219;&#21153;&#8220;&#34164;&#21547;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;40&#20301;&#21442;&#36187;&#32773;&#30340;643&#20221;&#25552;&#20132;&#65292;&#31532;2&#20010;&#23376;&#20219;&#21153;&#8220;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;23&#20301;&#21442;&#36187;&#32773;&#30340;364&#20221;&#25552;&#20132;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22823;&#37096;&#20998;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#34164;&#21547;&#20219;&#21153;&#19978;&#26410;&#33021;&#26126;&#26174;&#20248;&#20110;&#22823;&#22810;&#25968;&#31867;&#22522;&#32447;&#65292;&#32780;&#25105;&#20204;&#35266;&#23519;&#21040;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#34164;&#21547;&#20219;&#21153;&#12290;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2 tasks, a Natural Language Inference (NLI) task, and an evidence selection task on clinical trial data. The proposed challenges require multi-hop biomedical and numerical reasoning, which are of significant importance to the development of systems capable of large-scale interpretation and retrieval of medical evidence, to provide personalized evidence-based care.  Task 1, the entailment task, received 643 submissions from 40 participants, and Task 2, the evidence selection task, received 364 submissions from 23 participants. The tasks are challenging, with the majority of submitted systems failing to significantly outperform the majority class baseline on the entailment task, and we observe significantly better performance on the evidence selection task than on the entailment task. Increasing the number of model parameters leads to a di
&lt;/p&gt;</description></item><item><title>ExeKGLib&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;Python&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#21487;&#24110;&#21161;&#19981;&#20855;&#22791;&#28145;&#20837;ML&#30693;&#35782;&#30340;&#29992;&#25143;&#26500;&#24314;&#21487;&#25191;&#34892;&#30340;ML&#24037;&#20316;&#27969;&#65292;&#24182;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02966</link><description>&lt;p&gt;
ExeKGLib&#65306;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#24211;
&lt;/p&gt;
&lt;p&gt;
ExeKGLib: Knowledge Graphs-Empowered Machine Learning Analytics. (arXiv:2305.02966v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02966
&lt;/p&gt;
&lt;p&gt;
ExeKGLib&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;Python&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#21487;&#24110;&#21161;&#19981;&#20855;&#22791;&#28145;&#20837;ML&#30693;&#35782;&#30340;&#29992;&#25143;&#26500;&#24314;&#21487;&#25191;&#34892;&#30340;ML&#24037;&#20316;&#27969;&#65292;&#24182;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24211;&#23545;ML&#20174;&#19994;&#32773;&#24320;&#25918;&#12290;&#20856;&#22411;&#30340;ML&#27969;&#31243;&#26159;&#22797;&#26434;&#30340;&#65292;&#30001;&#19968;&#31995;&#21015;&#27493;&#39588;&#32452;&#25104;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#35843;&#29992;&#20102;&#20960;&#20010;ML&#24211;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ExeKGLib&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24211;&#65292;&#20801;&#35768;&#20855;&#26377;&#32534;&#30721;&#25216;&#33021;&#21644;&#26368;&#23567;ML&#30693;&#35782;&#30340;&#29992;&#25143;&#26500;&#24314;ML&#27969;&#27700;&#32447;&#12290;ExeKGLib&#20381;&#36182;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#25913;&#21892;&#26500;&#24314;&#30340;ML&#24037;&#20316;&#27969;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#29992;&#24615;&#65292;&#24182;&#30830;&#20445;&#20854;&#21487;&#25191;&#34892;&#24615;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;ExeKGLib&#30340;&#29992;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;ML&#20195;&#30721;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#23637;&#31034;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning (ML) libraries are accessible online for ML practitioners. Typical ML pipelines are complex and consist of a series of steps, each of them invoking several ML libraries. In this demo paper, we present ExeKGLib, a Python library that allows users with coding skills and minimal ML knowledge to build ML pipelines. ExeKGLib relies on knowledge graphs to improve the transparency and reusability of the built ML workflows, and to ensure that they are executable. We demonstrate the usage of ExeKGLib and compare it with conventional ML code to show its benefits.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#26435;&#37325;&#24433;&#21709;&#30340;&#35745;&#25968;&#36172;&#21338;&#26426;(WTB)&#35774;&#32622;&#65292;&#36890;&#36807;Repeated Exposure Optimality(REO)&#26469;&#30740;&#31350;&#23427;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#28385;&#36275;REO&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.02955</link><description>&lt;p&gt;
&#21463;&#26435;&#37325;&#24433;&#21709;&#30340;&#35745;&#25968;&#36172;&#21338;&#26426;: &#36890;&#36807;&#37325;&#22797;&#26292;&#38706;&#26469;&#20811;&#26381;&#19981;&#21487;&#35299;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weighted Tallying Bandits: Overcoming Intractability via Repeated Exposure Optimality. (arXiv:2305.02955v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02955
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#26435;&#37325;&#24433;&#21709;&#30340;&#35745;&#25968;&#36172;&#21338;&#26426;(WTB)&#35774;&#32622;&#65292;&#36890;&#36807;Repeated Exposure Optimality(REO)&#26469;&#30740;&#31350;&#23427;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#28385;&#36275;REO&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#25110;&#20247;&#21253;&#24212;&#29992;&#20013;&#65292;&#20154;&#31867;&#30340;&#20559;&#22909;&#25110;&#33021;&#21147;&#36890;&#24120;&#26159;&#31639;&#27861;&#26368;&#36817;&#34892;&#21160;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290; &#30456;&#20851;&#24037;&#20316;&#24050;&#32463;&#24418;&#24335;&#21270;&#20102;&#35774;&#32622;&#65292;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#34892;&#21160;&#30340;&#25439;&#22833;&#26159;&#26368;&#36817;$m$&#20010;&#26102;&#38388;&#27493;&#20013;&#35813;&#34892;&#21160;&#30340;&#25773;&#25918;&#27425;&#25968;&#30340;&#20989;&#25968;&#65292;&#20854;&#20013;$m$&#23545;&#24212;&#20110;&#20154;&#31867;&#35760;&#24518;&#33021;&#21147;&#30340;&#19978;&#38480;&#12290; &#20026;&#20102;&#26356;&#24544;&#23454;&#22320;&#21453;&#26144;&#20154;&#31867;&#35760;&#24518;&#38543;&#26102;&#38388;&#30340;&#34928;&#20943;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21463;&#26435;&#37325;&#24433;&#21709;&#30340;&#35745;&#25968;&#36172;&#21338;&#26426;(WTB)&#65292;&#23427;&#36890;&#36807;&#35201;&#27714;&#34892;&#21160;&#25439;&#22833;&#26159;&#26368;&#36817;$m$&#20010;&#26102;&#38388;&#27493;&#20013;&#35813;&#33218;&#34987;&#29609;&#30340;&#27425;&#25968;&#30340;&#21152;&#26435;&#24635;&#21644;&#30340;&#20989;&#25968;&#26469;&#27010;&#25324;&#36825;&#20010;&#35774;&#32622;&#12290;&#38500;&#38750;&#36827;&#19968;&#27493;&#20551;&#35774;&#65292;&#21542;&#21017;WTB&#35774;&#32622;&#26159;&#19981;&#21487;&#35299;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;Repeated Exposure Optimality(REO)&#19979;&#30740;&#31350;&#20102;&#23427;&#65292;&#35813;&#26465;&#20214;&#26159;&#21463;&#20154;&#20307;&#29983;&#29702;&#23398;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#23427;&#35201;&#27714;&#23384;&#22312;&#19968;&#31181;&#34892;&#21160;&#65292;&#24403;&#21453;&#22797;&#25773;&#25918;&#26102;&#65292;&#26368;&#32456;&#23558;&#20135;&#29983;&#27604;&#20219;&#20309;&#20854;&#20182;&#34892;&#21160;&#26356;&#23567;&#30340;&#25439;&#22833;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#28385;&#36275;WTB&#35774;&#32622;&#19979;&#30340;REO&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#22320;&#32553;&#25918;$m$&#21644;&#34892;&#21160;&#38598;&#22823;&#23567;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290; &#25105;&#20204;&#30340;&#35777;&#26126;&#25216;&#26415;&#35201;&#27714;&#22312;&#19968;&#31181;&#35299;&#32806;&#24418;&#24335;&#19979;&#36827;&#34892;&#26032;&#39062;&#30340;&#27987;&#24230;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommender system or crowdsourcing applications of online learning, a human's preferences or abilities are often a function of the algorithm's recent actions. Motivated by this, a significant line of work has formalized settings where an action's loss is a function of the number of times that action was recently played in the prior $m$ timesteps, where $m$ corresponds to a bound on human memory capacity. To more faithfully capture decay of human memory with time, we introduce the Weighted Tallying Bandit (WTB), which generalizes this setting by requiring that an action's loss is a function of a \emph{weighted} summation of the number of times that arm was played in the last $m$ timesteps. This WTB setting is intractable without further assumption. So we study it under Repeated Exposure Optimality (REO), a condition motivated by the literature on human physiology, which requires the existence of an action that when repetitively played will eventually yield smaller loss than any othe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#31181;&#32676;&#21327;&#21161;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#26469;&#33258;&#31181;&#32676;&#20248;&#21270;&#36845;&#20195;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28155;&#21152;&#21040;&#31163;&#31574;&#30053;&#26356;&#26032;&#20013;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20165;&#20351;&#29992;&#31181;&#32676;&#20248;&#21270;&#30340;&#26368;&#26032;&#25968;&#25454;&#20316;&#20026;&#31163;&#31574;&#30053;&#26356;&#26032;&#30340;&#26657;&#27491;&#39033;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#21892;&#20102;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02949</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#31181;&#32676;&#21327;&#21161;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Population-assisted Off-policy Reinforcement Learning. (arXiv:2305.02949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#31181;&#32676;&#21327;&#21161;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#26469;&#33258;&#31181;&#32676;&#20248;&#21270;&#36845;&#20195;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28155;&#21152;&#21040;&#31163;&#31574;&#30053;&#26356;&#26032;&#20013;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20165;&#20351;&#29992;&#31181;&#32676;&#20248;&#21270;&#30340;&#26368;&#26032;&#25968;&#25454;&#20316;&#20026;&#31163;&#31574;&#30053;&#26356;&#26032;&#30340;&#26657;&#27491;&#39033;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#21892;&#20102;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30001;&#20110;&#26799;&#24230;&#26356;&#26032;&#21644;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#25968;&#25454;&#37325;&#29992;&#32780;&#20855;&#26377;&#39640;&#25928;&#30340;&#26679;&#26412;&#21033;&#29992;&#29575;&#65292;&#20294;&#30001;&#20110;&#26377;&#38480;&#30340;&#25506;&#32034;&#33021;&#21147;&#24448;&#24448;&#38590;&#20197;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#31181;&#32676;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#20294;&#20854;&#21551;&#21457;&#24335;&#40657;&#30418;&#25805;&#20316;&#25928;&#29575;&#20302;&#19979;&#12290;&#36817;&#26399;&#30340;&#31639;&#27861;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20849;&#20139;&#22238;&#25918;&#32531;&#20914;&#21306;&#36830;&#25509;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#31181;&#32676;&#20248;&#21270;&#36845;&#20195;&#20013;&#20351;&#29992;&#22810;&#26679;&#21270;&#25968;&#25454;&#23545;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#20808;&#20998;&#26512;&#20102;&#22522;&#20110;&#31181;&#32676;&#20248;&#21270;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20351;&#29992;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#31181;&#32676;&#25968;&#25454;&#21487;&#33021;&#20250;&#24341;&#20837;&#34987;&#24573;&#35270;&#30340;&#35823;&#24046;&#24182;&#24433;&#21709;&#24615;&#33021;&#12290;&#20026;&#20102;&#26816;&#39564;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21644;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#35774;&#35745;&#65292;&#24182;&#22312;&#24320;&#25918;AI gym&#20013;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#31181;&#32676;&#25968;&#25454;&#30830;&#23454;&#20250;&#25439;&#23475;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#31181;&#32676;&#20248;&#21270;&#30340;&#26368;&#26032;&#25968;&#25454;&#20316;&#20026;&#31163;&#31574;&#30053;&#26356;&#26032;&#30340;&#26657;&#27491;&#39033;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20010;&#26657;&#27491;&#39033;&#21487;&#20197;&#22312;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#26041;&#38754;&#37117;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results su
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;</title><link>http://arxiv.org/abs/2305.02942</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#34913;&#37327;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#22312;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging gradient-derived metrics for data selection and valuation in differentially private training. (arXiv:2305.02942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02942
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30417;&#31649;&#25285;&#24551;&#21644;&#21442;&#19982;&#24230;&#30340;&#19981;&#36275;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#26159;&#35299;&#20915;&#30417;&#31649;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#26159;&#20854;&#20013;&#26368;&#24120;&#29992;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#26799;&#24230;&#20449;&#24687;&#26469;&#35782;&#21035;&#38544;&#31169;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26368;&#20005;&#26684;&#30340;&#38544;&#31169;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#30528;&#33021;&#22815;&#20026;&#23458;&#25143;&#25552;&#20379;&#26377;&#21407;&#21017;&#30340;&#25968;&#25454;&#36873;&#25321;&#24037;&#20855;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining high-quality data for collaborative training of machine learning models can be a challenging task due to A) the regulatory concerns and B) lack of incentive to participate. The first issue can be addressed through the use of privacy enhancing technologies (PET), one of the most frequently used one being differentially private (DP) training. The second challenge can be addressed by identifying which data points can be beneficial for model training and rewarding data owners for sharing this data. However, DP in deep learning typically adversely affects atypical (often informative) data samples, making it difficult to assess the usefulness of individual contributions. In this work we investigate how to leverage gradient information to identify training samples of interest in private training settings. We show that there exist techniques which are able to provide the clients with the tools for principled data selection even in strictest privacy settings.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#20998;&#25104;&#38598;&#32676;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#25311;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#26631;&#20934;&#27491;&#24577;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.02930</link><description>&lt;p&gt;
&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Piecewise Normalizing Flows. (arXiv:2305.02930v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02930
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#20998;&#25104;&#38598;&#32676;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#25311;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#26631;&#20934;&#27491;&#24577;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#19968;&#31181;&#36890;&#36807;&#20174;&#22522;&#30784;&#20998;&#24067;&#36827;&#34892;&#21487;&#36870;&#36716;&#25442;&#26469;&#23545;&#22797;&#26434;&#27010;&#29575;&#23494;&#24230;&#36827;&#34892;&#24314;&#27169;&#30340;&#25104;&#29087;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#20998;&#24067;&#33021;&#21542;&#31934;&#30830;&#22320;&#34987;&#24402;&#19968;&#21270;&#27969;&#25152;&#25429;&#25417;&#65292;&#24378;&#28872;&#21463;&#21040;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#30446;&#26631;&#21644;&#22522;&#30784;&#20998;&#24067;&#20043;&#38388;&#30340;&#25299;&#25169;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#24046;&#65292;&#22914;&#23545;&#20110;&#22810;&#27169;&#24577;&#38382;&#39064;&#12290;&#19968;&#20123;&#19981;&#21516;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411; [Izmailov et al., 2020&#12289;Ardizzone et al., 2020&#12289;Hagemann and Neumayer, 2021] &#25110;&#23398;&#20064;&#25509;&#21463;/&#25298;&#32477;&#37319;&#26679; [Stimper et al., 2022] &#26469;&#20462;&#25913;&#22522;&#30784;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#27573;&#24402;&#19968;&#21270;&#27969;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#20998;&#25104;&#38598;&#32676;&#65292;&#24182;&#35757;&#32451;&#19968;&#31995;&#21015;&#27969;&#26469;&#27169;&#25311;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows are an established approach for modelling complex probability densities through invertible transformations from a base distribution. However, the accuracy with which the target distribution can be captured by the normalizing flow is strongly influenced by the topology of the base distribution. A mismatch between the topology of the target and the base can result in a poor performance, as is the case for multi-modal problems. A number of different works have attempted to modify the topology of the base distribution to better match the target, either through the use of Gaussian Mixture Models [Izmailov et al., 2020, Ardizzone et al., 2020, Hagemann and Neumayer, 2021] or learned accept/reject sampling [Stimper et al., 2022]. We introduce piecewise normalizing flows which divide the target distribution into clusters, with topologies that better match the standard normal base distribution, and train a series of flows to model complex multi-modal targets. The piecewise nat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#65288;ULTR&#65289;&#30340;&#22522;&#30784;&#27010;&#24565;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#21450;&#20960;&#31181;&#23454;&#38469;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;&#25945;&#31243;&#20998;&#20026;&#22235;&#20010;&#37096;&#20998;&#65306;&#20559;&#24046;&#30340;&#27010;&#36848;&#65292;ULTR&#30340;&#26368;&#26032;&#20272;&#35745;&#25216;&#26415;&#65292;ULTR&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;ULTR&#19982;&#25490;&#21517;&#20844;&#24179;&#24615;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.02914</link><description>&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#30340;&#22522;&#30784;&#21644;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in the Foundations and Applications of Unbiased Learning to Rank. (arXiv:2305.02914v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#65288;ULTR&#65289;&#30340;&#22522;&#30784;&#27010;&#24565;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#21450;&#20960;&#31181;&#23454;&#38469;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;&#25945;&#31243;&#20998;&#20026;&#22235;&#20010;&#37096;&#20998;&#65306;&#20559;&#24046;&#30340;&#27010;&#36848;&#65292;ULTR&#30340;&#26368;&#26032;&#20272;&#35745;&#25216;&#26415;&#65292;ULTR&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;ULTR&#19982;&#25490;&#21517;&#20844;&#24179;&#24615;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#65288;ULTR&#65289;&#39046;&#22495;&#33258;&#35806;&#29983;&#20197;&#26469;&#19968;&#30452;&#22788;&#20110;&#38750;&#24120;&#27963;&#36291;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20960;&#39033;&#26377;&#24433;&#21709;&#21147;&#30340;&#36827;&#23637;&#12290;&#26412;&#25945;&#31243;&#26082;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#26680;&#24515;&#27010;&#24565;&#65292;&#21448;&#27010;&#36848;&#20102;&#20854;&#22522;&#30784;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#20854;&#26041;&#27861;&#30340;&#20960;&#31181;&#24212;&#29992;&#12290;&#26412;&#25945;&#31243;&#20998;&#20026;&#22235;&#37096;&#20998;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21487;&#20197;&#29992;ULTR&#26041;&#27861;&#35299;&#20915;&#30340;&#19981;&#21516;&#24418;&#24335;&#30340;&#20559;&#24046;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20840;&#38754;&#35752;&#35770;&#20102;ULTR&#39046;&#22495;&#30340;&#26368;&#26032;&#20272;&#35745;&#25216;&#26415;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;ULTR&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21457;&#24067;&#32467;&#26524;&#12290;&#31532;&#22235;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;ULTR&#19982;&#25490;&#21517;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#21453;&#24605;&#20102;ULTR&#30740;&#31350;&#21450;&#20854;&#24212;&#29992;&#30340;&#26410;&#26469;&#12290;&#26412;&#25945;&#31243;&#26088;&#22312;&#20351;&#23545;&#24320;&#21457;&#26032;&#30340;ULTR&#35299;&#20915;&#26041;&#26696;&#25110;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21033;&#29992;&#23427;&#20204;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#19994;&#23454;&#36341;&#32773;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception, the field of unbiased learning to rank (ULTR) has remained very active and has seen several impactful advancements in recent years. This tutorial provides both an introduction to the core concepts of the field and an overview of recent advancements in its foundations along with several applications of its methods. The tutorial is divided into four parts: Firstly, we give an overview of the different forms of bias that can be addressed with ULTR methods. Secondly, we present a comprehensive discussion of the latest estimation techniques in the ULTR field. Thirdly, we survey published results of ULTR in real-world applications. Fourthly, we discuss the connection between ULTR and fairness in ranking. We end by briefly reflecting on the future of ULTR research and its applications. This tutorial is intended to benefit both researchers and industry practitioners who are interested in developing new ULTR solutions or utilizing them in real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#19968;&#33268;&#24615;&#20248;&#21270;&#65288;CBO&#65289;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20114;&#21160;&#31890;&#23376;&#31995;&#32479;&#23454;&#29616;&#23545;&#20110;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#20013;&#21508;&#20010;&#32676;&#32452;&#30340;&#26377;&#25928;&#27169;&#22411;&#35757;&#32451;.</title><link>http://arxiv.org/abs/2305.02894</link><description>&lt;p&gt;
&#21033;&#29992;&#19968;&#33268;&#24615;&#20248;&#21270;&#23454;&#29616;&#38598;&#32676;&#32852;&#37030;&#23398;&#20064;&#30340;&#32452;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
FedCBO: Reaching Group Consensus in Clustered Federated Learning through Consensus-based Optimization. (arXiv:2305.02894v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#19968;&#33268;&#24615;&#20248;&#21270;&#65288;CBO&#65289;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20114;&#21160;&#31890;&#23376;&#31995;&#32479;&#23454;&#29616;&#23545;&#20110;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#20013;&#21508;&#20010;&#32676;&#32452;&#30340;&#26377;&#25928;&#27169;&#22411;&#35757;&#32451;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#26694;&#26550;&#65292;&#26088;&#22312;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#29992;&#25143;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#26377;&#33258;&#24049;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#25968;&#25454;&#38544;&#31169;&#21644;&#36890;&#20449;&#20002;&#22833;&#32422;&#26463;&#12290;&#22312;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#20551;&#35774;&#29992;&#25143;&#20043;&#38388;&#23384;&#22312;&#38468;&#21152;&#30340;&#26410;&#30693;&#32676;&#32452;&#32467;&#26500;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#35757;&#32451;&#23545;&#27599;&#20010;&#32676;&#32452;&#26377;&#29992;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20026;&#25152;&#26377;&#29992;&#25143;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#20248;&#21270;&#65288;CBO&#65289;&#30340;&#24605;&#24819;&#65292;&#35299;&#20915;&#20102;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;CBO&#31867;&#22411;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#20114;&#21160;&#31890;&#23376;&#31995;&#32479;&#65292;&#24573;&#30053;&#20102;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#24471;&#21040;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#25512;&#29702;&#25903;&#25345;&#65292;&#21253;&#25324;&#25551;&#36848;&#25105;&#20204;&#30340;&#31890;&#23376;&#31995;&#32479;&#22823;&#37327;&#31890;&#23376;&#26497;&#38480;&#30340;&#24179;&#22343;&#22330;&#20998;&#26512;&#65292;&#20197;&#21450;&#21516;&#26102;&#20840;&#23616;&#20248;&#21270;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an important framework in modern machine learning that seeks to integrate the training of learning models from multiple users, each user having their own local data set, in a way that is sensitive to data privacy and to communication loss constraints. In clustered federated learning, one assumes an additional unknown group structure among users, and the goal is to train models that are useful for each group, rather than simply training a single global model for all users. In this paper, we propose a novel solution to the problem of clustered federated learning that is inspired by ideas in consensus-based optimization (CBO). Our new CBO-type method is based on a system of interacting particles that is oblivious to group memberships. Our model is motivated by rigorous mathematical reasoning, including a mean field analysis describing the large number of particles limit of our particle system, as well as convergence guarantees for the simultaneous global optimization
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27604;&#29305;&#24179;&#38754;&#32534;&#30721;&#23454;&#29616;&#30340;&#36755;&#20837;&#23618;&#20108;&#20540;&#21270;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30001;&#20110;&#25968;&#25454;&#25193;&#23637;&#23548;&#33268;&#30340;&#35745;&#31639;&#37327;&#22686;&#21152;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23436;&#20840;&#20108;&#20540;&#21270;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.02885</link><description>&lt;p&gt;
&#27604;&#29305;&#24179;&#38754;&#32534;&#30721;&#23454;&#29616;&#30340;&#36755;&#20837;&#23618;&#20108;&#20540;&#21270;
&lt;/p&gt;
&lt;p&gt;
Input Layer Binarization with Bit-Plane Encoding. (arXiv:2305.02885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02885
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27604;&#29305;&#24179;&#38754;&#32534;&#30721;&#23454;&#29616;&#30340;&#36755;&#20837;&#23618;&#20108;&#20540;&#21270;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30001;&#20110;&#25968;&#25454;&#25193;&#23637;&#23548;&#33268;&#30340;&#35745;&#31639;&#37327;&#22686;&#21152;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23436;&#20840;&#20108;&#20540;&#21270;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#20351;&#29992;1&#20301;&#26435;&#37325;&#21644;&#28608;&#27963;&#26469;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#39640;&#25928;&#25191;&#34892;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#23558;&#31532;&#19968;&#23618;&#20108;&#20540;&#21270;&#30340;&#25928;&#26524;&#19981;&#20339;&#65292;&#23548;&#33268;&#31934;&#24230;&#25439;&#22833;&#24456;&#22823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#20351;&#29992;8&#20301;&#34920;&#31034;&#36755;&#20837;&#25968;&#25454;&#26469;&#36827;&#34892;&#31532;&#19968;&#23618;&#20108;&#20540;&#21270;&#65307;&#25105;&#20204;&#21033;&#29992;&#26631;&#20934;&#30340;&#27604;&#29305;&#24179;&#38754;&#32534;&#30721;&#25353;&#27604;&#29305;&#30340;&#26041;&#24335;&#25552;&#21462;&#29305;&#24449;(&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;); &#32463;&#36807;&#37325;&#26032;&#21152;&#26435;&#21644;&#29305;&#24449;&#34701;&#21512;&#65292;&#26368;&#32456;&#30340;&#27169;&#22411;&#26159;&#23436;&#20840;&#20108;&#20540;&#21270;&#30340;&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#23618;&#20108;&#20540;&#21270;&#26041;&#27861;&#26159;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#12290;&#35813;&#27010;&#24565;&#22312;&#19977;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#65288;CIFAR10&#12289;SVHN&#21644;CIFAR100&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary Neural Networks (BNNs) use 1-bit weights and activations to efficiently execute deep convolutional neural networks on edge devices. Nevertheless, the binarization of the first layer is conventionally excluded, as it leads to a large accuracy loss. The few works addressing the first layer binarization, typically increase the number of input channels to enhance data representation; such data expansion raises the amount of operations needed and it is feasible only on systems with enough computational resources. In this work, we present a new method to binarize the first layer using directly the 8-bit representation of input data; we exploit the standard bit-planes encoding to extract features bit-wise (using depth-wise convolutions); after a re-weighting stage, features are fused again. The resulting model is fully binarized and our first layer binarization approach is model independent. The concept is evaluated on three classification datasets (CIFAR10, SVHN and CIFAR100) for diff
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#23553;&#35013;&#65292;&#29992;&#20110;&#22122;&#22768;&#22686;&#24378;RL&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#26032;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#20195;&#29702;&#20154;&#25506;&#32034;&#21644;&#25913;&#21892;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#25511;&#21046;&#22122;&#22768;&#27880;&#20837;&#39057;&#29575;&#30340;&#36229;&#21442;&#25968;&#22122;&#22768;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;RL&#31639;&#27861;&#65292;Soft Actor-Critic&#65288;SAC&#65289;&#65292;Twin Delayed DDPG&#65288;TD3&#65289;&#21644;Proximal Policy&#65292;&#20197;&#30740;&#31350;&#36825;&#20123;&#23553;&#35013;&#23545;&#22238;&#25253;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02882</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#22024;&#26434;&#29615;&#22659;&#22686;&#24378;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple Noisy Environment Augmentation for Reinforcement Learning. (arXiv:2305.02882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#23553;&#35013;&#65292;&#29992;&#20110;&#22122;&#22768;&#22686;&#24378;RL&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#26032;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#20195;&#29702;&#20154;&#25506;&#32034;&#21644;&#25913;&#21892;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#25511;&#21046;&#22122;&#22768;&#27880;&#20837;&#39057;&#29575;&#30340;&#36229;&#21442;&#25968;&#22122;&#22768;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;RL&#31639;&#27861;&#65292;Soft Actor-Critic&#65288;SAC&#65289;&#65292;Twin Delayed DDPG&#65288;TD3&#65289;&#21644;Proximal Policy&#65292;&#20197;&#30740;&#31350;&#36825;&#20123;&#23553;&#35013;&#23545;&#22238;&#25253;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#24212;&#29992;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#22686;&#24378;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#19968;&#31995;&#21015;&#36890;&#29992;&#23553;&#35013;&#65292;&#35774;&#35745;&#29992;&#20110;&#22122;&#22768;&#22686;&#24378;RL&#29615;&#22659;&#65292;&#24182;&#40723;&#21169;&#20195;&#29702;&#20154;&#25506;&#32034;&#21644;&#25913;&#21892;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;RL&#31639;&#27861;&#21644;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38598;&#20013;&#20110;&#28041;&#21450;&#29366;&#24577;&#12289;&#22870;&#21169;&#21644;&#36716;&#25442;&#21160;&#24577;&#30340;&#22686;&#24378;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#22686;&#24378;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#22122;&#22768;&#29575;&#36229;&#21442;&#25968;&#65292;&#20197;&#25511;&#21046;&#22122;&#22768;&#27880;&#20837;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#27969;&#34892;&#30340;RL&#31639;&#27861;&#65292;Soft Actor-Critic&#65288;SAC&#65289;&#65292;Twin Delayed DDPG&#65288;TD3&#65289;&#21644;Proximal Policy&#36827;&#34892;&#23454;&#39564;&#65292;&#30740;&#31350;&#36825;&#20123;&#23553;&#35013;&#23545;&#22238;&#25253;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is a widely used technique for improving model performance in machine learning, particularly in computer vision and natural language processing. Recently, there has been increasing interest in applying augmentation techniques to reinforcement learning (RL) problems, with a focus on image-based augmentation. In this paper, we explore a set of generic wrappers designed to augment RL environments with noise and encourage agent exploration and improve training data diversity which are applicable to a broad spectrum of RL algorithms and environments. Specifically, we concentrate on augmentations concerning states, rewards, and transition dynamics and introduce two novel augmentation techniques. In addition, we introduce a noise rate hyperparameter for control over the frequency of noise injection. We present experimental results on the impact of these wrappers on return using three popular RL algorithms, Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), and Proximal Policy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#24615;&#38556;&#30861;&#65292;&#22914;&#33618;&#33436;&#39640;&#21407;&#21644;&#25351;&#25968;&#25439;&#22833;&#38598;&#20013;&#65292;&#20351;&#29992;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#21644;&#26126;&#30830;&#25439;&#22833;&#20250;&#20135;&#29983;&#19968;&#31181;&#26032;&#30340;&#33618;&#33436;&#39640;&#21407;&#29616;&#35937;&#12290;&#26368;&#22823;&#22343;&#20540;&#24046;&#21487;&#20197;&#26159;&#20302;&#31209;&#19988;&#21487;&#35757;&#32451;&#30340;&#25110;&#20840;&#23616;&#24615;&#19988;&#19981;&#21487;&#35757;&#32451;&#30340;&#12290;&#20294;&#26159;&#65292;&#21487;&#35757;&#32451;&#24615;&#25152;&#38656;&#30340;&#20302;&#31209;&#25439;&#22833;&#36890;&#24120;&#19981;&#33021;&#21306;&#20998;&#39640;&#39057;&#21644;&#20302;&#39057;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.02881</link><description>&lt;p&gt;
&#37327;&#23376;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#21487;&#35757;&#32451;&#24615;&#38556;&#30861;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Trainability barriers and opportunities in quantum generative modeling. (arXiv:2305.02881v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#24615;&#38556;&#30861;&#65292;&#22914;&#33618;&#33436;&#39640;&#21407;&#21644;&#25351;&#25968;&#25439;&#22833;&#38598;&#20013;&#65292;&#20351;&#29992;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#21644;&#26126;&#30830;&#25439;&#22833;&#20250;&#20135;&#29983;&#19968;&#31181;&#26032;&#30340;&#33618;&#33436;&#39640;&#21407;&#29616;&#35937;&#12290;&#26368;&#22823;&#22343;&#20540;&#24046;&#21487;&#20197;&#26159;&#20302;&#31209;&#19988;&#21487;&#35757;&#32451;&#30340;&#25110;&#20840;&#23616;&#24615;&#19988;&#19981;&#21487;&#35757;&#32451;&#30340;&#12290;&#20294;&#26159;&#65292;&#21487;&#35757;&#32451;&#24615;&#25152;&#38656;&#30340;&#20302;&#31209;&#25439;&#22833;&#36890;&#24120;&#19981;&#33021;&#21306;&#20998;&#39640;&#39057;&#21644;&#20302;&#39057;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#26412;&#36136;&#39640;&#25928;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#22240;&#27492;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#23454;&#29616;&#36817;&#26399;&#20248;&#21183;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21487;&#25193;&#23637;&#24615;&#20173;&#23384;&#22312;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#24615;&#38556;&#30861;&#65292;&#22914;&#33618;&#33436;&#39640;&#21407;&#21644;&#25351;&#25968;&#25439;&#22833;&#38598;&#20013;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26126;&#30830;&#21644;&#38544;&#21547;&#27169;&#22411;&#12289;&#25439;&#22833;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#22522;&#20110;&#37327;&#23376;&#30005;&#36335;&#30340;&#27169;&#22411;&#65289;&#21644;&#26126;&#30830;&#25439;&#22833;&#65288;&#22914;KL&#25955;&#24230;&#65289;&#20250;&#20135;&#29983;&#19968;&#31181;&#26032;&#30340;&#33618;&#33436;&#39640;&#21407;&#29616;&#35937;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26368;&#22823;&#22343;&#20540;&#24046;&#65288;MMD&#65289;&#65292;&#20316;&#20026;&#38544;&#24335;&#25439;&#22833;&#30340;&#19968;&#20010;&#27969;&#34892;&#20363;&#23376;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#35813;&#35266;&#27979;&#37327;&#21487;&#33021;&#26159;&#20302;&#31209;&#19988;&#21487;&#35757;&#32451;&#30340;&#65292;&#20063;&#21487;&#33021;&#26159;&#20840;&#23616;&#24615;&#19988;&#19981;&#21487;&#35757;&#32451;&#30340;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#26680;&#20989;&#25968;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21516;&#26102;&#24378;&#35843;&#65292;&#21487;&#35757;&#32451;&#24615;&#25152;&#38656;&#30340;&#20302;&#31209;&#25439;&#22833;&#36890;&#24120;&#19981;&#33021;&#21306;&#20998;&#39640;&#39057;&#21644;&#20302;&#39057;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum generative models, in providing inherently efficient sampling strategies, show promise for achieving a near-term advantage on quantum hardware. Nonetheless, important questions remain regarding their scalability. In this work, we investigate the barriers to the trainability of quantum generative models posed by barren plateaus and exponential loss concentration. We explore the interplay between explicit and implicit models and losses, and show that using implicit generative models (such as quantum circuit-based models) with explicit losses (such as the KL divergence) leads to a new flavour of barren plateau. In contrast, the Maximum Mean Discrepancy (MMD), which is a popular example of an implicit loss, can be viewed as the expectation value of an observable that is either low-bodied and trainable, or global and untrainable depending on the choice of kernel. However, in parallel, we highlight that the low-bodied losses required for trainability cannot in general distinguish hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#29992;&#20110;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#35268;&#27169;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#19981;&#36275;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02866</link><description>&lt;p&gt;
&#20998;&#23618;Transformer&#29992;&#20110;&#21487;&#25193;&#23637;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Transformer for Scalable Graph Learning. (arXiv:2305.02866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#29992;&#20110;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#35268;&#27169;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#19981;&#36275;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;Transformer&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#24182;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#23454;&#29616;&#30340;&#22270;Transformer&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#23567;&#35268;&#27169;&#22270;&#30340;&#34920;&#31034;&#19978;&#65292;&#20840;&#23616;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#23545;&#20110;&#24212;&#29992;&#20110;&#36739;&#22823;&#35268;&#27169;&#22270;&#30340;&#20840;&#25209;&#37327;&#35757;&#32451;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#24517;&#35201;&#30340;&#39640;&#23618;&#27425;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#20316;&#20026;&#36825;&#20123;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;HSGT&#25104;&#21151;&#22320;&#23558;Transformer&#26550;&#26500;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#31895;&#21270;&#25216;&#26415;&#26500;&#24314;&#30340;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#26377;&#25928;&#22320;&#26356;&#26032;&#21644;&#23384;&#20648;&#22810;&#23610;&#24230;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;HSGT&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Transformer is gaining increasing attention in the field of machine learning and has demonstrated state-of-the-art performance on benchmarks for graph representation learning. However, as current implementations of Graph Transformer primarily focus on learning representations of small-scale graphs, the quadratic complexity of the global self-attention mechanism presents a challenge for full-batch training when applied to larger graphs. Additionally, conventional sampling-based methods fail to capture necessary high-level contextual information, resulting in a significant loss of performance. In this paper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a solution to these challenges. HSGT successfully scales the Transformer architecture to node representation learning tasks on large-scale graphs, while maintaining high performance. By utilizing graph hierarchies constructed through coarsening techniques, HSGT efficiently updates and stores multi-scale informat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#22823;&#22240;&#26524;&#29109;&#21407;&#29702;&#26469;&#23398;&#20064;&#20195;&#29702;&#36981;&#23432;&#38480;&#21046;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#20351;&#29992;&#36981;&#23432;&#36825;&#20123;&#38480;&#21046;&#30340;&#20195;&#29702;&#30340;&#23454;&#20363;&#36827;&#34892;&#23398;&#20064;&#12290;&#27492;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02857</link><description>&lt;p&gt;
&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Causal Entropy Inverse Constrained Reinforcement Learning. (arXiv:2305.02857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#22823;&#22240;&#26524;&#29109;&#21407;&#29702;&#26469;&#23398;&#20064;&#20195;&#29702;&#36981;&#23432;&#38480;&#21046;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#20351;&#29992;&#36981;&#23432;&#36825;&#20123;&#38480;&#21046;&#30340;&#20195;&#29702;&#30340;&#23454;&#20363;&#36827;&#34892;&#23398;&#20064;&#12290;&#27492;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#24403;&#20154;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36827;&#34892;&#20132;&#20114;&#26102;&#65292;&#20195;&#29702;&#30340;&#34892;&#20026;&#19982;&#35813;&#29615;&#22659;&#30340;&#20215;&#20540;&#35266;&#12289;&#31038;&#20250;&#35268;&#33539;&#25110;&#20854;&#20182;&#35201;&#27714;&#30456;&#19968;&#33268;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29615;&#22659;&#37117;&#26377;&#38590;&#20197;&#35268;&#23450;&#21644;&#36716;&#31227;&#32473;&#23398;&#20064;&#20195;&#29702;&#30340;&#38544;&#21547;&#38480;&#21046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#22823;&#22240;&#26524;&#29109;&#21407;&#29702;&#26469;&#23398;&#20064;&#20195;&#29702;&#36981;&#23432;&#38480;&#21046;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#36981;&#23432;&#36825;&#20123;&#38480;&#21046;&#30340;&#20195;&#29702;&#30340;&#23454;&#20363;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#25152;&#33719;&#24471;&#30340;&#22870;&#21169;&#21644;&#36829;&#21453;&#32422;&#26463;&#30340;&#27425;&#25968;&#26469;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26681;&#25454;&#20854;&#23545;&#20854;&#20182;&#20195;&#29702;&#30340;&#21487;&#36716;&#31227;&#24615;&#26469;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying artificial agents in real-world environments where they interact with humans, it is crucial that their behavior is aligned with the values, social norms or other requirements of that environment. However, many environments have implicit constraints that are difficult to specify and transfer to a learning agent. To address this challenge, we propose a novel method that utilizes the principle of maximum causal entropy to learn constraints and an optimal policy that adheres to these constraints, using demonstrations of agents that abide by the constraints. We prove convergence in a tabular setting and provide an approximation which scales to complex environments. We evaluate the effectiveness of the learned policy by assessing the reward received and the number of constraint violations, and we evaluate the learned cost function based on its transferability to other agents. Our method has been shown to outperform state-of-the-art approaches across a variety of tasks and envi
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#32858;&#31867;&#20013;&#65292;&#20915;&#31574;&#26641;&#28145;&#24230;&#26159;&#26080;&#27861;&#20943;&#23569;&#30340;&#22266;&#26377;&#22797;&#26434;&#24230;&#24230;&#37327;&#20043;&#19968;&#65292;&#20943;&#23569;&#28145;&#24230;&#20250;&#26174;&#33879;&#38477;&#20302;&#32858;&#31867;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02850</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#32858;&#31867;&#20013;&#28145;&#24230;&#20943;&#23569;&#30340;&#19981;&#21487;&#33021;&#24615;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Impossibility of Depth Reduction in Explainable Clustering. (arXiv:2305.02850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02850
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#32858;&#31867;&#20013;&#65292;&#20915;&#31574;&#26641;&#28145;&#24230;&#26159;&#26080;&#27861;&#20943;&#23569;&#30340;&#22266;&#26377;&#22797;&#26434;&#24230;&#24230;&#37327;&#20043;&#19968;&#65292;&#20943;&#23569;&#28145;&#24230;&#20250;&#26174;&#33879;&#38477;&#20302;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21487;&#35299;&#37322;&#32858;&#31867;&#24341;&#36215;&#20102;&#35768;&#22810;&#20851;&#27880;&#12290;&#26412;&#35770;&#25991;&#22312;Euclidean&#24179;&#38754;&#20013;&#35777;&#26126;&#65292;&#23545;&#20110;&#21487;&#35299;&#37322;&#30340;k-means&#21644;k-median&#32858;&#31867;&#38382;&#39064;&#65292;&#20915;&#31574;&#26641;&#30340;&#28145;&#24230;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#20043;&#19968;&#65292;&#26080;&#27861;&#20943;&#23569;&#32780;&#19981;&#26174;&#33879;&#38477;&#20302;&#32858;&#31867;&#36136;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#22312;Euclidean&#24179;&#38754;&#19978;&#30340;&#25968;&#25454;X&#65292;&#28145;&#24230;&#20026;k-1&#30340;&#20915;&#31574;&#26641;&#30340;k-means/k-median&#32858;&#31867;&#20195;&#20215;&#19982;X&#30340;&#20248;&#21270;&#32858;&#31867;&#20195;&#20215;&#30456;&#21516;&#65292;&#20294;&#26159;&#23545;&#20110;&#28145;&#24230;&#23567;&#20110;k-1&#30340;&#20915;&#31574;&#26641;&#65292;&#20854;&#32858;&#31867;&#20195;&#20215;&#30456;&#23545;&#20110;&#26368;&#20248;&#32858;&#31867;&#20195;&#20215;&#32780;&#35328;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#25105;&#20204;&#36824;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;k-center&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years Explainable Clustering has gathered a lot of attention. Dasgupta et al. [ICML'20] initiated the study of explainable k-means and k-median clustering problems where the explanation is captured by a threshold decision tree which partitions the space at each node using axis parallel hyperplanes. Recently, Laber et al. [Pattern Recognition'23] made a case to consider the depth of the decision tree as an additional complexity measure of interest.  In this work, we prove that even when the input points are in the Euclidean plane, then any depth reduction in the explanation incurs unbounded loss in the k-means and k-median cost. Formally, we show that there exists a data set X in the Euclidean plane, for which there is a decision tree of depth k-1 whose k-means/k-median cost matches the optimal clustering cost of X, but every decision tree of depth less than k-1 has unbounded cost w.r.t. the optimal cost of clustering. We extend our results to the k-center objective as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;VAEs&#21644;Transformers&#26500;&#24314;&#20004;&#31181;&#20855;&#26377;&#24402;&#32435;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#33021;&#22815;&#23558;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20998;&#31163;&#20026;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#20102;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02810</link><description>&lt;p&gt;
&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpretable Sentence Representation with Variational Autoencoders and Attention. (arXiv:2305.02810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;VAEs&#21644;Transformers&#26500;&#24314;&#20004;&#31181;&#20855;&#26377;&#24402;&#32435;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#33021;&#22815;&#23558;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20998;&#31163;&#20026;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#20102;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#36817;&#19968;&#20123;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#23558;&#35266;&#23519;&#32467;&#26524;&#19982;&#38544;&#34255;&#30340;&#29983;&#25104;&#22240;&#32032;&#32852;&#31995;&#36215;&#26469;&#26041;&#38754;&#24456;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#25928;&#29575;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#20063;&#24456;&#26377;&#25928;&#12290;&#25105;&#20204;&#39318;&#20808;&#21024;&#38500;&#21322;&#30417;&#30563;VAEs&#36816;&#34892;&#26041;&#26696;&#20013;&#30340;&#19981;&#24517;&#35201;&#32452;&#20214;&#65292;&#20351;&#24471;&#23427;&#20204;&#26356;&#24555;&#36895;&#12289;&#26356;&#23567;&#12289;&#26356;&#26131;&#20110;&#35774;&#35745;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;VAEs&#21644;Transformer&#26500;&#24314;&#20102;&#20004;&#20010;&#20855;&#26377;&#24402;&#32435;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#23558;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20998;&#31163;&#25104;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#20102;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this thesis, we develop methods to enhance the interpretability of recent representation learning techniques in natural language processing (NLP) while accounting for the unavailability of annotated data. We choose to leverage Variational Autoencoders (VAEs) due to their efficiency in relating observations to latent generative factors and their effectiveness in data-efficient learning and interpretable representation learning. As a first contribution, we identify and remove unnecessary components in the functioning scheme of semi-supervised VAEs making them faster, smaller and easier to design. Our second and main contribution is to use VAEs and Transformers to build two models with inductive bias to separate information in latent representations into understandable concepts without annotated data. The first model, Attention-Driven VAE (ADVAE), is able to separately represent and control information about syntactic roles in sentences. The second model, QKVAE, uses separate latent va
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#23384;&#22312;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#26469;&#20248;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#20808;&#21069;&#30740;&#31350;&#25351;&#20986;&#65292;&#22522;&#20110;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24178;&#39044;&#21487;&#20197;&#30830;&#20445;&#27604;&#20363;&#20195;&#34920;&#24615;&#65292;&#24182;&#22312;&#23384;&#22312;&#20559;&#35265;&#26102;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#25928;&#29992;&#12290;&#32780;&#26412;&#25991;&#21017;&#25506;&#35752;&#20102;&#19968;&#32452;&#33021;&#22815;&#25429;&#25417;&#36825;&#31181;&#30446;&#30340;&#30340;&#23376;&#27169;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.02806</link><description>&lt;p&gt;
&#22312;&#23384;&#22312;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Maximizing Submodular Functions for Recommendation in the Presence of Biases. (arXiv:2305.02806v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02806
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#23384;&#22312;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#26469;&#20248;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#20808;&#21069;&#30740;&#31350;&#25351;&#20986;&#65292;&#22522;&#20110;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24178;&#39044;&#21487;&#20197;&#30830;&#20445;&#27604;&#20363;&#20195;&#34920;&#24615;&#65292;&#24182;&#22312;&#23384;&#22312;&#20559;&#35265;&#26102;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#25928;&#29992;&#12290;&#32780;&#26412;&#25991;&#21017;&#25506;&#35752;&#20102;&#19968;&#32452;&#33021;&#22815;&#25429;&#25417;&#36825;&#31181;&#30446;&#30340;&#30340;&#23376;&#27169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#38598;&#36873;&#25321;&#20219;&#21153;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#25628;&#32034;&#24341;&#25806;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#35201;&#27714;&#36873;&#25321;&#19968;&#20123;&#26368;&#22823;&#21270;&#29992;&#25143;&#20215;&#20540;&#30340;&#29289;&#21697;&#23376;&#38598;&#12290;&#23376;&#38598;&#30340;&#20215;&#20540;&#24448;&#24448;&#21576;&#29616;&#20986;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#22240;&#27492;&#65292;&#20351;&#29992;&#23376;&#27169;&#20989;&#25968;&#26469;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21457;&#29616;&#36755;&#20837;&#20855;&#26377;&#31038;&#20250;&#20559;&#35265;&#65292;&#20250;&#38477;&#20302;&#36755;&#20986;&#23376;&#38598;&#30340;&#25928;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#24178;&#39044;&#20197;&#25552;&#39640;&#20854;&#25928;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#32452;&#23376;&#27169;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#65292;&#36825;&#20123;&#20989;&#25968;&#28085;&#30422;&#20102;&#19978;&#36848;&#24212;&#29992;&#20013;&#20986;&#29616;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subset selection tasks, arise in recommendation systems and search engines and ask to select a subset of items that maximize the value for the user. The values of subsets often display diminishing returns, and hence, submodular functions have been used to model them. If the inputs defining the submodular function are known, then existing algorithms can be used. In many applications, however, inputs have been observed to have social biases that reduce the utility of the output subset. Hence, interventions to improve the utility are desired. Prior works focus on maximizing linear functions -- a special case of submodular functions -- and show that fairness constraint-based interventions can not only ensure proportional representation but also achieve near-optimal utility in the presence of biases. We study the maximization of a family of submodular functions that capture functions arising in the aforementioned applications. Our first result is that, unlike linear functions, constraint-ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#20197;&#35299;&#20915;&#20197;&#24448;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02803</link><description>&lt;p&gt;
&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#24352;&#37327;PCA
&lt;/p&gt;
&lt;p&gt;
Tensor PCA from basis in tensor space. (arXiv:2305.02803v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#20197;&#35299;&#20915;&#20197;&#24448;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#20197;&#21069;&#36890;&#36807;&#36845;&#20195;&#27714;&#35299;&#20248;&#21270;&#38382;&#39064;&#26469;&#25552;&#21462;&#20302;&#32500;&#23376;&#31354;&#38388;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20174;&#23454;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#20013;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#65292;&#20174;&#32780;&#23558;&#22522;&#30784;&#30340;&#23548;&#20986;&#38382;&#39064;&#36716;&#21270;&#20026;&#29305;&#24449;&#20540;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#24773;&#20917;&#30340;&#23548;&#20986;&#65306;i&#65289;&#20174;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#20013;&#23548;&#20986;&#22522;&#30784;&#65307;ii&#65289;&#23548;&#20986;&#31209;&#20026;1&#30340;&#22522;&#30784;&#65307;iii&#65289;&#20174;&#23376;&#31354;&#38388;&#20013;&#23548;&#20986;&#22522;&#30784;&#12290;&#29305;&#21035;&#26159;&#65292;&#35777;&#26126;&#20102;&#23454;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#26041;&#31243;&#19982;&#26631;&#20934;&#30697;&#38453;&#29305;&#24449;&#20540;&#26041;&#31243;&#30340;&#31561;&#20215;&#24615;&#12290;&#38024;&#23545;&#25152;&#32771;&#34385;&#30340;&#19977;&#31181;&#24773;&#20917;&#65292;&#37319;&#29992;&#20102;&#23376;&#31354;&#38388;&#26041;&#27861;&#26469;&#23548;&#20986;&#24352;&#37327;PCA&#12290;&#22522;&#20110;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to present a mathematical framework for tensor PCA. The proposed approach is able to overcome the limitations of previous methods that extract a low dimensional subspace by iteratively solving an optimization problem. The core of the proposed approach is the derivation of a basis in tensor space from a real self-adjoint tensor operator, thus reducing the problem of deriving a basis to an eigenvalue problem. Three different cases have been studied to derive: i) a basis from a self-adjoint tensor operator; ii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence between eigenvalue equation for a real self-adjoint tensor operator and standard matrix eigenvalue equation has been proven. For all the three cases considered, a subspace approach has been adopted to derive a tensor PCA. Experiments on image datasets validate the proposed mathematical framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21322;&#30417;&#30563;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#31867;&#21035;&#20998;&#24067;&#24863;&#30693;&#20266;&#26631;&#35760;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#25511;&#21046;&#20266;&#26631;&#31614;&#25968;&#30446;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#20934;&#30830;&#22320;&#36924;&#36817;&#30495;&#23454;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02795</link><description>&lt;p&gt;
&#38754;&#21521;&#21322;&#30417;&#30563;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#31867;&#21035;&#20998;&#24067;&#24863;&#30693;&#20266;&#26631;&#35760;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Class-Distribution-Aware Pseudo Labeling for Semi-Supervised Multi-Label Learning. (arXiv:2305.02795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21322;&#30417;&#30563;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#31867;&#21035;&#20998;&#24067;&#24863;&#30693;&#20266;&#26631;&#35760;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#25511;&#21046;&#20266;&#26631;&#31614;&#25968;&#30446;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#20934;&#30830;&#22320;&#36924;&#36817;&#30495;&#23454;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#26631;&#35760;&#26159;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20449;&#24687;&#30340;&#27969;&#34892;&#19988;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#23454;&#20363;&#30340;&#20266;&#26631;&#35760;&#26041;&#27861;&#36890;&#24120;&#26681;&#25454;&#20854;&#39044;&#27979;&#27010;&#29575;&#20026;&#27599;&#20010;&#26410;&#26631;&#35760;&#23454;&#20363;&#20998;&#37197;&#19968;&#20010;&#20266;&#26631;&#31614;&#12290;&#30001;&#20110;&#30495;&#23454;&#26631;&#31614;&#25968;&#30446;&#26410;&#30693;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#22810;&#26631;&#31614;&#23398;&#20064;&#65288;SSMLL&#65289;&#22330;&#26223;&#19979;&#38590;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#38754;&#20020;&#24341;&#20837;&#20551;&#27491;&#26631;&#31614;&#25110;&#24573;&#30053;&#30495;&#27491;&#26631;&#31614;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;SSMLL&#38382;&#39064;&#30340;&#31867;&#21035;&#20998;&#24067;&#24863;&#30693;&#20266;&#26631;&#35760;&#65288;CAP&#65289;&#26041;&#27861;&#65292;&#40723;&#21169;&#20266;&#26631;&#31614;&#30340;&#31867;&#21035;&#20998;&#24067;&#25509;&#36817;&#30495;&#23454;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21253;&#25324;&#31867;&#21035;&#24863;&#30693;&#38408;&#20540;&#30340;&#27491;&#21017;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#25511;&#21046;&#27599;&#20010;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#25968;&#30446;&#12290;&#37492;&#20110;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#31034;&#20363;&#26159;&#26681;&#25454;&#21516;&#19968;&#20998;&#24067;&#37319;&#26679;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#26631;&#35760;&#31034;&#20363;&#30340;&#31867;&#21035;&#20998;&#24067;&#30830;&#23450;&#38408;&#20540;&#24182;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#19968;&#36215;&#26356;&#26032;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo labeling is a popular and effective method to leverage the information of unlabeled data. Conventional instance-aware pseudo labeling methods often assign each unlabeled instance with a pseudo label based on its predicted probabilities. However, due to the unknown number of true labels, these methods cannot generalize well to semi-supervised multi-label learning (SSMLL) scenarios, since they would suffer from the risk of either introducing false positive labels or neglecting true positive ones. In this paper, we propose to solve the SSMLL problems by performing Class-distribution-Aware Pseudo labeling (CAP), which encourages the class distribution of pseudo labels to approximate the true one. Specifically, we design a regularized learning framework consisting of the class-aware thresholds to control the number of pseudo labels for each class. Given that the labeled and unlabeled examples are sampled according to the same distribution, we determine the thresholds by exploiting th
&lt;/p&gt;</description></item><item><title>BranchNorm&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;Transformer&#30340;&#38750;&#27531;&#24046;&#20998;&#25903;&#65292;&#29702;&#35770;&#19978;&#31283;&#23450;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BranchNorm&#22312;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.02790</link><description>&lt;p&gt;
BranchNorm: &#40065;&#26834;&#22320;&#25193;&#23637;&#26497;&#28145;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
BranchNorm: Robustly Scaling Extremely Deep Transformers. (arXiv:2305.02790v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02790
&lt;/p&gt;
&lt;p&gt;
BranchNorm&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;Transformer&#30340;&#38750;&#27531;&#24046;&#20998;&#25903;&#65292;&#29702;&#35770;&#19978;&#31283;&#23450;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BranchNorm&#22312;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;DeepNorm&#23558;Transformer&#25193;&#23637;&#21040;&#26497;&#28145;&#65288;&#21363;1000&#23618;&#65289;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#25193;&#23637;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#31283;&#23450;&#28145;&#24230;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;DeepNorm&#35797;&#22270;&#23558;&#27169;&#22411;&#26356;&#26032;&#32422;&#26463;&#20026;&#19968;&#20010;&#24658;&#23450;&#20540;&#12290;&#23613;&#31649;&#24212;&#29992;&#36825;&#31181;&#32422;&#26463;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#21463;&#30410;&#65292;&#20294;&#21487;&#33021;&#23548;&#33268;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#27169;&#22411;&#35757;&#32451;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BranchNorm&#65292;&#23427;&#26681;&#25454;&#35757;&#32451;&#26399;&#38388;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;Transformer&#30340;&#38750;&#27531;&#24046;&#20998;&#25903;&#12290;BranchNorm&#19981;&#20165;&#22312;&#26089;&#26399;&#38454;&#27573;&#29702;&#35770;&#19978;&#31283;&#23450;&#20102;&#35757;&#32451;&#65292;&#32780;&#19988;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#22810;&#20010;&#32763;&#35793;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BranchNorm&#22312;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, DeepNorm scales Transformers into extremely deep (i.e., 1000 layers) and reveals the promising potential of deep scaling. To stabilize the training of deep models, DeepNorm (Wang et al., 2022) attempts to constrain the model update to a constant value. Although applying such a constraint can benefit the early stage of model training, it may lead to undertrained models during the whole training procedure. In this paper, we propose BranchNorm, which dynamically rescales the non-residual branch of Transformer in accordance with the training period. BranchNorm not only theoretically stabilizes the training with smooth gradient norms at the early stage, but also encourages better convergence in the subsequent training stage. Experiment results on multiple translation tasks demonstrate that BranchNorm achieves a better trade-off between training stability and converge performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;SGD&#30340;&#38750;&#32447;&#24615;LFT&#27169;&#22411;&#65288;MNNL&#65289;&#29992;&#20110;&#20174;&#39640;&#32500;&#19981;&#23436;&#25972;&#24352;&#37327;&#20013;&#25552;&#21462;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#65292;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#21160;&#24577;&#32593;&#32476;&#30340;&#34920;&#24449;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.02782</link><description>&lt;p&gt;
&#21160;&#24577;&#32593;&#32476;&#34920;&#24449;&#30340;&#21160;&#37327;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Momentum-Incorporated Non-Negative Latent Factorization of Tensors Model for Dynamic Network Representation. (arXiv:2305.02782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;SGD&#30340;&#38750;&#32447;&#24615;LFT&#27169;&#22411;&#65288;MNNL&#65289;&#29992;&#20110;&#20174;&#39640;&#32500;&#19981;&#23436;&#25972;&#24352;&#37327;&#20013;&#25552;&#21462;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#65292;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#21160;&#24577;&#32593;&#32476;&#30340;&#34920;&#24449;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#21160;&#24577;&#32593;&#32476;&#65288;LDN&#65289;&#30001;&#20110;&#20854;&#23454;&#20307;&#25968;&#37327;&#21644;&#22823;&#35268;&#27169;&#21160;&#24577;&#20132;&#20114;&#32780;&#25104;&#20026;&#35768;&#22810;&#22823;&#25968;&#25454;&#30456;&#20851;&#24212;&#29992;&#31243;&#24207;&#30340;&#25968;&#25454;&#28304;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#21253;&#21547;&#26377;&#26102;&#38388;&#27169;&#24335;&#30693;&#35782;&#30340;&#39640;&#32500;&#19981;&#23436;&#25972;&#65288;HDI&#65289;&#24352;&#37327;&#12290;&#24352;&#37327;&#30340;&#28508;&#22312;&#22240;&#23376;&#20998;&#35299;&#65288;LFT&#65289;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#36825;&#20123;&#26102;&#38388;&#27169;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#27714;&#35299;&#22120;&#26469;&#24314;&#31435;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;SGD&#30340;LFT&#27169;&#22411;&#36890;&#24120;&#21463;&#21040;&#35757;&#32451;&#26041;&#26696;&#30340;&#38480;&#21046;&#65292;&#19988;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;SGD&#30340;&#38750;&#32447;&#24615;LFT&#27169;&#22411;&#65288;MNNL&#65289;&#65292;&#35813;&#27169;&#22411;&#20174;HDI&#24352;&#37327;&#20013;&#25552;&#21462;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#65292;&#20351;&#24471;&#35757;&#32451;&#26080;&#32422;&#26463;&#24182;&#19982;&#19968;&#33324;&#30340;&#35757;&#32451;&#26041;&#26696;&#20860;&#23481;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25910;&#25947;&#31934;&#24230;&#21644;&#36895;&#24230;&#12290;&#20004;&#20010;LDN&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;MNNL&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large-scale dynamic network (LDN) is a source of data in many big data-related applications due to their large number of entities and large-scale dynamic interactions. They can be modeled as a high-dimensional incomplete (HDI) tensor that contains a wealth of knowledge about time patterns. A Latent factorization of tensors (LFT) model efficiently extracts this time pattern, which can be established using stochastic gradient descent (SGD) solvers. However, LFT models based on SGD are often limited by training schemes and have poor tail convergence. To solve this problem, this paper proposes a novel nonlinear LFT model (MNNL) based on momentum-incorporated SGD, which extracts non-negative latent factors from HDI tensors to make training unconstrained and compatible with general training schemes, while improving convergence accuracy and speed. Empirical studies on two LDN datasets show that compared to existing models, the MNNL model has higher prediction accuracy and convergence speed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#21306;&#22495;&#25551;&#36848;&#31526;&#65292;&#23427;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#25551;&#36848;&#36229;&#31435;&#26041;&#20307;&#26469;&#39044;&#27979;&#29305;&#24449;&#20540;&#21487;&#26356;&#25913;&#20294;&#19981;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;"&#21363;&#20351;&#26159;"&#21442;&#25968;&#65292;&#25581;&#31034;&#20915;&#31574;&#30340;&#29305;&#24449;&#21644;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.02780</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#21306;&#22495;&#25551;&#36848;&#31526;&#65306;&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#30340;&#23616;&#37096;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interpretable Regional Descriptors: Hyperbox-Based Local Explanations. (arXiv:2305.02780v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#21306;&#22495;&#25551;&#36848;&#31526;&#65292;&#23427;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#25551;&#36848;&#36229;&#31435;&#26041;&#20307;&#26469;&#39044;&#27979;&#29305;&#24449;&#20540;&#21487;&#26356;&#25913;&#20294;&#19981;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;"&#21363;&#20351;&#26159;"&#21442;&#25968;&#65292;&#25581;&#31034;&#20915;&#31574;&#30340;&#29305;&#24449;&#21644;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#23616;&#37096;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#30340;&#21306;&#22495;&#25551;&#36848;&#31526;&#65288;IRDs&#65289;&#65292;&#23427;&#20204;&#26159;&#25551;&#36848;&#35266;&#27979;&#20540;&#29305;&#24449;&#20540;&#21487;&#26356;&#25913;&#32780;&#19981;&#24433;&#21709;&#20854;&#39044;&#27979;&#30340;&#36229;&#31435;&#26041;&#20307;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#32452;&#8220;&#21363;&#20351;&#26159;&#8221;&#21442;&#25968;&#65288;&#21322;&#20107;&#23454;&#30340;&#35299;&#37322;&#65289;&#65292;&#23427;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#39044;&#27979;&#65292;&#24182;&#25351;&#20986;&#21738;&#20123;&#29305;&#24449;&#24433;&#21709;&#20102;&#39044;&#27979;&#20197;&#21450;&#26159;&#21542;&#23384;&#22312;&#28857;&#20559;&#24046;&#25110;&#19981;&#21487;&#20449;&#12290;&#19968;&#20010;&#20855;&#20307;&#30340;&#29992;&#20363;&#23637;&#31034;&#20102;&#23427;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26500;&#24314;&#32773;&#21644;&#20915;&#31574;&#21463;&#24433;&#21709;&#20154;&#21592;&#37117;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#25105;&#20204;&#23558;IRDs&#30340;&#25628;&#32034;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;IRDs&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#21253;&#25324;&#26399;&#26395;&#12289;&#21021;&#22987;&#21270;&#25216;&#26415;&#21644;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29616;&#26377;&#30340;&#36229;&#31435;&#26041;&#20307;&#26041;&#27861;&#36866;&#24212;&#21040;&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#12290;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;&#22810;&#20010;&#36136;&#37327;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#31181;&#25913;&#36827;IRDs&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces interpretable regional descriptors, or IRDs, for local, model-agnostic interpretations. IRDs are hyperboxes that describe how an observation's feature values can be changed without affecting its prediction. They justify a prediction by providing a set of "even if" arguments (semi-factual explanations), and they indicate which features affect a prediction and whether pointwise biases or implausibilities exist. A concrete use case shows that this is valuable for both machine learning modelers and persons subject to a decision. We formalize the search for IRDs as an optimization problem and introduce a unifying framework for computing IRDs that covers desiderata, initialization techniques, and a post-processing method. We show how existing hyperbox methods can be adapted to fit into this unified framework. A benchmark study compares the methods based on several quality measures and identifies two strategies to improve IRDs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;pFedGate&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#31232;&#30095;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#20351;&#24471;&#23458;&#25143;&#31471;&#21487;&#20197;&#21457;&#25381;&#20854;&#27169;&#22411;&#23481;&#37327;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.02776</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#27169;&#22411;&#33258;&#36866;&#24212;&#23454;&#29616;&#39640;&#25928;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Personalized Federated Learning via Sparse Model-Adaptation. (arXiv:2305.02776v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02776
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;pFedGate&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#31232;&#30095;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#20351;&#24471;&#23458;&#25143;&#31471;&#21487;&#20197;&#21457;&#25381;&#20854;&#27169;&#22411;&#23481;&#37327;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26088;&#22312;&#20026;&#22810;&#20010;&#23458;&#25143;&#31471;&#22521;&#35757;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20849;&#20139;&#20854;&#31169;&#26377;&#25968;&#25454;&#12290;&#30001;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#26500;&#24615;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20010;&#24615;&#21270;FL&#65292;&#36890;&#36807;&#36741;&#21161;&#20840;&#23616;&#27169;&#22411;&#23398;&#20064;&#24182;&#37096;&#32626;&#19981;&#21516;&#30340;&#23616;&#37096;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#22312;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#26041;&#38754;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#23481;&#37327;&#21644;&#25928;&#29575;&#21463;&#21040;&#26368;&#20302;&#36164;&#28304;&#23458;&#25143;&#31471;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#20010;&#24615;&#21270;FL&#30340;&#24615;&#33021;&#19981;&#20339;&#21644;&#23454;&#29992;&#24615;&#26377;&#38480;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;pFedGate&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#22320;&#23398;&#20064;&#31232;&#30095;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#20010;&#24615;&#21270;FL&#12290;&#36890;&#36807;&#36731;&#37327;&#32423;&#21487;&#35757;&#32451;&#30340;&#38376;&#25511;&#23618;&#65292;pFedGate&#33021;&#22815;&#20135;&#29983;&#19981;&#21516;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#20174;&#32780;&#21457;&#25381;&#23458;&#25143;&#31471;&#22312;&#27169;&#22411;&#23481;&#37327;&#26041;&#38754;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#32771;&#34385;&#21040;&#24322;&#26500;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aims to train machine learning models for multiple clients without sharing their own private data. Due to the heterogeneity of clients' local data distribution, recent studies explore the personalized FL that learns and deploys distinct local models with the help of auxiliary global models. However, the clients can be heterogeneous in terms of not only local data distribution, but also their computation and communication resources. The capacity and efficiency of personalized models are restricted by the lowest-resource clients, leading to sub-optimal performance and limited practicality of personalized FL. To overcome these challenges, we propose a novel approach named pFedGate for efficient personalized FL by adaptively and efficiently learning sparse local models. With a lightweight trainable gating layer, pFedGate enables clients to reach their full potential in model capacity by generating different sparse models accounting for both the heterogeneous data di
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VendorLink&#30340;NLP&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#38142;&#25509;&#26263;&#32593;&#24066;&#22330;&#19978;&#30340;&#20379;&#24212;&#21830;&#34987;&#36801;&#31227;&#21644;&#28508;&#22312;&#21035;&#21517;&#65292;&#20943;&#23569;&#38750;&#27861;&#24066;&#22330;&#30340;&#21311;&#21517;&#24615;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.02763</link><description>&lt;p&gt;
VendorLink&#65306;&#19968;&#31181;NLP&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#21644;&#38142;&#25509;&#26263;&#32593;&#24066;&#22330;&#19978;&#30340;&#20379;&#24212;&#21830;&#34987;&#36801;&#31227;&#21644;&#28508;&#22312;&#21035;&#21517;
&lt;/p&gt;
&lt;p&gt;
VendorLink: An NLP approach for Identifying &amp; Linking Vendor Migrants &amp; Potential Aliases on Darknet Markets. (arXiv:2305.02763v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VendorLink&#30340;NLP&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#38142;&#25509;&#26263;&#32593;&#24066;&#22330;&#19978;&#30340;&#20379;&#24212;&#21830;&#34987;&#36801;&#31227;&#21644;&#28508;&#22312;&#21035;&#21517;&#65292;&#20943;&#23569;&#38750;&#27861;&#24066;&#22330;&#30340;&#21311;&#21517;&#24615;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26263;&#32593;&#19978;&#30340;&#21311;&#21517;&#24615;&#20351;&#24471;&#20379;&#24212;&#21830;&#21487;&#20197;&#20351;&#29992;&#22810;&#20010;&#20379;&#24212;&#21830;&#21035;&#21517;&#25110;&#39057;&#32321;&#36801;&#31227;&#24066;&#22330;&#32780;&#19981;&#34987;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#22312;&#26263;&#32593;&#19978;&#21457;&#29616;&#38750;&#27861;&#24066;&#22330;&#21450;&#20854;&#32852;&#31995;&#20154;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35782;&#21035;&#38750;&#27861;&#24066;&#22330;&#21644;&#20379;&#24212;&#21830;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VendorLink&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;NLP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#26597;&#20889;&#20316;&#27169;&#24335;&#26469;&#39564;&#35777;&#12289;&#35782;&#21035;&#21644;&#38142;&#25509;&#19971;&#20010;&#20844;&#20849;&#26263;&#32593;&#24066;&#22330;&#19978;&#30340;&#21807;&#19968;&#20379;&#24212;&#21830;&#24080;&#25143;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#19981;&#21516;&#65292;VendorLink&#21033;&#29992;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#30340;&#20248;&#21183;&#26469;&#25191;&#34892;&#23553;&#38381;&#38598;&#20379;&#24212;&#21830;&#39564;&#35777;&#12289;&#24320;&#25918;&#38598;&#20379;&#24212;&#21830;&#35782;&#21035;&#21644;&#20302;&#36164;&#28304;&#24066;&#22330;&#36866;&#24212;&#20219;&#21153;&#12290;&#36890;&#36807;VendorLink&#65292;&#25105;&#20204;&#22312;Alphabay-Dreams-Silk&#25968;&#25454;&#38598;&#20013;&#25581;&#31034;&#20102;15&#20010;&#31227;&#27665;&#21644;71&#20010;&#28508;&#22312;&#21035;&#21517;&#65292;&#22312;Valhalla-Berlusconi&#25968;&#25454;&#38598;&#20013;&#25581;&#31034;&#20102;17&#20010;&#31227;&#27665;&#21644;3&#20010;&#28508;&#22312;&#21035;&#21517;&#65292;&#22312;Traderoute-Agora&#25968;&#25454;&#38598;&#20013;&#25581;&#31034;&#20102;75&#20010;&#31227;&#27665;&#21644;10&#20010;&#28508;&#22312;&#21035;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The anonymity on the Darknet allows vendors to stay undetected by using multiple vendor aliases or frequently migrating between markets. Consequently, illegal markets and their connections are challenging to uncover on the Darknet. To identify relationships between illegal markets and their vendors, we propose VendorLink, an NLP-based approach that examines writing patterns to verify, identify, and link unique vendor accounts across text advertisements (ads) on seven public Darknet markets. In contrast to existing literature, VendorLink utilizes the strength of supervised pre-training to perform closed-set vendor verification, open-set vendor identification, and low-resource market adaption tasks. Through VendorLink, we uncover (i) 15 migrants and 71 potential aliases in the Alphabay-Dreams-Silk dataset, (ii) 17 migrants and 3 potential aliases in the Valhalla-Berlusconi dataset, and (iii) 75 migrants and 10 potential aliases in the Traderoute-Agora dataset. Altogether, our approach ca
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;&#65288;MDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21407;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#26469;&#33258;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#19981;&#20805;&#20998;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02757</link><description>&lt;p&gt;
&#19981;&#20805;&#20998;&#26631;&#27880;&#19979;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Domain Learning From Insufficient Annotations. (arXiv:2305.02757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02757
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;&#65288;MDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21407;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#26469;&#33258;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#19981;&#20805;&#20998;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#23398;&#20064;(MDL)&#25351;&#21516;&#26102;&#22312;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#25110;&#19968;&#32452;&#27169;&#22411;&#12290;&#20256;&#32479;&#26041;&#27861;&#24378;&#35843;&#22495;&#20849;&#20139;&#20449;&#24687;&#30340;&#25552;&#21462;&#21644;&#22495;&#31169;&#26377;&#20449;&#24687;&#30340;&#20445;&#30041;&#65292;&#36981;&#24490;&#20849;&#20139;-&#31169;&#26377;&#26550;&#26500;(SP&#27169;&#22411;)&#65292;&#36825;&#27604;&#21333;&#39046;&#22495;&#23398;&#20064;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#20010;&#39046;&#22495;&#20013;&#26377;&#38480;&#30340;&#24050;&#27880;&#37322;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#20256;&#32479;&#30417;&#30563;MDL&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;(MDCL)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#26469;&#33258;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#19981;&#20805;&#20998;&#27880;&#37322;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MDCL&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#22495;&#38388;&#35821;&#20041;&#23545;&#40784;&#21644;&#22495;&#20869;&#23545;&#27604;&#12290;&#21069;&#32773;&#26088;&#22312;&#23558;&#19981;&#21516;&#39046;&#22495;&#20013;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#30340;&#24050;&#26631;&#27880;&#23454;&#20363;&#22312;&#20849;&#20139;&#30340;&#38544;&#31354;&#38388;&#20013;&#23545;&#40784;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#22312;&#27599;&#20010;&#39046;&#22495;&#20869;&#26368;&#22823;&#21270;&#20998;&#31163;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22810;&#39046;&#22495;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;MDCL&#26041;&#27861;&#22312;&#21508;&#31181;&#27880;&#37322;&#26041;&#26696;&#19979;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;MDL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-domain learning (MDL) refers to simultaneously constructing a model or a set of models on datasets collected from different domains. Conventional approaches emphasize domain-shared information extraction and domain-private information preservation, following the shared-private framework (SP models), which offers significant advantages over single-domain learning. However, the limited availability of annotated data in each domain considerably hinders the effectiveness of conventional supervised MDL approaches in real-world applications. In this paper, we introduce a novel method called multi-domain contrastive learning (MDCL) to alleviate the impact of insufficient annotations by capturing both semantic and structural information from both labeled and unlabeled data.Specifically, MDCL comprises two modules: inter-domain semantic alignment and intra-domain contrast. The former aims to align annotated instances of the same semantic category from distinct domains within a shared hidd
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#20197;&#21450;&#25945;&#23398;&#20064;&#32773;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2305.02749</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Reinforcement Learning via a Causal World Model. (arXiv:2305.02749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#20197;&#21450;&#25945;&#23398;&#20064;&#32773;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#35299;&#37322;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#34892;&#21160;&#21487;&#33021;&#23545;&#26410;&#26469;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65306;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#32780;&#19981;&#39044;&#20808;&#30693;&#36947;&#29615;&#22659;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#21040;&#21160;&#20316;&#30340;&#24433;&#21709;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#22240;&#26524;&#38142;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#20174;&#32780;&#25581;&#31034;&#20986;&#34892;&#21160;&#26159;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#30340;&#12290;&#19982;&#22823;&#22810;&#25968;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#20302;&#20934;&#30830;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#35299;&#37322;&#24615;&#21644;&#23398;&#20064;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#20844;&#27491;&#24615;&#19982;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20010;&#24615;&#21270;&#20844;&#27491;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#20934;&#30830;&#24615;&#24046;&#24322;&#20197;&#21450;&#22312;FL&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#26102;&#25552;&#20379;&#21442;&#19982;&#28608;&#21169;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02728</link><description>&lt;p&gt;
&#20844;&#27491;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#20943;&#23569;&#20010;&#24615;&#21270;&#38656;&#27714;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Fair Federated Learning reduce the need for Personalisation?. (arXiv:2305.02728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#20844;&#27491;&#24615;&#19982;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20010;&#24615;&#21270;&#20844;&#27491;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#20934;&#30830;&#24615;&#24046;&#24322;&#20197;&#21450;&#22312;FL&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#26102;&#25552;&#20379;&#21442;&#19982;&#28608;&#21169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22312;&#36793;&#32536;&#23458;&#25143;&#31471;&#19978;&#35757;&#32451;ML&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#27169;&#22411;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#20250;&#26377;&#25152;&#24046;&#24322;&#65292;&#36825;&#20250;&#36896;&#25104;&#23545;&#20110;&#27809;&#26377;&#20174;FL&#20013;&#21463;&#30410;&#30340;&#23458;&#25143;&#31471;&#21442;&#19982;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#20844;&#27491;FL&#36890;&#36807;&#20851;&#27880;&#25439;&#22833;&#26356;&#39640;&#30340;&#23458;&#25143;&#31471;&#26469;&#20943;&#23569;&#20934;&#30830;&#24615;&#24046;&#24322;&#65292;&#32780;&#20010;&#24615;&#21270;&#35843;&#25972;&#21017;&#22312;&#26412;&#22320;&#24494;&#35843;&#27169;&#22411;&#12290;&#22312;FL&#27169;&#22411;&#30456;&#23545;&#20110;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#34920;&#29616;&#36739;&#24046;&#26102;&#65292;&#20010;&#24615;&#21270;&#25552;&#20379;&#20102;&#21442;&#19982;&#28608;&#21169;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#20004;&#31181;&#20844;&#27491;FL&#31639;&#27861;&#20316;&#20026;&#20010;&#24615;&#21270;&#30340;&#36215;&#28857;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20844;&#27491;FL&#30340;&#26041;&#27861;&#23545;&#20110;&#35821;&#35328;&#20219;&#21153;&#24182;&#27809;&#26377;&#20135;&#29983;&#30456;&#23545;&#24615;&#33021;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#20219;&#21153;&#20013;&#21487;&#33021;&#20250;&#20351;&#34920;&#29616;&#19981;&#20339;&#30340;&#23458;&#25143;&#31471;&#25968;&#37327;&#22686;&#21152;&#19968;&#20493;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#20844;&#27491;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35821;&#35328;&#21644;&#22270;&#20687;&#20219;&#21153;&#20013;&#20943;&#23569;&#20934;&#30830;&#24615;&#24046;&#24322;&#30340;&#21516;&#26102;&#65292;&#22312;FL&#27169;&#22411;&#30456;&#23545;&#20110;&#26412;&#22320;&#27169;&#22411;&#34920;&#29616;&#36739;&#24046;&#26102;&#25552;&#20379;&#20102;&#21442;&#19982;&#28608;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables training ML models on edge clients without sharing data. However, the federated model's performance on local data varies, disincentivising the participation of clients who benefit little from FL. Fair FL reduces accuracy disparity by focusing on clients with higher losses while personalisation locally fine-tunes the model. Personalisation provides a participation incentive when an FL model underperforms relative to one trained locally. For situations where the federated model provides a lower accuracy than a model trained entirely locally by a client, personalisation improves the accuracy of the pre-trained federated weights to be similar to or exceed those of the local client model. This paper evaluates two Fair FL (FFL) algorithms as starting points for personalisation. Our results show that FFL provides no benefit to relative performance in a language task and may double the number of underperforming clients for an image task. Instead, we propose Pers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#31639;&#27861;&#26469;&#20998;&#26512;&#39640;&#32500;&#29615;&#22659;&#21644;&#20892;&#19994;&#25968;&#25454;&#12290;&#36890;&#36807;&#32771;&#34385;&#32452;&#32467;&#26500;&#21644;&#20351;&#29992;&#20004;&#27493;&#25552;&#21319;&#26041;&#27861;&#65292;&#25105;&#20204;&#39044;&#27979;&#20102;&#20892;&#27665;&#22312;&#38754;&#23545;&#27668;&#20505;&#28798;&#23475;&#26102;&#30340;&#36130;&#21153;&#33030;&#24369;&#24615;&#12290;&#37325;&#35201;&#30340;&#39044;&#27979;&#21464;&#37327;&#21253;&#25324;&#33258;&#28982;&#36164;&#20135;&#12289;&#28748;&#28297;&#31867;&#22411;&#21644;&#38468;&#36817;&#20892;&#22330;&#30340;&#20316;&#29289;&#25439;&#22351;&#12290;&#20132;&#20114;&#20316;&#29992;&#20063;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.02699</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#31639;&#27861;&#24314;&#27169;&#29615;&#22659;&#21644;&#20892;&#19994;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Using interpretable boosting algorithms for modeling environmental and agricultural data. (arXiv:2305.02699v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#25552;&#21319;&#31639;&#27861;&#26469;&#20998;&#26512;&#39640;&#32500;&#29615;&#22659;&#21644;&#20892;&#19994;&#25968;&#25454;&#12290;&#36890;&#36807;&#32771;&#34385;&#32452;&#32467;&#26500;&#21644;&#20351;&#29992;&#20004;&#27493;&#25552;&#21319;&#26041;&#27861;&#65292;&#25105;&#20204;&#39044;&#27979;&#20102;&#20892;&#27665;&#22312;&#38754;&#23545;&#27668;&#20505;&#28798;&#23475;&#26102;&#30340;&#36130;&#21153;&#33030;&#24369;&#24615;&#12290;&#37325;&#35201;&#30340;&#39044;&#27979;&#21464;&#37327;&#21253;&#25324;&#33258;&#28982;&#36164;&#20135;&#12289;&#28748;&#28297;&#31867;&#22411;&#21644;&#38468;&#36817;&#20892;&#22330;&#30340;&#20316;&#29289;&#25439;&#22351;&#12290;&#20132;&#20114;&#20316;&#29992;&#20063;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38416;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#23725;&#27491;&#21017;&#21270;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#25552;&#21319;&#31639;&#27861;&#26469;&#20998;&#26512;&#39640;&#32500;&#29615;&#22659;&#25968;&#25454;&#12290;&#25105;&#20204;&#20197;&#26234;&#21033;&#21644;&#31361;&#23612;&#26031;&#30340;&#20892;&#27665;&#22312;&#38754;&#23545;&#27668;&#20505;&#28798;&#23475;&#26102;&#30340;&#36130;&#21153;&#33030;&#24369;&#24615;&#20026;&#20363;&#65292;&#20351;&#29992;&#29615;&#22659;&#12289;&#31038;&#20250;&#12289;&#20154;&#31867;&#21644;&#29983;&#29289;&#29289;&#29702;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#32771;&#34385;&#32452;&#32467;&#26500;&#20197;&#21450;&#22914;&#20309;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#25214;&#21040;&#20132;&#20114;&#20316;&#29992;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#25552;&#21319;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#21151;&#25928;&#37117;&#24471;&#21040;&#20102;&#23454;&#35777;&#21644;&#35752;&#35770;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20004;&#27493;&#25552;&#21319;&#20013;&#24341;&#20837;&#20132;&#20114;&#20316;&#29992;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;&#22312;&#39044;&#27979;&#25152;&#26377;&#31867;&#22411;&#30340;&#33030;&#24369;&#24615;&#26041;&#38754;&#65292;&#26368;&#37325;&#35201;&#30340;&#21464;&#37327;&#26159;&#33258;&#28982;&#36164;&#20135;&#12290;&#20854;&#20182;&#37325;&#35201;&#21464;&#37327;&#21253;&#25324;&#28748;&#28297;&#31867;&#22411;&#12289;&#32463;&#27982;&#36164;&#20135;&#21644;&#38468;&#36817;&#20892;&#22330;&#20316;&#29289;&#25439;&#22351;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe how interpretable boosting algorithms based on ridge-regularized generalized linear models can be used to analyze high-dimensional environmental data. We illustrate this by using environmental, social, human and biophysical data to predict the financial vulnerability of farmers in Chile and Tunisia against climate hazards. We show how group structures can be considered and how interactions can be found in high-dimensional datasets using a novel 2-step boosting approach. The advantages and efficacy of the proposed method are shown and discussed. Results indicate that the presence of interaction effects only improves predictive power when included in two-step boosting. The most important variable in predicting all types of vulnerabilities are natural assets. Other important variables are the type of irrigation, economic assets and the presence of crop damage of near farms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#39044;&#27979;&#19982;&#26032;&#35266;&#23519;&#20540;&#20043;&#38388;&#24046;&#24322;&#35745;&#31639;&#24322;&#24120;&#24471;&#20998;&#65292;F1&#24471;&#20998;&#20026;0.821&#65292;&#20026;&#24320;&#21457;&#31283;&#20581;&#30340;&#32570;&#38519;&#26816;&#27979;&#26041;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.02695</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
In-situ Anomaly Detection in Additive Manufacturing with Graph Neural Networks. (arXiv:2305.02695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#39044;&#27979;&#19982;&#26032;&#35266;&#23519;&#20540;&#20043;&#38388;&#24046;&#24322;&#35745;&#31639;&#24322;&#24120;&#24471;&#20998;&#65292;F1&#24471;&#20998;&#20026;0.821&#65292;&#20026;&#24320;&#21457;&#31283;&#20581;&#30340;&#32570;&#38519;&#26816;&#27979;&#26041;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#65292;&#23558;&#35774;&#35745;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#20135;&#21697;&#38754;&#20020;&#30528;&#32597;&#35265;&#20107;&#20214;&#23548;&#33268;&#32570;&#38519;&#24418;&#25104;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#22330;&#26816;&#27979;&#36825;&#20123;&#20107;&#20214;&#21487;&#38477;&#20302;&#26816;&#26597;&#25104;&#26412;&#12289;&#23454;&#29616;&#30699;&#27491;&#25514;&#26045;&#65292;&#20063;&#26159;&#23454;&#29616;&#37327;&#36523;&#23450;&#21046;&#26448;&#26009;&#24615;&#33021;&#30340;&#31532;&#19968;&#27493;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#28608;&#20809;&#36755;&#20837;&#20449;&#24687;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#26631;&#20934;&#30340;&#28608;&#20809;&#29076;&#21270;&#26465;&#20214;&#12290;&#28982;&#21518;&#36890;&#36807;&#39044;&#27979;&#19982;&#26032;&#35266;&#23519;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#35745;&#31639;&#24322;&#24120;&#24471;&#20998;&#12290;&#35813;&#27169;&#22411;&#22312;&#19968;&#20010;&#24050;&#30693;&#32570;&#38519;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;F1&#24471;&#20998;&#20026;0.821&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26159;&#24320;&#21457;&#31283;&#20581;&#30340;&#32570;&#38519;&#26816;&#27979;&#26041;&#27861;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transforming a design into a high-quality product is a challenge in metal additive manufacturing due to rare events which can cause defects to form. Detecting these events in-situ could, however, reduce inspection costs, enable corrective action, and is the first step towards a future of tailored material properties. In this study a model is trained on laser input information to predict nominal laser melting conditions. An anomaly score is then calculated by taking the difference between the predictions and new observations. The model is evaluated on a dataset with known defects achieving an F1 score of 0.821. This study shows that anomaly detection methods are an important tool in developing robust defect detection methods.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PubMed&#25968;&#25454;&#24211;&#30340;PGB&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#24322;&#26500;&#22270;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#21644;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;21&#20010;&#31995;&#32479;&#24615;&#35780;&#20215;&#20027;&#39064;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.02691</link><description>&lt;p&gt;
PGB&#65306;&#29992;&#20110;&#24322;&#26500;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#30340;PubMed&#22270;&#25968;&#25454;&#38598;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PGB: A PubMed Graph Benchmark for Heterogeneous Network Representation Learning. (arXiv:2305.02691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02691
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PubMed&#25968;&#25454;&#24211;&#30340;PGB&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#24322;&#26500;&#22270;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#21644;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;21&#20010;&#31995;&#32479;&#24615;&#35780;&#20215;&#20027;&#39064;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#25968;&#37327;&#24555;&#36895;&#22686;&#38271;&#65292;&#20294;&#26159;&#25429;&#25417;&#36825;&#20123;&#25991;&#31456;&#30340;&#25991;&#29486;&#20449;&#24687;&#30340;&#24322;&#36136;&#24615;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#23613;&#31649;&#36890;&#36807;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#25366;&#25496;&#30740;&#31350;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#25429;&#25417;&#21040;&#20102;PubMed&#25968;&#25454;&#24211;&#30340;&#24322;&#36136;&#24615;&#20173;&#19981;&#28165;&#26970;&#65292;&#32780;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;3300&#19975;&#31687;&#25991;&#31456;&#30340;&#24222;&#22823;&#25968;&#23383;&#36164;&#26009;&#24211;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PubMed Graph Benchmark&#65288;PGB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#24322;&#26500;&#22270;&#23884;&#20837;&#30340;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;PGB&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#24322;&#26500;&#32593;&#32476;&#20043;&#19968;&#65292;&#21253;&#21547;3000&#19975;&#31687;&#33521;&#25991;&#25991;&#31456;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#21253;&#21547;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#20316;&#32773;&#12289;&#24341;&#29992;&#12289;MeSH&#26415;&#35821;&#12289;MeSH&#23618;&#27425;&#32467;&#26500;&#21644;&#20854;&#20182;&#19968;&#20123;&#20449;&#24687;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;3&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;21&#20010;&#31995;&#32479;&#24615;&#35780;&#20215;&#20027;&#39064;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;&#22312;PGB&#20013;&#65292;&#25105;&#20204;&#23558;&#19982;PubMed&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#30456;&#20851;&#30340;&#20803;&#25968;&#25454;&#32858;&#21512;&#25104;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a rapid growth in biomedical literature, yet capturing the heterogeneity of the bibliographic information of these articles remains relatively understudied. Although graph mining research via heterogeneous graph neural networks has taken center stage, it remains unclear whether these approaches capture the heterogeneity of the PubMed database, a vast digital repository containing over 33 million articles. We introduce PubMed Graph Benchmark (PGB), a new benchmark dataset for evaluating heterogeneous graph embeddings for biomedical literature. PGB is one of the largest heterogeneous networks to date and consists of 30 million English articles. The benchmark contains rich metadata including abstract, authors, citations, MeSH terms, MeSH hierarchy, and some other information. The benchmark contains an evaluation task of 21 systematic reviews topics from 3 different datasets. In PGB, we aggregate the metadata associated with the biomedical articles from PubMed into a unified
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#65292;&#20294;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.02657</link><description>&lt;p&gt;
&#28145;&#24230;&#23485;&#26494;&#24347;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#20248;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;
Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#65292;&#20294;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23450;&#20041;&#22312;&#26377;&#30028;&#22495;$\mathcal X \subset \mathbb R^{d}$&#19978;&#30340;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#39318;&#20808;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#34987;&#30456;&#24212;&#30340;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#22238;&#24402;&#25152;&#23436;&#20840;&#25551;&#32472;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#35889;&#29305;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#22312;$\mathcal{X}$&#19978;&#20026;&#27491;&#23450;&#65292;&#20854;&#29305;&#24449;&#20540;&#34928;&#20943;&#29575;&#20026;$(d+1)/d$&#12290;&#30001;&#20110;&#26680;&#22238;&#24402;&#20013;&#24050;&#32463;&#24314;&#31435;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\mathcal X \subset \mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\mathbb S^{d}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BA&#31639;&#27861;&#30340;&#19968;&#31181;&#26032;&#30340;&#20462;&#25913;&#65292;&#36890;&#36807;&#35753;&#20056;&#25968;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36890;&#36807;&#19968;&#32500;&#27714;&#26681;&#26469;&#26356;&#26032;&#65292;&#36825;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#25152;&#38656;&#22833;&#30495;&#30340;RD&#20989;&#25968;&#65292;&#32780;&#26080;&#38656;&#20687;&#21407;&#22987;&#31639;&#27861;&#19968;&#26679;&#25506;&#32034;&#25972;&#20010;RD&#26354;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.02650</link><description>&lt;p&gt;
Blahut&#21644;Arimoto&#30340;&#20027;&#39064;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Variations on a Theme by Blahut and Arimoto. (arXiv:2305.02650v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BA&#31639;&#27861;&#30340;&#19968;&#31181;&#26032;&#30340;&#20462;&#25913;&#65292;&#36890;&#36807;&#35753;&#20056;&#25968;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36890;&#36807;&#19968;&#32500;&#27714;&#26681;&#26469;&#26356;&#26032;&#65292;&#36825;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#25152;&#38656;&#22833;&#30495;&#30340;RD&#20989;&#25968;&#65292;&#32780;&#26080;&#38656;&#20687;&#21407;&#22987;&#31639;&#27861;&#19968;&#26679;&#25506;&#32034;&#25972;&#20010;RD&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Blahut-Arimoto&#65288;BA&#65289;&#31639;&#27861;&#22312;&#35745;&#31639;&#36895;&#29575;&#22833;&#30495;&#65288;RD&#65289;&#20989;&#25968;&#26041;&#38754;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20132;&#26367;&#26368;&#23567;&#21270;&#24102;&#26377;&#22266;&#23450;&#20056;&#25968;&#30340;Lagrangian&#20855;&#26377;&#29702;&#24819;&#30340;&#21333;&#35843;&#25910;&#25947;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BA&#31639;&#27861;&#30340;&#26032;&#39062;&#20462;&#25913;&#65292;&#20351;&#20056;&#25968;&#27599;&#27425;&#36845;&#20195;&#36890;&#36807;&#30456;&#23545;&#20110;&#21333;&#35843;&#21333;&#21464;&#37327;&#20989;&#25968;&#30340;&#19968;&#32500;&#27714;&#26681;&#27493;&#39588;&#26356;&#26032;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#29275;&#39039;&#27861;&#26377;&#25928;&#23454;&#29616;&#12290;&#36825;&#20801;&#35768;&#20197;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#26356;&#26032;&#20056;&#25968;&#65292;&#20811;&#26381;&#20102;&#21407;&#22987;BA&#31639;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#65292;&#20854;&#20013;&#20056;&#25968;&#22312;&#25972;&#20010;&#36845;&#20195;&#36807;&#31243;&#20013;&#37117;&#26159;&#22266;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#20462;&#25913;&#21518;&#30340;&#31639;&#27861;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#25152;&#38656;&#22833;&#30495;&#30340;RD&#20989;&#25968;&#65292;&#32780;&#19981;&#20687;&#21407;&#22987;BA&#31639;&#27861;&#19968;&#26679;&#25506;&#32034;&#25972;&#20010;RD&#26354;&#32447;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#20462;&#25913;&#21518;&#30340;&#31639;&#27861;&#20173;&#20250;&#25910;&#25947;&#21040;RD&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Blahut-Arimoto (BA) algorithm has played a fundamental role in the numerical computation of rate-distortion (RD) functions. This algorithm possesses a desirable monotonic convergence property by alternatively minimizing its Lagrangian with a fixed multiplier. In this paper, we propose a novel modification of the BA algorithm, letting the multiplier be updated in each iteration via a one-dimensional root-finding step with respect to a monotonic univariate function, which can be efficiently implemented by Newton's method. This allows the multiplier to be updated in a flexible and efficient manner, overcoming a major drawback of the original BA algorithm wherein the multiplier is fixed throughout iterations. Consequently, the modified algorithm is capable of directly computing the RD function for a given target distortion, without exploring the entire RD curve as in the original BA algorithm. A theoretical analysis shows that the modified algorithm still converges to the RD function a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.02640</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#23384;&#22312;&#38544;&#24615;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#25968;&#25454;&#33539;&#24335;&#65306;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#21333;&#20540;&#30340;&#21333;&#20010;&#39592;&#26550;&#32467;&#26500;&#65292;&#21644;&#19981;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#22810;&#20540;&#30340;&#19968;&#32452;&#22810;&#39592;&#26550;&#32467;&#26500;&#12290;&#22810;&#20010;&#39592;&#26550;&#24341;&#20837;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#65292;&#22810;&#20010;&#20540;&#24341;&#20837;&#20102;&#20998;&#24067;&#20551;&#35774;&#30340;&#26080;&#33021;&#21147;&#65292;&#36825;&#20004;&#32773;&#23548;&#33268;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#33267;&#20170;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#24378;&#24230;&#32780;&#19981;&#26159;&#29420;&#31435;&#22122;&#22768;&#20316;&#20026;&#28508;&#21464;&#37327;&#26469;&#35843;&#33410;&#35777;&#25454;&#19979;&#30028;&#12290;&#36890;&#36807;&#36825;&#31181;&#35774;&#35745;&#24605;&#24819;&#65292;&#19981;&#21516;&#39592;&#26550;&#30340;&#22240;&#26524;&#24378;&#24230;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#34920;&#31034;&#20026;&#21333;&#20540;&#22240;&#26524;&#22270;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#22270;G&#20998;&#35299;&#20026;&#20004;&#20010;&#30456;&#20851;&#23376;&#22270;O&#21644;C&#12290;O&#21253;&#21547;&#35266;&#23519;&#33410;&#28857;&#20043;&#38388;&#30340;&#32431;&#20851;&#31995;&#65292;&#32780;C&#34920;&#31034;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#25277;&#26679;&#65292;&#24182;&#37319;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#36827;&#34892;&#26657;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OPT&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#65292;&#24182;&#19988;&#26657;&#20934;&#26174;&#31034;&#20986;&#20013;&#24230;&#30340;&#36870;&#27604;&#20363;&#32553;&#25918;&#19982;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.02633</link><description>&lt;p&gt;
&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Conformal Nucleus Sampling. (arXiv:2305.02633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#25277;&#26679;&#65292;&#24182;&#37319;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#36827;&#34892;&#26657;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OPT&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#65292;&#24182;&#19988;&#26657;&#20934;&#26174;&#31034;&#20986;&#20013;&#24230;&#30340;&#36870;&#27604;&#20363;&#32553;&#25918;&#19982;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#36807;&#31243;&#26159;&#22522;&#20110;&#20381;&#27425;&#25277;&#26679;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#22522;&#20110;&#26680;&#24515;&#65288;top-p&#65289;&#25277;&#26679;&#30340;&#35299;&#30721;&#36807;&#31243;&#20250;&#20174;&#26368;&#23567;&#21487;&#33021;&#30340;&#21333;&#35789;&#38598;&#20013;&#36873;&#25321;&#65292;&#36825;&#20123;&#21333;&#35789;&#30340;&#32047;&#35745;&#27010;&#29575;&#36229;&#36807;&#27010;&#29575;p&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;top-p&#38598;&#26159;&#21542;&#30495;&#27491;&#19982;&#20854;&#27010;&#29575;&#21547;&#20041;&#23545;&#40784;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#31181;&#26657;&#20934;&#31243;&#24207;&#65292;&#26681;&#25454;&#25152;&#38656;&#30340;&#32622;&#20449;&#27700;&#24179;&#65292;&#19987;&#27880;&#20110;&#26500;&#24314;&#26368;&#23567;&#39044;&#27979;&#38598;&#65292;&#20197;&#26657;&#20934;&#21442;&#25968;p&#20316;&#20026;&#19979;&#19968;&#20010;&#21333;&#35789;&#20998;&#24067;&#29109;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;OPT&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#65292;&#26657;&#20934;&#26174;&#31034;&#20986;&#20013;&#24230;&#30340;&#36870;&#27604;&#20363;&#32553;&#25918;&#19982;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models generate text based on successively sampling the next word. A decoding procedure based on nucleus (top-$p$) sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability $p$. In this work, we assess whether a top-$p$ set is indeed aligned with its probabilistic meaning in various linguistic contexts. We employ conformal prediction, a calibration procedure that focuses on the construction of minimal prediction sets according to a desired confidence level, to calibrate the parameter $p$ as a function of the entropy of the next word distribution. We find that OPT models are overconfident, and that calibration shows a moderate inverse scaling with model size.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#35821;&#35328;&#29305;&#24449;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#29992;&#20110;&#20998;&#26512;&#20010;&#20307;&#21644;&#20849;&#20139;&#25277;&#35937;&#30340;&#24418;&#25104;&#21450;&#20854;&#23545;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20248;&#21270;&#20449;&#24687;&#20869;&#23481;&#20197;&#26368;&#22823;&#21270;&#23398;&#29983;&#22870;&#21169;&#25913;&#21892;&#20102;&#20449;&#24687;&#32534;&#30721;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02632</link><description>&lt;p&gt;
&#19968;&#31181;&#31038;&#20132;&#23398;&#20064;&#20195;&#29702;&#20013;&#35821;&#35328;&#20986;&#29616;&#19982;&#20998;&#26512;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A framework for the emergence and analysis of language in social learning agents. (arXiv:2305.02632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#35821;&#35328;&#29305;&#24449;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#29992;&#20110;&#20998;&#26512;&#20010;&#20307;&#21644;&#20849;&#20139;&#25277;&#35937;&#30340;&#24418;&#25104;&#21450;&#20854;&#23545;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20248;&#21270;&#20449;&#24687;&#20869;&#23481;&#20197;&#26368;&#22823;&#21270;&#23398;&#29983;&#22870;&#21169;&#25913;&#21892;&#20102;&#20449;&#24687;&#32534;&#30721;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#30740;&#31350;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#26222;&#36866;&#24615;&#21644;&#34920;&#24449;&#19981;&#21464;&#24615;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#22312;&#31038;&#20132;&#32422;&#26463;&#19979;&#28436;&#21270;&#24418;&#25104;&#21487;&#20256;&#36798;&#30340;&#34920;&#24449;&#65292;&#23637;&#31034;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#29992;&#20110;&#20998;&#26512;&#20010;&#20307;&#21644;&#20849;&#20139;&#25277;&#35937;&#30340;&#24418;&#25104;&#21450;&#20854;&#23545;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#35813;&#36890;&#20449;&#21327;&#35758;&#26088;&#22312;&#36890;&#36807;&#20302;&#32500;&#34920;&#31034;&#32534;&#30721;&#39640;&#32500;&#20449;&#24687;&#65292;&#27169;&#25311;&#35821;&#35328;&#29305;&#24449;&#12290;&#20351;&#29992;&#32593;&#26684;&#19990;&#30028;&#36855;&#23467;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#25945;&#24072;ANNs&#21521;&#23398;&#29983;ANN&#20256;&#36882;&#21387;&#32553;&#28040;&#24687;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23398;&#29983;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#30446;&#26631;&#21457;&#29616;&#29575;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19990;&#30028;&#20013;&#25512;&#24191;&#20102;&#30446;&#26631;&#20301;&#32622;&#12290;&#36827;&#19968;&#27493;&#20248;&#21270;&#20449;&#24687;&#20869;&#23481;&#20197;&#26368;&#22823;&#21270;&#23398;&#29983;&#22870;&#21169;&#25913;&#21892;&#20102;&#20449;&#24687;&#32534;&#30721;&#65292;&#34920;&#26126;&#31934;&#30830;&#30340;&#34920;&#24449;&#21487;&#20197;&#22312;&#36890;&#20449;&#21327;&#35758;&#20013;&#24471;&#21040;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks (ANNs) are increasingly used as research models, but questions remain about their generalizability and representational invariance. Biological neural networks under social constraints evolved to enable communicable representations, demonstrating generalization capabilities. This study proposes a communication protocol between cooperative agents to analyze the formation of individual and shared abstractions and their impact on task performance. This communication protocol aims to mimic language features by encoding high-dimensional information through low-dimensional representation. Using grid-world mazes and reinforcement learning, teacher ANNs pass a compressed message to a student ANN for better task completion. Through this, the student achieves a higher goal-finding rate and generalizes the goal location across task worlds. Further optimizing message content to maximize student reward improves information encoding, suggesting that an accurate representati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#25552;&#20379;&#20102;&#24773;&#24863;&#35745;&#31639;&#20013;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#30340;&#19968;&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#32654;&#22269;&#27861;&#24459;&#32972;&#26223;&#19979;&#34913;&#37327;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#25351;&#26631;&#65292;&#24182;&#22312;&#33258;&#21160;&#35270;&#39057;&#38754;&#35797;&#20013;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#27979;&#37327;&#20102;&#19968;&#20123;&#31867;&#22411;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#40723;&#21169;&#24773;&#24863;&#35745;&#31639;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#23558;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#32435;&#20837;&#30740;&#31350;&#36807;&#31243;&#21644;&#20135;&#21697;&#20013;&#65292;&#24182;&#32771;&#34385;&#20182;&#20204;&#22312;&#20419;&#36827;&#20844;&#24179;&#21512;&#29702;&#31995;&#32479;&#26041;&#38754;&#30340;&#20316;&#29992;&#12289;&#20195;&#29702;&#21644;&#36131;&#20219;&#12290;</title><link>http://arxiv.org/abs/2305.02629</link><description>&lt;p&gt;
&#23558;&#24515;&#29702;&#27979;&#37327;&#23398;&#21644;&#35745;&#31639;&#35270;&#35282;&#25972;&#21512;&#20110;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#65306;&#33258;&#21160;&#35270;&#39057;&#38754;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Integrating Psychometrics and Computing Perspectives on Bias and Fairness in Affective Computing: A Case Study of Automated Video Interviews. (arXiv:2305.02629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#25552;&#20379;&#20102;&#24773;&#24863;&#35745;&#31639;&#20013;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#30340;&#19968;&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#32654;&#22269;&#27861;&#24459;&#32972;&#26223;&#19979;&#34913;&#37327;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#25351;&#26631;&#65292;&#24182;&#22312;&#33258;&#21160;&#35270;&#39057;&#38754;&#35797;&#20013;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#27979;&#37327;&#20102;&#19968;&#20123;&#31867;&#22411;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#40723;&#21169;&#24773;&#24863;&#35745;&#31639;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#23558;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#32435;&#20837;&#30740;&#31350;&#36807;&#31243;&#21644;&#20135;&#21697;&#20013;&#65292;&#24182;&#32771;&#34385;&#20182;&#20204;&#22312;&#20419;&#36827;&#20844;&#24179;&#21512;&#29702;&#31995;&#32479;&#26041;&#38754;&#30340;&#20316;&#29992;&#12289;&#20195;&#29702;&#21644;&#36131;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#23545;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#22312;&#24773;&#24863;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#30340;&#35770;&#36848;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#20154;&#38469;&#20132;&#27969;&#26694;&#26550;&#65292;&#38416;&#26126;&#20102;&#22914;&#20309;&#30830;&#23450;&#22312;&#25512;&#26029;&#20154;&#31867;&#24773;&#24863;&#21644;&#20854;&#20182;&#24515;&#29702;&#32467;&#26500;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#20559;&#35265;&#26469;&#28304;&#12290;&#35752;&#35770;&#20102;&#34913;&#37327;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#25351;&#26631;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#32654;&#22269;&#27861;&#24459;&#32972;&#26223;&#19979;&#30340;&#30456;&#20851;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#35828;&#26126;&#22914;&#20309;&#22312;&#33258;&#21160;&#35270;&#39057;&#38754;&#35797;&#20013;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#27979;&#37327;&#19968;&#20123;&#31867;&#22411;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#65292;&#20197;&#25512;&#26029;&#24212;&#32856;&#32773;&#30340;&#20154;&#26684;&#21644;&#21487;&#38599;&#20323;&#24615;&#12290;&#25105;&#20204;&#40723;&#21169;&#24773;&#24863;&#35745;&#31639;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#23558;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#32435;&#20837;&#30740;&#31350;&#36807;&#31243;&#21644;&#20135;&#21697;&#20013;&#65292;&#24182;&#32771;&#34385;&#20182;&#20204;&#22312;&#20419;&#36827;&#20844;&#24179;&#21512;&#29702;&#31995;&#32479;&#26041;&#38754;&#30340;&#20316;&#29992;&#12289;&#20195;&#29702;&#21644;&#36131;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a psychometric-grounded exposition of bias and fairness as applied to a typical machine learning pipeline for affective computing. We expand on an interpersonal communication framework to elucidate how to identify sources of bias that may arise in the process of inferring human emotions and other psychological constructs from observed behavior. Various methods and metrics for measuring fairness and bias are discussed along with pertinent implications within the United States legal context. We illustrate how to measure some types of bias and fairness in a case study involving automatic personality and hireability inference from multimodal data collected in video interviews for mock job applications. We encourage affective computing researchers and practitioners to encapsulate bias and fairness in their research processes and products and to consider their role, agency, and responsibility in promoting equitable and just systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;cGANs&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20020;&#30028;&#28909;&#27969;&#26102;&#37325;&#26500;&#27832;&#33150;&#31995;&#32479;&#28909;&#25968;&#25454;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#26377;&#26395;&#25104;&#20026;&#21487;&#38752;&#30340;&#38750;&#20405;&#20837;&#24335;&#20020;&#30028;&#28909;&#27969;&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02622</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#20020;&#30028;&#28909;&#27969;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Critical heat flux diagnosis using conditional generative adversarial networks. (arXiv:2305.02622v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;cGANs&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20020;&#30028;&#28909;&#27969;&#26102;&#37325;&#26500;&#27832;&#33150;&#31995;&#32479;&#28909;&#25968;&#25454;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#26377;&#26395;&#25104;&#20026;&#21487;&#38752;&#30340;&#38750;&#20405;&#20837;&#24335;&#20020;&#30028;&#28909;&#27969;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#30028;&#28909;&#27969; (CHF) &#26159;&#39640;&#28909;&#27969;&#37327;&#28909;&#21147;&#31995;&#32479;&#20013;&#27832;&#33150;&#25442;&#28909;&#36807;&#31243;&#30340;&#20851;&#38190;&#23433;&#20840;&#30028;&#38480;&#12290;&#37492;&#23450;CHF&#23545;&#20110;&#38450;&#27490;&#35774;&#22791;&#25439;&#22351;&#21644;&#30830;&#20445;&#31995;&#32479;&#25972;&#20307;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20854;&#20855;&#26377;&#22797;&#26434;&#24615;&#65292;&#38590;&#20197;&#36827;&#34892;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#36825;&#19968;&#22797;&#26434;&#29616;&#35937;&#65292;&#21508;&#31181;&#26041;&#27861;&#23398;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20294;&#26159;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#30340;&#33719;&#21462;&#21463;&#21040;&#20102;&#24040;&#22823;&#30340;&#36164;&#28304;&#28040;&#32791;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (cGANs) &#30340;&#25968;&#25454;&#39537;&#21160;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;CHF&#26102;&#37325;&#26500;&#27832;&#33150;&#31995;&#32479;&#30340;&#28909;&#25968;&#25454;&#12290;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#20381;&#36182;&#20110;&#25104;&#23545;&#22270;&#20687;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;&#27969;&#27832;&#33150;&#23454;&#39564;&#20013;&#33719;&#24471;&#30340;&#20840;&#21453;&#23556;&#21487;&#35270;&#21270;&#21644;&#32418;&#22806;&#28909;&#27979;&#28201;&#27979;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#26377;&#21487;&#33021;&#25552;&#20379;&#23558;&#30456;&#30028;&#38754;&#21160;&#21147;&#23398;&#21644;&#28909;&#25200;&#21160;&#32852;&#31995;&#36215;&#26469;&#30340;&#35777;&#25454;&#65292;&#36824;&#21487;&#33021;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#38750;&#20405;&#20837;&#24335;&#20020;&#30028;&#28909;&#27969;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The critical heat flux (CHF) is an essential safety boundary in boiling heat transfer processes employed in high heat flux thermal-hydraulic systems. Identifying CHF is vital for preventing equipment damage and ensuring overall system safety, yet it is challenging due to the complexity of the phenomena. For an in-depth understanding of the complicated phenomena, various methodologies have been devised, but the acquisition of high-resolution data is limited by the substantial resource consumption required. This study presents a data-driven, image-to-image translation method for reconstructing thermal data of a boiling system at CHF using conditional generative adversarial networks (cGANs). The supervised learning process relies on paired images, which include total reflection visualizations and infrared thermometry measurements obtained from flow boiling experiments. Our proposed approach has the potential to not only provide evidence connecting phase interface dynamics with thermal dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02614</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#21450;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling. (arXiv:2305.02614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#37319;&#26679;&#12289;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#30340;&#20248;&#21270;&#21450;&#21160;&#24577;&#36873;&#25321;&#26080;&#26631;&#35760;&#25968;&#25454;&#31561;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#23547;&#25214;&#40657;&#31665;&#20989;&#25968;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#34429;&#28982;&#40657;&#31665;&#20989;&#25968;&#30340;&#35780;&#20272;&#25104;&#26412;&#24448;&#24448;&#24456;&#39640;&#65292;&#20294;&#20943;&#23569;&#26114;&#36149;&#26631;&#35760;&#25968;&#25454;&#30340;&#20351;&#29992;&#26159;&#29702;&#24819;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;BO&#29615;&#22659;&#19979;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#20854;&#20013;&#65292;&#20851;&#38190;&#22312;&#20110;&#36873;&#25321;&#39564;&#35777;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20197;&#25552;&#39640;BO&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20248;&#21270;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#37319;&#26679;&#65292;&#25105;&#20204;&#37319;&#29992;&#40657;&#31665;&#21442;&#25968;&#21270;&#37319;&#26679;&#20998;&#24067;&#65292;&#23558;&#20854;&#20248;&#21270;&#20026;&#25152;&#37319;&#29992;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#12290;&#26356;&#36827;&#19968;&#27493;&#65292;&#36890;&#36807;&#20174;&#21160;&#24577;&#36866;&#24212;&#30340;&#26497;&#20540;&#20998;&#24067;&#20013;&#36873;&#25321;&#26410;&#26631;&#31614;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;BO&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;BO&#26041;&#27861;&#22312;&#23398;&#20064;&#21518;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#20351;&#20854;&#21487;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a powerful tool for seeking the global optimum of black-box functions. While evaluations of the black-box functions can be highly costly, it is desirable to reduce the use of expensive labeled data. For the first time, we introduce a teacher-student model to exploit semi-supervised learning that can make use of large amounts of unlabelled data under the context of BO. Importantly, we show that the selection of the validation and unlabeled data is key to the performance of BO. To optimize the sampling of unlabeled data, we employ a black-box parameterized sampling distribution optimized as part of the employed bi-level optimization framework. Taking one step further, we demonstrate that the performance of BO can be further improved by selecting unlabeled data from a dynamically fitted extreme value distribution. Our BO method operates in a learned latent space with reduced dimensionality, making it scalable to high-dimensional problems. The proposed approac
&lt;/p&gt;</description></item><item><title>IMAP&#26159;&#19968;&#20010;&#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;&#65292;&#26080;&#38656;&#21463;&#23475;&#32773;&#31574;&#30053;&#30340;&#20219;&#20309;&#30693;&#35782;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#40657;&#30418;&#35268;&#36991;&#25915;&#20987;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02605</link><description>&lt;p&gt;
IMAP: &#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
IMAP: Intrinsically Motivated Adversarial Policy. (arXiv:2305.02605v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02605
&lt;/p&gt;
&lt;p&gt;
IMAP&#26159;&#19968;&#20010;&#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;&#65292;&#26080;&#38656;&#21463;&#23475;&#32773;&#31574;&#30053;&#30340;&#20219;&#20309;&#30693;&#35782;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#40657;&#30418;&#35268;&#36991;&#25915;&#20987;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#23481;&#26131;&#21463;&#21040;&#35268;&#36991;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23545;&#31574;&#30053;&#25110;&#20540;&#32593;&#32476;&#30340;&#36755;&#20837;&#25110;&#36755;&#20986;&#27880;&#20837;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65307;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#23545;&#25163;&#38388;&#25509;&#24433;&#21709;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#12290; &#23545;&#25239;&#24615;&#31574;&#30053;&#20026;&#35299;&#20915;&#27492;&#31867;&#25915;&#20987;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#21463;&#23475;&#32773;&#25919;&#31574;&#30340;&#23436;&#32654;&#25110;&#37096;&#20998;&#30693;&#35782;&#65292;&#35201;&#20040;&#30001;&#20110;&#20219;&#21153;&#30456;&#20851;&#22870;&#21169;&#30340;&#31232;&#30095;&#24615;&#32780;&#23548;&#33268;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#25919;&#31574;&#65288;IMAP&#65289;&#65292;&#29992;&#20110;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#39640;&#25928;&#30340;&#40657;&#30418;&#35268;&#36991;&#25915;&#20987;&#65292;&#32780;&#19981;&#38656;&#20219;&#20309;&#20851;&#20110;&#21463;&#23475;&#32773;&#31574;&#30053;&#30340;&#30693;&#35782;&#12290; IMAP&#21033;&#29992;&#22522;&#20110;&#29366;&#24577;&#35206;&#30422;&#29575;&#65292;&#31574;&#30053;&#35206;&#30422;&#29575;&#65292;&#39118;&#38505;&#21644;&#25919;&#31574;&#20998;&#27495;&#30340;&#22235;&#20010;&#20869;&#22312;&#30446;&#26631;&#65292;&#20197;&#40723;&#21169;&#25506;&#32034;&#24182;&#21457;&#29616;&#26356;&#24378;&#30340;&#25915;&#20987;&#25216;&#33021;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#23454;&#21147;&#30340;&#23545;&#25163;&#30340;&#21487;&#25512;&#24191;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IMAP&#22312;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;Atari&#28216;&#25103;&#65292;&#19968;&#20010;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#21644;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) agents are known to be vulnerable to evasion attacks during deployment. In single-agent environments, attackers can inject imperceptible perturbations on the policy or value network's inputs or outputs; in multi-agent environments, attackers can control an adversarial opponent to indirectly influence the victim's observation. Adversarial policies offer a promising solution to craft such attacks. Still, current approaches either require perfect or partial knowledge of the victim policy or suffer from sample inefficiency due to the sparsity of task-related rewards. To overcome these limitations, we propose the Intrinsically Motivated Adversarial Policy (IMAP) for efficient black-box evasion attacks in single- and multi-agent environments without any knowledge of the victim policy. IMAP uses four intrinsic objectives based on state coverage, policy coverage, risk, and policy divergence to encourage exploration and discover stronger attacking skills. We also des
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;LayerNorm&#22312;Transformers&#30340;Attention&#23618;&#20013;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#21521;&#37327;&#36827;&#34892;&#25237;&#24433;&#24182;&#23545;&#25152;&#26377;&#21521;&#37327;&#36827;&#34892;&#32553;&#25918;&#65292;LayerNorm&#21487;&#20197;&#24110;&#21161;&#27880;&#24847;&#21147;&#26426;&#21046;&#26356;&#22909;&#22320;&#22788;&#29702;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2305.02582</link><description>&lt;p&gt;
&#20851;&#20110;LayerNorm&#22312;Transformers&#30340;Attention&#20013;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Expressivity Role of LayerNorm in Transformers' Attention. (arXiv:2305.02582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;LayerNorm&#22312;Transformers&#30340;Attention&#23618;&#20013;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#21521;&#37327;&#36827;&#34892;&#25237;&#24433;&#24182;&#23545;&#25152;&#26377;&#21521;&#37327;&#36827;&#34892;&#32553;&#25918;&#65292;LayerNorm&#21487;&#20197;&#24110;&#21161;&#27880;&#24847;&#21147;&#26426;&#21046;&#26356;&#22909;&#22320;&#22788;&#29702;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Layer Normalization&#65288;LayerNorm&#65289;&#26159;&#25152;&#26377;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#37117;&#20855;&#26377;&#30340;&#32452;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;LayerNorm&#23545;&#38543;&#21518;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#19982;&#36890;&#24120;&#35748;&#20026;LayerNorm&#20165;&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#24402;&#19968;&#21270;&#28608;&#27963;&#20540;&#21644;&#22312;&#21453;&#21521;&#20256;&#25773;&#26399;&#38388;&#24402;&#19968;&#21270;&#26799;&#24230;&#30340;&#20849;&#35782;&#19981;&#21516;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;LayerNorm&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#35748;&#20026;&#23427;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;a&#65289;&#23558;&#36755;&#20837;&#21521;&#37327;&#25237;&#24433;&#21040;&#27491;&#20132;&#20110;$\left[1,1,...,1\right]$&#21521;&#37327;&#30340;$d-1$&#31354;&#38388;&#65292;&#24182;&#19988;&#65288;b&#65289;&#23558;&#25152;&#26377;&#21521;&#37327;&#32553;&#25918;&#21040;&#30456;&#21516;&#30340;$\sqrt{d}$&#33539;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27599;&#20010;&#32452;&#20214;&#23545;&#38543;&#21518;&#30340;Transformers&#20013;&#30340;&#27880;&#24847;&#21147;&#23618;&#37117;&#24456;&#37325;&#35201;&#65306;&#65288;a&#65289;&#25237;&#24433;&#20801;&#35768;&#27880;&#24847;&#26426;&#21046;&#21019;&#24314;&#19968;&#20010;&#20851;&#27880;&#25152;&#26377;&#38190;&#31561;&#37327;&#30340;&#27880;&#24847;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#27492;&#25805;&#20316;&#30340;&#38656;&#35201;&#65307;&#65288;b&#65289;&#32553;&#25918;&#20351;&#27599;&#20010;&#38190;&#37117;&#26377;&#21487;&#33021;&#25509;&#25910;&#21040;&#26368;&#39640;&#30340;&#27880;&#24847;&#21147;&#20998;&#37197;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models. In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm's only role is to normalize the activations during the forward pass, and their gradients during the backward pass. We consider a geometric interpretation of LayerNorm and show that it consists of two components: (a) projection of the input vectors to a $d-1$ space that is orthogonal to the $\left[1,1,...,1\right]$ vector, and (b) scaling of all vectors to the same norm of $\sqrt{d}$. We show that each of these components is important for the attention layer that follows it in Transformers: (a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation by the attention; and (b) scaling allows each key to potentially receive the highest a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22270;&#23398;&#20064;&#21644;&#27169;&#22411;&#25311;&#21512;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;LRSM&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02573</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#20998;&#23618;&#27169;&#22411;&#20013;&#30340;&#32852;&#21512;&#22270;&#23398;&#20064;&#21644;&#27169;&#22411;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Joint Graph Learning and Model Fitting in Laplacian Regularized Stratified Models. (arXiv:2305.02573v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22270;&#23398;&#20064;&#21644;&#27169;&#22411;&#25311;&#21512;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;LRSM&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#20998;&#23618;&#27169;&#22411;&#65288;LRSM&#65289;&#26159;&#21033;&#29992;&#23376;&#38382;&#39064;&#30340;&#26174;&#24335;&#25110;&#38544;&#24335;&#32593;&#32476;&#32467;&#26500;&#65292;&#30001;&#20998;&#31867;&#29305;&#24449;&#31216;&#20026;&#23618;&#65288;&#20363;&#22914;&#24180;&#40836;&#12289;&#21306;&#22495;&#12289;&#26102;&#38388;&#12289;&#39044;&#27979;&#26102;&#38388;&#12289;&#31561;&#65289;&#65292;&#24182;&#20174;&#30456;&#37051;&#23618;&#20013;&#33719;&#21462;&#25968;&#25454;&#20197;&#22686;&#24378;&#27599;&#20010;&#23376;&#38382;&#39064;&#30340;&#21442;&#25968;&#23398;&#20064;&#12290;&#23427;&#20204;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#34920;&#31034;&#23398;&#20064;&#65292;&#22270;&#32858;&#31867;&#65292;&#26368;&#22823;&#38388;&#38548;&#20998;&#31867;&#21644;&#19968;&#33324;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LRSM&#30740;&#31350;&#35201;&#20040;&#20551;&#35774;&#24050;&#30693;&#22270;&#24418;&#65292;&#35201;&#20040;&#20165;&#38480;&#20110;&#29305;&#23450;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;LRSM&#20013;&#22270;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#21644;&#25935;&#24863;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#33410;&#28857;&#20043;&#38388;&#21442;&#25968;&#27604;&#20363;&#21644;&#26679;&#26412;&#37327;&#19981;&#24179;&#34913;&#26102;&#65292;&#25935;&#24863;&#24615;&#21487;&#33021;&#20250;&#20219;&#24847;&#22686;&#22823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#22312;&#25311;&#21512;&#27169;&#22411;&#30340;&#21516;&#26102;&#32852;&#21512;&#23398;&#20064;&#22270;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;LRSM&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#32852;&#21512;&#22270;&#23398;&#20064;&#21644;&#27169;&#22411;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20132;&#26367;&#23398;&#20064;&#22270;&#65288;&#36890;&#36807;&#31232;&#30095;&#36870;&#21327;&#26041;&#24046;&#20272;&#35745;&#65289;&#21644;&#25311;&#21512;&#27169;&#22411;&#65288;&#36890;&#36807;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65289;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36824;&#25581;&#31034;&#20102;&#26377;&#20851;&#38382;&#39064;&#30340;&#26377;&#36259;&#22270;&#26696;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Laplacian regularized stratified models (LRSM) are models that utilize the explicit or implicit network structure of the sub-problems as defined by the categorical features called strata (e.g., age, region, time, forecast horizon, etc.), and draw upon data from neighboring strata to enhance the parameter learning of each sub-problem. They have been widely applied in machine learning and signal processing problems, including but not limited to time series forecasting, representation learning, graph clustering, max-margin classification, and general few-shot learning. Nevertheless, existing works on LRSM have either assumed a known graph or are restricted to specific applications. In this paper, we start by showing the importance and sensitivity of graph weights in LRSM, and provably show that the sensitivity can be arbitrarily large when the parameter scales and sample sizes are heavily imbalanced across nodes. We then propose a generic approach to jointly learn the graph while fitting 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#21487;&#25193;&#23637;&#32534;&#30721;&#20013;&#30340;&#26465;&#20214;&#32534;&#30721;&#21644;&#27531;&#24046;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#65292;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02562</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#32534;&#30721;&#20013;&#30340;&#26465;&#20214;&#32534;&#30721;&#21644;&#27531;&#24046;&#32534;&#30721;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Conditional and Residual Methods in Scalable Coding for Humans and Machines. (arXiv:2305.02562v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02562
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#21487;&#25193;&#23637;&#32534;&#30721;&#20013;&#30340;&#26465;&#20214;&#32534;&#30721;&#21644;&#27531;&#24046;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#65292;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#21487;&#25193;&#23637;&#32534;&#30721;&#20013;&#30340;&#26465;&#20214;&#32534;&#30721;&#21644;&#27531;&#24046;&#32534;&#30721;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21487;&#29992;&#30340;&#20449;&#24687;&#26469;&#20248;&#21270;&#37325;&#26500;&#20219;&#21153;&#30340;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20449;&#24687;&#20998;&#26512;&#65292;&#25552;&#20379;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#26465;&#20214;&#32534;&#30721;&#65292;&#20855;&#22791;&#22686;&#21152;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#31867;&#20284;&#30340;&#21487;&#25805;&#20316;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#65292;&#20854;&#20013;&#19968;&#31181;&#23454;&#39564;&#20351;&#29992;&#22312;Cityscapes&#25968;&#25454;&#38598;&#19978;&#21019;&#24314;&#30340;&#35821;&#20041;&#20998;&#21106;&#34920;&#31034;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#21019;&#24314;&#30340;&#30446;&#26631;&#26816;&#27979;&#34920;&#31034;&#12290;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26465;&#20214;&#21644;&#27531;&#24046;&#26041;&#27861;&#20043;&#38388;&#30340;&#31867;&#20284;&#24615;&#33021;&#65292;&#24471;&#21040;&#30340;&#36895;&#29575;&#22833;&#30495;&#26354;&#32447;&#21253;&#21547;&#22312;&#25105;&#20204;&#30340;&#22522;&#32447;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present methods for conditional and residual coding in the context of scalable coding for humans and machines. Our focus is on optimizing the rate-distortion performance of the reconstruction task using the information available in the computer vision task. We include an information analysis of both approaches to provide baselines and also propose an entropy model suitable for conditional coding with increased modelling capacity and similar tractability as previous work. We apply these methods to image reconstruction, using, in one instance, representations created for semantic segmentation on the Cityscapes dataset, and in another instance, representations created for object detection on the COCO dataset. In both experiments, we obtain similar performance between the conditional and residual methods, with the resulting rate-distortion curves contained within our baselines.
&lt;/p&gt;</description></item><item><title>ChatGPT&#21644;Bard&#31561;AI&#24037;&#20855;&#38656;&#35201;&#25345;&#32493;&#22823;&#37327;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20294;&#29616;&#34892;&#30340;&#29256;&#26435;&#27861;&#21017;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#19982;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#23558;&#26377;&#21161;&#20110;&#23558;AI&#24037;&#20855;&#19982;&#22823;&#22810;&#25968;&#29256;&#26435;&#25968;&#25454;&#25317;&#26377;&#32773;&#20043;&#38388;&#30340;&#25932;&#23545;&#20851;&#31995;&#36716;&#21464;&#20026;&#21512;&#20316;&#20851;&#31995;&#65292;&#20351;AI&#29983;&#24577;&#31995;&#32479;&#26356;&#20581;&#24247;&#12290;</title><link>http://arxiv.org/abs/2305.02555</link><description>&lt;p&gt;
ChatGPT&#21644;Bard&#26159;&#21542;&#24212;&#35813;&#19982;&#20854;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#65311;AI&#26102;&#20195;&#30340;&#26032;&#21830;&#19994;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era. (arXiv:2305.02555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02555
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;Bard&#31561;AI&#24037;&#20855;&#38656;&#35201;&#25345;&#32493;&#22823;&#37327;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20294;&#29616;&#34892;&#30340;&#29256;&#26435;&#27861;&#21017;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#19982;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#23558;&#26377;&#21161;&#20110;&#23558;AI&#24037;&#20855;&#19982;&#22823;&#22810;&#25968;&#29256;&#26435;&#25968;&#25454;&#25317;&#26377;&#32773;&#20043;&#38388;&#30340;&#25932;&#23545;&#20851;&#31995;&#36716;&#21464;&#20026;&#21512;&#20316;&#20851;&#31995;&#65292;&#20351;AI&#29983;&#24577;&#31995;&#32479;&#26356;&#20581;&#24247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25105;&#20204;&#27491;&#36827;&#20837;&#30495;&#27491;&#30340;AI&#26102;&#20195;&#12290;&#25105;&#20204;&#21487;&#20197;&#39044;&#35265;&#65292;&#21331;&#36234;&#30340;AI&#24037;&#20855;&#24456;&#24555;&#23558;&#33719;&#24471;&#21487;&#35266;&#30340;&#21033;&#28070;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#38500;&#20102;&#20256;&#32479;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#32929;&#19996;&#65292;AI&#24037;&#20855;&#26159;&#21542;&#24212;&#35813;&#19982;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#65311;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#12290;&#22823;&#22411;AI&#24037;&#20855;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22987;&#32456;&#38656;&#35201;&#26356;&#22810;&#12289;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#19981;&#26029;&#25913;&#36827;&#65292;&#20294;&#24403;&#21069;&#30340;&#29256;&#26435;&#27861;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#22312;AI&#24037;&#20855;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#20998;&#20139;&#25910;&#30410;&#21487;&#20197;&#23558;&#24403;&#21069;&#25932;&#23545;&#30340;&#38646;&#21644;&#28216;&#25103;&#20851;&#31995;&#36716;&#21464;&#20026;&#19968;&#31181;&#21512;&#20316;&#21644;&#20114;&#21033;&#30340;&#20851;&#31995;&#65292;&#32780;&#36825;&#31181;&#20851;&#31995;&#23545;&#20110;&#20419;&#36827;AI&#24037;&#20855;&#12289;&#29992;&#25143;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#33391;&#24615;&#24490;&#29615;&#21457;&#23637;&#12289;&#25512;&#21160;AI&#25216;&#26415;&#24182;&#24314;&#31435;&#20581;&#24247;&#30340;AI&#29983;&#24577;&#31995;&#32479;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25910;&#30410;&#20998;&#20139;&#21830;&#19994;&#27169;&#24335;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
With various AI tools such as ChatGPT becoming increasingly popular, we are entering a true AI era. We can foresee that exceptional AI tools will soon reap considerable profits. A crucial question arise: should AI tools share revenue with their training data providers in additional to traditional stakeholders and shareholders? The answer is Yes. Large AI tools, such as large language models, always require more and better quality data to continuously improve, but current copyright laws limit their access to various types of data. Sharing revenue between AI tools and their data providers could transform the current hostile zero-sum game relationship between AI tools and a majority of copyrighted data owners into a collaborative and mutually beneficial one, which is necessary to facilitate the development of a virtuous cycle among AI tools, their users and data providers that drives forward AI technology and builds a healthy AI ecosystem. However, current revenue-sharing business models 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65288;FormNetV2&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02549</link><description>&lt;p&gt;
FormNetV2&#65306;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (arXiv:2305.02549v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65288;FormNetV2&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#34920;&#26684;&#25991;&#26723;&#29702;&#35299;&#20013;&#30340;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#23637;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21040;&#20854;&#20182;&#27169;&#24577;&#30340;&#26041;&#27861;&#38656;&#35201;&#20180;&#32454;&#30340;&#22810;&#20219;&#21153;&#35843;&#25972;&#12289;&#22797;&#26434;&#30340;&#37325;&#26500;&#30446;&#26631;&#35774;&#35745;&#25110;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;FormNetV2&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#20013;&#30340;&#22810;&#27169;&#24577;&#22270;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#12290;&#22270;&#23545;&#27604;&#30446;&#26631;&#26368;&#22823;&#21270;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#19968;&#33268;&#24615;&#65292;&#20026;&#25152;&#26377;&#27169;&#24577;&#25552;&#20379;&#33258;&#28982;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32780;&#19981;&#38656;&#35201;&#29305;&#27530;&#30340;&#23450;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#36830;&#25509;&#22270;&#36793;&#32536;&#30340;&#19968;&#23545;&#26631;&#35760;&#30340;&#36793;&#26694;&#20869;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#65292;&#25429;&#25417;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#32780;&#26080;&#38656;&#21152;&#36733;&#32463;&#36807;&#22797;&#26434;&#21644;&#21333;&#29420;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#23884;&#20837;&#22120;&#12290;FormNetV2&#22312;FUNSD&#12289;CORD&#12289;SROIE&#21644;Payment&#22522;&#20934;&#27979;&#35797;&#20013;&#30830;&#31435;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss. The graph contrastive objective maximizes the agreement of multimodal representations, providing a natural interplay for all modalities without special customization. In addition, we extract image features within the bounding box that joins a pair of tokens connected by a graph edge, capturing more targeted visual cues without loading a sophisticated and separately pre-trained image embedder. FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE and Payment benchmarks with 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#32447;&#24615;&#26102;&#38388;&#30340;&#40065;&#26834;PCA&#31639;&#27861;&#65292;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#30340;&#35823;&#24046;&#20445;&#35777;&#65292;&#24182;&#19988;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#21333;&#36941;&#27969;&#24335;PCA&#31639;&#27861;&#65292;&#20855;&#26377;&#20960;&#20046;&#32447;&#24615;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.02544</link><description>&lt;p&gt;
&#24322;&#24120;&#20540;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#36817;&#20284;&#32447;&#24615;&#26102;&#38388;&#21644;&#27969;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA. (arXiv:2305.02544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#32447;&#24615;&#26102;&#38388;&#30340;&#40065;&#26834;PCA&#31639;&#27861;&#65292;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#30340;&#35823;&#24046;&#20445;&#35777;&#65292;&#24182;&#19988;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#21333;&#36941;&#27969;&#24335;PCA&#31639;&#27861;&#65292;&#20855;&#26377;&#20960;&#20046;&#32447;&#24615;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20854;&#20013;&#32473;&#23450;&#26469;&#33258;&#20998;&#24067;&#30340;$\mathbb{R}^d$&#30340;&#25968;&#25454;&#38598;&#65292;&#20219;&#21153;&#26159;&#25214;&#21040;&#19968;&#20010;&#21333;&#20301;&#21521;&#37327;$v$&#65292;&#22312;&#27839;$v$&#25237;&#24433;&#21518;&#65292;&#36817;&#20284;&#22320;&#26368;&#22823;&#21270;&#20998;&#24067;&#30340;&#26041;&#24046;&#12290;&#23613;&#31649;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20219;&#21153;&#65292;&#20294;&#22914;&#26524;&#25968;&#25454;&#21253;&#21547;&#21363;&#20351;&#26159;&#23569;&#37327;&#30340;&#24322;&#24120;&#20540;&#65292;&#26631;&#20934;&#20272;&#35745;&#22120;&#20063;&#20250;&#20005;&#37325;&#22833;&#36133;&#65292;&#36825;&#28608;&#21457;&#20102;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#24320;&#21457;&#20986;&#35745;&#31639;&#25928;&#29575;&#36739;&#39640;&#30340;&#40065;&#26834;PCA&#31639;&#27861;&#65292;&#20294;&#35201;&#20040;&#38656;&#35201;&#36229;&#32447;&#24615;&#26102;&#38388;&#65292;&#35201;&#20040;&#20855;&#26377;&#27425;&#20248;&#30340;&#35823;&#24046;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20986;&#19968;&#31181;&#36817;&#20284;&#32447;&#24615;&#26102;&#38388;&#30340;&#40065;&#26834;PCA&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#30340;&#35823;&#24046;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#21333;&#36941;&#27969;&#24335;PCA&#31639;&#27861;&#65292;&#20854;&#20869;&#23384;&#20351;&#29992;&#20960;&#20046;&#19982;&#32500;&#25968;&#25104;&#32447;&#24615;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study principal component analysis (PCA), where given a dataset in $\mathbb{R}^d$ from a distribution, the task is to find a unit vector $v$ that approximately maximizes the variance of the distribution after being projected along $v$. Despite being a classical task, standard estimators fail drastically if the data contains even a small fraction of outliers, motivating the problem of robust PCA. Recent work has developed computationally-efficient algorithms for robust PCA that either take super-linear time or have sub-optimal error guarantees. Our main contribution is to develop a nearly-linear time algorithm for robust PCA with near-optimal error guarantees. We also develop a single-pass streaming algorithm for robust PCA with memory usage nearly-linear in the dimension.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Monte-Carlo&#20272;&#35745;&#22120;&#65292;&#23427;&#33021;&#22815;&#20462;&#27491;&#22312;&#25238;&#38899;&#21452;&#21521;&#20869;&#23481;&#24066;&#22330;&#24179;&#21488;&#19978;&#23454;&#39564;&#26102;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#22330;&#23454;&#39564;&#20013;&#23558;&#23454;&#39564;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.02542</link><description>&lt;p&gt;
&#20462;&#27491;&#24178;&#25200;&#38382;&#39064;&#65306;&#20197;&#25238;&#38899;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Correcting for Interference in Experiments: A Case Study at Douyin. (arXiv:2305.02542v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02542
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Monte-Carlo&#20272;&#35745;&#22120;&#65292;&#23427;&#33021;&#22815;&#20462;&#27491;&#22312;&#25238;&#38899;&#21452;&#21521;&#20869;&#23481;&#24066;&#22330;&#24179;&#21488;&#19978;&#23454;&#39564;&#26102;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#22330;&#23454;&#39564;&#20013;&#23558;&#23454;&#39564;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24178;&#25200;&#26159;&#22312;&#21452;&#21521;&#20869;&#23481;&#24066;&#22330;&#24179;&#21488;&#19978;&#36827;&#34892;&#23454;&#39564;&#26102;&#30340;&#26222;&#36941;&#38382;&#39064;&#65292;&#20363;&#22914;&#20013;&#22269;&#29256;&#30340;TikTok&#8212;&#8212;&#25238;&#38899;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#21019;&#20316;&#32773;&#26159;&#23454;&#39564;&#30340;&#33258;&#28982;&#21333;&#20301;&#65292;&#20294;&#26159;&#21019;&#20316;&#32773;&#36890;&#36807;&#20105;&#22842;&#35266;&#20247;&#26377;&#38480;&#30340;&#26102;&#38388;&#21644;&#27880;&#24847;&#21147;&#20114;&#30456;&#24178;&#25200;&#12290;&#30446;&#21069;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#8220;&#26420;&#32032;&#8221;&#20272;&#35745;&#22120;&#31616;&#21333;&#22320;&#24573;&#30053;&#20102;&#24178;&#25200;&#65292;&#20294;&#36825;&#26679;&#20570;&#20250;&#20135;&#29983;&#27835;&#30103;&#25928;&#26524;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#23454;&#39564;&#25512;&#26029;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25919;&#31574;&#35780;&#20272;&#20013;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#20559;&#24046;&#24456;&#23567;&#65292;&#20294;&#24418;&#24335;&#19978;&#34892;&#20026;&#31574;&#30053;&#19981;&#22815;&#23454;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;Q&#20540;&#24046;&#24322;&#8221;&#30340;(DQ)&#25216;&#26415;&#30340;&#26032;&#22411;Monte-Carlo&#20272;&#35745;&#22120;&#65292;&#23427;&#36798;&#21040;&#20102;&#27835;&#30103;&#25928;&#26524;&#22312;&#20108;&#38454;&#30340;&#20559;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#21457;&#23637;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25919;&#31574;&#35780;&#20272;&#27888;&#21202;&#23637;&#24320;&#29702;&#35770;&#65292;&#23558;DQ&#29702;&#35770;&#25193;&#23637;&#21040;&#25152;&#26377;&#20027;&#35201;&#30340;MDP&#20844;&#24335;&#12290;&#22312;&#23454;&#36341;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#34987;&#37096;&#32626;&#22312;&#25238;&#38899;&#19978;&#65292;&#24182;&#22312;&#29616;&#22330;&#23454;&#39564;&#20013;&#23558;&#23454;&#39564;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interference is a ubiquitous problem in experiments conducted on two-sided content marketplaces, such as Douyin (China's analog of TikTok). In many cases, creators are the natural unit of experimentation, but creators interfere with each other through competition for viewers' limited time and attention. "Naive" estimators currently used in practice simply ignore the interference, but in doing so incur bias on the order of the treatment effect. We formalize the problem of inference in such experiments as one of policy evaluation. Off-policy estimators, while unbiased, are impractically high variance. We introduce a novel Monte-Carlo estimator, based on "Differences-in-Qs" (DQ) techniques, which achieves bias that is second-order in the treatment effect, while remaining sample-efficient to estimate. On the theoretical side, our contribution is to develop a generalized theory of Taylor expansions for policy evaluation, which extends DQ theory to all major MDP formulations. On the practica
&lt;/p&gt;</description></item><item><title>Cuttlefish &#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#20302;&#31209;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#22240;&#24335;&#20998;&#35299;&#36229;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#21152;&#36895;&#65292;&#21487;&#29983;&#25104;&#27604;&#23436;&#20840;&#31209;&#35757;&#32451;&#23567;&#22810;&#36798; 5.6 &#20493;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.02538</link><description>&lt;p&gt;
Cuttlefish: &#26080;&#38656;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#20302;&#31209;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Cuttlefish: Low-rank Model Training without All The Tuning. (arXiv:2305.02538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02538
&lt;/p&gt;
&lt;p&gt;
Cuttlefish &#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#20302;&#31209;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#22240;&#24335;&#20998;&#35299;&#36229;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#21152;&#36895;&#65292;&#21487;&#29983;&#25104;&#27604;&#23436;&#20840;&#31209;&#35757;&#32451;&#23567;&#22810;&#36798; 5.6 &#20493;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35757;&#32451;&#20302;&#31209;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24635;&#25968;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#21152;&#36895;&#12290;&#20294;&#26159;&#65292;&#20302;&#31209;&#27169;&#22411;&#30340;&#35757;&#32451;&#38656;&#35201;&#35843;&#25972;&#22810;&#20010;&#22240;&#24335;&#20998;&#35299;&#36229;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837; Cuttlefish&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#20302;&#31209;&#35757;&#32451;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#22240;&#24335;&#20998;&#35299;&#36229;&#21442;&#25968;&#30340;&#38656;&#35201;&#12290;Cuttlefish &#21033;&#29992;&#20102;&#19968;&#31181;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#22312;&#20960;&#20010; epoch &#30340;&#23436;&#20840;&#31209;&#35757;&#32451;&#21518;&#65292;&#27599;&#20010;&#23618;&#30340;&#31283;&#23450;&#31209;&#31283;&#23450;&#22312;&#19968;&#20010;&#24120;&#25968;&#20540;&#12290;&#19968;&#26086;&#25152;&#26377;&#23618;&#30340;&#31283;&#23450;&#31209;&#37117;&#25910;&#25947;&#65292;Cuttlefish &#23601;&#20174;&#23436;&#20840;&#31209;&#35757;&#32451;&#20999;&#25442;&#21040;&#20302;&#31209;&#35757;&#32451;&#65292;&#23558;&#27599;&#20010;&#22240;&#24335;&#20998;&#35299;&#30340;&#32500;&#25968;&#35774;&#32622;&#20026;&#20854;&#30456;&#24212;&#30340;&#31283;&#23450;&#31209;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992; Cuttlefish &#29983;&#25104;&#30340;&#27169;&#22411;&#27604;&#23436;&#20840;&#31209;&#35757;&#32451;&#23567;&#22810;&#36798; 5.6 &#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02527</link><description>&lt;p&gt;
&#12298;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward. (arXiv:2305.02527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#12290;&#22870;&#21169;&#30340;&#24310;&#36831;&#21644;&#22797;&#26434;&#24615;&#24847;&#21619;&#30528;&#22312;&#32473;&#23450;&#29366;&#24577;&#19979;&#37319;&#21462;&#34892;&#21160;&#29983;&#25104;&#30340;&#22870;&#21169;&#34987;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#24310;&#36831;&#30340;&#26102;&#38388;&#23454;&#20363;&#20013;&#34987;&#39034;&#24207;&#23454;&#29616;&#12290;&#37096;&#20998;&#21311;&#21517;&#23646;&#24615;&#24847;&#21619;&#30528;&#23545;&#20110;&#27599;&#20010;&#29366;&#24577;&#65292;&#23398;&#20064;&#32773;&#21482;&#35266;&#23519;&#21040;&#22312;&#35813;&#29366;&#24577;&#19979;&#37319;&#21462;&#19981;&#21516;&#34892;&#21160;&#20135;&#29983;&#30340;&#36807;&#21435;&#22870;&#21169;&#32452;&#25104;&#37096;&#20998;&#30340;&#24635;&#21644;&#65292;&#20294;&#26159;&#22312;&#35266;&#23519;&#23454;&#20363;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\mathrm{DUCRL2}$&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33719;&#24471;&#27492;&#35774;&#32622;&#30340;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#34920;&#26126;&#23427;&#23454;&#29616;&#20102;$\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ &#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$S$&#21644;$A$&#20998;&#21035;&#26159;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;$D$&#26159;MDP&#30340;&#30452;&#24452;&#65292;$d$&#26159;&#19968;&#20010;&#30001;&#26368;&#22823;&#22870;&#21169;&#24310;&#36831;&#38480;&#21046;&#30340;&#21442;&#25968;&#65292;$T$&#34920;&#31034;&#26102;&#38388;&#30340;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate an infinite-horizon average reward Markov Decision Process (MDP) with delayed, composite, and partially anonymous reward feedback. The delay and compositeness of rewards mean that rewards generated as a result of taking an action at a given state are fragmented into different components, and they are sequentially realized at delayed time instances. The partial anonymity attribute implies that a learner, for each state, only observes the aggregate of past reward components generated as a result of different actions taken at that state, but realized at the observation instance. We propose an algorithm named $\mathrm{DUCRL2}$ to obtain a near-optimal policy for this setting and show that it achieves a regret bound of $\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ where $S$ and $A$ are the sizes of the state and action spaces, respectively, $D$ is the diameter of the MDP, $d$ is a parameter upper bounded by the maximum reward delay, and $T$ denotes the time horizon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25928;&#29575;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#30340;&#20108;&#36827;&#21046;GNN&#25512;&#29702;&#21518;&#31471;&#31639;&#27861;&#65292;&#29992;&#20110;&#20805;&#20998;&#21457;&#25381;GPU&#19978;&#20301;&#25805;&#20316;&#30340;&#29305;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#20986;&#30340;&#31639;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#20108;&#36827;&#21046;GNN&#23454;&#29616;&#25552;&#39640;&#20102;8-22&#20493;&#30340;&#24615;&#33021;&#65292;&#20445;&#25345;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02522</link><description>&lt;p&gt;
BitGNN&#65306;&#37322;&#25918;&#20108;&#36827;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;GPU&#19978;&#30340;&#24615;&#33021;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
BitGNN: Unleashing the Performance Potential of Binary Graph Neural Networks on GPUs. (arXiv:2305.02522v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25928;&#29575;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#30340;&#20108;&#36827;&#21046;GNN&#25512;&#29702;&#21518;&#31471;&#31639;&#27861;&#65292;&#29992;&#20110;&#20805;&#20998;&#21457;&#25381;GPU&#19978;&#20301;&#25805;&#20316;&#30340;&#29305;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#20986;&#30340;&#31639;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#20108;&#36827;&#21046;GNN&#23454;&#29616;&#25552;&#39640;&#20102;8-22&#20493;&#30340;&#24615;&#33021;&#65292;&#20445;&#25345;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23545;&#24352;&#37327;&#36827;&#34892;&#20108;&#20540;&#21270;&#65292;&#20108;&#36827;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#33410;&#30465;GNN&#35745;&#31639;&#30340;&#35745;&#31639;&#37327;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#31639;&#27861;&#35774;&#35745;&#25110;&#35757;&#32451;&#25216;&#26415;&#19978;&#65292;&#27809;&#26377;&#23436;&#20840;&#23454;&#29616;&#23558;&#24615;&#33021;&#28508;&#21147;&#26174;&#29616;&#21040;&#21152;&#36895;&#22120;&#30828;&#20214;&#19978;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20174;&#25928;&#29575;&#30340;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#20102;&#20108;&#36827;&#21046;GNN&#25512;&#29702;&#21518;&#31471;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25277;&#35937;&#21644;&#25216;&#26415;&#65292;&#20197;&#26368;&#20339;&#22320;&#26144;&#23556;&#20108;&#36827;&#21046;GNN&#21450;&#20854;&#35745;&#31639;&#65292;&#20197;&#36866;&#24212;GPU&#19978;&#30340;&#20301;&#25805;&#20316;&#30340;&#29305;&#24615;&#12290;&#22312;&#20351;&#29992;GCNs&#12289;GraphSAGE&#21644;GraphSAINT&#30340;&#30495;&#23454;&#22270;&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#20445;&#25345;&#30456;&#21516;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#20108;&#36827;&#21046;GNN&#23454;&#29616;&#25552;&#39640;&#20102;8-22&#20493;&#12290;BitGNN&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that Binary Graph Neural Networks (GNNs) are promising for saving computations of GNNs through binarized tensors. Prior work, however, mainly focused on algorithm designs or training techniques, leaving it open to how to materialize the performance potential on accelerator hardware fully. This work redesigns the binary GNN inference backend from the efficiency perspective. It fills the gap by proposing a series of abstractions and techniques to map binary GNNs and their computations best to fit the nature of bit manipulations on GPUs. Results on real-world graphs with GCNs, GraphSAGE, and GraphSAINT show that the proposed techniques outperform state-of-the-art binary GNN implementations by 8-22X with the same accuracy maintained. BitGNN code is publicly available.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25945;&#24072;-&#23398;&#29983;&#26426;&#21046;&#23454;&#29616;&#20174;0.5T MRI&#37325;&#24314;1.5T-like&#22270;&#20687;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20174;&#20302;&#22330;&#24378;&#21040;&#39640;&#22330;&#24378;MRI&#22270;&#20687;&#30340;&#26144;&#23556;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02509</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#22686;&#24378;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;0.5T MRI&#30340;1.5T-like&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Enabled Score-Based Generative Model for 1.5T-Like Image Reconstruction from 0.5T MRI. (arXiv:2305.02509v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02509
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25945;&#24072;-&#23398;&#29983;&#26426;&#21046;&#23454;&#29616;&#20174;0.5T MRI&#37325;&#24314;1.5T-like&#22270;&#20687;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20174;&#20302;&#22330;&#24378;&#21040;&#39640;&#22330;&#24378;MRI&#22270;&#20687;&#30340;&#26144;&#23556;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22312;&#20302;&#22330;&#24378;&#19979;&#20855;&#26377;&#38477;&#20302;&#30340;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#65292;&#22312;&#20174;&#39640;&#22330;&#24378;MRI&#22270;&#20687;&#29983;&#25104;&#20302;&#22330;&#24378;MRI&#22270;&#20687;&#26102;&#65292;&#20449;&#21495;&#20250;&#21463;&#21040;&#30772;&#22351;&#65292;&#22240;&#27492;&#37325;&#24314;&#20302;&#22330;&#24378;&#20687;&#39640;&#22330;&#24378;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#33719;&#21462;&#25104;&#23545;&#30340;&#20302;&#22330;&#24378;&#21644;&#39640;&#22330;&#24378;MR&#22270;&#20687;&#36890;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21457;&#29616;&#65292;&#36825;&#20123;&#25361;&#25112;&#30340;&#32452;&#21512;&#20351;&#24471;&#30452;&#25509;&#23398;&#20064;&#20174;&#20302;&#22330;&#24378;MR&#22270;&#20687;&#21040;&#39640;&#22330;&#24378;MR&#22270;&#20687;&#30340;&#26144;&#23556;&#30340;&#24120;&#35268;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#25945;&#24072; - &#23398;&#29983;&#26426;&#21046;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#25945;&#24072;&#20174;&#39640;&#22330;&#24378;&#21040;&#20302;&#22330;&#24378;MR&#22270;&#20687;&#20013;&#23398;&#20064;&#38477;&#32423;&#36807;&#31243;&#65292;&#24182;&#29983;&#25104;&#20266;&#25104;&#23545;&#30340;&#39640;&#22330;&#24378;&#21644;&#20302;&#22330;&#24378;MRI&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#22522;&#20110;&#20998;&#25968;&#30340;&#23398;&#29983;&#35299;&#20915;&#37325;&#24314;&#39640;&#22330;&#24378;&#22270;&#20687;&#30340;&#36870;&#38382;&#39064;&#65292;&#26368;&#32456;&#29983;&#25104;1.5T-like&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic resonance imaging (MRI) is known to have reduced signal-to-noise ratios (SNR) at lower field strengths, leading to signal degradation when producing a low-field MRI image from a high-field one. Therefore, reconstructing a high-field-like image from a low-field MRI is a complex problem due to the ill-posed nature of the task. Additionally, obtaining paired low-field and high-field MR images is often not practical. We theoretically uncovered that the combination of these challenges renders conventional deep learning methods that directly learn the mapping from a low-field MR image to a high-field MR image unsuitable. To overcome these challenges, we introduce a novel meta-learning approach that employs a teacher-student mechanism. Firstly, an optimal-transport-driven teacher learns the degradation process from high-field to low-field MR images and generates pseudo-paired high-field and low-field MRI images. Then, a score-based student solves the inverse problem of reconstructing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#19968;&#31181;&#26032;&#30340;&#31038;&#20250;&#24515;&#29702;&#23398;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#27531;&#24046;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#32593;&#32476;&#36129;&#29486;&#19981;&#36275;&#38382;&#39064;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#21644;&#25913;&#36827;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#27531;&#24046;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02507</link><description>&lt;p&gt;
Stimulative Training++&#65306;&#36229;&#36234;&#27531;&#24046;&#32593;&#32476;&#24615;&#33021;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Stimulative Training++: Go Beyond The Performance Limits of Residual Networks. (arXiv:2305.02507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19968;&#31181;&#26032;&#30340;&#31038;&#20250;&#24515;&#29702;&#23398;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#27531;&#24046;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#32593;&#32476;&#36129;&#29486;&#19981;&#36275;&#38382;&#39064;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#21644;&#25913;&#36827;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#27531;&#24046;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27531;&#24046;&#32593;&#32476;&#22312;&#36817;&#26399;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#25104;&#21151;&#65292;&#24182;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#12290;&#26412;&#25991;&#20174;&#19968;&#31181;&#26032;&#30340;&#31038;&#20250;&#24515;&#29702;&#23398;&#35282;&#24230;&#37325;&#26032;&#32771;&#23519;&#20102;&#27531;&#24046;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#20197;&#21450;&#19977;&#31181;&#25913;&#36827;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#27531;&#24046;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#31867;&#20284;&#31038;&#20250;&#36129;&#29486;&#30340;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#27531;&#24046;&#32593;&#32476;&#20869;&#37096;&#65292;&#23376;&#32593;&#32476;&#22312;&#32676;&#20307;&#20013;&#24037;&#20316;&#26102;&#20542;&#21521;&#20110;&#27604;&#29420;&#33258;&#24037;&#20316;&#26102;&#20184;&#20986;&#26356;&#23569;&#30340;&#21162;&#21147;&#65292;&#36825;&#34987;&#25105;&#20204;&#23450;&#20041;&#20026;&#8220;&#32593;&#32476;&#36129;&#29486;&#19981;&#36275;&#8221;&#12290;&#19982;&#31038;&#20250;&#20013;&#34920;&#29616;&#20986;&#30340;&#20010;&#20307;&#29983;&#20135;&#21147;&#21644;&#25972;&#20307;&#32489;&#25928;&#19979;&#38477;&#31867;&#20284;&#65292;&#32593;&#32476;&#36129;&#29486;&#19981;&#36275;&#24517;&#28982;&#23548;&#33268;&#27531;&#24046;&#32593;&#32476;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Residual networks have shown great success and become indispensable in recent deep neural network models. In this work, we aim to re-investigate the training process of residual networks from a novel social psychology perspective of loafing, and further propose a new training scheme as well as three improved strategies for boosting residual networks beyond their performance limits. Previous research has suggested that residual networks can be considered as ensembles of shallow networks, which implies that the final performance of a residual network is influenced by a group of subnetworks. We identify a previously overlooked problem that is analogous to social loafing, where subnetworks within a residual network are prone to exert less effort when working as part of a group compared to working alone. We define this problem as \textit{network loafing}. Similar to the decreased individual productivity and overall performance as demonstrated in society, network loafing inevitably causes su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#23450;&#20041;&#22312;&#38543;&#26426;&#21464;&#37327;&#38598;&#19978;&#30340;&#32852;&#21512;&#23494;&#24230;&#30340;&#33539;&#30068;&#21450;&#20854;&#24847;&#20041;&#65292;&#20197;&#24110;&#21161;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.02506</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#35299;&#23494;&#24230;&#30340;&#23383;&#31526;&#20018;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
String Diagrams with Factorized Densities. (arXiv:2305.02506v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#23450;&#20041;&#22312;&#38543;&#26426;&#21464;&#37327;&#38598;&#19978;&#30340;&#32852;&#21512;&#23494;&#24230;&#30340;&#33539;&#30068;&#21450;&#20854;&#24847;&#20041;&#65292;&#20197;&#24110;&#21161;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20851;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#27169;&#22411;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#22320;&#24378;&#35843;&#20102;&#38656;&#35201;&#22312;&#25193;&#23637;&#23450;&#21521;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#31867;&#20043;&#38388;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#30340;&#24517;&#35201;&#24615;&#12290;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#27169;&#22411;&#37117;&#23450;&#20041;&#20102;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#21487;&#20197;&#29992;&#20110;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#31232;&#30095;&#32467;&#26500;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#26377;&#20851;&#27010;&#29575;&#26144;&#23556;&#30340;&#39532;&#23572;&#21487;&#22827;&#33539;&#30068;&#30340;&#24037;&#20316;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#33539;&#30068;&#65292;&#20854;&#24577;&#23556;&#23558;&#20998;&#21035;&#30001;&#27599;&#20010;&#26679;&#26412;&#31354;&#38388;&#20998;&#35299;&#30340;&#32852;&#21512;&#23494;&#24230;&#19982;&#20174;&#26679;&#26412;&#21040;&#36820;&#22238;&#20540;&#30340;&#30830;&#23450;&#24615;&#26144;&#23556;&#32452;&#21512;&#12290;&#36825;&#26159;&#36808;&#21521;&#26368;&#36817;&#30340;&#33539;&#30068;&#35770;&#27010;&#29575;&#27979;&#24230;&#25551;&#36848;&#21644;&#36890;&#24120;&#22312;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#20998;&#35299;&#23494;&#24230;&#30340;&#25805;&#20316;&#23450;&#20041;&#20043;&#38388;&#30340;&#32553;&#23567;&#24046;&#36317;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research on probabilistic programs and causal models has highlighted the need to reason compositionally about model classes that extend directed graphical models. Both probabilistic programs and causal models define a joint probability density over a set of random variables, and exhibit sparse structure that can be used to reason about causation and conditional independence. This work builds on recent work on Markov categories of probabilistic mappings to define a category whose morphisms combine a joint density, factorized over each sample space, with a deterministic mapping from samples to return values. This is a step towards closing the gap between recent category-theoretic descriptions of probability measures, and the operational definitions of factorized densities that are commonly employed in probabilistic programming and causal inference.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32479;&#19968;&#22788;&#29702;&#22810;&#27169;&#24577;&#30005;&#23376;&#30149;&#21382;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36339;&#36807;&#29942;&#39048;&#21644;&#27169;&#24577;&#24863;&#30693;&#27880;&#24847;&#21147;&#35299;&#20915;&#20102;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#20917;&#65292;&#21462;&#24471;&#20102;&#22312;&#27515;&#20129;&#29575;&#12289;&#34880;&#31649;&#21152;&#21387;&#32032;&#38656;&#35201;&#21644;&#25554;&#31649;&#38656;&#35201;&#39044;&#27979;&#31561;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.02504</link><description>&lt;p&gt;
&#32479;&#19968;&#22810;&#27169;&#24577;&#25968;&#25454;&#23884;&#20837;&#21644;&#27169;&#24577;&#24863;&#30693;&#27880;&#24847;&#21147;&#23398;&#20064;&#32570;&#22833;&#27169;&#24577;&#30005;&#23376;&#30149;&#21382;
&lt;/p&gt;
&lt;p&gt;
Learning Missing Modal Electronic Health Records with Unified Multi-modal Data Embedding and Modality-Aware Attention. (arXiv:2305.02504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02504
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32479;&#19968;&#22788;&#29702;&#22810;&#27169;&#24577;&#30005;&#23376;&#30149;&#21382;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36339;&#36807;&#29942;&#39048;&#21644;&#27169;&#24577;&#24863;&#30693;&#27880;&#24847;&#21147;&#35299;&#20915;&#20102;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#20917;&#65292;&#21462;&#24471;&#20102;&#22312;&#27515;&#20129;&#29575;&#12289;&#34880;&#31649;&#21152;&#21387;&#32032;&#38656;&#35201;&#21644;&#25554;&#31649;&#38656;&#35201;&#39044;&#27979;&#31561;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;(EHR)&#36890;&#36807;&#22810;&#31181;&#27169;&#24577;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#22810;&#27169;&#24577;EHR&#30446;&#21069;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#25968;&#25454;&#23884;&#20837;&#21644;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#20917;&#12290;&#32570;&#20047;&#36328;&#27169;&#24577;&#30340;&#20849;&#20139;&#23884;&#20837;&#20989;&#25968;&#20250;&#20002;&#24323;&#19981;&#21516;EHR&#27169;&#24577;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;EHR&#30740;&#31350;&#20165;&#20381;&#36182;EHR&#26102;&#24207;&#65292;&#24182;&#19988;&#22240;&#27492;&#65292;EHR&#20013;&#30340;&#32570;&#22833;&#27169;&#24577;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#19968;&#22810;&#27169;&#24577;&#38598;&#23884;&#20837;(UMSE)&#21644;&#27169;&#24577;&#24863;&#30693;&#27880;&#24847;&#21147;(MAA)&#19982;&#36339;&#36807;&#29942;&#39048;(SB)&#12290;UMSE&#23545;&#24453;&#25152;&#26377;EHR&#27169;&#24577;&#32780;&#26080;&#38656;&#21333;&#29420;&#30340;&#25554;&#34917;&#27169;&#22359;&#25110;&#23481;&#26131;&#20986;&#38169;&#30340;&#21521;&#21069;&#20256;&#36882;&#65292;&#32780;MAA&#19982;SB&#23398;&#20064;&#32570;&#22833;&#30340;&#27169;&#24577;EHR&#65292;&#20855;&#26377;&#26377;&#25928;&#30340;&#27169;&#24577;&#24863;&#30693;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;MIMIC-IV&#25968;&#25454;&#38598;&#20013;&#30340;&#27515;&#20129;&#29575;&#12289;&#34880;&#31649;&#21152;&#21387;&#32032;&#38656;&#35201;&#21644;&#25554;&#31649;&#38656;&#35201;&#39044;&#27979;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Record (EHR) provides abundant information through various modalities. However, learning multi-modal EHR is currently facing two major challenges, namely, 1) data embedding and 2) cases with missing modality. A lack of shared embedding function across modalities can discard the temporal relationship between different EHR modalities. On the other hand, most EHR studies are limited to relying only on EHR Times-series, and therefore, missing modality in EHR has not been well-explored. Therefore, in this study, we introduce a Unified Multi-modal Set Embedding (UMSE) and Modality-Aware Attention (MAA) with Skip Bottleneck (SB). UMSE treats all EHR modalities without a separate imputation module or error-prone carry-forward, whereas MAA with SB learns missing modal EHR with effective modality-aware attention. Our model outperforms other baseline models in mortality, vasopressor need, and intubation need prediction with the MIMIC-IV dataset.
&lt;/p&gt;</description></item><item><title>AutoML-GPT &#26159;&#19968;&#31181;&#22522;&#20110; GPT &#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#22320;&#21033;&#29992;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33258;&#21160;&#21270;&#35757;&#32451;&#31649;&#36947;&#65292;&#33410;&#32422;&#20102;&#36873;&#25321;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.02499</link><description>&lt;p&gt;
AutoML-GPT: &#22522;&#20110; GPT &#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AutoML-GPT: Automatic Machine Learning with GPT. (arXiv:2305.02499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02499
&lt;/p&gt;
&lt;p&gt;
AutoML-GPT &#26159;&#19968;&#31181;&#22522;&#20110; GPT &#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#22320;&#21033;&#29992;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33258;&#21160;&#21270;&#35757;&#32451;&#31649;&#36947;&#65292;&#33410;&#32422;&#20102;&#36873;&#25321;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI &#20219;&#21153;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#21644;&#39046;&#22495;&#12290;&#34429;&#28982;&#20026;&#29305;&#23450;&#20219;&#21153;&#21644;&#24212;&#29992;&#31243;&#24207;&#35774;&#35745;&#20102;&#20247;&#22810; AI &#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#25237;&#20837;&#26469;&#26597;&#25214;&#27491;&#30830;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#36229;&#21442;&#25968;&#12290;&#26368;&#36817;&#65292;&#20687; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#21457;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#24182;&#33258;&#21160;&#21033;&#29992; LLM &#33258;&#21160;&#21270;&#35757;&#32451;&#31649;&#36947;&#30340;&#24819;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#27010;&#24565;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102; AutoML-GPT&#65292;&#23427;&#37319;&#29992; GPT &#20316;&#20026;&#36830;&#25509;&#22810;&#31181; AI &#27169;&#22411;&#30340;&#26725;&#26753;&#65292;&#24182;&#21160;&#24577;&#22320;&#20351;&#29992;&#20248;&#21270;&#36229;&#21442;&#25968;&#35757;&#32451;&#27169;&#22411;&#12290;AutoML-GPT &#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#21345;&#20013;&#21160;&#24577;&#33719;&#21462;&#29992;&#25143;&#35831;&#27714;&#65292;&#24182;&#32452;&#25104;&#30456;&#24212;&#30340;&#25552;&#31034;&#27573;&#33853;&#12290;&#26368;&#32456;&#65292;&#36890;&#36807;&#36825;&#20010;&#25552;&#31034;&#27573;&#33853;&#65292;AutoML-GPT &#23558;&#33258;&#21160;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#27169;&#22411;&#26550;&#26500;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI tasks encompass a wide range of domains and fields. While numerous AI models have been designed for specific tasks and applications, they often require considerable human efforts in finding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing LLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and composes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct the experiments from data processing to model architecture, hyperparameter tuning,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#26412;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#22810;GNN&#21644;&#22686;&#24378;&#22270;&#23545;&#27604;&#26694;&#26550;MAG&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#36731;&#37327;&#32423;&#23454;&#20363;L-MAG&#21644;M-MAG&#12290;</title><link>http://arxiv.org/abs/2305.02496</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Graph Contrastive Learning for Anomaly Detection. (arXiv:2305.02496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#26412;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#22810;GNN&#21644;&#22686;&#24378;&#22270;&#23545;&#27604;&#26694;&#26550;MAG&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#36731;&#37327;&#32423;&#23454;&#20363;L-MAG&#21644;M-MAG&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#19982;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#22270;&#23545;&#27604;&#24322;&#24120;&#26816;&#27979;&#65288;GCAD&#65289;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#36890;&#36807;&#22270;&#25193;&#20805;&#21644;&#22810;&#23610;&#24230;&#23545;&#27604;&#27169;&#22359;&#25913;&#21892;&#26816;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22359;&#30340;&#22522;&#26412;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#21644;&#22270;&#25193;&#20805;&#26426;&#21046;&#65292;&#24182;&#35266;&#23519;&#21040;&#22810;&#23610;&#24230;&#23545;&#27604;&#27169;&#22359;&#24182;&#27809;&#26377;&#22686;&#24378;&#34920;&#36798;&#65292;&#32780;&#22810;GNN&#27169;&#22359;&#26159;&#38544;&#34255;&#30340;&#36129;&#29486;&#32773;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24448;&#24448;&#23558;&#22810;GNN&#24102;&#26469;&#30340;&#25910;&#30410;&#24402;&#22240;&#20110;&#22810;&#23610;&#24230;&#27169;&#22359;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#31181;&#35823;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;GNN&#21644;&#22686;&#24378;&#22270;&#23545;&#27604;&#26694;&#26550;MAG&#65292;&#23558;&#29616;&#26377;&#30340;GCAD&#26041;&#27861;&#32479;&#19968;&#22312;&#23545;&#27604;&#33258;&#30417;&#30563;&#30340;&#35270;&#35282;&#19979;&#12290;&#25105;&#20204;&#20174;MAG&#26694;&#26550;&#20013;&#25552;&#21462;&#20102;&#20004;&#20010;&#21464;&#20307;&#65292;L-MAG&#21644;M-MAG&#12290;L-MAG&#26159;&#36731;&#37327;&#32423;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Graph neural networks (GNNs) with contrastive learning for anomaly detection has drawn rising attention recently. Existing graph contrastive anomaly detection (GCAD) methods have primarily focused on improving detection capability through graph augmentation and multi-scale contrast modules. However, the underlying mechanisms of how these modules work have not been fully explored. We dive into the multi-scale and graph augmentation mechanism and observed that multi-scale contrast modules do not enhance the expression, while the multi-GNN modules are the hidden contributors. Previous studies have tended to attribute the benefits brought by multi-GNN to the multi-scale modules. In the paper, we delve into the misconception and propose Multi-GNN and Augmented Graph contrastive framework MAG, which unified the existing GCAD methods in the contrastive self-supervised perspective. We extracted two variants from the MAG framework, L-MAG and M-MAG. The L-MAG is the lightweight instanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#20197;&#24212;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#65292;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#32463;&#27982;&#25928;&#30410;&#65292;&#24182;&#20026;&#29615;&#22659;&#20570;&#20986;&#36129;&#29486;&#12290;&#35813;&#33539;&#22411;&#29702;&#35770;&#30340;&#26694;&#26550;&#23558;&#22312;&#20004;&#37096;&#20998;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;</title><link>http://arxiv.org/abs/2305.02485</link><description>&lt;p&gt;
&#22914;&#20309;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#26410;&#26469;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#65311;&#31532;&#19968;&#37096;&#20998;&#65306;&#33539;&#22411;&#29702;&#35770;&#12290;&#65288;arXiv:2305.02485v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 1: A Paradigmatic Theory. (arXiv:2305.02485v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#20197;&#24212;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#65292;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#32463;&#27982;&#25928;&#30410;&#65292;&#24182;&#20026;&#29615;&#22659;&#20570;&#20986;&#36129;&#29486;&#12290;&#35813;&#33539;&#22411;&#29702;&#35770;&#30340;&#26694;&#26550;&#23558;&#22312;&#20004;&#37096;&#20998;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#30005;&#21147;&#34892;&#19994;&#33073;&#30899;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#37325;&#26032;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#26159;&#19968;&#31181;&#23439;&#35266;&#23618;&#38754;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#39640;&#28183;&#36879;&#29575;&#65292;&#24182;&#23454;&#29616;&#30005;&#21147;&#31995;&#32479;&#30340;&#25805;&#20316;&#23433;&#20840;&#12289;&#32463;&#27982;&#25928;&#29575;&#21644;&#29615;&#22659;&#21451;&#22909;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24066;&#22330;&#35774;&#35745;&#26041;&#27861;&#23398;&#23384;&#22312;&#20110;&#33021;&#28304;&#29616;&#36135;&#24066;&#22330;&#65288;ESM&#65289;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#65288;ASM&#65289;&#21644;&#37329;&#34701;&#24066;&#22330;&#65288;FM&#65289;&#20043;&#38388;&#21327;&#35843;&#19981;&#36275;&#65292;&#21363;&#8220;&#32852;&#21512;&#24066;&#22330;&#8221;&#65292;&#20197;&#21450;&#32570;&#20047;&#21487;&#38752;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#26412;&#25991;&#23558;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#27169;&#25311;&#65292;&#24320;&#21457;&#32852;&#21512;&#24066;&#22330;&#35774;&#35745;&#30340;&#33539;&#22411;&#29702;&#35770;&#21644;&#35814;&#32454;&#26041;&#27861;&#12290;&#31532;&#19968;&#37096;&#20998;&#25552;&#20986;&#20102;&#36825;&#31181;&#26032;&#22411;&#24066;&#22330;&#35774;&#35745;&#21746;&#23398;&#30340;&#29702;&#35770;&#21644;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#24635;&#32467;&#20102;&#22312;&#35774;&#35745;&#32852;&#21512;&#24066;&#22330;&#26102;&#23384;&#22312;&#30340;&#26377;&#20105;&#35758;&#30340;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#20316;&#20026;&#30446;&#26631;&#30740;&#31350;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In face of the pressing need of decarbonization in the power sector, the re-design of electricity market is necessary as a Marco-level approach to accommodate the high penetration of renewable generations, and to achieve power system operation security, economic efficiency, and environmental friendliness. However, existing market design methodologies suffer from the lack of coordination among energy spot market (ESM), ancillary service market (ASM) and financial market (FM), i.e., the "joint market", and the lack of reliable simulation-based verification. To tackle these deficiencies, this two-part paper develops a paradigmatic theory and detailed methods of the joint market design using reinforcement-learning (RL)-based simulation. In Part 1, the theory and framework of this novel market design philosophy are proposed. First, the controversial market design options while designing the joint market are summarized as the targeted research questions. Second, the Markov game model is deve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#26469;&#33258;&#19981;&#21516;&#28304;&#30340;&#26368;&#26032;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;&#28909;&#25104;&#20687;&#12289;&#32418;&#22806;&#28909;&#25104;&#20687;&#12289;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#20197;&#21450;&#34880;&#28082;&#26816;&#27979;&#20013;&#21457;&#29616;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#36825;&#20123;&#25216;&#26415;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20415;&#23452;&#65292;&#24182;&#19988;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02482</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20083;&#33146;&#30284;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Breast Cancer Diagnosis Using Machine Learning Techniques. (arXiv:2305.02482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#26469;&#33258;&#19981;&#21516;&#28304;&#30340;&#26368;&#26032;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;&#28909;&#25104;&#20687;&#12289;&#32418;&#22806;&#28909;&#25104;&#20687;&#12289;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#20197;&#21450;&#34880;&#28082;&#26816;&#27979;&#20013;&#21457;&#29616;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#36825;&#20123;&#25216;&#26415;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20415;&#23452;&#65292;&#24182;&#19988;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#22899;&#24615;&#29983;&#21629;&#20013;&#26368;&#20855;&#23041;&#32961;&#30340;&#30142;&#30149;&#20043;&#19968;&#65292;&#22240;&#27492;&#26089;&#26399;&#21644;&#20934;&#30830;&#30340;&#35786;&#26029;&#22312;&#20943;&#23569;&#24739;&#32773;&#27515;&#20129;&#39118;&#38505;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#35745;&#31639;&#24037;&#20855;&#12289;&#32418;&#22806;&#30456;&#26426;&#20197;&#21450;&#29983;&#29289;&#38459;&#25239;&#23450;&#37327;&#35774;&#22791;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#20854;&#20182;&#21442;&#32771;&#25216;&#26415;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#22914;&#28909;&#25104;&#20687;&#12289;&#32418;&#22806;&#28909;&#25104;&#20687;&#12289;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#20197;&#21450;&#34880;&#28082;&#26816;&#27979;&#20013;&#21457;&#29616;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22240;&#27492;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20415;&#23452;&#12290;&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65292;&#19978;&#36848;&#25216;&#26415;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#20083;&#33146;&#30284;&#35786;&#26029;&#30340;&#24182;&#34892;&#21644;&#25193;&#23637;&#26041;&#27861;&#65292;&#35768;&#22810;&#20316;&#32773;&#24471;&#20986;&#32467;&#35770;&#65292;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#24403;&#31579;&#26597;&#26041;&#27861;&#19982;&#20020;&#24202;&#35786;&#26029;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#26469;&#33258;&#19981;&#21516;&#28304;&#30340;&#26368;&#26032;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#32771;&#34385;&#23427;&#20204;&#30340;&#24615;&#33021;&#12289;&#21487;&#29992;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Breast cancer is one of the most threatening diseases in women's life; thus, the early and accurate diagnosis plays a key role in reducing the risk of death in a patient's life. Mammography stands as the reference technique for breast cancer screening; nevertheless, many countries still lack access to mammograms due to economic, social, and cultural issues. Latest advances in computational tools, infrared cameras and devices for bio-impedance quantification, have given a chance to emerge other reference techniques like thermography, infrared thermography, electrical impedance tomography and biomarkers found in blood tests, therefore being faster, reliable and cheaper than other methods. In the last two decades, the techniques mentioned above have been considered as parallel and extended approaches for breast cancer diagnosis, as well many authors concluded that false positives and false negatives rates are significantly reduced. Moreover, when a screening method works together with a c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#24212;&#29992;&#30340;&#19968;&#33324;&#24615;&#27969;&#31243;&#65292;&#24182;&#20026;&#24320;&#21457;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#25351;&#21335;&#65292;&#20197;&#20415;&#20182;&#20204;&#33021;&#22815;&#37096;&#32626;&#21644;&#32500;&#25252;&#33258;&#24049;&#30340;&#27169;&#22411;&#65307;&#37325;&#28857;&#20851;&#27880;&#21253;&#25324;&#25968;&#25454;&#28304;&#12289;&#20934;&#22791;&#12289;&#24037;&#31243;&#12289;&#24037;&#20855;&#12289;&#38271;&#26399;&#30417;&#25511;&#21644;&#26356;&#26032;&#12289;&#20197;&#21450;&#36947;&#24503;&#32771;&#34385;&#31561;&#26041;&#38754;&#65292;&#36825;&#20026;MLHOps&#20840;&#27969;&#31243;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.02474</link><description>&lt;p&gt;
MLHOps: &#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#36816;&#33829;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MLHOps: Machine Learning for Healthcare Operations. (arXiv:2305.02474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#24212;&#29992;&#30340;&#19968;&#33324;&#24615;&#27969;&#31243;&#65292;&#24182;&#20026;&#24320;&#21457;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#25351;&#21335;&#65292;&#20197;&#20415;&#20182;&#20204;&#33021;&#22815;&#37096;&#32626;&#21644;&#32500;&#25252;&#33258;&#24049;&#30340;&#27169;&#22411;&#65307;&#37325;&#28857;&#20851;&#27880;&#21253;&#25324;&#25968;&#25454;&#28304;&#12289;&#20934;&#22791;&#12289;&#24037;&#31243;&#12289;&#24037;&#20855;&#12289;&#38271;&#26399;&#30417;&#25511;&#21644;&#26356;&#26032;&#12289;&#20197;&#21450;&#36947;&#24503;&#32771;&#34385;&#31561;&#26041;&#38754;&#65292;&#36825;&#20026;MLHOps&#20840;&#27969;&#31243;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20581;&#24247;&#36816;&#33829;&#65288;MLHOps&#65289;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#21487;&#38752;&#12289;&#39640;&#25928;&#12289;&#21487;&#29992;&#21644;&#36947;&#24503;&#30340;&#37096;&#32626;&#21644;&#32500;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#24037;&#20316;&#30340;&#32508;&#36848;&#21644;&#24320;&#21457;&#20154;&#21592;&#12289;&#20020;&#24202;&#21307;&#29983;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#37096;&#32626;&#21644;&#32500;&#25252;&#33258;&#24049;&#27169;&#22411;&#30340;&#25351;&#21335;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#25805;&#20316;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;MLHOps&#27969;&#27700;&#32447;&#30340;&#21021;&#22987;&#35774;&#32622;&#65288;&#21253;&#25324;&#25968;&#25454;&#28304;&#12289;&#20934;&#22791;&#12289;&#24037;&#31243;&#21644;&#24037;&#20855;&#65289;&#12290;&#25105;&#20204;&#38543;&#21518;&#25551;&#36848;&#20102;&#38271;&#26399;&#30417;&#27979;&#21644;&#26356;&#26032;&#65288;&#21253;&#25324;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#21644;&#27169;&#22411;&#26356;&#26032;&#65289;&#21644;&#36947;&#24503;&#32771;&#34385;&#65288;&#21253;&#25324;&#20559;&#35265;&#12289;&#20844;&#24179;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#38544;&#31169;&#65289;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;MLHOps&#20840;&#27969;&#31243;&#20174;&#27010;&#24565;&#21040;&#21021;&#22987;&#21644;&#25345;&#32493;&#37096;&#32626;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Health Operations (MLHOps) is the combination of processes for reliable, efficient, usable, and ethical deployment and maintenance of machine learning models in healthcare settings. This paper provides both a survey of work in this area and guidelines for developers and clinicians to deploy and maintain their own models in clinical practice. We cover the foundational concepts of general machine learning operations, describe the initial setup of MLHOps pipelines (including data sources, preparation, engineering, and tools). We then describe long-term monitoring and updating (including data distribution shifts and model updating) and ethical considerations (including bias, fairness, interpretability, and privacy). This work therefore provides guidance across the full pipeline of MLHOps from conception to initial and ongoing deployment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#22238;&#24402;&#30340;&#28508;&#22312;&#32467;&#26500;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#26410;&#30693;&#26426;&#27969;&#24418;&#19978;&#20351;&#29992;&#27969;&#24418;&#23398;&#20064;&#21644;&#22270;&#23884;&#20837;&#25216;&#26415;&#36827;&#34892;&#21709;&#24212;&#39044;&#27979;&#65292;&#24182;&#20026;&#36825;&#20123;&#21709;&#24212;&#24314;&#31435;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.02473</link><description>&lt;p&gt;
&#26410;&#30693;&#26426;&#27969;&#24418;&#19978;&#30340;&#28508;&#22312;&#32467;&#26500;&#32593;&#32476;&#20013;&#30340;&#21322;&#30417;&#30563;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semisupervised regression in latent structure networks on unknown manifolds. (arXiv:2305.02473v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#22238;&#24402;&#30340;&#28508;&#22312;&#32467;&#26500;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#26410;&#30693;&#26426;&#27969;&#24418;&#19978;&#20351;&#29992;&#27969;&#24418;&#23398;&#20064;&#21644;&#22270;&#23884;&#20837;&#25216;&#26415;&#36827;&#34892;&#21709;&#24212;&#39044;&#27979;&#65292;&#24182;&#20026;&#36825;&#20123;&#21709;&#24212;&#24314;&#31435;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22270;&#22312;&#24314;&#27169;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#32593;&#32476;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28508;&#22312;&#20301;&#32622;&#38543;&#26426;&#22270;&#27169;&#22411;&#35748;&#20026;&#27599;&#20010;&#33410;&#28857;&#37117;&#19982;&#28508;&#22312;&#20301;&#32622;&#21521;&#37327;&#30456;&#20851;&#32852;&#65292;&#24182;&#19988;&#36825;&#20123;&#21521;&#37327;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36981;&#24490;&#26576;&#20123;&#20960;&#20309;&#32467;&#26500;&#12290;&#26412;&#25991;&#32771;&#34385;&#38543;&#26426;&#28857;&#31215;&#22270;&#65292;&#20854;&#20013;&#22312;&#20854;&#21508;&#33258;&#30340;&#28508;&#22312;&#20301;&#32622;&#30340;&#20869;&#31215;&#32473;&#23450;&#30340;&#27010;&#29575;&#19979;&#24418;&#25104;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#36793;&#32536;&#12290;&#25105;&#20204;&#20551;&#35774;&#28508;&#22312;&#20301;&#32622;&#21521;&#37327;&#20301;&#20110;&#26410;&#30693;&#30340;&#19968;&#32500;&#26354;&#32447;&#19978;&#65292;&#24182;&#36890;&#36807;&#22238;&#24402;&#27169;&#22411;&#19982;&#21709;&#24212;&#21327;&#21464;&#37327;&#32806;&#21512;&#12290;&#21033;&#29992;&#28508;&#22312;&#20301;&#32622;&#21521;&#37327;&#30340;&#24213;&#23618;&#20960;&#20309;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24418;&#23398;&#20064;&#21644;&#22270;&#23884;&#20837;&#25216;&#26415;&#65292;&#20197;&#39044;&#27979;&#26679;&#26412;&#22806;&#33410;&#28857;&#19978;&#30340;&#21709;&#24212;&#21464;&#37327;&#65292;&#24182;&#20026;&#36825;&#20123;&#21709;&#24212;&#24314;&#31435;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#27169;&#25311;&#21644;&#23545;Drosophila&#22823;&#33041;&#25968;&#25454;&#30340;&#24212;&#29992;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random graphs are increasingly becoming objects of interest for modeling networks in a wide range of applications. Latent position random graph models posit that each node is associated with a latent position vector, and that these vectors follow some geometric structure in the latent space. In this paper, we consider random dot product graphs, in which an edge is formed between two nodes with probability given by the inner product of their respective latent positions. We assume that the latent position vectors lie on an unknown one-dimensional curve and are coupled with a response covariate via a regression model. Using the geometry of the underlying latent position vectors, we propose a manifold learning and graph embedding technique to predict the response variable on out-of-sample nodes, and we establish convergence guarantees for these responses. Our theoretical results are supported by simulations and an application to Drosophila brain data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25552;&#39640;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#24182;&#20351;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#26469;&#39564;&#35777;69&#20010;&#26032;&#30340;&#31995;&#22806;&#34892;&#26143;&#12290;</title><link>http://arxiv.org/abs/2305.02470</link><description>&lt;p&gt;
&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30340;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65306;&#20351;&#29992;ExoMiner&#30340;&#22810;&#37325;&#24615;&#22686;&#24378;&#39564;&#35777;69&#20010;&#26032;&#34892;&#26143;
&lt;/p&gt;
&lt;p&gt;
Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New Exoplanets Using The Multiplicity Boost of ExoMiner. (arXiv:2305.02470v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02470
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25552;&#39640;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#24182;&#20351;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#26469;&#39564;&#35777;69&#20010;&#26032;&#30340;&#31995;&#22806;&#34892;&#26143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24050;&#30693;&#30340;&#31995;&#22806;&#34892;&#26143;&#26159;&#36890;&#36807;&#39564;&#35777;&#25216;&#26415;&#32780;&#19981;&#26159;&#36890;&#36807;&#34917;&#20805;&#35266;&#27979;&#36827;&#34892;&#30830;&#35748;&#30340;&#12290;&#36825;&#20123;&#25216;&#26415;&#29983;&#25104;&#30340;&#20998;&#25968;&#36890;&#24120;&#20195;&#34920;&#20102;&#26377;&#20851;&#20449;&#21495;&#30340;&#26576;&#20123;&#20449;&#24687;&#65288;&#29992;x&#34920;&#31034;&#65289;&#32473;&#20986;&#30340;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30340;&#27010;&#29575;&#65288;y&#65288;x&#65289;=&#34892;&#26143;&#65289;&#12290;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30830;&#35748;&#22120;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#20960;&#20010;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;vespa&#65288;Morton&#31561;&#20154;2016&#65289;&#12289;Robovetter&#65288;Coughlin&#31561;&#20154;2017&#65289;&#12289;AstroNet&#65288;Shallue&#21644;Vanderburg 2018&#65289;&#12289;ExoNet&#65288;Ansdel&#31561;&#20154;2018&#65289;&#12289;GPC&#21644;RFC&#65288;Armstrong&#31561;&#20154;2020&#65289;&#20197;&#21450;ExoMiner&#65288;Valizadegan&#31561;&#20154;2022&#65289;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#31867;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing exoplanets are discovered using validation techniques rather than being confirmed by complementary observations. These techniques generate a score that is typically the probability of the transit signal being an exoplanet (y(x)=exoplanet) given some information related to that signal (represented by x). Except for the validation technique in Rowe et al. (2014) that uses multiplicity information to generate these probability scores, the existing validation techniques ignore the multiplicity boost information. In this work, we introduce a framework with the following premise: given an existing transit signal vetter (classifier), improve its performance using multiplicity information. We apply this framework to several existing classifiers, which include vespa (Morton et al. 2016), Robovetter (Coughlin et al. 2017), AstroNet (Shallue &amp; Vanderburg 2018), ExoNet (Ansdel et al. 2018), GPC and RFC (Armstrong et al. 2020), and ExoMiner (Valizadegan et al. 2022), to support our cl
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;AI&#31995;&#32479;&#24212;&#35813;&#26377;&#38754;&#26495;&#20197;&#25552;&#39640;&#20854;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#24182;&#19988;&#30028;&#38754;&#24212;&#35813;&#20855;&#26377;&#22522;&#20110;&#31995;&#32479;&#27169;&#22411;&#21644;&#29992;&#25143;&#27169;&#22411;&#29366;&#24577;&#30340;&#24182;&#34892;&#26174;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.02469</link><description>&lt;p&gt;
&#31995;&#32479;&#27169;&#22411;&#21644;&#29992;&#25143;&#27169;&#22411;&#65306;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#38754;&#26495;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
The System Model and the User Model: Exploring AI Dashboard Design. (arXiv:2305.02469v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02469
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;AI&#31995;&#32479;&#24212;&#35813;&#26377;&#38754;&#26495;&#20197;&#25552;&#39640;&#20854;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#24182;&#19988;&#30028;&#38754;&#24212;&#35813;&#20855;&#26377;&#22522;&#20110;&#31995;&#32479;&#27169;&#22411;&#21644;&#29992;&#25143;&#27169;&#22411;&#29366;&#24577;&#30340;&#24182;&#34892;&#26174;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#30028;&#38754;&#35774;&#35745;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25512;&#27979;&#24615;&#35770;&#25991;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#21253;&#25324;&#34987;&#25253;&#36947;&#30340;&#19981;&#33391;&#20132;&#20114;&#12290;&#25105;&#20204;&#35748;&#20026;&#38382;&#39064;&#30340;&#37096;&#20998;&#21407;&#22240;&#26159;&#25991;&#26412;&#24182;&#19981;&#26159;&#25152;&#26377;&#20320;&#38656;&#35201;&#30340;&#19996;&#35199;&#65306;&#22797;&#26434;&#30340;AI&#31995;&#32479;&#24212;&#35813;&#26377;&#38754;&#26495;&#65292;&#23601;&#20687;&#25152;&#26377;&#20854;&#20182;&#22797;&#26434;&#30340;&#35774;&#22791;&#19968;&#26679;&#12290;&#20551;&#35774;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;AI&#31995;&#32479;&#23558;&#21253;&#21547;&#21487;&#35299;&#37322;&#30340;&#21608;&#22260;&#19990;&#30028;&#26041;&#38754;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35752;&#35770;&#36825;&#20123;&#38754;&#26495;&#21487;&#33021;&#26174;&#31034;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#23545;&#20110;&#35768;&#22810;&#31995;&#32479;&#26469;&#35828;&#65292;&#26368;&#37325;&#35201;&#30340;&#27169;&#22411;&#23558;&#26159;&#29992;&#25143;&#27169;&#22411;&#21644;&#31995;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#31995;&#32479;&#27169;&#22411;&#8221;&#21644;&#8220;&#29992;&#25143;&#27169;&#22411;&#8221;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#38754;&#21521;&#22522;&#20110;&#23545;&#35805;&#30340;AI&#31995;&#32479;&#30340;&#30028;&#38754;&#24212;&#35813;&#20855;&#26377;&#22522;&#20110;&#31995;&#32479;&#27169;&#22411;&#21644;&#29992;&#25143;&#27169;&#22411;&#29366;&#24577;&#30340;&#24182;&#34892;&#26174;&#31034;&#12290;&#25214;&#21040;&#35782;&#21035;&#12289;&#35299;&#37322;&#21644;&#26174;&#31034;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#26041;&#27861;&#24212;&#35813;&#26159;&#30028;&#38754;&#30740;&#31350;&#30340;&#26680;&#24515;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is a speculative essay on interface design and artificial intelligence. Recently there has been a surge of attention to chatbots based on large language models, including widely reported unsavory interactions. We contend that part of the problem is that text is not all you need: sophisticated AI systems should have dashboards, just like all other complicated devices. Assuming the hypothesis that AI systems based on neural networks will contain interpretable models of aspects of the world around them, we discuss what data such dashboards might display. We conjecture that, for many systems, the two most important models will be of the user and of the system itself. We call these the System Model and User Model. We argue that, for usability and safety, interfaces to dialogue-based AI systems should have a parallel display based on the state of the System Model and the User Model. Finding ways to identify, interpret, and display these two models should be a core part of interface rese
&lt;/p&gt;</description></item><item><title>Shap-E&#33021;&#22815;&#29983;&#25104;&#26377;&#26465;&#20214;&#30340;3D&#38544;&#24335;&#20989;&#25968;&#65292;&#21487;&#20197;&#21576;&#29616;&#20026;&#32441;&#29702;&#32593;&#26684;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#25910;&#25947;&#26356;&#24555;&#19988;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#36136;&#37327;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.02463</link><description>&lt;p&gt;
Shap-E: &#29983;&#25104;&#26377;&#26465;&#20214;&#30340;3D&#38544;&#24335;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Shap-E: Generating Conditional 3D Implicit Functions. (arXiv:2305.02463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02463
&lt;/p&gt;
&lt;p&gt;
Shap-E&#33021;&#22815;&#29983;&#25104;&#26377;&#26465;&#20214;&#30340;3D&#38544;&#24335;&#20989;&#25968;&#65292;&#21487;&#20197;&#21576;&#29616;&#20026;&#32441;&#29702;&#32593;&#26684;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#25910;&#25947;&#26356;&#24555;&#19988;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#36136;&#37327;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Shap-E&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;3D&#27169;&#22411;&#30340;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#26368;&#36817;&#30340;3D&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;Shap-E&#30452;&#25509;&#29983;&#25104;&#21487;&#20197;&#21576;&#29616;&#20026;&#32441;&#29702;&#32593;&#26684;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#38544;&#24335;&#20989;&#25968;&#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#30340;&#36755;&#20986;&#34920;&#31034;&#12290;&#25105;&#20204;&#20998;&#20004;&#20010;&#38454;&#27573;&#35757;&#32451;Shap-E&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#23558;3D&#27169;&#22411;&#30830;&#23450;&#24615;&#22320;&#26144;&#23556;&#21040;&#38544;&#24335;&#20989;&#25968;&#30340;&#21442;&#25968;&#20013;&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#36755;&#20986;&#32534;&#30721;&#22120;&#30340;&#32467;&#26524;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;3D&#25991;&#26412;&#25968;&#25454;&#21305;&#37197;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;3D&#27169;&#22411;&#12290;&#19982;&#28857;&#20113;&#30340;&#26174;&#24335;&#29983;&#25104;&#27169;&#22411;Point-E&#30456;&#27604;&#65292;Shap-E&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#22312;&#23545;&#39640;&#32500;&#24230;&#12289;&#22810;&#34920;&#31034;&#36755;&#20986;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#20102;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;https://github.com/openai/shap-e&#19978;&#20844;&#24320;&#20102;&#27169;&#22411;&#26435;&#37325;&#12289;&#25512;&#29702;&#20195;&#30721;&#21644;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at https://github.com/openai/shap-e.
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#27969;&#26159;&#19968;&#31181;&#25193;&#23637;&#26631;&#20934;&#21270;&#27969;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#32467;&#21512;&#27969;&#19982;&#24352;&#37327;&#32593;&#32476;&#26469;&#25913;&#21892;&#22312;&#23398;&#20064;&#22810;&#23792;&#20998;&#24067;&#30340;&#22256;&#38590;&#21464;&#20998;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02460</link><description>&lt;p&gt;
&#24352;&#37327;&#27969;&#65306;&#19968;&#31181;&#21464;&#20998;&#25512;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensorizing flows: a tool for variational inference. (arXiv:2305.02460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02460
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#27969;&#26159;&#19968;&#31181;&#25193;&#23637;&#26631;&#20934;&#21270;&#27969;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#32467;&#21512;&#27969;&#19982;&#24352;&#37327;&#32593;&#32476;&#26469;&#25913;&#21892;&#22312;&#23398;&#20064;&#22810;&#23792;&#20998;&#24067;&#30340;&#22256;&#38590;&#21464;&#20998;&#25512;&#26029;&#20219;&#21153;&#20013;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#26631;&#20934;&#21270;&#27969;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#26631;&#20934;&#21270;&#27969;&#36824;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#21464;&#20998;&#25512;&#26029;&#65292;&#20854;&#20013;&#25105;&#20204;&#23581;&#35797;&#22522;&#20110;&#20998;&#24067;&#30340;&#23545;&#25968;&#20284;&#28982;&#25110;&#33021;&#37327;&#20989;&#25968;&#23398;&#20064;&#37319;&#26679;&#22120;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#25968;&#25454;&#12290;&#22312;&#21464;&#20998;&#25512;&#26029;&#20013;&#65292;&#27491;&#24577;&#27969;&#20013;&#20351;&#29992;&#30340;&#21442;&#32771;&#39640;&#26031;&#20998;&#24067;&#30340;&#21333;&#23792;&#24615;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#22810;&#23792;&#20998;&#24067;&#26102;&#20986;&#29616;&#22256;&#38590;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#20934;&#27969;&#30340;&#25193;&#23637;&#65292;&#22312;&#36825;&#20010;&#25193;&#23637;&#20013;&#65292;&#39640;&#26031;&#21442;&#32771;&#34987;&#19968;&#20010;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#65288;&#29305;&#21035;&#26159;&#30697;&#38453;&#31215;&#24577;&#25110;&#24352;&#37327;&#21015;&#36710;&#65289;&#26500;&#36896;&#30340;&#21442;&#32771;&#20998;&#24067;&#25152;&#21462;&#20195;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#22256;&#38590;&#30340;&#21464;&#20998;&#25512;&#26029;&#20219;&#21153;&#20013;&#32467;&#21512;&#27969;&#21644;&#24352;&#37327;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#25913;&#21892;&#20351;&#29992;&#20219;&#19968;&#24037;&#20855;&#25152;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fueled by the expressive power of deep neural networks, normalizing flows have achieved spectacular success in generative modeling, or learning to draw new samples from a distribution given a finite dataset of training samples. Normalizing flows have also been applied successfully to variational inference, wherein one attempts to learn a sampler based on an expression for the log-likelihood or energy function of the distribution, rather than on data. In variational inference, the unimodality of the reference Gaussian distribution used within the normalizing flow can cause difficulties in learning multimodal distributions. We introduce an extension of normalizing flows in which the Gaussian reference is replaced with a reference distribution that is constructed via a tensor network, specifically a matrix product state or tensor train. We show that by combining flows with tensor networks on difficult variational inference tasks, we can improve on the results obtained by using either tool
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#25506;&#31350;&#20102;&#22522;&#20110;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;PRC&#30340;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#25351;&#23548;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.02459</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#19982;&#20027;&#21160;&#23398;&#20064;&#29992;&#20110;&#20849;&#40483;&#26816;&#27979;&#65306;&#35299;&#20915;&#31232;&#26377;&#31867;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge. (arXiv:2305.02459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#25506;&#31350;&#20102;&#22522;&#20110;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;PRC&#30340;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#25351;&#23548;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#31995;&#32479;&#20351;&#24471;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#20363;&#33021;&#22815;&#24471;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23545;&#20110;&#31232;&#26377;&#31867;&#20219;&#21153;&#65288;&#21363;&#31867;&#21035;&#26631;&#31614;&#38750;&#24120;&#23569;&#35265;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&lt;5%&#30340;&#26679;&#26412;&#65289;&#65292;&#25968;&#25454;&#37319;&#38598;&#38556;&#30861;&#20173;&#28982;&#23384;&#22312;&#12290;&#20027;&#21160;&#23398;&#20064;&#19968;&#33324;&#34987;&#25552;&#20986;&#29992;&#20110;&#32531;&#35299;&#36825;&#31181;&#25361;&#25112;&#65292;&#20294;&#36873;&#25321;&#31574;&#30053;&#65292;&#21363;&#36873;&#25321;&#31232;&#26377;&#31867;&#31034;&#20363;&#30340;&#26631;&#20934;&#65292;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#21464;&#21387;&#22120;&#21487;&#20197;&#23454;&#29616;&#36845;&#20195;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#35299;&#20915;&#20102;&#36890;&#36807;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#25552;&#20986;&#30340;&#31232;&#26377;&#31867;&#27010;&#29575;&#65288;PRC&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;&#29305;&#23450;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65288;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25910;&#38598;&#35748;&#30693;&#20849;&#25391;&#30340;&#35821;&#35328;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#36825;&#20123;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;PRC&#26159;&#25351;&#23548;&#27880;&#37322;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#26368;&#32456;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#36716;&#31227;&#23398;&#20064;&#21487;&#20197;&#22312;&#31232;&#32570;&#25968;&#25454;&#24773;&#20917;&#19979;&#25552;&#20379;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer-based systems have enabled greater accuracies with fewer training examples, data acquisition obstacles still persist for rare-class tasks -- when the class label is very infrequent (e.g. &lt; 5% of samples). Active learning has in general been proposed to alleviate such challenges, but choice of selection strategy, the criteria by which rare-class examples are chosen, has not been systematically evaluated. Further, transformers enable iterative transfer-learning approaches. We propose and investigate transfer- and active learning solutions to the rare class problem of dissonance detection through utilizing models trained on closely related tasks and the evaluation of acquisition strategies, including a proposed probability-of-rare-class (PRC) approach. We perform these experiments for a specific rare class problem: collecting language samples of cognitive dissonance from social media. We find that PRC is a simple and effective strategy to guide annotations and ultimately
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#37319;&#26679;&#30340;&#27969;&#24335;PCA&#31639;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#35813;&#31639;&#27861;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#31532;&#19968;&#20010;&#23574;&#38160;&#29575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#26041;&#26696;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.02456</link><description>&lt;p&gt;
&#38754;&#21521;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#30340;&#27969;&#24335;PCA&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Streaming PCA for Markovian Data. (arXiv:2305.02456v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#37319;&#26679;&#30340;&#27969;&#24335;PCA&#31639;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#35813;&#31639;&#27861;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#31532;&#19968;&#20010;&#23574;&#38160;&#29575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#26041;&#26696;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Oja&#22312;1982&#24180;&#30340;&#32463;&#20856;&#35770;&#25991;&#20013;&#39318;&#27425;&#25552;&#20986;&#20197;&#26469;&#65292;Oja&#31639;&#27861;&#24050;&#25104;&#20026;&#27969;&#24335;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24335;PCA&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#20174;&#19968;&#20010;&#19981;&#21487;&#32422;&#12289;&#26080;&#21608;&#26399;&#12289;&#21487;&#36870;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#24179;&#31283;&#20998;&#24067;&#30340;&#26410;&#30693;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#21069;&#19968;&#20010;&#29305;&#24449;&#21521;&#37327;&#12290;&#36825;&#31181;&#24773;&#20917;&#36866;&#29992;&#20110;&#21482;&#33021;&#20174;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;(MCMC)&#31867;&#22411;&#30340;&#31639;&#27861;&#20013;&#37319;&#26679;&#25968;&#25454;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#23545;&#35813;&#38142;&#30340;&#24179;&#31283;&#20998;&#24067;&#30340;&#21442;&#25968;&#36827;&#34892;&#25512;&#26029;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;Oja&#31639;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#37117;&#20551;&#23450;&#25968;&#25454;&#28857;&#26159;IID&#37319;&#26679;&#30340;&#12290;&#23545;&#20110;&#20855;&#26377;&#39532;&#23572;&#21487;&#22827;&#20381;&#36182;&#20851;&#31995;&#30340;&#25968;&#25454;&#27969;&#65292;&#20154;&#20204;&#36890;&#24120;&#23545;&#25968;&#25454;&#36827;&#34892;&#19979;&#37319;&#26679;&#20197;&#33719;&#24471;"&#20960;&#20046;"&#29420;&#31435;&#30340;&#25968;&#25454;&#27969;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;Oja&#31639;&#27861;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#31532;&#19968;&#20010;&#23574;&#38160;&#29575;&#65292;&#20854;&#20013;&#21435;&#25481;&#20102;$n$&#30340;&#23545;&#25968;&#20381;&#36182;&#24615;&#65292;&#32467;&#26524;&#26159;$\mathcal{O}(n^{-1})$&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#26696;&#26469;&#35843;&#25972;&#31639;&#27861;&#30340;&#27493;&#38271;&#65292;&#23427;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#20013;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception in Erikki Oja's seminal paper in 1982, Oja's algorithm has become an established method for streaming principle component analysis (PCA). We study the problem of streaming PCA, where the data-points are sampled from an irreducible, aperiodic, and reversible Markov chain. Our goal is to estimate the top eigenvector of the unknown covariance matrix of the stationary distribution. This setting has implications in situations where data can only be sampled from a Markov Chain Monte Carlo (MCMC) type algorithm, and the goal is to do inference for parameters of the stationary distribution of this chain. Most convergence guarantees for Oja's algorithm in the literature assume that the data-points are sampled IID. For data streams with Markovian dependence, one typically downsamples the data to get a "nearly" independent data stream. In this paper, we obtain the first sharp rate for Oja's algorithm on the entire data, where we remove the logarithmic dependence on $n$ resulti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36125;&#21494;&#26031;&#23433;&#20840;&#39564;&#35777;&#30340;&#31639;&#27861;&#65292;&#23558;&#40657;&#30418;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#36716;&#21270;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#27010;&#29575;&#20195;&#29702;&#27169;&#22411;&#25311;&#21512;&#24555;&#36895;&#39044;&#27979;&#25925;&#38556;&#65292;&#21033;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#20272;&#35745;&#25805;&#20316;&#22495;&#20869;&#30340;&#25925;&#38556;&#27010;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#39640;&#32500;&#12289;&#21361;&#38505;&#12289;&#35745;&#31639;&#26114;&#36149;&#30340;&#31995;&#32479;&#30340;&#39640;&#25928;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.02449</link><description>&lt;p&gt;
&#40657;&#30418;&#31995;&#32479;&#30340;&#36125;&#21494;&#26031;&#23433;&#20840;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Bayesian Safety Validation for Black-Box Systems. (arXiv:2305.02449v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36125;&#21494;&#26031;&#23433;&#20840;&#39564;&#35777;&#30340;&#31639;&#27861;&#65292;&#23558;&#40657;&#30418;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#36716;&#21270;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#27010;&#29575;&#20195;&#29702;&#27169;&#22411;&#25311;&#21512;&#24555;&#36895;&#39044;&#27979;&#25925;&#38556;&#65292;&#21033;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#20272;&#35745;&#25805;&#20316;&#22495;&#20869;&#30340;&#25925;&#38556;&#27010;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#39640;&#32500;&#12289;&#21361;&#38505;&#12289;&#35745;&#31639;&#26114;&#36149;&#30340;&#31995;&#32479;&#30340;&#39640;&#25928;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20934;&#30830;&#20272;&#35745;&#25925;&#38556;&#27010;&#29575;&#23545;&#35748;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#12289;&#21361;&#38505;&#27979;&#35797;&#22330;&#26223;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#20223;&#30495;&#22120;&#65292;&#20272;&#35745;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#30740;&#31350;&#39640;&#25928;&#20272;&#35745;&#25216;&#26415;&#21313;&#20998;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;&#40657;&#30418;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#31639;&#27861;&#8212;&#8212;&#36125;&#21494;&#26031;&#23433;&#20840;&#39564;&#35777;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#25311;&#21512;&#27010;&#29575;&#20195;&#29702;&#27169;&#22411;&#26469;&#39640;&#25928;&#39044;&#27979;&#25925;&#38556;&#12290;&#35813;&#31639;&#27861;&#26088;&#22312;&#25628;&#32034;&#25925;&#38556;&#12289;&#35745;&#31639;&#26368;&#21487;&#33021;&#30340;&#25925;&#38556;&#65292;&#24182;&#21033;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#20272;&#35745;&#25805;&#20316;&#22495;&#20869;&#30340;&#25925;&#38556;&#27010;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#37319;&#38598;&#20989;&#25968;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#35206;&#30422;&#35774;&#35745;&#31354;&#38388;&#12289;&#20248;&#21270;&#35299;&#26512;&#27966;&#29983;&#30340;&#25925;&#38556;&#36793;&#30028;&#21644;&#37319;&#26679;&#39044;&#27979;&#30340;&#25925;&#38556;&#21306;&#22495;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;&#20027;&#35201;&#28041;&#21450;&#21482;&#36755;&#20986;&#20108;&#36827;&#21046;&#25351;&#26631;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately estimating the probability of failure for safety-critical systems is important for certification. Estimation is often challenging due to high-dimensional input spaces, dangerous test scenarios, and computationally expensive simulators; thus, efficient estimation techniques are important to study. This work reframes the problem of black-box safety validation as a Bayesian optimization problem and introduces an algorithm, Bayesian safety validation, that iteratively fits a probabilistic surrogate model to efficiently predict failures. The algorithm is designed to search for failures, compute the most-likely failure, and estimate the failure probability over an operating domain using importance sampling. We introduce a set of three acquisition functions that focus on reducing uncertainty by covering the design space, optimizing the analytically derived failure boundaries, and sampling the predicted failure regions. Mainly concerned with systems that only output a binary indicat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#25945;&#23398;&#24605;&#24819;&#30340;&#32852;&#37030;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#35745;&#65292;&#36890;&#36807;&#38544;&#24335;&#26412;&#22320;&#22870;&#21169;&#35843;&#25972;&#26469;&#25351;&#23548;&#23458;&#25143;&#31471;&#26397;&#30528;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#38431;&#26381;&#21153;&#31471;&#25552;&#20986;&#20102;&#32769;&#34382;&#26426;&#23398;&#20064;&#21644;&#30446;&#26631;&#25945;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.02441</link><description>&lt;p&gt;
&#22522;&#20110;&#22870;&#21169;&#25945;&#23398;&#30340;&#32852;&#37030;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Reward Teaching for Federated Multi-armed Bandits. (arXiv:2305.02441v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#25945;&#23398;&#24605;&#24819;&#30340;&#32852;&#37030;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#35745;&#65292;&#36890;&#36807;&#38544;&#24335;&#26412;&#22320;&#22870;&#21169;&#35843;&#25972;&#26469;&#25351;&#23548;&#23458;&#25143;&#31471;&#26397;&#30528;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#38431;&#26381;&#21153;&#31471;&#25552;&#20986;&#20102;&#32769;&#34382;&#26426;&#23398;&#20064;&#21644;&#30446;&#26631;&#25945;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#37096;&#20998;&#24050;&#26377;&#30340;&#32852;&#37030;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;FMAB&#65289;&#35774;&#35745;&#37117;&#22522;&#20110;&#20551;&#35774;&#23458;&#25143;&#31471;&#20250;&#23454;&#29616;&#25351;&#23450;&#30340;&#35774;&#35745;&#26469;&#19982;&#26381;&#21153;&#22120;&#21327;&#20316;&#12290;&#20294;&#23454;&#38469;&#19978;&#65292;&#21487;&#33021;&#26080;&#27861;&#20462;&#25913;&#23458;&#25143;&#31471;&#29616;&#26377;&#30340;&#21327;&#35758;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#24037;&#20316;&#20851;&#27880;&#22987;&#32456;&#26368;&#22823;&#21270;&#20854;&#20010;&#20307;&#32047;&#31215;&#22870;&#21169;&#30340;&#23458;&#25143;&#31471;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;&#22870;&#21169;&#25945;&#23398;&#8221;&#30340;&#26032;&#24605;&#24819;&#65292;&#21363;&#36890;&#36807;&#38544;&#24335;&#30340;&#26412;&#22320;&#22870;&#21169;&#35843;&#25972;&#25351;&#23548;&#23458;&#25143;&#31471;&#26397;&#30528;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#26381;&#21153;&#22120;&#38754;&#20020;&#20004;&#20010;&#23494;&#20999;&#32806;&#21512;&#30340;&#20219;&#21153;&#65292;&#21363;&#32769;&#34382;&#26426;&#23398;&#20064;&#21644;&#30446;&#26631;&#25945;&#23398;&#65292;&#23427;&#20204;&#30340;&#32467;&#21512;&#38750;&#24120;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026; &#8220;Teaching-After-Learning&#65288;TAL&#65289;&#8221; &#30340;&#20998;&#38454;&#27573;&#26041;&#27861;&#65292;&#20998;&#21035;&#40723;&#21169;&#21644;&#38480;&#21046;&#23458;&#25143;&#31471;&#30340;&#25506;&#32034;&#12290;&#24403;&#23458;&#25143;&#31471;&#31574;&#30053;&#28385;&#36275;&#19968;&#23450;&#30340;&#28201;&#21644;&#35201;&#27714;&#26102;&#65292;&#24314;&#31435;&#20102;TAL&#30340;&#32508;&#21512;&#24615;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#25216;&#26415;&#26041;&#27861;&#26469;&#20998;&#26512;TAL&#30340;&#28909;&#21551;&#21160;&#21644;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TAL&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;FMAB&#35774;&#35745;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing federated multi-armed bandits (FMAB) designs are based on the presumption that clients will implement the specified design to collaborate with the server. In reality, however, it may not be possible to modify the client's existing protocols. To address this challenge, this work focuses on clients who always maximize their individual cumulative rewards, and introduces a novel idea of "reward teaching", where the server guides the clients towards global optimality through implicit local reward adjustments. Under this framework, the server faces two tightly coupled tasks of bandit learning and target teaching, whose combination is non-trivial and challenging. A phased approach, called Teaching-After-Learning (TAL), is first designed to encourage and discourage clients' explorations separately. General performance analyses of TAL are established when the clients' strategies satisfy certain mild requirements. With novel technical approaches developed to analyze the warm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;Transformer API&#19978;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#30456;&#20851;&#25104;&#26412;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#22312;&#36719;&#20214;&#21644;&#30828;&#20214;&#20248;&#21270;&#21644;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2305.02440</link><description>&lt;p&gt;
&#24265;&#20215;&#35780;&#20272;&#33258;&#22238;&#24402;Transformer API&#25512;&#26029;&#25928;&#29575;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs. (arXiv:2305.02440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;Transformer API&#19978;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#30456;&#20851;&#25104;&#26412;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#22312;&#36719;&#20214;&#21644;&#30828;&#20214;&#20248;&#21270;&#21644;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LMM)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24456;&#22810;&#26368;&#20808;&#36827;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#25512;&#29702;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#38750;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;: &#37096;&#32626;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#39069;&#22806;&#25104;&#26412;&#26159;&#21542;&#20540;&#24471;&#39044;&#26399;&#30340;&#33021;&#21147;&#25552;&#21319;?&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#26435;&#34913;&#38656;&#35201;&#19968;&#20010;&#25512;&#29702;&#25928;&#29575;&#24230;&#37327;&#65292;&#23427;&#26082;&#21487;&#20197;&#22312;&#26469;&#33258;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#27169;&#22411;&#20043;&#38388;&#36731;&#26494;&#27604;&#36739;&#65292;&#21448;&#21487;&#20197;&#20195;&#34920;&#22312;&#38548;&#31163;&#24615;&#33021;&#29615;&#22659;&#20013;&#36816;&#34892;&#26597;&#35810;&#30340;&#30495;&#23454;&#25104;&#26412;&#12290;&#20294;&#26159;&#65292;&#20170;&#22825;&#35775;&#38382;LLMs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20165;&#38480;&#20110;&#40657;&#21283;&#23376;&#25991;&#26412;&#29983;&#25104;API&#65292;&#36890;&#36807;&#27492;&#25509;&#21475;&#27979;&#37327;&#30340;&#21407;&#22987;&#36816;&#34892;&#26102;&#38388;&#19981;&#33021;&#28385;&#36275;&#36825;&#20123;&#24895;&#26395;:&#27169;&#22411;&#25552;&#20379;&#32773;&#21487;&#20197;&#24212;&#29992;&#19982;&#27169;&#22411;&#19981;&#30456;&#20851;&#30340;&#21508;&#31181;&#36719;&#20214;&#21644;&#30828;&#20214;&#20248;&#21270;&#65292;&#32780;&#22312;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#19978;&#25552;&#20379;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24615;&#33021;&#20105;&#29992;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#22312;&#29992;&#20110;&#33258;&#22238;&#24402;Transformer API&#20013;&#26381;&#21153;&#30340;&#27169;&#22411;&#19978;&#36816;&#34892;&#26597;&#35810;&#30340;&#30456;&#20851;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) power many state-of-the-art systems in natural language processing. However, these models are extremely computationally expensive, even at inference time, raising the natural question: when is the extra cost of deploying a larger model worth the anticipated boost in capabilities? Better understanding this tradeoff fundamentally could benefit from an inference efficiency metric that is both (i) easily comparable across models from different providers, and (ii) representative of the true cost of running queries in an isolated performance environment. Unfortunately, access to LLMs today is largely restricted to black-box text generation APIs and raw runtimes measured through this interface do not satisfy these desiderata: model providers can apply various software and hardware optimizations orthogonal to the model, and models served on shared infrastructure are susceptible to performance contention. To circumvent these problems, we propose a new metric for com
&lt;/p&gt;</description></item><item><title>GAMIVAL&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28216;&#25103;&#19987;&#29992;&#26080;&#21442;&#32771;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22810;&#31181;&#20248;&#28857;&#12290;&#22312;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#30340;&#20027;&#35266;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;NR VQA&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02422</link><description>&lt;p&gt;
GAMIVAL&#65306;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#30340;&#35270;&#39057;&#36136;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content. (arXiv:2305.02422v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02422
&lt;/p&gt;
&lt;p&gt;
GAMIVAL&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28216;&#25103;&#19987;&#29992;&#26080;&#21442;&#32771;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22810;&#31181;&#20248;&#28857;&#12290;&#22312;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#30340;&#20027;&#35266;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;NR VQA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31227;&#21160;&#20113;&#28216;&#25103;&#34892;&#19994;&#36805;&#36895;&#22686;&#38271;&#12290;&#24403;&#28216;&#25103;&#35270;&#39057;&#20174;&#20113;&#26381;&#21153;&#22120;&#20256;&#36755;&#21040;&#23458;&#25143;&#31471;&#35774;&#22791;&#26102;&#65292;&#38656;&#35201;&#19968;&#31181;&#21487;&#20197;&#30417;&#27979;&#22833;&#30495;&#35270;&#39057;&#36136;&#37327;&#32780;&#26080;&#38656;&#21442;&#32771;&#35270;&#39057;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#30001;&#35745;&#31639;&#26426;&#22270;&#24418;&#24341;&#25806;&#28210;&#26579;&#30340;&#27969;&#24335;&#28216;&#25103;&#35270;&#39057;&#36136;&#37327;&#30340;&#26080;&#21442;&#32771;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;NR VQA&#65289;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#28216;&#25103;&#20869;&#23481;&#36890;&#24120;&#22312;&#32479;&#35745;&#19978;&#19982;&#33258;&#28982;&#35270;&#39057;&#19981;&#21516;&#65292;&#32570;&#20047;&#32454;&#33410;&#65292;&#24182;&#21253;&#21547;&#35768;&#22810;&#24179;&#28369;&#21306;&#22495;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#21517;&#20026;Gaming Video Quality Evaluator&#65288;GAMIVAL&#65289;&#30340;&#26032;&#22411;&#28216;&#25103;&#19987;&#29992;NR VQA&#27169;&#22411;&#65292;&#32467;&#21512;&#21644;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#28216;&#25103;&#22833;&#30495;&#22330;&#26223;&#32479;&#35745;&#27169;&#22411;&#12289;&#31070;&#32463;&#22122;&#22768;&#27169;&#22411;&#21644;&#23458;&#35266;&#36136;&#37327;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;GAMIVAL&#24050;&#32463;&#22312;&#19968;&#20010;&#22823;&#22411;&#30340;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#20027;&#35266;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#22312;&#28216;&#25103;&#20869;&#23481;&#30340;NR VQA&#27169;&#22411;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mobile cloud gaming industry has been rapidly growing over the last decade. When streaming gaming videos are transmitted to customers' client devices from cloud servers, algorithms that can monitor distorted video quality without having any reference video available are desirable tools. However, creating No-Reference Video Quality Assessment (NR VQA) models that can accurately predict the quality of streaming gaming videos rendered by computer graphics engines is a challenging problem, since gaming content generally differs statistically from naturalistic videos, often lacks detail, and contains many smooth regions. Until recently, the problem has been further complicated by the lack of adequate subjective quality databases of mobile gaming content. We have created a new gaming-specific NR VQA model called the Gaming Video Quality Evaluator (GAMIVAL), which combines and leverages the advantages of spatial and temporal gaming distorted scene statistics models, a neural noise model, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02412</link><description>&lt;p&gt;
&#35745;&#21010;&#12289;&#28040;&#38500;&#21644;&#36319;&#36394;&#8212;&#8212;&#35821;&#35328;&#27169;&#22411;&#26159;&#20855;&#22791;&#20307;&#39564;&#30340;&#26234;&#33021;&#20307;&#30340;&#33391;&#24072;&#30410;&#21451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents. (arXiv:2305.02412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#25429;&#25417;&#21040;&#20851;&#20110;&#19990;&#30028;&#30340;&#31243;&#24207;&#21270;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;LLM&#20135;&#29983;&#30340;&#25277;&#35937;&#35745;&#21010;&#26469;&#31616;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#20219;&#21153;&#65292;&#36890;&#36807;&#21160;&#20316;&#25171;&#20998;&#25110;&#21160;&#20316;&#24314;&#27169;&#65288;&#24494;&#35843;&#65289;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#26550;&#26500;&#32487;&#25215;&#20102;&#20960;&#20010;&#38480;&#21046;&#65292;&#20351;&#24471;LLM&#38590;&#20197;&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#65306;&#20363;&#22914;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#24494;&#35843;&#30340;&#25928;&#29575;&#65292;&#39044;&#35757;&#32451;&#30340;&#20559;&#35265;&#20197;&#21450;&#19982;&#38750;&#25991;&#26412;&#29615;&#22659;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#19982;&#20302;&#32423;&#21035;&#21487;&#35757;&#32451;&#30340;&#25191;&#34892;&#22120;&#20445;&#25345;&#20860;&#23481;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;LLMs&#20013;&#30340;&#30693;&#35782;&#26469;&#31616;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#35299;&#20915;&#38382;&#39064;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;Plan&#65292;Eliminate&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#12290;&#35745;&#21010;&#27169;&#22359;&#23558;&#20219;&#21153;&#25551;&#36848;&#36716;&#21270;&#20026;&#39640;&#23618;&#27425;&#23376;&#20219;&#21153;&#30340;&#21015;&#34920;&#12290;&#28040;&#38500;&#27169;&#22359;&#20174;&#24403;&#21069;&#23376;&#20219;&#21153;&#30340;&#35266;&#23519;&#20013;&#23631;&#34109;&#19981;&#30456;&#20851;&#30340;&#23545;&#35937;&#21644;&#23481;&#22120;&#12290;&#26368;&#21518;&#65292;&#36319;&#36394;&#27169;&#22359;&#30830;&#23450;&#26234;&#33021;&#20307;&#26159;&#21542;&#24050;&#32463;&#23454;&#29616;&#20102;&#24403;&#21069;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accompli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#27491;&#21017;&#27969;&#31639;&#27861;&#26469;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#35268;&#33539;&#22330;&#29702;&#35770;&#20013;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;SU(3)&#35268;&#33539;&#22330;&#29702;&#35770;&#30340;&#21407;&#29702;&#35777;&#26126;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;&#20986;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02402</link><description>&lt;p&gt;
&#27491;&#21017;&#27969;&#22312;&#20219;&#24847;&#31354;&#26102;&#32500;&#24230;&#35268;&#33539;&#22330;&#29702;&#35770;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows for lattice gauge theory in arbitrary space-time dimension. (arXiv:2305.02402v1 [hep-lat])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#27491;&#21017;&#27969;&#31639;&#27861;&#26469;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#35268;&#33539;&#22330;&#29702;&#35770;&#20013;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;SU(3)&#35268;&#33539;&#22330;&#29702;&#35770;&#30340;&#21407;&#29702;&#35777;&#26126;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;&#20986;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#27969;&#24050;&#32463;&#34987;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20108;&#32500;&#35268;&#33539;&#22330;&#29702;&#35770;&#20013;&#30340;&#22330;&#24577;&#37319;&#26679;&#65292;&#20294;&#22312;&#26356;&#39640;&#32500;&#31354;&#26102;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#21457;&#23637;&#65292;&#20351;&#29992;&#35268;&#33539;&#31561;&#21464;&#27969;&#20307;&#31995;&#32467;&#26500;&#65292;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#30340;&#35268;&#21017;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35752;&#35770;&#20855;&#26377;&#21487;&#36861;&#28335;&#21644;&#26080;&#20559;&#38597;&#21487;&#27604;&#34892;&#21015;&#24335;&#30340;&#33945;&#30382;&#33258;&#22238;&#24402;&#36716;&#25442;&#65292;&#36825;&#26159;&#21487;&#20280;&#32553;&#21644;&#36817;&#20284;&#31934;&#30830;&#30340;&#27969;&#20307;&#24577;&#37319;&#26679;&#31639;&#27861;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#30830;&#20445;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#22312;&#22235;&#32500;&#26102;&#31354;&#20013;&#36827;&#34892;&#20102;SU(3)&#35268;&#33539;&#22330;&#29702;&#35770;&#30340;&#21407;&#29702;&#35777;&#26126;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications of normalizing flows to the sampling of field configurations in lattice gauge theory have so far been explored almost exclusively in two space-time dimensions. We report new algorithmic developments of gauge-equivariant flow architectures facilitating the generalization to higher-dimensional lattice geometries. Specifically, we discuss masked autoregressive transformations with tractable and unbiased Jacobian determinants, a key ingredient for scalable and asymptotically exact flow-based sampling algorithms. For concreteness, results from a proof-of-principle application to SU(3) lattice gauge theory in four space-time dimensions are reported.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22235;&#31181;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#31181;&#22522;&#20110;S-DOTA&#30340;&#21512;&#25104;&#22686;&#24378;&#26041;&#27861;&#12289;&#19968;&#20010;&#22522;&#20110;ICC profile&#30340;&#39068;&#33394;&#26657;&#20934;&#26041;&#27861;&#21644;&#19968;&#20010;&#20256;&#32479;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;S-DOTA&#22686;&#24378;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23383;&#30149;&#29702;&#22270;&#20687;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02401</link><description>&lt;p&gt;
&#8220;&#21512;&#25104; DOmain-Targeted Augmentation (S-DOTA) &#25552;&#21319;&#25968;&#23383;&#30149;&#29702;&#23398;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#8221;
&lt;/p&gt;
&lt;p&gt;
Synthetic DOmain-Targeted Augmentation (S-DOTA) Improves Model Generalization in Digital Pathology. (arXiv:2305.02401v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22235;&#31181;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#31181;&#22522;&#20110;S-DOTA&#30340;&#21512;&#25104;&#22686;&#24378;&#26041;&#27861;&#12289;&#19968;&#20010;&#22522;&#20110;ICC profile&#30340;&#39068;&#33394;&#26657;&#20934;&#26041;&#27861;&#21644;&#19968;&#20010;&#20256;&#32479;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;S-DOTA&#22686;&#24378;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23383;&#30149;&#29702;&#22270;&#20687;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26377;&#26395;&#25913;&#21892;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#30340;&#24739;&#32773;&#32467;&#26524;&#65292;&#20294;&#26159;&#36825;&#20123;&#24037;&#20855;&#30340;&#27867;&#21270;&#24615;&#33021;&#30446;&#21069;&#21463;&#21040;&#32452;&#32455;&#26679;&#26412;&#21046;&#22791;&#12289;&#26579;&#33394;&#31243;&#24207;&#21644;&#25195;&#25551;&#35774;&#22791;&#24046;&#24322;&#30340;&#25935;&#24863;&#24615;&#38480;&#21046;&#65292;&#36825;&#20123;&#24046;&#24322;&#20250;&#23548;&#33268;&#25968;&#23383;&#20999;&#29255;&#20013;&#30340;&#22495;&#28418;&#31227;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#21512;&#25104; DOmain-Targeted Augmentation (S-DOTA) &#26041;&#27861;&#8212;&#8212;CycleGAN-enabled Scanner Transform (ST) &#21644; targeted Stain Vector Augmentation (SVA)&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#22269;&#38469;&#33394;&#24425;&#22996;&#21592;&#20250; (ICC) &#22522;&#20110;&#37197;&#32622;&#25991;&#20214;&#30340;&#39068;&#33394;&#26657;&#20934; (ICC Cal) &#26041;&#27861;&#21644;&#20351;&#29992;&#20256;&#32479;&#20142;&#24230;&#12289;&#39068;&#33394;&#21644;&#22122;&#22768;&#22686;&#24378;&#30340;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#25216;&#26415;&#25552;&#39640;&#27169;&#22411;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#35774;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#33021;&#21147;&#65306;&#21253;&#25324;&#22235;&#20010;&#27169;&#22411;&#12289;&#20004;&#31181;&#27169;&#22411;&#31867;&#22411;(&#32452;&#32455;&#20998;&#21106;&#21644;&#32454;&#32990;&#20998;&#31867;)&#12289;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#12289;&#20845;&#20010;&#23454;&#39564;&#23460;&#12289;&#20845;&#20010;&#25195;&#25551;&#20202;&#21644;&#19977;&#20010;&#25351;&#26631;(&#32925;&#30149;&#12289;&#32467;&#32928;&#30284;&#21644;&#30452;&#32928;&#30284;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have the potential to improve patient outcomes in digital pathology. However, generalization of these tools is currently limited by sensitivity to variations in tissue preparation, staining procedures and scanning equipment that lead to domain shift in digitized slides. To overcome this limitation and improve model generalization, we studied the effectiveness of two Synthetic DOmain-Targeted Augmentation (S-DOTA) methods, namely CycleGAN-enabled Scanner Transform (ST) and targeted Stain Vector Augmentation (SVA), and compared them against the International Color Consortium (ICC) profile-based color calibration (ICC Cal) method and a baseline method using traditional brightness, color and noise augmentations. We evaluated the ability of these techniques to improve model generalization to various tasks and settings: four models, two model types (tissue segmentation and cell classification), two loss functions, six labs, six scanners, and three indications (hep
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23545;&#20840;&#29699;&#26862;&#26519;&#37326;&#28779;&#39118;&#38505;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#21457;&#29616;2050&#24180;&#21040;2080&#24180;&#26399;&#38388;&#65292;&#20840;&#29699;&#26862;&#26519;&#30899;&#25269;&#28040;&#39033;&#30446;&#23558;&#38754;&#20020;&#26356;&#22823;&#30340;&#28779;&#28798;&#39118;&#38505;&#65292;&#20854;&#20013;&#28779;&#28798;&#30340;&#26292;&#38706;&#31243;&#24230;&#39044;&#35745;&#23558;&#22686;&#21152;55%[37-76%]&#12290;</title><link>http://arxiv.org/abs/2305.02397</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25581;&#31034;&#20840;&#29699;&#26862;&#26519;&#30899;&#25269;&#28040;&#39033;&#30446;&#26410;&#26469;&#28779;&#28798;&#39118;&#38505;&#26222;&#36941;&#19978;&#21319;
&lt;/p&gt;
&lt;p&gt;
Widespread Increases in Future Wildfire Risk to Global Forest Carbon Offset Projects Revealed by Explainable AI. (arXiv:2305.02397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02397
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23545;&#20840;&#29699;&#26862;&#26519;&#37326;&#28779;&#39118;&#38505;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#21457;&#29616;2050&#24180;&#21040;2080&#24180;&#26399;&#38388;&#65292;&#20840;&#29699;&#26862;&#26519;&#30899;&#25269;&#28040;&#39033;&#30446;&#23558;&#38754;&#20020;&#26356;&#22823;&#30340;&#28779;&#28798;&#39118;&#38505;&#65292;&#20854;&#20013;&#28779;&#28798;&#30340;&#26292;&#38706;&#31243;&#24230;&#39044;&#35745;&#23558;&#22686;&#21152;55%[37-76%]&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30899;&#25269;&#28040;&#35745;&#21010;&#22312;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#26862;&#26519;&#30899;&#25269;&#28040;&#39033;&#30446;&#38271;&#26399;&#31283;&#23450;&#21644;&#21487;&#34892;&#24615;&#38754;&#20020;&#30340;&#19968;&#20010;&#26032;&#23041;&#32961;&#26159;&#23665;&#28779;&#65292;&#21487;&#20197;&#37322;&#25918;&#22823;&#37327;&#30899;&#24182;&#38480;&#21046;&#30456;&#20851;&#25269;&#28040;&#31215;&#20998;&#30340;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#26862;&#26519;&#30899;&#39033;&#30446;&#30340;&#28779;&#28798;&#39118;&#38505;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#38271;&#26399;&#28779;&#28798;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27169;&#22411;&#65292;&#22522;&#20110;700&#19975;&#20840;&#29699;&#21355;&#26143;&#37326;&#28779;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#12289;&#22686;&#24378;&#20934;&#30830;&#24615;&#30340;&#20840;&#29699;&#37326;&#28779;&#39118;&#38505;&#39044;&#27979;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#19988;&#35813;&#27169;&#22411;&#20248;&#20110;&#32654;&#22269;&#22269;&#23478;&#22823;&#27668;&#30740;&#31350;&#20013;&#24515;&#30340;&#39046;&#20808;&#28779;&#28798;&#27169;&#22411;&#12290;&#24212;&#29992;&#20110;190&#20010;&#20840;&#29699;&#26862;&#26519;&#30899;&#39033;&#30446;&#30340;&#38598;&#21512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20013;&#31561;&#24773;&#26223;&#19979;&#65288;SSP2-4.5&#65289;&#65292;&#28779;&#28798;&#26292;&#38706;&#39044;&#35745;&#20250;&#22312;2080&#24180;&#19978;&#21319;55% [37-76%]&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20840;&#29699;&#26862;&#26519;&#30899;&#25269;&#28040;&#39033;&#30446;&#38754;&#20020;&#26356;&#22823;&#30340;&#28779;&#28798;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Carbon offset programs are critical in the fight against climate change. One emerging threat to the long-term stability and viability of forest carbon offset projects is wildfires, which can release large amounts of carbon and limit the efficacy of associated offsetting credits. However, analysis of wildfire risk to forest carbon projects is challenging because existing models for forecasting long-term fire risk are limited in predictive accuracy. Therefore, we propose an explainable artificial intelligence (XAI) model trained on 7 million global satellite wildfire observations. Validation results suggest substantial potential for high resolution, enhanced accuracy projections of global wildfire risk, and the model outperforms the U.S. National Center for Atmospheric Research's leading fire model. Applied to a collection of 190 global forest carbon projects, we find that fire exposure is projected to increase 55% [37-76%] by 2080 under a mid-range scenario (SSP2-4.5). Our results indic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#38477;&#20302;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#36798;&#21040;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02396</link><description>&lt;p&gt;
&#29305;&#24449;&#24037;&#31243;&#33021;&#24110;&#21161;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Feature Engineering Help Quantum Machine Learning for Malware Detection?. (arXiv:2305.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#38477;&#20302;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#36798;&#21040;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#25968;&#37327;&#21644;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#35768;&#22810;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#30340;&#27969;&#34892;ML&#27169;&#22411;&#37117;&#26159;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#20123;&#26377;&#30417;&#30563;&#20998;&#31867;&#22120;&#36890;&#24120;&#23545;&#26032;&#22411;&#24694;&#24847;&#36719;&#20214;&#30340;&#25512;&#24191;&#25928;&#26524;&#19981;&#22909;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#32463;&#24120;&#37325;&#26032;&#35757;&#32451;&#23427;&#20204;&#20197;&#26816;&#27979;&#26032;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#37327;&#23376;ML&#19982;&#29305;&#24449;&#36873;&#25321;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#38477;&#20302;&#25968;&#25454;&#22823;&#23567;&#21644;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#22521;&#35757;&#26102;&#38388;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;XGBoost&#36873;&#25321;&#30340;&#29305;&#24449;&#30340;VQC&#22312;&#27169;&#25311;&#22120;&#19978;&#21487;&#20197;&#33719;&#24471;78.91&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;XGBoost&#36873;&#25321;&#30340;&#29305;&#24449;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;IBM 5 qubits&#26426;&#22120;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#20026;74&#65285;&#65288;+-11.35&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing number and sophistication of malware attacks, malware detection systems based on machine learning (ML) grow in importance. At the same time, many popular ML models used in malware classification are supervised solutions. These supervised classifiers often do not generalize well to novel malware. Therefore, they need to be re-trained frequently to detect new malware specimens, which can be time-consuming. Our work addresses this problem in a hybrid framework of theoretical Quantum ML, combined with feature selection strategies to reduce the data size and malware classifier training time. The preliminary results show that VQC with XGBoost selected features can get a 78.91% test accuracy on the simulator. The average accuracy for the model trained using the features selected with XGBoost was 74% (+- 11.35%) on the IBM 5 qubits machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.02394</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#22240;&#30340;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#27169;&#24335;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#35757;&#32451;&#26399;&#38388;&#21521;&#27169;&#22411;&#28155;&#21152;&#21518;&#38376;&#26159;&#26377;&#25928;&#30340;&#12290;&#38450;&#24481;&#27492;&#31867;&#21518;&#38376;&#25915;&#20987;&#24050;&#21464;&#24471;&#32039;&#36843;&#21644;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AttDef&#30340;&#39640;&#25928;&#24402;&#22240;&#31649;&#36947;&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20855;&#26377;&#36739;&#22823;&#24402;&#22240;&#20998;&#25968;&#30340;&#20196;&#29260;&#35270;&#20026;&#28508;&#22312;&#35302;&#21457;&#22120;&#65292;&#22240;&#20026;&#36739;&#22823;&#30340;&#24402;&#22240;&#35789;&#23545;&#20110;&#38169;&#35823;&#39044;&#27979;&#32467;&#26524;&#20570;&#20986;&#36739;&#22823;&#36129;&#29486;&#65292;&#22240;&#27492;&#26356;&#26377;&#21487;&#33021;&#26159;&#27745;&#26579;&#35302;&#21457;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#22806;&#37096;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#21306;&#20998;&#36755;&#20837;&#26159;&#21542;&#34987;&#27745;&#26579;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#31181;&#24120;&#35265;&#30340;&#25915;&#20987;&#22330;&#26223;&#65288;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#65289;&#20013;&#20855;&#26377;&#36275;&#22815;&#30340;&#27867;&#21270;&#24615;&#65292;&#36825;&#19968;&#28857;&#25345;&#32493;&#25913;&#21892;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;AttDef&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#20004;&#31181;&#25915;&#20987;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;79.97%&#65288;&#25552;&#39640;&#20102;56.59%&#65289;&#21644;48.34%&#65288;&#25552;&#39640;&#20102;15.25%&#65289;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26799;&#24230;&#39044;&#27979;&#35299;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;&#26356;&#22909;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#22312;&#38543;&#26426;PCFG&#19979;&#35299;&#26512;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#21152;&#20837;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.02386</link><description>&lt;p&gt;
&#29992;Transformer&#36924;&#36817;CKY&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximating CKY with Transformers. (arXiv:2305.02386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26799;&#24230;&#39044;&#27979;&#35299;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;&#26356;&#22909;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#22312;&#38543;&#26426;PCFG&#19979;&#35299;&#26512;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#21152;&#20837;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#30452;&#25509;&#39044;&#27979;&#21477;&#23376;&#30340;&#35299;&#26512;&#65292;&#36991;&#20813;&#20102;CKY&#31639;&#27861;&#23545;&#21477;&#23376;&#38271;&#24230;&#30340;&#19977;&#27425;&#20381;&#36182;&#12290;&#22312;&#26631;&#20934;&#30340;&#32452;&#25104;&#21477;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#27604;&#20351;&#29992;CKY&#30340;&#21487;&#27604;&#20998;&#26512;&#22120;&#21462;&#24471;&#20102;&#31454;&#20105;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22312;&#38543;&#26426;PCFG&#19979;&#36827;&#34892;&#35299;&#26512;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35821;&#27861;&#21464;&#24471;&#26356;&#21152;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126;Transformer&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;CKY&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#65292;&#32467;&#21512;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20110;&#22270;&#34920;&#34920;&#31034;&#30340;&#26799;&#24230;&#26469;&#39044;&#27979;&#35299;&#26512;&#65292;&#31867;&#27604;&#20110;CKY&#31639;&#27861;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#19968;&#20010;&#20998;&#21306;&#20989;&#25968;&#21464;&#20307;&#30340;&#23376;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the ability of transformer models to approximate the CKY algorithm, using them to directly predict a parse and thus avoid the CKY algorithm's cubic dependence on sentence length. We find that on standard constituency parsing benchmarks this approach achieves competitive or better performance than comparable parsers that make use of CKY, while being faster. We also evaluate the viability of this approach for parsing under random PCFGs. Here we find that performance declines as the grammar becomes more ambiguous, suggesting that the transformer is not fully capturing the CKY computation. However, we also find that incorporating additional inductive bias is helpful, and we propose a novel approach that makes use of gradients with respect to chart representations in predicting the parse, in analogy with the CKY algorithm being the subgradient of a partition function variant with respect to the chart.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#38899;&#39057;&#34920;&#31034;&#22312;&#23569;&#26679;&#26412;&#22768;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#26032;&#39062;&#22768;&#23398;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#36890;&#29992;&#25928;&#29992;&#65292;&#35777;&#26126;&#20854;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#24182;&#21551;&#29992;&#20102;&#30456;&#24212;&#30340;&#23569;&#26679;&#26412;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.02382</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26816;&#27979;&#26032;&#39062;&#21644;&#32454;&#31890;&#24230;&#22768;&#23398;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Learning to Detect Novel and Fine-Grained Acoustic Sequences Using Pretrained Audio Representations. (arXiv:2305.02382v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#38899;&#39057;&#34920;&#31034;&#22312;&#23569;&#26679;&#26412;&#22768;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#26032;&#39062;&#22768;&#23398;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#36890;&#29992;&#25928;&#29992;&#65292;&#35777;&#26126;&#20854;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#24182;&#21551;&#29992;&#20102;&#30456;&#24212;&#30340;&#23569;&#26679;&#26412;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#38899;&#39057;&#34920;&#31034;&#22312;&#23569;&#26679;&#26412;&#22768;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#29305;&#21035;&#38024;&#23545;&#23569;&#26679;&#26412;&#26816;&#27979;&#26032;&#39062;&#22768;&#23398;&#24207;&#21015;&#30340;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#21363;&#22312;&#19981;&#20551;&#35774;&#38750;&#30446;&#26631;&#38899;&#39057;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20855;&#26377;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#26102;&#38388;&#32467;&#26500;&#30340;&#22768;&#20107;&#20214;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#39044;&#35757;&#32451;&#36866;&#24403;&#34920;&#31034;&#30340;&#36807;&#31243;&#65292;&#20197;&#21450;&#23558;&#23427;&#20204;&#36716;&#31227;&#21040;&#25105;&#20204;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#36890;&#29992;&#25928;&#29992;&#65292;&#20197;&#21450;&#36890;&#36807;&#30001;&#30495;&#23454;&#19990;&#30028;&#30340;&#22768;&#23398;&#24207;&#21015;&#26500;&#25104;&#30340;&#20219;&#21153;&#26469;&#25552;&#20986;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#36866;&#29992;&#20110;&#25152;&#25552;&#20986;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#21551;&#29992;&#25105;&#20204;&#30340;&#23569;&#26679;&#26412;&#26694;&#26550;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates pretrained audio representations for few shot Sound Event Detection. We specifically address the task of few shot detection of novel acoustic sequences, or sound events with semantically meaningful temporal structure, without assuming access to non-target audio. We develop procedures for pretraining suitable representations, and methods which transfer them to our few shot learning scenario. Our experiments evaluate the general purpose utility of our pretrained representations on AudioSet, and the utility of proposed few shot methods via tasks constructed from real-world acoustic sequences. Our pretrained embeddings are suitable to the proposed task, and enable multiple aspects of our few shot framework.
&lt;/p&gt;</description></item><item><title>MaskSearch&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#32034;&#24341;&#25216;&#26415;&#21644;&#39640;&#25928;&#30340;&#36807;&#28388;&#22120;&#39564;&#35777;&#26597;&#35810;&#25191;&#34892;&#26694;&#26550;&#65292;&#21152;&#36895;&#23545;&#22270;&#20687;&#25513;&#27169;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#65292;&#21487;&#20197;&#23558;&#20010;&#20307;&#26597;&#35810;&#21152;&#36895;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02375</link><description>&lt;p&gt;
MaskSearch&#65306;&#35268;&#27169;&#21270;&#26597;&#35810;&#22270;&#20687;&#25513;&#27169;
&lt;/p&gt;
&lt;p&gt;
MaskSearch: Querying Image Masks at Scale. (arXiv:2305.02375v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02375
&lt;/p&gt;
&lt;p&gt;
MaskSearch&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#32034;&#24341;&#25216;&#26415;&#21644;&#39640;&#25928;&#30340;&#36807;&#28388;&#22120;&#39564;&#35777;&#26597;&#35810;&#25191;&#34892;&#26694;&#26550;&#65292;&#21152;&#36895;&#23545;&#22270;&#20687;&#25513;&#27169;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#65292;&#21487;&#20197;&#23558;&#20010;&#20307;&#26597;&#35810;&#21152;&#36895;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#25968;&#25454;&#24211;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#20250;&#29983;&#25104;&#27880;&#37322;&#22270;&#20687;&#20869;&#23481;&#30340;&#25513;&#27169;&#65288;&#20363;&#22914;&#26174;&#30528;&#24615;&#22320;&#22270;&#65292;&#20998;&#21106;&#22320;&#22270;&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#65288;&#20363;&#22914;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#20102;&#34394;&#20551;&#20851;&#32852;&#65292;&#25110;&#32773;&#22270;&#20687;&#26159;&#21542;&#34987;&#24694;&#24847;&#20462;&#25913;&#20197;&#35823;&#23548;&#27169;&#22411;&#65289;&#12290;&#23613;&#31649;&#22522;&#20110;&#25513;&#27169;&#23646;&#24615;&#26816;&#32034;&#31034;&#20363;&#30340;&#26597;&#35810;&#23545;&#23454;&#36341;&#32773;&#24456;&#26377;&#20215;&#20540;&#65292;&#20294;&#29616;&#26377;&#31995;&#32479;&#26080;&#27861;&#39640;&#25928;&#22320;&#25903;&#25345;&#27492;&#31867;&#26597;&#35810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#30830;&#23450;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;MaskSearch&#65292;&#30528;&#37325;&#20110;&#21152;&#36895;&#23545;&#22270;&#20687;&#25513;&#27169;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#12290;MaskSearch&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32034;&#24341;&#25216;&#26415;&#21644;&#19968;&#31181;&#39640;&#25928;&#30340;&#36807;&#28388;&#22120;&#39564;&#35777;&#26597;&#35810;&#25191;&#34892;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21407;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#32034;&#24341;&#22823;&#23567;&#32422;&#20026;&#25968;&#25454;5%&#30340;MaskSearch&#21487;&#20197;&#23558;&#20010;&#20307;&#26597;&#35810;&#21152;&#36895;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#25506;&#32034;&#30340;&#21508;&#31181;&#22810;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#19978;&#22987;&#32456;&#34920;&#29616;&#20248;&#24322;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning tasks over image databases often generate masks that annotate image content (e.g., saliency maps, segmentation maps) and enable a variety of applications (e.g., determine if a model is learning spurious correlations or if an image was maliciously modified to mislead a model). While queries that retrieve examples based on mask properties are valuable to practitioners, existing systems do not support such queries efficiently. In this paper, we formalize the problem and propose a system, MaskSearch, that focuses on accelerating queries over databases of image masks. MaskSearch leverages a novel indexing technique and an efficient filter-verification query execution framework. Experiments on real-world datasets with our prototype show that MaskSearch, using indexes approximately 5% the size of the data, accelerates individual queries by up to two orders of magnitude and consistently outperforms existing methods on various multi-query workloads that simulate dataset explora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26694;&#26550;&#21644;&#26032;&#30340;&#32858;&#21512;&#25351;&#26631;&#65292;&#29992;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#24494;&#20998;&#27169;&#22411;&#12290;&#20854;&#20013;&#26032;&#30340;&#32858;&#21512;&#25351;&#26631;&#22522;&#20110;&#19977;&#31181;&#26041;&#27861;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.02368</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#24230;&#37327;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Metric Tools for Sensitivity Analysis with Applications to Neural Networks. (arXiv:2305.02368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26694;&#26550;&#21644;&#26032;&#30340;&#32858;&#21512;&#25351;&#26631;&#65292;&#29992;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#24494;&#20998;&#27169;&#22411;&#12290;&#20854;&#20013;&#26032;&#30340;&#32858;&#21512;&#25351;&#26631;&#22522;&#20110;&#19977;&#31181;&#26041;&#27861;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#32771;&#34385;&#29992;&#20110;&#25317;&#26377;&#37325;&#22823;&#31038;&#20250;&#24433;&#21709;&#30340;&#33258;&#20027;&#20915;&#31574;&#65292;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#24037;&#20316;&#30340;&#38656;&#27714;&#36805;&#36895;&#22686;&#38271;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#26088;&#22312;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#30340;&#39044;&#27979;&#25552;&#20379;&#35299;&#37322;&#65292;&#20197;&#20351;&#35813;&#27169;&#22411;&#23545;&#29992;&#25143;&#26356;&#20855;&#21487;&#20449;&#24230;&#21644;&#36879;&#26126;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20351;&#29992;&#24230;&#37327;&#25216;&#24039;&#26469;&#30740;&#31350;&#25935;&#24863;&#24230;&#20998;&#26512;&#12290;&#20174;&#36825;&#20010;&#24230;&#37327;&#26694;&#26550;&#24320;&#22987;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#32858;&#21512;&#25351;&#26631;&#65292;&#20197;&#26681;&#25454;&#19977;&#31181;&#26041;&#27861;&#20174;&#20559;&#23548;&#25968;&#20013;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65306;&#23616;&#37096;&#25200;&#21160;&#20998;&#26512;&#12289;&#20840;&#23616;&#25200;&#21160;&#20998;&#26512;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#36825;&#20123;&#25351;&#26631;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#24494;&#20998;&#27169;&#22411;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Machine Learning models are considered for autonomous decisions with significant social impact, the need for understanding how these models work rises rapidly. Explainable Artificial Intelligence (XAI) aims to provide interpretations for predictions made by Machine Learning models, in order to make the model trustworthy and more transparent for the user. For example, selecting relevant input variables for the problem directly impacts the model's ability to learn and make accurate predictions, so obtaining information about input importance play a crucial role when training the model. One of the main XAI techniques to obtain input variable importance is the sensitivity analysis based on partial derivatives. However, existing literature of this method provide no justification of the aggregation metrics used to retrieved information from the partial derivatives.  In this paper, a theoretical framework is proposed to study sensitivities of ML models using metric techniques. From this me
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;</title><link>http://arxiv.org/abs/2305.02350</link><description>&lt;p&gt;
&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using Language Models on Low-end Hardware. (arXiv:2305.02350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;CNN&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#32452;&#25104;&#20102;&#21253;&#25324;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#35805;&#39064;&#12289;&#24773;&#24863;&#21644;&#39118;&#26684;&#30340;8&#32452;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#24635;&#32467;&#25104;&#19968;&#20010;&#26435;&#34913;&#21015;&#34920;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper evaluates the viability of using fixed language models for training text classification networks on low-end hardware. We combine language models with a CNN architecture and put together a comprehensive benchmark with 8 datasets covering single-label and multi-label classification of topic, sentiment, and genre. Our observations are distilled into a list of trade-offs, concluding that there are scenarios, where not fine-tuning a language model yields competitive effectiveness at faster training, requiring only a quarter of the memory compared to fine-tuning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22330;&#35770;&#35745;&#31639;&#30340;&#22270;&#35299;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#21333;&#19968;&#26465;&#20214;&#20915;&#23450;&#20102;&#25152;&#26377;&#31070;&#32463;&#20803;&#39044;&#28608;&#27963;&#30340;&#20851;&#32852;&#20989;&#25968;&#30340;&#20020;&#30028;&#24615;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#25512;&#21160;&#28145;&#24230;&#23398;&#20064;&#21644;&#22330;&#35770;&#27169;&#25311;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.02334</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#29702;&#35770;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Structures of Neural Network Effective Theories. (arXiv:2305.02334v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22330;&#35770;&#35745;&#31639;&#30340;&#22270;&#35299;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#21333;&#19968;&#26465;&#20214;&#20915;&#23450;&#20102;&#25152;&#26377;&#31070;&#32463;&#20803;&#39044;&#28608;&#27963;&#30340;&#20851;&#32852;&#20989;&#25968;&#30340;&#20020;&#30028;&#24615;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#25512;&#21160;&#28145;&#24230;&#23398;&#20064;&#21644;&#22330;&#35770;&#27169;&#25311;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#29366;&#24577;&#19979;&#30340;&#26377;&#25928;&#22330;&#35770;&#65288;EFT&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26497;&#22823;&#22320;&#31616;&#21270;&#35745;&#31639;&#26377;&#38480;&#23485;&#24230;&#20462;&#27491;&#31070;&#32463;&#20803;&#32479;&#35745;&#37327;&#30340;&#36807;&#31243;&#12290;EFT&#35745;&#31639;&#30340;&#32467;&#26500;&#20351;&#24471;&#25152;&#26377;&#31070;&#32463;&#20803;&#39044;&#28608;&#27963;&#30340;&#20851;&#32852;&#20989;&#25968;&#30340;&#20020;&#30028;&#24615;&#37117;&#21463;&#21040;&#21333;&#19968;&#26465;&#20214;&#30340;&#25511;&#21046;&#12290;&#29702;&#35299;&#36825;&#26679;&#30340;EFT&#21487;&#33021;&#26377;&#21161;&#20110;&#36827;&#23637;&#28145;&#24230;&#23398;&#20064;&#21644;&#22330;&#35770;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a diagrammatic approach to effective field theories (EFTs) corresponding to deep neural networks at initialization, which dramatically simplifies computations of finite-width corrections to neuron statistics. The structures of EFT calculations make it transparent that a single condition governs criticality of all connected correlators of neuron preactivations. Understanding of such EFTs may facilitate progress in both deep learning and field theory simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#22810;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;CMDML&#65289;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22810;&#33021;&#28304;&#24322;&#24120;&#26816;&#27979;&#12290;CMDML &#33021;&#22815;&#32508;&#21512;&#19981;&#21516;&#33021;&#28304;&#28304;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23646;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02323</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#22810;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#22810;&#33021;&#28304;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Correlation-Driven Multi-Level Multimodal Learning for Anomaly Detection on Multiple Energy Sources. (arXiv:2305.02323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#22810;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;CMDML&#65289;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22810;&#33021;&#28304;&#24322;&#24120;&#26816;&#27979;&#12290;CMDML &#33021;&#22815;&#32508;&#21512;&#19981;&#21516;&#33021;&#28304;&#28304;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23646;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#35745;&#37327;&#22522;&#30784;&#35774;&#26045;&#65288;AMI&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#26234;&#33021;&#33021;&#28304;&#28040;&#36153;&#27979;&#37327;&#31995;&#32479;&#12290;&#30005;&#21147;&#26159;AMI&#21487;&#20197;&#25910;&#38598;&#30340;&#20195;&#34920;&#24615;&#33021;&#28304;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#27979;&#24322;&#24120;&#33021;&#28304;&#28040;&#32791;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#21333;&#20010;&#33021;&#28304;&#28304;&#19978;&#65292;&#21363;&#30005;&#21147;&#12290;&#26368;&#36817;&#65292;&#20854;&#20182;&#33021;&#28304;&#28304;&#22914;&#27700;&#12289;&#27668;&#21644;&#20379;&#26262;&#20063;&#34987;&#31215;&#26497;&#25910;&#38598;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#36328;&#22810;&#20010;&#33021;&#28304;&#28304;&#30340;&#24322;&#24120;&#26816;&#27979;&#32479;&#19968;&#26041;&#27861;;&#28982;&#32780;&#65292;&#30740;&#31350;&#23578;&#26410;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#22266;&#26377;&#22256;&#38590;&#22312;&#20110;&#24322;&#24120;&#36890;&#24120;&#27809;&#26377;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#24322;&#24120;&#23450;&#20041;&#24037;&#20316;&#20165;&#20381;&#36182;&#20110;&#20010;&#20307;&#33021;&#28304;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;&#33021;&#28304;&#28304;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#23450;&#20041;&#24322;&#24120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#22810;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;CMDML&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#22810;&#20010;&#33021;&#28304;&#28304;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CMDML&#38598;&#25104;&#20102;&#22810;&#20010;&#23618;&#27425;&#30340;&#29305;&#24449;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25429;&#25417;&#19981;&#21516;&#33021;&#28304;&#28304;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CMDML&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced metering infrastructure (AMI) has been widely used as an intelligent energy consumption measurement system. Electric power was the representative energy source that can be collected by AMI; most existing studies to detect abnormal energy consumption have focused on a single energy source, i.e., power. Recently, other energy sources such as water, gas, and heating have also been actively collected. As a result, it is necessary to develop a unified methodology for anomaly detection across multiple energy sources; however, research efforts have rarely been made to tackle this issue. The inherent difficulty with this issue stems from the fact that anomalies are not usually annotated. Moreover, existing works of anomaly definition depend on only individual energy sources. In this paper, we first propose a method for defining anomalies considering not only individual energy sources but also correlations between them. Then, we propose a new Correlation-driven Multi-Level Multimodal L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#23637;&#24179;&#27969;&#24418;&#30340;&#31639;&#27861;FlatNet&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01777</link><description>&lt;p&gt;
&#36890;&#36807;&#27969;&#24418;&#23637;&#24179;&#21644;&#37325;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning via Manifold Flattening and Reconstruction. (arXiv:2305.01777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#23637;&#24179;&#27969;&#24418;&#30340;&#31639;&#27861;FlatNet&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#27969;&#24418;&#30340;&#26377;&#38480;&#26679;&#26412;&#20013;&#26174;&#24335;&#26500;&#24314;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#32447;&#24615;&#21270;&#21644;&#37325;&#26500;&#23884;&#20837;&#23376;&#27969;&#24418;&#12290;&#25105;&#20204;&#25152;&#29983;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#31216;&#20026;&#23637;&#24179;&#32593;&#32476;&#65288;FlatNet&#65289;&#65292;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#35745;&#31639;&#19978;&#21487;&#25193;&#23637;&#24615;&#24378;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36825;&#31181;&#24179;&#34913;&#36890;&#24120;&#22312;&#22522;&#20110;&#27969;&#24418;&#30340;&#23398;&#20064;&#26041;&#27861;&#20013;&#38590;&#20197;&#23454;&#29616;&#12290;&#25105;&#20204;&#22522;&#20110;&#21512;&#25104;&#30340;&#39640;&#32500;&#27969;&#24418;&#25968;&#25454;&#21644;2D&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#65292;&#24182;&#19982;&#20854;&#20182;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes an algorithm for explicitly constructing a pair of neural networks that linearize and reconstruct an embedded submanifold, from finite samples of this manifold. Our such-generated neural networks, called flattening networks (FlatNet), are theoretically interpretable, computationally feasible at scale, and generalize well to test data, a balance not typically found in manifold-based learning methods. We present empirical results and comparisons to other models on synthetic high-dimensional manifold data and 2D image data. Our code is publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BrainNPT&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.01666</link><description>&lt;p&gt;
BrainNPT&#65306;&#29992;&#20110;&#33041;&#32593;&#32476;&#20998;&#31867;&#30340;Transformer&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BrainNPT: Pre-training of Transformer networks for brain network classification. (arXiv:2305.01666v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BrainNPT&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#33041;&#25104;&#20687;&#20998;&#26512;&#26041;&#38754;&#30340;&#36827;&#23637;&#36805;&#36895;&#65292;&#20294;&#24448;&#24448;&#21463;&#21040;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#26223;&#30340;&#29305;&#24449;&#23398;&#20064;&#25913;&#36827;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#22312;&#33041;&#32593;&#32476;&#20998;&#26512;&#20013;&#65292;&#36825;&#31181;&#25216;&#26415;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;Transformer&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20026;&#37325;&#28857;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21517;&#20026;BrainNPT&#65292;&#29992;&#20110;&#33041;&#21151;&#33021;&#32593;&#32476;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&lt;cls&gt;&#26631;&#35760;&#20316;&#20026;&#20998;&#31867;&#23884;&#20837;&#21521;&#37327;&#65292;&#20197;&#20415;&#20110;Transformer&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#33719;&#33041;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#39044;&#35757;&#32451;&#26550;&#26500;&#65292;&#29992;&#20110;BrainNPT&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#26469;&#23398;&#20064;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have advanced quickly in brain imaging analysis over the past few years, but they are usually restricted by the limited labeled data. Pre-trained model on unlabeled data has presented promising improvement in feature learning in many domains, including natural language processing and computer vision. However, this technique is under-explored in brain network analysis. In this paper, we focused on pre-training methods with Transformer networks to leverage existing unlabeled data for brain functional network classification. First, we proposed a Transformer-based neural network, named as BrainNPT, for brain functional network classification. The proposed method leveraged &lt;cls&gt; token as a classification embedding vector for the Transformer model to effectively capture the representation of brain network. Second, We proposed a pre-training architecture with two pre-training strategies for BrainNPT model to leverage unlabeled brain network data to learn the structure in
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.01649</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalizing Dataset Distillation via Deep Generative Prior. (arXiv:2305.01649v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26088;&#22312;&#23558;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#20960;&#20010;&#21512;&#25104;&#22270;&#20687;&#20013;&#12290;&#20854;&#24605;&#24819;&#26159;&#21512;&#25104;&#23569;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#28857;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#32473;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#36924;&#36817;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#24182;&#25193;&#23637;&#21040;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#26469;&#21512;&#25104;&#33976;&#39311;&#30340;&#25968;&#25454;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation aims to distill an entire dataset's knowledge into a few synthetic images. The idea is to synthesize a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite recent progress in the field, existing dataset distillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pre-trained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model's latent space. Our method augments existing techniques, significantly improving cross-architecture generalization in all settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01322</link><description>&lt;p&gt;
&#22522;&#20110;Option&#26694;&#26550;&#30340;&#22810;&#27169;&#24335;&#25506;&#32034;&#33258;&#20027;&#38750;&#21333;&#20307;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework. (arXiv:2305.01322v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25506;&#32034;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#8220;&#22914;&#20309;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#32780;&#8220;&#20309;&#26102;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#30740;&#31350;&#19968;&#30452;&#27809;&#26377;&#25104;&#20026;&#37325;&#28857;&#12290;&#20856;&#22411;&#30340;&#25506;&#32034;&#34892;&#20026;&#36890;&#24120;&#23558;&#25506;&#32034;&#34892;&#20026;&#19982;&#26234;&#33021;&#20307;&#30340;&#24320;&#21457;&#21033;&#29992;&#34892;&#20026;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#38750;&#21333;&#20307;&#25506;&#32034;&#34892;&#20026;&#30340;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#27169;&#24335;&#20999;&#25442;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#30340;&#26368;&#32456;&#30446;&#30340;&#26159;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#20027;&#20915;&#23450;&#20309;&#26102;&#25506;&#32034;&#25110;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;Option&#26694;&#26550;&#20013;&#25551;&#36848;&#20102;&#33258;&#20027;&#22810;&#27169;&#24335;&#25506;&#32034;&#30340;&#21021;&#22987;&#30740;&#31350;&#12290;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#30340;&#26356;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most exploration research on reinforcement learning (RL) has paid attention to `the way of exploration', which is `how to explore'. The other exploration research, `when to explore', has not been the main focus of RL exploration research. \textcolor{black}{The issue of `when' of a monolithic exploration in the usual RL exploration behaviour binds an exploratory action to an exploitational action of an agent. Recently, a non-monolithic exploration research has emerged to examine the mode-switching exploration behaviour of humans and animals.} The ultimate purpose of our research is to enable an agent to decide when to explore or exploit autonomously. We describe the initial research of an autonomous multi-mode exploration of non-monolithic behaviour in an options framework. The higher performance of our method is shown against the existing non-monolithic exploration method through comparative experimental results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19987;&#19994;&#30693;&#35782;&#26641;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38598;&#20307;&#20915;&#31574;&#20013;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01063</link><description>&lt;p&gt;
&#19987;&#19994;&#30693;&#35782;&#26641;&#22312;&#38598;&#20307;&#20915;&#31574;&#20013;&#35299;&#20915;&#30693;&#35782;&#23616;&#38480;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Expertise Trees Resolve Knowledge Limitations in Collective Decision-Making. (arXiv:2305.01063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19987;&#19994;&#30693;&#35782;&#26641;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38598;&#20307;&#20915;&#31574;&#20013;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#20915;&#31574;&#32773;&#25552;&#20379;&#24314;&#35758;&#30340;&#19987;&#23478;&#24448;&#24448;&#20250;&#26174;&#31034;&#20986;&#38543;&#38382;&#39064;&#23454;&#20363;&#21464;&#21270;&#32780;&#21464;&#21270;&#30340;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38024;&#23545;&#23569;&#25968;&#24773;&#20917;&#30340;&#27425;&#20248;&#25110;&#27495;&#35270;&#24615;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#30693;&#35782;&#28145;&#24230;&#21644;&#24191;&#24230;&#30340;&#21464;&#21270;&#24314;&#27169;&#20026;&#23558;&#38382;&#39064;&#31354;&#38388;&#21010;&#20998;&#20026;&#19981;&#21516;&#19987;&#19994;&#30693;&#35782;&#21306;&#22495;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#26032;&#31639;&#27861;&#65292;&#23427;&#20204;&#26126;&#30830;&#32771;&#34385;&#24182;&#36866;&#24212;&#38382;&#39064;&#23454;&#20363;&#19982;&#19987;&#23478;&#30693;&#35782;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#24182;&#24378;&#35843;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#26597;&#35810;&#30340;&#22825;&#30495;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19987;&#19994;&#30693;&#35782;&#26641;&#65292;&#23427;&#26500;&#24314;&#20915;&#31574;&#26641;&#65292;&#20351;&#23398;&#20064;&#32773;&#33021;&#22815;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#29616;&#26377;&#26041;&#27861;&#34987;&#35777;&#26126;&#19981;&#36275;&#20197;&#35299;&#20915;&#38382;&#39064;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experts advising decision-makers are likely to display expertise which varies as a function of the problem instance. In practice, this may lead to sub-optimal or discriminatory decisions against minority cases. In this work we model such changes in depth and breadth of knowledge as a partitioning of the problem space into regions of differing expertise. We provide here new algorithms that explicitly consider and adapt to the relationship between problem instances and experts' knowledge. We first propose and highlight the drawbacks of a naive approach based on nearest neighbor queries. To address these drawbacks we then introduce a novel algorithm - expertise trees - that constructs decision trees enabling the learner to select appropriate models. We provide theoretical insights and empirically validate the improved performance of our novel approach on a range of problems for which existing methods proved to be inadequate.
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22495;&#22686;&#37327;3D&#29289;&#20307;&#26816;&#27979;&#65292;GMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#22823;&#24178;&#25200;&#24674;&#22797;&#31574;&#30053;&#65292;&#21487;&#22312;&#24494;&#35843;&#26102;&#23450;&#26399;&#20174;&#20197;&#21069;&#30340;&#39046;&#22495;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.14460</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#22823;&#24178;&#25200;&#24674;&#22797;&#30340;&#22495;&#22686;&#37327;3D&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Gradient-based Maximally Interfered Retrieval for Domain Incremental 3D Object Detection. (arXiv:2304.14460v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14460
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22495;&#22686;&#37327;3D&#29289;&#20307;&#26816;&#27979;&#65292;GMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#22823;&#24178;&#25200;&#24674;&#22797;&#31574;&#30053;&#65292;&#21487;&#22312;&#24494;&#35843;&#26102;&#23450;&#26399;&#20174;&#20197;&#21069;&#30340;&#39046;&#22495;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25152;&#26377;&#22825;&#27668;&#26465;&#20214;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#20173;&#28982;&#26159;&#23454;&#29616;&#33258;&#20027;&#36710;&#36742;&#24191;&#27867;&#37096;&#32626;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#22240;&#20026;&#30446;&#21069;&#22823;&#37096;&#20998;&#24037;&#20316;&#26159;&#22312;&#28165;&#26224;&#30340;&#22825;&#27668;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#25512;&#24191;&#21040;&#36870;&#22659;&#22825;&#27668;&#26465;&#20214;&#65292;&#30417;&#30563;&#26041;&#27861;&#22312;&#25152;&#26377;&#22825;&#27668;&#25968;&#25454;&#19978;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#34920;&#29616;&#24471;&#26368;&#22909;&#65292;&#32780;&#19981;&#26159;&#22312;&#28165;&#26224;&#22825;&#27668;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#20174;&#25152;&#26377;&#25968;&#25454;&#24320;&#22987;&#35757;&#32451;&#26368;&#32456;&#20250;&#22240;&#25968;&#25454;&#38598;&#19981;&#26029;&#22686;&#38271;&#24182;&#21253;&#21547;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#22825;&#27668;&#26465;&#20214;&#32780;&#21464;&#24471;&#19981;&#21487;&#34892;&#21644;&#26114;&#36149;&#12290;&#32780;&#22312;&#19981;&#21516;&#22825;&#27668;&#39046;&#22495;&#30340;&#21407;&#22987;&#25968;&#25454;&#19978;&#36827;&#34892;&#31616;&#21333;&#30340;&#24494;&#35843;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#20808;&#21069;&#23398;&#20064;&#39046;&#22495;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#21463;&#22238;&#25918;&#24335;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#22823;&#24178;&#25200;&#24674;&#22797;&#65288;GMIR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#22238;&#25918;&#36827;&#34892;&#26799;&#24230;&#37319;&#26679;&#30340;&#31574;&#30053;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;GMIR&#20250;&#23450;&#26399;&#20174;&#20197;&#21069;&#30340;&#39046;&#22495;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D object detection in all weather conditions remains a key challenge to enable the widespread deployment of autonomous vehicles, as most work to date has been performed on clear weather data. In order to generalize to adverse weather conditions, supervised methods perform best if trained from scratch on all weather data instead of finetuning a model pretrained on clear weather data. Training from scratch on all data will eventually become computationally infeasible and expensive as datasets continue to grow and encompass the full extent of possible weather conditions. On the other hand, naive finetuning on data from a different weather domain can result in catastrophic forgetting of the previously learned domain. Inspired by the success of replay-based continual learning methods, we propose Gradient-based Maximally Interfered Retrieval (GMIR), a gradient based sampling strategy for replay. During finetuning, GMIR periodically retrieves samples from the previous domain dataset
&lt;/p&gt;</description></item><item><title>DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2304.14108</link><description>&lt;p&gt;
DataComp&#65306;&#23547;&#25214;&#19979;&#19968;&#20195;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14108
&lt;/p&gt;
&lt;p&gt;
DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22312;&#36817;&#26399;&#30340;&#31361;&#30772;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#27604;&#22914;CLIP&#12289;Stable Diffusion&#21644;GPT-4&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25968;&#25454;&#38598;&#24456;&#23569;&#24471;&#21040;&#19982;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31639;&#27861;&#21516;&#31561;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataComp&#65292;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#35757;&#32451;&#20195;&#30721;&#26159;&#22266;&#23450;&#30340;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Common Crawl&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#20854;&#20013;&#21253;&#21547;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#12290;&#21442;&#21152;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#65292;&#24182;&#36890;&#36807;&#36816;&#34892;&#25105;&#20204;&#26631;&#20934;&#21270;&#30340;CLIP&#35757;&#32451;&#20195;&#30721;&#24182;&#22312;38&#20010;&#19979;&#28216;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#22810;&#20010;&#35268;&#27169;&#65292;&#22235;&#20010;&#20505;&#36873;&#27744;&#22823;&#23567;&#21644;&#30456;&#24212;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#28085;&#30422;&#20102;&#20174;12.8M&#21040;12.8B&#20010;&#26679;&#26412;&#12290;&#36825;&#31181;&#22810;&#35268;&#27169;&#35774;&#35745;&#26377;&#21161;&#20110;&#30740;&#31350;&#35268;&#27169;&#36235;&#21183;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#36873;&#25321;&#20313;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#20248;&#21270;&#21518;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36712;&#36857;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#38598;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12579</link><description>&lt;p&gt;
&#23398;&#20064;&#36712;&#36857;&#26159;&#27867;&#21270;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Trajectories are Generalization Indicators. (arXiv:2304.12579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#20248;&#21270;&#21518;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36712;&#36857;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#38598;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20248;&#21270;&#26102;&#23545;&#24212;&#30340;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#32447;&#24615;&#36817;&#20284;&#20989;&#25968;&#26469;&#27169;&#25311;&#36712;&#36857;&#20449;&#24687;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26356;&#20016;&#23500;&#36712;&#36857;&#20449;&#24687;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27867;&#21270;&#19978;&#30028;&#20381;&#36182;&#20110;&#23398;&#20064;&#36712;&#36857;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#35757;&#32451;&#38598;&#30340;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#20043;&#38388;&#30340;&#27604;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#19981;&#21516;&#35757;&#32451;&#27493;&#39588;&#12289;&#23398;&#20064;&#29575;&#21644;&#26631;&#31614;&#22122;&#22768;&#27700;&#24179;&#19979;&#30340;&#27867;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to investigate the connection between learning trajectories of the Deep Neural Networks (DNNs) and their corresponding generalization capabilities when being optimized with broadly used gradient descent and stochastic gradient descent algorithms. In this paper, we construct Linear Approximation Function to model the trajectory information and we propose a new generalization bound with richer trajectory information based on it. Our proposed generalization bound relies on the complexity of learning trajectory and the ratio between the bias and diversity of training set. Experimental results indicate that the proposed method effectively captures the generalization trend across various training steps, learning rates, and label noise levels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.09355</link><description>&lt;p&gt;
&#21387;&#32553;&#19982;&#21542;&#8212;&#8212;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#20449;&#24687;&#35770;:&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#33539;&#20363;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#26126;&#30830;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#12290;&#20449;&#24687;&#35770;&#22312;&#29702;&#35299;&#21644;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#34987;&#24212;&#29992;&#20110;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#20248;&#21270;&#21387;&#32553;&#21644;&#30456;&#20851;&#20449;&#24687;&#20445;&#23384;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26368;&#20339;&#20449;&#24687;&#30446;&#26631;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30740;&#31350;&#34701;&#21512;&#25104;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#30740;&#31350;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have demonstrated remarkable performance in supervised learning tasks but require large amounts of labeled data. Self-supervised learning offers an alternative paradigm, enabling the model to learn from data without explicit labels. Information theory has been instrumental in understanding and optimizing deep neural networks. Specifically, the information bottleneck principle has been applied to optimize the trade-off between compression and relevant information preservation in supervised settings. However, the optimal information objective in self-supervised learning remains unclear. In this paper, we review various approaches to self-supervised learning from an information-theoretic standpoint and present a unified framework that formalizes the \textit{self-supervised information-theoretic learning problem}. We integrate existing research into a coherent framework, examine recent self-supervised methods, and identify research opportunities and challenges. Moreove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.07056</link><description>&lt;p&gt;
&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#24863;&#30693;&#36136;&#37327;&#35780;&#20272;&#65306;&#22522;&#20934;&#21644;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method. (arXiv:2304.07056v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#38656;&#27714;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#25104;&#21151;&#20351;&#24471;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#28151;&#21512;&#35270;&#39057;&#32534;&#30721;&#33539;&#22260;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#31354;&#38388;&#21644;&#26102;&#38388;&#22495;&#20013;&#25197;&#26354;&#31867;&#22411;&#30340;&#26497;&#22823;&#22810;&#26679;&#24615;&#65292;&#20174;&#20256;&#32479;&#30340;&#28151;&#21512;&#32534;&#30721;&#26694;&#26550;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#32473;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;VQA&#65289;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#36825;&#26159;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#12290;&#35813;&#25968;&#25454;&#24211;&#21253;&#21547; 3,240 &#20010;&#21387;&#32553;&#30340;&#38754;&#37096;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#22810;&#20010;&#21387;&#32553;&#32423;&#21035;&#65292;&#36825;&#20123;&#29255;&#27573;&#26469;&#33258; 135 &#20010;&#28304;&#35270;&#39057;&#65292;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an exponential increase in the demand for face video compression, and the success of artificial intelligence has expanded the boundaries beyond traditional hybrid video coding. Generative coding approaches have been identified as promising alternatives with reasonable perceptual rate-distortion trade-offs, leveraging the statistical priors of face videos. However, the great diversity of distortion types in spatial and temporal domains, ranging from the traditional hybrid coding frameworks to generative models, present grand challenges in compressed face video quality assessment (VQA). In this paper, we introduce the large-scale Compressed Face Video Quality Assessment (CFVQA) database, which is the first attempt to systematically understand the perceptual quality and diversified compression distortions in face videos. The database contains 3,240 compressed face video clips in multiple compression levels, which are derived from 135 source videos with diversif
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.04099</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#23884;&#20837;&#20174;&#36830;&#32493;&#26032;&#38395;&#27969;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#23454;&#26102;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#25925;&#20107;&#65292;&#26377;&#21161;&#20110;&#20154;&#20204;&#22312;&#19981;&#38656;&#35201;&#26114;&#36149;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30740;&#31350;&#30340;&#26222;&#36941;&#26041;&#27861;&#26159;&#29992;&#31526;&#21495;&#25110;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#23558;&#23427;&#20204;&#36880;&#27493;&#32858;&#31867;&#25104;&#25925;&#20107;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#36827;&#19968;&#27493;&#25913;&#21892;&#23884;&#20837;&#65292;&#20294;&#26159;&#36890;&#36807;&#26080;&#24046;&#21035;&#22320;&#32534;&#30721;&#25991;&#31456;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#26469;&#30452;&#25509;&#37319;&#29992;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;&#23500;&#21547;&#25991;&#26412;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#26032;&#38395;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#21477;&#23376;&#32534;&#30721;&#22120;&#26469;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30340;&#24819;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#26694;&#26550;USTORY&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#65292;&#21363;&#20027;&#39064;&#21644;&#26102;&#38388;&#24863;&#30693;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26032;&#39062;&#24615;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22270;&#35770;&#30340;&#35821;&#35328;&#21051;&#30011;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20004;&#31181;&#24773;&#24418;&#19979;&#65292;&#32431;&#31929;&#21644;&#36817;&#20284;&#30340;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#30683;&#30462;&#22270;$G$&#26469;&#25429;&#25417; $\mathcal{H}$ &#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#21457;&#29616;&#20998;&#25968;&#22242;&#25968;&#21644;&#22242;&#25968;&#26159;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#23545;&#20854;&#36827;&#34892;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.03996</link><description>&lt;p&gt;
&#29992;&#22270;&#35770;&#32479;&#19968;&#21051;&#30011;&#24046;&#20998;&#38544;&#31169;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Unified Characterization of Private Learnability via Graph Theory. (arXiv:2304.03996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22270;&#35770;&#30340;&#35821;&#35328;&#21051;&#30011;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20004;&#31181;&#24773;&#24418;&#19979;&#65292;&#32431;&#31929;&#21644;&#36817;&#20284;&#30340;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#30683;&#30462;&#22270;$G$&#26469;&#25429;&#25417; $\mathcal{H}$ &#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#21457;&#29616;&#20998;&#25968;&#22242;&#25968;&#21644;&#22242;&#25968;&#26159;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#23545;&#20854;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#21051;&#30011;&#32431;&#31929;&#30340;&#21644;&#36817;&#20284;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22270;&#35770;&#30340;&#35821;&#35328;:&#23545;&#20110;&#19968;&#20010;&#27010;&#24565;&#31867; $\mathcal{H}$,&#25105;&#20204;&#23450;&#20041;&#20102; $\mathcal{H}$ &#30340;&#30683;&#30462;&#22270; $G$&#12290;&#23427;&#30340;&#39030;&#28857;&#26159;&#21487;&#23454;&#29616;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;&#26524;&#20004;&#20010;&#25968;&#25454;&#38598; $S$&#65292;$S'$ &#30456;&#20114;&#30683;&#30462;(&#21363;&#65292;&#22312; $S$ &#21644; $S'$ &#20013;&#26377;&#19968;&#20010;&#28857; $x$ &#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#35760;)&#65292;&#21017;&#23427;&#20204;&#20043;&#38388;&#26377;&#19968;&#26465;&#36793;&#36830;&#25509;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;$G$ &#30340;&#32452;&#21512;&#32467;&#26500;&#19982;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#32431;&#31929;&#30340;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#30340;&#25429;&#33719;&#20026; $G$ &#30340;&#20998;&#25968;&#22242;&#25968;&#12290;&#22312;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#30340;&#25429;&#33719;&#20026; $G$ &#30340;&#22242;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#21487;&#23398;&#20064;&#24615;&#30340;&#22270;&#35770;&#32500;&#24230;&#65306;&#22242;&#32500;&#21644;&#20998;&#25968;&#22242;&#32500;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#30683;&#30462;&#22270;&#30340;&#19968;&#20123;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#26469;&#20272;&#35745; $G$ &#30340;&#36825;&#20123;&#24230;&#37327;&#65292;&#36890;&#36807;&#36825;&#20123;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20960;&#31181;&#27010;&#24565;&#31867;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a unified framework for characterizing pure and approximate differentially private (DP) learnabiliity. The framework uses the language of graph theory: for a concept class $\mathcal{H}$, we define the contradiction graph $G$ of $\mathcal{H}$. It vertices are realizable datasets, and two datasets $S,S'$ are connected by an edge if they contradict each other (i.e., there is a point $x$ that is labeled differently in $S$ and $S'$). Our main finding is that the combinatorial structure of $G$ is deeply related to learning $\mathcal{H}$ under DP. Learning $\mathcal{H}$ under pure DP is captured by the fractional clique number of $G$. Learning $\mathcal{H}$ under approximate DP is captured by the clique number of $G$. Consequently, we identify graph-theoretic dimensions that characterize DP learnability: the clique dimension and fractional clique dimension. Along the way, we reveal properties of the contradiction graph which may be of independent interest. We also suggest several o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#32447;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#65292;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#21644;&#25104;&#24180;&#23398;&#20064;&#32773;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#23545;&#23398;&#20064;&#20869;&#23481;&#33719;&#21462;&#21644;&#23398;&#20064;&#20307;&#39564;&#26377;&#30528;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.03784</link><description>&lt;p&gt;
&#29983;&#25104;AI&#29992;&#20110;&#23398;&#20064;&#65306;&#30740;&#31350;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI for learning: Investigating the potential of synthetic learning videos. (arXiv:2304.03784v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#32447;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#65292;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#21644;&#25104;&#24180;&#23398;&#20064;&#32773;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21512;&#25104;&#23398;&#20064;&#35270;&#39057;&#23545;&#23398;&#20064;&#20869;&#23481;&#33719;&#21462;&#21644;&#23398;&#20064;&#20307;&#39564;&#26377;&#30528;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#24341;&#36215;&#20102;&#20840;&#29699;&#30340;&#20851;&#27880;&#12290;&#20687;Dalle-2&#21644;ChatGPT&#36825;&#26679;&#30340;&#24037;&#20855;&#34920;&#26126;&#65292;&#20197;&#21069;&#34987;&#35748;&#20026;&#36229;&#20986;&#20102;AI&#33021;&#21147;&#30340;&#20219;&#21153;&#29616;&#22312;&#21487;&#20197;&#20197;&#21508;&#31181;&#26032;&#26041;&#24335;&#22686;&#21152;&#21019;&#24847;&#23186;&#20307;&#30340;&#29983;&#20135;&#21147;&#65292;&#21253;&#25324;&#29983;&#25104;&#21512;&#25104;&#35270;&#39057;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#21512;&#25104;&#35270;&#39057;&#26469;&#21019;&#24314;&#22312;&#22312;&#32447;&#25945;&#32946;&#29615;&#22659;&#19979;&#21487;&#34892;&#30340;&#25945;&#32946;&#20869;&#23481;&#30340;&#25928;&#29992;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#30740;&#31350;&#35843;&#26597;AI&#29983;&#25104;&#30340;&#21512;&#25104;&#23186;&#20307;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25945;&#32946;&#20215;&#20540;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#38543;&#26426;&#23558;&#25104;&#24180;&#23398;&#20064;&#32773;&#65288;n = 83&#65289;&#20998;&#37197;&#21040;&#20004;&#31181;&#24494;&#22411;&#23398;&#20064;&#26465;&#20214;&#20043;&#19968;&#65292;&#25910;&#38598;&#21069;&#21518;&#23398;&#20064;&#35780;&#20272;&#65292;&#24182;&#35843;&#26597;&#21442;&#19982;&#32773;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#25511;&#21046;&#32452;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative artificial intelligence (AI) have captured worldwide attention. Tools such as Dalle-2 and ChatGPT suggest that tasks previously thought to be beyond the capabilities of AI may now augment the productivity of creative media in various new ways, including through the generation of synthetic video. This research paper explores the utility of using AI-generated synthetic video to create viable educational content for online educational settings. To date, there is limited research investigating the real-world educational value of AI-generated synthetic media. To address this gap, we examined the impact of using AI-generated synthetic video in an online learning platform on both learners content acquisition and learning experience. We took a mixed-method approach, randomly assigning adult learners (n=83) into one of two micro-learning conditions, collecting pre- and post-learning assessments, and surveying participants on their learning experience. The control c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;</title><link>http://arxiv.org/abs/2303.12743</link><description>&lt;p&gt;
DR.CPO&#65306;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#12289;&#38543;&#26426;&#25918;&#32622;&#21644; HPR &#36974;&#34109;&#23454;&#29616;&#30340;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#19977;&#32500;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#24120;&#29992;&#20110;&#25913;&#36827;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#26368;&#22522;&#26412;&#30340;&#26041;&#27861;&#21253;&#25324;&#25554;&#20837;&#22797;&#21046;&#23545;&#35937;&#21644;&#26059;&#36716;&#21644;&#32553;&#25918;&#25972;&#20010;&#35757;&#32451;&#24103;&#12290;&#20063;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#21464;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21487;&#33021;&#24615;&#30456;&#27604;&#30456;&#24403;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#26500;&#36896;&#25972;&#20307;&#23545;&#35937;&#65292;&#33258;&#30001;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#20026;&#20102;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20174;&#29616;&#23454;&#19990;&#30028;&#35266;&#23519;&#21040;&#30340;&#22810;&#20010;&#23545;&#35937;&#38543;&#26426;&#32452;&#21512;&#25104;&#21333;&#20010;&#23545;&#35937;&#12290;&#19982;&#29616;&#26377;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#22312;&#35757;&#32451;&#24103;&#20013;&#65292;&#22240;&#20026;&#36866;&#24403;&#30340;&#36974;&#25377;&#21487;&#20197;&#21453;&#26144;&#22312;&#26368;&#32456;&#25972;&#20307;&#23545;&#35937;&#20013;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#38450;&#27490;&#36807;&#24230;&#22686;&#24378;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#23618;&#36974;&#25377;&#27010;&#29575;&#35774;&#32622;&#65292;&#36890;&#36807;&#23545;&#35937;&#30340;&#20301;&#32622;&#21644;&#22823;&#23567;&#35843;&#25972;&#36974;&#25377;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
&lt;/p&gt;</description></item><item><title>DR-VIDAL&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24178;&#39044;&#25514;&#26045;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#22788;&#29702;&#28151;&#28102;&#20559;&#24046;&#21644;&#27169;&#22411;&#19981;&#33391;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.04201</link><description>&lt;p&gt;
DR-VIDAL--&#21452;&#37325;&#31283;&#20581;&#21464;&#20998;&#20449;&#24687;&#35770;&#28145;&#24230;&#23545;&#25239;&#23398;&#20064;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21453;&#20107;&#23454;&#39044;&#27979;&#21644;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation on Real World Data. (arXiv:2303.04201v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04201
&lt;/p&gt;
&lt;p&gt;
DR-VIDAL&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24178;&#39044;&#25514;&#26045;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#22788;&#29702;&#28151;&#28102;&#20559;&#24046;&#21644;&#27169;&#22411;&#19981;&#33391;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30495;&#23454;&#19990;&#30028;&#30340;&#35266;&#23519;&#24615;&#65288;&#38750;&#38543;&#26426;&#21270;&#65289;&#25968;&#25454;&#20013;&#30830;&#23450;&#24178;&#39044;&#25514;&#26045;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#20363;&#22914;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#27835;&#30103;&#37325;&#29992;&#65292;&#30001;&#20110;&#28508;&#22312;&#20559;&#24046;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25913;&#36827;&#20102;&#20256;&#32479;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#20010;&#24615;&#21270;&#27835;&#30103;&#25928;&#26524;&#65288;ITE&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#37325;&#31283;&#20581;&#21464;&#20998;&#20449;&#24687;&#35770;&#28145;&#24230;&#23545;&#25239;&#23398;&#20064;&#65288;DR-VIDAL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#27835;&#30103;&#21644;&#32467;&#26524;&#20004;&#20010;&#32852;&#21512;&#27169;&#22411;&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#30830;&#20445;&#26080;&#20559;&#30340;ITE&#20272;&#35745;&#65292;&#21363;&#20351;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#35774;&#23450;&#19981;&#27491;&#30830;&#12290;DR-VIDAL&#25972;&#21512;&#20102;&#65306; &#65288;i&#65289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26681;&#25454;&#22240;&#26524;&#20551;&#35774;&#23558;&#28151;&#28102;&#21464;&#37327;&#20998;&#35299;&#20026;&#28508;&#22312;&#21464;&#37327;; &#65288;ii&#65289;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;Info-GAN&#65289;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#24773;&#20917;; &#65288;iii&#65289;&#19968;&#20010;&#21452;&#37325;&#31283;&#20581;&#22359;&#65292;&#20854;&#20013;&#21253;&#25324;&#27835;&#30103;&#20542;&#21521;&#20110;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65288;Infant Health&#21644;Development Program&#65292;Transforming Clinical Practice Initiative [TCPI]&#65289;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DR-VIDAL&#22312;&#20272;&#35745;ITE&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#22788;&#29702;&#28151;&#28102;&#20559;&#24046;&#21644;&#27169;&#22411;&#19981;&#27491;&#30830;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining causal effects of interventions onto outcomes from real-world, observational (non-randomized) data, e.g., treatment repurposing using electronic health records, is challenging due to underlying bias. Causal deep learning has improved over traditional techniques for estimating individualized treatment effects (ITE). We present the Doubly Robust Variational Information-theoretic Deep Adversarial Learning (DR-VIDAL), a novel generative framework that combines two joint models of treatment and outcome, ensuring an unbiased ITE estimation even when one of the two is misspecified. DR-VIDAL integrates: (i) a variational autoencoder (VAE) to factorize confounders into latent variables according to causal assumptions; (ii) an information-theoretic generative adversarial network (Info-GAN) to generate counterfactuals; (iii) a doubly robust block incorporating treatment propensities for outcome predictions. On synthetic and real-world datasets (Infant Health and Development Program, T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30149;&#29702;&#26816;&#27979;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#36890;&#36807;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#22810;&#31181;&#23574;&#31471;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#24037;&#19994;&#21644;&#21307;&#30103;&#25991;&#29486;&#20013;&#26032;&#24320;&#21457;&#30340;&#29305;&#24449;&#24314;&#27169;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#25968;&#25454;&#38598;&#19978;&#21019;&#31435;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.00609</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30149;&#29702;&#26816;&#27979;&#65306;&#28145;&#20837;&#25506;&#31350;&#29616;&#26377;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Pathology Detection: A Deep Dive Into the State of the Art. (arXiv:2303.00609v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30149;&#29702;&#26816;&#27979;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#36890;&#36807;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#22810;&#31181;&#23574;&#31471;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#24037;&#19994;&#21644;&#21307;&#30103;&#25991;&#29486;&#20013;&#26032;&#24320;&#21457;&#30340;&#29305;&#24449;&#24314;&#27169;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#25968;&#25454;&#38598;&#19978;&#21019;&#31435;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26080;&#30417;&#30563;&#26041;&#27861;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#30149;&#29702;&#26816;&#27979;&#21644;&#20998;&#21106;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#25215;&#35834;&#21487;&#20197;&#20943;&#36731;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#22312;&#26816;&#27979;&#20219;&#20309;&#19968;&#31181;&#32597;&#35265;&#30149;&#29702;&#26041;&#38754;&#27604;&#26377;&#30417;&#30563;&#26041;&#27861;&#26356;&#20855;&#26222;&#36866;&#24615;&#12290;&#38543;&#30528;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979; (UAD) &#25991;&#29486;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#26032;&#30340;&#33539;&#24335;&#30340;&#20986;&#29616;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#22312;&#19968;&#20010;&#20844;&#20849;&#26694;&#26550;&#20013;&#19981;&#26029;&#22320;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#37325;&#26032;&#35780;&#20272;&#26368;&#26032;&#25216;&#26415;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22810;&#31181;&#23574;&#31471; UAD &#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#24050;&#22312;&#33041; MRI &#30340; UAD &#20013;&#30830;&#31435;&#30340;&#26368;&#26032;&#25216;&#26415;&#36827;&#23637;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24037;&#19994;&#21644;&#21307;&#30103;&#25991;&#29486;&#20013;&#26032;&#24320;&#21457;&#30340;&#29305;&#24449;&#24314;&#27169;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#25968;&#25454;&#38598;&#19978;&#21019;&#31435;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep unsupervised approaches are gathering increased attention for applications such as pathology detection and segmentation in medical images since they promise to alleviate the need for large labeled datasets and are more generalizable than their supervised counterparts in detecting any kind of rare pathology. As the Unsupervised Anomaly Detection (UAD) literature continuously grows and new paradigms emerge, it is vital to continuously evaluate and benchmark new methods in a common framework, in order to reassess the state-of-the-art (SOTA) and identify promising research directions. To this end, we evaluate a diverse selection of cutting-edge UAD methods on multiple medical datasets, comparing them against the established SOTA in UAD for brain MRI. Our experiments demonstrate that newly developed feature-modeling methods from the industrial and medical literature achieve increased performance compared to previous work and set the new SOTA in a variety of modalities and datasets. Add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#23545;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#20351;&#29992;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#22312;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09833</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#25552;&#39640;&#20102;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#20449;&#24515;
&lt;/p&gt;
&lt;p&gt;
Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification. (arXiv:2302.09833v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#23545;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#20351;&#29992;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#22312;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#25110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#30001;&#20110;&#20854;&#22823;&#23567;&#21644;&#32570;&#20047;&#20687;&#32032;&#32423;&#27880;&#37322;&#65292;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#23545;&#20110;&#20020;&#24202;&#35786;&#26029;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26032;&#27169;&#22411;&#12290;&#22810;&#23454;&#20363;&#23398;&#20064;&#38656;&#35201;&#21019;&#24314;&#34917;&#19969;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#34917;&#19969;&#30340;&#32534;&#30721;&#36827;&#34892;&#35786;&#26029;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;ResNet-50&#65289;&#36827;&#34892;&#34917;&#19969;&#32534;&#30721;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;KimiaNet&#26159;&#19968;&#31181;&#22522;&#20110;DenseNet121&#30340;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#22312;TCGA&#20999;&#29255;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#23545;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35843;&#26597;&#38754;&#21521;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#65306;1&#65289;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;CLAM&#27169;&#22411;&#21644;2&#65289;&#33258;&#27880;&#24847;&#30340;TransMIL&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whole Slide Images (WSIs) or histopathology images are used in digital pathology. WSIs pose great challenges to deep learning models for clinical diagnosis, owing to their size and lack of pixel-level annotations. With the recent advancements in computational pathology, newer multiple-instance learning-based models have been proposed. Multiple-instance learning for WSIs necessitates creating patches and uses the encoding of these patches for diagnosis. These models use generic pre-trained models (ResNet-50 pre-trained on ImageNet) for patch encoding. The recently proposed KimiaNet, a DenseNet121 model pre-trained on TCGA slides, is a domain-specific pre-trained model. This paper shows the effect of domain-specific pre-training on WSI classification. To investigate the effect of domain-specific pre-training, we considered the current state-of-the-art multiple-instance learning models, 1) CLAM, an attention-based model, and 2) TransMIL, a self-attention-based model, and evaluated the mod
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#29289;&#29702;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#29992;&#20110;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#23545;&#27169;&#22411;&#28779;&#31661;&#29123;&#28903;&#23460;&#20013;&#30340;&#28608;&#20809;&#36215;&#29190;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2302.08629</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#21442;&#25968;&#21270;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#28779;&#31661;&#29123;&#28903;&#23460;&#28608;&#20809;&#36215;&#29190;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Physics-based parameterized neural ordinary differential equations: prediction of laser ignition in a rocket combustor. (arXiv:2302.08629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#29289;&#29702;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#29992;&#20110;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#23545;&#27169;&#22411;&#28779;&#31661;&#29123;&#28903;&#23460;&#20013;&#30340;&#28608;&#20809;&#36215;&#29190;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#21442;&#25968;&#21270;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(PNODE)&#23545;&#27169;&#22411;&#28779;&#31661;&#29123;&#28903;&#23460;&#28608;&#20809;&#36215;&#29190;&#36827;&#34892;&#38477;&#38454;&#24314;&#27169;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#28608;&#20809;&#36215;&#29190;&#39640;&#32500;&#21442;&#25968;&#30340;&#20989;&#25968;&#23884;&#20837;&#20854;&#20013;&#65292;&#20197;&#39044;&#27979;&#21253;&#25324;&#28909;&#28304;&#20989;&#25968;&#12289;&#39044;&#25351;&#25968;&#22240;&#23376;&#21644;&#27963;&#21270;&#33021;&#31561;&#22312;&#20869;&#30340;0D&#27969;&#21160;&#27169;&#22411;&#20013;&#30340;&#21508;&#31181;&#26415;&#35821;&#12290;&#20351;&#29992;0D&#27969;&#21160;&#27169;&#22411;&#30340;&#25511;&#21046;&#26041;&#31243;&#65292;&#25105;&#20204;&#30340;PNODE&#21482;&#38656;&#35201;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#23601;&#21487;&#20197;&#39044;&#27979;&#28201;&#24230;&#12289;&#21387;&#21147;&#21644;&#29289;&#31181;&#30340;&#36136;&#37327;&#20998;&#25968;&#31561;&#21508;&#31181;&#25968;&#37327;&#30340;&#36712;&#36857;&#65292;&#21516;&#26102;&#28385;&#36275;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#39640;&#20445;&#30495;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;(CFD)&#27169;&#25311;&#28608;&#20809;&#35825;&#23548;&#28857;&#28779;&#30340;&#21407;&#22411;&#28779;&#31661;&#29123;&#28903;&#23460;&#30340;&#35299;&#20915;&#24555;&#29031;&#19978;&#39564;&#35777;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;PNODE&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;PNODE&#30340;&#24615;&#33021;&#19982;&#26680;&#23725;&#22238;&#24402;&#21644;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a novel physics-based data-driven framework for reduced-order modeling of laser ignition in a model rocket combustor based on parameterized neural ordinary differential equations (PNODE). Deep neural networks are embedded as functions of high-dimensional parameters of laser ignition to predict various terms in a 0D flow model including the heat source function, pre-exponential factors, and activation energy. Using the governing equations of a 0D flow model, our PNODE needs only a limited number of training samples and predicts trajectories of various quantities such as temperature, pressure, and mass fractions of species while satisfying physical constraints. We validate our physics-based PNODE on solution snapshots of high-fidelity Computational Fluid Dynamics (CFD) simulations of laser-induced ignition in a prototype rocket combustor. We compare the performance of our physics-based PNODE with that of kernel ridge regression and fully connected neural networks
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#37096;&#20998;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#36866;&#24212;&#21644;&#25506;&#32034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#21464;&#21387;&#22120;&#36827;&#34892;&#25512;&#29702;&#36807;&#31243;&#23398;&#20064;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#20551;&#35774;&#31354;&#38388;&#65292;&#20551;&#35774;&#34920;&#31034;&#20026;&#23567;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#21487;&#20197;&#22312;&#24615;&#20215;&#27604;&#39640;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#22312;Symbolic Alchemy&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#19982;&#31934;&#30830;&#21518;&#39564;&#25277;&#26679;&#30456;&#36817;&#30340;&#36866;&#24212;&#36895;&#24230;&#21644;&#25506;&#32034;&#21033;&#29992;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.04250</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#25512;&#26029;&#37096;&#20998;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#19978;&#19979;&#25991;&#36866;&#24212;&#21644;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Learning How to Infer Partial MDPs for In-Context Adaptation and Exploration. (arXiv:2302.04250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#37096;&#20998;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#36866;&#24212;&#21644;&#25506;&#32034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#21464;&#21387;&#22120;&#36827;&#34892;&#25512;&#29702;&#36807;&#31243;&#23398;&#20064;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#20551;&#35774;&#31354;&#38388;&#65292;&#20551;&#35774;&#34920;&#31034;&#20026;&#23567;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#21487;&#20197;&#22312;&#24615;&#20215;&#27604;&#39640;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#22312;Symbolic Alchemy&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#19982;&#31934;&#30830;&#21518;&#39564;&#25277;&#26679;&#30456;&#36817;&#30340;&#36866;&#24212;&#36895;&#24230;&#21644;&#25506;&#32034;&#21033;&#29992;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20219;&#21153;&#38388;&#36827;&#34892;&#27867;&#21270;&#65292;&#26234;&#33021;&#20307;&#24212;&#35813;&#20174;&#36807;&#21435;&#30340;&#20219;&#21153;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;&#21644;&#25506;&#32034;&#12290;&#25105;&#20204;&#20851;&#27880;&#19978;&#19979;&#25991;&#36866;&#24212;&#21644;&#25506;&#32034;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#21482;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#65292;&#21363;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;/&#25110;&#22870;&#21169;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#26356;&#26032;&#12290;&#21518;&#39564;&#25277;&#26679;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#38656;&#35201;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#21160;&#24577;&#35268;&#21010;&#65292;&#36890;&#24120;&#28041;&#21450;&#26410;&#30693;&#37327;&#65288;&#20363;&#22914;&#65292;&#20808;&#39564;&#65289;&#21644;&#26114;&#36149;&#30340;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21464;&#21387;&#22120;&#26469;&#20174;&#35757;&#32451;&#20219;&#21153;&#20013;&#23398;&#20064;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#32771;&#34385;&#19968;&#20010;&#20551;&#35774;&#31354;&#38388;&#30340;&#37096;&#20998;&#27169;&#22411;&#65292;&#34920;&#31034;&#20026;&#23567;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#23545;&#20110;&#21160;&#24577;&#35268;&#21010;&#26469;&#35828;&#26159;&#24265;&#20215;&#30340;&#12290;&#22312;&#25105;&#20204;&#29256;&#26412;&#30340;Symbolic Alchemy&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36866;&#24212;&#36895;&#24230;&#21644;&#25506;&#32034;&#21033;&#29992;&#24179;&#34913;&#25509;&#36817;&#20110;&#31934;&#30830;&#30340;&#21518;&#39564;&#25277;&#26679;&#31070;&#35861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21363;&#20351;&#37096;&#20998;&#27169;&#22411;&#25490;&#38500;&#20102;r
&lt;/p&gt;
&lt;p&gt;
To generalize across tasks, an agent should acquire knowledge from past tasks that facilitate adaptation and exploration in future tasks. We focus on the problem of in-context adaptation and exploration, where an agent only relies on context, i.e., history of states, actions and/or rewards, rather than gradient-based updates. Posterior sampling (extension of Thompson sampling) is a promising approach, but it requires Bayesian inference and dynamic programming, which often involve unknowns (e.g., a prior) and costly computations. To address these difficulties, we use a transformer to learn an inference process from training tasks and consider a hypothesis space of partial models, represented as small Markov decision processes that are cheap for dynamic programming. In our version of the Symbolic Alchemy benchmark, our method's adaptation speed and exploration-exploitation balance approach those of an exact posterior sampling oracle. We also show that even though partial models exclude r
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#26816;&#27979;&#30456;&#20851;&#25968;&#25454;&#24211;&#20013;&#30340;&#30456;&#21464;&#38382;&#39064;&#65292;&#26681;&#25454;$n$&#21644;$d$&#30340;&#28176;&#36817;&#35268;&#24459;&#30830;&#23450;&#20102;&#26368;&#20248;&#26816;&#27979;&#23637;&#29616;&#20986;&#30456;&#21464;&#30340;&#23574;&#38160;&#38408;&#20540;&#65292;&#34917;&#20805;&#20102;&#30697;&#38453;&#24863;&#24212;&#27169;&#22411;&#20013;&#30456;&#20301;&#24674;&#22797;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.03380</link><description>&lt;p&gt;
&#26816;&#27979;&#30456;&#20851;&#25968;&#25454;&#24211;&#20013;&#30340;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase Transitions in the Detection of Correlated Databases. (arXiv:2302.03380v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03380
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#26816;&#27979;&#30456;&#20851;&#25968;&#25454;&#24211;&#20013;&#30340;&#30456;&#21464;&#38382;&#39064;&#65292;&#26681;&#25454;$n$&#21644;$d$&#30340;&#28176;&#36817;&#35268;&#24459;&#30830;&#23450;&#20102;&#26368;&#20248;&#26816;&#27979;&#23637;&#29616;&#20986;&#30456;&#21464;&#30340;&#23574;&#38160;&#38408;&#20540;&#65292;&#34917;&#20805;&#20102;&#30697;&#38453;&#24863;&#24212;&#27169;&#22411;&#20013;&#30456;&#20301;&#24674;&#22797;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26816;&#27979;&#30001;$n$&#20010;&#29992;&#25143;&#32452;&#25104;&#30340;&#20004;&#20010;&#39640;&#26031;&#25968;&#25454;&#24211;$\mathsf{X}\in\mathbb{R}^{n\times d}$&#21644;$\mathsf{Y}^{n\times d}$&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65306;&#22312;&#38646;&#20551;&#35774;&#19979;&#65292;&#36825;&#20004;&#20010;&#25968;&#25454;&#24211;&#26159;&#32479;&#35745;&#29420;&#31435;&#30340;&#12290;&#22312;&#26367;&#20195;&#20551;&#35774;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#26410;&#30693;&#25490;&#21015;$\sigma$&#65288;&#25110;&#34892;&#25490;&#21015;&#65289;&#65292;&#20351;&#24471;$\mathsf{X}$&#19982;$\mathsf{Y}^\sigma$&#30456;&#20851;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;$n$&#21644;$d$&#30340;&#28176;&#36817;&#21306;&#22495;&#19979;&#65292;&#26368;&#20248;&#26816;&#27979;&#26174;&#31034;&#20986;&#30456;&#21464;&#30340;&#23574;&#38160;&#38408;&#20540;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;$\rho^2d\to 0$&#65292;&#24403;$d\to \infty$&#26102;&#65292;&#21017;&#26080;&#35770;$n$&#30340;&#20540;&#22914;&#20309;&#65292;&#24369;&#26816;&#27979;&#65288;&#30053;&#20248;&#20110;&#38543;&#26426;&#29468;&#27979;&#65289;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#36825;&#34917;&#20805;&#20102;&#30697;&#38453;&#24863;&#24212;&#27169;&#22411;&#20013;&#30456;&#20301;&#24674;&#22797;&#30340;&#24615;&#33021;&#65292;&#35813;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;&#20302;&#31209;&#30697;&#38453;&#65292;&#32780;&#19981;&#26159;&#26816;&#27979;&#20004;&#20010;&#30697;&#38453;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of detecting the correlation between two Gaussian databases $\mathsf{X}\in\mathbb{R}^{n\times d}$ and $\mathsf{Y}^{n\times d}$, each composed of $n$ users with $d$ features. This problem is relevant in the analysis of social media, computational biology, etc. We formulate this as a hypothesis testing problem: under the null hypothesis, these two databases are statistically independent. Under the alternative, however, there exists an unknown permutation $\sigma$ over the set of $n$ users (or, row permutation), such that $\mathsf{X}$ is $\rho$-correlated with $\mathsf{Y}^\sigma$, a permuted version of $\mathsf{Y}$. We determine sharp thresholds at which optimal testing exhibits a phase transition, depending on the asymptotic regime of $n$ and $d$. Specifically, we prove that if $\rho^2d\to0$, as $d\to\infty$, then weak detection (performing slightly better than random guessing) is statistically impossible, irrespectively of the value of $n$. This compliments the perf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23618;&#21464;&#20998;&#20998;&#26512;&#35777;&#26126;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#25104;&#21151;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#30340;&#25968;&#25454;&#26465;&#20214;&#24471;&#21040;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2302.01798</link><description>&lt;p&gt;
&#36890;&#36807;&#23618;&#21464;&#20998;&#20998;&#26512;&#35299;&#37322;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Interpretations of Domain Adaptations via Layer Variational Analysis. (arXiv:2302.01798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23618;&#21464;&#20998;&#20998;&#26512;&#35777;&#26126;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#25104;&#21151;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#30340;&#25968;&#25454;&#26465;&#20214;&#24471;&#21040;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study establishes the theory of transfer learning in deep learning through formal derivations and heuristic analysis, proving that the success of transfer learning can be guaranteed with corresponding data conditions. An alternative method for network-based transfer learning is proposed, which shows an increase in efficiency and accuracy for domain adaptation.
&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#24615;&#33021;&#65292;&#20294;&#26377;&#38480;&#30340;&#25991;&#29486;&#25253;&#36947;&#20102;&#20854;&#32972;&#21518;&#30340;&#26426;&#21046;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#27491;&#24335;&#30340;&#25512;&#23548;&#21644;&#21551;&#21457;&#24335;&#20998;&#26512;&#65292;&#20197;&#21046;&#23450;&#28145;&#24230;&#23398;&#20064;&#20013;&#36716;&#31227;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#23618;&#21464;&#20998;&#20998;&#26512;&#35777;&#26126;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#25104;&#21151;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#30340;&#25968;&#25454;&#26465;&#20214;&#24471;&#21040;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#35745;&#31639;&#20135;&#29983;&#20102;&#23545;&#30693;&#35782;&#36716;&#31227;&#36807;&#31243;&#30340;&#30452;&#35266;&#35299;&#37322;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;&#24403;&#36866;&#24212;&#26399;&#38388;&#30340;&#26032;&#39046;&#22495;&#25968;&#25454;&#36275;&#22815;&#31232;&#30095;&#26102;&#65292;&#23427;&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;&#23545;&#21508;&#31181;&#20219;&#21153;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#27604;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is known to perform efficiently in many applications empirically, yet limited literature reports the mechanism behind the scene. This study establishes both formal derivations and heuristic analysis to formulate the theory of transfer learning in deep learning. Our framework utilizing layer variational analysis proves that the success of transfer learning can be guaranteed with corresponding data conditions. Moreover, our theoretical calculation yields intuitive interpretations towards the knowledge transfer process. Subsequently, an alternative method for network-based transfer learning is derived. The method shows an increase in efficiency and accuracy for domain adaptation. It is particularly advantageous when new domain data is sufficiently sparse during adaptation. Numerical experiments over diverse tasks validated our theory and verified that our analytic expression achieved better performance in domain adaptation than the gradient descent method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#23545;&#39640;&#25928;&#35757;&#32451;Transformer&#39046;&#22495;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22312;&#35757;&#32451;&#20013;&#20026;&#20013;&#38388;&#24352;&#37327;&#33410;&#30465;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#26041;&#27861;&#19982;&#30828;&#20214;/&#31639;&#27861;&#20849;&#21516;&#35774;&#35745;&#30340;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2302.01107</link><description>&lt;p&gt;
Transformers&#35757;&#32451;&#30340;&#39640;&#25928;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Efficient Training of Transformers. (arXiv:2302.01107v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#39640;&#25928;&#35757;&#32451;Transformer&#39046;&#22495;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22312;&#35757;&#32451;&#20013;&#20026;&#20013;&#38388;&#24352;&#37327;&#33410;&#30465;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#26041;&#27861;&#19982;&#30828;&#20214;/&#31639;&#27861;&#20849;&#21516;&#35774;&#35745;&#30340;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformers&#30340;&#21457;&#23637;&#20026;&#35745;&#31639;&#36164;&#28304;&#25552;&#20986;&#20102;&#24040;&#22823;&#35201;&#27714;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#39640;&#25928;&#35757;&#32451;&#25216;&#26415;&#20197;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#20351;Transformer&#30340;&#35757;&#32451;&#26356;&#24555;&#12289;&#26356;&#20302;&#25104;&#26412;&#19988;&#26356;&#39640;&#31934;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#39640;&#25928;&#35757;&#32451;Transformer&#30340;&#39318;&#20010;&#31995;&#32479;&#32508;&#36848;&#65292;&#35206;&#30422;&#20102;&#21152;&#36895;&#31639;&#26415;&#21644;&#30828;&#20214;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#21069;&#32773;&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#20026;&#20013;&#38388;&#24352;&#37327;&#33410;&#30465;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#30828;&#20214;/&#31639;&#27861;&#20849;&#21516;&#35774;&#35745;&#30340;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;FLAD&#65292;&#24182;&#38024;&#23545;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#23616;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;FLAD&#21644;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#27604;&#36739;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.00674</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#26469;&#25913;&#21892;&#23567;&#26679;&#26412;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;FLAD&#65292;&#24182;&#38024;&#23545;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#23616;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;FLAD&#21644;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#27604;&#36739;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#26377;&#20215;&#20540;&#65292;&#20294;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#30340;&#27169;&#22411;&#19988;&#19981;&#36807;&#24230;&#25311;&#21512;&#23569;&#25968;&#26631;&#35760;&#25968;&#25454;&#28857;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20851;&#27880;&#36741;&#21161;&#25968;&#25454;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#65288;FLAD&#65289;&#65292;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#26399;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#20102;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38543;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#21576;&#32447;&#24615;&#65288;&#25110;&#26356;&#24046;&#65289;&#32553;&#25918;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;FLAD&#19982;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#32622;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25506;&#32034;&#19982;&#21033;&#29992;&#22256;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25512;&#23548;&#20986;&#31639;&#27861;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#25193;&#23637;&#21040;&#27604;&#20808;&#21069;&#26041;&#27861;&#22810;100&#20493;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#8212;&#8212;EXP3-FLAD&#21644;UCB1-FLAD&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20808;&#21069;&#21482;&#36827;&#34892;&#25506;&#32034;&#25110;&#21033;&#29992;&#30340;FLAD&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning is valuable in many real-world applications, but learning a generalizable model without overfitting to the few labeled datapoints is challenging. In this work, we focus on Few-shot Learning with Auxiliary Data (FLAD), a training paradigm that assumes access to auxiliary data during few-shot learning in hopes of improving generalization. Previous works have proposed automated methods for mixing auxiliary and target data, but these methods typically scale linearly (or worse) with the number of auxiliary datasets, limiting their practicality. In this work we relate FLAD to the explore-exploit dilemma that is central to the multi-armed bandit setting and derive algorithms whose computational complexity is independent of the number of auxiliary datasets, allowing us to scale to 100x more auxiliary datasets than prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and compare them with prior FLAD methods that either explore or exploit, finding that the com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RTD-AE&#30340;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#20445;&#25345;&#25968;&#25454;&#25299;&#25169;&#32467;&#26500;&#30340;&#38477;&#32500;&#34920;&#31034;,&#22312;&#20445;&#30041;&#20840;&#23616;&#32467;&#26500;&#21644;&#25299;&#25169;&#24615;&#26041;&#38754;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31454;&#20105;&#23545;&#25163;&#12290;</title><link>http://arxiv.org/abs/2302.00136</link><description>&lt;p&gt;
&#23398;&#20064;&#20445;&#25345;&#25299;&#25169;&#32467;&#26500;&#30340;&#25968;&#25454;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Topology-Preserving Data Representations. (arXiv:2302.00136v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RTD-AE&#30340;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#20445;&#25345;&#25968;&#25454;&#25299;&#25169;&#32467;&#26500;&#30340;&#38477;&#32500;&#34920;&#31034;,&#22312;&#20445;&#30041;&#20840;&#23616;&#32467;&#26500;&#21644;&#25299;&#25169;&#24615;&#26041;&#38754;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#25299;&#25169;&#20445;&#25345;&#25968;&#25454;&#34920;&#31034;&#65288;&#38477;&#32500;&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#24378;&#21046;&#25299;&#25169;&#29305;&#24449;&#65288;&#32858;&#31867;&#12289;&#29615;&#12289;2D&#31354;&#27934;&#31561;&#65289;&#21450;&#20854;&#26412;&#22320;&#21270;&#30340;&#30456;&#20284;&#24615;&#25552;&#20379;&#25968;&#25454;&#27969;&#24418;&#21644;&#20854;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#25299;&#25169;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22312;&#21407;&#22987;&#39640;&#32500;&#25968;&#25454;&#21644;&#20302;&#32500;&#34920;&#31034;&#20043;&#38388;&#26368;&#23567;&#21270;&#34920;&#31034;&#25299;&#25169;&#25955;&#24230;&#65288;RTD&#65289;&#12290;RTD&#26368;&#23567;&#21270;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#25299;&#25169;&#29305;&#24449;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;RTD&#20998;&#21270;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20026;&#33258;&#32534;&#30721;&#22120;&#25439;&#22833;&#39033;&#12290; RTD-AE&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#65292;&#36890;&#36807;&#32447;&#24615;&#30456;&#20851;&#12289;&#19977;&#37325;&#36317;&#31163;&#25490;&#21517;&#20934;&#30830;&#24615;&#20197;&#21450;&#25345;&#20037;&#26465;&#24418;&#30721;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#31561;&#27979;&#37327;&#65292;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#25968;&#25454;&#27969;&#24418;&#30340;&#20840;&#23616;&#32467;&#26500;&#21644;&#25299;&#25169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for learning topology-preserving data representations (dimensionality reduction). The method aims to provide topological similarity between the data manifold and its latent representation via enforcing the similarity in topological features (clusters, loops, 2D voids, etc.) and their localization. The core of the method is the minimization of the Representation Topology Divergence (RTD) between original high-dimensional data and low-dimensional representation in latent space. RTD minimization provides closeness in topological features with strong theoretical guarantees. We develop a scheme for RTD differentiation and apply it as a loss term for the autoencoder. The proposed method "RTD-AE" better preserves the global structure and topology of the data manifold than state-of-the-art competitors as measured by linear correlation, triplet distance ranking accuracy, and Wasserstein distance between persistence barcodes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;logit&#27169;&#22411;&#30340;&#25512;&#26029;&#26694;&#26550;&#65292;&#21487;&#20197;&#27979;&#35797;&#26368;&#20248;&#20135;&#21697;&#32452;&#21512;&#26159;&#21542;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2301.12254</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;Logit&#27169;&#22411;&#20013;&#26368;&#20248;&#20135;&#21697;&#32452;&#21512;&#30340;&#32452;&#21512;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Combinatorial Inference on the Optimal Assortment in Multinomial Logit Models. (arXiv:2301.12254v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;logit&#27169;&#22411;&#30340;&#25512;&#26029;&#26694;&#26550;&#65292;&#21487;&#20197;&#27979;&#35797;&#26368;&#20248;&#20135;&#21697;&#32452;&#21512;&#26159;&#21542;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#30340;&#20135;&#21697;&#32452;&#21512;&#20248;&#21270;&#24050;&#32463;&#25104;&#20026;&#23454;&#36341;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#26029;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#35797;&#26368;&#20248;&#20135;&#21697;&#32452;&#21512;&#26159;&#21542;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;&#22810;&#39033;&#24335;logit&#65288;MNL&#65289;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#30740;&#31350;&#26368;&#20248;&#32452;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assortment optimization has received active explorations in the past few decades due to its practical importance. Despite the extensive literature dealing with optimization algorithms and latent score estimation, uncertainty quantification for the optimal assortment still needs to be explored and is of great practical significance. Instead of estimating and recovering the complete optimal offer set, decision-makers may only be interested in testing whether a given property holds true for the optimal assortment, such as whether they should include several products of interest in the optimal set, or how many categories of products the optimal set should include. This paper proposes a novel inferential framework for testing such properties. We consider the widely adopted multinomial logit (MNL) model, where we assume that each customer will purchase an item within the offered products with a probability proportional to the underlying preference score associated with the product. We reduce
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#36719;&#32422;&#26463;&#65292;&#23558;&#32972;&#26223;&#30693;&#35782;&#65288;&#31526;&#21495;&#25968;&#23398;&#32422;&#26463;&#65289;&#32435;&#20837;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#21487;&#20197;&#25552;&#39640;&#20854;&#25628;&#32034;&#25928;&#29575;&#21644;&#27169;&#22411;&#24847;&#20041;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#19982;&#29702;&#35770;&#30456;&#20851;&#19988;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2301.11919</link><description>&lt;p&gt;
&#22312;&#35745;&#31639;&#20195;&#25968;&#31995;&#32479;&#20013;&#24341;&#20837;&#32972;&#26223;&#30693;&#35782;&#30340;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Incorporating Background Knowledge in Symbolic Regression using a Computer Algebra System. (arXiv:2301.11919v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11919
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#36719;&#32422;&#26463;&#65292;&#23558;&#32972;&#26223;&#30693;&#35782;&#65288;&#31526;&#21495;&#25968;&#23398;&#32422;&#26463;&#65289;&#32435;&#20837;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#21487;&#20197;&#25552;&#39640;&#20854;&#25628;&#32034;&#25928;&#29575;&#21644;&#27169;&#22411;&#24847;&#20041;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#19982;&#29702;&#35770;&#30456;&#20851;&#19988;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#21487;&#20197;&#29983;&#25104;&#31526;&#21512;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#35299;&#37322;&#24615;&#31934;&#31616;&#34920;&#36798;&#24335;&#65292;&#27604;&#40657;&#21283;&#23376;&#26041;&#27861;&#26356;&#22686;&#21152;&#20102;&#20154;&#31867;&#23545;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;&#21152;&#20837;&#32972;&#26223;&#30693;&#35782;&#65288;&#20197;&#31526;&#21495;&#25968;&#23398;&#32422;&#26463;&#24418;&#24335;&#65289;&#20801;&#35768;&#29983;&#25104;&#19982;&#29702;&#35770;&#30456;&#20851;&#19988;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#23558;&#32422;&#26463;&#28155;&#21152;&#21040;&#20256;&#32479;&#30340;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#30340;SR&#65288;PySR&#65289;&#20197;&#21450;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#30340;&#36125;&#21494;&#26031;SR&#20307;&#31995;&#32467;&#26500;&#65288;&#36125;&#21494;&#26031;&#26426;&#22120;&#31185;&#23398;&#23478;&#65289;&#20013;&#65292;&#24182;&#23558;&#36825;&#20123;&#24212;&#29992;&#20110;&#20174;&#23454;&#39564;&#12289;&#21382;&#21490;&#25968;&#25454;&#38598;&#20013;&#37325;&#26032;&#21457;&#29616;&#21560;&#38468;&#26041;&#31243;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#30828;&#32422;&#26463;&#20250;&#22952;&#30861;GA&#21644;MCMC SR&#30340;&#25628;&#32034;&#65292;&#20294;&#36719;&#32422;&#26463;&#21487;&#20197;&#25552;&#39640;&#25628;&#32034;&#26377;&#25928;&#24615;&#21644;&#27169;&#22411;&#24847;&#20041;&#24615;&#65292;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#22914;&#26524;&#32422;&#26463;&#19982;&#25968;&#25454;&#19981;&#30456;&#20851;&#65292;&#21017;&#21487;&#33021;&#20250;&#25439;&#23475;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic Regression (SR) can generate interpretable, concise expressions that fit a given dataset, allowing for more human understanding of the structure than black-box approaches. The addition of background knowledge (in the form of symbolic mathematical constraints) allows for the generation of expressions that are meaningful with respect to theory while also being consistent with data. We specifically examine the addition of constraints to traditional genetic algorithm (GA) based SR (PySR) as well as a Markov-chain Monte Carlo (MCMC) based Bayesian SR architecture (Bayesian Machine Scientist), and apply these to rediscovering adsorption equations from experimental, historical datasets. We find that, while hard constraints prevent GA and MCMC SR from searching, soft constraints can lead to improved performance both in terms of search effectiveness and model meaningfulness, with computational costs increasing by about an order-of-magnitude. If the constraints do not correlate well wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#65306;&#35299;&#37322;&#21644;&#23547;&#25214;&#22909;&#30340;&#31034;&#33539;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#22312;&#25512;&#29702;&#26102;&#23454;&#29616;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#30340;&#26174;&#33879;&#25928;&#29575;&#65292;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#36825;&#31181;&#33021;&#21147;&#23545;&#23569;&#37327;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#24456;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#36125;&#21494;&#26031;&#35270;&#35282;&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#20174;&#31034;&#33539;&#20013;&#38544;&#21547;&#22320;&#25512;&#26029;&#20986;&#30456;&#20851;&#20449;&#24687;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;&#22312;&#27492;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#22522;&#32447;&#30340;&#24179;&#22343;&#20540;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#27599;&#20010; GPT2 &#21644; GPT3 &#27169;&#22411;&#26377;&#26174;&#30528;&#30340; 12.5% &#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#28418;&#31227;&#39033;&#30340;&#25968;&#23398;&#20998;&#26512;&#12290;&#36890;&#36807;&#27425;&#27969;&#24418;&#20551;&#35774;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#22788;&#29702;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22343;&#20540;&#28418;&#31227;&#20989;&#25968;&#21644;&#24471;&#20998;&#20989;&#25968;&#28176;&#36817;&#21457;&#25955;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.07882</link><description>&lt;p&gt;
&#22522;&#20110;&#27425;&#27969;&#24418;&#20551;&#35774;&#19979;&#25193;&#25955;&#27169;&#22411;&#22855;&#24322;&#24615;&#30340;&#25968;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mathematical analysis of singularities in the diffusion model under the submanifold assumption. (arXiv:2301.07882v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#28418;&#31227;&#39033;&#30340;&#25968;&#23398;&#20998;&#26512;&#12290;&#36890;&#36807;&#27425;&#27969;&#24418;&#20551;&#35774;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#22788;&#29702;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22343;&#20540;&#28418;&#31227;&#20989;&#25968;&#21644;&#24471;&#20998;&#20989;&#25968;&#28176;&#36817;&#21457;&#25955;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#23398;&#20998;&#26512;&#12290;&#20197;&#26465;&#20214;&#26399;&#26395;&#34920;&#31034;&#21453;&#21521;&#37319;&#26679;&#27969;&#31243;&#30340;&#28418;&#31227;&#39033;&#65292;&#20854;&#20013;&#28041;&#21450;&#25968;&#25454;&#20998;&#24067;&#21644;&#21069;&#21521;&#25193;&#25955;&#12290;&#35757;&#32451;&#36807;&#31243;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#26465;&#20214;&#26399;&#26395;&#30456;&#20851;&#30340;&#22343;&#26041;&#27531;&#24046;&#26469;&#23547;&#25214;&#27492;&#31867;&#28418;&#31227;&#20989;&#25968;&#12290;&#20351;&#29992;&#21069;&#21521;&#25193;&#25955;&#30340;Green&#20989;&#25968;&#30340;&#23567;&#26102;&#38388;&#36817;&#20284;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DDPM&#20013;&#30340;&#35299;&#26512;&#22343;&#20540;&#28418;&#31227;&#20989;&#25968;&#21644;SGM&#20013;&#30340;&#24471;&#20998;&#20989;&#25968;&#22312;&#37319;&#26679;&#36807;&#31243;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#23545;&#20110;&#20687;&#37027;&#20123;&#38598;&#20013;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#32780;&#35328;&#65292;&#28176;&#36817;&#22320;&#21457;&#25955;&#65292;&#22240;&#27492;&#38590;&#20197;&#36890;&#36807;&#32593;&#32476;&#36827;&#34892;&#36924;&#36817;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22312;&#22788;&#29702;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#26102;&#20173;&#28982;&#20445;&#25345;&#26377;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#26469;&#35828;&#26126;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provide several mathematical analyses of the diffusion model in machine learning. The drift term of the backwards sampling process is represented as a conditional expectation involving the data distribution and the forward diffusion. The training process aims to find such a drift function by minimizing the mean-squared residue related to the conditional expectation. Using small-time approximations of the Green's function of the forward diffusion, we show that the analytical mean drift function in DDPM and the score function in SGM asymptotically blow up in the final stages of the sampling process for singular data distributions such as those concentrated on lower-dimensional manifolds, and is therefore difficult to approximate by a network. To overcome this difficulty, we derive a new target function and associated loss, which remains bounded even for singular data distributions. We illustrate the theoretical findings with several numerical examples.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;&#23545;&#39640;&#39057;&#32452;&#20214;&#25910;&#25947;&#30340;&#38459;&#30861;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2301.05816</link><description>&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#21160;&#24577;&#29702;&#35299;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Understanding the Spectral Bias of Coordinate Based MLPs Via Training Dynamics. (arXiv:2301.05816v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;&#23545;&#39640;&#39057;&#32452;&#20214;&#25910;&#25947;&#30340;&#38459;&#30861;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#20559;&#32622;&#26159;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37325;&#35201;&#35266;&#23519;&#32467;&#26524;&#65292;&#23427;&#34920;&#31034;&#32593;&#32476;&#22312;&#25910;&#25947;&#21040;&#26356;&#39640;&#39057;&#29575;&#32452;&#20214;&#21069;&#65292;&#20250;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#30340;&#20302;&#39057;&#34920;&#31034;&#12290;&#36825;&#19968;&#23646;&#24615;&#19982;&#36229;&#21442;&#25968;&#32593;&#32476;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#26377;&#20851;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#22330;&#26223;&#28210;&#26579;&#26102;&#65292;&#37319;&#29992;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#21033;&#29992;&#23494;&#38598;&#30340;&#20302;&#32500;&#22352;&#26631;&#36755;&#20837;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#35889;&#20559;&#24046;&#65292;&#23436;&#20840;&#38459;&#30861;&#20102;&#25910;&#25947;&#21040;&#39640;&#39057;&#32452;&#20214;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#21644;&#20613;&#37324;&#21494;&#20998;&#26512;&#26469;&#35299;&#37322;&#22352;&#26631;&#31995;&#20013;&#30340;&#35889;&#20559;&#24046;&#21450;&#20854;&#20005;&#37325;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#22240;&#20026;NTK&#19981;&#33021;&#25429;&#25417;&#21040;&#30495;&#27491;&#30340;&#32593;&#32476;&#21160;&#24577;&#65292;&#32780;&#20613;&#37324;&#21494;&#20998;&#26512;&#21482;&#33021;&#25552;&#20379;&#23545;&#39057;&#29575;&#32452;&#20214;&#30340;&#20840;&#23616;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral bias is an important observation of neural network training, stating that the network will learn a low frequency representation of the target function before converging to higher frequency components. This property is interesting due to its link to good generalization in over-parameterized networks. However, in applications to scene rendering, where multi-layer perceptrons (MLPs) with ReLU activations utilize dense, low dimensional coordinate based inputs, a severe spectral bias occurs that obstructs convergence to high freqeuncy components entirely. In order to overcome this limitation, one can encode the inputs using high frequency sinusoids. Previous works attempted to explain both spectral bias and its severity in the coordinate based regime using Neural Tangent Kernel (NTK) and Fourier analysis. However, such methods come with various limitations, since NTK does not capture real network dynamics, and Fourier analysis only offers a global perspective on the frequency compo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#33258;&#36866;&#24212;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26435;&#34913;&#30340;&#22312;&#32447;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23558;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#22312;&#32447;MDP&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.05032</link><description>&lt;p&gt;
&#38754;&#21521;&#22686;&#37327;&#23398;&#20064;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Hyperparameter Optimization for Class-Incremental Learning. (arXiv:2301.05032v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#33258;&#36866;&#24212;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26435;&#34913;&#30340;&#22312;&#32447;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23558;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#22312;&#32447;MDP&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#22686;&#21152;&#31867;&#21035;&#26102;&#65292;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#26088;&#22312;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#12290;CIL&#30340;&#22266;&#26377;&#25361;&#25112;&#26159;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#65292;&#21363;CIL&#27169;&#22411;&#24212;&#20445;&#25345;&#31283;&#23450;&#20197;&#20445;&#30041;&#26087;&#30693;&#35782;&#24182;&#20445;&#25345;&#21487;&#22609;&#24615;&#20197;&#21560;&#25910;&#26032;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#27809;&#26377;&#20219;&#20309;&#19968;&#31181;&#29616;&#26377;&#30340;CIL&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25509;&#25910;&#25968;&#25454;&#35774;&#32622;&#19979;&#23454;&#29616;&#26368;&#20339;&#30340;&#26435;&#34913;&#8212;&#8212;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#21322;&#24320;&#22987;&#65288;TFH&#65289;&#30340;&#35774;&#32622;&#38656;&#35201;&#26356;&#22810;&#30340;&#31283;&#23450;&#24615;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#65288;TFS&#65289;&#30340;&#35774;&#32622;&#38656;&#35201;&#26356;&#22810;&#30340;&#21487;&#22609;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#26435;&#34913;&#65292;&#32780;&#19981;&#38656;&#35201;&#20107;&#20808;&#20102;&#35299;&#35774;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#24433;&#21709;&#26435;&#34913;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#65292;&#20363;&#22914;&#65292;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#25439;&#22833;&#26435;&#37325;&#12289;&#23398;&#20064;&#29575;&#21644;&#20998;&#31867;&#22120;&#31867;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243; &#24418;&#24335;&#21270;&#20026;&#22312;&#32447;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning (CIL) aims to train a classification model while the number of classes increases phase-by-phase. An inherent challenge of CIL is the stability-plasticity tradeoff, i.e., CIL models should keep stable to retain old knowledge and keep plastic to absorb new knowledge. However, none of the existing CIL models can achieve the optimal tradeoff in different data-receiving settings--where typically the training-from-half (TFH) setting needs more stability, but the training-from-scratch (TFS) needs more plasticity. To this end, we design an online learning method that can adaptively optimize the tradeoff without knowing the setting as a priori. Specifically, we first introduce the key hyperparameters that influence the trade-off, e.g., knowledge distillation (KD) loss weights, learning rates, and classifier types. Then, we formulate the hyperparameter optimization process as an online Markov Decision Process (MDP) problem and propose a specific algorithm to solve it. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;ProxSPS&#31639;&#27861;&#65292;&#30456;&#27604;&#38543;&#26426;Polyak&#27493;&#38271;&#65288;SPS&#65289;&#26356;&#31283;&#23450;&#26131;&#35843;&#25972;&#65292;&#21516;&#26102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#23548;&#33268;&#32593;&#32476;&#20855;&#26377;&#26356;&#23567;&#30340;&#26435;&#37325;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2301.04935</link><description>&lt;p&gt;
&#19968;&#31181;&#38543;&#26426;Proximal Polyak&#27493;&#38271;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Stochastic Proximal Polyak Step Size. (arXiv:2301.04935v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;ProxSPS&#31639;&#27861;&#65292;&#30456;&#27604;&#38543;&#26426;Polyak&#27493;&#38271;&#65288;SPS&#65289;&#26356;&#31283;&#23450;&#26131;&#35843;&#25972;&#65292;&#21516;&#26102;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#23548;&#33268;&#32593;&#32476;&#20855;&#26377;&#26356;&#23567;&#30340;&#26435;&#37325;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#26426;Polyak&#27493;&#38271;&#65288;SPS&#65289;&#24050;&#25104;&#20026;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31454;&#20105;&#24615;&#33258;&#36866;&#24212;&#27493;&#38271;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ProxSPS&#65292;&#36825;&#26159;SPS&#30340;proximal&#21464;&#20307;&#65292;&#21487;&#20197;&#22788;&#29702;&#27491;&#21017;&#21270;&#39033;&#12290;&#24320;&#21457;SPS&#30340;proximal&#21464;&#20307;&#29305;&#21035;&#37325;&#35201;&#65292;&#22240;&#20026;SPS&#38656;&#35201;&#30446;&#26631;&#20989;&#25968;&#30340;&#19979;&#30028;&#25165;&#33021;&#21457;&#25381;&#33391;&#22909;&#30340;&#20316;&#29992;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#26159;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#39033;&#30340;&#24635;&#21644;&#26102;&#65292;&#21487;&#29992;&#30340;&#24635;&#21644;&#19979;&#30028;&#20272;&#35745;&#21487;&#33021;&#19981;&#20934;&#30830;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;ProxSPS&#21482;&#38656;&#35201;&#23545;&#25439;&#22833;&#36827;&#34892;&#19979;&#30028;&#20272;&#35745;&#65292;&#32780;&#36825;&#36890;&#24120;&#24456;&#23481;&#26131;&#24471;&#21040;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ProxSPS&#26356;&#26131;&#20110;&#35843;&#25972;&#65292;&#22312;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#26356;&#31283;&#23450;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;ProxSPS&#34920;&#29616;&#19982;AdamW&#19968;&#26679;&#22909;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#35843;&#25972;&#65292;&#24182;&#19988;&#23548;&#33268;&#20855;&#26377;&#26356;&#23567;&#26435;&#37325;&#21442;&#25968;&#30340;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#20026;ProxSPS&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#21253;&#25324;&#38750;&#20809;&#28369;&#12289;&#20809;&#28369;&#12289;&#24369;&#20984;&#21644;&#24378;&#20984;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the stochastic Polyak step size (SPS) has emerged as a competitive adaptive step size scheme for stochastic gradient descent. Here we develop ProxSPS, a proximal variant of SPS that can handle regularization terms. Developing a proximal variant of SPS is particularly important, since SPS requires a lower bound of the objective function to work well. When the objective function is the sum of a loss and a regularizer, available estimates of a lower bound of the sum can be loose. In contrast, ProxSPS only requires a lower bound for the loss which is often readily available. As a consequence, we show that ProxSPS is easier to tune and more stable in the presence of regularization. Furthermore for image classification tasks, ProxSPS performs as well as AdamW with little to no tuning, and results in a network with smaller weight parameters. We also provide an extensive convergence analysis for ProxSPS that includes the non-smooth, smooth, weakly convex and strongly convex setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#20856;&#35299;&#30721;&#22120;&#20248;&#21270;&#31070;&#32463;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#24212;&#29992;BCJR&#31639;&#27861;&#36827;&#34892;&#20102;&#26368;&#20248;&#35299;&#30721;&#65292;&#24418;&#25104;&#20102;&#36845;&#20195;Turbo&#35299;&#30721;&#22120;&#65292;&#20248;&#21270;&#20102;&#23398;&#20064;&#32534;&#30721;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10355</link><description>&lt;p&gt;
&#20248;&#21270;&#20018;&#32852;&#31070;&#32463;&#32534;&#30721;&#30340;&#32463;&#20856;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Optimizing Serially Concatenated Neural Codes with Classical Decoders. (arXiv:2212.10355v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#20856;&#35299;&#30721;&#22120;&#20248;&#21270;&#31070;&#32463;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#24212;&#29992;BCJR&#31639;&#27861;&#36827;&#34892;&#20102;&#26368;&#20248;&#35299;&#30721;&#65292;&#24418;&#25104;&#20102;&#36845;&#20195;Turbo&#35299;&#30721;&#22120;&#65292;&#20248;&#21270;&#20102;&#23398;&#20064;&#32534;&#30721;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30701;&#38271;&#24230;&#32534;&#30721;&#20013;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32534;&#30721;&#24207;&#21015;&#29983;&#25104;&#22120;&#8212;&#8212;&#23454;&#20540;&#31070;&#32463;&#32534;&#30721;&#22120;&#19982;&#32463;&#20856;&#35299;&#30721;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#32463;&#20856;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#36825;&#20123;&#31070;&#32463;&#32534;&#30721;&#30340;&#20248;&#28857;&#21450;&#32570;&#38519;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#32534;&#30721;&#30340;&#23616;&#37096;&#24863;&#21463;&#37326;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;BCJR&#31639;&#27861;&#23545;&#20854;&#36827;&#34892;&#26368;&#20248;&#35299;&#30721;&#65292;&#24182;&#19988;&#20445;&#35777;&#35745;&#31639;&#22797;&#26434;&#24230;&#21487;&#25805;&#20316;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#26497;&#22823;&#21518;&#39564;&#65288;MAP&#65289;&#20998;&#35299;&#22120;&#26469;&#26500;&#24314;&#32463;&#20856;&#65288;&#36845;&#20195;&#65289;Turbo&#35299;&#30721;&#22120;&#65292;&#23545;&#20018;&#32852;&#25110;&#24182;&#32852;CNN&#32534;&#30721;&#22120;&#36827;&#34892;&#36817;&#20284;&#30340;&#26368;&#22823;&#20284;&#28982;&#65288;ML&#65289;&#35299;&#30721;&#26469;&#20248;&#21270;&#23398;&#20064;&#32534;&#30721;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23558;&#32463;&#20856;&#35299;&#30721;&#31639;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32534;&#30721;&#22120;&#24182;&#20248;&#21270;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
For improving short-length codes, we demonstrate that classic decoders can also be used with real-valued, neural encoders, i.e., deep-learning based codeword sequence generators. Here, the classical decoder can be a valuable tool to gain insights into these neural codes and shed light on weaknesses. Specifically, the turbo-autoencoder is a recently developed channel coding scheme where both encoder and decoder are replaced by neural networks. We first show that the limited receptive field of convolutional neural network (CNN)-based codes enables the application of the BCJR algorithm to optimally decode them with feasible computational complexity. These maximum a posteriori (MAP) component decoders then are used to form classical (iterative) turbo decoders for parallel or serially concatenated CNN encoders, offering a close-to-maximum likelihood (ML) decoding of the learned codes. To the best of our knowledge, this is the first time that a classical decoding algorithm is applied to a no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.00735</link><description>&lt;p&gt;
xTrimoABFold&#65306;&#26080;&#22810;&#24207;&#21015;&#27604;&#23545;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00735
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25239;&#20307;&#24037;&#31243;&#39046;&#22495;&#65292;&#35774;&#35745;&#19968;&#20010;&#26032;&#22411;&#25239;&#20307;&#20197;&#27491;&#30830;&#22320;&#32467;&#21512;&#29305;&#23450;&#25239;&#21407;&#30340;&#34920;&#20301;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20102;&#35299;&#25239;&#20307;&#32467;&#26500;&#21644;&#20854;&#34920;&#20301;&#21487;&#20197;&#20419;&#36827;&#23545;&#20854;&#21151;&#33021;&#30340;&#26426;&#21046;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#20174;&#20854;&#24207;&#21015;&#39044;&#27979;&#25239;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#19968;&#39033;&#39640;&#24230;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#65292;&#32780;AlphaFold2&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#20110;&#25239;&#20307;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25239;&#20307;&#30340;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDRs&#65289;&#65292;&#20854;&#39044;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#26680;&#30697;&#38453;&#30340;&#31232;&#30095;&#20195;&#25968;&#65292;&#21487;&#29992;&#20110;&#39640;&#25928;&#20998;&#26512;&#25955;&#20081;&#25968;&#25454;&#24182;&#35745;&#31639;&#26356;&#22797;&#26434;&#30340;&#30697;&#38453;&#20989;&#25968;&#65292;&#22914;${\bm A}^\alpha$&#25110;$\exp({\bm A})$&#65292;&#24182;&#24212;&#29992;&#20110;&#31354;&#38388;&#32479;&#35745;&#23398;&#20013;&#30340;&#39640;&#26031;&#36807;&#31243;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.11681</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#26680;&#30697;&#38453;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Multiresolution kernel matrix algebra. (arXiv:2211.11681v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11681
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#26680;&#30697;&#38453;&#30340;&#31232;&#30095;&#20195;&#25968;&#65292;&#21487;&#29992;&#20110;&#39640;&#25928;&#20998;&#26512;&#25955;&#20081;&#25968;&#25454;&#24182;&#35745;&#31639;&#26356;&#22797;&#26434;&#30340;&#30697;&#38453;&#20989;&#25968;&#65292;&#22914;${\bm A}^\alpha$&#25110;$\exp({\bm A})$&#65292;&#24182;&#24212;&#29992;&#20110;&#31354;&#38388;&#32479;&#35745;&#23398;&#20013;&#30340;&#39640;&#26031;&#36807;&#31243;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21387;&#32553;&#26680;&#30697;&#38453;&#30340;&#31232;&#30095;&#20195;&#25968;&#65292;&#20197;&#23454;&#29616;&#23545;&#25955;&#20081;&#25968;&#25454;&#30340;&#39640;&#25928;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#37319;&#26679;&#21387;&#32553;&#26680;&#30697;&#38453;&#21487;&#20197;&#22312;&#26576;&#31181;S&#26684;&#24335;&#19979;&#20135;&#29983;&#26368;&#20248;&#31232;&#30095;&#30697;&#38453;&#12290;&#38024;&#23545;&#26377;&#38480;&#21487;&#24494;&#26680;&#30340;&#26680;&#30697;&#38453;&#65292;&#21487;&#20197;&#23454;&#29616;S&#26684;&#24335;&#30697;&#38453;&#30340;&#21152;&#27861;&#21644;&#20056;&#27861;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#20195;&#20215;&#38543;&#30697;&#38453;&#22823;&#23567;$N$&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#35777;&#26126;&#24182;&#21033;&#29992;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#26680;&#30697;&#38453;&#65288;&#22914;&#26524;&#23384;&#22312;&#65289;&#30340;&#36870;&#30697;&#38453;&#20063;&#21487;&#20197;&#22312;S&#26684;&#24335;&#19979;&#34987;&#21387;&#32553;&#12290;&#36873;&#25321;&#36870;&#36816;&#31639;&#21487;&#20197;&#30452;&#25509;&#35745;&#31639;&#30456;&#24212;&#31232;&#30095;&#27169;&#24335;&#20013;&#30340;&#26465;&#30446;&#12290;S&#26684;&#24335;&#30697;&#38453;&#25805;&#20316;&#20351;&#24471;&#33021;&#22815;&#39640;&#25928;&#36817;&#20284;&#35745;&#31639;&#26356;&#22797;&#26434;&#30340;&#30697;&#38453;&#20989;&#25968;&#65292;&#22914;${\bm A}^\alpha$&#25110;$\exp({\bm A})$&#12290;&#35813;&#30697;&#38453;&#20195;&#25968;&#36890;&#36807;&#20266;&#24494;&#20998;&#35745;&#31639;&#24471;&#21040;&#25968;&#23398;&#19978;&#30340;&#27491;&#24403;&#24615;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#29992;&#20110;&#31354;&#38388;&#32479;&#35745;&#23398;&#30340;&#39640;&#25928;&#39640;&#26031;&#36807;&#31243;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a sparse algebra for samplet compressed kernel matrices, to enable efficient scattered data analysis. We show the compression of kernel matrices by means of samplets produces optimally sparse matrices in a certain S-format. It can be performed in cost and memory that scale essentially linearly with the matrix size $N$, for kernels of finite differentiability, along with addition and multiplication of S-formatted matrices. We prove and exploit the fact that the inverse of a kernel matrix (if it exists) is compressible in the S-format as well. Selected inversion allows to directly compute the entries in the corresponding sparsity pattern. The S-formatted matrix operations enable the efficient, approximate computation of more complicated matrix functions such as ${\bm A}^\alpha$ or $\exp({\bm A})$. The matrix algebra is justified mathematically by pseudo differential calculus. As an application, efficient Gaussian process learning algorithms for spatial statistics is considered
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#22768;&#37096;&#27468;&#26354;&#20998;&#31163;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598; MedleyVox&#65292;&#20026;&#35299;&#20915;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#29616;&#26377;&#22810;&#22768;&#37096;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#21508;&#31181;&#21333;&#22768;&#37096;&#25968;&#25454;&#38598;&#26500;&#24314;&#22810;&#22768;&#37096;&#28151;&#21512;&#29289;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;&#65288;iSRNet&#65289;&#65292;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.07302</link><description>&lt;p&gt;
MedleyVox: &#19968;&#20221;&#29992;&#20110;&#22810;&#22768;&#37096;&#27468;&#26354;&#20998;&#31163;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MedleyVox: An Evaluation Dataset for Multiple Singing Voices Separation. (arXiv:2211.07302v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#22768;&#37096;&#27468;&#26354;&#20998;&#31163;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598; MedleyVox&#65292;&#20026;&#35299;&#20915;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#29616;&#26377;&#22810;&#22768;&#37096;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#21508;&#31181;&#21333;&#22768;&#37096;&#25968;&#25454;&#38598;&#26500;&#24314;&#22810;&#22768;&#37096;&#28151;&#21512;&#29289;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;&#65288;iSRNet&#65289;&#65292;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#22768;&#37096;&#27468;&#26354;&#20998;&#31163;&#26159;&#38899;&#20048;&#28304;&#20998;&#31163;&#30740;&#31350;&#20013;&#40092;&#20026;&#20154;&#30693;&#30340;&#39046;&#22495;&#12290;&#32570;&#20047;&#22522;&#20934;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#20854;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#22810;&#22768;&#37096;&#27468;&#26354;&#20998;&#31163;&#25552;&#20379;&#20102;&#22522;&#20934;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MedleyVox&#65292;&#36825;&#26159;&#19968;&#20221;&#29992;&#20110;&#22810;&#22768;&#37096;&#27468;&#26354;&#20998;&#31163;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#20998;&#31867;&#20026; i) &#21512;&#21809;&#12289;ii) &#20108;&#37325;&#21809;&#12289;iii) &#20027;&#22768;&#37096;&#21644;&#20854;&#20313;&#22768;&#37096;&#12289;&#21644; iv) N &#22768;&#37096;&#20998;&#31163;&#26469;&#25351;&#23450;&#35813;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#23450;&#20041;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#35299;&#20915;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#29616;&#26377;&#22810;&#22768;&#37096;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21508;&#31181;&#21333;&#22768;&#37096;&#25968;&#25454;&#38598;&#26500;&#24314;&#22810;&#22768;&#37096;&#28151;&#21512;&#29289;&#30340;&#31574;&#30053;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;&#65288;iSRNet&#65289;&#65292;&#23427;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#20998;&#31163;&#32593;&#32476;&#30340;&#21021;&#22987;&#20272;&#35745;&#12290;&#19982; Conv-TasNet &#21644;&#22810;&#22768;&#37096;&#28151;&#21512;&#29289;&#26500;&#24314;&#31574;&#30053;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#25152;&#25552;&#20986;&#30340; iSRNet &#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Separation of multiple singing voices into each voice is a rarely studied area in music source separation research. The absence of a benchmark dataset has hindered its progress. In this paper, we present an evaluation dataset and provide baseline studies for multiple singing voices separation. First, we introduce MedleyVox, an evaluation dataset for multiple singing voices separation. We specify the problem definition in this dataset by categorizing it into i) unison, ii) duet, iii) main vs. rest, and iv) N-singing separation. Second, to overcome the absence of existing multi-singing datasets for a training purpose, we present a strategy for construction of multiple singing mixtures using various single-singing datasets. Third, we propose the improved super-resolution network (iSRNet), which greatly enhances initial estimates of separation networks. Jointly trained with the Conv-TasNet and the multi-singing mixture construction strategy, the proposed iSRNet achieved comparable performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31243;&#24207;&#65292;&#23427;&#32467;&#21512;&#20102;&#38750;&#32447;&#24615;AC&#30005;&#21147;&#27969;&#26041;&#31243;&#30340;ground truth&#65292;&#20197;&#30830;&#23450;&#26368;&#22351;&#30340;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#12290;&#20351;&#29992;&#39034;&#24207;&#28155;&#21152;&#26377;&#38024;&#23545;&#24615;&#30340;&#20999;&#21106;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#25910;&#32039;&#25105;&#20204;&#30340;&#20844;&#24335;&#65292;&#30452;&#21040;&#35299;&#20915;&#26041;&#26696;&#36275;&#22815;&#32039;&#23494;&#25110;&#36798;&#21040;&#19968;&#20010;&#23433;&#20840;&#24615;&#38408;&#20540;&#12290;</title><link>http://arxiv.org/abs/2211.07125</link><description>&lt;p&gt;
AC&#30005;&#21147;&#27969;&#30340;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20840;&#23616;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Global Performance Guarantees for Neural Network Models of AC Power Flow. (arXiv:2211.07125v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31243;&#24207;&#65292;&#23427;&#32467;&#21512;&#20102;&#38750;&#32447;&#24615;AC&#30005;&#21147;&#27969;&#26041;&#31243;&#30340;ground truth&#65292;&#20197;&#30830;&#23450;&#26368;&#22351;&#30340;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#12290;&#20351;&#29992;&#39034;&#24207;&#28155;&#21152;&#26377;&#38024;&#23545;&#24615;&#30340;&#20999;&#21106;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#25910;&#32039;&#25105;&#20204;&#30340;&#20844;&#24335;&#65292;&#30452;&#21040;&#35299;&#20915;&#26041;&#26696;&#36275;&#22815;&#32039;&#23494;&#25110;&#36798;&#21040;&#19968;&#20010;&#23433;&#20840;&#24615;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#29983;&#25104;&#26082;&#24555;&#21448;&#20934;&#30340;&#40657;&#30418;&#23376;&#27169;&#22411;&#12290;&#20294;&#20005;&#26684;&#39564;&#35777;&#40657;&#30418;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26159;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23545;&#20110;&#30005;&#21147;&#31995;&#32479;&#26469;&#35828;&#65292;&#23398;&#20064;AC&#30005;&#21147;&#27969;&#26159;&#20219;&#20309;&#24076;&#26395;&#26174;&#33879;&#21152;&#36895;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#40657;&#30418;&#27169;&#22411;&#30340;&#22522;&#30707;&#65292;&#26080;&#35770;&#26159;&#20026;&#20102;&#20248;&#21270;&#12289;&#25511;&#21046;&#36824;&#26159;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#39318;&#27425;&#24320;&#21457;&#19968;&#31181;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31243;&#24207;&#65292;&#23427;&#32467;&#21512;&#20102;&#38750;&#32447;&#24615;AC&#30005;&#21147;&#27969;&#26041;&#31243;&#30340;ground truth&#65292;&#20197;&#30830;&#23450;&#26368;&#22351;&#30340;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;Sequential Targeted Tightening (STT)&#65292;&#23427;&#21033;&#29992;&#26494;&#24347;&#30340;&#20984;&#35268;&#21010;&#37325;&#26500;&#20102;&#21407;&#22987;&#30340;&#39564;&#35777;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#26159;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#65288;MIQP&#65289;&#12290;&#36890;&#36807;&#39034;&#24207;&#28155;&#21152;&#26377;&#38024;&#23545;&#24615;&#30340;&#20999;&#21106;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#25910;&#32039;&#25105;&#20204;&#30340;&#20844;&#24335;&#65292;&#30452;&#21040;&#35299;&#20915;&#26041;&#26696;&#36275;&#22815;&#32039;&#23494;&#25110;&#36798;&#21040;&#19968;&#20010;&#23433;&#20840;&#24615;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning can generate black-box surrogate models which are both extremely fast and highly accurate. Rigorously verifying the accuracy of these black-box models, however, is computationally challenging. When it comes to power systems, learning AC power flow is the cornerstone of any machine learning surrogate model wishing to drastically accelerate computations, whether it is for optimization, control, or dynamics. This paper develops for the first time, to our knowledge, a tractable neural network verification procedure which incorporates the ground truth of the non-linear AC power flow equations to determine worst-case neural network performance. Our approach, termed Sequential Targeted Tightening (STT), leverages a loosely convexified reformulation of the original verification problem, which is a mixed integer quadratic program (MIQP). Using the sequential addition of targeted cuts, we iteratively tighten our formulation until either the solution is sufficiently tight or a sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#24418;&#24335;&#65288;epsilon-SupInfoNCE&#65289;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#27491;&#21017;&#21270;&#25439;&#22833;&#65288;FairKL&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#26377;&#20559;&#25968;&#25454;&#20013;&#23398;&#20064;&#26080;&#20559;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.05568</link><description>&lt;p&gt;
&#26080;&#20559;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unbiased Supervised Contrastive Learning. (arXiv:2211.05568v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#24418;&#24335;&#65288;epsilon-SupInfoNCE&#65289;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#27491;&#21017;&#21270;&#25439;&#22833;&#65288;FairKL&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#26377;&#20559;&#25968;&#25454;&#20013;&#23398;&#20064;&#26080;&#20559;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25968;&#25454;&#38598;&#23384;&#22312;&#20559;&#24046;&#65292;&#21363;&#23427;&#20204;&#21253;&#21547;&#20165;&#22312;&#25968;&#25454;&#38598;&#20013;&#19982;&#30446;&#26631;&#31867;&#39640;&#24230;&#30456;&#20851;&#30340;&#26131;&#20110;&#23398;&#20064;&#30340;&#29305;&#24449;&#65292;&#20294;&#19981;&#22312;&#30495;&#23454;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#12290;&#22240;&#27492;&#65292;&#20174;&#26377;&#20559;&#25968;&#25454;&#20013;&#23398;&#20064;&#26080;&#20559;&#27169;&#22411;&#24050;&#25104;&#20026;&#36817;&#24180;&#26469;&#38750;&#24120;&#30456;&#20851;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23398;&#20064;&#23545;&#20559;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#34920;&#24449;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#28548;&#28165;&#20026;&#20160;&#20040;&#26368;&#36817;&#30340;&#23545;&#27604;&#25439;&#22833;&#65288;InfoNCE&#65292;SupCon&#31561;&#65289;&#22312;&#22788;&#29702;&#20559;&#24046;&#25968;&#25454;&#26102;&#21487;&#33021;&#22833;&#36133;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#24418;&#24335;&#65288;epsilon-SupInfoNCE&#65289;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#27491;&#36127;&#26679;&#26412;&#20043;&#38388;&#26368;&#23567;&#36317;&#31163;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;FairKL&#65292;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#21363;&#20351;&#22312;&#26497;&#24230;&#20559;&#24046;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#24456;&#22909;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many datasets are biased, namely they contain easy-to-learn features that are highly correlated with the target class only in the dataset but not in the true underlying distribution of the data. For this reason, learning unbiased models from biased data has become a very relevant research topic in the last years. In this work, we tackle the problem of learning representations that are robust to biases. We first present a margin-based theoretical framework that allows us to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when dealing with biased data. Based on that, we derive a novel formulation of the supervised contrastive loss (epsilon-SupInfoNCE), providing more accurate control of the minimal distance between positive and negative samples. Furthermore, thanks to our theoretical framework, we also propose FairKL, a new debiasing regularization loss, that works well even with extremely biased data. We validate the proposed losses on standard vision datasets inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#20013;&#32570;&#22833;&#25968;&#25454;&#36716;&#31227;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;DAMS&#26041;&#27861;&#12290;&#38024;&#23545;&#32570;&#22833;&#25968;&#25454;&#25351;&#26631;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#21327;&#21464;&#37327;&#36716;&#31227;&#34987;&#36829;&#21453;&#12289;&#26368;&#20248;&#28304;&#39044;&#27979;&#22120;&#21487;&#33021;&#27604;&#24635;&#26159;&#39044;&#27979;&#22343;&#20540;&#34920;&#29616;&#26356;&#24046;&#12289;&#26368;&#20248;&#30446;&#26631;&#39044;&#27979;&#22120;&#21487;&#34987;&#35782;&#21035;&#31561;&#12290;</title><link>http://arxiv.org/abs/2211.02093</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#36716;&#31227;&#19979;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation under Missingness Shift. (arXiv:2211.02093v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#20013;&#32570;&#22833;&#25968;&#25454;&#36716;&#31227;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;DAMS&#26041;&#27861;&#12290;&#38024;&#23545;&#32570;&#22833;&#25968;&#25454;&#25351;&#26631;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#21327;&#21464;&#37327;&#36716;&#31227;&#34987;&#36829;&#21453;&#12289;&#26368;&#20248;&#28304;&#39044;&#27979;&#22120;&#21487;&#33021;&#27604;&#24635;&#26159;&#39044;&#27979;&#22343;&#20540;&#34920;&#29616;&#26356;&#24046;&#12289;&#26368;&#20248;&#30446;&#26631;&#39044;&#27979;&#22120;&#21487;&#34987;&#35782;&#21035;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#30340;&#27604;&#29575;&#36890;&#24120;&#20250;&#22240;&#35760;&#24405;&#25919;&#31574;&#32780;&#21464;&#21270;&#65292;&#21363;&#20351;&#22522;&#30784;&#29305;&#24449;&#30456;&#23545;&#31283;&#23450;&#65292;&#36825;&#31181;&#24773;&#20917;&#21487;&#33021;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#21644;&#22320;&#28857;&#21457;&#29983;&#21464;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#32570;&#22833;&#25968;&#25454;&#36716;&#31227;&#19979;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;(DAMS)&#12290;&#22312;&#36825;&#37324;&#65292;(&#26377;&#26631;&#31614;&#30340;)&#28304;&#25968;&#25454;&#21644;(&#26080;&#26631;&#31614;&#30340;)&#30446;&#26631;&#25968;&#25454;&#21487;&#20197;&#20114;&#25442;&#65292;&#20294;&#23384;&#22312;&#19981;&#21516;&#30340;&#32570;&#22833;&#25968;&#25454;&#26426;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22914;&#26524;&#32570;&#22833;&#25968;&#25454;&#25351;&#26631;&#26159;&#21487;&#29992;&#30340;&#65292;DAMS&#23601;&#20250;&#24402;&#32467;&#20026;&#21327;&#21464;&#37327;&#36716;&#31227;&#12290;&#38024;&#23545;&#36825;&#31181;&#25351;&#26631;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#20026;&#22522;&#20110;&#23436;&#20840;&#38543;&#26426;&#19979;&#25253;&#21578;&#30340;&#35752;&#35770;&#25552;&#20379;&#20102;&#20197;&#19979;&#29702;&#35770;&#32467;&#26524;&#65306;(i)&#21327;&#21464;&#37327;&#36716;&#31227;&#34987;&#36829;&#21453;&#20102;(&#38656;&#35201;&#36866;&#24212;)&#65307;(ii)&#19982;&#24635;&#26159;&#39044;&#27979;&#22343;&#20540;&#30456;&#27604;&#65292;&#26368;&#20248;&#30340;&#32447;&#24615;&#28304;&#39044;&#27979;&#22120;&#22312;&#30446;&#26631;&#22495;&#19978;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#21464;&#24471;&#26356;&#31967;&#65307;(iii)&#21363;&#20351;&#32570;&#22833;&#29575;&#26412;&#36523;&#26080;&#27861;&#30830;&#23450;&#65292;&#20063;&#33021;&#22815;&#35782;&#21035;&#20986;&#26368;&#20248;&#30340;&#30446;&#26631;&#39044;&#27979;&#22120;&#65307;(iv)&#23545;&#20110;&#32447;&#24615;&#27169;&#22411;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#26512;&#35843;&#25972;&#21487;&#20197;&#24471;&#21040;&#26368;&#20248;&#21442;&#25968;&#30340;&#19968;&#33268;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rates of missing data often depend on record-keeping policies and thus may change across times and locations, even when the underlying features are comparatively stable. In this paper, we introduce the problem of Domain Adaptation under Missingness Shift (DAMS). Here, (labeled) source data and (unlabeled) target data would be exchangeable but for different missing data mechanisms. We show that if missing data indicators are available, DAMS reduces to covariate shift. Addressing cases where such indicators are absent, we establish the following theoretical results for underreporting completely at random: (i) covariate shift is violated (adaptation is required); (ii) the optimal linear source predictor can perform arbitrarily worse on the target domain than always predicting the mean; (iii) the optimal target predictor can be identified, even when the missingness rates themselves are not; and (iv) for linear models, a simple analytic adjustment yields consistent estimates of the optimal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20122;&#27954;&#22320;&#38081;&#31995;&#32479;&#30340;&#23458;&#27969;&#25968;&#25454;&#21644;&#27668;&#35937;&#35760;&#24405;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#65292;&#21457;&#29616;&#25226;&#22825;&#27668;&#21464;&#37327;&#21152;&#20837;&#39044;&#27979;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#21608;&#26411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.13965</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25506;&#31350;&#22825;&#27668;&#23545;&#22320;&#38081;&#23458;&#27969;&#39044;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the impact of weather on Metro demand forecasting using machine learning method. (arXiv:2210.13965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20122;&#27954;&#22320;&#38081;&#31995;&#32479;&#30340;&#23458;&#27969;&#25968;&#25454;&#21644;&#27668;&#35937;&#35760;&#24405;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#65292;&#21457;&#29616;&#25226;&#22825;&#27668;&#21464;&#37327;&#21152;&#20837;&#39044;&#27979;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#21608;&#26411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#30340;&#21457;&#23637;&#20026;&#22478;&#24066;&#20132;&#36890;&#31649;&#29702;&#21644;&#32531;&#35299;&#25317;&#25380;&#31561;&#38382;&#39064;&#25552;&#20379;&#20102;&#37325;&#35201;&#24110;&#21161;&#12290;&#26412;&#25991;&#20351;&#29992;2018&#24180;4&#26376;&#33267;6&#26376;&#20122;&#27954;&#19968;&#20010;&#22320;&#38081;&#31995;&#32479;&#30340;&#30495;&#23454;&#23458;&#27969;&#25968;&#25454;&#65292;&#36827;&#34892;&#30701;&#26102;&#27573;&#23458;&#27969;&#39044;&#27979;&#65292;&#23558;&#36710;&#31449;&#20998;&#20026;&#22235;&#31867;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25910;&#38598;&#20102;&#21516;&#26399;&#27668;&#35937;&#35760;&#24405;&#12290;&#28982;&#21518;&#37319;&#29992;&#19981;&#21516;&#36755;&#20837;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#20803;&#22238;&#24402;&#26469;&#35780;&#20272;&#27599;&#31181;&#22825;&#27668;&#20803;&#32032;&#23545;&#20856;&#22411;&#22320;&#38081;&#31449;&#23567;&#26102;&#23458;&#27969;&#39044;&#27979;&#30340;&#25913;&#21892;&#25928;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36755;&#20837;&#22825;&#27668;&#21464;&#37327;&#21487;&#25552;&#39640;&#21608;&#26411;&#39044;&#27979;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;&#24037;&#20316;&#26085;&#39044;&#27979;&#24615;&#33021;&#20165;&#30053;&#26377;&#25913;&#21892;&#65292;&#19981;&#21516;&#20803;&#32032;&#23545;&#39044;&#27979;&#30340;&#36129;&#29486;&#31243;&#24230;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban rail transit provides significant comprehensive benefits such as large traffic volume and high speed, serving as one of the most important components of urban traffic construction management and congestion solution. Using real passenger flow data of an Asian subway system from April to June of 2018, this work analyzes the space-time distribution of the passenger flow using short-term traffic flow prediction. Stations are divided into four types for passenger flow forecasting, and meteorological records are collected for the same period. Then, machine learning methods with different inputs are applied and multivariate regression is performed to evaluate the improvement effect of each weather element on passenger flow forecasting of representative metro stations on hourly basis. Our results show that by inputting weather variables the precision of prediction on weekends enhanced while the performance on weekdays only improved marginally, while the contribution of different elements
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20135;&#29983;&#21551;&#21457;&#24335;&#26469;&#25351;&#23548;&#26377;&#21521;&#25511;&#21046;&#22120;&#21512;&#25104;&#31639;&#27861;&#30340;&#22686;&#37327;&#25506;&#32034;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.05393</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21363;&#20852;&#25511;&#21046;&#22120;&#21512;&#25104;&#31574;&#30053;&#25506;&#32034;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploration Policies for On-the-Fly Controller Synthesis: A Reinforcement Learning Approach. (arXiv:2210.05393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20135;&#29983;&#21551;&#21457;&#24335;&#26469;&#25351;&#23548;&#26377;&#21521;&#25511;&#21046;&#22120;&#21512;&#25104;&#31639;&#27861;&#30340;&#22686;&#37327;&#25506;&#32034;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#22120;&#21512;&#25104;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#30340;&#19968;&#31181;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33719;&#24471;&#21551;&#21457;&#24335;&#26469;&#36827;&#34892;&#26377;&#21521;&#25511;&#21046;&#22120;&#21512;&#25104;&#30340;&#31639;&#27861;&#30740;&#31350;&#12290;&#27492;&#31639;&#27861;&#30340;&#22686;&#37327;&#25506;&#32034;&#30001;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#20154;&#24037;&#35774;&#35745;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controller synthesis is in essence a case of model-based planning for non-deterministic environments in which plans (actually ''strategies'') are meant to preserve system goals indefinitely. In the case of supervisory control environments are specified as the parallel composition of state machines and valid strategies are required to be ''non-blocking'' (i.e., always enabling the environment to reach certain marked states) in addition to safe (i.e., keep the system within a safe zone). Recently, On-the-fly Directed Controller Synthesis techniques were proposed to avoid the exploration of the entire -and exponentially large-environment space, at the cost of non-maximal permissiveness, to either find a strategy or conclude that there is none. The incremental exploration of the plant is currently guided by a domain-independent human-designed heuristic. In this work, we propose a new method for obtaining heuristics based on Reinforcement Learning (RL). The synthesis algorithm is thus frame
&lt;/p&gt;</description></item><item><title>DALL-E-Bot&#26159;&#31532;&#19968;&#20010;&#23558;Web&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#21040;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#24037;&#20316;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#31034;&#20363;&#25110;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#38646;-shot&#22320;&#25512;&#26029;&#24182;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#65292;&#20855;&#26377;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36825;&#20026;&#26426;&#22120;&#20154;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2210.02438</link><description>&lt;p&gt;
DALL-E-Bot&#65306;&#23558;Web&#35268;&#27169;&#30340;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#21040;&#26426;&#22120;&#20154;&#23398;&#20013;
&lt;/p&gt;
&lt;p&gt;
DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics. (arXiv:2210.02438v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02438
&lt;/p&gt;
&lt;p&gt;
DALL-E-Bot&#26159;&#31532;&#19968;&#20010;&#23558;Web&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#21040;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#24037;&#20316;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#31034;&#20363;&#25110;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#38646;-shot&#22320;&#25512;&#26029;&#24182;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#65292;&#20855;&#26377;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36825;&#20026;&#26426;&#22120;&#20154;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#20102;&#23558;Web&#35268;&#27169;&#30340;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#30340;&#24037;&#20316;&#12290;DALL-E-Bot&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#65292;&#39318;&#20808;&#25512;&#26029;&#20986;&#36825;&#20123;&#29289;&#20307;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#28982;&#21518;&#29983;&#25104;&#20195;&#34920;&#36825;&#20123;&#29289;&#20307;&#30340;&#33258;&#28982;&#12289;&#31867;&#20154;&#30340;&#25490;&#21015;&#22270;&#20687;&#65292;&#24182;&#26368;&#32456;&#26681;&#25454;&#30446;&#26631;&#22270;&#20687;&#29289;&#29702;&#22320;&#25490;&#21015;&#36825;&#20123;&#29289;&#20307;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26159;&#20351;&#29992;DALL-E&#23454;&#29616;&#30340;&#38646;-shot&#26041;&#24335;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#31034;&#20363;&#25490;&#21015;&#12289;&#25968;&#25454;&#25910;&#38598;&#25110;&#35757;&#32451;&#12290;&#30001;&#20110;DALL-E&#30340;Web&#35268;&#27169;&#39044;&#35757;&#32451;&#65292;&#22312;&#19981;&#21463;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#29289;&#20307;&#25110;&#22330;&#26223;&#24773;&#20917;&#19979;&#65292;DALL-E-Bot&#26159;&#23436;&#20840;&#33258;&#20027;&#30340;&#12290;&#40723;&#21169;&#30495;&#23454;&#19990;&#30028;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#20154;&#31867;&#30740;&#31350;&#21644;&#23458;&#35266;&#25351;&#26631;&#65292;&#34920;&#26126;&#23558;Web&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#26426;&#22120;&#20154;&#31649;&#36947;&#20013;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21487;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#12289;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the first work to explore web-scale diffusion models for robotics. DALL-E-Bot enables a robot to rearrange objects in a scene, by first inferring a text description of those objects, then generating an image representing a natural, human-like arrangement of those objects, and finally physically arranging the objects according to that goal image. We show that this is possible zero-shot using DALL-E, without needing any further example arrangements, data collection, or training. DALL-E-Bot is fully autonomous and is not restricted to a pre-defined set of objects or scenes, thanks to DALL-E's web-scale pre-training. Encouraging real-world results, with both human studies and objective metrics, show that integrating web-scale diffusion models into robotics pipelines is a promising direction for scalable, unsupervised robot learning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#24863;&#30693;&#30340;&#20248;&#21270;&#22120;CrAM&#65292;&#20854;&#20135;&#29983;&#30340;&#23494;&#38598;&#27169;&#22411;&#21487;&#20197;&#22312;&#35757;&#32451;&#21518;&#36890;&#36807;&#21333;&#27425;&#21387;&#32553;&#32780;&#19981;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#31934;&#24230;&#25439;&#22833;&#65292;&#35813;&#20248;&#21270;&#22120;&#22312;&#19968;&#20123;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.14200</link><description>&lt;p&gt;
CrAM:&#19968;&#31181;&#21387;&#32553;&#24863;&#30693;&#30340;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
CrAM: A Compression-Aware Minimizer. (arXiv:2207.14200v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14200
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#24863;&#30693;&#30340;&#20248;&#21270;&#22120;CrAM&#65292;&#20854;&#20135;&#29983;&#30340;&#23494;&#38598;&#27169;&#22411;&#21487;&#20197;&#22312;&#35757;&#32451;&#21518;&#36890;&#36807;&#21333;&#27425;&#21387;&#32553;&#32780;&#19981;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#31934;&#24230;&#25439;&#22833;&#65292;&#35813;&#20248;&#21270;&#22120;&#22312;&#19968;&#20123;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#21098;&#26525;&#21644;/&#25110;&#37327;&#21270;&#36827;&#34892;&#21387;&#32553;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#24863;&#30693;&#30340;&#20248;&#21270;&#22120;CrAM&#65292;&#36890;&#36807;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#25913;&#21464;&#20248;&#21270;&#27493;&#39588;&#65292;&#20135;&#29983;&#20986;&#30340;&#27169;&#22411;&#22312;&#21387;&#32553;&#25805;&#20316;&#65288;&#22914;&#21098;&#26525;&#65289;&#19979;&#20855;&#26377;&#31283;&#23450;&#30340;&#23616;&#37096;&#25439;&#22833;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;CrAM&#35757;&#32451;&#30340;&#23494;&#38598;&#27169;&#22411;&#24212;&#35813;&#22312;&#35757;&#32451;&#21518;&#33021;&#22815;&#36890;&#36807;&#21333;&#27425;&#21387;&#32553;&#32780;&#19981;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#31934;&#24230;&#25439;&#22833;&#12290;&#22312;&#19968;&#20123;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#22914;ImageNet&#20998;&#31867;&#30340;&#27531;&#24046;&#32593;&#32476;&#21644;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;BERT&#27169;&#22411;&#65292;&#37117;&#34920;&#26126;CrAM&#20135;&#29983;&#30340;&#23494;&#38598;&#27169;&#22411;&#21487;&#20197;&#27604;&#26631;&#20934;&#30340;SGD / Adam&#22522;&#32447;&#26356;&#21152;&#20934;&#30830;&#65292;&#20294;&#22312;&#26435;&#37325;&#21098;&#26525;&#25805;&#20316;&#19979;&#31283;&#23450;&#65292;&#21363;&#21487;&#20197;&#22312;&#19968;&#27425;&#21098;&#26525;&#21040;70-80&#65285;&#31232;&#30095;&#24230;&#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#31934;&#24230;&#25439;&#22833;&#65292;&#21040;90&#65285;&#26102;&#20855;&#26377;&#21512;&#29702;&#65288;&#32422;1&#65285;&#65289;&#30340;&#31934;&#24230;&#25439;&#22833;&#65292;&#36825;&#19982;&#28176;&#36827;&#24335;&#21387;&#32553;&#26041;&#27861;&#30456;&#24403;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) often have to be compressed, via pruning and/or quantization, before they can be deployed in practical settings. In this work we propose a new compression-aware minimizer dubbed CrAM that modifies the optimization step in a principled way, in order to produce models whose local loss behavior is stable under compression operations such as pruning. Thus, dense models trained via CrAM should be compressible post-training, in a single step, without significant accuracy loss. Experimental results on standard benchmarks, such as residual networks for ImageNet classification and BERT models for language modelling, show that CrAM produces dense models that can be more accurate than the standard SGD/Adam-based baselines, but which are stable under weight pruning: specifically, we can prune models in one-shot to 70-80% sparsity with almost no accuracy loss, and to 90% with reasonable ($\sim 1\%$) accuracy loss, which is competitive with gradual compression methods. Ad
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#23884;&#20837;&#32858;&#21512;&#21327;&#35758;\scheme&#65292;&#35813;&#21327;&#35758;&#22312;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#33021;&#22815;&#22312;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#23454;&#29616;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#23884;&#20837;&#34920;&#31034;&#32858;&#21512;&#12290;</title><link>http://arxiv.org/abs/2206.09097</link><description>&lt;p&gt;
&#23433;&#20840;&#23884;&#20837;&#32858;&#21512;&#29992;&#20110;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Secure Embedding Aggregation for Federated Representation Learning. (arXiv:2206.09097v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#23884;&#20837;&#32858;&#21512;&#21327;&#35758;\scheme&#65292;&#35813;&#21327;&#35758;&#22312;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#33021;&#22815;&#22312;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#23454;&#29616;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#23884;&#20837;&#34920;&#31034;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32771;&#34385;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#21327;&#21161;&#19979;&#65292;&#19968;&#32452;$N$&#20010;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#23427;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#65292;&#29992;&#20110;&#34920;&#31034;&#65288;&#25110;&#23884;&#20837;&#65289;&#19968;&#32452;&#23454;&#20307;&#65288;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#29992;&#25143;&#65289;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#20026;&#20102;&#32858;&#21512;&#23458;&#25143;&#31471;&#31169;&#26377;&#35757;&#32451;&#30340;&#26412;&#22320;&#23884;&#20837;&#34920;&#31034;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;\scheme &#30340;&#23433;&#20840;&#23884;&#20837;&#32858;&#21512;&#21327;&#35758;&#65292;&#23427;&#21033;&#29992;&#25152;&#26377;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#28508;&#22312;&#32858;&#21512;&#26426;&#20250;&#65292;&#21516;&#26102;&#25552;&#20379;&#23545;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#23454;&#20307;&#21644;&#30456;&#24212;&#23884;&#20837;&#34920;&#31034;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#38450;&#27490;&#22909;&#22855;&#30340;&#26381;&#21153;&#22120;&#21644;&#22810;&#36798; $T&lt;N/2$&#30340;&#20018;&#36890;&#23458;&#25143;&#31471;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a federated representation learning framework, where with the assistance of a central server, a group of $N$ distributed clients train collaboratively over their private data, for the representations (or embeddings) of a set of entities (e.g., users in a social network). Under this framework, for the key step of aggregating local embeddings trained privately at the clients, we develop a secure embedding aggregation protocol named \scheme, which leverages all potential aggregation opportunities among all the clients, while providing privacy guarantees for the set of local entities and corresponding embeddings \emph{simultaneously} at each client, against a curious server and up to $T &lt; N/2$ colluding clients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#26680;&#21270;Stein&#36317;&#31163;&#26500;&#24314;&#21518;&#39564;Coreset&#30340;MBRL&#26041;&#27861;&#65292;&#22312;&#25918;&#26494;&#36716;&#31227;&#27169;&#22411;&#39640;&#26031;&#25110;Lipschitz&#30340;&#38480;&#21046;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2206.01162</link><description>&lt;p&gt;
&#24102;&#26377;&#26680;&#21270;Stein&#36317;&#31163;&#30340;&#21518;&#39564;Coreset&#26500;&#24314;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning. (arXiv:2206.01162v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#26680;&#21270;Stein&#36317;&#31163;&#26500;&#24314;&#21518;&#39564;Coreset&#30340;MBRL&#26041;&#27861;&#65292;&#22312;&#25918;&#26494;&#36716;&#31227;&#27169;&#22411;&#39640;&#26031;&#25110;Lipschitz&#30340;&#38480;&#21046;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#22823;&#31354;&#38388;&#30340;&#29702;&#35770;&#20445;&#35777;&#22823;&#22810;&#25968;&#38480;&#20110;&#36716;&#31227;&#27169;&#22411;&#20026;&#39640;&#26031;&#25110;Lipschitz&#30340;&#24773;&#20917;&#19979;&#65292;&#24182;&#19988;&#38656;&#35201;&#21518;&#39564;&#20272;&#35745;&#20854;&#34920;&#31034;&#22797;&#26434;&#24230;&#38543;&#26102;&#38388;&#22686;&#38271;&#32780;&#26080;&#30028;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;MBRL&#26041;&#27861;&#65292;(i) &#25918;&#26494;&#30446;&#26631;&#36716;&#31227;&#27169;&#22411;&#23646;&#20110;&#36890;&#29992;&#28151;&#21512;&#27169;&#22411;&#26063;&#30340;&#20551;&#35774;&#65307;(ii) &#36890;&#36807;&#21253;&#21547;&#21387;&#32553;&#27493;&#39588;&#20197;&#20165;&#30001;&#32479;&#35745;&#26174;&#30528;&#30340;&#36807;&#21435;&#29366;&#24577; - &#25805;&#20316;&#23545;&#30340;&#36125;&#21494;&#26031;Coreset&#32452;&#25104;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#65307;(iii) &#34920;&#29616;&#20986;&#20122;&#32447;&#24615;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;Stein&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26500;&#36896;&#21518;&#39564;&#21644;&#30446;&#26631;&#19978;&#28385;&#36275;&#24179;&#28369;&#24615;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#20801;&#35768;&#20197;&#26680;Stein&#36317;&#31163;&#65288;KSD&#65289;&#30340;&#24418;&#24335;&#23553;&#38381;&#22320;&#35780;&#20272;&#20998;&#24067;&#36317;&#31163;&#12290;&#21069;&#38754;&#25552;&#21040;&#30340;&#21518;&#39564;Coreset&#26500;&#24314;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#36825;&#20010;KSD&#26469;&#23436;&#25104;&#30340;&#65292;&#20854;&#20013;&#28151;&#21512;&#32452;&#20214;&#30340;&#20301;&#32622;&#21462;&#20915;&#20110;l-inf&#39044;&#31639;&#65292;&#20197;&#38480;&#21046;&#20013;&#24515;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#27169;&#25311;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based approaches to reinforcement learning (MBRL) exhibit favorable performance in practice, but their theoretical guarantees in large spaces are mostly restricted to the setting when transition model is Gaussian or Lipschitz, and demands a posterior estimate whose representational complexity grows unbounded with time. In this work, we develop a novel MBRL method (i) which relaxes the assumptions on the target transition model to belong to a generic family of mixture models; (ii) is applicable to large-scale training by incorporating a compression step such that the posterior estimate consists of a Bayesian coreset of only statistically significant past state-action pairs; and (iii) exhibits a sublinear Bayesian regret. To achieve these results, we adopt an approach based upon Stein's method, which, under a smoothness condition on the constructed posterior and target, allows distributional distance to be evaluated in closed form as the kernelized Stein discrepancy (KSD). The afor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21355;&#26143;&#26143;&#24231;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#26681;&#25454;&#21355;&#26143;&#30340;&#36890;&#20449;&#33021;&#21147;&#12289;&#26143;&#24231;&#35774;&#35745;&#21644;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#20301;&#32622;&#25552;&#20986;&#20102;&#21355;&#26143;FL&#30340;&#20998;&#31867;&#12290;&#21355;&#26143;FL&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2206.00307</link><description>&lt;p&gt;
&#21355;&#26143;&#26143;&#24231;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in Satellite Constellations. (arXiv:2206.00307v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21355;&#26143;&#26143;&#24231;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#26681;&#25454;&#21355;&#26143;&#30340;&#36890;&#20449;&#33021;&#21147;&#12289;&#26143;&#24231;&#35774;&#35745;&#21644;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#20301;&#32622;&#25552;&#20986;&#20102;&#21355;&#26143;FL&#30340;&#20998;&#31867;&#12290;&#21355;&#26143;FL&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#36866;&#29992;&#20110;&#36830;&#36890;&#24615;&#26377;&#38480;&#21644;&#38388;&#27463;&#24615;&#30340;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21355;&#26143;&#26143;&#24231;&#23545;FL&#24102;&#26469;&#30340;&#26032;&#29615;&#22659;&#65292;&#20854;&#20013;&#36830;&#25509;&#27169;&#24335;&#26126;&#26174;&#19981;&#21516;&#20110;&#24120;&#35268;&#30340;&#22320;&#38754;FL&#12290;&#37325;&#28857;&#26159;&#22312;&#20302;&#22320;&#29699;&#36712;&#36947;&#20013;&#30340;&#22823;&#22411;&#21355;&#26143;&#26143;&#24231;&#65292;&#27599;&#20010;&#21355;&#26143;&#20351;&#29992;&#26412;&#22320;&#23384;&#20648;&#30340;&#25968;&#25454;&#38598;&#21442;&#19982;&#25968;&#25454;&#39537;&#21160;&#30340;FL&#20219;&#21153;&#12290;&#36825;&#31181;&#24773;&#20917;&#28304;&#20110;&#36830;&#25509;&#23567;&#22411;&#21355;&#26143;&#30340;&#20806;&#32423;&#26143;&#24231;&#21644;&#21355;&#26143;&#20013;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#26681;&#25454;&#21355;&#26143;&#30340;&#36890;&#20449;&#33021;&#21147;&#12289;&#26143;&#24231;&#35774;&#35745;&#21644;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#20301;&#32622;&#25552;&#20986;&#20102;&#21355;&#26143;FL&#30340;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#24403;&#21069;&#39046;&#22495;&#26368;&#26032;&#25216;&#26415;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#21355;&#26143;FL&#30340;&#29420;&#29305;&#25361;&#25112;&#19982;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has recently emerged as a distributed machine learning paradigm for systems with limited and intermittent connectivity. This paper presents the new context brought to FL by satellite constellations, where the connectivity patterns are significantly different from the ones observed in conventional terrestrial FL. The focus is on large constellations in low Earth orbit (LEO), where each satellites participates in a data-driven FL task using a locally stored dataset. This scenario is motivated by the trend towards mega constellations of interconnected small satellites in LEO and the integration of artificial intelligence in satellites. We propose a classification of satellite FL based on the communication capabilities of the satellites, the constellation design, and the location of the parameter server. A comprehensive overview of the current state-of-the-art in this field is provided and the unique challenges and opportunities of satellite FL are discussed. Finall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.15171</link><description>&lt;p&gt;
&#24102;&#26377;&#23646;&#24615;&#21024;&#38500;&#23376;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#21644;&#25353;&#38656;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#20559;&#35265;&#21453;&#26144;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24494;&#35843;&#29256;&#26412;&#20013;&#12290;&#24120;&#35265;&#30340;&#22788;&#29702;&#20559;&#24046;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#20248;&#21270;&#26631;&#20934;&#65292;&#24182;&#26356;&#26032;&#27169;&#22411;&#20197;&#36798;&#21040;&#26032;&#30340;&#21435;&#20559;&#32622;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26368;&#32456;&#29992;&#25143;&#21644;&#20174;&#19994;&#20154;&#21592;&#21487;&#33021;&#26356;&#21916;&#27426;&#20999;&#25442;&#22238;&#21407;&#22987;&#27169;&#22411;&#65292;&#25110;&#20165;&#23545;&#29305;&#23450;&#23376;&#38598;&#30340;&#20445;&#25252;&#23646;&#24615;&#24212;&#29992;&#21435;&#20559;&#32622;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#21253;&#25324;&#29420;&#31435;&#39640;&#24230;&#31232;&#30095;&#30340;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#20854;&#20013;&#27599;&#20010;&#21435;&#20559;&#32622;&#27169;&#22359;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#8220;diff&#8221;&#21098;&#26525;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#20110;&#21508;&#31181;&#34920;&#31034;&#20998;&#31163;&#20248;&#21270;&#30340;&#26032;&#22411;&#35757;&#32451;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;MiniDisc&#30340;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2205.14570</link><description>&lt;p&gt;
MiniDisc: &#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
MiniDisc: Minimal Distillation Schedule for Language Model Compression. (arXiv:2205.14570v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;MiniDisc&#30340;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#23481;&#37327;&#24046;&#36317;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#25928;&#26524;&#19981;&#20339;&#65292;&#24341;&#20837;&#20102;&#25945;&#24072;&#21161;&#25163;&#36741;&#21161;&#33976;&#39311;&#26469;&#24357;&#34917;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25945;&#24072;&#21161;&#25163;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#23581;&#35797;&#25165;&#33021;&#35843;&#24230;&#20986;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65288;MiniDisc&#65289;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;MiniDisc&#26159;&#22522;&#20110;&#25945;&#24072;&#21161;&#25163;&#30340;&#35268;&#27169;-&#24615;&#33021;&#30340;&#26435;&#34913;&#26469;&#24230;&#37327;&#25945;&#24072;&#21161;&#25163;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#19981;&#23545;&#23398;&#29983;&#36827;&#34892;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have uncovered that language model distillation is less effective when facing a large capacity gap between the teacher and the student, and introduced teacher assistant-based distillation to bridge the gap. As a connection, the scale and the performance of the teacher assistant is of vital importance to bring the knowledge from the teacher to the student. However, existing teacher assistant-based methods require maximally many trials before scheduling an optimal teacher assistant. To this end, we propose a minimal distillation schedule (MiniDisc) for scheduling the optimal teacher assistant in minimally one trial. In particular, motivated by the finding that the performance of the student is positively correlated to the scale-performance tradeoff of the teacher assistant, MiniDisc is designed with a $\lambda$-tradeoff to measure the optimality of the teacher assistant without trial distillation to the student. MiniDisc then can schedule the optimal teacher assistant with
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#20272;&#35745;&#26041;&#27861;&#25913;&#32534;&#21040;&#20102;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#19978;&#65292;&#21629;&#21517;&#20026;TREX&#21644;BoostIn&#65292;&#26088;&#22312;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;GBDT&#30340;&#39044;&#27979;&#21644;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.00359</link><description>&lt;p&gt;
&#36866;&#24212;&#24182;&#35780;&#20272;&#29992;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#24433;&#21709;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees. (arXiv:2205.00359v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#20272;&#35745;&#26041;&#27861;&#25913;&#32534;&#21040;&#20102;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#19978;&#65292;&#21629;&#21517;&#20026;TREX&#21644;BoostIn&#65292;&#26088;&#22312;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;GBDT&#30340;&#39044;&#27979;&#21644;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#20272;&#35745;&#20998;&#26512;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#26356;&#25913;&#22914;&#20309;&#23548;&#33268;&#19981;&#21516;&#30340;&#27169;&#22411;&#39044;&#27979;&#65307;&#36825;&#31181;&#20998;&#26512;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#39044;&#27979;&#12289;&#20570;&#20986;&#36825;&#20123;&#39044;&#27979;&#30340;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24433;&#21709;&#20272;&#35745;&#25216;&#26415;&#37117;&#26159;&#20026;&#20855;&#26377;&#36830;&#32493;&#21442;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#30340;&#12290;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#31867;&#21035;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#40657;&#30418;&#23376;&#65292;&#20855;&#26377;&#19981;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299; GBDT &#39044;&#27979;&#24182;&#26222;&#36941;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#26368;&#36817;&#21644;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#20272;&#35745;&#26041;&#27861;&#36866;&#24212;&#21040;&#20102; GBDT &#19978;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992; representer-point &#26041;&#27861;&#21644; TracIn &#26041;&#27861;&#25913;&#32534;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026; TREX &#21644; BoostIn&#65307;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/jjbrophy47/tree_influence &#19978;&#25214;&#21040;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#26041;&#27861;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982; LeafInfluence &#21644;&#20854;&#20182;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influence estimation analyzes how changes to the training data can lead to different model predictions; this analysis can help us better understand these predictions, the models making those predictions, and the data sets they're trained on. However, most influence-estimation techniques are designed for deep learning models with continuous parameters. Gradient-boosted decision trees (GBDTs) are a powerful and widely-used class of models; however, these models are black boxes with opaque decision-making processes. In the pursuit of better understanding GBDT predictions and generally improving these models, we adapt recent and popular influence-estimation methods designed for deep learning models to GBDTs. Specifically, we adapt representer-point methods and TracIn, denoting our new methods TREX and BoostIn, respectively; source code is available at https://github.com/jjbrophy47/tree_influence. We compare these methods to LeafInfluence and other baselines using 5 different evaluation mea
&lt;/p&gt;</description></item><item><title>&#22312;&#24494;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#21306;&#38388;&#30028;&#27010;&#24565;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#36793;&#30028;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#20445;&#30041;&#35757;&#32451;&#20219;&#21153;&#21608;&#22260;&#30340;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#25554;&#20540;&#26469;&#20154;&#20026;&#24418;&#25104;&#26032;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2204.03511</link><description>&lt;p&gt;
&#24494;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#21306;&#38388;&#30028;&#25554;&#20540;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interval Bound Interpolation for Few-shot Learning with Few Tasks. (arXiv:2204.03511v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03511
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#21306;&#38388;&#30028;&#27010;&#24565;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#36793;&#30028;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#20445;&#30041;&#35757;&#32451;&#20219;&#21153;&#21608;&#22260;&#30340;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#25554;&#20540;&#26469;&#20154;&#20026;&#24418;&#25104;&#26032;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#23567;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#23558;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#25152;&#33719;&#21462;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#30456;&#21516;&#20219;&#21153;&#20998;&#24067;&#20013;&#30340;&#26032;&#20219;&#21153;&#12290;&#23454;&#29616;&#26377;&#25928;&#30340;&#23569;&#27425;&#23398;&#20064;&#27867;&#21270;&#30340;&#22522;&#26412;&#21069;&#25552;&#26159;&#23398;&#20064;&#20219;&#21153;&#27969;&#24418;&#30340;&#22909;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#22312;&#20165;&#26377;&#21463;&#38480;&#25968;&#37327;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#23569;&#20219;&#21153;&#23569;&#23398;&#20064;&#24773;&#20917;&#19979;&#65292;&#26174;&#24335;&#22320;&#20445;&#30041;&#20219;&#21153;&#27969;&#24418;&#20013;&#30340;&#26412;&#22320;&#37051;&#22495;&#24182;&#21033;&#29992;&#20854;&#29983;&#25104;&#35757;&#32451;&#20154;&#24037;&#20219;&#21153;&#21487;&#20197;&#21327;&#21161;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#23436;&#20840;&#24378;&#38887;&#24615;&#35757;&#32451;&#25991;&#29486;&#20013;&#30340;&#21306;&#38388;&#30028;&#27010;&#24565;&#24341;&#20837;&#21040;&#20102;&#24494;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#12290;&#21306;&#38388;&#30028;&#29992;&#20110;&#25551;&#36848;&#35757;&#32451;&#20219;&#21153;&#21608;&#22260;&#30340;&#39046;&#22495;&#12290;&#36825;&#20123;&#37051;&#22495;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#36793;&#30028;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#20445;&#30041;&#12290;&#28982;&#21518;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#36890;&#36807;&#25554;&#20540;&#26469;&#20154;&#20026;&#24418;&#25104;&#26032;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning aims to transfer the knowledge acquired from training on a diverse set of tasks to unseen tasks from the same task distribution with a limited amount of labeled data. The underlying requirement for effective few-shot generalization is to learn a good representation of the task manifold. This becomes more difficult when only a limited number of tasks are available for training. In such a few-task few-shot setting, it is beneficial to explicitly preserve the local neighborhoods from the task manifold and exploit this to generate artificial tasks for training. To this end, we introduce the notion of interval bounds from the provably robust training literature to few-shot learning. The interval bounds are used to characterize neighborhoods around the training tasks. These neighborhoods can then be preserved by minimizing the distance between a task and its respective bounds. We then use a novel strategy to artificially form new tasks for training by interpolating between 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#22914;&#20309;&#23558;&#25991;&#26412;&#25968;&#25454;&#19982;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#30456;&#32467;&#21512;&#20197;&#21152;&#24378;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#34920;&#24449;&#30340;&#36136;&#37327;&#65292;&#25552;&#20986;&#20102;ECOLA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#26102;&#38388;&#22240;&#32032;&#65292;&#24182;&#23558;&#25991;&#26412;&#20449;&#24687;&#27880;&#20837;&#21040;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#20013;&#12290;</title><link>http://arxiv.org/abs/2203.09590</link><description>&lt;p&gt;
ECOLA: &#20351;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#34920;&#31034;&#22686;&#24378;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
ECOLA: Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#22914;&#20309;&#23558;&#25991;&#26412;&#25968;&#25454;&#19982;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#30456;&#32467;&#21512;&#20197;&#21152;&#24378;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#34920;&#24449;&#30340;&#36136;&#37327;&#65292;&#25552;&#20986;&#20102;ECOLA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#26102;&#38388;&#22240;&#32032;&#65292;&#24182;&#23558;&#25991;&#26412;&#20449;&#24687;&#27880;&#20837;&#21040;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30693;&#35782;&#23884;&#20837;&#27169;&#22411;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#22240;&#27492;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20197;&#21033;&#29992;&#25991;&#26412;&#26469;&#22686;&#24378;&#30693;&#35782;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#20110;&#21253;&#21547;&#26102;&#38388;&#20381;&#36182;&#20107;&#20214;&#30693;&#35782;&#21644;&#22797;&#26434;&#26102;&#38388;&#21160;&#24577;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;tKG&#65289;&#12290; &#29305;&#21035;&#26159;&#65292;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#30693;&#35782;&#23884;&#20837;&#26159;&#29420;&#31435;&#20110;&#26102;&#38388;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;tKG&#27169;&#22411;&#20013;&#65292;&#23454;&#20307;&#23884;&#20837;&#36890;&#24120;&#20250;&#19981;&#26029;&#28436;&#21270;&#65292;&#36825;&#23601;&#25552;&#20986;&#20102;&#23558;&#26102;&#38388;&#30456;&#20851;&#25991;&#26412;&#19982;&#23454;&#20307;&#23545;&#40784;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#30740;&#31350;&#22914;&#20309;&#23558;&#25991;&#26412;&#25968;&#25454;&#19982;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#12290;&#20316;&#20026;&#36825;&#39033;&#20219;&#21153;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#34920;&#31034;&#22686;&#24378;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#65288;ECOLA&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#26102;&#38388;&#22240;&#32032;&#65292;&#24182;&#23558;&#25991;&#26412;&#20449;&#24687;&#27880;&#20837;&#21040;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#20013;&#12290;&#20026;&#20102;&#35780;&#20272;ECOLA&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Since conventional knowledge embedding models cannot take full advantage of the abundant textual information, there have been extensive research efforts in enhancing knowledge embedding using texts. However, existing enhancement approaches cannot apply to temporal knowledge graphs (tKGs), which contain time-dependent event knowledge with complex temporal dynamics. Specifically, existing enhancement approaches often assume knowledge embedding is time-independent. In contrast, the entity embedding in tKG models usually evolves, which poses the challenge of aligning temporally relevant texts with entities. To this end, we propose to study enhancing temporal knowledge embedding with textual data in this paper. As an approach to this task, we propose Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), which takes the temporal aspect into account and injects textual information into temporal knowledge embedding. To evaluate ECOLA, we introduce three n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65292;&#37319;&#29992;&#23618;&#32423;&#27169;&#22359;&#32452;&#32455;&#25299;&#25169;&#32467;&#26500;&#65292;&#24212;&#29992;&#20110;&#20840;&#23616;&#25628;&#32034;&#31354;&#38388;&#65292;&#32467;&#21512;&#31574;&#30053;&#31995;&#32479;&#26469;&#25512;&#24191;&#34920;&#29616;&#33391;&#22909;&#30340;&#23376;&#32467;&#26500;&#65292;&#36890;&#36807;Fashion-MNIST&#21644;NAS-Bench101&#30340;&#23454;&#39564;&#24471;&#20986;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2107.08484</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#23618;&#32423;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#30340;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Evolutionary Algorithm for Hierarchical Neural Architecture Search. (arXiv:2107.08484v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65292;&#37319;&#29992;&#23618;&#32423;&#27169;&#22359;&#32452;&#32455;&#25299;&#25169;&#32467;&#26500;&#65292;&#24212;&#29992;&#20110;&#20840;&#23616;&#25628;&#32034;&#31354;&#38388;&#65292;&#32467;&#21512;&#31574;&#30053;&#31995;&#32479;&#26469;&#25512;&#24191;&#34920;&#29616;&#33391;&#22909;&#30340;&#23376;&#32467;&#26500;&#65292;&#36890;&#36807;Fashion-MNIST&#21644;NAS-Bench101&#30340;&#23454;&#39564;&#24471;&#20986;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65292;&#36866;&#29992;&#20110;&#20840;&#23616;&#25628;&#32034;&#31354;&#38388;&#12290;&#35813;&#31639;&#27861;&#30340;&#26550;&#26500;&#34920;&#31034;&#23558;&#25299;&#25169;&#32467;&#26500;&#32452;&#32455;&#20026;&#22810;&#20010;&#23618;&#32423;&#27169;&#22359;&#65292;&#35774;&#35745;&#36807;&#31243;&#21033;&#29992;&#35813;&#34920;&#31034;&#26469;&#25506;&#32034;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#31574;&#30053;&#31995;&#32479;&#65292;&#25512;&#24191;&#34920;&#29616;&#33391;&#22909;&#30340;&#23376;&#32467;&#26500;&#21040;&#21518;&#32493;&#19990;&#20195;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;Fashion-MNIST&#21644;NAS-Bench101&#65292;&#30456;&#23545;&#36739;&#23569;&#30340;&#19990;&#20195;&#21363;&#21487;&#23454;&#29616;&#20998;&#21035;&#36798;&#21040;$93.2\%$&#21644;$94.8\%$&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel evolutionary algorithm for neural architecture search, applicable to global search spaces. The algorithm's architectural representation organizes the topology in multiple hierarchical modules, while the design process exploits this representation, in order to explore the search space. We also employ a curation system, which promotes the utilization of well performing sub-structures to subsequent generations. We apply our method to Fashion-MNIST and NAS-Bench101, achieving accuracies of $93.2\%$ and $94.8\%$ respectively in a relatively small number of generations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#12289;&#36866;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#26174;&#24335;&#21033;&#29992;&#20989;&#25968;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2104.09371</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Non-linear Functional Modeling using Neural Networks. (arXiv:2104.09371v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.09371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#12289;&#36866;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#26174;&#24335;&#21033;&#29992;&#20989;&#25968;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#25968;&#25454;&#27169;&#22411;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#38750;&#32447;&#24615;&#24314;&#27169;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#22312;&#20989;&#25968;&#25968;&#25454;&#35774;&#32622;&#26041;&#38754;&#21364;&#24456;&#23569;&#26377;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65306;&#19968;&#31181;&#26159;&#20855;&#26377;&#36830;&#32493;&#38544;&#34255;&#23618;&#30340;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#20989;&#25968;&#30452;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FDNN&#65289;&#65292;&#21478;&#19968;&#31181;&#21017;&#21033;&#29992;&#22522;&#25193;&#23637;&#21644;&#36830;&#32493;&#38544;&#34255;&#23618;&#65292;&#31216;&#20026;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;FBNN&#65289;&#12290;&#20004;&#31181;&#21464;&#20307;&#37117;&#26159;&#35774;&#35745;&#29992;&#26469;&#26174;&#24335;&#21033;&#29992;&#20989;&#25968;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#32467;&#26500;&#12290;&#20026;&#20102;&#25311;&#21512;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#20989;&#25968;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of non-linear models for functional data based on neural networks. Deep learning has been very successful in non-linear modeling, but there has been little work done in the functional data setting. We propose two variations of our framework: a functional neural network with continuous hidden layers, called the Functional Direct Neural Network (FDNN), and a second version that utilizes basis expansions and continuous hidden layers, called the Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data. To fit these models we derive a functional gradient based optimization algorithm. The effectiveness of the proposed methods in handling complex functional models is demonstrated by comprehensive simulation studies and real data examples.
&lt;/p&gt;</description></item><item><title>&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#22312;&#20892;&#39135;&#21697;&#34892;&#19994;&#25968;&#25454;&#20849;&#20139;&#26041;&#38754;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20849;&#20139;&#20998;&#25955;&#25968;&#25454;&#12289;&#20445;&#25252;&#25968;&#25454;&#36129;&#29486;&#32773;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#26500;&#24314;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2104.07468</link><description>&lt;p&gt;
&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#22312;&#20892;&#39135;&#21697;&#34892;&#19994;&#25968;&#25454;&#20849;&#20139;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Cross-Silo Federated Learning in Facilitating Data Sharing in the Agri-Food Sector. (arXiv:2104.07468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.07468
&lt;/p&gt;
&lt;p&gt;
&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#22312;&#20892;&#39135;&#21697;&#34892;&#19994;&#25968;&#25454;&#20849;&#20139;&#26041;&#38754;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20849;&#20139;&#20998;&#25955;&#25968;&#25454;&#12289;&#20445;&#25252;&#25968;&#25454;&#36129;&#29486;&#32773;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#26500;&#24314;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20849;&#20139;&#20173;&#28982;&#26159;&#26222;&#36941;&#37319;&#29992;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#23588;&#20854;&#26159;&#22312;&#20892;&#39135;&#21697;&#34892;&#19994;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#30340;&#20445;&#23494;&#24615;&#26159;&#24456;&#33258;&#28982;&#30340;&#65292;&#22240;&#20026;&#25968;&#25454;&#26159;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#23453;&#36149;&#36164;&#20135;&#65292;&#38656;&#24688;&#24403;&#22320;&#20351;&#29992;&#25165;&#33021;&#20026;&#20854;&#25552;&#20379;&#26377;&#29992;&#30340;&#36816;&#33829;&#21644;&#27969;&#31243;&#27934;&#23519;&#65292;&#20174;&#32780;&#33719;&#24471;&#31454;&#20105;&#20248;&#21183;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#25191;&#34892;&#24471;&#22909;&#65292;&#32780;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#65292;&#20363;&#22914;&#32852;&#37030;&#23398;&#20064;&#21644;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#22522;&#30784;&#35774;&#26045;&#21644;&#25903;&#25345;&#25216;&#26415;&#65292;&#20351;&#29992;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#19981;&#24517;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#26412;&#36523;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#25968;&#25454;&#65288;&#21363;&#26410;&#34987;&#20132;&#25442;&#25110;&#20849;&#20139;&#20294;&#20445;&#30041;&#32473;&#20132;&#26131;&#20249;&#20276;&#30340;&#25968;&#25454;&#65289;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20892;&#39135;&#21697;&#34892;&#19994;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#30340;&#36136;&#37327;&#24182;&#20943;&#23569;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20419;&#36827;&#20102;&#19981;&#21516;&#32452;&#32455;&#20043;&#38388;&#30340;&#25968;&#25454;&#20849;&#20139;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#25968;&#25454;&#36129;&#29486;&#32773;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data sharing remains a major hindering factor when it comes to adopting emerging AI technologies in general, but particularly in the agri-food sector. Protectiveness of data is natural in this setting; data is a precious commodity for data owners, which if used properly can provide them with useful insights on operations and processes leading to a competitive advantage. Unfortunately, novel AI technologies often require large amounts of training data in order to perform well, something that in many scenarios is unrealistic. However, recent machine learning advances, e.g. federated learning and privacy-preserving technologies, can offer a solution to this issue via providing the infrastructure and underpinning technologies needed to use data from various sources to train models without ever sharing the raw data themselves. In this paper, we propose a technical solution based on federated learning that uses decentralized data, (i.e. data that are not exchanged or shared but remain with t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36890;&#29992;&#21270;&#30340;&#22810;&#26680;&#23398;&#20064;&#26041;&#27861;&#65288;NGMKL&#65289;&#65292;&#23558;&#20256;&#32479;&#30340;&#22810;&#26680;&#23398;&#20064;&#26694;&#26550;&#25193;&#23637;&#21040;&#20855;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#23548;&#33268;&#26356;&#39640;&#30340;&#35782;&#21035;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2102.13337</link><description>&lt;p&gt;
&#22810;&#26680;&#23398;&#20064;&#30340;&#31070;&#32463;&#36890;&#29992;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural Generalization of Multiple Kernel Learning. (arXiv:2102.13337v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.13337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36890;&#29992;&#21270;&#30340;&#22810;&#26680;&#23398;&#20064;&#26041;&#27861;&#65288;NGMKL&#65289;&#65292;&#23558;&#20256;&#32479;&#30340;&#22810;&#26680;&#23398;&#20064;&#26694;&#26550;&#25193;&#23637;&#21040;&#20855;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#23548;&#33268;&#26356;&#39640;&#30340;&#35782;&#21035;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26680;&#23398;&#20064;&#26159;&#23398;&#20064;&#26680;&#20989;&#25968;&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;MKL&#31639;&#27861;&#22686;&#24378;&#20102;&#26680;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#22797;&#26434;&#24230;&#36739;&#20302;&#65292;&#22312;&#35782;&#21035;&#31934;&#24230;&#26041;&#38754;&#19981;&#22914;&#36825;&#20123;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20856;&#22411;&#30340;MKL&#31639;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#20855;&#26377;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#36825;&#31181;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26680;&#23398;&#20064;&#30340;&#31070;&#32463;&#36890;&#29992;&#21270;&#65288;NGMKL&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#22810;&#26680;&#23398;&#20064;&#26694;&#26550;&#25193;&#23637;&#21040;&#20855;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;MKL&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#35782;&#21035;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple Kernel Learning is a conventional way to learn the kernel function in kernel-based methods. MKL algorithms enhance the performance of kernel methods. However, these methods have a lower complexity compared to deep learning models and are inferior to these models in terms of recognition accuracy. Deep learning models can learn complex functions by applying nonlinear transformations to data through several layers. In this paper, we show that a typical MKL algorithm can be interpreted as a one-layer neural network with linear activation functions. By this interpretation, we propose a Neural Generalization of Multiple Kernel Learning (NGMKL), which extends the conventional multiple kernel learning framework to a multi-layer neural network with nonlinear activation functions. Our experiments on several benchmarks show that the proposed method improves the complexity of MKL algorithms and leads to higher recognition accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39318;&#20010;&#22823;&#20110;100&#20010;&#21477;&#23376;&#25968;&#25454;&#38598;&#30340;NLP&#23454;&#39564;&#32467;&#26524;&#65292;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#35299;&#20915;&#31616;&#21333;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#30340;NLP&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#32452;&#21512;&#27169;&#22411;&#30340;&#21547;&#20041;&#19982;&#37327;&#23376;&#29702;&#35770;&#20855;&#26377;&#24418;&#24335;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2102.12846</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;QNLP&#65306;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36816;&#34892;&#32452;&#21512;&#27169;&#22411;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
QNLP in Practice: Running Compositional Models of Meaning on a Quantum Computer. (arXiv:2102.12846v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39318;&#20010;&#22823;&#20110;100&#20010;&#21477;&#23376;&#25968;&#25454;&#38598;&#30340;NLP&#23454;&#39564;&#32467;&#26524;&#65292;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#35299;&#20915;&#31616;&#21333;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#30340;NLP&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#32452;&#21512;&#27169;&#22411;&#30340;&#21547;&#20041;&#19982;&#37327;&#23376;&#29702;&#35770;&#20855;&#26377;&#24418;&#24335;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;QNLP&#65289;&#28041;&#21450;&#35774;&#35745;&#21644;&#23454;&#29616;&#26088;&#22312;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#36816;&#34892;&#30340;NLP&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39318;&#20010;&#22823;&#20110;100&#20010;&#21477;&#23376;&#25968;&#25454;&#38598;&#30340;NLP&#23454;&#39564;&#32467;&#26524;&#12290;&#21033;&#29992;&#30001;Coecke&#12289;Sadrzadeh&#21644;Clark&#65288;2010&#65289;&#25552;&#20986;&#30340;&#21547;&#20041;&#32452;&#21512;&#27169;&#22411;&#19982;&#37327;&#23376;&#29702;&#35770;&#30340;&#24418;&#24335;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20855;&#26377;&#33258;&#28982;&#26144;&#23556;&#21040;&#37327;&#23376;&#30005;&#36335;&#30340;&#21477;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#23454;&#29616;&#24182;&#25104;&#21151;&#35757;&#32451;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#35299;&#20915;&#31616;&#21333;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#30340;NLP&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#37327;&#23376;&#27169;&#25311;&#65292;&#27604;&#36739;&#20102;Coecke&#31561;&#20154;&#30340;&#35821;&#27861;&#25935;&#24863;&#27169;&#22411;&#19982;&#20351;&#29992;&#36739;&#23569;&#25110;&#26080;&#35821;&#27861;&#30340;&#20004;&#20010;&#22522;&#32447;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#8220;&#35789;&#34955;&#8221;&#27169;&#22411;&#30340;&#37327;&#23376;&#27169;&#25311;&#65292;&#20854;&#20013;&#26681;&#26412;&#19981;&#32771;&#34385;&#35821;&#27861;&#65292;&#20197;&#21450;&#21333;&#35789;&#24207;&#21015;&#27169;&#22411;&#30340;&#37327;&#23376;&#27169;&#25311;&#65292;&#20165;&#23562;&#37325;&#21333;&#35789;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Natural Language Processing (QNLP) deals with the design and implementation of NLP models intended to be run on quantum hardware. In this paper, we present results on the first NLP experiments conducted on Noisy Intermediate-Scale Quantum (NISQ) computers for datasets of size greater than 100 sentences. Exploiting the formal similarity of the compositional model of meaning by Coecke, Sadrzadeh and Clark (2010) with quantum theory, we create representations for sentences that have a natural mapping to quantum circuits. We use these representations to implement and successfully train NLP models that solve simple sentence classification tasks on quantum hardware. We conduct quantum simulations that compare the syntax-sensitive model of Coecke et al. with two baselines that use less or no syntax; specifically, we implement the quantum analogues of a "bag-of-words" model, where syntax is not taken into account at all, and of a word-sequence model, where only word order is respected.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; GTEA&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26102;&#24207;&#20132;&#20114;&#22270;&#19978;&#36827;&#34892;&#24402;&#32435;&#23398;&#20064;&#65292;&#32467;&#21512;&#20102;&#26102;&#38388;&#21160;&#24577;&#24314;&#27169;&#21644;&#22270;&#23884;&#20837;&#65292;&#36890;&#36807;&#32858;&#21512;&#30456;&#37051;&#33410;&#28857;&#21644;&#36793;&#23884;&#20837;&#30340;&#29305;&#24449;&#65292;&#20849;&#21516;&#23398;&#20064;&#20102; TIG &#30340;&#25299;&#25169;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#19988;&#24341;&#20837;&#20102;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2009.05266</link><description>&lt;p&gt;
GTEA: &#36890;&#36807;&#26102;&#38388;&#36793;&#32858;&#21512;&#22312;&#26102;&#24207;&#20132;&#20114;&#22270;&#19978;&#36827;&#34892;&#24402;&#32435;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GTEA: Inductive Representation Learning on Temporal Interaction Graphs via Temporal Edge Aggregation. (arXiv:2009.05266v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.05266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; GTEA&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26102;&#24207;&#20132;&#20114;&#22270;&#19978;&#36827;&#34892;&#24402;&#32435;&#23398;&#20064;&#65292;&#32467;&#21512;&#20102;&#26102;&#38388;&#21160;&#24577;&#24314;&#27169;&#21644;&#22270;&#23884;&#20837;&#65292;&#36890;&#36807;&#32858;&#21512;&#30456;&#37051;&#33410;&#28857;&#21644;&#36793;&#23884;&#20837;&#30340;&#29305;&#24449;&#65292;&#20849;&#21516;&#23398;&#20064;&#20102; TIG &#30340;&#25299;&#25169;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#19988;&#24341;&#20837;&#20102;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Graph Temporal Edge Aggregation (GTEA)&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26102;&#24207;&#20132;&#20114;&#22270;&#65288;TIGs&#65289;&#19978;&#36827;&#34892;&#24402;&#32435;&#23398;&#20064;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#24037;&#20316;&#65292;GTEA&#23558;&#20132;&#20114;&#24207;&#21015;&#30340;&#26102;&#38388;&#21160;&#24577;&#24314;&#27169;&#22312;&#36830;&#32493;&#26102;&#38388;&#31354;&#38388;&#20013;&#65292;&#24182;&#21516;&#26102;&#21033;&#29992;&#22270;&#20013;&#20016;&#23500;&#30340;&#33410;&#28857;&#21644;&#36793;/&#20132;&#20114;&#23646;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#24207;&#21015;&#27169;&#22411;&#19982;&#26102;&#38388;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20132;&#20114;&#21160;&#21147;&#23398;&#12290;&#36825;&#26377;&#21161;&#20110;&#25429;&#25417;&#33410;&#28857;&#23545;&#27839;&#21382;&#21490;&#30340;&#22797;&#26434;&#26102;&#38388;&#20132;&#20114;&#27169;&#24335;&#65292;&#29983;&#25104;&#36793;&#23884;&#20837;&#21487;&#20197;&#36755;&#20837;&#21040;GNN&#39592;&#24178;&#12290;&#36890;&#36807;&#32858;&#21512;&#30456;&#37051;&#33410;&#28857;&#21644;&#30456;&#24212;&#30340;&#36793;&#23884;&#20837;&#30340;&#29305;&#24449;&#65292;GTEA&#20849;&#21516;&#23398;&#20064;TIG&#30340;&#25299;&#25169;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#33258;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#37051;&#23621;&#32858;&#21512;&#65292;&#31361;&#20986;&#26356;&#37325;&#35201;&#30340;&#37051;&#23621;&#24182;&#25233;&#21046;GTEA&#30340;&#29712;&#30862;&#22122;&#22768;&#12290;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#25972;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the Graph Temporal Edge Aggregation (GTEA) framework for inductive learning on Temporal Interaction Graphs (TIGs). Different from previous works, GTEA models the temporal dynamics of interaction sequences in the continuous-time space and simultaneously takes advantage of both rich node and edge/ interaction attributes in the graph. Concretely, we integrate a sequence model with a time encoder to learn pairwise interactional dynamics between two adjacent nodes.This helps capture complex temporal interactional patterns of a node pair along the history, which generates edge embeddings that can be fed into a GNN backbone. By aggregating features of neighboring nodes and the corresponding edge embeddings, GTEA jointly learns both topological and temporal dependencies of a TIG. In addition, a sparsity-inducing self-attention scheme is incorporated for neighbor aggregation, which highlights more important neighbors and suppresses trivial noises for GTEA. By jointly o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23500;&#26377;&#23646;&#24615;&#32593;&#32476;&#20013;&#39030;&#28857;&#25552;&#21517;&#30340;&#21452;&#37325;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20869;&#23481;&#24863;&#30693;&#30340;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#19981;&#21033;&#29992;&#20869;&#23481;&#21644;&#19978;&#19979;&#25991;&#30340;&#39030;&#28857;&#25552;&#21517;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2005.02151</link><description>&lt;p&gt;
&#23500;&#26377;&#23646;&#24615;&#32593;&#32476;&#20013;&#30340;&#39030;&#28857;&#25552;&#21517;
&lt;/p&gt;
&lt;p&gt;
Vertex Nomination in Richly Attributed Networks. (arXiv:2005.02151v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.02151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23500;&#26377;&#23646;&#24615;&#32593;&#32476;&#20013;&#39030;&#28857;&#25552;&#21517;&#30340;&#21452;&#37325;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20869;&#23481;&#24863;&#30693;&#30340;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#19981;&#21033;&#29992;&#20869;&#23481;&#21644;&#19978;&#19979;&#25991;&#30340;&#39030;&#28857;&#25552;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39030;&#28857;&#25552;&#21517;&#26159;&#19968;&#39033;&#36731;&#24230;&#30417;&#30563;&#30340;&#32593;&#32476;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#24863;&#20852;&#36259;&#30340;&#19968;&#24352;&#22270;&#30340;&#39030;&#28857;&#34987;&#29992;&#26469;&#26597;&#35810;&#31532;&#20108;&#24352;&#22270;&#20197;&#21457;&#29616;&#24863;&#20852;&#36259;&#30340;&#31532;&#20108;&#24352;&#22270;&#30340;&#39030;&#28857;&#12290;&#19982;&#20854;&#20182;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#31867;&#20284;&#65292;&#39030;&#28857;&#25552;&#21517;&#26041;&#26696;&#30340;&#36755;&#20986;&#26159;&#31532;&#20108;&#24352;&#22270;&#20013;&#39030;&#28857;&#30340;&#25490;&#24207;&#21015;&#34920;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#26410;&#30693;&#30340;&#24863;&#20852;&#36259;&#30340;&#39030;&#28857;&#24212;&#35813;&#38598;&#20013;&#22312;&#21015;&#34920;&#30340;&#39030;&#37096;&#12290;&#39030;&#28857;&#25552;&#21517;&#26041;&#26696;&#20026;&#39640;&#25928;&#22320;&#25366;&#25496;&#22797;&#26434;&#32593;&#32476;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#36341;&#20004;&#26041;&#38754;&#25506;&#35752;&#20102;&#20869;&#23481;&#65288;&#21363;&#36793;&#32536;&#21644;&#39030;&#28857;&#23646;&#24615;&#65289;&#21644;&#19978;&#19979;&#25991;&#65288;&#21363;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65289;&#22312;&#39030;&#28857;&#25552;&#21517;&#20013;&#30340;&#21452;&#37325;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#21033;&#29992;&#20869;&#23481;&#21644;&#19978;&#19979;&#25991;&#30340;&#39030;&#28857;&#25552;&#21517;&#26041;&#26696;&#33021;&#22815;&#36229;&#36234;&#20165;&#21033;&#29992;&#20869;&#23481;&#25110;&#19978;&#19979;&#25991;&#30340;&#26041;&#26696;&#12290;&#34429;&#28982;&#20869;&#23481;&#21644;&#19978;&#19979;&#25991;&#30340;&#32852;&#21512;&#25928;&#29992;&#22312;&#20854;&#20182;&#32593;&#32476;&#20998;&#26512;&#20219;&#21153;&#20013;&#24050;&#32463;&#24471;&#21040;&#35777;&#23454;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#22312;&#39030;&#28857;&#25552;&#21517;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#31181;&#32852;&#21512;&#25928;&#29992;&#20063;&#26159;&#25104;&#31435;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20869;&#23481;&#24863;&#30693;&#30340;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#29992;&#20110;&#39030;&#28857;&#25552;&#21517;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#32593;&#32476;&#23646;&#24615;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;&#31038;&#20132;&#21644;&#24341;&#29992;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#19981;&#21033;&#29992;&#20869;&#23481;&#21644;&#19978;&#19979;&#25991;&#30340;&#29616;&#26377;&#30340;&#39030;&#28857;&#25552;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertex nomination is a lightly-supervised network information retrieval task in which vertices of interest in one graph are used to query a second graph to discover vertices of interest in the second graph. Similar to other information retrieval tasks, the output of a vertex nomination scheme is a ranked list of the vertices in the second graph, with the heretofore unknown vertices of interest ideally concentrating at the top of the list. Vertex nomination schemes provide a useful suite of tools for efficiently mining complex networks for pertinent information. In this paper, we explore, both theoretically and practically, the dual roles of content (i.e., edge and vertex attributes) and context (i.e., network topology) in vertex nomination. We provide necessary and sufficient conditions under which vertex nomination schemes that leverage both content and context outperform schemes that leverage only content or context separately. While the joint utility of both content and context has 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#25512;&#31574;&#30053;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#37327;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23454;&#26102;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#31639;&#23376;&#29702;&#35770;&#24037;&#20855;&#65292;&#20998;&#26512;&#20102;&#21407;&#22987;&#38382;&#39064;&#21644;&#23545;&#20598;&#38382;&#39064;&#30340;&#26174;&#24335;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2004.11709</link><description>&lt;p&gt;
&#22522;&#20110;&#22806;&#25512;&#30340;&#26102;&#21464;&#20984;&#20248;&#21270;&#39044;&#27979;&#26657;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extrapolation-based Prediction-Correction Methods for Time-varying Convex Optimization. (arXiv:2004.11709v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.11709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#25512;&#31574;&#30053;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#37327;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23454;&#26102;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#31639;&#23376;&#29702;&#35770;&#24037;&#20855;&#65292;&#20998;&#26512;&#20102;&#21407;&#22987;&#38382;&#39064;&#21644;&#23545;&#20598;&#38382;&#39064;&#30340;&#26174;&#24335;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#32463;&#24120;&#36935;&#21040;&#12289;&#20855;&#26377;&#25968;&#25454;&#27969;&#30340;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#35752;&#35770;&#20102;&#22522;&#20110;&#39044;&#27979;&#26657;&#27491;&#33539;&#24335;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65292;&#21253;&#25324;&#21407;&#22987;&#38382;&#39064;&#21644;&#23545;&#20598;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20986;&#29616;&#22312;&#35768;&#22810;&#20449;&#21495;&#22788;&#29702;&#38382;&#39064;&#20013;&#30340;&#20856;&#22411;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#21046;&#39044;&#27979;&#31574;&#30053;&#65292;&#21363;&#22522;&#20110;&#22806;&#25512;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#36816;&#29992;&#31639;&#23376;&#29702;&#35770;&#24037;&#20855;&#65292;&#25105;&#20204;&#20998;&#21035;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#20316;&#29992;&#20110;&#21407;&#22987;&#38382;&#39064;&#21644;&#23545;&#20598;&#38382;&#39064;&#26102;&#30340;&#25910;&#25947;&#24615;&#65292;&#25512;&#23548;&#20986;&#20102;&#36861;&#36394;&#35823;&#24046;&#65288;&#21363;&#19982;&#26102;&#21464;&#26368;&#20248;&#35299;&#36317;&#31163;&#65289;&#30340;&#26174;&#24335;&#30028;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#35813;&#31639;&#27861;&#24212;&#29992;&#20110;&#20449;&#21495;&#22788;&#29702;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#38382;&#39064;&#26102;&#30340;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on the solution of online optimization problems that arise often in signal processing and machine learning, in which we have access to streaming sources of data. We discuss algorithms for online optimization based on the prediction-correction paradigm, both in the primal and dual space. In particular, we leverage the typical regularized least-squares structure appearing in many signal processing problems to propose a novel and tailored prediction strategy, which we call extrapolation-based. By using tools from operator theory, we then analyze the convergence of the proposed methods as applied both to primal and dual problems, deriving an explicit bound for the tracking error, that is, the distance from the time-varying optimal solution. We further discuss the empirical performance of the algorithm when applied to signal processing, machine learning, and robotics problems.
&lt;/p&gt;</description></item></channel></rss>