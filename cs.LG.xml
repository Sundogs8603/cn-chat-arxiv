<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01261</link><description>&lt;p&gt;
TEDDY: &#22522;&#20110;&#24230;&#37327;&#21028;&#21035;&#31574;&#30053;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TEDDY: Trimming Edges with Degree-based Discrimination strategY
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01261
&lt;/p&gt;
&lt;p&gt;
TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Chen&#31561;&#20154;&#22312;2021&#24180;&#25552;&#20986;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20197;&#26469;&#65292;&#23547;&#25214;&#22270;&#25277;&#22870;&#31080;&#65288;GLT&#65289;&#30340;&#30740;&#31350;&#24050;&#25104;&#20026;GNN&#31038;&#21306;&#30340;&#37325;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#65292;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#29616;&#19982;&#21407;&#22987;&#23494;&#38598;&#32593;&#32476;&#30456;&#24403;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#26356;&#31232;&#30095;&#30340;GLT&#12290;&#21516;&#26102;&#65292;&#22270;&#32467;&#26500;&#20316;&#20026;GNN&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#20063;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#36817;&#20960;&#39033;&#30740;&#31350;&#30340;&#38416;&#26126;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20851;&#20110;GLT&#30340;&#30740;&#31350;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22270;&#32467;&#26500;&#20013;&#30340;&#20869;&#22312;&#36335;&#24452;&#65292;&#24182;&#20197;&#36845;&#20195;&#26041;&#24335;&#35782;&#21035;&#31080;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#32791;&#26102;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;TEDDY&#65292;&#19968;&#31181;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#24182;&#25972;&#21512;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#19968;&#27425;&#24615;&#36793;&#32536;&#31232;&#30095;&#21270;&#26694;&#26550;&#12290;&#22312;&#36827;&#34892;&#36793;&#32536;&#31232;&#30095;&#21270;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient desc
&lt;/p&gt;</description></item><item><title>BDM&#26159;&#19968;&#31181;&#21033;&#29992;&#32852;&#21512;&#25193;&#25955;&#36807;&#31243;&#32039;&#23494;&#32806;&#21512;&#20808;&#39564;&#20449;&#24687;&#19982;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#19987;&#27880;&#20110;3D&#24418;&#29366;&#37325;&#24314;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#26631;&#31614;&#30340;&#20016;&#23500;&#20808;&#39564;&#20449;&#24687;&#26469;&#25913;&#21892;&#33258;&#19979;&#32780;&#19978;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.06973</link><description>&lt;p&gt;
&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#30340;&#36125;&#21494;&#26031;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bayesian Diffusion Models for 3D Shape Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06973
&lt;/p&gt;
&lt;p&gt;
BDM&#26159;&#19968;&#31181;&#21033;&#29992;&#32852;&#21512;&#25193;&#25955;&#36807;&#31243;&#32039;&#23494;&#32806;&#21512;&#20808;&#39564;&#20449;&#24687;&#19982;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#19987;&#27880;&#20110;3D&#24418;&#29366;&#37325;&#24314;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#26631;&#31614;&#30340;&#20016;&#23500;&#20808;&#39564;&#20449;&#24687;&#26469;&#25913;&#21892;&#33258;&#19979;&#32780;&#19978;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#25193;&#25955;&#27169;&#22411;&#65288;BDM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#36807;&#31243;&#23558;&#33258;&#19978;&#32780;&#19979;&#65288;&#20808;&#39564;&#65289;&#20449;&#24687;&#19982;&#33258;&#19979;&#32780;&#19978;&#65288;&#25968;&#25454;&#39537;&#21160;&#65289;&#36807;&#31243;&#32039;&#23494;&#32806;&#21512;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BDM&#22312;3D&#24418;&#29366;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#22522;&#20110;&#37197;&#23545;&#65288;&#30417;&#30563;&#65289;&#25968;&#25454;&#26631;&#31614;&#65288;&#20363;&#22914;&#22270;&#20687;-&#28857;&#20113;&#65289;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#20856;&#22411;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;BDM&#36890;&#36807;&#24341;&#20837;&#26469;&#33258;&#29420;&#31435;&#26631;&#31614;&#65288;&#20363;&#22914;&#28857;&#20113;&#65289;&#30340;&#20016;&#23500;&#20808;&#39564;&#20449;&#24687;&#26469;&#25913;&#36827;&#33258;&#19979;&#32780;&#19978;&#30340;3D&#37325;&#24314;&#12290;&#19982;&#26631;&#20934;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#30456;&#21453;&#65292;&#20854;&#38656;&#35201;&#26126;&#30830;&#30340;&#20808;&#39564;&#21644;&#20284;&#28982;&#36827;&#34892;&#25512;&#26029;&#65292;BDM&#36890;&#36807;&#23398;&#20064;&#30340;&#26799;&#24230;&#35745;&#31639;&#32593;&#32476;&#36890;&#36807;&#32806;&#21512;&#25193;&#25955;&#36807;&#31243;&#25191;&#34892;&#26080;&#32541;&#20449;&#24687;&#34701;&#21512;&#12290;&#25105;&#20204;BDM&#30340;&#29305;&#27530;&#20043;&#22788;&#22312;&#20110;&#20854;&#33021;&#22815;&#22312;&#33258;&#19978;&#32780;&#19979;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#20027;&#21160;&#26377;&#25928;&#20449;&#24687;&#20132;&#25442;&#21644;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06973v1 Announce Type: cross  Abstract: We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21338;&#24328;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#25439;&#22833;&#20248;&#21270;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#26377;&#38480;&#21046;&#26465;&#20214;&#19979;&#25214;&#21040;&#20102;&#29702;&#35770;&#26368;&#20339;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#20808;&#39564;&#30693;&#35782;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#38543;&#26426;&#21270;&#34920;&#31034;&#30340;&#26377;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06971</link><description>&lt;p&gt;
&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#31867;&#30340;&#34920;&#31034;&#23398;&#20064;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
A representation-learning game for classes of prediction tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21338;&#24328;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#39044;&#27979;&#25439;&#22833;&#20248;&#21270;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#26377;&#38480;&#21046;&#26465;&#20214;&#19979;&#25214;&#21040;&#20102;&#29702;&#35770;&#26368;&#20339;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#20808;&#39564;&#30693;&#35782;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#38543;&#26426;&#21270;&#34920;&#31034;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20165;&#26377;&#20851;&#20110;&#26410;&#26469;&#39044;&#27979;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21338;&#24328;&#30340;&#23398;&#20064;&#38477;&#32500;&#29305;&#24449;&#21521;&#37327;&#34920;&#31034;&#30340;&#24418;&#24335;&#12290;&#22312;&#36825;&#20010;&#21338;&#24328;&#20013;&#65292;&#31532;&#19968;&#20010;&#29609;&#23478;&#36873;&#25321;&#34920;&#31034;&#65292;&#28982;&#21518;&#31532;&#20108;&#20010;&#29609;&#23478;&#20174;&#32473;&#23450;&#31867;&#20013;&#23545;&#25163;&#36873;&#20986;&#19968;&#20010;&#39044;&#27979;&#20219;&#21153;&#65292;&#20195;&#34920;&#20102;&#20808;&#39564;&#30693;&#35782;&#12290;&#31532;&#19968;&#20010;&#29609;&#23478;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#65292;&#31532;&#20108;&#20010;&#29609;&#23478;&#21017;&#26159;&#26368;&#22823;&#21270;&#21518;&#24724;&#65306;&#20351;&#29992;&#34920;&#31034;&#36827;&#34892;&#39044;&#27979;&#30340;&#26368;&#23567;&#25439;&#22833;&#65292;&#19982;&#20351;&#29992;&#21407;&#22987;&#29305;&#24449;&#30456;&#27604;&#30340;&#25439;&#22833;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06971v1 Announce Type: new  Abstract: We propose a game-based formulation for learning dimensionality-reducing representations of feature vectors, when only a prior knowledge on future prediction tasks is available. In this game, the first player chooses a representation, and then the second player adversarially chooses a prediction task from a given class, representing the prior knowledge. The first player aims is to minimize, and the second player to maximize, the regret: The minimal prediction loss using the representation, compared to the same loss using the original features. For the canonical setting in which the representation, the response to predict and the predictors are all linear functions, and under the mean squared error loss function, we derive the theoretically optimal representation in pure strategies, which shows the effectiveness of the prior knowledge, and the optimal regret in mixed strategies, which shows the usefulness of randomizing the representation
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Mixture of Experts&#30340;Di-SkilL&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#27599;&#20010;&#19987;&#23478;&#21644;&#20854;&#30456;&#20851;&#19978;&#19979;&#25991;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#22312;&#30456;&#20284;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.06966</link><description>&lt;p&gt;
&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#30340;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#33719;&#21462;&#22810;&#26679;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06966
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Mixture of Experts&#30340;Di-SkilL&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#27599;&#20010;&#19987;&#23478;&#21644;&#20854;&#30456;&#20851;&#19978;&#19979;&#25991;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#22312;&#30456;&#20284;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#33719;&#21462;&#33391;&#22909;&#34920;&#29616;&#31574;&#30053;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#24120;&#29992;&#30340;&#39640;&#26031;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;RL&#20013;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Di-SkilL&#30340;RL&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#65292;&#20854;&#20013;&#27599;&#20010;&#19987;&#23478;&#23558;&#25216;&#33021;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#19978;&#19979;&#25991;&#36816;&#21160;&#21407;&#35821;&#12290;Di-SkilL&#20248;&#21270;&#27599;&#20010;&#19987;&#23478;&#21450;&#20854;&#30456;&#20851;&#19978;&#19979;&#25991;&#20998;&#24067;&#20197;&#36798;&#21040;&#26368;&#22823;&#29109;&#30446;&#26631;&#65292;&#28608;&#21169;&#22312;&#30456;&#20284;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#22810;&#26679;&#25216;&#33021;&#12290;&#27599;&#20010;&#19987;&#23478;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20351;&#24471;&#33258;&#21160;&#35838;&#31243;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#65292;&#20351;&#27599;&#20010;&#19987;&#23478;&#33021;&#22815;&#19987;&#27880;&#20110;&#20854;&#22312;&#19978;&#19979;&#25991;&#31354;&#38388;&#30340;&#26368;&#20339;&#34920;&#29616;&#23376;&#21306;&#22495;&#12290;&#20026;&#20102;&#20811;&#26381;&#22312;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#29615;&#22659;&#26410;&#30693;&#19978;&#19979;&#25991;&#27010;&#29575;&#31354;&#38388;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#30828;&#24615;&#19981;&#36830;&#32493;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#26469;&#34920;&#31034;&#27599;&#20010;&#19987;&#23478;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06966v1 Announce Type: new  Abstract: Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy. However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization. We propose \textbf{Di}verse \textbf{Skil}l \textbf{L}earning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive. Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts. The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space. To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distri
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.06963</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
The pitfalls of next-token prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06963
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#25285;&#24551;&#65306;&#19968;&#20010;&#20165;&#20165;&#22522;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#24544;&#23454;&#22320;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#32463;&#24120;&#28151;&#28102;&#30340;&#20004;&#20010;&#38454;&#27573; -- &#33258;&#22238;&#24402;&#25512;&#26029;&#21644;&#25945;&#24072;&#24378;&#21046;&#35757;&#32451; -- &#24517;&#39035;&#34987;&#21306;&#21035;&#23545;&#24453;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#19968;&#33324;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#25945;&#24072;&#24378;&#21046;&#22914;&#20309;&#22833;&#36133;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#35745;&#21010;&#20219;&#21153;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;Transformer&#21644;Mamba&#26550;&#26500;&#22312;&#23454;&#36341;&#20013;&#20197;&#36825;&#31181;&#26041;&#24335;&#22833;&#36133; -- &#23613;&#31649;&#20219;&#21153;&#26412;&#36523;&#24456;&#23481;&#26131;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06963v1 Announce Type: cross  Abstract: Can a mere next-token predictor faithfully model human intelligence? We crystallize this intuitive concern, which is fragmented in the literature. As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn. We provide preliminary
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#26032;&#22411;&#20108;&#32500;&#28151;&#21512;&#26377;&#26426;&#26080;&#26426;&#38041;&#38043;&#30719;&#32467;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#21407;&#23376;&#38388;&#21183;&#65292;&#21487;&#20197;&#23454;&#29616;&#21270;&#23398;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.06955</link><description>&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26032;&#22411;&#20108;&#32500;&#28151;&#21512;&#26377;&#26426;&#26080;&#26426;&#38041;&#38043;&#30719;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic Perovskites
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06955
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#26032;&#22411;&#20108;&#32500;&#28151;&#21512;&#26377;&#26426;&#26080;&#26426;&#38041;&#38043;&#30719;&#32467;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#21407;&#23376;&#38388;&#21183;&#65292;&#21487;&#20197;&#23454;&#29616;&#21270;&#23398;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#32500;&#28151;&#21512;&#26377;&#26426;&#26080;&#26426;&#38041;&#38043;&#30719;&#65288;HOIPs&#65289;&#20195;&#34920;&#20102;&#19968;&#31867;&#26082;&#36866;&#29992;&#20110;&#20809;&#21560;&#25910;&#21448;&#36866;&#29992;&#20110;&#21457;&#23556;&#30340;&#30005;&#23376;&#27963;&#24615;&#26448;&#26009;&#12290;HOIPs&#30340;&#35774;&#35745;&#31354;&#38388;&#38750;&#24120;&#24222;&#22823;&#65292;&#22240;&#20026;&#21508;&#31181;&#19981;&#21516;&#30340;&#26377;&#26426;&#38451;&#31163;&#23376;&#21487;&#20197;&#19982;&#19981;&#21516;&#30340;&#26080;&#26426;&#26694;&#26550;&#32467;&#21512;&#12290;&#36825;&#31181;&#24040;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#20351;&#30005;&#23376;&#21644;&#26426;&#26800;&#24615;&#33021;&#21487;&#20197;&#36827;&#34892;&#21487;&#35843;&#65292;&#20294;&#20063;&#38656;&#35201;&#21457;&#23637;&#26032;&#30340;&#24037;&#20855;&#26469;&#25512;&#26029;&#20505;&#36873;&#32467;&#26500;&#30340;&#39640;&#36890;&#37327;&#20998;&#23376;&#21160;&#21147;&#23398;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#12289;&#39640;&#25928;&#12289;&#21487;&#36716;&#31227;&#19988;&#24191;&#27867;&#36866;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#21407;&#23376;&#38388;&#21183;&#65288;MLIP&#65289;&#29992;&#20110;&#39044;&#27979;&#26032;&#22411;&#20108;&#32500;HOIPs&#30340;&#32467;&#26500;&#12290;&#20351;&#29992;MACE&#20307;&#31995;&#32467;&#26500;&#65292;&#22312;86&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#25253;&#36947;&#30340;HOIP&#32467;&#26500;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;MLIP&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;73&#20010;&#26410;&#35265;&#36807;&#30340;&#38041;&#38043;&#30719;&#32452;&#25104;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#21442;&#32771;&#30005;&#23376;&#32467;&#26500;&#26041;&#27861;&#30456;&#27604;&#30340;&#21270;&#23398;&#31934;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32487;&#32493;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06955v1 Announce Type: cross  Abstract: Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent a promising class of electronically active materials for both light absorption and emission. The design space of HOIPs is extremely large, since a diverse space of organic cations can be combined with different inorganic frameworks. This immense design space allows for tunable electronic and mechanical properties, but also necessitates the development of new tools for in silico high throughput analysis of candidate structures. In this work, we present an accurate, efficient, transferable and widely applicable machine learning interatomic potential (MLIP) for predicting the structure of new 2D HOIPs. Using the MACE architecture, an MLIP is trained on 86 diverse experimentally reported HOIP structures. The model is tested on 73 unseen perovskite compositions, and achieves chemical accuracy with respect to the reference electronic structure method. Our model is then co
&lt;/p&gt;</description></item><item><title>SELMA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#25216;&#33021;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#65292;&#20174;&#32780;&#25913;&#36827;T2I&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.06952</link><description>&lt;p&gt;
SELMA&#65306;&#23398;&#20064;&#21644;&#21512;&#24182;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#30340;&#25216;&#33021;&#29305;&#23450;&#25991;&#26412;&#21040;&#22270;&#20687;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06952
&lt;/p&gt;
&lt;p&gt;
SELMA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#25216;&#33021;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#65292;&#20174;&#32780;&#25913;&#36827;T2I&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21019;&#24314;&#22270;&#20687;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;T2I&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#31934;&#30830;&#21305;&#37197;&#25991;&#26412;&#36755;&#20837;&#32454;&#33410;&#30340;&#22270;&#20687;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#19981;&#20339;&#65292;&#27604;&#22914;&#19981;&#27491;&#30830;&#30340;&#31354;&#38388;&#20851;&#31995;&#25110;&#32570;&#22833;&#23545;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SELMA&#65306;&#20855;&#26377;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#30340;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#65292;&#36825;&#26159;&#19968;&#31181;&#25913;&#36827;T2I&#27169;&#22411;&#24544;&#23454;&#24230;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#25216;&#33021;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#25216;&#33021;&#29305;&#23450;&#19987;&#23478;&#23398;&#20064;&#21644;&#21512;&#24182;&#12290;&#39318;&#20808;&#65292;SELMA&#21033;&#29992;LLM&#30340;&#29615;&#22659;&#23398;&#20064;&#33021;&#21147;&#29983;&#25104;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25945;&#25480;&#19981;&#21516;&#30340;&#25216;&#33021;&#65292;&#28982;&#21518;&#22522;&#20110;&#25552;&#31034;&#20351;&#29992;T2I&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#12290;&#25509;&#19979;&#26469;&#65292;SELMA&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#21333;&#25216;&#33021;&#30340;LoRA&#65288;&#20302;&#31209;&#35843;&#25972;&#65289;&#19987;&#23478;&#35843;&#25972;T2I&#27169;&#22411;&#21040;&#26032;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06952v1 Announce Type: cross  Abstract: Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts follow
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#32593;&#30417;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#21644;&#25925;&#38556;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#23545;&#20256;&#32479;&#30417;&#25511;&#31995;&#32479;&#30340;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2403.06942</link><description>&lt;p&gt;
&#20351;&#29992;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#30005;&#32593;&#30417;&#27979;&#21644;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06942
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#32593;&#30417;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#21644;&#25925;&#38556;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#23545;&#20256;&#32479;&#30417;&#25511;&#31995;&#32479;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19979;&#19968;&#20195;&#30005;&#32593;&#30417;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#30340;&#26696;&#20363;&#65292;&#21033;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#25512;&#26029;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;AI&#25903;&#25345;&#30340;&#25968;&#25454;&#21387;&#32553;&#21644;&#25925;&#38556;&#26816;&#27979;&#30340;&#30417;&#27979;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#22522;&#20110;SCADA&#21644;&#21516;&#27493;&#30456;&#37327;&#25216;&#26415;&#26500;&#24314;&#30340;&#24191;&#22495;&#30417;&#27979;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06942v1 Announce Type: cross  Abstract: Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in generative artificial intelligence (AI), machine learning, and statistical inference. Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection.   Methods and Results: The architecture of the proposed design originates from the Wiener-Kallianpur innovation representation of a random process that transforms causally a stationary random process into an innovation sequence with independent and identically distributed random variables. This work presents a generative AI approach that (i) learns an innovation autoencoder that extracts innovation se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#38024;&#23545;&#30382;&#36136;&#21402;&#24230;&#36712;&#36857;&#39044;&#27979;&#65292;&#22312;&#24212;&#23545;&#25968;&#25454;&#32570;&#22833;&#21644;&#23545;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.06940</link><description>&lt;p&gt;
&#29992;&#20110;&#30382;&#36136;&#21402;&#24230;&#36712;&#36857;&#39044;&#27979;&#30340;&#26465;&#20214;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#38024;&#23545;&#30382;&#36136;&#21402;&#24230;&#36712;&#36857;&#39044;&#27979;&#65292;&#22312;&#24212;&#23545;&#25968;&#25454;&#32570;&#22833;&#21644;&#23545;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#19968;&#31181;&#20197;&#20010;&#20307;&#20043;&#38388;&#30149;&#31243;&#36827;&#23637;&#36895;&#29575;&#19981;&#21516;&#20026;&#29305;&#24449;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#30382;&#36136;&#21402;&#24230;&#65288;CTh&#65289;&#30340;&#21464;&#21270;&#19982;&#30142;&#30149;&#36827;&#23637;&#23494;&#20999;&#30456;&#20851;&#12290;&#20934;&#30830;&#39044;&#27979;CTh&#36712;&#36857;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26089;&#26399;&#35786;&#26029;&#21644;&#24178;&#39044;&#31574;&#30053;&#65292;&#20026;&#24739;&#32773;&#25552;&#20379;&#21450;&#26102;&#25252;&#29702;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#36825;&#20123;&#30740;&#31350;&#30340;&#32437;&#21521;&#25968;&#25454;&#36890;&#24120;&#23384;&#22312;&#26102;&#38388;&#31232;&#30095;&#21644;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#22312;&#20934;&#30830;&#24314;&#27169;&#30142;&#30149;&#36827;&#23637;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#23616;&#38480;&#24615;&#36739;&#22823;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27809;&#26377;&#32570;&#22833;&#26465;&#30446;&#30340;&#25968;&#25454;&#38598;&#19978;&#25110;&#35201;&#27714;&#23545;CTh&#36827;&#23637;&#36827;&#34892;&#39044;&#23450;&#20041;&#20551;&#35774;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#24102;&#26377;&#22522;&#32447;&#20449;&#24687;&#65288;&#22914;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#21021;&#27493;&#35786;&#26029;&#65289;&#30340;CTh&#36712;&#36857;&#30340;&#26465;&#20214;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06940v1 Announce Type: cross  Abstract: Alzheimer's Disease (AD) is a neurodegenerative condition characterized by diverse progression rates among individuals, with changes in cortical thickness (CTh) closely linked to its progression. Accurately forecasting CTh trajectories can significantly enhance early diagnosis and intervention strategies, providing timely care. However, the longitudinal data essential for these studies often suffer from temporal sparsity and incompleteness, presenting substantial challenges in modeling the disease's progression accurately. Existing methods are limited, focusing primarily on datasets without missing entries or requiring predefined assumptions about CTh progression. To overcome these obstacles, we propose a conditional score-based diffusion model specifically designed to generate CTh trajectories with the given baseline information, such as age, sex, and initial diagnosis. Our conditional diffusion model utilizes all available data durin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#20219;&#21153;CFKGR&#65292;&#26412;&#25991;&#23558;&#30693;&#35782;&#22270;&#34917;&#20840;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#20551;&#35774;&#21069;&#25552;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;COULDD&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#34920;&#26126;KGEs&#21487;&#20197;&#23398;&#20064;&#22270;&#20013;&#30340;&#27169;&#24335;&#65292;&#26816;&#27979;&#20986;&#21512;&#29702;&#30340;&#21453;&#20107;&#23454;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.06936</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Reasoning with Knowledge Graph Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06936
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#20219;&#21153;CFKGR&#65292;&#26412;&#25991;&#23558;&#30693;&#35782;&#22270;&#34917;&#20840;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#20551;&#35774;&#21069;&#25552;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;COULDD&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#34920;&#26126;KGEs&#21487;&#20197;&#23398;&#20064;&#22270;&#20013;&#30340;&#27169;&#24335;&#65292;&#26816;&#27979;&#20986;&#21512;&#29702;&#30340;&#21453;&#20107;&#23454;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGEs&#65289;&#26368;&#21021;&#26159;&#20026;&#20102;&#25512;&#26029;&#19981;&#23436;&#25972;&#30693;&#35782;&#24211;&#20013;&#32570;&#22833;&#30340;&#30495;&#23454;&#20107;&#23454;&#32780;&#24320;&#21457;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#20219;&#21153;CFKGR&#23558;&#30693;&#35782;&#22270;&#34917;&#20840;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#19990;&#30028;&#29366;&#24577;&#24314;&#27169;&#20026;&#30693;&#35782;&#22270;&#65292;&#20551;&#35774;&#24773;&#26223;&#20026;&#28155;&#21152;&#21040;&#22270;&#20013;&#30340;&#36793;&#65292;&#23545;&#22270;&#30340;&#21512;&#29702;&#21464;&#21270;&#20026;&#36923;&#36753;&#35268;&#21017;&#25512;&#29702;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#20551;&#35774;&#24773;&#26223;&#21450;&#23545;&#21407;&#22987;&#30693;&#35782;&#22270;&#30340;&#21512;&#29702;&#25913;&#21464;&#20197;&#21450;&#24212;&#35813;&#20445;&#30041;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;COULDD&#65292;&#19968;&#31181;&#38024;&#23545;&#29305;&#23450;&#20551;&#35774;&#21069;&#25552;&#35843;&#25972;&#29616;&#26377;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;KGEs&#21487;&#20197;&#22312;&#27809;&#26377;&#26174;&#24335;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20174;&#22270;&#20013;&#23398;&#20064;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;COULDD&#35843;&#25972;&#21518;&#30340;KGEs&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#36981;&#24490;&#36923;&#36753;&#35268;&#21017;&#30340;&#22270;&#30340;&#21512;&#29702;&#21453;&#20107;&#23454;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06936v1 Announce Type: cross  Abstract: Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories. In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR. We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules. We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained. We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark. Our results indicate that KGEs learn patterns in the graph without explicit training. We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow the
&lt;/p&gt;</description></item><item><title>Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06925</link><description>&lt;p&gt;
Transformers&#23398;&#20064;&#20302;&#25935;&#24863;&#24615;&#20989;&#25968;&#30340;&#31616;&#21333;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Simplicity Bias of Transformers to Learn Low Sensitivity Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06925
&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#23427;&#20204;&#20855;&#26377;&#30340;&#24402;&#32435;&#20559;&#24046;&#20197;&#21450;&#36825;&#20123;&#20559;&#24046;&#22914;&#20309;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19981;&#21516;&#30340;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#23545;&#36755;&#20837;&#20013;&#30340;&#38543;&#26426;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#27010;&#24565;&#21270;&#20026;&#19968;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#30340;&#27010;&#24565;&#65292;&#36825;&#20026;&#35299;&#37322;transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#30340;&#31616;&#21333;&#24615;&#21644;&#35889;&#20559;&#24046;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;transformers&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#27604;&#20854;&#20182;&#26367;&#20195;&#26550;&#26500;&#65288;&#22914;LSTMs&#12289;MLPs&#21644;CNNs&#65289;&#20855;&#26377;&#26356;&#20302;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20302;&#25935;&#24863;&#24615;&#20559;&#24046;&#19982;&#25913;&#36827;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06925v1 Announce Type: cross  Abstract: Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities. We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that low-sensitivity bias correlates with impro
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#38754;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#38416;&#26126;&#20102;&#24403;&#21069;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#29702;&#35299;&#65292;&#26088;&#22312;&#24110;&#21161;&#31435;&#27861;&#32773;&#21644;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#22312;AI&#30417;&#31649;&#39046;&#22495;&#20570;&#20986;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.06910</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Responsible Artificial Intelligence: A Structured Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06910
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#38754;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#38416;&#26126;&#20102;&#24403;&#21069;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#29702;&#35299;&#65292;&#26088;&#22312;&#24110;&#21161;&#31435;&#27861;&#32773;&#21644;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#22312;AI&#30417;&#31649;&#39046;&#22495;&#20570;&#20986;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06910v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#25105;&#20204;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25512;&#36827;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#27010;&#24565;&#65292;&#22312;&#27431;&#30431;&#25919;&#31574;&#35752;&#35770;&#20013;&#26085;&#30410;&#37325;&#35201;&#12290;&#27431;&#30431;&#26368;&#36817;&#21457;&#24067;&#20102;&#20960;&#20221;&#24378;&#35843;AI&#20449;&#20219;&#24517;&#35201;&#24615;&#30340;&#20986;&#29256;&#29289;&#65292;&#24378;&#35843;AI&#20316;&#20026;&#26377;&#30410;&#24037;&#20855;&#21644;&#28508;&#22312;&#27494;&#22120;&#30340;&#21452;&#37325;&#24615;&#36136;&#12290;&#36825;&#31181;&#20108;&#20803;&#24615;&#31361;&#26174;&#20102;&#22269;&#38469;&#30417;&#31649;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#38656;&#35201;&#25351;&#23548;&#20844;&#21496;&#22312;AI&#21457;&#23637;&#20013;&#36981;&#23432;&#36825;&#20123;&#35268;&#23450;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#24110;&#21161;&#31435;&#27861;&#32773;&#21644;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#22312;AI&#30417;&#31649;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#23548;&#33322;&#65292;&#30830;&#23450;&#26410;&#26469;&#20851;&#27880;&#30340;&#28966;&#28857;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36127;&#36131;&#20219;AI&#30340;&#20840;&#38754;&#19988;&#25454;&#25105;&#20204;&#25152;&#30693;&#31532;&#19968;&#20010;&#32479;&#19968;&#23450;&#20041;&#12290;&#36890;&#36807;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#23545;&#36127;&#36131;&#20219;AI&#30340;&#24403;&#21069;&#29702;&#35299;&#12290;&#20511;&#37492;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06910v1 Announce Type: new  Abstract: Our research endeavors to advance the concept of responsible artificial intelligence (AI), a topic of increasing importance within EU policy discussions. The EU has recently issued several publications emphasizing the necessity of trust in AI, underscoring the dual nature of AI as both a beneficial tool and a potential weapon. This dichotomy highlights the urgent need for international regulation. Concurrently, there is a need for frameworks that guide companies in AI development, ensuring compliance with such regulations. Our research aims to assist lawmakers and machine learning practitioners in navigating the evolving landscape of AI regulation, identifying focal areas for future attention. This paper introduces a comprehensive and, to our knowledge, the first unified definition of responsible AI. Through a structured literature review, we elucidate the current understanding of responsible AI. Drawing from this analysis, we propose an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.06906</link><description>&lt;p&gt;
&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22312;&#32771;&#34385;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#25512;&#36831;&#22810;&#20301;&#19987;&#23478;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06906
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25512;&#36831;&#65288;L2D&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#31995;&#32479;&#20013;&#23558;&#20915;&#31574;&#25512;&#36831;&#32473;&#20154;&#31867;&#65292;&#20174;&#32780;&#22312;&#20154;&#31867;&#26356;&#26377;&#21487;&#33021;&#27491;&#30830;&#26102;&#25512;&#36831;&#20915;&#31574;&#12290;&#29616;&#26377;L2D&#30740;&#31350;&#24573;&#35270;&#20102;&#38459;&#30861;&#20854;&#23454;&#38469;&#37319;&#29992;&#30340;&#30495;&#23454;&#31995;&#32479;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21363;&#65306;&#24573;&#35270;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#65292;&#20854;&#20013;&#31532;1&#31867;&#21644;&#31532;2&#31867;&#38169;&#35823;&#30340;&#25104;&#26412;&#19981;&#21516;&#65307;&#35201;&#27714;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#24182;&#21457;&#20154;&#31867;&#39044;&#27979;&#65307;&#19981;&#22788;&#29702;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#12290;DeCCaF&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;L2D&#26041;&#27861;&#65292;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#24314;&#27169;&#20154;&#31867;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#20943;&#23569;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#24182;&#20351;&#29992;&#32422;&#26463;&#32534;&#31243;&#26469;&#20840;&#23616;&#26368;&#23567;&#21270;&#38169;&#35823;&#25104;&#26412;&#65292;&#21516;&#26102;&#32771;&#34385;&#24037;&#20316;&#37327;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#31995;&#21015;&#20013;&#27979;&#35797;&#20102;DeCCaF
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06906v1 Announce Type: cross  Abstract: Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#65292;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#23545;&#20110;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#39640;SNR&#20540;&#20250;&#23548;&#33268;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#20302;SNR&#20540;&#21017;&#20250;&#23548;&#33268;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.06903</link><description>&lt;p&gt;
&#20855;&#26377;&#36866;&#24230;&#36755;&#20837;&#32500;&#24230;&#30340;&#27844;&#28431;ReLU&#32593;&#32476;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Benign overfitting in leaky ReLU networks with moderate input dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06903
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#65292;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#23545;&#20110;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#39640;SNR&#20540;&#20250;&#23548;&#33268;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#20302;SNR&#20540;&#21017;&#20250;&#23548;&#33268;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#25506;&#35752;&#20102;&#19968;&#20010;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#32654;&#22320;&#25311;&#21512;&#22024;&#26434;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#21448;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20108;&#23618;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38024;&#23545;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#32771;&#34385;&#36755;&#20837;&#25968;&#25454;&#65292;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#20849;&#21516;&#20449;&#21495;&#21644;&#19968;&#20010;&#38543;&#26426;&#22122;&#22768;&#25104;&#20998;&#30340;&#24635;&#21644;&#65292;&#36825;&#20004;&#32773;&#30456;&#20114;&#27491;&#20132;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#65292;&#23548;&#33268;&#20102;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#65288;&#26377;&#23475;&#65289;&#36807;&#25311;&#21512;&#65306;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;SNR&#24456;&#39640;&#65292;&#21017;&#21457;&#29983;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#30456;&#21453;&#65292;&#22914;&#26524;SNR&#24456;&#20302;&#65292;&#21017;&#21457;&#29983;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#23558;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#24402;&#22240;&#20110;&#19968;&#20010;&#36817;&#20284;&#36793;&#30028;&#26368;&#22823;&#21270;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38128;&#38142;&#25439;&#22833;&#19979;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#27844;&#28431;ReLU&#32593;&#32476;&#28385;&#36275;&#36825;&#19968;&#24615;&#36136;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;nea
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06903v1 Announce Type: new  Abstract: The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well. We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property. In contrast to prior work we do not require nea
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#29289;&#21147;&#23398;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;LIBR+&#26041;&#27861;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26415;&#20013;&#32925;&#33039;&#37197;&#20934;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#26415;&#20013;&#27979;&#37327;&#30340;&#31232;&#30095;&#24615;&#21644;&#21464;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06901</link><description>&lt;p&gt;
LIBR+: &#36890;&#36807;&#23398;&#20064;&#22522;&#20110;&#29983;&#29289;&#21147;&#23398;&#21464;&#24418;&#30340;&#24046;&#20540;&#26469;&#25913;&#36827;&#26415;&#20013;&#32925;&#33039;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06901
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#29289;&#21147;&#23398;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;LIBR+&#26041;&#27861;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26415;&#20013;&#32925;&#33039;&#37197;&#20934;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#26415;&#20013;&#27979;&#37327;&#30340;&#31232;&#30095;&#24615;&#21644;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#29615;&#22659;&#23545;&#26415;&#20013;&#22120;&#23448;&#24418;&#29366;&#19982;&#26415;&#21069;&#25104;&#20687;&#20960;&#20309;&#24418;&#29366;&#30340;&#37197;&#20934;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#29983;&#29289;&#21147;&#23398;&#27169;&#22411;&#30340;&#37197;&#20934;&#20173;&#28982;&#24456;&#27969;&#34892;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#26415;&#20013;&#27979;&#37327;&#30340;&#31232;&#30095;&#24615;&#21644;&#21464;&#24322;&#24615;&#20197;&#21450;&#25163;&#26415;&#36807;&#31243;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#26377;&#38480;&#22120;&#23448;&#22320;&#38754;&#30495;&#23454;&#21464;&#24418;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#28151;&#21512;&#8221;&#37197;&#20934;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#32447;&#24615;&#24377;&#24615;&#29983;&#29289;&#21147;&#23398;&#30340;&#32447;&#24615;&#21270;&#36845;&#20195;&#36793;&#30028;&#37325;&#24314;&#65288;LIBR&#65289;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20854;&#27531;&#24046;&#20197;&#33719;&#21462;&#22320;&#38754;&#30495;&#23454;&#21464;&#24418;&#65288;LIBR+&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21046;&#23450;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#26679;&#26465;&#27531;&#24046;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;SR-GCN&#65289;&#26469;&#21560;&#25910;&#31232;&#30095;&#21644;&#21464;&#37327;&#26415;&#20013;&#27979;&#37327;&#30340;&#20449;&#24687;&#65292;&#24182;&#26377;&#25928;&#22320;&#36890;&#36807;3D&#22120;&#23448;&#30340;&#20960;&#20309;&#24418;&#29366;&#20256;&#25773;&#12290;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06901v1 Announce Type: cross  Abstract: The surgical environment imposes unique challenges to the intraoperative registration of organ shapes to their preoperatively-imaged geometry. Biomechanical model-based registration remains popular, while deep learning solutions remain limited due to the sparsity and variability of intraoperative measurements and the limited ground-truth deformation of an organ that can be obtained during the surgery. In this paper, we propose a novel \textit{hybrid} registration approach that leverage a linearized iterative boundary reconstruction (LIBR) method based on linear elastic biomechanics, and use deep neural networks to learn its residual to the ground-truth deformation (LIBR+). We further formulate a dual-branch spline-residual graph convolutional neural network (SR-GCN) to assimilate information from sparse and variable intraoperative measurements and effectively propagate it through the geometry of the 3D organ. Experiments on a large int
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#24352;&#37327;&#32593;&#32476;&#24191;&#27867;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#23376;&#22686;&#24378;&#22788;&#29702;&#33021;&#21147;&#26377;&#25928;&#24212;&#23545;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;</title><link>https://arxiv.org/abs/2403.06890</link><description>&lt;p&gt;
&#37327;&#23376;&#24352;&#37327;&#32593;&#32476;&#22312;&#34507;&#30333;&#36136;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Quantum Tensor Networks for Protein Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06890
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#24352;&#37327;&#32593;&#32476;&#24191;&#27867;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#23376;&#22686;&#24378;&#22788;&#29702;&#33021;&#21147;&#26377;&#25928;&#24212;&#23545;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#34507;&#30333;&#36136;&#24207;&#21015;&#21487;&#20197;&#34987;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#21477;&#23376;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#26694;&#26550;&#35299;&#26512;&#25104;&#21512;&#29702;&#37327;&#23376;&#27604;&#29305;&#30340;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65292;&#36825;&#20123;&#30005;&#36335;&#21487;&#20197;&#34987;&#35757;&#32451;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#19982;&#34507;&#30333;&#36136;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#22522;&#20110;&#34507;&#30333;&#36136;&#30340;&#32454;&#32990;&#20122;&#32454;&#32990;&#20301;&#32622;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#26159;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#29983;&#29289;&#36807;&#31243;&#21644;&#30142;&#30149;&#26426;&#21046;&#12290;&#21033;&#29992;&#37327;&#23376;&#22686;&#24378;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37327;&#23376;&#24352;&#37327;&#32593;&#32476;&#65288;QTN&#65289;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35814;&#32454;&#30340;&#26041;&#27861;&#35770;&#65292;&#23558;QTN&#26550;&#26500;&#35843;&#25972;&#21040;&#34507;&#30333;&#36136;&#25968;&#25454;&#30340;&#24494;&#22937;&#35201;&#27714;&#65292;&#25903;&#25345;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21463;&#20256;&#32479;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21551;&#21457;&#30340;&#20004;&#31181;&#19981;&#21516;&#30340;QTNs&#65292;&#20197;&#35299;&#20915;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06890v1 Announce Type: cross  Abstract: We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems. We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences. We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results. We demonstrate two distinct QTNs, inspired by classical recurrent neural networks (RNN) and convolutional neural networks (CNN), to solve th
&lt;/p&gt;</description></item><item><title>HiRA-Pro&#26159;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#23545;&#40784;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#30340;&#36807;&#31243;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23545;&#40784;&#20855;&#26377;&#20122;&#27627;&#31186;&#29616;&#35937;&#30340;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#26234;&#33021;&#21046;&#36896;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.06888</link><description>&lt;p&gt;
HiRA-Pro: &#39640;&#20998;&#36776;&#29575;&#23545;&#40784;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#30340;&#36807;&#31243;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HiRA-Pro: High resolution alignment of multimodal spatio-temporal data: a process physics driven approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06888
&lt;/p&gt;
&lt;p&gt;
HiRA-Pro&#26159;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#23545;&#40784;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#30340;&#36807;&#31243;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23545;&#40784;&#20855;&#26377;&#20122;&#27627;&#31186;&#29616;&#35937;&#30340;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#26234;&#33021;&#21046;&#36896;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HiRA-Pro&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#31243;&#24207;&#65292;&#29992;&#20110;&#22312;&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#19979;&#23545;&#26469;&#33258;&#23637;&#31034;&#22810;&#26679;&#30636;&#24577;&#12289;&#38750;&#32447;&#24615;&#38543;&#26426;&#21160;&#24577;&#30340;&#30495;&#23454;&#19990;&#30028;&#36807;&#31243;&#21644;&#31995;&#32479;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#36827;&#34892;&#23545;&#40784;&#65292;&#20363;&#22914;&#21046;&#36896;&#26426;&#22120;&#12290;&#23427;&#22522;&#20110;&#35782;&#21035;&#21644;&#21516;&#27493;&#36825;&#20123;&#19981;&#21516;&#20449;&#21495;&#20013;&#26174;&#33879;&#36816;&#21160;&#23398;&#21644;&#21160;&#21147;&#23398;&#20107;&#20214;&#30340;&#36807;&#31243;&#29305;&#24449;&#12290;HiRA-Pro&#35299;&#20915;&#20102;&#23545;&#40784;&#20855;&#26377;&#20122;&#27627;&#31186;&#29616;&#35937;&#30340;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32780;&#20256;&#32479;&#30340;&#26102;&#38388;&#25139;&#12289;&#22806;&#37096;&#35302;&#21457;&#22120;&#25110;&#22522;&#20110;&#26102;&#38047;&#30340;&#23545;&#40784;&#26041;&#27861;&#21017;&#38590;&#20197;&#32988;&#20219;&#12290;HiRA-Pro&#30340;&#26377;&#25928;&#24615;&#22312;&#26234;&#33021;&#21046;&#36896;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#23637;&#31034;&#65292;&#22312;&#36825;&#37324;&#65292;&#23427;&#23545;&#26469;&#33258;Optomec-LENS MTS 500&#28151;&#21512;&#26426;&#22120;&#36827;&#34892;3D&#25171;&#21360;&#21644;&#38115;&#21066;&#25805;&#20316;&#26399;&#38388;&#33719;&#21462;&#30340;13+&#36890;&#36947;&#25968;&#25454;&#36827;&#34892;&#20102;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#23545;&#40784;&#25968;&#25454;&#34987;&#20307;&#32032;&#21270;&#65292;&#29983;&#25104;&#23545;&#24212;&#20110;&#25152;&#21046;&#36896;&#38646;&#20214;&#19978;&#30340;&#29289;&#29702;&#20307;&#32032;&#30340;0.25&#31186;&#23545;&#40784;&#25968;&#25454;&#22359;&#12290;HiRA-Pro&#30340;&#20248;&#36234;&#24615;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#36827;&#19968;&#27493;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06888v1 Announce Type: cross  Abstract: We present HiRA-Pro, a novel procedure to align, at high spatio-temporal resolutions, multimodal signals from real-world processes and systems that exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing machines. It is based on discerning and synchronizing the process signatures of salient kinematic and dynamic events in these disparate signals. HiRA-Pro addresses the challenge of aligning data with sub-millisecond phenomena, where traditional timestamp, external trigger, or clock-based alignment methods fall short. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing context, where it aligns data from 13+ channels acquired during 3D-printing and milling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned data is then voxelized to generate 0.25 second aligned data chunks that correspond to physical voxels on the produced part. The superiority of HiRA-Pro is further showcased thro
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22914;&#20309;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#21457;&#29616;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06880</link><description>&lt;p&gt;
&#25581;&#31034;&#21463;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22312;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06880
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#22914;&#20309;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#21457;&#29616;&#20102;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24188;&#20799;&#20174;&#31232;&#30095;&#21453;&#39304;&#30340;&#33258;&#30001;&#25506;&#32034;&#36880;&#28176;&#21457;&#23637;&#20026;&#21033;&#29992;&#20808;&#21069;&#32463;&#39564;&#36827;&#34892;&#20197;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#23398;&#20064;&#65292;&#33719;&#24471;&#26356;&#23494;&#38598;&#22870;&#21169;&#12290;&#21463;&#27492;&#24188;&#20799;&#21551;&#21457;&#30340;&#22870;&#21169;&#36716;&#25442;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#19981;&#21516;&#22870;&#21169;&#36716;&#25442;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#24847;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#20174;&#31232;&#30095;&#21040;&#22522;&#20110;&#28508;&#22312;&#30340;&#23494;&#38598;&#22870;&#21169;&#30340;&#36716;&#25442;&#65292;&#36825;&#20004;&#32773;&#20849;&#20139;&#26080;&#35770;&#22870;&#21169;&#21464;&#21270;&#22343;&#20026;&#26368;&#20339;&#31574;&#30053;&#12290;&#36890;&#36807;&#21253;&#25324;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#23548;&#33322;&#21644;&#26426;&#26800;&#33218;&#25805;&#20316;&#20219;&#21153;&#22312;&#20869;&#30340;&#21508;&#31181;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22870;&#21169;&#36716;&#25442;&#26174;&#33879;&#24433;&#21709;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#12290;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#21463;&#24188;&#20799;&#21551;&#21457;&#30340;&#31232;&#30095;&#36716;&#23494;&#38598;&#65288;S2D&#65289;&#36716;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#36825;&#20123;&#24615;&#33021;&#25351;&#26631;&#22806;&#65292;&#20351;&#29992;&#20132;&#21449;&#23494;&#24230;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36716;&#25442;&#65292;&#29305;&#21035;&#26159;S2D&#65292;&#20351;&#31574;&#30053;&#25439;&#22833;&#26223;&#35266;&#26356;&#21152;&#24179;&#28369;&#65292;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06880v1 Announce Type: cross  Abstract: Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20010;&#20307;OOD&#24230;&#37327;&#32467;&#21512;&#25104;&#19968;&#20010;&#32452;&#21512;OOD&#24230;&#37327;&#65288;COOD&#65289;&#30340;&#26694;&#26550;&#65292;&#22312;&#22823;&#35268;&#27169;&#29983;&#29289;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#34920;&#29616;&#20986;&#22312;TPR@1% FPR&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20010;&#20307;OOD&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.06874</link><description>&lt;p&gt;
COOD&#65306;&#20351;&#29992;&#22810;&#31181;&#24230;&#37327;&#32467;&#21512;&#36827;&#34892;&#22823;&#35268;&#27169;&#23618;&#27425;&#20998;&#31867;&#20013;&#30340;&#24322;&#24120;&#21644;&#26032;&#31867;&#21035;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
COOD: Combined out-of-distribution detection using multiple measures for anomaly &amp; novel class detection in large-scale hierarchical classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20010;&#20307;OOD&#24230;&#37327;&#32467;&#21512;&#25104;&#19968;&#20010;&#32452;&#21512;OOD&#24230;&#37327;&#65288;COOD&#65289;&#30340;&#26694;&#26550;&#65292;&#22312;&#22823;&#35268;&#27169;&#29983;&#29289;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#34920;&#29616;&#20986;&#22312;TPR@1% FPR&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20010;&#20307;OOD&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24615;&#33021;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#65292;&#21253;&#25324;&#24322;&#24120;&#21644;&#26032;&#31867;&#21035;&#65292;&#26159;&#20998;&#31867;&#27169;&#22411;&#23454;&#38469;&#24212;&#29992;&#30340;&#37325;&#35201;&#20808;&#20915;&#26465;&#20214;&#12290;&#26412;&#25991;&#20851;&#27880;&#22270;&#20687;&#20013;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#65292;&#28041;&#21450;&#22823;&#22411;&#25968;&#25454;&#24211;&#12289;&#22823;&#37327;&#32454;&#31890;&#24230;&#23618;&#27425;&#31867;&#12289;&#20005;&#37325;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20197;&#21450;&#19981;&#21516;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#20010;&#20307;OOD&#24230;&#37327;&#32467;&#21512;&#25104;&#19968;&#20010;&#32452;&#21512;OOD&#65288;COOD&#65289;&#24230;&#37327;&#65292;&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#12290;&#20010;&#20307;&#24230;&#37327;&#21253;&#25324;&#20960;&#20010;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#24230;&#37327;&#21644;&#20960;&#20010;&#26032;&#39062;&#30340;OOD&#24230;&#37327;&#65292;&#36825;&#20123;&#24230;&#37327;&#26159;&#20026;&#20102;&#26032;&#31867;&#21035;&#26816;&#27979;&#21644;&#20998;&#23618;&#31867;&#32467;&#26500;&#32780;&#24320;&#21457;&#30340;&#12290;COOD&#22312;&#19977;&#20010;&#22823;&#35268;&#27169;&#65288;500k+&#22270;&#20687;&#65289;&#29983;&#29289;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#29992;&#20110;&#24322;&#24120;&#21644;&#26032;&#31867;&#21035;&#26816;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;COOD&#22312;TPR@1% FPR&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20010;&#20307;&#24230;&#37327;&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06874v1 Announce Type: cross  Abstract: High-performing out-of-distribution (OOD) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models. In this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality. We propose a framework for combining individual OOD measures into one combined OOD (COOD) measure using a supervised model. The individual measures are several existing state-of-the-art measures and several novel OOD measures developed with novel class detection and hierarchical class structure in mind. COOD was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection. We show that COOD outperforms individual, including state-of-the-art, OOD measures by a large margin in terms of TPR@1% FPR in the majority 
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#32487;&#32493;&#23398;&#20064;&#24212;&#29992;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#33719;&#24471;&#20102;&#22686;&#37327;&#26799;&#24230;&#21644;&#22686;&#37327;&#36817;&#31471;&#26041;&#27861;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#19988;&#20854;&#39044;&#26399;&#22797;&#26434;&#24230;&#30028;&#38480;&#20960;&#20046;&#19982;&#24050;&#30693;&#26368;&#20339;&#24179;&#22343;&#36845;&#20195;&#30340;&#30028;&#38480;&#30456;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2403.06873</link><description>&lt;p&gt;
&#22686;&#37327;&#26041;&#27861;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#24615;&#21450;&#22312;&#32487;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Last Iterate Convergence of Incremental Methods and Applications in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06873
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32487;&#32493;&#23398;&#20064;&#24212;&#29992;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#33719;&#24471;&#20102;&#22686;&#37327;&#26799;&#24230;&#21644;&#22686;&#37327;&#36817;&#31471;&#26041;&#27861;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#19988;&#20854;&#39044;&#26399;&#22797;&#26434;&#24230;&#30028;&#38480;&#20960;&#20046;&#19982;&#24050;&#30693;&#26368;&#20339;&#24179;&#22343;&#36845;&#20195;&#30340;&#30028;&#38480;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#26799;&#24230;&#26041;&#27861;&#21644;&#22686;&#37327;&#36817;&#31471;&#26041;&#27861;&#26159;&#19968;&#31867;&#22522;&#26412;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24191;&#27867;&#30740;&#31350;&#30340;&#26377;&#38480;&#21644;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23601;&#25910;&#25947;&#24615;&#20445;&#35777;&#32780;&#35328;&#65292;&#38750;&#28176;&#36827;&#65288;&#19968;&#38454;&#25110;&#36817;&#31471;&#65289;&#30340;&#39044;&#26399;&#22797;&#26434;&#24615;&#30028;&#38480;&#26368;&#36817;&#25165;&#24471;&#21040;&#65292;&#24182;&#19988;&#20960;&#20046;&#20165;&#36866;&#29992;&#20110;&#24179;&#22343;&#36845;&#20195;&#12290;&#21463;&#32487;&#32493;&#23398;&#20064;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#23545;&#19968;&#33324;&#20984;&#24179;&#28369;&#65288;&#20004;&#32773;&#65289;&#21644;&#20984;Lipschitz&#65288;&#23545;&#20110;&#36817;&#31471;&#21464;&#31181;&#65289;&#35774;&#32622;&#20013;&#22686;&#37327;&#26799;&#24230;&#21644;&#22686;&#37327;&#36817;&#31471;&#26041;&#27861;&#30340;&#26368;&#21518;&#36845;&#20195;&#30340;&#39318;&#20010;&#25910;&#25947;&#20445;&#35777;&#12290;&#25105;&#20204;&#23545;&#26368;&#21518;&#36845;&#20195;&#30340;&#39044;&#26399;&#22797;&#26434;&#24615;&#30028;&#38480;&#20960;&#20046;&#19982;&#26368;&#20339;&#24050;&#30693;&#30340;&#24179;&#22343;&#36845;&#20195;&#30340;&#39044;&#26399;&#22797;&#26434;&#24615;&#30028;&#38480;&#30456;&#21305;&#37197;&#65288;&#21363;&#21305;&#37197;&#33267;&#24179;&#26041;&#26681;&#23545;&#25968;&#25110;&#23545;&#25968;&#22240;&#23376;&#65289;&#65292;&#23545;&#20004;&#31867;&#26041;&#27861;&#22343;&#36866;&#29992;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#21152;&#26435;&#24179;&#22343;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06873v1 Announce Type: cross  Abstract: Incremental gradient methods and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, broadly studied in the literature. Yet, when it comes to their convergence guarantees, nonasymptotic (first-order or proximal) oracle complexity bounds have been obtained fairly recently, almost exclusively applying to the average iterate. Motivated by applications in continual learning, we obtain the first convergence guarantees for the last iterate of both incremental gradient and incremental proximal methods, in general convex smooth (for both) and convex Lipschitz (for the proximal variants) settings. Our oracle complexity bounds for the last iterate nearly match (i.e., match up to a square-root-log or a log factor) the best known oracle complexity bounds for the average iterate, for both classes of methods. We further obtain generalizations of our results to weighted averaging of th
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#38190;&#22240;&#32032;&#30340;&#26032;&#29702;&#35770;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.06871</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Ability of Unsupervised Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06871
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#38190;&#22240;&#32032;&#30340;&#26032;&#29702;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#30340;&#34920;&#31034;&#20989;&#25968;&#22914;&#20309;&#24433;&#21709;&#24494;&#35843;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35299;&#12290;&#29616;&#26377;&#29702;&#35770;&#30740;&#31350;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#30340;&#20998;&#24067;&#21644;&#20219;&#21153;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#38416;&#26126;&#20102;&#24433;&#21709;&#20174;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#33719;&#24471;&#30340;&#30693;&#35782;&#22312;&#38543;&#21518;&#30340;&#24494;&#35843;&#38454;&#27573;&#30340;&#21487;&#20256;&#36882;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#26368;&#32456;&#24433;&#21709;&#20102;&#24494;&#35843;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#24212;&#29992;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#20004;&#31181;&#19981;&#21516;&#24773;&#26223;&#30340;&#27867;&#21270;&#30028;&#38480;&#65306;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#21644;&#33945;&#29256;&#33258;&#32534;&#30721;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06871v1 Announce Type: new  Abstract: Recent advances in unsupervised learning have shown that unsupervised pre-training, followed by fine-tuning, can improve model generalization. However, a rigorous understanding of how the representation function learned on an unlabeled dataset affects the generalization of the fine-tuned model is lacking. Existing theoretical research does not adequately account for the heterogeneity of the distribution and tasks in pre-training and fine-tuning stage. To bridge this gap, this paper introduces a novel theoretical framework that illuminates the critical factor influencing the transferability of knowledge acquired during unsupervised pre-training to the subsequent fine-tuning phase, ultimately affecting the generalization capabilities of the fine-tuned model on downstream tasks. We apply our theoretical framework to analyze generalization bound of two distinct scenarios: Context Encoder pre-training with deep neural networks and Masked Auto
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#21097;&#20313;&#25552;&#31034;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#32423;&#36866;&#24212;&#26426;&#21046;&#26469;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#25552;&#31034;&#20914;&#31361;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06870</link><description>&lt;p&gt;
&#35821;&#20041;&#21097;&#20313;&#25552;&#31034;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic Residual Prompts for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06870
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#21097;&#20313;&#25552;&#31034;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#32423;&#36866;&#24212;&#26426;&#21046;&#26469;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#25552;&#31034;&#20914;&#31361;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#20923;&#32467;&#20102;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20391;&#37325;&#20110;&#35757;&#32451;&#19968;&#20123;&#31216;&#20026;&#25552;&#31034;&#30340;&#21442;&#25968;&#21521;&#37327;&#12290;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#22823;&#22810;&#25968;&#23558;&#36825;&#20123;&#21521;&#37327;&#32452;&#32455;&#22312;&#19968;&#20010;&#38190;-&#20540;&#23545;&#27744;&#20013;&#65292;&#24182;&#20351;&#29992;&#36755;&#20837;&#22270;&#20687;&#20316;&#20026;&#26597;&#35810;&#26469;&#26816;&#32034;&#25552;&#31034;&#65288;&#20540;&#65289;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20219;&#21153;&#30340;&#36827;&#34892;&#65292;&#30001;&#20110;&#38190;&#26159;&#23398;&#20064;&#30340;&#65292;&#25552;&#31034;&#36873;&#25321;&#31574;&#30053;&#26412;&#36523;&#20063;&#20250;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#26159;&#29616;&#26377;&#26041;&#27861;&#32463;&#24120;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20351;&#36873;&#25321;&#31574;&#30053;&#26356;&#21152;&#31283;&#23450;&#65292;&#25105;&#20204;&#35831;&#27714;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#22312;&#20004;&#32423;&#36866;&#24212;&#26426;&#21046;&#20013;&#36873;&#25321;&#25105;&#20204;&#30340;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31532;&#19968;&#32423;&#21033;&#29992;&#26631;&#20934;&#25991;&#26412;&#25552;&#31034;&#26469;&#35843;&#25972;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24418;&#25104;&#31283;&#23450;&#30340;&#31867;&#21407;&#22411;&#12290;&#32780;&#31532;&#20108;&#32423;&#21017;&#23558;&#36825;&#20123;&#21407;&#22411;&#19982;&#26597;&#35810;&#22270;&#20687;&#19968;&#36215;&#29992;&#20316;&#38190;&#26469;&#32034;&#24341;&#19968;&#20010;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06870v1 Announce Type: new  Abstract: Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06869</link><description>&lt;p&gt;
&#22312;&#26377;&#22122;&#22768;&#22522;&#30784;&#27169;&#22411;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#35843;&#25972;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#25110;&#25104;&#26412;&#36807;&#39640;&#65292;&#21487;&#33021;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#24615;&#36136;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#26377;&#22122;&#22768;&#30340;ImageNet-1K&#12289;YFCC15M&#21644;CC12M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20351;&#21516;&#39046;&#22495;&#65288;ID&#65289;&#24615;&#33021;&#21463;&#30410;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#31867;&#20284;&#20998;&#24067;&#65292;&#20294;&#23427;&#24635;&#26159;&#20250;&#30772;&#22351;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#24615;&#33021;&#65292;&#22312;&#37027;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22320;&#29702;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#27801;&#28448;&#34647;&#34411;&#30340;&#32321;&#27542;&#22320;&#65292;&#26377;&#26395;&#25552;&#21319;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25511;&#21046;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2403.06860</link><description>&lt;p&gt;
&#38750;&#27954;&#27801;&#28448;&#34647;&#34411;&#32321;&#27542;&#22320;&#39044;&#27979;&#30340;&#22320;&#29702;&#31354;&#38388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22320;&#29702;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#27801;&#28448;&#34647;&#34411;&#30340;&#32321;&#27542;&#22320;&#65292;&#26377;&#26395;&#25552;&#21319;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25511;&#21046;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27801;&#28448;&#34647;&#34411;&#25104;&#32676;&#25104;&#38431;&#23545;&#20892;&#19994;&#21644;&#39135;&#21697;&#23433;&#20840;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25805;&#20316;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#34647;&#34411;&#30340;&#32321;&#27542;&#22320;&#65292;&#26377;&#26395;&#22686;&#24378;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25511;&#21046;&#25514;&#26045;&#12290;&#25105;&#20204;&#20174;&#32852;&#21512;&#22269;&#31918;&#39135;&#21644;&#20892;&#19994;&#32452;&#32455;(UN-FAO)&#30340;&#34647;&#34411;&#35266;&#27979;&#35760;&#24405;&#20013;&#25972;&#29702;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#26102;&#31354;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#65306;&#36965;&#24863;&#29615;&#22659;&#21644;&#27668;&#20505;&#25968;&#25454;&#65292;&#20197;&#21450;&#22810;&#20809;&#35889;&#22320;&#29699;&#35266;&#27979;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;(&#19977;&#32500;&#21644;&#22522;&#20110;LSTM&#30340;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;)&#65292;&#20197;&#21450;Jakubik&#31561;&#20154;&#20110;2023&#24180;&#26368;&#26032;&#21457;&#24067;&#30340;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;Prithvi&#12290;&#36825;&#20123;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#32447;&#65292;&#22522;&#20110;Prithvi&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;NASA&#30340;&#35856;&#35843;Landsat&#21644;&#21736;&#20853;-2(HLS)&#22810;&#20809;&#35889;&#22270;&#20687;&#36827;&#34892;&#24494;&#35843;&#32780;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06860v1 Announce Type: new  Abstract: Desert locust swarms present a major threat to agriculture and food security. Addressing this challenge, our study develops an operationally-ready model for predicting locust breeding grounds, which has the potential to enhance early warning systems and targeted control measures. We curated a dataset from the United Nations Food and Agriculture Organization's (UN-FAO) locust observation records and analyzed it using two types of spatio-temporal input features: remotely-sensed environmental and climate data as well as multi-spectral earth observation images. Our approach employed custom deep learning models (three-dimensional and LSTM-based recurrent convolutional networks), along with the geospatial foundational model Prithvi recently released by Jakubik et al., 2023. These models notably outperformed existing baselines, with the Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized Landsat and Sentinel-2 (HLS) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#23545;&#34892;&#20026;&#27169;&#22411;&#35823;&#24046;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35266;&#27979;&#25968;&#25454;&#19982;&#20551;&#23450;&#34892;&#20026;&#27169;&#22411;&#19981;&#21516;&#20294;&#19981;&#24341;&#21457;&#38169;&#35823;&#30340;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.06854</link><description>&lt;p&gt;
&#37327;&#21270;&#36870;&#24378;&#21270;&#23398;&#20064;&#23545;&#35823;&#24046;&#35268;&#23450;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#23545;&#34892;&#20026;&#27169;&#22411;&#35823;&#24046;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35266;&#27979;&#25968;&#25454;&#19982;&#20551;&#23450;&#34892;&#20026;&#27169;&#22411;&#19981;&#21516;&#20294;&#19981;&#24341;&#21457;&#38169;&#35823;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26088;&#22312;&#20174;&#20195;&#29702;&#30340;&#34892;&#20026;&#65288;&#34920;&#31034;&#20026;&#31574;&#30053;$\pi$&#65289;&#20013;&#25512;&#26029;&#20854;&#20559;&#22909;&#65288;&#34920;&#31034;&#20026;&#22870;&#21169;&#20989;&#25968;$R$&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#25551;&#36848;$\pi$&#19982;$R$&#20851;&#31995;&#30340;&#34892;&#20026;&#27169;&#22411;&#12290;&#24403;&#21069;&#25991;&#29486;&#20013;&#65292;&#26368;&#24120;&#35265;&#30340;&#34892;&#20026;&#27169;&#22411;&#26159;&#26368;&#20248;&#24615;&#12289;Boltzmann-&#29702;&#24615;&#21644;&#22240;&#26524;&#29109;&#26368;&#22823;&#21270;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20559;&#22909;&#19982;&#20854;&#34892;&#20026;&#20043;&#38388;&#30340;&#30495;&#23454;&#20851;&#31995;&#35201;&#27604;&#20219;&#20309;&#36825;&#20123;&#34892;&#20026;&#27169;&#22411;&#22797;&#26434;&#24471;&#22810;&#12290;&#36825;&#24847;&#21619;&#30528;&#34892;&#20026;&#27169;&#22411;&#23384;&#22312;&#35268;&#23450;&#38169;&#35823;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#24341;&#21457;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#31995;&#32479;&#35823;&#24046;&#25285;&#24551;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;IRL&#38382;&#39064;&#23545;&#34892;&#20026;&#27169;&#22411;&#35823;&#24046;&#30340;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23436;&#20840;&#25551;&#36848;&#35266;&#27979;&#25968;&#25454;&#22914;&#20309;&#21487;&#33021;&#19982;&#20551;&#23450;&#30340;&#34892;&#20026;&#27169;&#22411;&#19981;&#21516;&#32780;&#19981;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06854v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) aims to infer an agent's preferences (represented as a reward function $R$) from their behaviour (represented as a policy $\pi$). To do this, we need a behavioural model of how $\pi$ relates to $R$. In the current literature, the most common behavioural models are optimality, Boltzmann-rationality, and causal entropy maximisation. However, the true relationship between a human's preferences and their behaviour is much more complex than any of these behavioural models. This means that the behavioural models are misspecified, which raises the concern that they may lead to systematic errors if applied to real data. In this paper, we analyse how sensitive the IRL problem is to misspecification of the behavioural model. Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error abov
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#38469;&#25968;&#25454;&#20013;&#35782;&#21035;&#26032;&#29983;&#20799;&#20998;&#23081;&#20107;&#20214;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#26088;&#22312;&#35774;&#35745;&#20986;&#19968;&#27454;&#29992;&#25143;&#21451;&#22909;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#39640;&#23545;&#39640;&#39118;&#38505;&#24739;&#32773;&#30340;&#35782;&#21035;&#29575;&#21644;&#24178;&#39044;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.06843</link><description>&lt;p&gt;
&#38754;&#21521;&#25903;&#25345;&#26032;&#29983;&#20799;&#31185;&#21307;&#29983;&#30340;&#20998;&#23081;&#23460;&#25945;&#32946;&#24037;&#20855;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards an educational tool for supporting neonatologists in the delivery room
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06843
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#38469;&#25968;&#25454;&#20013;&#35782;&#21035;&#26032;&#29983;&#20799;&#20998;&#23081;&#20107;&#20214;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#26088;&#22312;&#35774;&#35745;&#20986;&#19968;&#27454;&#29992;&#25143;&#21451;&#22909;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#39640;&#23545;&#39640;&#39118;&#38505;&#24739;&#32773;&#30340;&#35782;&#21035;&#29575;&#21644;&#24178;&#39044;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#26377;&#35777;&#25454;&#34920;&#26126;&#20960;&#20010;&#22240;&#32032;&#21487;&#33021;&#22686;&#21152;&#23156;&#20799;&#22312;&#20986;&#29983;&#26102;&#38656;&#35201;&#31283;&#23450;&#25110;&#22797;&#33487;&#25805;&#20316;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39118;&#38505;&#22240;&#32032;&#23578;&#26410;&#23436;&#20840;&#30693;&#26195;&#65292;&#24182;&#19988;&#30446;&#21069;&#23578;&#26080;&#36866;&#29992;&#20110;&#39044;&#27979;&#39640;&#39118;&#38505;&#24773;&#20917;&#30340;&#26222;&#36941;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#38480;&#21046;&#21644;&#20986;&#29983;&#26102;&#38656;&#35201;&#36827;&#34892;&#22797;&#33487;&#30340;&#38656;&#27714;&#30456;&#23545;&#32597;&#35265;&#65292;&#26377;&#24517;&#35201;&#23545;&#36127;&#36131;&#20998;&#23081;&#23460;&#26032;&#29983;&#20799;&#25252;&#29702;&#30340;&#21307;&#25252;&#20154;&#21592;&#36827;&#34892;&#23450;&#26399;&#22521;&#35757;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#38469;&#25968;&#25454;&#20013;&#35782;&#21035;&#39118;&#38505;&#22240;&#32032;&#21450;&#20854;&#23545;&#20998;&#23081;&#20107;&#20214;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#20197;&#24110;&#21161;&#20154;&#21592;&#36880;&#27493;&#22686;&#21152;&#21644;&#26356;&#26032;&#20182;&#20204;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#33021;&#22815;&#25552;&#39640;&#39640;&#39118;&#38505;&#24739;&#32773;&#30340;&#35782;&#21035;&#29575;&#21644;&#35268;&#21010;&#36866;&#24403;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06843v1 Announce Type: new  Abstract: Nowadays, there is evidence that several factors may increase the risk, for an infant, to require stabilisation or resuscitation manoeuvres at birth. However, this risk factors are not completely known, and a universally applicable model for predicting high-risk situations is not available yet. Considering both these limitations and the fact that the need for resuscitation at birth is a rare event, periodic training of the healthcare personnel responsible for newborn caring in the delivery room is mandatory.   In this paper, we propose a machine learning approach for identifying risk factors and their impact on the birth event from real data, which can be used by personnel to progressively increase and update their knowledge. Our final goal will be the one of designing a user-friendly mobile application, able to improve the recognition rate and the planning of the appropriate interventions on high-risk patients.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.06833</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#23558;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#21527;&#65311;&#25105;&#20204;&#20855;&#20307;&#25351;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35843;&#33410;&#25351;&#20196;&#30340;&#25216;&#26415;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#65292;&#20026;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25171;&#24320;&#20102;&#26080;&#25968;&#26032;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs&#32570;&#20047;&#20854;&#20182;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24050;&#24314;&#31435;&#20026;&#35268;&#33539;&#30340;&#22522;&#26412;&#23433;&#20840;&#29305;&#24615;&#65292;&#27604;&#22914;&#25351;&#20196;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#23548;&#33268;&#23427;&#20204;&#21457;&#29983;&#25925;&#38556;&#25110;&#26131;&#21463;&#31532;&#19977;&#26041;&#25805;&#25511;&#21644;&#24178;&#25200;&#65288;&#20363;&#22914;&#36890;&#36807;&#38388;&#25509;&#25552;&#31034;/&#21629;&#20196;&#27880;&#20837;&#65289;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#29978;&#33267;&#27809;&#26377;&#30830;&#20999;&#23450;&#20041;&#36825;&#31181;&#20998;&#31163;&#31350;&#31455;&#24847;&#21619;&#30528;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#27979;&#35797;&#20854;&#36829;&#21453;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#20174;&#27169;&#22411;&#30340;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SEP&#65288;&#24212;&#35813;&#25191;&#34892;&#36824;&#26159;&#22788;&#29702;&#65311;&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#20801;&#35768;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 Announce Type: cross  Abstract: Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#39044;&#27979;&#21464;&#37327;&#30340;&#31163;&#25955;&#20540;&#24182;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#21464;&#37327;&#29992;&#20110;&#20016;&#23500;&#22238;&#24402;&#38382;&#39064;&#30340;&#21021;&#22987;&#21521;&#37327;&#65292;&#32463;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06829</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#31867;&#22120;&#26500;&#24314;&#21464;&#37327;&#36741;&#21161;&#22238;&#24402;&#65306;&#19968;&#20010;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Constructing Variables Using Classifiers as an Aid to Regression: An Empirical Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06829
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#39044;&#27979;&#21464;&#37327;&#30340;&#31163;&#25955;&#20540;&#24182;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#21464;&#37327;&#29992;&#20110;&#20016;&#23500;&#22238;&#24402;&#38382;&#39064;&#30340;&#21021;&#22987;&#21521;&#37327;&#65292;&#32463;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21019;&#24314;&#21464;&#37327;&#65288;&#22312;&#22238;&#24402;&#30340;&#24773;&#20917;&#19979;&#65289;&#65292;&#20197;&#34917;&#20805;&#21021;&#22987;&#36755;&#20837;&#21521;&#37327;&#20013;&#25152;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#36816;&#34892;&#65292;&#22312;&#35813;&#27493;&#39588;&#20013;&#65292;&#35201;&#22238;&#24402;&#30340;&#21464;&#37327;&#30340;&#36830;&#32493;&#20540;&#34987;&#31163;&#25955;&#21270;&#20026;&#19968;&#32452;&#38388;&#38548;&#65292;&#28982;&#21518;&#29992;&#20110;&#23450;&#20041;&#20540;&#38408;&#12290;&#28982;&#21518;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#35201;&#22238;&#24402;&#30340;&#20540;&#26159;&#21542;&#23567;&#20110;&#25110;&#31561;&#20110;&#36825;&#20123;&#38408;&#20540;&#20013;&#30340;&#27599;&#19968;&#20010;&#12290;&#28982;&#21518;&#20998;&#31867;&#22120;&#30340;&#19981;&#21516;&#36755;&#20986;&#34987;&#38142;&#25509;&#22312;&#19968;&#20010;&#38468;&#21152;&#30340;&#21464;&#37327;&#21521;&#37327;&#20013;&#65292;&#20016;&#23500;&#20102;&#22238;&#24402;&#38382;&#39064;&#30340;&#21021;&#22987;&#21521;&#37327;&#12290;&#23454;&#29616;&#30340;&#31995;&#32479;&#22240;&#27492;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#39044;&#22788;&#29702;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;5&#31181;&#31867;&#22411;&#30340;&#22238;&#24402;&#22120;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#20016;&#23500;&#26041;&#27861;&#65292;&#24182;&#22312;33&#20010;&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06829v1 Announce Type: new  Abstract: This paper proposes a method for the automatic creation of variables (in the case of regression) that complement the information contained in the initial input vector. The method works as a pre-processing step in which the continuous values of the variable to be regressed are discretized into a set of intervals which are then used to define value thresholds. Then classifiers are trained to predict whether the value to be regressed is less than or equal to each of these thresholds. The different outputs of the classifiers are then concatenated in the form of an additional vector of variables that enriches the initial vector of the regression problem. The implemented system can thus be considered as a generic pre-processing tool. We tested the proposed enrichment method with 5 types of regressors and evaluated it in 33 regression datasets. Our experimental results confirm the interest of the approach.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;In-context Exploration-Exploitation (ICEE)&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;Transformer&#27169;&#22411;&#20869;&#37096;&#36827;&#34892;&#25506;&#32034;-&#21033;&#29992;&#26435;&#34913;&#65292;&#25552;&#39640;&#20102;&#22312;-context&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06826</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25506;&#32034;-&#21033;&#29992;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-context Exploration-Exploitation for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06826
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;In-context Exploration-Exploitation (ICEE)&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;Transformer&#27169;&#22411;&#20869;&#37096;&#36827;&#34892;&#25506;&#32034;-&#21033;&#29992;&#26435;&#34913;&#65292;&#25552;&#39640;&#20102;&#22312;-context&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;-context&#23398;&#20064;&#26159;&#22312;&#32447;&#31574;&#30053;&#23398;&#20064;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#23454;&#29616;&#65292;&#26080;&#38656;&#26799;&#24230;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#25910;&#38598;&#22823;&#37327;&#35757;&#32451;&#36712;&#36857;&#38598;&#24182;&#35757;&#32451;&#22823;&#22411;Transformer&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#26174;&#33879;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;In-context Exploration-Exploitation&#65288;ICEE&#65289;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#20248;&#21270;&#22312;-context&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;ICEE&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#22312;Transformer&#27169;&#22411;&#20013;&#25191;&#34892;&#25506;&#32034;-&#21033;&#29992;&#26435;&#34913;&#65292;&#19981;&#38656;&#35201;&#26174;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#22240;&#27492;&#65292;ICEE&#21487;&#20197;&#20687;&#39640;&#26031;&#36807;&#31243;&#20559;&#24046;&#26041;&#27861;&#37027;&#26679;&#26377;&#25928;&#22320;&#35299;&#20915;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#26102;&#38388;&#26174;&#30528;&#36739;&#30701;&#12290;&#36890;&#36807;&#22312;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;ICEE&#33021;&#22815;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;R
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06826v1 Announce Type: cross  Abstract: In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new R
&lt;/p&gt;</description></item><item><title>GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#29256;&#26412;&#65292;&#31532;&#19968;&#20010;&#29256;&#26412;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#31532;&#20108;&#20010;&#29256;&#26412;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.06817</link><description>&lt;p&gt;
&#30446;&#26631;&#20449;&#24687;&#26356;&#26377;&#25928;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Targeted Messages More Effective?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06817
&lt;/p&gt;
&lt;p&gt;
GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#29256;&#26412;&#65292;&#31532;&#19968;&#20010;&#29256;&#26412;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#31532;&#20108;&#20010;&#29256;&#26412;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#29992;&#20110;&#22270;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#26412;&#36136;&#19978;&#65292;GNN&#26159;&#19968;&#20010;&#20998;&#24067;&#24335;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#20854;&#21463;&#21040;&#20174;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#21442;&#25968;&#30340;&#25511;&#21046;&#12290;&#23427;&#22312;&#22270;&#30340;&#39030;&#28857;&#19978;&#25805;&#20316;&#65306;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#39030;&#28857;&#22312;&#27599;&#20010;&#20256;&#20837;&#36793;&#19978;&#25509;&#25910;&#19968;&#26465;&#28040;&#24687;&#65292;&#32858;&#21512;&#36825;&#20123;&#28040;&#24687;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#24403;&#21069;&#30340;&#29366;&#24577;&#21644;&#32858;&#21512;&#30340;&#28040;&#24687;&#26356;&#26032;&#23427;&#20204;&#30340;&#29366;&#24577;&#12290;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#21487;&#20197;&#29992;&#24102;&#35745;&#25968;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#26576;&#20123;&#29255;&#27573;&#21644;Weisfeiler-Lehman&#31639;&#27861;&#26469;&#25551;&#36848;&#12290;GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#19981;&#21516;&#30340;&#29256;&#26412;&#12290;&#22312;&#31532;&#19968;&#20010;&#29256;&#26412;&#20013;&#65292;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#22312;&#31532;&#20108;&#20010;&#29256;&#26412;&#20013;&#65292;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;&#23454;&#38469;&#19978;&#65292;&#36825;&#20004;&#20010;&#29256;&#26412;&#37117;&#34987;&#20351;&#29992;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;GNN&#30340;&#29702;&#35770;&#22823;&#22810;&#38598;&#20013;&#22312;&#31532;&#19968;&#20010;&#29256;&#26412;&#19978;&#12290;&#22312;&#36923;&#36753;&#26041;&#38754;&#65292;&#36825;&#20004;&#20010;&#29256;&#26412;&#23545;&#24212;&#30528;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06817v1 Announce Type: cross  Abstract: Graph neural networks (GNN) are deep learning architectures for graphs. Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data. It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages. The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm.   The core GNN architecture comes in two different versions. In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices. In practice, both of these versions are used, but the theory of GNNs so far mostly focused on the first one. On the logical side, the two versions correspond to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;Kullback-Leibler&#25955;&#24230;&#35757;&#32451;&#22823;&#35268;&#27169;&#12289;&#38750;&#20809;&#28369;&#30340;Maxent&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.06816</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#12289;&#38750;&#20809;&#28369;&#26368;&#22823;&#29109;&#27169;&#22411;&#30340;&#39640;&#25928;&#19968;&#38454;&#31639;&#27861;&#21450;&#20854;&#22312;&#37326;&#28779;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient first-order algorithms for large-scale, non-smooth maximum entropy models with application to wildfire science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06816
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;Kullback-Leibler&#25955;&#24230;&#35757;&#32451;&#22823;&#35268;&#27169;&#12289;&#38750;&#20809;&#28369;&#30340;Maxent&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#65288;Maxent&#65289;&#27169;&#22411;&#26159;&#19968;&#31867;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#27010;&#29575;&#20998;&#24067;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#30001;&#20110;&#29616;&#20195;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#65292;Maxent&#27169;&#22411;&#38656;&#35201;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#26469;&#36866;&#24212;&#22823;&#25968;&#25454;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#35757;&#32451;&#22823;&#35268;&#27169;&#12289;&#38750;&#20809;&#28369;Maxent&#27169;&#22411;&#30340;&#29616;&#26377;&#31639;&#27861;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19968;&#38454;&#31639;&#27861;&#21033;&#29992;Kullback-Leibler&#25955;&#24230;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#22823;&#35268;&#27169;&#19988;&#38750;&#20809;&#28369;&#30340;Maxent&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06816v1 Announce Type: cross  Abstract: Maximum entropy (Maxent) models are a class of statistical models that use the maximum entropy principle to estimate probability distributions from data. Due to the size of modern data sets, Maxent models need efficient optimization algorithms to scale well for big data applications. State-of-the-art algorithms for Maxent models, however, were not originally designed to handle big data sets; these algorithms either rely on technical devices that may yield unreliable numerical results, scale poorly, or require smoothness assumptions that many practical Maxent models lack. In this paper, we present novel optimization algorithms that overcome the shortcomings of state-of-the-art algorithms for training large-scale, non-smooth Maxent models. Our proposed first-order algorithms leverage the Kullback-Leibler divergence to train large-scale and non-smooth Maxent models efficiently. For Maxent models with discrete probability distribution of $
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;{\epsilon}-&#31070;&#32463;Thompson&#37319;&#26679;&#22312;&#24085;&#37329;&#26862;&#30149;&#27835;&#30103;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#35299;&#20915;&#20256;&#32479;DBS&#35774;&#22791;&#30340;&#33021;&#28304;&#25928;&#29575;&#21644;&#21103;&#20316;&#29992;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;DBS&#30340;&#30446;&#26631;</title><link>https://arxiv.org/abs/2403.06814</link><description>&lt;p&gt;
{\epsilon}-&#31070;&#32463;Thompson&#37319;&#26679;&#22312;&#24085;&#37329;&#26862;&#30149;&#27835;&#30103;&#30340;&#28145;&#24230;&#33041;&#37096;&#21050;&#28608;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
{\epsilon}-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06814
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;{\epsilon}-&#31070;&#32463;Thompson&#37319;&#26679;&#22312;&#24085;&#37329;&#26862;&#30149;&#27835;&#30103;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#35299;&#20915;&#20256;&#32479;DBS&#35774;&#22791;&#30340;&#33021;&#28304;&#25928;&#29575;&#21644;&#21103;&#20316;&#29992;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;DBS&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#33041;&#37096;&#21050;&#28608;(DBS)&#20316;&#20026;&#32531;&#35299;&#24085;&#37329;&#26862;&#30149;(PD)&#36816;&#21160;&#30151;&#29366;&#30340;&#26377;&#25928;&#24178;&#39044;&#25163;&#27573;&#12290;&#20256;&#32479;&#30340;&#21830;&#29992;DBS&#35774;&#22791;&#20165;&#33021;&#21521;&#22823;&#33041;&#22522;&#24213;&#31070;&#32463;&#33410;(BG)&#21306;&#22495;&#25552;&#20379;&#22266;&#23450;&#39057;&#29575;&#30340;&#21608;&#26399;&#33033;&#20914;&#65292;&#21363;&#36830;&#32493;DBS(cDBS)&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26222;&#36941;&#23384;&#22312;&#33021;&#28304;&#25928;&#29575;&#20302;&#21644;&#21103;&#20316;&#29992;&#65288;&#20363;&#22914;&#35328;&#35821;&#38556;&#30861;&#65289;&#31561;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#33258;&#36866;&#24212;DBS(aDBS)&#19978;&#65292;&#20197;&#35299;&#20915;cDBS&#30340;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;(RL)&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#26469;&#35843;&#25972;&#21050;&#28608;&#30340;&#39057;&#29575;&#65292;&#20197;&#36798;&#21040;&#26082;&#33021;&#28304;&#25928;&#29575;&#21448;&#33021;&#27835;&#30103;&#26377;&#25928;&#30340;&#30446;&#30340;&#12290;&#28982;&#32780;&#65292;RL&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20351;&#24471;&#23558;RL&#31574;&#30053;&#25972;&#21512;&#21040;&#23454;&#26102;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#21464;&#24471;&#38590;&#20197;&#23454;&#29616;&#65292;&#32780;&#36825;&#26159;aDBS&#25152;&#38656;&#35201;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19978;&#19979;&#25991;&#22810;&#33218;&#32769;&#34382;&#26426;(CMAB)&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06814v1 Announce Type: new  Abstract: Deep Brain Stimulation (DBS) stands as an effective intervention for alleviating the motor symptoms of Parkinson's disease (PD). Traditional commercial DBS devices are only able to deliver fixed-frequency periodic pulses to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS). However, they in general suffer from energy inefficiency and side effects, such as speech impairment. Recent research has focused on adaptive DBS (aDBS) to resolve the limitations of cDBS. Specifically, reinforcement learning (RL) based approaches have been developed to adapt the frequencies of the stimuli in order to achieve both energy efficiency and treatment efficacy. However, RL approaches in general require significant amount of training data and computational resources, making it intractable to integrate RL policies into real-time embedded systems as needed in aDBS. In contrast, contextual multi-armed bandits (CMAB) in general lead to bet
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21333;&#35843;&#32858;&#21512;&#20989;&#25968;&#23454;&#29616;&#20102;&#19968;&#31181;&#20010;&#20307;&#20844;&#24179;&#24615;&#23457;&#35745;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#22810;&#20010;&#23457;&#35745;&#21592;&#23545;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#20998;&#26512;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.06812</link><description>&lt;p&gt;
&#21333;&#35843;&#20010;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monotone Individual Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21333;&#35843;&#32858;&#21512;&#20989;&#25968;&#23454;&#29616;&#20102;&#19968;&#31181;&#20010;&#20307;&#20844;&#24179;&#24615;&#23457;&#35745;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#22810;&#20010;&#23457;&#35745;&#21592;&#23545;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#20998;&#26512;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24102;&#26377;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22312;&#32447;&#23398;&#20064;&#32773;&#21162;&#21147;&#22312;&#26368;&#22823;&#21270;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#30830;&#20445;&#30456;&#20284;&#20010;&#20307;&#21463;&#21040;&#31867;&#20284;&#23545;&#24453;&#12290;&#25105;&#20204;&#39318;&#20808;&#25193;&#23637;&#20102;Gillen&#31561;&#20154;&#65288;2018&#24180;&#65289;&#65307;Bechavod&#31561;&#20154;&#65288;2020&#24180;&#65289;&#30340;&#26694;&#26550;&#65292;&#36825;&#20123;&#26694;&#26550;&#20381;&#36182;&#20110;&#26469;&#33258;&#20154;&#31867;&#23457;&#35745;&#21592;&#26377;&#20851;&#20844;&#24179;&#24615;&#36829;&#35268;&#30340;&#21453;&#39304;&#65292;&#22240;&#20026;&#25105;&#20204;&#32771;&#34385;&#20102;&#33021;&#22815;&#32858;&#21512;&#20219;&#24847;&#25968;&#37327;&#23457;&#35745;&#21592;&#21453;&#39304;&#30340;&#23457;&#35745;&#26041;&#26696;&#65292;&#20351;&#29992;&#20102;&#25105;&#20204;&#31216;&#20026;&#21333;&#35843;&#32858;&#21512;&#20989;&#25968;&#30340;&#20016;&#23500;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#23457;&#35745;&#26041;&#26696;&#30340;&#29305;&#24615;&#65292;&#36890;&#36807;&#23454;&#38469;&#23558;&#22810;&#20010;&#23457;&#35745;&#21592;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#23457;&#35745;&#20998;&#26512;&#31616;&#21270;&#20026;&#65288;&#23454;&#20363;&#29305;&#23450;&#30340;&#65289;&#21333;&#20010;&#23457;&#35745;&#21592;&#30340;&#23457;&#35745;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29616;&#19978;&#30028;&#21069;&#27839;&#30340;oracle-efficient&#31639;&#27861;&#65292;&#20998;&#21035;&#20026;&#36951;&#25022;&#20540;&#21644;&#20844;&#24179;&#24615;&#36829;&#35268;&#27425;&#25968;&#30340;&#19978;&#30028;$(\mathcal{O}(T^{1/2+2b}),\mathcal{O}(T^{3/4-b}))$&#65292;&#20854;&#20013;$0\leq b$
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06812v1 Announce Type: new  Abstract: We revisit the problem of online learning with individual fairness, where an online learner strives to maximize predictive accuracy while ensuring that similar individuals are treated similarly. We first extend the frameworks of Gillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human auditors regarding fairness violations, as we consider auditing schemes that are capable of aggregating feedback from any number of auditors, using a rich class we term monotone aggregation functions. We then prove a characterization for such auditing schemes, practically reducing the analysis of auditing for individual fairness by multiple auditors to that of auditing by (instance-specific) single auditors. Using our generalized framework, we present an oracle-efficient algorithm achieving an upper bound frontier of $(\mathcal{O}(T^{1/2+2b}),\mathcal{O}(T^{3/4-b}))$ respectively for regret, number of fairness violations, for $0\leq b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#23454;&#29616;&#20102;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.06807</link><description>&lt;p&gt;
&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multistep Consistency Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#23454;&#29616;&#20102;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30456;&#23545;&#23481;&#26131;&#35757;&#32451;&#65292;&#20294;&#29983;&#25104;&#26679;&#26412;&#38656;&#35201;&#35768;&#22810;&#27493;&#39588;&#12290;&#19968;&#33268;&#24615;&#27169;&#22411;&#26356;&#38590;&#35757;&#32451;&#65292;&#20294;&#21487;&#20197;&#22312;&#19968;&#20010;&#27493;&#39588;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65306;&#36890;&#36807;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;TRACT&#30340;&#32479;&#19968;&#65292;&#21487;&#20197;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65306;&#22312;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#26159;&#20256;&#32479;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32780;&#25105;&#20204;&#23637;&#31034;&#20102;$\infty$&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#26159;&#25193;&#25955;&#27169;&#22411;&#12290;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#23558;&#26679;&#26412;&#39044;&#31639;&#20174;&#21333;&#27493;&#22686;&#21152;&#21040;2-8&#27493;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#22823;&#37096;&#20998;&#37319;&#26679;&#36895;&#24230;&#20248;&#21183;&#12290;&#22312;Imagenet 64&#19978;8&#27493;&#36798;&#21040;1.4&#30340;FID&#65292;&#22312;Imagenet128&#19978;8&#27493;&#36798;&#21040;2.1&#30340;FID&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06807v1 Announce Type: new  Abstract: Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step.   In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality. Specifically, a 1-step consistency model is a conventional consistency model whereas we show that a $\infty$-step consistency model is a diffusion model.   Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#22312;&#24179;&#22343;&#22870;&#21169;MDPs&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06806</link><description>&lt;p&gt;
&#20851;&#20110;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#31574;&#30053;&#26799;&#24230;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06806
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#22312;&#24179;&#22343;&#22870;&#21169;MDPs&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#22312;&#26080;&#38480;&#26102;&#38388;&#38388;&#38548;&#30340;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#31574;&#30053;&#26799;&#24230;&#30340;&#26377;&#38480;&#26102;&#38388;&#20840;&#23616;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20855;&#26377;&#26377;&#38480;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#36941;&#21382;&#26631;&#31614;MDP&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#31574;&#30053;&#26799;&#24230;&#36845;&#20195;&#20197;$O\left({\frac{1}{T}}\right)$&#30340;&#27425;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#36825;&#23545;&#24212;&#20110;$O\left({\log(T)}\right)$&#30340;&#21518;&#24724;&#65292;&#20854;&#20013;$T$&#34920;&#31034;&#36845;&#20195;&#27425;&#25968;&#12290;&#20043;&#21069;&#20851;&#20110;&#25240;&#29616;&#22870;&#21169;MDPs&#24615;&#33021;&#30028;&#30340;&#24037;&#20316;&#26080;&#27861;&#25512;&#24191;&#21040;&#24179;&#22343;&#22870;&#21169;MDPs&#65292;&#22240;&#20026;&#30028;&#20250;&#38543;&#26377;&#25928;&#35270;&#36317;&#30340;&#20116;&#27425;&#26041;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#35777;&#26126;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#22312;&#24179;&#22343;&#22870;&#21169;MDPs&#20013;&#25910;&#25947;&#65292;&#24182;&#24471;&#21040;&#26377;&#38480;&#26102;&#38388;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#19982;&#29616;&#26377;&#30340;&#25240;&#29616;&#22870;&#21169;&#24615;&#33021;&#30028;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24615;&#33021;&#30028;&#20855;&#26377;&#26126;&#30830;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06806v1 Announce Type: new  Abstract: We present the first finite time global convergence analysis of policy gradient in the context of infinite horizon average reward Markov decision processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite state and action spaces. Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O\left({\frac{1}{T}}\right),$ which translates to $O\left({\log(T)}\right)$ regret, where $T$ represents the number of iterations. Prior work on performance bounds for discounted reward MDPs cannot be extended to average reward MDPs because the bounds grow proportional to the fifth power of the effective horizon. Thus, our primary contribution is in proving that the policy gradient algorithm converges for average-reward MDPs and in obtaining finite-time performance guarantees. In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit depende
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25200;&#21160;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65288;DPAAT&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25552;&#39640;&#31283;&#20581;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#27867;&#21270;&#65292;&#36890;&#36807;&#23558;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#32622;&#20110;&#21160;&#24577;&#23398;&#20064;&#29615;&#22659;&#20013;&#29983;&#25104;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#32423;&#25200;&#21160;&#65292;&#24182;&#36890;&#36807;&#25439;&#22833;&#20449;&#24687;&#25910;&#38598;&#25552;&#20379;&#21160;&#24577;&#26356;&#26032;&#30340;&#26631;&#20934;&#65292;&#20197;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06798</link><description>&lt;p&gt;
&#21160;&#24577;&#25200;&#21160;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dynamic Perturbation-Adaptive Adversarial Training on Medical Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25200;&#21160;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65288;DPAAT&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25552;&#39640;&#31283;&#20581;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#27867;&#21270;&#65292;&#36890;&#36807;&#23558;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#32622;&#20110;&#21160;&#24577;&#23398;&#20064;&#29615;&#22659;&#20013;&#29983;&#25104;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#32423;&#25200;&#21160;&#65292;&#24182;&#36890;&#36807;&#25439;&#22833;&#20449;&#24687;&#25910;&#38598;&#25552;&#20379;&#21160;&#24577;&#26356;&#26032;&#30340;&#26631;&#20934;&#65292;&#20197;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#26679;&#26412;&#65288;AEs&#65289;&#19982;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#38590;&#20197;&#23519;&#35273;&#30340;&#30456;&#20284;&#24615;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#32593;&#32476;&#31283;&#20581;&#24615;&#30340;&#20005;&#37325;&#20851;&#20999;&#12290;&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#23545;&#24212;&#23545;&#24694;&#24847;AEs&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#25552;&#39640;&#31283;&#20581;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20294;&#20811;&#26381;&#30001;AT&#24341;&#36215;&#30340;&#32593;&#32476;&#27867;&#21270;&#19979;&#38477;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25200;&#21160;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65288;DPAAT&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25552;&#39640;&#31283;&#20581;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#27867;&#21270;&#65292;&#23558;AT&#25918;&#32622;&#22312;&#21160;&#24577;&#23398;&#20064;&#29615;&#22659;&#20013;&#20197;&#29983;&#25104;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#32423;&#25200;&#21160;&#65292;&#24182;&#36890;&#36807;&#25439;&#22833;&#20449;&#24687;&#25910;&#38598;&#25552;&#20379;&#21160;&#24577;&#26356;&#26032;&#30340;&#26631;&#20934;&#65292;&#20197;&#22788;&#29702;&#20256;&#32479;AT&#26041;&#27861;&#20013;&#22266;&#23450;&#25200;&#21160;&#22823;&#23567;&#30340;&#32570;&#38519;&#20197;&#21450;&#23545;&#22806;&#37096;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06798v1 Announce Type: cross  Abstract: Remarkable successes were made in Medical Image Classification (MIC) recently, mainly due to wide applications of convolutional neural networks (CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity with raw data, raising serious concerns on network robustness. Although adversarial training (AT), in responding to malevolent AEs, was recognized as an effective approach to improve robustness, it was challenging to overcome generalization decline of networks caused by the AT. In this paper, in order to reserve high generalization while improving robustness, we proposed a dynamic perturbation-adaptive adversarial training (DPAAT) method, which placed AT in a dynamic learning environment to generate adaptive data-level perturbations and provided a dynamically updated criterion by loss information collections to handle the disadvantage of fixed perturbation sizes in conventional AT methods and the dependence on extern
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20869;&#37096;&#34920;&#31034;&#65292;&#38024;&#23545;&#21482;&#26377;&#21333;&#20010;&#30913;&#22270;&#20687;&#21450;&#20854;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06797</link><description>&lt;p&gt;
&#21033;&#29992;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#30913;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leveraging Internal Representations of Model for Magnetic Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20869;&#37096;&#34920;&#31034;&#65292;&#38024;&#23545;&#21482;&#26377;&#21333;&#20010;&#30913;&#22270;&#20687;&#21450;&#20854;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#36793;&#32536;&#35774;&#22791;&#29983;&#25104;&#30340;&#25968;&#25454;&#20855;&#26377;&#35757;&#32451;&#26234;&#33021;&#33258;&#20027;&#31995;&#32479;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#21508;&#31181;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#24182;&#21033;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#30001;&#20110;&#25968;&#25454;&#30862;&#29255;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#25935;&#24863;&#23384;&#20648;&#65292;&#23433;&#20840;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#39072;&#35206;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21482;&#26377;&#21333;&#20010;&#30913;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#22270;&#20687;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#29983;&#25104;&#31616;&#27905;&#32780;&#23500;&#26377;&#20449;&#24687;&#30340;&#26679;&#26412;&#65292;&#26088;&#22312;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#24182;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#20197;&#26368;&#23569;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06797v1 Announce Type: new  Abstract: Data generated by edge devices has the potential to train intelligent autonomous systems across various domains. Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations. This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available. We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity. Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results. This methodology presents a promising avenue for training machine learning models with minimal data.
&lt;/p&gt;</description></item><item><title>&#23558;&#20107;&#20214;&#31867;&#22411;&#37325;&#26032;&#23450;&#20041;&#20026;&#8220;&#21407;&#22411;&#8221;&#65292;&#20854;&#30001;&#19968;&#32452;&#31216;&#20026;&#8220;&#38754;&#21521;&#8221;&#30340;&#29420;&#29305;&#23450;&#37327;&#32500;&#24230;&#32452;&#21512;&#29305;&#24449;&#21270;&#65292;&#32780;&#19981;&#26159;&#24378;&#21046;&#25191;&#34892;&#20005;&#26684;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#20174;&#32780;&#20801;&#35768;&#28041;&#21450;&#32676;&#20307;&#37051;&#36817;&#22810;&#20010;&#21407;&#22411;&#30340;&#21160;&#24577;&#30340;&#28151;&#21512;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.06771</link><description>&lt;p&gt;
&#37325;&#26032;&#23450;&#20041;&#26102;&#38388;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#31867;&#22411;&#21644;&#32676;&#20307;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Redefining Event Types and Group Evolution in Temporal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06771
&lt;/p&gt;
&lt;p&gt;
&#23558;&#20107;&#20214;&#31867;&#22411;&#37325;&#26032;&#23450;&#20041;&#20026;&#8220;&#21407;&#22411;&#8221;&#65292;&#20854;&#30001;&#19968;&#32452;&#31216;&#20026;&#8220;&#38754;&#21521;&#8221;&#30340;&#29420;&#29305;&#23450;&#37327;&#32500;&#24230;&#32452;&#21512;&#29305;&#24449;&#21270;&#65292;&#32780;&#19981;&#26159;&#24378;&#21046;&#25191;&#34892;&#20005;&#26684;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#20174;&#32780;&#20801;&#35768;&#28041;&#21450;&#32676;&#20307;&#37051;&#36817;&#22810;&#20010;&#21407;&#22411;&#30340;&#21160;&#24577;&#30340;&#28151;&#21512;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32676; - &#22914;&#28857;&#30340;&#32858;&#31867;&#25110;&#33410;&#28857;&#30340;&#31038;&#21306; - &#22312;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#25366;&#25496;&#20219;&#21153;&#26102;&#26159;&#22522;&#30784;&#30340;&#12290;&#22312;&#26102;&#38388;&#25968;&#25454;&#20013;&#65292;&#34920;&#24449;&#32676;&#20307;&#28436;&#21270;&#30340;&#20027;&#35201;&#26041;&#27861;&#19968;&#30452;&#26159;&#36890;&#36807;&#35782;&#21035;&#8220;&#20107;&#20214;&#8221;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#36890;&#24120;&#25551;&#36848;&#30340;&#20107;&#20214;&#65292;&#20363;&#22914;&#25910;&#32553;/&#22686;&#38271;&#12289;&#20998;&#35010;/&#21512;&#24182;&#65292;&#24448;&#24448;&#26159;&#20219;&#24847;&#23450;&#20041;&#30340;&#65292;&#23548;&#33268;&#29702;&#35770;/&#39044;&#23450;&#20041;&#31867;&#22411;&#19982;&#30495;&#23454;&#25968;&#25454;&#32676;&#20307;&#35266;&#23519;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#36229;&#36234;&#29616;&#26377;&#20998;&#31867;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#20107;&#20214;&#35270;&#20026;&#30001;&#25105;&#20204;&#31216;&#20026;&#8220;&#38754;&#21521;&#8221;&#30340;&#19968;&#32452;&#29305;&#23450;&#23450;&#37327;&#32500;&#24230;&#30340;&#32452;&#21512;&#25152;&#29305;&#24449;&#21270;&#30340;&#8220;&#21407;&#22411;&#8221;&#12290;&#32676;&#20307;&#21160;&#24577;&#30001;&#23427;&#20204;&#22312;&#38754;&#21521;&#31354;&#38388;&#20869;&#30340;&#20301;&#32622;&#25152;&#23450;&#20041;&#65292;&#20854;&#20013;&#21407;&#22411;&#20107;&#20214;&#21344;&#25454;&#26497;&#31471;&#20301;&#32622;&#12290;&#22240;&#27492;&#65292;&#19982;&#24378;&#21046;&#25191;&#34892;&#20005;&#26684;&#30340;&#20107;&#20214;&#31867;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20801;&#35768;&#28041;&#21450;&#21040;&#32676;&#20307;&#37051;&#36817;&#22810;&#20010;&#21407;&#22411;&#30340;&#21160;&#24577;&#30340;&#28151;&#21512;&#25551;&#36848;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#26469;&#33258;&#22810;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#32676;&#20307;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06771v1 Announce Type: new  Abstract: Groups -- such as clusters of points or communities of nodes -- are fundamental when addressing various data mining tasks. In temporal data, the predominant approach for characterizing group evolution has been through the identification of ``events". However, the events usually described in the literature, e.g., shrinks/growths, splits/merges, are often arbitrarily defined, creating a gap between such theoretical/predefined types and real-data group observations. Moving beyond existing taxonomies, we think of events as ``archetypes" characterized by a unique combination of quantitative dimensions that we call ``facets". Group dynamics are defined by their position within the facet space, where archetypal events occupy extremities. Thus, rather than enforcing strict event types, our approach can allow for hybrid descriptions of dynamics involving group proximity to multiple archetypes. We apply our framework to evolving groups from severa
&lt;/p&gt;</description></item><item><title>XB-MAML&#24341;&#20837;&#20102;&#23398;&#20064;&#21487;&#25193;&#23637;&#22522;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#24418;&#25104;&#26377;&#25928;&#21021;&#22987;&#21270;&#65292;&#35266;&#23519;&#24182;&#21033;&#29992;&#22522;&#19982;&#24494;&#35843;&#21442;&#25968;&#22312;&#21521;&#37327;&#31354;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#22312;&#22810;&#39046;&#22495;&#20803;&#23398;&#20064;&#20013;&#21462;&#24471;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.06768</link><description>&lt;p&gt;
XB-MAML: &#23398;&#20064;&#21487;&#25193;&#23637;&#22522;&#21442;&#25968;&#20197;&#23454;&#29616;&#20855;&#26377;&#24191;&#27867;&#20219;&#21153;&#35206;&#30422;&#33539;&#22260;&#30340;&#26377;&#25928;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06768
&lt;/p&gt;
&lt;p&gt;
XB-MAML&#24341;&#20837;&#20102;&#23398;&#20064;&#21487;&#25193;&#23637;&#22522;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#24418;&#25104;&#26377;&#25928;&#21021;&#22987;&#21270;&#65292;&#35266;&#23519;&#24182;&#21033;&#29992;&#22522;&#19982;&#24494;&#35843;&#21442;&#25968;&#22312;&#21521;&#37327;&#31354;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#22312;&#22810;&#39046;&#22495;&#20803;&#23398;&#20064;&#20013;&#21462;&#24471;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#36861;&#27714;&#26377;&#25928;&#30340;&#21021;&#22987;&#21270;&#27169;&#22411;&#65292;&#24050;&#25104;&#20026;&#22788;&#29702;&#26410;&#30693;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#20803;&#23398;&#20064;&#22120;&#35797;&#22270;&#28085;&#30422;&#24191;&#27867;&#30340;&#20219;&#21153;&#20998;&#24067;&#26102;&#65292;&#21363;&#36328;&#19981;&#21516;&#25968;&#25454;&#38598;&#25110;&#39046;&#22495;&#36827;&#34892;&#23398;&#20064;&#26102;&#65292;&#19968;&#20010;&#38480;&#21046;&#20173;&#28982;&#26174;&#32780;&#26131;&#35265;&#12290;&#26368;&#36817;&#65292;&#19968;&#32452;&#20316;&#21697;&#23581;&#35797;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#21021;&#22987;&#21270;&#26469;&#35206;&#30422;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#22312;&#33258;&#36866;&#24212;&#25193;&#23637;&#21021;&#22987;&#21270;&#26041;&#38754;&#21463;&#38480;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;XB-MAML&#65292;&#23427;&#23398;&#20064;&#21487;&#25193;&#23637;&#30340;&#22522;&#26412;&#21442;&#25968;&#65292;&#20854;&#20013;&#23427;&#20204;&#32447;&#24615;&#32452;&#21512;&#20197;&#24418;&#25104;&#32473;&#23450;&#20219;&#21153;&#30340;&#26377;&#25928;&#21021;&#22987;&#21270;&#12290;XB-MAML&#35266;&#23519;&#22522;&#19982;&#24494;&#35843;&#21442;&#25968;&#24352;&#25104;&#30340;&#21521;&#37327;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#20915;&#23450;&#26159;&#21542;&#25193;&#23637;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#39046;&#22495;&#20803;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#20316;&#21697;&#65292;&#24182;&#20026;&#33719;&#21462;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#26679;&#24402;&#32435;&#20559;&#24046;&#25171;&#24320;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06768v1 Announce Type: new  Abstract: Meta-learning, which pursues an effective initialization model, has emerged as a promising approach to handling unseen tasks. However, a limitation remains to be evident when a meta-learner tries to encompass a wide range of task distribution, e.g., learning across distinctive datasets or domains. Recently, a group of works has attempted to employ multiple model initializations to cover widely-ranging tasks, but they are limited in adaptively expanding initializations. We introduce XB-MAML, which learns expandable basis parameters, where they are linearly combined to form an effective initialization to a given task. XB-MAML observes the discrepancy between the vector space spanned by the basis and fine-tuned parameters to decide whether to expand the basis. Our method surpasses the existing works in the multi-domain meta-learning benchmarks and opens up new chances of meta-learning for obtaining the diverse inductive bias that can be com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20687;&#32032;&#32423;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#26657;&#20934;&#35823;&#24046;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#20197;&#25552;&#39640;&#26657;&#20934;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.06759</link><description>&lt;p&gt;
&#24179;&#22343;&#26657;&#20934;&#35823;&#24046;&#65306;&#19968;&#31181;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Average Calibration Error: A Differentiable Loss for Improved Reliability in Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06759
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20687;&#32032;&#32423;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#26657;&#20934;&#35823;&#24046;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#20197;&#25552;&#39640;&#26657;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#20135;&#29983;&#19982;&#32463;&#39564;&#35266;&#23519;&#19981;&#19968;&#33268;&#30340;&#36807;&#20110;&#33258;&#20449;&#30340;&#32467;&#26524;&#65292;&#36825;&#31181;&#26657;&#20934;&#38169;&#35823;&#25361;&#25112;&#30528;&#23427;&#20204;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25913;&#21892;&#20687;&#32032;&#32423;&#26657;&#20934;&#32780;&#19981;&#20250;&#25439;&#23475;&#20998;&#21106;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23613;&#31649;&#20351;&#29992;&#30828;&#20998;&#31665;&#65292;&#36825;&#31181;&#25439;&#22833;&#26159;&#30452;&#25509;&#21487;&#24494;&#30340;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#36817;&#20284;&#20294;&#21487;&#24494;&#30340;&#26367;&#20195;&#25110;&#36719;&#20998;&#31665;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#30340;&#27010;&#24565;&#65292;&#36825;&#19968;&#27010;&#24565;&#25512;&#24191;&#20102;&#26631;&#20934;&#30340;&#21487;&#38752;&#24615;&#22270;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#38598;&#32423;&#21035;&#32858;&#21512;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#32454;&#21270;&#26657;&#20934;&#30340;&#35270;&#35273;&#35780;&#20272;&#12290;&#20351;&#29992;mL1-ACE&#65292;&#25105;&#20204;&#23558;&#24179;&#22343;&#21644;&#26368;&#22823;&#26657;&#20934;&#35823;&#24046;&#20998;&#21035;&#38477;&#20302;&#20102;45%&#21644;55%&#65292;&#21516;&#26102;&#22312;BraTS 2021&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;87%&#30340;Dice&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20998;&#20139;&#25105;&#20204;&#30340;&#20195;&#30721;: https://github
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06759v1 Announce Type: cross  Abstract: Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: https://github
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Koopman&#21512;&#22863;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#21512;&#22863;&#20135;&#29983;&#20855;&#26377;&#39640;&#27169;&#22411;&#38388;&#26041;&#24046;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25913;&#21892;&#38598;&#25104;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.06757</link><description>&lt;p&gt;
Koopman&#21512;&#22863;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Koopman Ensembles for Probabilistic Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06757
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Koopman&#21512;&#22863;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#21512;&#22863;&#20135;&#29983;&#20855;&#26377;&#39640;&#27169;&#22411;&#38388;&#26041;&#24046;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25913;&#21892;&#38598;&#25104;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#32972;&#26223;&#19979;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Koopman&#31639;&#23376;&#30340;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#32477;&#22823;&#22810;&#25968;&#36825;&#31867;&#30740;&#31350;&#20165;&#38480;&#20110;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#32780;&#22312;&#20687;&#27668;&#35937;&#23398;&#21644;&#27668;&#35937;&#23398;&#36825;&#26679;&#30340;&#39046;&#22495;&#65292;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#27169;&#22411;&#21512;&#22863;&#20197;&#20135;&#29983;&#38543;&#26426;&#36755;&#20986;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30495;&#23454;&#36965;&#24863;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#23454;&#39564;&#34920;&#26126;&#65292;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#30340;&#38598;&#25104;&#36807;&#20998;&#33258;&#20449;&#65292;&#24182;&#19988;&#20351;&#29992;&#26126;&#30830;&#40723;&#21169;&#25104;&#21592;&#29983;&#25104;&#20855;&#26377;&#39640;&#27169;&#22411;&#38388;&#26041;&#24046;&#30340;&#39044;&#27979;&#30340;&#35757;&#32451;&#26631;&#20934;&#26497;&#22823;&#25913;&#21892;&#20102;&#38598;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06757v1 Announce Type: new  Abstract: In the context of an increasing popularity of data-driven models to represent dynamical systems, many machine learning-based implementations of the Koopman operator have recently been proposed. However, the vast majority of those works are limited to deterministic predictions, while the knowledge of uncertainty is critical in fields like meteorology and climatology. In this work, we investigate the training of ensembles of models to produce stochastic outputs. We show through experiments on real remote sensing image time series that ensembles of independently trained models are highly overconfident and that using a training criterion that explicitly encourages the members to produce predictions with high inter-model variances greatly improves the uncertainty quantification of the ensembles.
&lt;/p&gt;</description></item><item><title>ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.06754</link><description>&lt;p&gt;
ALaRM: &#36890;&#36807;&#20998;&#23618;&#22870;&#21169;&#24314;&#27169;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ALaRM: Align Language Models via Hierarchical Rewards Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06754
&lt;/p&gt;
&lt;p&gt;
ALaRM&#26159;&#31532;&#19968;&#20010;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#24314;&#27169;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;ALaRM&#65292;&#31532;&#19968;&#20010;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20174;&#20154;&#31867;&#21453;&#39304;&#27169;&#22411;&#20998;&#23618;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#40784;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#20154;&#31867;&#30417;&#30563;&#20449;&#21495;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36890;&#36807;&#23558;&#25972;&#20307;&#22870;&#21169;&#19982;&#29305;&#23450;&#26041;&#38754;&#30340;&#22870;&#21169;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#21644;&#19968;&#33268;&#22320;&#25351;&#23548;&#26397;&#30528;&#26399;&#26395;&#30340;&#32467;&#26524;&#21069;&#36827;&#65292;&#23588;&#20854;&#22312;&#22797;&#26434;&#21644;&#24320;&#25918;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#21644;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26426;&#21046;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#38271;&#31687;&#38382;&#39064;&#22238;&#31572;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#20351;&#29992;gpt-3.5-turbo&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06754v1 Announce Type: cross  Abstract: We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demon
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#30340;&#20219;&#21153;&#26080;&#20851;&#36890;&#20449;&#31574;&#30053;&#65292;&#22312;&#21512;&#20316;&#22810;&#26426;&#22120;&#20154;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26080;&#38656;&#24494;&#35843;&#30340;&#20219;&#21153;&#27867;&#21270;&#65292;&#25903;&#25345;&#26356;&#22810;Agent&#25968;&#37327;&#30340;&#25193;&#23637;&#65292;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06750</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#26080;&#20851;&#30340;&#36890;&#20449;&#26469;&#27867;&#21270;&#22810;Agent&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Generalising Multi-Agent Cooperation through Task-Agnostic Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06750
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#30340;&#20219;&#21153;&#26080;&#20851;&#36890;&#20449;&#31574;&#30053;&#65292;&#22312;&#21512;&#20316;&#22810;&#26426;&#22120;&#20154;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26080;&#38656;&#24494;&#35843;&#30340;&#20219;&#21153;&#27867;&#21270;&#65292;&#25903;&#25345;&#26356;&#22810;Agent&#25968;&#37327;&#30340;&#25193;&#23637;&#65292;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21512;&#20316;&#22810;&#26426;&#22120;&#20154;&#38382;&#39064;&#20013;&#30340;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#36890;&#20449;&#26041;&#27861;&#20960;&#20046;&#23436;&#20840;&#26159;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#65292;&#20026;&#27599;&#20010;&#29420;&#29305;&#30340;&#20219;&#21153;&#35757;&#32451;&#26032;&#30340;&#36890;&#20449;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#36866;&#29992;&#20110;&#32473;&#23450;&#29615;&#22659;&#20013;&#20219;&#20309;&#20219;&#21153;&#30340;&#36890;&#20449;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#31181;&#20302;&#25928;&#38382;&#39064;&#12290;&#25105;&#20204;&#33258;&#30417;&#30563;&#22320;&#20351;&#29992;&#19968;&#32452;&#33258;&#21160;&#32534;&#30721;&#22120;&#39044;&#20808;&#35757;&#32451;&#36890;&#20449;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#22870;&#21169;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#21487;&#21464;&#25968;&#37327;&#30340;Agent&#35266;&#27979;&#20013;&#23398;&#20064;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#25105;&#20204;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#31574;&#30053;&#20445;&#35777;&#25910;&#25947;&#65292;&#24182;&#23545;&#30001;&#25105;&#20204;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#36817;&#20284;&#24341;&#20837;&#30340;&#20540;&#35823;&#24046;&#19978;&#30028;&#36827;&#34892;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#26032;&#20219;&#21153;&#30340;&#26080;&#32541;&#36866;&#24212;&#65292;&#26080;&#38656;&#24494;&#35843;&#36890;&#20449;&#31574;&#30053;&#65292;&#20248;&#38597;&#22320;&#25903;&#25345;&#27604;&#35757;&#32451;&#26399;&#38388;&#26356;&#22810;Agent&#65292;&#20197;&#21450;&#26816;&#27979;&#33539;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06750v1 Announce Type: cross  Abstract: Existing communication methods for multi-agent reinforcement learning (MARL) in cooperative multi-robot problems are almost exclusively task-specific, training new communication strategies for each unique task. We address this inefficiency by introducing a communication strategy applicable to any task within a given environment. We pre-train the communication strategy without task-specific reward guidance in a self-supervised manner using a set autoencoder. Our objective is to learn a fixed-size latent Markov state from a variable number of agent observations. Under mild assumptions, we prove that policies using our latent representations are guaranteed to converge, and upper bound the value error introduced by our Markov state approximation. Our method enables seamless adaptation to novel tasks without fine-tuning the communication strategy, gracefully supports scaling to more agents than present during training, and detects out-of-di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25463;&#24452;&#23398;&#20064;&#29616;&#35937;&#25193;&#23637;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#65292;&#21457;&#29616;&#20020;&#24202;&#27880;&#37322;&#21644;&#29305;&#23450;&#25968;&#25454;&#22788;&#29702;&#26041;&#24335;&#21487;&#33021;&#35823;&#23548;&#27169;&#22411;&#24182;&#24433;&#21709;&#20998;&#21106;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#32531;&#35299;&#25463;&#24452;&#23398;&#20064;&#24433;&#21709;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.06748</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25463;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shortcut Learning in Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25463;&#24452;&#23398;&#20064;&#29616;&#35937;&#25193;&#23637;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#65292;&#21457;&#29616;&#20020;&#24202;&#27880;&#37322;&#21644;&#29305;&#23450;&#25968;&#25454;&#22788;&#29702;&#26041;&#24335;&#21487;&#33021;&#35823;&#23548;&#27169;&#22411;&#24182;&#24433;&#21709;&#20998;&#21106;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#32531;&#35299;&#25463;&#24452;&#23398;&#20064;&#24433;&#21709;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25463;&#24452;&#23398;&#20064;&#26159;&#19968;&#31181;&#29616;&#35937;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20248;&#20808;&#23398;&#20064;&#31616;&#21333;&#12289;&#28508;&#22312;&#35823;&#23548;&#30340;&#25968;&#25454;&#25552;&#31034;&#65292;&#36825;&#20123;&#25552;&#31034;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#24456;&#38590;&#27867;&#21270;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#25506;&#35752;&#36825;&#19968;&#29616;&#35937;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#65292;&#32780;&#36825;&#39033;&#30740;&#31350;&#23558;&#25463;&#24452;&#23398;&#20064;&#25506;&#32034;&#24310;&#20280;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20020;&#24202;&#27880;&#37322;&#22914;&#21345;&#23610;&#65292;&#20197;&#21450;&#25968;&#25454;&#38598;&#20013;&#38646;&#22635;&#20805;&#21367;&#31215;&#21644;&#20013;&#24515;&#35009;&#21098;&#30340;&#32452;&#21512;&#21487;&#20197;&#26080;&#24847;&#20013;&#20316;&#20026;&#25463;&#24452;&#65292;&#24433;&#21709;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#20294;&#24120;&#35265;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#35782;&#21035;&#21644;&#35780;&#20272;&#20102;&#25463;&#24452;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32531;&#35299;&#25463;&#24452;&#23398;&#20064;&#24433;&#21709;&#12289;&#25552;&#39640;&#20998;&#21106;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#25581;&#31034;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25463;&#24452;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06748v1 Announce Type: cross  Abstract: Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation. We demonstrate that clinical annotations such as calipers, and the combination of zero-padded convolutions and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy. We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks. In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models. By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#24452;&#21521;&#26680;&#20989;&#25968;&#30340;&#27888;&#21202;&#32423;&#25968;&#36924;&#36817;&#65292;&#24314;&#31435;&#20102;&#23545;&#26680;&#20989;&#25968;&#30340;&#36739;&#22909;&#36817;&#20284;&#65292;&#35777;&#23454;&#20102;&#21487;&#20197;&#20351;&#29992;&#27604;&#25991;&#29486;&#20013;&#26356;&#23567;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06731</link><description>&lt;p&gt;
&#23545;&#26680;&#20989;&#25968;&#30340;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Approximation of Kernel functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#24452;&#21521;&#26680;&#20989;&#25968;&#30340;&#27888;&#21202;&#32423;&#25968;&#36924;&#36817;&#65292;&#24314;&#31435;&#20102;&#23545;&#26680;&#20989;&#25968;&#30340;&#36739;&#22909;&#36817;&#20284;&#65292;&#35777;&#23454;&#20102;&#21487;&#20197;&#20351;&#29992;&#27604;&#25991;&#29486;&#20013;&#26356;&#23567;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#21508;&#31181;&#26041;&#27861;&#37117;&#24314;&#31435;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#32771;&#34385;&#30340;&#26680;&#20989;&#25968;&#22522;&#30784;&#20043;&#19978;&#12290;&#22312;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#26681;&#25454;&#38382;&#39064;&#21644;&#25968;&#25454;&#30340;&#29305;&#24449;&#36873;&#25321;&#26680;&#20989;&#25968;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20010;&#26680;&#20989;&#25968;&#25512;&#26029;&#37027;&#20123;&#27809;&#26377;&#35266;&#27979;&#21040;&#35299;&#37322;&#25968;&#25454;&#30340;&#28857;&#30340;&#21709;&#24212;&#21464;&#37327;&#12290;&#26412;&#25991;&#30740;&#31350;&#30340;&#25968;&#25454;&#20301;&#20110;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#32039;&#33268;&#38598;&#21512;&#20013;&#65292;&#24182;&#35299;&#20915;&#20102;&#26680;&#20989;&#25968;&#26412;&#36523;&#30340;&#36817;&#20284;&#38382;&#39064;&#12290;&#26032;&#26041;&#27861;&#32771;&#34385;&#20102;&#24452;&#21521;&#26680;&#20989;&#25968;&#30340;&#27888;&#21202;&#32423;&#25968;&#36924;&#36817;&#12290;&#23545;&#20110;&#21333;&#20301;&#31435;&#26041;&#20307;&#19978;&#30340;&#39640;&#26031;&#26680;&#20989;&#25968;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#20851;&#32852;&#29305;&#24449;&#20989;&#25968;&#30340;&#19978;&#38480;&#65292;&#36825;&#20010;&#19978;&#38480;&#38543;&#25351;&#25968;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#35813;&#26032;&#26041;&#27861;&#35777;&#26126;&#20102;&#27604;&#25991;&#29486;&#20013;&#32771;&#34385;&#30340;&#26356;&#23567;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#20174;&#32780;&#25972;&#20307;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#36817;&#20284;&#12290;&#36825;&#19968;&#25913;&#36827;&#35777;&#23454;&#20102;&#20302;&#31209;&#36924;&#36817;&#26041;&#27861;&#65292;&#22914;Nystrom&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06731v1 Announce Type: cross  Abstract: Various methods in statistical learning build on kernels considered in reproducing kernel Hilbert spaces. In applications, the kernel is often selected based on characteristics of the problem and the data. This kernel is then employed to infer response variables at points, where no explanatory data were observed. The data considered here are located in compact sets in higher dimensions and the paper addresses approximations of the kernel itself. The new approach considers Taylor series approximations of radial kernel functions. For the Gauss kernel on the unit cube, the paper establishes an upper bound of the associated eigenfunctions, which grows only polynomially with respect to the index. The novel approach substantiates smaller regularization parameters than considered in the literature, overall leading to better approximations. This improvement confirms low rank approximation methods such as the Nystr\"om method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23545;&#27604;&#65288;ProCo&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20272;&#35745;&#29305;&#24449;&#31354;&#38388;&#20013;&#27599;&#20010;&#31867;&#21035;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#23545;&#27604;&#23545;&#36827;&#34892;&#37319;&#26679;</title><link>https://arxiv.org/abs/2403.06726</link><description>&lt;p&gt;
&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#30340;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Contrastive Learning for Long-Tailed Visual Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06726
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23545;&#27604;&#65288;ProCo&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20272;&#35745;&#29305;&#24449;&#31354;&#38388;&#20013;&#27599;&#20010;&#31867;&#21035;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#23545;&#27604;&#23545;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#20998;&#24067;&#32463;&#24120;&#20986;&#29616;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#65292;&#20854;&#20013;&#22823;&#37327;&#23569;&#25968;&#31867;&#21035;&#21253;&#21547;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#38382;&#39064;&#20005;&#37325;&#24433;&#21709;&#20102;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#31639;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#24179;&#34913;&#30340;&#35757;&#32451;&#38598;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22312;&#32531;&#35299;&#25968;&#25454;&#19981;&#24179;&#34913;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#21463;&#21040;&#22266;&#26377;&#25361;&#25112;&#30340;&#22256;&#25200;&#65306;&#23427;&#38656;&#35201;&#36275;&#22815;&#22823;&#25209;&#27425;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#26500;&#24314;&#28085;&#30422;&#25152;&#26377;&#31867;&#21035;&#30340;&#23545;&#27604;&#23545;&#65292;&#28982;&#32780;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#28385;&#36275;&#36825;&#19968;&#35201;&#27714;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23545;&#27604;&#65288;ProCo&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20272;&#35745;&#29305;&#24449;&#31354;&#38388;&#20013;&#27599;&#20010;&#31867;&#21035;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#23545;&#27604;&#23545;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06726v1 Announce Type: new  Abstract: Long-tailed distributions frequently emerge in real-world data, where a large number of minority categories contain a limited number of samples. Such imbalance issue considerably impairs the performance of standard supervised learning algorithms, which are mainly designed for balanced training sets. Recent investigations have revealed that supervised contrastive learning exhibits promising potential in alleviating the data imbalance. However, the performance of supervised contrastive learning is plagued by an inherent challenge: it necessitates sufficiently large batches of training data to construct contrastive pairs that cover all categories, yet this requirement is difficult to meet in the context of class-imbalanced data. To overcome this obstacle, we propose a novel probabilistic contrastive (ProCo) learning algorithm that estimates the data distribution of the samples from each class in the feature space, and samples contrastive pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.06725</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#37325;&#35201;&#24615;&#26426;&#21046;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#22522;&#20110;&#23398;&#29983;&#30340;&#21382;&#21490;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#30693;&#35782;&#25484;&#25569;&#31243;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;KT&#65288;DLKT&#65289;&#26041;&#27861;&#22312;KT&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#22914;&#39044;&#31639;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#20114;&#21160;&#38750;&#24120;&#26377;&#38480;&#65292;&#21363;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#12290;&#30452;&#25509;&#22312;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DLKT&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#24456;&#38590;&#36873;&#25321;&#36866;&#24403;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;KT&#26694;&#26550;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#30427;&#34892;&#30340;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hodge-Laplacian&#24322;&#26500;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;HL-HGAT&#65289;&#65292;&#36890;&#36807;HL&#21367;&#31215;&#28388;&#27874;&#22120;&#12289;&#21333;&#32431;&#25237;&#24433;&#21644;&#21333;&#32431;&#27880;&#24847;&#27744;&#21270;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#23398;&#20064;&#36328;k-&#21333;&#32431;&#20307;&#30340;&#24322;&#26500;&#20449;&#21495;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.06687</link><description>&lt;p&gt;
&#29992;Hodge-Laplacian&#21644;&#27880;&#24847;&#26426;&#21046;&#36827;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hodge-Laplacian&#24322;&#26500;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;HL-HGAT&#65289;&#65292;&#36890;&#36807;HL&#21367;&#31215;&#28388;&#27874;&#22120;&#12289;&#21333;&#32431;&#25237;&#24433;&#21644;&#21333;&#32431;&#27880;&#24847;&#27744;&#21270;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#23398;&#20064;&#36328;k-&#21333;&#32431;&#20307;&#30340;&#24322;&#26500;&#20449;&#21495;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#25429;&#25417;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#23558;&#22270;&#20316;&#20026;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#30340;&#26032;&#35270;&#35282;&#65292;&#21253;&#21547;&#33410;&#28857;&#12289;&#36793;&#12289;&#19977;&#35282;&#24418;&#21644;k-&#21333;&#32431;&#20307;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#22312;&#20219;&#20309;k-&#21333;&#32431;&#20307;&#19978;&#23450;&#20041;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;Hodge-Laplacian&#24322;&#26500;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;HL-HGAT&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#36328;k-&#21333;&#32431;&#20307;&#30340;&#24322;&#26500;&#20449;&#21495;&#34920;&#31034;&#12290;HL-HGAT&#21253;&#21547;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;HL&#21367;&#31215;&#28388;&#27874;&#22120;&#65288;HL-filters&#65289;&#12289;&#21333;&#32431;&#25237;&#24433;&#65288;SP&#65289;&#21644;&#21333;&#32431;&#27880;&#24847;&#27744;&#21270;&#65288;SAP&#65289;&#36816;&#31639;&#31526;&#65292;&#24212;&#29992;&#20110;k-&#21333;&#32431;&#20307;&#12290;HL-filters&#21033;&#29992;&#30001;Hodge-Laplacian&#65288;HL&#65289;&#31639;&#23376;&#32534;&#30721;&#30340;k-&#21333;&#32431;&#20307;&#30340;&#29420;&#29305;&#25299;&#25169;&#32467;&#26500;&#65292;&#22312;k-th HL&#31639;&#23376;&#30340;&#39057;&#35889;&#22495;&#20869;&#36816;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;HL-filters&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#23637;&#31034;&#20102;&#22312;&#35889;&#22495;&#35299;&#20915;&#24322;&#26500;&#22270;&#27880;&#24847;&#36328;k-&#21333;&#32431;&#20307;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06687v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have proven effective in capturing relationships among nodes in a graph. This study introduces a novel perspective by considering a graph as a simplicial complex, encompassing nodes, edges, triangles, and $k$-simplices, enabling the definition of graph-structured data on any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous graph attention network (HL-HGAT), designed to learn heterogeneous signal representations across $k$-simplices. The HL-HGAT incorporates three key components: HL convolutional filters (HL-filters), simplicial projection (SP), and simplicial attention pooling (SAP) operators, applied to $k$-simplices. HL-filters leverage the unique topology of $k$-simplices encoded by the Hodge-Laplacian (HL) operator, operating within the spectral domain of the $k$-th HL operator. To address computation challenges, we introduce a polynomial approximation for HL-filters, exhibiting spatia
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22312;&#40654;&#26364;&#20248;&#21270;&#20013;&#37319;&#29992;&#27010;&#29575;&#26799;&#24230;&#35745;&#31639;&#35302;&#21457;&#22120;&#30340;Loopless SVRG&#21644;PAGE&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#35777;&#26126;&#12289;&#25552;&#39640;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06677</link><description>&lt;p&gt;
&#27969;&#30021;&#30340;&#40654;&#26364;&#39046;&#22495;&#65306;&#26080;&#29615;&#26041;&#24046;&#20943;&#23569;&#30340;&#39640;&#25928;&#40654;&#26364;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Streamlining in the Riemannian Realm: Efficient Riemannian Optimization with Loopless Variance Reduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06677
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22312;&#40654;&#26364;&#20248;&#21270;&#20013;&#37319;&#29992;&#27010;&#29575;&#26799;&#24230;&#35745;&#31639;&#35302;&#21457;&#22120;&#30340;Loopless SVRG&#21644;PAGE&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#35777;&#26126;&#12289;&#25552;&#39640;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#27431;&#20960;&#37324;&#24471;&#21644;&#40654;&#26364;&#35774;&#32622;&#20013;&#20351;&#29992;&#30340;&#20851;&#38190;&#26041;&#24046;&#20943;&#23569;&#26426;&#21046;&#12290;&#40654;&#26364;&#26041;&#24046;&#20943;&#23569;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#21452;&#24490;&#29615;&#32467;&#26500;&#65292;&#22312;&#27599;&#20010;&#24490;&#29615;&#30340;&#24320;&#22987;&#35745;&#31639;&#23436;&#25972;&#26799;&#24230;&#12290;&#30830;&#23450;&#26368;&#20339;&#20869;&#24490;&#29615;&#38271;&#24230;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#21462;&#20915;&#20110;&#24378;&#20984;&#24615;&#25110;&#20809;&#28369;&#24230;&#24120;&#25968;&#65292;&#36825;&#20123;&#24120;&#25968;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#25110;&#38590;&#20197;&#20272;&#35745;&#12290;&#21463;&#27431;&#20960;&#37324;&#24471;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#29615;&#40654;&#26364;SVRG&#65288;R-LSVRG&#65289;&#21644;PAGE&#65288;R-PAGE&#65289;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#29992;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#30828;&#24065;&#32763;&#36716;&#35302;&#21457;&#30340;&#27010;&#29575;&#26799;&#24230;&#35745;&#31639;&#26367;&#25442;&#20102;&#22806;&#24490;&#29615;&#65292;&#30830;&#20445;&#20102;&#26356;&#31616;&#21333;&#30340;&#35777;&#26126;&#12289;&#39640;&#25928;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#23574;&#38160;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#23558;R-PAGE&#20316;&#20026;&#38750;&#20984;&#40654;&#26364;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#37325;&#35201;&#35774;&#32622;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06677v1 Announce Type: cross  Abstract: In this study, we investigate stochastic optimization on Riemannian manifolds, focusing on the crucial variance reduction mechanism used in both Euclidean and Riemannian settings. Riemannian variance-reduced methods usually involve a double-loop structure, computing a full gradient at the start of each loop. Determining the optimal inner loop length is challenging in practice, as it depends on strong convexity or smoothness constants, which are often unknown or hard to estimate. Motivated by Euclidean methods, we introduce the Riemannian Loopless SVRG (R-LSVRG) and PAGE (R-PAGE) methods. These methods replace the outer loop with probabilistic gradient computation triggered by a coin flip in each iteration, ensuring simpler proofs, efficient hyperparameter selection, and sharp convergence guarantees. Using R-PAGE as a framework for non-convex Riemannian optimization, we demonstrate its applicability to various important settings. For ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38544;&#31169;&#25935;&#24863;&#39046;&#22495;&#20013;&#22914;&#20309;&#35774;&#35745;&#19968;&#31181;FL&#21327;&#35758;&#65292;&#26082;&#33021;&#20445;&#35777;&#38544;&#31169;&#65292;&#21448;&#33021;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#35745;&#20986;&#23545;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#26377;&#30410;&#22788;&#30340;&#21327;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.06672</link><description>&lt;p&gt;
&#22312;&#38544;&#31169;&#25935;&#24863;&#39046;&#22495;&#20013;&#20174;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#21487;&#35777;&#26126;&#30340;&#20114;&#24800;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38544;&#31169;&#25935;&#24863;&#39046;&#22495;&#20013;&#22914;&#20309;&#35774;&#35745;&#19968;&#31181;FL&#21327;&#35758;&#65292;&#26082;&#33021;&#20445;&#35777;&#38544;&#31169;&#65292;&#21448;&#33021;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#35745;&#20986;&#23545;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#26377;&#30410;&#22788;&#30340;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#25968;&#25454;&#25152;&#26377;&#32773;&#36890;&#36807;&#20174;&#24444;&#27492;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#30410;&#26469;&#35757;&#32451;&#20934;&#30830;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26381;&#21153;&#22120;&#21487;&#20197;&#35774;&#35745;&#19968;&#31181;&#23545;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#26377;&#21033;&#30340;FL&#21327;&#35758;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#22343;&#20540;&#20272;&#35745;&#21644;&#20984;&#38543;&#26426;&#20248;&#21270;&#32972;&#26223;&#19979;&#23384;&#22312;&#30456;&#20114;&#26377;&#21033;&#21327;&#35758;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#23545;&#31216;&#38544;&#31169;&#20559;&#22909;&#19979;&#65292;&#26368;&#22823;&#21270;&#24635;&#23458;&#25143;&#25928;&#29992;&#30340;&#21327;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26368;&#22823;&#21270;&#26368;&#32456;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21327;&#35758;&#65292;&#24182;&#22312;&#21512;&#25104;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06672v1 Announce Type: cross  Abstract: Cross-silo federated learning (FL) allows data owners to train accurate machine learning models by benefiting from each others private datasets. Unfortunately, the model accuracy benefits of collaboration are often undermined by privacy defenses. Therefore, to incentivize client participation in privacy-sensitive domains, a FL protocol should strike a delicate balance between privacy guarantees and end-model accuracy. In this paper, we study the question of when and how a server could design a FL protocol provably beneficial for all participants. First, we provide necessary and sufficient conditions for the existence of mutually beneficial protocols in the context of mean estimation and convex stochastic optimization. We also derive protocols that maximize the total clients' utility, given symmetric privacy preferences. Finally, we design protocols maximizing end-model accuracy and demonstrate their benefits in synthetic experiments.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#25506;&#32034;&#20102;Tangles&#30340;&#28508;&#21147;&#65292;&#23558;&#25968;&#25454;&#38598;&#20013;&#30340;&#31751;&#19982;&#22270;&#20013;&#30340;Tangles&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#22871;&#20851;&#20110;&#20174;&#39640;&#26031;&#28151;&#21512;&#20013;&#25277;&#21462;&#30340;&#25968;&#25454;&#38598;&#20013;Tangles&#30340;&#23450;&#37327;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.06671</link><description>&lt;p&gt;
&#35299;&#24320;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Untangling Gaussian Mixtures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06671
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#25506;&#32034;&#20102;Tangles&#30340;&#28508;&#21147;&#65292;&#23558;&#25968;&#25454;&#38598;&#20013;&#30340;&#31751;&#19982;&#22270;&#20013;&#30340;Tangles&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#22871;&#20851;&#20110;&#20174;&#39640;&#26031;&#28151;&#21512;&#20013;&#25277;&#21462;&#30340;&#25968;&#25454;&#38598;&#20013;Tangles&#30340;&#23450;&#37327;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tangles&#26368;&#21021;&#26159;&#20316;&#20026;&#19968;&#31181;&#27010;&#24565;&#24341;&#20837;&#30340;&#65292;&#29992;&#20110;&#24418;&#24335;&#21270;&#22270;&#20013;&#30340;&#39640;&#36830;&#25509;&#21306;&#22495;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#21457;&#29616;Tangles&#20063;&#26159;&#32467;&#26500;&#22270;&#35770;&#21644;&#25968;&#25454;&#31185;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#65306;&#24403;&#23558;&#25968;&#25454;&#38598;&#20013;&#30340;&#30456;&#20284;&#24615;&#35299;&#37322;&#20026;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#26102;&#65292;&#23547;&#25214;&#25968;&#25454;&#20013;&#30340;&#31751;&#22522;&#26412;&#19978;&#31561;&#21516;&#20110;&#22312;&#24213;&#23618;&#22270;&#20013;&#23547;&#25214;Tangles&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#20013;Tangles&#30340;&#28508;&#21147;&#65292;&#20316;&#20026;&#23545;&#31751;&#30340;&#24418;&#24335;&#21270;&#30740;&#31350;&#12290;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#36890;&#24120;&#36981;&#24490;&#27491;&#24577;&#20998;&#24067;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#20851;&#20110;&#20174;&#39640;&#26031;&#28151;&#21512;&#20013;&#25277;&#21462;&#30340;&#25968;&#25454;&#38598;&#20013;Tangles&#30340;&#23450;&#37327;&#29702;&#35770;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20026;&#25968;&#25454;&#36171;&#20104;&#20102;&#19968;&#20010;&#27169;&#25311;&#28857;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#22270;&#32467;&#26500;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;Tangle&#29702;&#35770;&#24212;&#29992;&#20110;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#26465;&#20214;&#65292;&#26681;&#25454;&#36825;&#20123;&#26465;&#20214;&#65292;&#19982;&#36793;&#38469;&#39640;&#26031;&#20998;&#24067;&#30456;&#20851;&#30340;Tangles&#20960;&#20046;&#24517;&#28982;&#20250;&#28176;&#36817;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06671v1 Announce Type: cross  Abstract: Tangles were originally introduced as a concept to formalize regions of high connectivity in graphs. In recent years, they have also been discovered as a link between structural graph theory and data science: when interpreting similarity in data sets as connectivity between points, finding clusters in the data essentially amounts to finding tangles in the underlying graphs. This paper further explores the potential of tangles in data sets as a means for a formal study of clusters. Real-world data often follow a normal distribution. Accounting for this, we develop a quantitative theory of tangles in data sets drawn from Gaussian mixtures. To this end, we equip the data with a graph structure that models similarity between the points and allows us to apply tangle theory to the data. We provide explicit conditions under which tangles associated with the marginal Gaussian distributions exist asymptotically almost surely. This can be consid
&lt;/p&gt;</description></item><item><title>PeerAiD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#21516;&#34892;&#32593;&#32476;&#23398;&#20064;&#23398;&#29983;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#33258;&#36523;&#30340;&#31034;&#20363;&#65292;&#26469;&#25552;&#21319;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06668</link><description>&lt;p&gt;
PeerAiD&#65306;&#25913;&#21892;&#19987;&#19994;&#21516;&#34892;&#23548;&#24072;&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06668
&lt;/p&gt;
&lt;p&gt;
PeerAiD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#21516;&#34892;&#32593;&#32476;&#23398;&#20064;&#23398;&#29983;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#33258;&#36523;&#30340;&#31034;&#20363;&#65292;&#26469;&#25552;&#21319;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#22312;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#26102;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#25239;&#24615;&#33976;&#39311;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#65292;&#26088;&#22312;&#25552;&#28860;&#25945;&#24072;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#25913;&#36827;&#23567;&#22411;&#23398;&#29983;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PeerAiD&#65292;&#36890;&#36807;&#35753;&#21516;&#34892;&#32593;&#32476;&#23398;&#20064;&#23398;&#29983;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#33258;&#36523;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26469;&#25913;&#36827;&#23545;&#25239;&#24615;&#33976;&#39311;&#12290;PeerAiD&#26159;&#19968;&#31181;&#23545;&#25239;&#24615;&#33976;&#39311;&#65292;&#21516;&#26102;&#35757;&#32451;&#21516;&#34892;&#32593;&#32476;&#21644;&#23398;&#29983;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06668v1 Announce Type: new  Abstract: Adversarial robustness of the neural network is a significant concern when it is applied to security-critical domains. In this situation, adversarial distillation is a promising option which aims to distill the robustness of the teacher network to improve the robustness of a small student network. Previous works pretrain the teacher network to make it robust to the adversarial examples aimed at itself. However, the adversarial examples are dependent on the parameters of the target network. The fixed teacher network inevitably degrades its robustness against the unseen transferred adversarial examples which targets the parameters of the student network in the adversarial distillation process. We propose PeerAiD to make a peer network learn the adversarial examples of the student network instead of adversarial examples aimed at itself. PeerAiD is an adversarial distillation that trains the peer network and the student network simultaneousl
&lt;/p&gt;</description></item><item><title>Smart-Infinity&#36890;&#36807;&#22312;&#30495;&#23454;&#31995;&#32479;&#19978;&#20351;&#29992;&#36817;&#23384;&#20648;&#22788;&#29702;&#35774;&#22791;&#65292;&#35299;&#20915;&#20102;&#23384;&#20648;&#22806;&#37096;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#23384;&#20648;&#24102;&#23485;&#29942;&#39048;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.06664</link><description>&lt;p&gt;
Smart-Infinity: &#20351;&#29992;&#30495;&#23454;&#31995;&#32479;&#19978;&#30340;&#36817;&#23384;&#20648;&#22788;&#29702;&#36827;&#34892;&#24555;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06664
&lt;/p&gt;
&lt;p&gt;
Smart-Infinity&#36890;&#36807;&#22312;&#30495;&#23454;&#31995;&#32479;&#19978;&#20351;&#29992;&#36817;&#23384;&#20648;&#22788;&#29702;&#35774;&#22791;&#65292;&#35299;&#20915;&#20102;&#23384;&#20648;&#22806;&#37096;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#23384;&#20648;&#24102;&#23485;&#29942;&#39048;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#24040;&#22823;&#36827;&#23637;&#20027;&#35201;&#26159;&#30001;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#39537;&#21160;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#22823;&#37327;&#20869;&#23384;&#23481;&#37327;&#30340;&#38656;&#27714;&#65292;&#38656;&#35201;&#20351;&#29992;&#25968;&#21313;&#20010;GPU&#25165;&#33021;&#28385;&#36275;&#23481;&#37327;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23384;&#20648;&#22806;&#37096;&#35757;&#32451;&#65292;&#23427;&#20351;&#29992;&#20027;&#26426;&#20869;&#23384;&#21644;&#23384;&#20648;&#20316;&#20026;&#25193;&#23637;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26174;&#28982;&#20250;&#20197;&#23384;&#20648;&#24102;&#23485;&#29942;&#39048;&#20026;&#20195;&#20215;&#65292;&#22240;&#20026;&#23384;&#20648;&#35774;&#22791;&#30340;&#24102;&#23485;&#27604;GPU&#35774;&#22791;&#20869;&#23384;&#20302;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;Smart-Infinity&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31995;&#32479;&#19978;&#20351;&#29992;&#36817;&#23384;&#20648;&#22788;&#29702;&#35774;&#22791;&#26469;&#35299;&#20915;&#23384;&#20648;&#22806;&#37096;&#21270;&#30340;LLM&#35757;&#32451;&#30340;&#23384;&#20648;&#24102;&#23485;&#29942;&#39048;&#12290;Smart-Infinity&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;SmartUpdate&#65292;&#23427;&#22312;&#33258;&#23450;&#20041;&#36817;&#23384;&#20648;&#21152;&#36895;&#22120;&#19978;&#25191;&#34892;&#21442;&#25968;&#26356;&#26032;&#12290;&#25105;&#20204;&#21457;&#29616;&#23558;&#21442;&#25968;&#26356;&#26032;&#31227;&#21040;&#23384;&#20648;&#31471;&#21487;&#20197;&#28040;&#38500;&#22823;&#37096;&#20998;&#23384;&#20648;&#27969;&#37327;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06664v1 Announce Type: cross  Abstract: The recent huge advance of Large Language Models (LLMs) is mainly driven by the increase in the number of parameters. This has led to substantial memory capacity requirements, necessitating the use of dozens of GPUs just to meet the capacity. One popular solution to this is storage-offloaded training, which uses host memory and storage as an extended memory hierarchy. However, this obviously comes at the cost of storage bandwidth bottleneck because storage devices have orders of magnitude lower bandwidth compared to that of GPU device memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck of storage-offloaded LLM training using near-storage processing devices on a real system. The main component of Smart-Infinity is SmartUpdate, which performs parameter updates on custom near-storage accelerators. We identify that moving parameter updates to the storage side removes most of the storage traffic. In addition, we p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2403.06659</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#27979;&#35797;&#26102;&#20020;&#24202;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06659
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#23454;&#36341;&#20013;&#29992;&#20110;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#30142;&#30149;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#12290;&#22312;&#26410;&#32463;&#27880;&#37322;&#30340;ECG&#25968;&#25454;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;eSSL&#65289;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#34920;&#24449;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#21487;&#20197;&#22312;&#25253;&#21578;&#20013;&#25214;&#21040;&#30340;&#20020;&#24202;&#30693;&#35782;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#27169;&#24577;&#23398;&#20064;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#65292;&#25552;&#20986;&#20102;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#38646;&#26679;&#26412;ECG&#20998;&#31867;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21033;&#29992;&#22806;&#37096;&#19987;&#23478;&#39564;&#35777;&#30340;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#65292;&#29983;&#25104;&#26356;&#22810;&#20851;&#20110;&#24739;&#32773;&#30149;&#21490;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06659v1 Announce Type: cross  Abstract: Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Ricci&#27969;&#30340;&#22823;&#33041;&#34920;&#38754;&#21327;&#26041;&#24046;&#25551;&#36848;&#31526;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#29992;&#20110;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;</title><link>https://arxiv.org/abs/2403.06645</link><description>&lt;p&gt;
&#22522;&#20110;Ricci&#27969;&#30340;&#22823;&#33041;&#34920;&#38754;&#21327;&#26041;&#24046;&#25551;&#36848;&#31526;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Ricci flow-based brain surface covariance descriptors for Alzheimer disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Ricci&#27969;&#30340;&#22823;&#33041;&#34920;&#38754;&#21327;&#26041;&#24046;&#25551;&#36848;&#31526;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#29992;&#20110;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;MRI&#22823;&#33041;&#25195;&#25551;&#20013;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#24182;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#12290;&#38543;&#30528;3D&#25104;&#20687;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;3D&#25968;&#25454;&#37319;&#38598;&#27604;&#20854;2D&#23545;&#24212;&#29289;&#26356;&#20855;&#21487;&#34892;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#27700;&#32447;&#65292;&#20174;&#30382;&#23618;&#34920;&#38754;&#21033;&#29992;Ricci&#33021;&#37327;&#20248;&#21270;&#25552;&#21462;&#26032;&#39062;&#30340;&#22522;&#20110;&#21327;&#26041;&#24046;&#30340;&#25551;&#36848;&#31526;&#65292;&#32780;&#38750;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#21521;&#37327;&#12290;&#36825;&#20123;&#21327;&#26041;&#24046;&#25551;&#36848;&#31526;&#26159;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#38750;&#32447;&#24615;&#27969;&#24418;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#27492;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#39640;&#26031;&#24452;&#21521;&#22522;&#20989;&#25968;&#23558;&#22522;&#20110;&#27969;&#24418;&#30340;&#20998;&#31867;&#24212;&#29992;&#20110;3D&#24418;&#29366;&#38382;&#39064;&#12290;&#23558;&#36825;&#19968;&#26032;&#39062;&#29305;&#24449;&#24212;&#29992;&#20110;&#24322;&#24120;&#30382;&#23618;&#33041;&#24418;&#24577;&#23398;&#20998;&#26512;&#20013;&#65292;&#21487;&#20197;&#29992;&#20110;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#22312;&#22823;&#32422;&#20004;&#30334;&#20010;3D MRI&#22823;&#33041;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36825;&#20123;&#27169;&#22411;&#26469;&#33258;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758; (ADNI) &#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06645v1 Announce Type: cross  Abstract: Automated feature extraction from MRI brain scans and diagnosis of Alzheimer's disease are ongoing challenges. With advances in 3D imaging technology, 3D data acquisition is becoming more viable and efficient than its 2D counterpart. Rather than using feature-based vectors, in this paper, for the first time, we suggest a pipeline to extract novel covariance-based descriptors from the cortical surface using the Ricci energy optimization. The covariance descriptors are components of the nonlinear manifold of symmetric positive-definite matrices, thus we focus on using the Gaussian radial basis function to apply manifold-based classification to the 3D shape problem. Applying this novel signature to the analysis of abnormal cortical brain morphometry allows for diagnosing Alzheimer's disease. Experimental studies performed on about two hundred 3D MRI brain models, gathered from Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset dem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35760;&#24518;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;LLMs&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#24615;&#33021;&#35780;&#20272;&#30340;&#26080;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06644</link><description>&lt;p&gt;
&#22823;&#35937;&#27704;&#36828;&#19981;&#20250;&#24536;&#35760;&#65306;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#35760;&#24518;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Elephants Never Forget: Testing Language Models for Memorization of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35760;&#24518;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;LLMs&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#24615;&#33021;&#35780;&#20272;&#30340;&#26080;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35768;&#22810;&#20154;&#24050;&#32463;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#20294;&#25968;&#25454;&#27745;&#26579;&#21644;&#35760;&#24518;&#21270;&#30340;&#20851;&#38190;&#38382;&#39064;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#31616;&#21333;&#30340;&#23450;&#24615;&#27979;&#35797;&#24320;&#22987;&#65292;&#27979;&#35797;LLM&#26159;&#21542;&#30693;&#36947;&#29305;&#24449;&#30340;&#21517;&#31216;&#21644;&#20540;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#35780;&#20272;&#27745;&#26579;&#31243;&#24230;&#65292;&#21253;&#25324;&#29992;&#20110;&#26465;&#20214;&#20998;&#24067;&#24314;&#27169;&#30340;&#32479;&#35745;&#27979;&#35797;&#21644;&#22235;&#20010;&#35782;&#21035;&#35760;&#24518;&#21270;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;LLMs&#22312;&#35768;&#22810;&#27969;&#34892;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#36825;&#31181;&#26292;&#38706;&#21487;&#33021;&#23548;&#33268;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#26080;&#25928;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#22240;&#20026;LLMs&#23454;&#38469;&#19978;&#24050;&#32463;&#36866;&#24212;&#20102;&#27979;&#35797;&#38598;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#31181;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#25968;&#25454;&#30340;&#37325;&#35201;&#32479;&#35745;&#20449;&#24687;&#65292;&#20294;&#26080;&#27861;&#36880;&#23383;&#22797;&#21046;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#65292;&#23613;&#31649;&#35265;&#36807;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06644v1 Announce Type: cross  Abstract: While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Starting with simple qualitative tests for whether an LLM knows the names and values of features, we introduce a variety of different techniques to assess the degrees of contamination, including statistical tests for conditional distribution modeling and four tests that identify memorization. Our investigation reveals that LLMs are pre-trained on many popular tabular datasets. This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to the test set. Interestingly, we also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim. On these datasets, although seen during 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31354;&#38388;CO2&#27987;&#24230;&#20998;&#24067;&#25552;&#20986;&#20004;&#31181;&#26032;&#29305;&#24449;&#65292;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#36827;&#34892;&#37327;&#21270;&#20998;&#26512;&#21518;&#21457;&#29616;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#22312;&#33258;&#28982;&#36890;&#39118;&#25151;&#38388;&#20013;&#30340;&#21344;&#29992;&#29366;&#24577;&#26816;&#27979;&#20934;&#30830;&#29575;&#26368;&#22810;&#21487;&#25552;&#39640;14.8&#20010;&#30334;&#20998;&#28857;&#65292;&#36798;&#21040;83.2&#65285;&#65288;F1&#20998;&#25968;0.84&#65289;&#12290;&#26377;&#36890;&#39118;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;87.6&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.06643</link><description>&lt;p&gt;
CO2&#22312;&#33258;&#28982;&#36890;&#39118;&#23398;&#26657;&#24314;&#31569;&#29289;&#20013;&#29992;&#20110;&#21344;&#29992;&#26816;&#27979;&#30340;&#31354;&#38388;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spatial features of CO2 for occupancy detection in a naturally ventilated school building
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06643
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31354;&#38388;CO2&#27987;&#24230;&#20998;&#24067;&#25552;&#20986;&#20004;&#31181;&#26032;&#29305;&#24449;&#65292;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#36827;&#34892;&#37327;&#21270;&#20998;&#26512;&#21518;&#21457;&#29616;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#22312;&#33258;&#28982;&#36890;&#39118;&#25151;&#38388;&#20013;&#30340;&#21344;&#29992;&#29366;&#24577;&#26816;&#27979;&#20934;&#30830;&#29575;&#26368;&#22810;&#21487;&#25552;&#39640;14.8&#20010;&#30334;&#20998;&#28857;&#65292;&#36798;&#21040;83.2&#65285;&#65288;F1&#20998;&#25968;0.84&#65289;&#12290;&#26377;&#36890;&#39118;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;87.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#21344;&#29992;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#24314;&#31569;&#33021;&#28304;&#25928;&#29575;&#21644;&#23621;&#20303;&#32773;&#33298;&#36866;&#24230;&#12290;&#22522;&#20110;CO2&#20256;&#24863;&#22120;&#30340;&#21344;&#29992;&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#25104;&#26412;&#20302;&#12289;&#24178;&#25200;&#23567;&#65292;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#33258;&#28982;&#36890;&#39118;&#24314;&#31569;&#20013;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#36890;&#39118;&#34892;&#20026;&#21644;&#27979;&#37327;&#31383;&#25143;&#23454;&#38469;&#25442;&#27668;&#37327;&#30340;&#22256;&#38590;&#65292;&#22522;&#20110;CO2&#30340;&#21344;&#29992;&#26816;&#27979;&#20934;&#30830;&#24615;&#36890;&#24120;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;CO2&#27987;&#24230;&#30340;&#31354;&#38388;&#20998;&#24067;&#30340;&#20004;&#31181;&#26032;&#39062;&#29305;&#24449;&#29992;&#20110;&#21344;&#29992;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#37327;&#21270;&#20998;&#26512;&#65292;&#21457;&#29616;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#36890;&#39118;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#28982;&#36890;&#39118;&#25151;&#38388;&#20013;&#30340;&#21344;&#29992;&#29366;&#24577;&#26816;&#27979;&#20934;&#30830;&#29575;&#21487;&#25552;&#39640;&#26368;&#22810;14.8&#20010;&#30334;&#20998;&#28857;&#65292;&#36798;&#21040;83.2&#65285;&#65288;F1&#20998;&#25968;0.84&#65289;&#12290;&#26377;&#36890;&#39118;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;87.6&#65285;&#65288;F1&#20998;&#25968;0. &#65294;84&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06643v1 Announce Type: new  Abstract: Accurate occupancy information helps to improve building energy efficiency and occupant comfort. Occupancy detection methods based on CO2 sensors have received attention due to their low cost and low intrusiveness. In naturally ventilated buildings, the accuracy of CO2-based occupancy detection is generally low in related studies due to the complex ventilation behavior and the difficulty in measuring the actual air exchange through windows. In this study, we present two novel features for occupancy detection based on the spatial distribution of the CO2 concentration. After a quantitative analysis with Support Vector Machine (SVM) as classifier, it was found that the accuracy of occupancy state detection in naturally ventilated rooms could be improved by up to 14.8 percentage points compared to the baseline, reaching 83.2 % (F1 score 0.84) without any ventilation information. With ventilation information, the accuracy reached 87.6 % (F1 s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#29289;&#20307;&#26816;&#27979;&#30340;&#33021;&#25928;&#24615;&#65292;&#36890;&#36807;&#24494;&#35843;&#26041;&#27861;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#23545;&#27169;&#22411;&#22312;&#24037;&#19994;&#29615;&#22659;&#25968;&#25454;&#38598;&#19978;&#30340;&#33021;&#28304;&#38656;&#27714;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.06631</link><description>&lt;p&gt;
&#35780;&#20272;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#29289;&#20307;&#26816;&#27979;&#20013;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#33021;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Energy Efficiency of Few-Shot Learning for Object Detection in Industrial Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#29289;&#20307;&#26816;&#27979;&#30340;&#33021;&#25928;&#24615;&#65292;&#36890;&#36807;&#24494;&#35843;&#26041;&#27861;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#23545;&#27169;&#22411;&#22312;&#24037;&#19994;&#29615;&#22659;&#25968;&#25454;&#38598;&#19978;&#30340;&#33021;&#28304;&#38656;&#27714;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#30340;&#26102;&#20195;&#65292;&#27169;&#22411;&#24615;&#33021;&#19968;&#30452;&#26159;&#39537;&#21160;&#21019;&#26032;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#23548;&#33268;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#20195;&#24037;&#19994;&#29615;&#22659;&#20013;&#65292;&#21487;&#25345;&#32493;&#24615;&#21644;&#33021;&#25928;&#24615;&#19968;&#30452;&#26159;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#22240;&#27492;&#38656;&#35201;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24494;&#35843;&#26041;&#27861;&#26469;&#20943;&#36731;&#27169;&#22411;&#35757;&#32451;&#30340;&#36127;&#25285;&#65292;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#65292;&#23558;&#26631;&#20934;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#38543;&#21518;&#65292;&#23545;&#24320;&#21457;&#30340;&#27169;&#22411;&#22312;&#19981;&#31283;&#23450;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#29289;&#20307;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#33021;&#28304;&#38656;&#27714;&#36827;&#34892;&#20102;&#28145;&#20837;&#26696;&#20363;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#31574;&#30053;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#36741;&#21161;&#35780;&#20272;&#25968;&#25454;&#65292;&#20197;&#21450;&#24615;&#33021;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06631v1 Announce Type: cross  Abstract: In the ever-evolving era of Artificial Intelligence (AI), model performance has constituted a key metric driving innovation, leading to an exponential growth in model size and complexity. However, sustainability and energy efficiency have been critical requirements during deployment in contemporary industrial settings, necessitating the use of data-efficient approaches such as few-shot learning. In this paper, to alleviate the burden of lengthy model training and minimize energy consumption, a finetuning approach to adapt standard object detection models to downstream tasks is examined. Subsequently, a thorough case study and evaluation of the energy demands of the developed models, applied in object detection benchmark datasets from volatile industrial environments is presented. Specifically, different finetuning strategies as well as utilization of ancillary evaluation data during training are examined, and the trade-off between perf
&lt;/p&gt;</description></item><item><title>&#23545;&#31216;&#40654;&#26364;&#20960;&#20309;&#21487;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20960;&#20309;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#65292;&#24182;&#19988;&#33021;&#22815;&#23558;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#26377;&#25928;&#25512;&#24191;&#21040;&#23545;&#31216;&#40654;&#26364;&#27969;&#24418;&#19978;&#12290;</title><link>https://arxiv.org/abs/2403.06612</link><description>&lt;p&gt;
&#23558;&#23545;&#31216;&#40654;&#26364;&#20960;&#20309;&#24212;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Pulling back symmetric Riemannian geometry for data analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06612
&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#40654;&#26364;&#20960;&#20309;&#21487;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20960;&#20309;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#65292;&#24182;&#19988;&#33021;&#22815;&#23558;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#26377;&#25928;&#25512;&#24191;&#21040;&#23545;&#31216;&#40654;&#26364;&#27969;&#24418;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#24448;&#24448;&#23384;&#22312;&#20110;&#20302;&#32500;&#38750;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#12290;&#38024;&#23545;&#36825;&#31181;&#25968;&#25454;&#38598;&#30340;&#29702;&#24819;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#24212;&#32771;&#34385;&#36825;&#31181;&#38750;&#32447;&#24615;&#20960;&#20309;&#12290;&#23545;&#31216;&#40654;&#26364;&#20960;&#20309;&#35774;&#32622;&#22240;&#22810;&#31181;&#21407;&#22240;&#32780;&#36866;&#29992;&#12290;&#39318;&#20808;&#65292;&#23427;&#20855;&#26377;&#20016;&#23500;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#21487;&#20197;&#35299;&#37322;&#24191;&#27867;&#33539;&#22260;&#30340;&#38750;&#32447;&#24615;&#20960;&#20309;&#65292;&#20174;&#32463;&#20856;&#38750;&#32447;&#24615;&#23884;&#20837;&#30340;&#32463;&#39564;&#35777;&#25454;&#20013;&#24050;&#32463;&#26174;&#31034;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#20960;&#20309;&#12290;&#20854;&#27425;&#65292;&#35768;&#22810;&#26368;&#21021;&#20026;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#24320;&#21457;&#30340;&#26631;&#20934;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#23545;&#31216;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#12290;&#27010;&#24565;&#19978;&#30340;&#25361;&#25112;&#28304;&#33258;&#20110;&#32570;&#20047;&#26500;&#24314;&#25968;&#25454;&#31354;&#38388;&#26412;&#36523;&#19978;&#30340;&#23545;&#31216;&#40654;&#26364;&#32467;&#26500;&#30340;&#25351;&#23548;&#26041;&#38024;&#20197;&#21450;&#32570;&#20047;&#20462;&#25913;&#25104;&#21151;&#30340;&#31639;&#27861;&#22312;&#23545;&#31216;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#30340;&#25351;&#23548;&#26041;&#38024;&#21040;&#27492;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;&#26412;&#24037;&#20316;&#32771;&#34385;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06612v1 Announce Type: cross  Abstract: Data sets tend to live in low-dimensional non-linear subspaces. Ideal data analysis tools for such data sets should therefore account for such non-linear geometry. The symmetric Riemannian geometry setting can be suitable for a variety of reasons. First, it comes with a rich mathematical structure to account for a wide range of non-linear geometries that has been shown to be able to capture the data geometry through empirical evidence from classical non-linear embedding. Second, many standard data analysis tools initially developed for data in Euclidean space can also be generalised efficiently to data on a symmetric Riemannian manifold. A conceptual challenge comes from the lack of guidelines for constructing a symmetric Riemannian structure on the data space itself and the lack of guidelines for modifying successful algorithms on symmetric Riemannian manifolds for data analysis to this setting. This work considers these challenges in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#29983;&#25104;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20559;&#35265;&#25968;&#25454;&#19978;&#35757;&#32451;&#20844;&#24179;&#30340;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#26631;&#27880;&#34394;&#20551;&#23646;&#24615;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#30340;&#34394;&#20551;&#23646;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#26174;&#31034;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06606</link><description>&lt;p&gt;
&#20998;&#24067;&#29983;&#25104;&#22686;&#24378;&#20844;&#24179;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Distributionally Generative Augmentation for Fair Facial Attribute Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#29983;&#25104;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20559;&#35265;&#25968;&#25454;&#19978;&#35757;&#32451;&#20844;&#24179;&#30340;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#26631;&#27880;&#34394;&#20551;&#23646;&#24615;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#30340;&#34394;&#20551;&#23646;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#26174;&#31034;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#65288;FAC&#65289;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#35757;&#32451;&#30340;FAC&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#19981;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#23376;&#32676;&#20307;&#20013;&#23637;&#31034;&#20934;&#30830;&#24615;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#19981;&#20844;&#24179;&#24615;&#20027;&#35201;&#24402;&#22240;&#20110;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#65292;&#20854;&#20013;&#19968;&#20123;&#34394;&#20551;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#30007;&#24615;&#65289;&#22312;&#32479;&#35745;&#19978;&#19982;&#30446;&#26631;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#24494;&#31505;&#65289;&#30456;&#20851;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27880;&#37325;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#34394;&#20551;&#23646;&#24615;&#30340;&#26631;&#31614;&#65292;&#20294;&#36825;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24102;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20844;&#24179;&#30340;FAC&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#27880;&#37322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#30340;&#34394;&#20551;&#23646;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#26126;&#30830;&#23637;&#31034;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#34394;&#20551;&#23646;&#24615;&#65292;&#23427;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#38543;&#21518;&#65292;&#23545;&#20110;&#27599;&#20010;&#22270;&#20687;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#36753;&#34394;&#20551;&#23646;&#24615;&#65292;&#38543;&#26426;&#25277;&#26679;&#19968;&#23450;&#31243;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06606v1 Announce Type: cross  Abstract: Facial Attribute Classification (FAC) holds substantial promise in widespread applications. However, FAC models trained by traditional methodologies can be unfair by exhibiting accuracy inconsistencies across varied data subpopulations. This unfairness is largely attributed to bias in data, where some spurious attributes (e.g., Male) statistically correlate with the target attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the labels of spurious attributes, which may be unavailable in practice. This work proposes a novel, generation-based two-stage framework to train a fair FAC model on biased data without additional annotation. Initially, we identify the potential spurious attributes based on generative models. Notably, it enhances interpretability by explicitly showing the spurious attributes in image space. Following this, for each image, we first edit the spurious attributes with a random degree sampled from
&lt;/p&gt;</description></item><item><title>&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06586</link><description>&lt;p&gt;
ContextGPT: &#23558;LLMs&#30693;&#35782;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06586
&lt;/p&gt;
&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#31227;&#21160;&#35745;&#31639;&#20013;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25991;&#29486;&#20013;&#26368;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#38656;&#35201;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#23558;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#21450;&#20854;&#21487;&#33021;&#21457;&#29983;&#30340;&#32972;&#26223;&#30340;&#24120;&#35782;&#30693;&#35782;&#27880;&#20837;HAR&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#20381;&#36182;&#20110;&#36923;&#36753;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#26412;&#20307;&#35770;&#65289;&#65292;&#20854;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#32500;&#25252;&#20197;&#25429;&#25417;&#26032;&#27963;&#21160;&#21644;&#19978;&#19979;&#25991;&#38656;&#35201;&#26174;&#33879;&#30340;&#20154;&#21147;&#24037;&#31243;&#21162;&#21147;&#12289;&#25216;&#26415;&#30693;&#35782;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#30340;&#24120;&#35782;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06586v1 Announce Type: cross  Abstract: Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;FFAD&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#33258;&#21160;&#32534;&#30721;&#22120;&#35780;&#20272;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36136;&#37327;&#65292;&#22635;&#34917;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35780;&#20272;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.06576</link><description>&lt;p&gt;
FFAD&#65306;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#33258;&#21160;&#32534;&#30721;&#22120;&#35780;&#20272;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#22411;&#24230;&#37327;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing Fourier Transform and Auto-encoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;FFAD&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#33258;&#21160;&#32534;&#30721;&#22120;&#35780;&#20272;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36136;&#37327;&#65292;&#22635;&#34917;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35780;&#20272;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22914;&#20309;&#26377;&#25928;&#35780;&#20272;&#21512;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#34429;&#28982;Fr&#233;chet Inception Distance&#65288;FID&#65289;&#34987;&#29992;&#20316;&#35780;&#20272;&#22270;&#20687;&#21512;&#25104;&#29983;&#25104;&#27169;&#22411;&#30340;&#26631;&#20934;&#24230;&#37327;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#23578;&#32570;&#20047;&#21487;&#27604;&#36739;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;Fr&#233;chet Fourier-transform Auto-encoder Distance&#65288;FFAD&#65289;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#19982;&#35780;&#20272;&#26102;&#38388;&#24207;&#21015;&#36136;&#37327;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;Fr&#233;chet Distance&#30340;&#32972;&#26223;&#19979;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;FFAD&#22312;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#26679;&#26412;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06576v1 Announce Type: new  Abstract: The success of deep learning-based generative models in producing realistic images, videos, and audios has led to a crucial consideration: how to effectively assess the quality of synthetic samples. While the Fr\'{e}chet Inception Distance (FID) serves as the standard metric for evaluating generative models in image synthesis, a comparable metric for time series data is notably absent. This gap in assessment capabilities stems from the absence of a widely accepted feature vector extractor pre-trained on benchmark time series datasets. In addressing these challenges related to assessing the quality of time series, particularly in the context of Fr\'echet Distance, this work proposes a novel solution leveraging the Fourier transform and Auto-encoder, termed the Fr\'{e}chet Fourier-transform Auto-encoder Distance (FFAD). Through our experimental results, we showcase the potential of FFAD for effectively distinguishing samples from different
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25506;&#32034;&#30446;&#26631;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;$L_1$-&#35206;&#30422;&#24230;&#20316;&#20026;&#26032;&#30340;&#25506;&#32034;&#30446;&#26631;&#65292;&#25903;&#25345;&#20869;&#22312;&#22797;&#26434;&#24230;&#25511;&#21046;&#12289;&#39640;&#25928;&#35268;&#21010;&#21644;&#28789;&#27963;&#38598;&#25104;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.06571</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22312;&#32447;&#25506;&#32034;&#26041;&#27861;&#65306;&#36890;&#36807;Coverability
&lt;/p&gt;
&lt;p&gt;
Scalable Online Exploration via Coverability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25506;&#32034;&#30446;&#26631;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;$L_1$-&#35206;&#30422;&#24230;&#20316;&#20026;&#26032;&#30340;&#25506;&#32034;&#30446;&#26631;&#65292;&#25903;&#25345;&#20869;&#22312;&#22797;&#26434;&#24230;&#25511;&#21046;&#12289;&#39640;&#25928;&#35268;&#21010;&#21644;&#28789;&#27963;&#38598;&#25104;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#23588;&#20854;&#23545;&#20110;&#38656;&#35201;&#20989;&#25968;&#36924;&#36817;&#30340;&#39640;&#32500;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25506;&#32034;&#30446;&#26631;&#8212;&#8212;&#20316;&#20026;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#20219;&#20309;&#22870;&#21169;&#20989;&#25968;&#30340;&#19979;&#28216;&#26368;&#22823;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#65292;&#21363;$L_1$-&#35206;&#30422;&#24230;&#65292;&#23427;&#27867;&#21270;&#20102;&#20197;&#24448;&#30340;&#25506;&#32034;&#26041;&#26696;&#65292;&#24182;&#25903;&#25345;&#19977;&#20010;&#22522;&#26412;&#24895;&#26395;&#65306;1.&#20869;&#22312;&#22797;&#26434;&#24230;&#25511;&#21046;&#12290;$L_1$-&#35206;&#30422;&#24230;&#19982;&#32467;&#26500;&#21442;&#25968;$L_1$-Coverability&#30456;&#20851;&#32852;&#65292;&#21453;&#26144;&#20102;&#28508;&#22312;MDP&#30340;&#20869;&#22312;&#32479;&#35745;&#22256;&#38590;&#24230;&#65292;&#21253;&#21547;Block&#21644;Low-Rank MDPs&#12290;2.&#39640;&#25928;&#35268;&#21010;&#12290;&#23545;&#20110;&#24050;&#30693;&#30340;MDP&#65292;&#20248;&#21270;$L_1$-&#35206;&#30422;&#24230;&#33021;&#22815;&#26377;&#25928;&#22320;&#38477;&#20302;&#21040;&#26631;&#20934;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#20801;&#35768;&#19982;&#35832;&#22914;&#31574;&#30053;&#26799;&#24230;&#21644;Q-learning&#31561;&#29616;&#25104;&#26041;&#27861;&#28789;&#27963;&#38598;&#25104;&#12290;3.&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;$L_1$-&#35206;&#30422;&#24230;&#30340;&#20248;&#21270;&#31561;&#21516;&#20110;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25805;&#20316;&#65292;&#23588;&#20854;&#22312;&#39640;&#32500;&#39046;&#22495;&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06571v1 Announce Type: new  Abstract: Exploration is a major challenge in reinforcement learning, especially for high-dimensional domains that require function approximation. We propose exploration objectives -- policy optimization objectives that enable downstream maximization of any reward function -- as a conceptual framework to systematize the study of exploration. Within this framework, we introduce a new objective, $L_1$-Coverage, which generalizes previous exploration schemes and supports three fundamental desiderata:   1. Intrinsic complexity control. $L_1$-Coverage is associated with a structural parameter, $L_1$-Coverability, which reflects the intrinsic statistical difficulty of the underlying MDP, subsuming Block and Low-Rank MDPs.   2. Efficient planning. For a known MDP, optimizing $L_1$-Coverage efficiently reduces to standard policy optimization, allowing flexible integration with off-the-shelf methods such as policy gradient and Q-learning approaches.   3. E
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#37325;&#32534;&#31243;&#29305;&#24615;&#65292;&#30740;&#31350;&#32773;&#25104;&#21151;&#23558;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#20581;&#20840;&#20154;&#30340;&#27169;&#22411;&#25913;&#32534;&#20026;&#36866;&#29992;&#20110;&#25130;&#32930;&#32773;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.06569</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#37325;&#32534;&#31243;&#22686;&#24378;&#20551;&#32930;&#20351;&#29992;&#32773;&#30340;&#20851;&#33410;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Joint Motion Prediction for Individuals with Limb Loss Through Model Reprogramming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06569
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#37325;&#32534;&#31243;&#29305;&#24615;&#65292;&#30740;&#31350;&#32773;&#25104;&#21151;&#23558;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#20581;&#20840;&#20154;&#30340;&#27169;&#22411;&#25913;&#32534;&#20026;&#36866;&#29992;&#20110;&#25130;&#32930;&#32773;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32930;&#20307;&#20007;&#22833;&#23548;&#33268;&#30340;&#34892;&#21160;&#38556;&#30861;&#26159;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#36827;&#30340;&#36741;&#21161;&#25216;&#26415;&#65288;&#22914;&#20551;&#32930;&#35774;&#22791;&#65289;&#30340;&#24320;&#21457;&#26377;&#21487;&#33021;&#22823;&#22823;&#25552;&#39640;&#25130;&#32930;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#35774;&#35745;&#36825;&#31867;&#25216;&#26415;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#20934;&#30830;&#39044;&#27979;&#32570;&#22833;&#32930;&#20307;&#30340;&#21442;&#32771;&#20851;&#33410;&#36816;&#21160;&#12290;&#28982;&#32780;&#65292;&#19982;&#22823;&#37327;&#26469;&#33258;&#20581;&#20840;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#24418;&#25104;&#23545;&#27604;&#65292;&#25130;&#32930;&#24739;&#32773;&#20851;&#33410;&#36816;&#21160;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#38459;&#30861;&#20102;&#36825;&#19968;&#20219;&#21153;&#30340;&#23436;&#25104;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#37325;&#32534;&#31243;&#29305;&#24615;&#65292;&#26080;&#38656;&#25913;&#21464;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#37325;&#26032;&#21033;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;&#27169;&#22411;&#23454;&#29616;&#26032;&#30446;&#26631;&#12290;&#36890;&#36807;&#20165;&#22312;&#25968;&#25454;&#32423;&#21035;&#36827;&#34892;&#25805;&#20316;&#65292;&#25105;&#20204;&#23558;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#20581;&#20840;&#20154;&#30340;&#27169;&#22411;&#25913;&#32534;&#20026;&#36866;&#29992;&#20110;&#25130;&#32930;&#32773;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#25512;&#36827;&#36741;&#21161;&#25216;&#26415;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06569v1 Announce Type: new  Abstract: Mobility impairment caused by limb loss is a significant challenge faced by millions of individuals worldwide. The development of advanced assistive technologies, such as prosthetic devices, has the potential to greatly improve the quality of life for amputee patients. A critical component in the design of such technologies is the accurate prediction of reference joint motion for the missing limb. However, this task is hindered by the scarcity of joint motion data available for amputee patients, in contrast to the substantial quantity of data from able-bodied subjects. To overcome this, we leverage deep learning's reprogramming property to repurpose well-trained models for a new goal without altering the model parameters. With only data-level manipulation, we adapt models originally designed for able-bodied people to forecast joint motion in amputees. The findings in this study have significant implications for advancing assistive tech a
&lt;/p&gt;</description></item><item><title>&#30830;&#35748;&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25581;&#31034;OpenAI&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#30340;&#19981;&#23436;&#25972;&#32454;&#33410;&#65292;&#24182;&#25506;&#31350;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#21487;&#38752;&#20844;&#24335;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.06563</link><description>&lt;p&gt;
&#25581;&#24320;&#32553;&#25918;&#23450;&#24459;&#20043;&#35868;&#65306;&#31532;&#19968;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Mystery of Scaling Laws: Part I
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06563
&lt;/p&gt;
&lt;p&gt;
&#30830;&#35748;&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25581;&#31034;OpenAI&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#30340;&#19981;&#23436;&#25972;&#32454;&#33410;&#65292;&#24182;&#25506;&#31350;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#21487;&#38752;&#20844;&#24335;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#21407;&#21017;&#34920;&#26126;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#31561;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#24130;&#23450;&#24459;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#21407;&#21017;&#22312;&#20248;&#21270;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21508;&#20010;&#26041;&#38754;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26368;&#32456;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#12289;Llama&#21644;Gemini&#65289;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;OpenAI&#30340;&#21407;&#22987;&#32553;&#25918;&#23450;&#24459;&#35770;&#25991;&#24182;&#26410;&#25259;&#38706;&#25512;&#23548;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#20844;&#24335;&#25152;&#24517;&#38656;&#30340;&#23436;&#25972;&#32454;&#33410;&#65292;&#20182;&#20204;&#30340;&#32467;&#35770;&#20165;&#22522;&#20110;&#21253;&#21547;&#39640;&#36798;15&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#19968;&#20123;&#21518;&#32493;&#20316;&#21697;&#35797;&#22270;&#25581;&#31034;&#36825;&#20123;&#32454;&#33410;&#24182;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#30053;&#20102;&#37325;&#35201;&#22240;&#32032;&#30340;&#35757;&#32451;&#20381;&#36182;&#24615;&#65292;&#22914;&#23398;&#20064;&#36895;&#29575;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#25209;&#37327;&#22823;&#23567;&#65292;&#23548;&#33268;&#23427;&#20204;&#26410;&#33021;&#24314;&#31435;&#19968;&#20010;&#21487;&#38752;&#30340;&#39044;&#27979;&#27979;&#35797;&#25439;&#22833;&#36712;&#36857;&#30340;&#20844;&#24335;&#12290;&#22312;&#26412;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06563v1 Announce Type: cross  Abstract: Scaling law principles indicate a power-law correlation between loss and variables such as model size, dataset size, and computational resources utilized during training. These principles play a vital role in optimizing various aspects of model pre-training, ultimately contributing to the success of large language models such as GPT-4, Llama and Gemini. However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters. Though some subsequent works attempt to unveil these details and scale to larger models, they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for predicting the test loss trajectory. In this technical report, we confirm that the scaling 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#21345;&#22374;-&#21704;&#36798;&#29595;&#24503;&#27969;&#24418;&#19978;&#25512;&#23548;&#20102;&#20999;&#29255;-&#21326;&#22622;&#26031;&#22374;&#36317;&#31163;&#30340;&#19968;&#33324;&#26500;&#36896;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#24212;&#29992;&#65292;&#24182;&#19988;&#25512;&#23548;&#20102;&#38750;&#21442;&#25968;&#26041;&#26696;&#20197;&#26368;&#23567;&#21270;&#36825;&#20123;&#26032;&#36317;&#31163;&#12290;</title><link>https://arxiv.org/abs/2403.06560</link><description>&lt;p&gt;
&#20999;&#29255;-&#21326;&#22622;&#26031;&#22374;&#36317;&#31163;&#21450;&#22312;&#21345;&#22374;-&#21704;&#36798;&#29595;&#24503;&#27969;&#24418;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#21345;&#22374;-&#21704;&#36798;&#29595;&#24503;&#27969;&#24418;&#19978;&#25512;&#23548;&#20102;&#20999;&#29255;-&#21326;&#22622;&#26031;&#22374;&#36317;&#31163;&#30340;&#19968;&#33324;&#26500;&#36896;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#24212;&#29992;&#65292;&#24182;&#19988;&#25512;&#23548;&#20102;&#38750;&#21442;&#25968;&#26041;&#26696;&#20197;&#26368;&#23567;&#21270;&#36825;&#20123;&#26032;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34987;&#24320;&#21457;&#25110;&#31227;&#26893;&#21040;&#40654;&#26364;&#27969;&#24418;&#19978;&#65292;&#20197;&#24212;&#23545;&#20855;&#26377;&#24050;&#30693;&#38750;&#27431;&#20960;&#20309;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#23545;&#26368;&#20248;&#36755;&#36816;&#65288;Optimal Transport, OT&#65289;&#26041;&#27861;&#21364;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#65292;&#19968;&#31181;&#27969;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#26159;&#20999;&#29255;-&#21326;&#22622;&#26031;&#22374;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#32500;&#21326;&#22622;&#26031;&#22374;&#36317;&#31163;&#30340;&#23553;&#38381;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22312;&#27969;&#24418;&#19978;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#21345;&#22374;-&#21704;&#36798;&#29595;&#24503;&#27969;&#24418;&#19978;&#20999;&#29255;-&#21326;&#22622;&#26031;&#22374;&#36317;&#31163;&#30340;&#19968;&#33324;&#26500;&#36896;&#65292;&#36825;&#20123;&#27969;&#24418;&#26159;&#20855;&#26377;&#38750;&#27491;&#26354;&#29575;&#30340;&#40654;&#26364;&#27969;&#24418;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#21452;&#26354;&#31354;&#38388;&#25110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#31354;&#38388;&#31561;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#38750;&#21442;&#25968;&#26041;&#26696;&#26469;&#26368;&#23567;&#21270;&#36825;&#20123;&#26032;&#36317;&#31163;&#65292;&#36890;&#36807;&#36817;&#20284;&#23427;&#20204;&#30340;&#21326;&#22622;&#26031;&#22374;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06560v1 Announce Type: new  Abstract: While many Machine Learning methods were developed or transposed on Riemannian manifolds to tackle data with known non Euclidean geometry, Optimal Transport (OT) methods on such spaces have not received much attention. The main OT tool on these spaces is the Wasserstein distance which suffers from a heavy computational burden. On Euclidean spaces, a popular alternative is the Sliced-Wasserstein distance, which leverages a closed-form solution of the Wasserstein distance in one dimension, but which is not readily available on manifolds. In this work, we derive general constructions of Sliced-Wasserstein distances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive curvature, which include among others Hyperbolic spaces or the space of Symmetric Positive Definite matrices. Then, we propose different applications. Additionally, we derive non-parametric schemes to minimize these new distances by approximating their Wasserste
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#26550;&#26500;&#65292;&#29992;&#20110;&#32534;&#30721;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#35282;&#33394;&#30340;&#36816;&#21160;&#23398;&#20449;&#24687;&#65292;&#33021;&#22815;&#34920;&#36798;&#36816;&#21160;&#20013;&#30340;&#24773;&#32490;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.06557</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26550;&#26500;&#26469;&#32534;&#30721;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#35282;&#33394;&#30340;&#36816;&#21160;&#23398;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Data-driven architecture to encode information in the kinematics of robots and artificial avatars
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06557
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#26550;&#26500;&#65292;&#29992;&#20110;&#32534;&#30721;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#35282;&#33394;&#30340;&#36816;&#21160;&#23398;&#20449;&#24687;&#65292;&#33021;&#22815;&#34920;&#36798;&#36816;&#21160;&#20013;&#30340;&#24773;&#32490;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#26550;&#26500;&#65292;&#29992;&#20110;&#20462;&#25913;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#35282;&#33394;&#30340;&#36816;&#21160;&#23398;&#65292;&#20197;&#32534;&#30721;&#29305;&#23450;&#20449;&#24687;&#65292;&#20363;&#22914;&#30001;&#20154;&#31867;&#25805;&#20316;&#21592;&#39537;&#21160;&#30340;&#35282;&#33394;&#25110;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#20013;&#26159;&#21542;&#23384;&#22312;&#24773;&#32490;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#25343;&#21462;&#25918;&#32622;&#20219;&#21153;&#30340;&#25235;&#21462;&#38454;&#27573;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06557v1 Announce Type: cross  Abstract: We present a data-driven control architecture for modifying the kinematics of robots and artificial avatars to encode specific information such as the presence or not of an emotion in the movements of an avatar or robot driven by a human operator. We validate our approach on an experimental dataset obtained during the reach-to-grasp phase of a pick-and-place task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Optimally Matched Hierarchy&#65288;OMH&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#19978;&#26045;&#21152;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#29305;&#24449;&#20248;&#21270;&#30340;&#23618;&#27425;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2403.06546</link><description>&lt;p&gt;
OMH: &#36890;&#36807;&#26368;&#20339;&#21305;&#37197;&#23618;&#27425;&#23454;&#29616;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Optimally Matched Hierarchy&#65288;OMH&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#19978;&#26045;&#21152;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#29305;&#24449;&#20248;&#21270;&#30340;&#23618;&#27425;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;USS&#65289;&#28041;&#21450;&#22312;&#19981;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#65292;&#26088;&#22312;&#20943;&#36731;&#24191;&#27867;&#20154;&#24037;&#26631;&#35760;&#30340;&#36127;&#25285;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Optimally Matched Hierarchy&#65288;OMH&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#22312;&#29305;&#24449;&#31354;&#38388;&#19978;&#26045;&#21152;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#29305;&#24449;&#20197;&#19981;&#21516;&#31890;&#24230;&#30340;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#31181;&#31232;&#30095;&#24615;&#30340;&#32467;&#26500;&#28304;&#33258;&#25105;&#20204;&#30340;&#23618;&#27425;&#32467;&#26500;&#65288;OMH&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06546v1 Announce Type: cross  Abstract: Unsupervised Semantic Segmentation (USS) involves segmenting images without relying on predefined labels, aiming to alleviate the burden of extensive human labeling. Existing methods utilize features generated by self-supervised models and specific priors for clustering. However, their clustering objectives are not involved in the optimization of the features during training. Additionally, due to the lack of clear class definitions in USS, the resulting segments may not align well with the clustering objective. In this paper, we introduce a novel approach called Optimally Matched Hierarchy (OMH) to simultaneously address the above issues. The core of our method lies in imposing structured sparsity on the feature space, which allows the features to encode information with different levels of granularity. The structure of this sparsity stems from our hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy among parallel cl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24418;&#24577;&#29305;&#23450;&#30340;IHC&#26579;&#33394;&#20998;&#35299;&#20026;&#21333;&#29420;&#30340;&#22270;&#20687;&#36890;&#36947;&#65292;&#29983;&#25104;&#20102;in-silico&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#32454;&#32990;&#26680;&#20998;&#21106;&#27169;&#22411;&#26102;&#22312;&#36136;&#37327;&#21644;&#25968;&#37327;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06545</link><description>&lt;p&gt;
ReStainGAN:&#21033;&#29992;IHC&#21040;IF&#26579;&#33394;&#22495;&#36716;&#25442;&#36827;&#34892;in-silico&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24418;&#24577;&#29305;&#23450;&#30340;IHC&#26579;&#33394;&#20998;&#35299;&#20026;&#21333;&#29420;&#30340;&#22270;&#20687;&#36890;&#36947;&#65292;&#29983;&#25104;&#20102;in-silico&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#32454;&#32990;&#26680;&#20998;&#21106;&#27169;&#22411;&#26102;&#22312;&#36136;&#37327;&#21644;&#25968;&#37327;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06545v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#36890;&#36807;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#25193;&#23637;&#29616;&#26377;&#27880;&#37322;&#30340;&#23454;&#29992;&#24615;&#21040;&#20855;&#26377;&#19981;&#21516;&#26579;&#33394;&#27169;&#24335;&#30340;&#26032;&#39046;&#22495;&#20013;&#65292;&#21487;&#20197;&#21019;&#36896;in-silico&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#36825;&#26377;&#21487;&#33021;&#22823;&#24133;&#38477;&#20302;&#26500;&#24314;&#35757;&#32451;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#22411;&#19988;&#20687;&#32032;&#31934;&#30830;&#30340;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20813;&#30123;&#33639;&#20809;&#65288;IF&#65289;&#22270;&#20687;&#20013;&#23558;&#24418;&#24577;&#29305;&#23450;&#30340;IHC&#26579;&#33394;&#20998;&#31163;&#25104;&#21333;&#29420;&#30340;&#22270;&#20687;&#36890;&#36947;&#65292;&#29983;&#25104;in-silico&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21019;&#24314;&#30340;in-silico&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#35757;&#32451;&#32454;&#32990;&#26680;&#20998;&#21106;&#27169;&#22411;&#22312;&#36136;&#37327;&#21644;&#25968;&#37327;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06545v1 Announce Type: cross  Abstract: The creation of in-silico datasets can expand the utility of existing annotations to new domains with different staining patterns in computational pathology. As such, it has the potential to significantly lower the cost associated with building large and pixel precise datasets needed to train supervised deep learning models. We propose a novel approach for the generation of in-silico immunohistochemistry (IHC) images by disentangling morphology specific IHC stains into separate image channels in immunofluorescence (IF) images. The proposed approach qualitatively and quantitatively outperforms baseline methods as proven by training nucleus segmentation models on the created in-silico datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeLAMA&#65292;&#19968;&#31181;&#20855;&#26377;&#21160;&#24577;&#21327;&#20316;&#22270;&#30340;&#20998;&#25955;&#22810;&#26234;&#33021;&#20307;&#32456;&#36523;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#20027;&#35782;&#21035;&#21327;&#20316;&#20851;&#31995;&#21644;&#36866;&#24212;&#21160;&#24577;&#20219;&#21153;&#26469;&#22686;&#24378;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.06535</link><description>&lt;p&gt;
&#20998;&#25955;&#21644;&#32456;&#36523;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeLAMA&#65292;&#19968;&#31181;&#20855;&#26377;&#21160;&#24577;&#21327;&#20316;&#22270;&#30340;&#20998;&#25955;&#22810;&#26234;&#33021;&#20307;&#32456;&#36523;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#20027;&#35782;&#21035;&#21327;&#20316;&#20851;&#31995;&#21644;&#36866;&#24212;&#21160;&#24577;&#20219;&#21153;&#26469;&#22686;&#24378;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#21644;&#32456;&#36523;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23398;&#20064;&#26088;&#22312;&#22686;&#24378;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#19981;&#38656;&#35201;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#38543;&#26102;&#38388;&#35299;&#20915;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#21327;&#20316;&#65292;&#26234;&#33021;&#20307;&#24212;&#35813;&#65306;i) &#22312;&#20998;&#25955;&#30340;&#26041;&#24335;&#19979;&#33258;&#20027;&#35782;&#21035;&#26377;&#30410;&#30340;&#21327;&#20316;&#20851;&#31995;&#65307;ii) &#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#30340;&#20219;&#21153;&#35266;&#23519;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeLAMA&#65292;&#19968;&#31181;&#20855;&#26377;&#21160;&#24577;&#21327;&#20316;&#22270;&#30340;&#20998;&#25955;&#22810;&#26234;&#33021;&#20307;&#32456;&#36523;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#20419;&#36827;&#33258;&#20027;&#21327;&#20316;&#20851;&#31995;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65292;&#28040;&#38500;&#20102;&#23545;&#22806;&#37096;&#20808;&#39564;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#20419;&#36827;&#36866;&#24212;&#21160;&#24577;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23384;&#20648;&#21333;&#20803;&#26469;&#25429;&#33719;&#26234;&#33021;&#20307;&#31215;&#32047;&#30340;&#23398;&#20064;&#21382;&#21490;&#21644;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#38480;&#30340;&#23384;&#20648;&#28040;&#32791;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#31995;&#32479;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06535v1 Announce Type: cross  Abstract: Decentralized and lifelong-adaptive multi-agent collaborative learning aims to enhance collaboration among multiple agents without a central server, with each agent solving varied tasks over time. To achieve efficient collaboration, agents should: i) autonomously identify beneficial collaborative relationships in a decentralized manner; and ii) adapt to dynamically changing task observations. In this paper, we propose DeLAMA, a decentralized multi-agent lifelong collaborative learning algorithm with dynamic collaboration graphs. To promote autonomous collaboration relationship learning, we propose a decentralized graph structure learning algorithm, eliminating the need for external priors. To facilitate adaptation to dynamic tasks, we design a memory unit to capture the agents' accumulated learning history and knowledge, while preserving finite storage consumption. To further augment the system's expressive capabilities and computation
&lt;/p&gt;</description></item><item><title>SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06534</link><description>&lt;p&gt;
SARDet-100K: &#38754;&#21521;&#22823;&#35268;&#27169;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06534
&lt;/p&gt;
&lt;p&gt;
SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#29289;&#20307;&#26816;&#27979;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#19981;&#21487;&#26367;&#20195;&#30340;&#20840;&#22825;&#20505;&#25104;&#20687;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#20027;&#35201;&#21253;&#21547; &lt;2K &#24352;&#22270;&#20687;&#65292;&#19988;&#20165;&#21253;&#21547;&#21333;&#31867;&#21035;&#29289;&#20307;&#65289;&#21644;&#28304;&#20195;&#30721;&#19981;&#21487;&#35775;&#38382;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598; SARDet-100K &#32467;&#26524;&#26159;&#23545; 10 &#20010;&#29616;&#26377; SAR &#26816;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#20837;&#35843;&#30740;&#12289;&#25910;&#38598;&#21644;&#26631;&#20934;&#21270;&#30340;&#20135;&#29289;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SARDet-100K &#26159;&#26377;&#21490;&#20197;&#26469;&#31532;&#19968;&#20010;&#36798;&#21040; COCO &#27700;&#24179;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035; SAR &#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#20973;&#20511;&#36825;&#19968;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#24182;&#25581;&#31034;&#20102; SAR &#29289;&#20307;&#26816;&#27979;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06534v1 Announce Type: cross  Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising &lt;2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#31354;&#20013;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#20869;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#30340;&#32852;&#37030;&#29256;&#26412;&#65292;&#33021;&#22815;&#21033;&#29992;&#26080;&#32447;&#20449;&#36947;&#30340;&#21472;&#21152;&#29305;&#24615;&#23454;&#29616;&#24555;&#36895;&#21487;&#20280;&#32553;&#30340;&#21442;&#25968;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#27493;&#38271;&#22686;&#24378;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06528</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#31354;&#20013;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Federated Learning Over the Air
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06528
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#31354;&#20013;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#20869;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#30340;&#32852;&#37030;&#29256;&#26412;&#65292;&#33021;&#22815;&#21033;&#29992;&#26080;&#32447;&#20449;&#36947;&#30340;&#21472;&#21152;&#29305;&#24615;&#23454;&#29616;&#24555;&#36895;&#21487;&#20280;&#32553;&#30340;&#21442;&#25968;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#27493;&#38271;&#22686;&#24378;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#31354;&#20013;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#30340;&#32852;&#37030;&#29256;&#26412;&#65292;&#29305;&#21035;&#26159;AdaGrad&#21644;Adam&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;&#26080;&#32447;&#20449;&#36947;&#30340;&#22266;&#26377;&#21472;&#21152;&#29305;&#24615;&#65292;&#20419;&#36827;&#20102;&#24555;&#36895;&#21487;&#20280;&#32553;&#30340;&#21442;&#25968;&#32858;&#21512;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26681;&#25454;&#20840;&#23616;&#26799;&#24230;&#26356;&#26032;&#21160;&#24577;&#35843;&#25972;&#27493;&#38271;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#35757;&#32451;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21253;&#25324;&#20449;&#36947;&#34928;&#33853;&#21644;&#24178;&#25200;&#23545;&#21508;&#31181;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22522;&#20110;AdaGrad&#30340;&#31639;&#27861;&#20197;$\mathcal{O}( \ln{(T)} /{ T^{ 1 - \frac{1}{\alpha} } } )$&#30340;&#36895;&#29575;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#24577;&#28857;&#65292;&#20854;&#20013;$\alpha$&#34920;&#31034;&#30005;&#30913;&#24178;&#25200;&#30340;&#25302;&#23614;&#25351;&#25968;&#12290;&#36825;&#19968;&#32467;&#26524;&#34920;&#26126;&#65292;&#24178;&#25200;&#20998;&#24067;&#30340;&#37325;&#23614;&#29305;&#24615;&#27700;&#24179;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06528v1 Announce Type: new  Abstract: We propose a federated version of adaptive gradient methods, particularly AdaGrad and Adam, within the framework of over-the-air model training. This approach capitalizes on the inherent superposition property of wireless channels, facilitating fast and scalable parameter aggregation. Meanwhile, it enhances the robustness of the model training process by dynamically adjusting the stepsize in accordance with the global gradient update. We derive the convergence rate of the training algorithms, encompassing the effects of channel fading and interference, for a broad spectrum of nonconvex loss functions. Our analysis shows that the AdaGrad-based algorithm converges to a stationary point at the rate of $\mathcal{O}( \ln{(T)} /{ T^{ 1 - \frac{1}{\alpha} } } )$, where $\alpha$ represents the tail index of the electromagnetic interference. This result indicates that the level of heavy-tailedness in interference distribution plays a crucial role
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20248;&#21270;&#33258;&#20027;&#21345;&#36710;&#30340;&#25112;&#26415;&#20915;&#31574;&#65292;&#23558;&#39640;&#32423;&#20915;&#31574;&#19982;&#20302;&#32423;&#25511;&#21046;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#25216;&#24039;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06524</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#22870;&#21169;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#20027;&#21345;&#36710;&#25112;&#26415;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06524
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20248;&#21270;&#33258;&#20027;&#21345;&#36710;&#30340;&#25112;&#26415;&#20915;&#31574;&#65292;&#23558;&#39640;&#32423;&#20915;&#31574;&#19982;&#20302;&#32423;&#25511;&#21046;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#25216;&#24039;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#33258;&#20027;&#21345;&#36710;&#30340;&#25112;&#26415;&#20915;&#31574;&#21046;&#23450;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#39640;&#36895;&#20844;&#36335;&#22330;&#26223;&#20013;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#21644;&#21464;&#36947;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#20302;&#32423;&#25511;&#21046;&#22120;&#20043;&#38388;&#20998;&#31163;&#39640;&#32423;&#20915;&#31574;&#36807;&#31243;&#21644;&#20302;&#32423;&#25511;&#21046;&#21160;&#20316;&#26159;&#26377;&#30410;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#22312;&#21345;&#36710;&#30340;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#65288;TCOP&#65289;&#20026;&#22522;&#30784;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#20989;&#25968;&#20248;&#21270;&#24615;&#33021;&#30340;&#19981;&#21516;&#26041;&#27861;&#65307;&#36890;&#36807;&#20026;&#22870;&#21169;&#20998;&#37327;&#28155;&#21152;&#26435;&#37325;&#65292;&#36890;&#36807;&#23545;&#22870;&#21169;&#20998;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06524v1 Announce Type: cross  Abstract: We develop a deep reinforcement learning framework for tactical decision making in an autonomous truck, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in a highway scenario. Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the reinforcement learning agent and the low-level controllers based on physical models. In the following, we study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck using different approaches; by adding weights to reward components, by normalizing the reward components and by using curriculum learning techniques.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#33258;&#21160;&#29983;&#25104;Python&#31243;&#24207;&#30340;TinyPy Generator&#24037;&#20855;&#21487;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#33021;&#36731;&#26494;&#29983;&#25104;&#22823;&#35268;&#27169;Python&#20195;&#30721;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.06503</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#33258;&#21160;&#29983;&#25104;Python&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Automatic Generation of Python Programs Using Context-Free Grammars
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06503
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#33258;&#21160;&#29983;&#25104;Python&#31243;&#24207;&#30340;TinyPy Generator&#24037;&#20855;&#21487;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#33021;&#36731;&#26494;&#29983;&#25104;&#22823;&#35268;&#27169;Python&#20195;&#30721;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#26032;&#30340;&#40644;&#37329;&#65292;&#25104;&#20026;&#21019;&#24314;&#26234;&#33021;&#31995;&#32479;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20195;&#30721;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TinyPy Generator&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#29983;&#25104;&#38543;&#26426;Python&#31243;&#24207;&#30340;&#24037;&#20855;&#12290;&#29983;&#25104;&#30340;&#31243;&#24207;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#20135;&#29983;&#35268;&#21017;&#65288;&#37319;&#29992;&#24052;&#31185;&#26031;-&#35834;&#23572;&#33539;&#24335;&#65288;BNF&#65289;&#26684;&#24335;&#65289;&#36882;&#24402;&#22320;&#29983;&#25104;&#20195;&#30721;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#31243;&#24230;&#30340;&#20195;&#30721;&#65292;&#33539;&#22260;&#20174;&#20165;&#21253;&#21547;&#36171;&#20540;&#30340;&#20195;&#30721;&#21040;&#21253;&#21547;&#26465;&#20214;&#21644;&#24490;&#29615;&#30340;&#26356;&#22797;&#26434;&#20195;&#30721;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24037;&#20855;&#23454;&#29616;&#20102;&#36731;&#26494;&#30340;&#22823;&#35268;&#27169;Python&#20195;&#30721;&#29983;&#25104;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#30410;&#22788;&#12290;TinyPy Generator&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#29305;&#21035;&#26377;&#29992;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;Python&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22823;&#37327;Python&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06503v1 Announce Type: cross  Abstract: In recent years, data has emerged as the new gold, serving as a powerful tool for creating intelligent systems. However, procuring high-quality data remains challenging, especially for code. To address this, we developed TinyPy Generator, a tool that generates random Python programs using a context-free grammar. The generated programs are guaranteed to be correct by construction. Our system uses custom production rules (in the Backus-Naur Form (BNF) format) to recursively generate code. This allows us to generate code with different levels of complexity, ranging from code containing only assignments to more complex code containing conditionals and loops. Our proposed tool enables effortless large-scale Python code generation, beneficial for a wide range of applications. TinyPy Generator is particularly useful in the field of machine learning, where it can generate substantial amounts of Python code for training Python language models. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;CLOUD&#65292;&#29992;&#20110;&#20165;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#26816;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#20849;&#21516;&#21407;&#22240;&#65292;&#26080;&#38656;&#23545;&#26410;&#35266;&#27979;&#21464;&#37327;&#30340;&#26041;&#31243;&#27169;&#22411;&#24418;&#24335;&#20570;&#20219;&#20309;&#20551;&#35774;</title><link>https://arxiv.org/abs/2403.06499</link><description>&lt;p&gt;
&#22522;&#20110;NML&#32534;&#30721;&#22312;&#31163;&#25955;&#12289;&#28151;&#21512;&#21644;&#36830;&#32493;&#21464;&#37327;&#20013;&#26816;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#20849;&#21516;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Detection of Unobserved Common Causes based on NML Code in Discrete, Mixed, and Continuous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06499
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;CLOUD&#65292;&#29992;&#20110;&#20165;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#26816;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#20849;&#21516;&#21407;&#22240;&#65292;&#26080;&#38656;&#23545;&#26410;&#35266;&#27979;&#21464;&#37327;&#30340;&#26041;&#31243;&#27169;&#22411;&#24418;&#24335;&#20570;&#20219;&#20309;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#20174;&#22240;&#26524;&#21457;&#29616;&#20013;&#26816;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#20849;&#21516;&#21407;&#22240;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#25152;&#26377;&#21487;&#33021;&#30340;&#22240;&#26524;&#20851;&#31995;&#24402;&#20026;&#22235;&#20010;&#31867;&#21035;&#65292;&#24182;&#26088;&#22312;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#19968;&#20010;&#65306;&#30452;&#25509;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20004;&#31181;&#24773;&#20917;&#65292;&#21464;&#37327;&#30456;&#20114;&#29420;&#31435;&#30340;&#24773;&#20917;&#65292;&#20197;&#21450;&#21464;&#37327;&#34987;&#28508;&#22312;&#28151;&#26434;&#22240;&#32032;&#25152;&#28151;&#28102;&#30340;&#24773;&#20917;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#35201;&#27714;&#26410;&#35266;&#27979;&#21040;&#30340;&#21464;&#37327;&#28385;&#36275;&#20854;&#26041;&#31243;&#27169;&#22411;&#24418;&#24335;&#30340;&#20551;&#35774;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65288;Kobayashi&#31561;&#65292;2022&#24180;&#65289;&#65292;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36825;&#20123;&#20551;&#35774;&#30340;&#31163;&#25955;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CLOUD&#12290;&#20351;&#29992;&#24402;&#19968;&#21270;&#26368;&#22823;&#20284;&#28982;&#65288;NML&#65289;&#32534;&#30721;&#65292;CLOUD&#20174;&#19968;&#32452;&#20505;&#36873;&#27169;&#22411;&#20013;&#36873;&#25321;&#19968;&#20010;&#29983;&#25104;&#35266;&#27979;&#25968;&#25454;&#26368;&#23567;&#32534;&#30721;&#38271;&#24230;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#23558;CLOUD&#25193;&#23637;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06499v1 Announce Type: cross  Abstract: Causal discovery in the presence of unobserved common causes from observational data only is a crucial but challenging problem. We categorize all possible causal relationships between two random variables into the following four categories and aim to identify one from observed data: two cases in which either of the direct causality exists, a case that variables are independent, and a case that variables are confounded by latent confounders. Although existing methods have been proposed to tackle this problem, they require unobserved variables to satisfy assumptions on the form of their equation models. In our previous study (Kobayashi et al., 2022), the first causal discovery method without such assumptions is proposed for discrete data and named CLOUD. Using Normalized Maximum Likelihood (NML) Code, CLOUD selects a model that yields the minimum codelength of the observed data from a set of model candidates. This paper extends CLOUD to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#20004;&#20010;&#25552;&#21319;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20174;&#31038;&#20132;&#22270;&#20013;&#23398;&#20064;&#25552;&#21319;&#20272;&#35745;&#26469;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#30340;&#20010;&#20307;&#25552;&#21319;&#24314;&#27169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.06489</link><description>&lt;p&gt;
&#20855;&#26377;&#20004;&#20010;&#25552;&#21319;&#20272;&#35745;&#22120;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26631;&#31614;&#31232;&#32570;&#30340;&#20010;&#20307;&#25552;&#21319;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network with Two Uplift Estimators for Label-Scarcity Individual Uplift Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#20004;&#20010;&#25552;&#21319;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20174;&#31038;&#20132;&#22270;&#20013;&#23398;&#20064;&#25552;&#21319;&#20272;&#35745;&#26469;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#30340;&#20010;&#20307;&#25552;&#21319;&#24314;&#27169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#24314;&#27169;&#26088;&#22312;&#27979;&#37327;&#20174;&#38543;&#26426;&#23454;&#39564;&#25110;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#31574;&#30053;&#25110;&#34892;&#21160;&#23545;&#29992;&#25143;&#30340;&#22686;&#37327;&#25928;&#24212;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25552;&#21319;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25552;&#21319;&#26041;&#27861;&#20165;&#20351;&#29992;&#20010;&#20307;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#19981;&#36275;&#20197;&#25429;&#25417;&#26377;&#20851;&#25552;&#21319;&#30340;&#26410;&#35266;&#23519;&#21040;&#21644;&#22797;&#26434;&#30340;&#38544;&#34255;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25552;&#21319;&#24314;&#27169;&#22330;&#26223;&#36890;&#24120;&#20855;&#26377;&#31232;&#32570;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22788;&#29702;&#32452;&#65292;&#36825;&#20063;&#23545;&#27169;&#22411;&#35757;&#32451;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#32771;&#34385;&#21040;&#37051;&#23621;&#30340;&#29305;&#24449;&#21644;&#31038;&#20250;&#20851;&#31995;&#23545;&#20110;&#34920;&#24449;&#29992;&#25143;&#30340;&#25552;&#21319;&#38750;&#24120;&#26377;&#20449;&#24687;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#25552;&#21319;&#20272;&#35745;&#22120;&#65292;&#31216;&#20026;GNUM&#65292;&#20197;&#20174;&#31038;&#20132;&#22270;&#20013;&#23398;&#20064;&#25552;&#21319;&#20272;&#35745;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#31867;&#21464;&#25442;&#30446;&#26631;&#30340;&#31532;&#19968;&#20010;&#20272;&#35745;&#22120;&#12290;&#35813;&#20272;&#35745;&#22120;&#36866;&#29992;&#20110;&#25152;&#26377;&#31867;&#22411;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#20840;&#38754;&#22320;&#24314;&#27169;&#22788;&#29702;&#32452;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06489v1 Announce Type: new  Abstract: Uplift modeling aims to measure the incremental effect, which we call uplift, of a strategy or action on the users from randomized experiments or observational data. Most existing uplift methods only use individual data, which are usually not informative enough to capture the unobserved and complex hidden factors regarding the uplift. Furthermore, uplift modeling scenario usually has scarce labeled data, especially for the treatment group, which also poses a great challenge for model training. Considering that the neighbors' features and the social relationships are very informative to characterize a user's uplift, we propose a graph neural network-based framework with two uplift estimators, called GNUM, to learn from the social graph for uplift estimation. Specifically, we design the first estimator based on a class-transformed target. The estimator is general for all types of outcomes, and is able to comprehensively model the treatment
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#25366;&#25496;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;COLA&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35686;&#25253;&#32858;&#21512;&#65292;&#33021;&#22815;&#32508;&#21512;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#35299;&#20915;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#35686;&#25253;&#32858;&#21512;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06485</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#30693;&#35782;&#24863;&#30693;&#35686;&#25253;&#32858;&#21512;&#65306;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06485
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#25366;&#25496;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;COLA&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35686;&#25253;&#32858;&#21512;&#65292;&#33021;&#22815;&#32508;&#21512;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#35299;&#20915;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#35686;&#25253;&#32858;&#21512;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20113;&#31995;&#32479;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#65292;&#31995;&#32479;&#25925;&#38556;&#20250;&#24341;&#21457;&#8220;&#35686;&#25253;&#39118;&#26292;&#8221;&#65292;&#21363;&#22823;&#37327;&#30456;&#20851;&#30340;&#35686;&#25253;&#12290;&#23613;&#31649;&#36825;&#20123;&#35686;&#25253;&#21487;&#20197;&#36861;&#28335;&#21040;&#23569;&#25968;&#20960;&#20010;&#26681;&#26412;&#21407;&#22240;&#65292;&#20294;&#21387;&#20498;&#24615;&#30340;&#25968;&#37327;&#20351;&#20154;&#24037;&#22788;&#29702;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#35686;&#25253;&#32858;&#21512;&#23545;&#24110;&#21161;&#24037;&#31243;&#24072;&#38598;&#20013;&#31934;&#21147;&#35299;&#20915;&#26681;&#26412;&#21407;&#22240;&#24182;&#20419;&#36827;&#25925;&#38556;&#35299;&#20915;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#25110;&#32479;&#35745;&#26041;&#27861;&#26469;&#32858;&#21512;&#35686;&#25253;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#35686;&#25253;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#32780;&#32479;&#35745;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#22788;&#29702;&#19981;&#32463;&#24120;&#21457;&#29983;&#30340;&#35686;&#25253;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#21363;&#35686;&#25253;&#30340;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#65288;SOP&#65289;&#20316;&#20026;&#34917;&#20805;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#25366;&#25496;&#21644;LLM&#65288;Large Language Model&#65289;&#25512;&#29702;&#30340;&#22312;&#32447;&#35686;&#25253;&#32858;&#21512;&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;COLA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06485v1 Announce Type: cross  Abstract: Due to the scale and complexity of cloud systems, a system failure would trigger an "alert storm", i.e., massive correlated alerts. Although these alerts can be traced back to a few root causes, the overwhelming number makes it infeasible for manual handling. Alert aggregation is thus critical to help engineers concentrate on the root cause and facilitate failure resolution. Existing methods typically utilize semantic similarity-based methods or statistical methods to aggregate alerts. However, semantic similarity-based methods overlook the causal rationale of alerts, while statistical methods can hardly handle infrequent alerts.   To tackle these limitations, we introduce leveraging external knowledge, i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose COLA, a novel hybrid approach based on correlation mining and LLM (Large Language Model) reasoning for online alert aggregation. The correlation mining modul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; motif-preserving Graph Neural Network with curriculum learning (MotifGNN)&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21407;&#22987;&#22270;&#30340;&#20302;&#38454;&#32467;&#26500;&#21644;&#22522;&#20110;&#22810;&#35270;&#22270;&#22270;&#26696;&#30340;&#39640;&#38454;&#32467;&#26500;&#65292;&#29992;&#20110;&#37329;&#34701;&#36829;&#32422;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.06482</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#30041;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#37329;&#34701;&#36829;&#32422;&#39044;&#27979;&#30340;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; motif-preserving Graph Neural Network with curriculum learning (MotifGNN)&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21407;&#22987;&#22270;&#30340;&#20302;&#38454;&#32467;&#26500;&#21644;&#22522;&#20110;&#22810;&#35270;&#22270;&#22270;&#26696;&#30340;&#39640;&#38454;&#32467;&#26500;&#65292;&#29992;&#20110;&#37329;&#34701;&#36829;&#32422;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#30340;&#37329;&#34701;&#36829;&#32422;&#39044;&#27979;&#22312;&#20449;&#29992;&#39118;&#38505;&#39044;&#27979;&#21644;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26088;&#22312;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#26410;&#33021;&#20607;&#36824;&#36151;&#27454;&#30340;&#27010;&#29575;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#25552;&#21462;&#29992;&#25143;&#20010;&#20154;&#29305;&#24449;&#65292;&#26500;&#24314;&#20108;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#36829;&#32422;&#39044;&#27979;&#65292;&#20294;&#23545;&#20110;&#20449;&#24687;&#26377;&#38480;&#30340;&#29992;&#25143;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#24471;&#21040;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#20445;&#30041;&#23376;&#22270;&#27169;&#24335;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#35838;&#31243;&#23398;&#20064;&#65288;MotifGNN&#65289;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#32570;&#21475;&#65292;&#20849;&#21516;&#23398;&#20064;&#21407;&#22987;&#22270;&#30340;&#20302;&#38454;&#32467;&#26500;&#21644;&#22522;&#20110;&#22810;&#35270;&#22270;&#22270;&#26696;&#30340;&#23376;&#22270;&#30340;&#39640;&#38454;&#32467;&#26500;&#65292;&#29992;&#20110;&#37329;&#34701;&#36829;&#32422;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06482v1 Announce Type: cross  Abstract: User financial default prediction plays a critical role in credit risk forecasting and management. It aims at predicting the probability that the user will fail to make the repayments in the future. Previous methods mainly extract a set of user individual features regarding his own profiles and behaviors and build a binary-classification model to make default predictions. However, these methods cannot get satisfied results, especially for users with limited information. Although recent efforts suggest that default prediction can be improved by social relations, they fail to capture the higher-order topology structure at the level of small subgraph patterns. In this paper, we fill in this gap by proposing a motif-preserving Graph Neural Network with curriculum learning (MotifGNN) to jointly learn the lower-order structures from the original graph and higherorder structures from multi-view motif-based graphs for financial default predict
&lt;/p&gt;</description></item><item><title>RL-MSA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#27861;&#65292;&#23558;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#38382;&#39064;&#24314;&#27169;&#20026;MDP&#65292;&#39318;&#27425;&#22312;&#31163;&#32447;&#38454;&#27573;&#23558;&#30452;&#34892;&#36710;&#20915;&#31574;&#25972;&#21512;&#20837;&#20844;&#20132;&#36710;&#36873;&#25321;&#20915;&#31574;&#65292;&#26377;&#25928;&#31616;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#32447;&#38454;&#27573;&#36890;&#36807;&#26102;&#38388;&#31383;&#21475;&#26426;&#21046;&#36827;&#34892;&#30452;&#34892;&#36710;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.06466</link><description>&lt;p&gt;
RL-MSA&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06466
&lt;/p&gt;
&lt;p&gt;
RL-MSA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#27861;&#65292;&#23558;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#38382;&#39064;&#24314;&#27169;&#20026;MDP&#65292;&#39318;&#27425;&#22312;&#31163;&#32447;&#38454;&#27573;&#23558;&#30452;&#34892;&#36710;&#20915;&#31574;&#25972;&#21512;&#20837;&#20844;&#20132;&#36710;&#36873;&#25321;&#20915;&#31574;&#65292;&#26377;&#25928;&#31616;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#32447;&#38454;&#27573;&#36890;&#36807;&#26102;&#38388;&#31383;&#21475;&#26426;&#21046;&#36827;&#34892;&#30452;&#34892;&#36710;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#38382;&#39064;&#65288;MLBSP&#65289;&#23545;&#20110;&#33410;&#30465;&#20844;&#20132;&#20844;&#21496;&#36816;&#33829;&#25104;&#26412;&#21644;&#20445;&#35777;&#20056;&#23458;&#26381;&#21153;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20197;&#31163;&#32447;&#26041;&#24335;&#29983;&#25104;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#26696;&#65292;&#28982;&#21518;&#26681;&#25454;&#35813;&#26041;&#26696;&#23433;&#25490;&#20844;&#20132;&#36710;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#35832;&#22914;&#20132;&#36890;&#25317;&#22581;&#20043;&#31867;&#30340;&#19981;&#30830;&#23450;&#20107;&#20214;&#32463;&#24120;&#21457;&#29983;&#65292;&#36825;&#21487;&#33021;&#20351;&#20107;&#20808;&#30830;&#23450;&#30340;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#26696;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#26412;&#25991;&#23558;MLBSP&#24314;&#27169;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#32447;&#36335;&#20844;&#20132;&#36710;&#35843;&#24230;&#26041;&#27861;&#65288;RL-MSA&#65289;&#65292;&#29992;&#20110;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#38454;&#27573;&#36827;&#34892;&#20844;&#20132;&#36710;&#35843;&#24230;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;&#23558;&#30452;&#34892;&#36710;&#20915;&#31574;&#25972;&#21512;&#21040;&#39318;&#27425;&#20986;&#29616;&#30340;&#20844;&#20132;&#36710;&#36873;&#25321;&#20915;&#31574;&#20013;&#65292;&#20197;&#31616;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#22312;&#32447;&#38454;&#27573;&#65292;&#36890;&#36807;&#22522;&#20110;&#31163;&#32447;&#38454;&#27573;&#23398;&#20250;&#30340;&#31574;&#30053;&#36827;&#34892;&#30452;&#34892;&#36710;&#20915;&#31574;&#65292;&#37319;&#29992;&#26102;&#38388;&#31383;&#21475;&#26426;&#21046;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#20010;&#26032;&#30340;&#26377;&#29992;&#29366;&#24577;&#29305;&#24449;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06466v1 Announce Type: cross  Abstract: Multiple Line Bus Scheduling Problem (MLBSP) is vital to save operational cost of bus company and guarantee service quality for passengers. Existing approaches typically generate a bus scheduling scheme in an offline manner and then schedule buses according to the scheme. In practice, uncertain events such as traffic congestion occur frequently, which may make the pre-determined bus scheduling scheme infeasible. In this paper, MLBSP is modeled as a Markov Decision Process (MDP). A Reinforcement Learning-based Multi-line bus Scheduling Approach (RL-MSA) is proposed for bus scheduling at both the offline and online phases. At the offline phase, deadhead decision is integrated into bus selection decision for the first time to simplify the learning problem. At the online phase, deadhead decision is made through a time window mechanism based on the policy learned at the offline phase. We develop several new and useful state features includi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;LSTM&#32593;&#32476;&#39044;&#27979;&#40614;&#27713;&#23494;&#24230;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24265;&#20215;&#20256;&#24863;&#22120;&#27979;&#37327;&#20540;&#35745;&#31639;&#23494;&#24230;&#65292;&#24110;&#21161;&#20943;&#23569;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.06458</link><description>&lt;p&gt;
&#20351;&#29992;LSTM&#32593;&#32476;&#39044;&#27979;&#40614;&#27713;&#23494;&#24230;
&lt;/p&gt;
&lt;p&gt;
Prediction of Wort Density with LSTM Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;LSTM&#32593;&#32476;&#39044;&#27979;&#40614;&#27713;&#23494;&#24230;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24265;&#20215;&#20256;&#24863;&#22120;&#27979;&#37327;&#20540;&#35745;&#31639;&#23494;&#24230;&#65292;&#24110;&#21161;&#20943;&#23569;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#36807;&#31243;&#20013;&#35768;&#22810;&#29289;&#29702;&#30446;&#26631;&#20540;&#24456;&#23481;&#26131;&#20986;&#38169;&#12289;&#32321;&#29712;&#25110;&#26114;&#36149;&#65292;&#26080;&#27861;&#33258;&#21160;&#27979;&#37327;&#12290;&#40614;&#27713;&#23494;&#24230;&#26159;&#19968;&#20010;&#29289;&#29702;&#30446;&#26631;&#20540;&#30340;&#20363;&#23376;&#65292;&#23545;&#20110;&#21860;&#37202;&#29983;&#20135;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#20256;&#24863;&#22120;&#24110;&#21161;&#37247;&#37202;&#24072;&#27979;&#37327;&#40614;&#27713;&#23494;&#24230;&#65292;&#20197;&#20943;&#23569;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38169;&#35823;&#12290;&#26041;&#27861;&#26159;&#20351;&#29992;&#21387;&#21147;&#25110;&#28201;&#24230;&#31561;&#24265;&#20215;&#26631;&#20934;&#20256;&#24863;&#22120;&#27979;&#37327;&#21040;&#30340;&#20540;&#26469;&#35745;&#31639;&#23494;&#24230;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#27979;&#37327;&#40614;&#27713;&#23494;&#24230;&#12290;&#35745;&#31639;&#32972;&#21518;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#21517;&#20026;LSTM&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06458v1 Announce Type: new  Abstract: Many physical target values in technical processes are error-prone, cumbersome, or expensive to measure automatically. One example of a physical target value is the wort density, which is an important value needed for beer production. This article introduces a system that helps the brewer measure wort density through sensors in order to reduce errors in manual data collection. Instead of a direct measurement of wort density, a method is developed that calculates the density from measured values acquired by inexpensive standard sensors such as pressure or temperature. The model behind the calculation is a neural network, known as LSTM.
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#32034;&#24341;&#26159;&#23558;&#25968;&#25454;&#24211;&#32034;&#24341;&#32467;&#26500;&#35270;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26368;&#36817;&#20986;&#29616;&#30340;&#36235;&#21183;&#12290;&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#23398;&#20064;&#22810;&#32500;&#32034;&#24341;&#32467;&#26500;&#30340;&#29616;&#29366;&#65292;&#20998;&#31867;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06456</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#32500;&#31354;&#38388;&#30340;&#23398;&#20064;&#32034;&#24341;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Learned Indexes for the Multi-dimensional Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06456
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32034;&#24341;&#26159;&#23558;&#25968;&#25454;&#24211;&#32034;&#24341;&#32467;&#26500;&#35270;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26368;&#36817;&#20986;&#29616;&#30340;&#36235;&#21183;&#12290;&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#23398;&#20064;&#22810;&#32500;&#32034;&#24341;&#32467;&#26500;&#30340;&#29616;&#29366;&#65292;&#20998;&#31867;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#26368;&#26032;&#30340;&#30740;&#31350;&#36235;&#21183;&#28041;&#21450;&#23558;&#25968;&#25454;&#24211;&#32034;&#24341;&#32467;&#26500;&#35270;&#20026;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#65292;&#21333;&#20010;&#25110;&#22810;&#20010;ML&#27169;&#22411;&#34987;&#35757;&#32451;&#26469;&#23398;&#20064;&#20174;&#38190;&#21040;&#25968;&#25454;&#38598;&#20869;&#37096;&#20301;&#32622;&#30340;&#26144;&#23556;&#12290;&#36825;&#31867;&#32034;&#24341;&#34987;&#31216;&#20026;&#8220;&#23398;&#20064;&#32034;&#24341;&#8221;&#12290;&#23398;&#20064;&#32034;&#24341;&#23637;&#31034;&#20102;&#23545;&#19968;&#32500;&#25968;&#25454;&#30340;&#25913;&#36827;&#25628;&#32034;&#24615;&#33021;&#21644;&#20943;&#23569;&#31354;&#38388;&#38656;&#27714;&#12290;&#19968;&#32500;&#23398;&#20064;&#32034;&#24341;&#30340;&#27010;&#24565;&#33258;&#28982;&#34987;&#25193;&#23637;&#21040;&#22810;&#32500;&#65288;&#20363;&#22914;&#65292;&#31354;&#38388;&#65289;&#25968;&#25454;&#65292;&#25512;&#21160;&#20102;&#8220;&#23398;&#20064;&#22810;&#32500;&#32034;&#24341;&#8221;&#30340;&#21457;&#23637;&#12290;&#36825;&#39033;&#35843;&#26597;&#20391;&#37325;&#20110;&#23398;&#20064;&#22810;&#32500;&#32034;&#24341;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#23457;&#26597;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#35299;&#37322;&#20102;&#27599;&#31181;&#25552;&#20986;&#26041;&#27861;&#32972;&#21518;&#30340;&#26680;&#24515;&#27010;&#24565;&#65292;&#24182;&#26681;&#25454;&#20960;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#26631;&#20934;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#27599;&#20010;&#23398;&#20064;&#22810;&#32500;&#32034;&#24341;&#36827;&#34892;&#20998;&#31867;&#21644;&#24402;&#31867;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06456v1 Announce Type: cross  Abstract: A recent research trend involves treating database index structures as Machine Learning (ML) models. In this domain, single or multiple ML models are trained to learn the mapping from keys to positions inside a data set. This class of indexes is known as "Learned Indexes." Learned indexes have demonstrated improved search performance and reduced space requirements for one-dimensional data. The concept of one-dimensional learned indexes has naturally been extended to multi-dimensional (e.g., spatial) data, leading to the development of "Learned Multi-dimensional Indexes". This survey focuses on learned multi-dimensional index structures. Specifically, it reviews the current state of this research area, explains the core concepts behind each proposed method, and classifies these methods based on several well-defined criteria. We present a taxonomy that classifies and categorizes each learned multi-dimensional index, and survey the existi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013; JEPA &#26550;&#26500;&#21551;&#21457;&#30340; Spatio-Temporal Joint Embedding Masked Autoencoder&#65288;ST-JEMA&#65289;&#29992;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.06432</link><description>&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32852;&#21512;&#23884;&#20837;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06432
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013; JEPA &#26550;&#26500;&#21551;&#21457;&#30340; Spatio-Temporal Joint Embedding Masked Autoencoder&#65288;ST-JEMA&#65289;&#29992;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06432v1 &#36890;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23398;&#20064;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#21487;&#20197;&#21306;&#20998;&#20154;&#33041;&#32593;&#32476;&#20013;&#30340;&#34920;&#29616;&#22411;&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#29992;&#20110;&#35757;&#32451;&#30340;&#22823;&#37327;&#26631;&#35760;&#20020;&#24202;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#36164;&#28304;&#23494;&#38598;&#24615;&#65292;&#36825;&#20351;&#24471;&#23454;&#38469;&#24212;&#29992;&#21464;&#24471;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#22312;&#26631;&#31614;&#31232;&#32570;&#35774;&#32622;&#20013;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23545;&#20110;&#34920;&#31034;&#23398;&#20064;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22312;&#21160;&#24577;&#22270;&#24418;&#19978;&#30340;&#24212;&#29992;&#20197;&#21450;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#26041;&#38754;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#38754;&#20020;&#30528;&#25429;&#25417;&#39640;&#32423;&#35821;&#20041;&#34920;&#31034;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26102;&#31354;&#32852;&#21512;&#23884;&#20837;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;ST-JEMA&#65289;&#65292;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#30340;&#21551;&#21457;&#12290;ST-JEMA&#37319;&#29992;&#20102;&#19968;&#31181;&#21463;JEPA&#21551;&#21457;&#30340;&#31574;&#30053;&#26469;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06432v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks. However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult. Leveraging unlabeled data thus becomes crucial for representation learning in a label-scarce setting. Although generative self-supervised learning techniques, especially masked autoencoders, have shown promising results in representation learning in various domains, their application to dynamic graphs for dynamic functional connectivity remains underexplored, facing challenges in capturing high-level semantic representations. Here, we introduce the Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA employs a JEPA-inspired strategy for reconstructin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#35270;&#35282;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27969;&#24418;&#19978;&#30340;&#24179;&#28369;&#26354;&#32447;&#27169;&#25311;&#22270;&#28436;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06425</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#35270;&#35282;&#30340;&#22270;&#28436;&#21270;&#19978;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Differential Geometric View and Explainability of GNN on Evolving Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06425
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#35270;&#35282;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27969;&#24418;&#19978;&#30340;&#24179;&#28369;&#26354;&#32447;&#27169;&#25311;&#22270;&#28436;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#21270;&#23398;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#29992;&#20110;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#22270;&#21487;&#33021;&#26159;&#28436;&#21270;&#30340;&#65292;&#24418;&#24335;&#21270;&#24314;&#27169;&#24182;&#29702;&#35299;&#35757;&#32451;&#21518;&#30340;GNN&#22914;&#20309;&#21709;&#24212;&#22270;&#28436;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20844;&#29702;&#24402;&#22240;&#23545;GNN&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#24179;&#28369;&#21442;&#25968;&#21270;&#65292;&#20854;&#20013;&#20998;&#24067;&#20301;&#20110;&#39640;&#32500;&#23884;&#20837;&#31354;&#38388;&#20869;&#30340;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#24494;&#20998;&#20960;&#20309;&#35270;&#35282;&#23558;&#20998;&#24067;&#28436;&#21270;&#24314;&#27169;&#20026;&#27969;&#24418;&#19978;&#30340;&#24179;&#28369;&#26354;&#32447;&#12290;&#25105;&#20204;&#37325;&#26032;&#21442;&#25968;&#21270;&#27969;&#24418;&#19978;&#30340;&#26354;&#32447;&#26063;&#65292;&#24182;&#35774;&#35745;&#19968;&#20010;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#25214;&#21040;&#31616;&#27905;&#22320;&#36817;&#20284;&#20998;&#24067;&#28436;&#21270;&#20197;&#20379;&#20154;&#31867;&#35299;&#37322;&#30340;&#21807;&#19968;&#26354;&#32447;&#12290;&#22312;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#36825;&#20123;&#20219;&#21153;&#28041;&#21450;&#21040;&#28436;&#21270;&#30340;&#22270;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#12289;&#24544;&#23454;&#24230;&#21644;&#30452;&#35266;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06425v1 Announce Type: cross  Abstract: Graphs are ubiquitous in social networks and biochemistry, where Graph Neural Networks (GNN) are the state-of-the-art models for prediction. Graphs can be evolving and it is vital to formally model and understand how a trained GNN responds to graph evolution. We propose a smooth parameterization of the GNN predicted distributions using axiomatic attribution, where the distributions are on a low-dimensional manifold within a high-dimensional embedding space. We exploit the differential geometric viewpoint to model distributional evolution as smooth curves on the manifold. We reparameterize families of curves on the manifold and design a convex optimization problem to find a unique curve that concisely approximates the distributional evolution for human interpretation. Extensive experiments on node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better sparsity, faithfulness, and intui
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#26681;&#25454;&#29305;&#24449;&#19982;&#26631;&#31614;&#30340;&#30456;&#20851;&#24615;&#26041;&#24046;&#26469;&#21306;&#20998;&#29305;&#24449;&#30340;&#25928;&#29992;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#28304;&#20219;&#21153;&#23398;&#20064;&#21040;&#22823;&#33268;&#20849;&#20139;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20248;&#21270;&#24635;&#20307;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.06424</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#33268;&#20849;&#20139;&#29305;&#24449;&#26469;&#23454;&#29616;&#39046;&#22495;&#20043;&#38388;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Bridging Domains with Approximately Shared Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#26681;&#25454;&#29305;&#24449;&#19982;&#26631;&#31614;&#30340;&#30456;&#20851;&#24615;&#26041;&#24046;&#26469;&#21306;&#20998;&#29305;&#24449;&#30340;&#25928;&#29992;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#28304;&#20219;&#21153;&#23398;&#20064;&#21040;&#22823;&#33268;&#20849;&#20139;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20248;&#21270;&#24635;&#20307;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#26088;&#22312;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#26410;&#30693;&#39046;&#22495;&#26102;&#38477;&#20302;&#24615;&#33021;&#19979;&#38477;&#12290;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#35774;&#35745;&#29305;&#24449;&#36873;&#25321;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#29616;&#26377;&#25991;&#29486;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#23384;&#22312;&#24726;&#35770;&#65306;&#26377;&#20123;&#20154;&#20027;&#24352;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#65292;&#32780;&#21478;&#19968;&#20123;&#20154;&#21017;&#26356;&#38738;&#30544;&#22810;&#26679;&#21270;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#26681;&#25454;&#23427;&#20204;&#19982;&#26631;&#31614; $y$ &#30340;&#30456;&#20851;&#24615;&#30340;&#26041;&#24046;&#26469;&#21306;&#20998;&#29305;&#24449;&#30340;&#25928;&#29992;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#19968;&#20010;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#30001;&#20174;&#28304;&#20219;&#21153;&#23398;&#20064;&#21040;&#30340;&#22823;&#33268;&#20849;&#20139;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#38656;&#35201;&#23398;&#20064;&#22823;&#33268;&#20849;&#20139;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20005;&#26684;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20197;&#21069;&#20851;&#20110;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#30340;&#32467;&#26524;&#32780;&#35328;&#65292;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#24635;&#20307;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06424v1 Announce Type: cross  Abstract: Multi-source domain adaptation aims to reduce performance degradation when applying machine learning models to unseen domains. A fundamental challenge is devising the optimal strategy for feature selection. Existing literature is somewhat paradoxical: some advocate for learning invariant features from source domains, while others favor more diverse features. To address the challenge, we propose a statistical framework that distinguishes the utilities of features based on the variance of their correlation to label $y$ across domains. Under our framework, we design and analyze a learning procedure consisting of learning approximately shared feature representation from source tasks and fine-tuning it on the target task. Our theoretical analysis necessitates the importance of learning approximately shared features instead of only the strictly invariant features and yields an improved population risk compared to previous results on both sou
&lt;/p&gt;</description></item><item><title>RLingua&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06420</link><description>&lt;p&gt;
RLingua&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06420
&lt;/p&gt;
&lt;p&gt;
RLingua&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#20197;&#20854;&#20302;&#26679;&#26412;&#25928;&#29575;&#32780;&#22768;&#21517;&#29436;&#34249;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RLingua&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#37096;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;RL&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25552;&#21462;LLMs&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20174;&#32780;&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#30340;&#21021;&#27493;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12290;&#23613;&#31649;&#19981;&#23436;&#32654;&#65292;LLM&#29983;&#25104;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#34987;&#29992;&#20110;&#22312;rollout&#26102;&#20197;&#34928;&#20943;&#27010;&#29575;&#29983;&#25104;&#21160;&#20316;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;RL&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#24182;&#20462;&#25913;&#20102;&#28436;&#21592;&#25439;&#22833;&#65292;&#20197;&#20351;&#31574;&#30053;&#23398;&#20064;&#26397;&#30528;LLM&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#35268;&#33539;&#21270;&#12290;RLingua&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#25913;&#21892;&#19981;&#23436;&#32654;&#30340;LLM&#29983;&#25104;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RLing
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06420v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations. To this end, we first present how to extract the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated. Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency. We employ the actor-critic framework and modify the actor loss to regularize the policy learning towards the LLM-generated controller. RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL. We demonstrated that RLing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#36827;&#34892;&#22240;&#26524;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FedCMFS&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#31639;&#27861;&#36890;&#36807;&#19977;&#20010;&#26032;&#39062;&#23376;&#31243;&#24207;&#65292;&#22312;&#19981;&#38598;&#20013;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27599;&#20010;&#31867;&#26631;&#31614;&#30340;&#30456;&#20851;&#29305;&#24449;&#65288;&#20505;&#36873;&#29238;&#33410;&#28857;&#21644;&#23376;&#33410;&#28857;&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.06419</link><description>&lt;p&gt;
&#32852;&#21512;&#29615;&#22659;&#20013;&#22240;&#26524;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Causal Multi-Label Feature Selection in Federated Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06419
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#36827;&#34892;&#22240;&#26524;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FedCMFS&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#31639;&#27861;&#36890;&#36807;&#19977;&#20010;&#26032;&#39062;&#23376;&#31243;&#24207;&#65292;&#22312;&#19981;&#38598;&#20013;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27599;&#20010;&#31867;&#26631;&#31614;&#30340;&#30456;&#20851;&#29305;&#24449;&#65288;&#20505;&#36873;&#29238;&#33410;&#28857;&#21644;&#23376;&#33410;&#28857;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#20316;&#20026;&#22788;&#29702;&#39640;&#32500;&#22810;&#26631;&#31614;&#25968;&#25454;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#20026;&#20102;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#29616;&#26377;&#30340;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23558;&#26469;&#33258;&#22810;&#20010;&#28304;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#65292;&#20174;&#25152;&#26377;&#26469;&#28304;&#38598;&#20013;&#25968;&#25454;&#24182;&#23558;&#20854;&#21512;&#24182;&#20026;&#21333;&#20010;&#25968;&#25454;&#38598;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#21512;&#29615;&#22659;&#20013;&#22240;&#26524;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19977;&#20010;&#26032;&#39062;&#23376;&#31243;&#24207;&#30340;&#32852;&#21512;&#22240;&#26524;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#65288;FedCMFS&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06419v1 Announce Type: new  Abstract: Multi-label feature selection serves as an effective mean for dealing with high-dimensional multi-label data. To achieve satisfactory performance, existing methods for multi-label feature selection often require the centralization of substantial data from multiple sources. However, in Federated setting, centralizing data from all sources and merging them into a single dataset is not feasible. To tackle this issue, in this paper, we study a challenging problem of causal multi-label feature selection in federated setting and propose a Federated Causal Multi-label Feature Selection (FedCMFS) algorithm with three novel subroutines. Specifically, FedCMFS first uses the FedCFL subroutine that considers the correlations among label-label, label-feature, and feature-feature to learn the relevant features (candidate parents and children) of each class label while preserving data privacy without centralizing data. Second, FedCMFS employs the FedCF
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#38590;&#20027;&#35201;&#34920;&#29616;&#22312;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#36890;&#36807;&#23545;&#19981;&#21516;&#20154;&#20026;&#25200;&#21160;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#20102;&#25200;&#21160;&#29305;&#24615;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#37327;&#21270;&#31283;&#20581;&#24615;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.06408</link><description>&lt;p&gt;
&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#38590;&#22312;&#21738;&#37324;&#65311;&#22522;&#20110;&#25200;&#21160;&#35270;&#35282;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06408
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22256;&#38590;&#20027;&#35201;&#34920;&#29616;&#22312;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#36890;&#36807;&#23545;&#19981;&#21516;&#20154;&#20026;&#25200;&#21160;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#20102;&#25200;&#21160;&#29305;&#24615;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#37327;&#21270;&#31283;&#20581;&#24615;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#65292;&#20294;&#20851;&#20110;&#37327;&#21270;&#19982;LLM&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#26377;&#24456;&#22810;&#24453;&#25506;&#32034;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#31181;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#26032;&#35270;&#35282;&#65292;&#23558;&#20854;&#35270;&#20026;&#28155;&#21152;&#21040;LLMs&#26435;&#37325;&#21644;&#28608;&#27963;&#19978;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;&#8220;&#25200;&#21160;&#35270;&#35282;&#8221;&#12290;&#21033;&#29992;&#36825;&#19968;&#35270;&#35282;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#20154;&#20026;&#25200;&#21160;&#30340;&#23454;&#39564;&#65292;&#25506;&#35752;&#23427;&#20204;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#25200;&#21160;&#30340;&#29305;&#24615;&#19982;LLM&#24615;&#33021;&#20043;&#38388;&#30340;&#20960;&#20010;&#32852;&#31995;&#65292;&#20026;&#22343;&#21248;&#37327;&#21270;&#30340;&#22833;&#36133;&#26696;&#20363;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#24182;&#26263;&#31034;&#20102;&#25913;&#21892;LLM&#37327;&#21270;&#31283;&#20581;&#24615;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06408v1 Announce Type: cross  Abstract: Quantization has emerged as a promising technique for improving the memory and computational efficiency of large language models (LLMs). Though the trade-off between performance and efficiency is well-known, there is still much to be learned about the relationship between quantization and LLM performance. To shed light on this relationship, we propose a new perspective on quantization, viewing it as perturbations added to the weights and activations of LLMs. We call this approach "the lens of perturbation". Using this lens, we conduct experiments with various artificial perturbations to explore their impact on LLM performance. Our findings reveal several connections between the properties of perturbations and LLM performance, providing insights into the failure cases of uniform quantization and suggesting potential solutions to improve the robustness of LLM quantization. To demonstrate the significance of our findings, we implement a s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#31070;&#32463;&#35821;&#32773;&#23884;&#20837;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#21069;&#31471;&#20272;&#31639;&#19981;&#30830;&#23450;&#24615;&#24182;&#20256;&#25773;&#33267;&#20313;&#24358;&#25171;&#20998;&#21518;&#31471;&#65292;&#23454;&#29616;&#20102;&#22312;&#35828;&#35805;&#32773;&#35782;&#21035;&#20013;&#38477;&#20302;&#38169;&#35823;&#29575;&#21644;&#26368;&#23567;DCF&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.06404</link><description>&lt;p&gt;
&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#25171;&#20998;&#29992;&#20110;&#31070;&#32463;&#35821;&#32773;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Cosine Scoring with Uncertainty for Neural Speaker Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06404
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#31070;&#32463;&#35821;&#32773;&#23884;&#20837;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#21069;&#31471;&#20272;&#31639;&#19981;&#30830;&#23450;&#24615;&#24182;&#20256;&#25773;&#33267;&#20313;&#24358;&#25171;&#20998;&#21518;&#31471;&#65292;&#23454;&#29616;&#20102;&#22312;&#35828;&#35805;&#32773;&#35782;&#21035;&#20013;&#38477;&#20302;&#38169;&#35823;&#29575;&#21644;&#26368;&#23567;DCF&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#32773;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#26088;&#22312;&#23398;&#20064;&#35328;&#35821;&#35805;&#35821;&#20013;&#23384;&#22312;&#30340;&#21464;&#24322;&#24615;&#12290;&#20256;&#32479;&#30340;&#20313;&#24358;&#25171;&#20998;&#22312;&#35828;&#35805;&#32773;&#35782;&#21035;&#20013;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#32570;&#20047;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35828;&#35805;&#32773;&#23884;&#20837;&#21069;&#31471;&#20272;&#31639;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#20854;&#20256;&#25773;&#33267;&#20313;&#24358;&#25171;&#20998;&#21518;&#31471;&#12290;&#22312;VoxCeleb&#21644;SITW&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#30830;&#35748;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#22788;&#29702;&#30001;&#23884;&#20837;&#20272;&#31639;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#19982;&#20256;&#32479;&#20313;&#24358;&#30456;&#20284;&#24615;&#30456;&#27604;&#65292;&#23427;&#22312;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;8.5&#65285;&#21644;9.8&#65285;&#30340;EER&#21644;minDCF&#24179;&#22343;&#38477;&#20302;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#36824;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06404v1 Announce Type: cross  Abstract: Uncertainty modeling in speaker representation aims to learn the variability present in speech utterances. While the conventional cosine-scoring is computationally efficient and prevalent in speaker recognition, it lacks the capability to handle uncertainty. To address this challenge, this paper proposes an approach for estimating uncertainty at the speaker embedding front-end and propagating it to the cosine scoring back-end. Experiments conducted on the VoxCeleb and SITW datasets confirmed the efficacy of the proposed method in handling uncertainty arising from embedding estimation. It achieved improvement with 8.5% and 9.8% average reductions in EER and minDCF compared to the conventional cosine similarity. It is also computationally efficient in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#26469;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20013;&#30340;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.06402</link><description>&lt;p&gt;
&#19968;&#20992;&#20999;&#19981;&#36866;&#29992;&#65306;&#23398;&#20064;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#22810;&#23569;&#20363;&#20026;&#20102;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
'One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#26469;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20013;&#30340;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06402v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#26032; Abstract: &#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#39044;&#27979;&#27169;&#22411;&#24050;&#32463;&#20174;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#21040;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#31181;&#24494;&#35843;&#30340;&#26497;&#31471;&#24418;&#24335;&#28041;&#21450;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#65288;&#20923;&#32467;&#30340;&#35299;&#30721;&#22120;&#21442;&#25968;&#65289;&#21482;&#21463;&#21040;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#21464;&#21270;&#65288;&#31216;&#20026;&#25351;&#20196;&#25110;&#25552;&#31034;&#65289;&#30340;&#25511;&#21046;&#12290;ICL&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#22312;&#25552;&#31034;&#20013;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#23454;&#20363;&#20316;&#20026;&#31034;&#20363;&#12290;&#23613;&#31649;&#29616;&#26377;&#24037;&#20316;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20026;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#20351;&#29992;&#38745;&#24577;&#25968;&#37327;&#30340;&#31034;&#20363;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20998;&#31867;&#22120;&#20013;&#20351;&#29992;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#30340;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;ICL&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#23545;&#20110;&#29305;&#23450;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#25512;&#29702;&#26102;&#20351;&#29992;&#30340;&#28436;&#31034;&#25968;&#37327;&#26159;&#21160;&#24577;&#35843;&#25972;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06402v1 Announce Type: new  Abstract: Predictive models in natural language processing (NLP) have evolved from training models from scratch to fine-tuning pre-trained models with labelled data. An extreme form of this fine-tuning involves in-context learning (ICL), where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or prompts). An important component of ICL is the use of a small number of labelled data instances as examples in the prompt. While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data. This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow of adaptive ICL (AICL), the number of demonstrations to employ during the inference on a particular data inst
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06398</link><description>&lt;p&gt;
&#20851;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#23485;&#24230;&#36882;&#20943;&#22238;&#25253;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Diminishing Returns of Width for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06398
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#25353;&#39034;&#24207;&#35757;&#32451;&#26032;&#20219;&#21153;&#26102;&#32463;&#24120;&#20986;&#29616;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#12290; &#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#20943;&#23569;&#65292;&#20294;&#23578;&#26410;&#20934;&#30830;&#21051;&#30011;&#23485;&#24230;&#21644;&#25345;&#32493;&#23398;&#20064;&#20043;&#38388;&#30340;&#30830;&#20999;&#20851;&#31995;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20854;&#20013;&#19968;&#20010;&#26368;&#26089;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#29702;&#35770;&#65292;&#24182;&#35777;&#26126;&#23485;&#24230;&#19982;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#20013;&#30340;&#36951;&#24536;&#30452;&#25509;&#30456;&#20851;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#19978;&#32463;&#39564;&#24615;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#65292;&#32467;&#26524;&#26174;&#31034;&#36882;&#20943;&#22238;&#25253;&#22914;&#25105;&#20204;&#30340;&#29702;&#35770;&#25152;&#39044;&#27979;&#30340;&#37027;&#26679;&#28165;&#26224;&#21487;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06398v1 Announce Type: cross  Abstract: While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.
&lt;/p&gt;</description></item><item><title>DeepSafeMPC&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#22810;&#26234;&#20307;&#29615;&#22659;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#24182;&#24212;&#29992;MARL&#21407;&#21017;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.06397</link><description>&lt;p&gt;
DeepSafeMPC: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06397
&lt;/p&gt;
&lt;p&gt;
DeepSafeMPC&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#22810;&#26234;&#20307;&#29615;&#22659;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#24182;&#24212;&#29992;MARL&#21407;&#21017;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;safe MARL&#65289;&#22312;&#26368;&#36817;&#20960;&#24180;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#65292;&#24378;&#35843;&#20102;&#26234;&#20307;&#19981;&#20165;&#38656;&#35201;&#20248;&#21270;&#20840;&#23616;&#22238;&#25253;&#65292;&#36824;&#38656;&#35201;&#36890;&#36807;&#34892;&#20026;&#32422;&#26463;&#36981;&#23432;&#23433;&#20840;&#35201;&#27714;&#30340;&#24517;&#35201;&#24615;&#12290;&#36817;&#26399;&#19968;&#20123;&#24037;&#20316;&#23558;&#25511;&#21046;&#29702;&#35770;&#19982;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#24212;&#29992;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26041;&#27861;&#30340;&#24212;&#29992;&#38750;&#24120;&#26377;&#38480;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22810;&#26234;&#20307;&#29615;&#22659;&#20013;&#22797;&#26434;&#19988;&#38544;&#24335;&#21160;&#24577;&#30340;&#29305;&#24615;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;DeepSafeMPC&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;DeepSafeMPC &#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#21033;&#29992;&#38598;&#20013;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24456;&#22909;&#22320;&#39044;&#27979;&#29615;&#22659;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;MARL&#21407;&#21017;&#26469;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06397v1 Announce Type: cross  Abstract: Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained attention in recent years, emphasizing the need for agents to not only optimize the global return but also adhere to safety requirements through behavioral constraints. Some recent work has integrated control theory with multi-agent reinforcement learning to address the challenge of ensuring safety. However, there have been only very limited applications of Model Predictive Control (MPC) methods in this domain, primarily due to the complex and implicit dynamics characteristic of multi-agent environments. To bridge this gap, we propose a novel method called Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of DeepSafeMPC is leveraging a entralized deep learning model to well predict environmental dynamics. Our method applies MARL principles to search for optimal solutions. Through the employme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#30340;&#26497;&#23567;&#20540;&#30340;&#23574;&#38160;&#24230;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#23545;&#39046;&#22495;&#36716;&#31227;&#20013;&#25968;&#25454;&#21464;&#21270;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#24314;&#31435;&#20102;&#23574;&#38160;&#24230;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20005;&#26684;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23574;&#38160;&#24230;&#30340;OOD&#27867;&#21270;&#30028;&#38480;&#65292;&#20026;&#40065;&#26834;&#31639;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;OOD&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06392</link><description>&lt;p&gt;
&#36890;&#36807;&#23574;&#38160;&#24230;&#23454;&#29616;&#20581;&#22766;&#30340;&#20986;&#22495;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Out-of-Distribution Generalization Bounds via Sharpness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#30340;&#26497;&#23567;&#20540;&#30340;&#23574;&#38160;&#24230;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#23545;&#39046;&#22495;&#36716;&#31227;&#20013;&#25968;&#25454;&#21464;&#21270;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#24314;&#31435;&#20102;&#23574;&#38160;&#24230;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20005;&#26684;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23574;&#38160;&#24230;&#30340;OOD&#27867;&#21270;&#30028;&#38480;&#65292;&#20026;&#40065;&#26834;&#31639;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;OOD&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23545;&#20110;&#39046;&#22495;&#20043;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#25110;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#36827;&#34892;&#27867;&#21270;&#65292;&#21363;OOD&#27867;&#21270;&#65292;&#20173;&#28982;&#32570;&#20047;&#36866;&#24403;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#20256;&#32479;&#30340;OOD&#30028;&#38480;&#20851;&#27880;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#19981;&#21516;&#36317;&#31163;&#27979;&#37327;&#65292;&#20294;&#26410;&#32771;&#34385;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#30340;&#26497;&#23567;&#20540;&#30340;&#23574;&#38160;&#24230;&#23545;&#27169;&#22411;&#22312;&#39046;&#22495;&#36716;&#31227;&#20013;&#25215;&#21463;&#25968;&#25454;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#36825;&#36890;&#24120;&#30001;&#27867;&#21270;&#20013;&#30340;"&#40065;&#26834;&#24615;"&#26469;&#25429;&#33719;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23574;&#38160;&#24230;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20005;&#26684;&#32852;&#31995;&#65292;&#20026;&#40065;&#26834;&#31639;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;OOD&#20445;&#35777;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;"&#24179;&#22374;&#26497;&#23567;&#20540;&#23548;&#33268;&#26356;&#22909;&#30340;OOD&#27867;&#21270;"&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#22522;&#20110;&#23574;&#38160;&#24230;&#30340;OOD&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06392v1 Announce Type: new  Abstract: Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees. Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model. As empirically shown in recent work, the sharpness of learned minima influences OOD generalization. To bridge this gap between optimization and OOD generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by "robustness" in generalization. In this paper, we give a rigorous connection between sharpness and robustness, which gives better OOD guarantees for robust algorithms. It also provides a theoretical backing for "flat minima leads to better OOD generalization". Overall, we propose a sharpness-based OOD generalization bound by taking robustness into consideration, re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30005;&#21147;&#32593;&#32476;&#30340;&#29983;&#25104;&#24335;AI&#25915;&#20987;&#30340;&#38646;&#20449;&#20219;&#26694;&#26550;&#65292;&#21487;&#26089;&#26399;&#26816;&#27979;&#28508;&#22312;&#30340;&#25915;&#20987;&#21521;&#37327;&#12289;&#35780;&#20272;&#23614;&#37096;&#39118;&#38505;&#30340;&#31283;&#23450;&#24615;&#25514;&#26045;&#65292;&#24182;&#23545;&#27492;&#31867;&#23041;&#32961;&#36827;&#34892;&#32531;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.06388</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#21644;&#38450;&#24481;&#21457;&#30005;&#32593;&#32476;&#20013;&#29983;&#25104;&#24335;AI&#25915;&#20987;&#30340;&#38646;&#20449;&#20219;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Zero Trust Framework for Realization and Defense Against Generative AI Attacks in Power Grid
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06388
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30005;&#21147;&#32593;&#32476;&#30340;&#29983;&#25104;&#24335;AI&#25915;&#20987;&#30340;&#38646;&#20449;&#20219;&#26694;&#26550;&#65292;&#21487;&#26089;&#26399;&#26816;&#27979;&#28508;&#22312;&#30340;&#25915;&#20987;&#21521;&#37327;&#12289;&#35780;&#20272;&#23614;&#37096;&#39118;&#38505;&#30340;&#31283;&#23450;&#24615;&#25514;&#26045;&#65292;&#24182;&#23545;&#27492;&#31867;&#23041;&#32961;&#36827;&#34892;&#32531;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#29983;&#25104;&#24335;AI&#65288;GenAI&#65289;&#25915;&#20987;&#23545;&#30005;&#21147;&#32593;&#32476;&#30340;&#28508;&#22312;&#24433;&#21709;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#24517;&#39035;&#35299;&#20915;&#20197;&#20445;&#25252;&#30005;&#21147;&#32593;&#32476;&#20813;&#21463;&#26032;&#25915;&#20987;&#21521;&#37327;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;&#30005;&#21147;&#32593;&#32476;&#20379;&#24212;&#38142;&#65288;PGSC&#65289;&#30340;&#38646;&#20449;&#20219;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#26089;&#26399;&#26816;&#27979;&#28508;&#22312;&#30340;GenAI&#39537;&#21160;&#25915;&#20987;&#21521;&#37327;&#65288;&#20363;&#22914;&#37325;&#25918;&#21644;&#21327;&#35758;&#22411;&#25915;&#20987;&#65289;&#65292;&#35780;&#20272;&#22522;&#20110;&#23614;&#37096;&#39118;&#38505;&#30340;&#31283;&#23450;&#24615;&#25514;&#26045;&#65292;&#24182;&#20943;&#36731;&#27492;&#31867;&#23041;&#32961;&#12290;&#39318;&#20808;&#65292;&#35774;&#35745;&#24182;&#26500;&#24314;&#20102;PGSC&#30340;&#26032;&#38646;&#20449;&#20219;&#31995;&#32479;&#27169;&#22411;&#65292;&#23558;&#20854;&#24418;&#25104;&#20026;&#19968;&#20010;&#23547;&#27714;&#36890;&#36807;&#23454;&#29616;&#21644;&#38450;&#24481;GenAI&#39537;&#21160;&#30340;&#32593;&#32476;&#25915;&#20987;&#26469;&#20445;&#35777;PGSC&#31283;&#23450;&#30340;&#38646;&#20449;&#20219;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#29305;&#23450;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#25915;&#20987;&#29983;&#25104;&#26426;&#21046;&#65292;&#20197;&#20026;&#36827;&#19968;&#27493;&#20102;&#35299;&#23041;&#32961;&#32780;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#32593;&#32476;&#31354;&#38388;&#12290;&#31532;&#19977;&#65292;&#22522;&#20110;&#23614;&#37096;&#39118;&#38505;&#30340;&#31283;&#23450;&#24615;&#25514;&#26045;&#35780;&#20272;&#21644;&#20943;&#36731;&#32593;&#32476;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06388v1 Announce Type: cross  Abstract: Understanding the potential of generative AI (GenAI)-based attacks on the power grid is a fundamental challenge that must be addressed in order to protect the power grid by realizing and validating risk in new attack vectors. In this paper, a novel zero trust framework for a power grid supply chain (PGSC) is proposed. This framework facilitates early detection of potential GenAI-driven attack vectors (e.g., replay and protocol-type attacks), assessment of tail risk-based stability measures, and mitigation of such threats. First, a new zero trust system model of PGSC is designed and formulated as a zero-trust problem that seeks to guarantee for a stable PGSC by realizing and defending against GenAI-driven cyber attacks. Second, in which a domain-specific generative adversarial networks (GAN)-based attack generation mechanism is developed to create a new vulnerability cyberspace for further understanding that threat. Third, tail-based ri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Fennec&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25152;&#26377;&#27169;&#22411;&#21644;&#21382;&#21490;&#20219;&#21153;&#26144;&#23556;&#21040;&#19968;&#20010;&#36801;&#31227;&#30456;&#20851;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#20415;&#25512;&#26029;&#26032;&#20219;&#21153;&#22312;&#36801;&#31227;&#31354;&#38388;&#20013;&#30340;&#34920;&#24449;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.06382</link><description>&lt;p&gt;
&#38024;&#23545;&#19979;&#28216;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Pre-Trained Model Recommendation for Downstream Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Fennec&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25152;&#26377;&#27169;&#22411;&#21644;&#21382;&#21490;&#20219;&#21153;&#26144;&#23556;&#21040;&#19968;&#20010;&#36801;&#31227;&#30456;&#20851;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#20415;&#25512;&#26029;&#26032;&#20219;&#21153;&#22312;&#36801;&#31227;&#31354;&#38388;&#20013;&#30340;&#34920;&#24449;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#27169;&#22411;&#36873;&#25321;&#26088;&#22312;&#23545;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#65292;&#24182;&#36873;&#25321;&#26368;&#36866;&#21512;&#26032;&#30446;&#26631;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#36890;&#24120;&#22312;&#33539;&#22260;&#19978;&#21463;&#38480;&#65292;&#24182;&#20542;&#21521;&#20110;&#24573;&#35270;&#27169;&#22411;&#19982;&#20219;&#21153;&#20043;&#38388;&#24494;&#22937;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21153;&#23454;&#30340;&#26694;&#26550; Fennec&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#24211;&#65292;&#21516;&#26102;&#32454;&#33268;&#32771;&#34385;&#20102;&#20219;&#21153;&#19982;&#27169;&#22411;&#20043;&#38388;&#30340;&#22797;&#26434;&#32852;&#31995;&#12290;&#20851;&#38190;&#27934;&#35265;&#22312;&#20110;&#23558;&#25152;&#26377;&#27169;&#22411;&#21644;&#21382;&#21490;&#20219;&#21153;&#26144;&#23556;&#21040;&#19968;&#20010;&#19982;&#36801;&#31227;&#30456;&#20851;&#30340;&#23376;&#31354;&#38388;&#20013;&#65292;&#27169;&#22411;&#21521;&#37327;&#21644;&#20219;&#21153;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#20195;&#34920;&#20102;&#21487;&#36801;&#31227;&#24615;&#30340;&#22823;&#23567;&#12290;&#19968;&#20010;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#20154;&#65292;&#22312;&#36801;&#31227;&#31354;&#38388;&#20013;&#25512;&#26029;&#26032;&#20219;&#21153;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#36991;&#24320;&#20102;&#36827;&#34892;&#22823;&#37327;&#21069;&#21521;&#20256;&#25773;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#27169;&#22411;&#22266;&#26377;&#24402;&#32435;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06382v1 Announce Type: cross  Abstract: As a fundamental problem in transfer learning, model selection aims to rank off-the-shelf pre-trained models and select the most suitable one for the new target task. Existing model selection techniques are often constrained in their scope and tend to overlook the nuanced relationships between models and tasks. In this paper, we present a pragmatic framework \textbf{Fennec}, delving into a diverse, large-scale model repository while meticulously considering the intricate connections between tasks and models. The key insight is to map all models and historical tasks into a transfer-related subspace, where the distance between model vectors and task vectors represents the magnitude of transferability. A large vision model, as a proxy, infers a new task's representation in the transfer space, thereby circumventing the computational burden of extensive forward passes. We also investigate the impact of the inherent inductive bias of models 
&lt;/p&gt;</description></item><item><title>FEATAUG&#26159;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#22686;&#24378;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#19968;&#23545;&#22810;&#20851;&#31995;&#34920;&#20013;&#33258;&#21160;&#25552;&#21462;&#20855;&#26377;&#35859;&#35789;&#24847;&#35782;&#30340;SQL&#26597;&#35810;&#65292;&#24357;&#34917;&#20102;Featuretools&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#32570;&#20047;&#35859;&#35789;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.06367</link><description>&lt;p&gt;
FeatAug&#65306;&#20174;&#19968;&#23545;&#22810;&#20851;&#31995;&#34920;&#33258;&#21160;&#36827;&#34892;&#29305;&#24449;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
FeatAug: Automatic Feature Augmentation From One-to-Many Relationship Tables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06367
&lt;/p&gt;
&lt;p&gt;
FEATAUG&#26159;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#22686;&#24378;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#19968;&#23545;&#22810;&#20851;&#31995;&#34920;&#20013;&#33258;&#21160;&#25552;&#21462;&#20855;&#26377;&#35859;&#35789;&#24847;&#35782;&#30340;SQL&#26597;&#35810;&#65292;&#24357;&#34917;&#20102;Featuretools&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#32570;&#20047;&#35859;&#35789;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06367v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20174;&#19968;&#23545;&#22810;&#20851;&#31995;&#34920;&#20013;&#36827;&#34892;&#29305;&#24449;&#22686;&#24378;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#20013;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22686;&#24378;&#22909;&#30340;&#29305;&#24449;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#38656;&#35201;&#25163;&#21160;&#25552;&#20986; SQL &#26597;&#35810;&#65292;&#36825;&#26159;&#32791;&#26102;&#30340;&#12290;Featuretools [1] &#26159;&#25968;&#25454;&#31185;&#23398;&#30028;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#36890;&#36807;&#20174;&#30456;&#20851;&#34920;&#20013;&#25552;&#21462;&#26032;&#29305;&#24449;&#26469;&#33258;&#21160;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#12290;&#23427;&#23558;&#27599;&#20010;&#29305;&#24449;&#34920;&#31034;&#20026;&#22312;&#30456;&#20851;&#34920;&#19978;&#30340;&#20998;&#32452;&#32858;&#21512; SQL &#26597;&#35810;&#65292;&#24182;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#36825;&#20123; SQL &#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23427;&#26410;&#22312;&#36825;&#20123;&#26597;&#35810;&#20013;&#21253;&#21547;&#35859;&#35789;&#65292;&#36825;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#26174;&#33879;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FEATAUG&#65292;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#22686;&#24378;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#19968;&#23545;&#22810;&#20851;&#31995;&#34920;&#20013;&#33258;&#21160;&#25552;&#21462;&#20855;&#26377;&#35859;&#35789;&#24847;&#35782;&#30340; SQL &#26597;&#35810;&#12290;&#36825;&#26679;&#30340;&#25193;&#23637;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#32771;&#34385;&#35859;&#35789;&#20250;&#25351;&#25968;&#32423;&#22686;&#21152;&#20505;&#36873;&#26597;&#35810;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06367v1 Announce Type: new  Abstract: Feature augmentation from one-to-many relationship tables is a critical but challenging problem in ML model development. To augment good features, data scientists need to come up with SQL queries manually, which is time-consuming. Featuretools [1] is a widely used tool by the data science community to automatically augment the training data by extracting new features from relevant tables. It represents each feature as a group-by aggregation SQL query on relevant tables and can automatically generate these SQL queries. However, it does not include predicates in these queries, which significantly limits its application in many real-world scenarios. To overcome this limitation, we propose FEATAUG, a new feature augmentation framework that automatically extracts predicate-aware SQL queries from one-to-many relationship tables. This extension is not trivial because considering predicates will exponentially increase the number of candidate que
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#65292;&#38024;&#23545;&#36719;Q-learning&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#20998;&#26512;&#65292;&#20026;&#20004;&#31181;&#36719;Q-learning&#31639;&#27861;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.06366</link><description>&lt;p&gt;
&#36719;Q-learning&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#20998;&#26512;&#65306;&#20999;&#25442;&#31995;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#65292;&#38024;&#23545;&#36719;Q-learning&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#20998;&#26512;&#65292;&#20026;&#20004;&#31181;&#36719;Q-learning&#31639;&#27861;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Soft Q-learning&#26159;Q-learning&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#29109;&#27491;&#21017;&#21270;&#20540;&#20989;&#25968;&#12290;&#23613;&#31649;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#23545;&#36719;Q-learning&#30340;&#29702;&#35770;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#36719;Q-learning&#31639;&#27861;&#30340;&#26032;&#39062;&#21644;&#32479;&#19968;&#30340;&#26377;&#38480;&#26102;&#38388;&#12289;&#25511;&#21046;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#31181;&#31867;&#22411;&#30340;&#36719;Q-learning&#31639;&#27861;&#65306;&#19968;&#31181;&#21033;&#29992;&#23545;&#25968;&#21644;&#25351;&#25968;&#36816;&#31639;&#23376;&#65292;&#21478;&#19968;&#31181;&#37319;&#29992;&#29627;&#23572;&#20857;&#26364;&#36816;&#31639;&#23376;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#65292;&#25105;&#20204;&#20026;&#20004;&#31181;&#36719;Q-learning&#31639;&#27861;&#25512;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#20998;&#26512;&#33021;&#22815;&#36890;&#36807;&#19982;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#24314;&#31435;&#32852;&#31995;&#26469;&#21152;&#28145;&#23545;&#36719;Q-learning&#30340;&#24403;&#21069;&#29702;&#35299;&#65292;&#29978;&#33267;&#20026;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06366v1 Announce Type: new  Abstract: Soft Q-learning is a variation of Q-learning designed to solve entropy regularized Markov decision problems where an agent aims to maximize the entropy regularized value function. Despite its empirical success, there have been limited theoretical studies of soft Q-learning to date. This paper aims to offer a novel and unified finite-time, control-theoretic analysis of soft Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one utilizing the log-sum-exp operator and the other employing the Boltzmann operator. By using dynamical switching system models, we derive novel finite-time error bounds for both soft Q-learning algorithms. We hope that our analysis will deepen the current understanding of soft Q-learning by establishing connections with switching system models and may even pave the way for new frameworks in the finite-time analysis of other reinforcement learning algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Boltzmann&#26041;&#31243;&#30340;BGK&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#20989;&#25968;&#26469;&#25913;&#21892;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.06342</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;Boltzmann&#26041;&#31243;&#30340;BGK&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Separable Physics-informed Neural Networks for Solving the BGK Model of the Boltzmann Equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Boltzmann&#26041;&#31243;&#30340;BGK&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#20989;&#25968;&#26469;&#25913;&#21892;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINNs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#35299;&#20915;Boltzmann&#26041;&#31243;&#30340;BGK&#27169;&#22411;&#12290;&#23613;&#31649;PINNs&#30340;&#26080;&#32593;&#26684;&#29305;&#24615;&#22312;&#22788;&#29702;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#22312;BGK&#25805;&#20316;&#31526;&#30340;&#31215;&#20998;&#31934;&#30830;&#35780;&#20272;&#26102;&#65292;&#24212;&#29992;&#27714;&#31215;&#35268;&#21017;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#21487;&#33021;&#24433;&#21709;&#26080;&#32593;&#26684;&#20248;&#21183;&#24182;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;SPINNs&#30340;&#35268;&#33539;polyadic&#20998;&#35299;&#32467;&#26500;&#21644;&#30697;&#35745;&#31639;&#30340;&#32447;&#24615;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#27714;&#31215;&#35268;&#21017;&#24212;&#29992;&#20013;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#31890;&#23376;&#23494;&#24230;&#20989;&#25968;&#30340;&#22810;&#23610;&#24230;&#24615;&#36136;&#20351;&#24471;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31934;&#30830;&#36924;&#36817;&#23439;&#35266;&#30697;&#22256;&#38590;&#12290;&#20026;&#20102;&#25913;&#36827;SPINN&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#26031;&#20989;&#25968;&#30340;&#31215;&#20998;&#21040;SPINNs&#20013;&#65292;&#24182;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06342v1 Announce Type: cross  Abstract: In this study, we introduce a method based on Separable Physics-Informed Neural Networks (SPINNs) for effectively solving the BGK model of the Boltzmann equation. While the mesh-free nature of PINNs offers significant advantages in handling high-dimensional partial differential equations (PDEs), challenges arise when applying quadrature rules for accurate integral evaluation in the BGK operator, which can compromise the mesh-free benefit and increase computational costs. To address this, we leverage the canonical polyadic decomposition structure of SPINNs and the linear nature of moment calculation, achieving a substantial reduction in computational expense for quadrature rule application. The multi-scale nature of the particle density function poses difficulties in precisely approximating macroscopic moments using neural networks. To improve SPINN training, we introduce the integration of Gaussian functions into SPINNs, coupled with a
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#35299;&#24320;&#20849;&#20139;&#21644;&#31169;&#26377;&#28508;&#22312;&#22240;&#32032;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#29305;&#23450;&#27169;&#24577;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06338</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#35299;&#24320;&#20849;&#20139;&#21644;&#31169;&#26377;&#28508;&#22312;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Disentangling shared and private latent factors in multimodal Variational Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06338
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#35299;&#24320;&#20849;&#20139;&#21644;&#31169;&#26377;&#28508;&#22312;&#22240;&#32032;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#29305;&#23450;&#27169;&#24577;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#30830;&#23450;&#21487;&#33021;&#19982;&#35266;&#27979;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#37325;&#35201;&#20915;&#23450;&#22240;&#32032;&#30456;&#20851;&#32852;&#30340;&#28508;&#22312;&#22240;&#32032;&#12290;&#20849;&#20139;&#22240;&#32032;&#21487;&#20197;&#29992;&#20110;&#35299;&#37322;&#36328;&#27169;&#24577;&#30340;&#21464;&#21270;&#65292;&#32780;&#20854;&#20182;&#22240;&#32032;&#21487;&#33021;&#26159;&#31169;&#26377;&#30340;&#65292;&#20165;&#29992;&#20110;&#35299;&#37322;&#21333;&#20010;&#27169;&#24577;&#12290;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#22914;MVAE&#21644;MMVAE&#65292;&#26159;&#25512;&#26029;&#36825;&#20123;&#28508;&#22312;&#28508;&#22240;&#32032;&#24182;&#23558;&#20849;&#20139;&#21464;&#24322;&#19982;&#31169;&#26377;&#21464;&#24322;&#20998;&#31163;&#30340;&#33258;&#28982;&#36873;&#25321;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#21487;&#38752;&#25191;&#34892;&#27492;&#35299;&#32544;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#20854;&#20013;&#29305;&#23450;&#20110;&#27169;&#24577;&#30340;&#21464;&#24322;&#25903;&#37197;&#20102;&#20849;&#20139;&#20449;&#21495;&#12290;&#37319;&#29992;&#20132;&#21449;&#27169;&#24577;&#39044;&#27979;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#23427;&#20204;&#23545;&#29305;&#23450;&#20110;&#27169;&#24577;&#30340;&#21464;&#24322;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06338v1 Announce Type: cross  Abstract: Generative models for multimodal data permit the identification of latent factors that may be associated with important determinants of observed data heterogeneity. Common or shared factors could be important for explaining variation across modalities whereas other factors may be private and important only for the explanation of a single modality. Multimodal Variational Autoencoders, such as MVAE and MMVAE, are a natural choice for inferring those underlying latent factors and separating shared variation from private. In this work, we investigate their capability to reliably perform this disentanglement. In particular, we highlight a challenging problem setting where modality-specific variation dominates the shared signal. Taking a cross-modal prediction perspective, we demonstrate limitations of existing models, and propose a modification how to make them more robust to modality-specific variation. Our findings are supported by experi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#20445;&#30041;&#20102;&#27169;&#22411;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#36991;&#20813;&#20102;&#32047;&#31215;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06328</link><description>&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#23454;&#29616;&#21487;&#36801;&#31227;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transferable Reinforcement Learning via Generalized Occupancy Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06328
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#20445;&#30041;&#20102;&#27169;&#22411;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#36991;&#20813;&#20102;&#32047;&#31215;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20195;&#29702;&#24517;&#39035;&#26159;&#36890;&#29992;&#30340; - &#20855;&#26377;&#24555;&#36895;&#36866;&#24212;&#21644;&#27010;&#25324;&#21040;&#19981;&#21516;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#20869;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#31639;&#27861;&#23398;&#20064;&#19990;&#30028;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#21160;&#24577;&#27169;&#22411;&#65292;&#21407;&#21017;&#19978;&#20351;&#23427;&#20204;&#33021;&#22815;&#27010;&#25324;&#21040;&#20219;&#24847;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#19968;&#27493;&#27169;&#22411;&#33258;&#28982;&#20250;&#21463;&#21040;&#32047;&#31215;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#20351;&#23427;&#20204;&#22312;&#20855;&#26377;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#22823;&#29366;&#24577;&#31354;&#38388;&#30340;&#38382;&#39064;&#19978;&#22833;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#26032;&#22411;&#27169;&#22411; - &#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65288;GOMs&#65289;&#65292;&#20445;&#30041;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#30340;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32047;&#31215;&#24615;&#38169;&#35823;&#12290;GOMs&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#35206;&#30422;&#19979;&#65292;&#24314;&#27169;&#32473;&#23450;&#29366;&#24577;&#30340;&#25152;&#26377;&#21487;&#33021;&#38271;&#26399;&#32467;&#26524;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#23454;&#29616;&#32473;&#23450;&#29366;&#24577;&#30340;&#29305;&#23450;&#32467;&#26524;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36805;&#36895;&#29992;&#20110;&#20026;&#20219;&#24847;&#26032;&#20219;&#21153;&#36873;&#25321;&#26368;&#20248;&#25805;&#20316;&#65292;&#32780;&#26080;&#38656;&#25285;&#24515;&#32047;&#31215;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06328v1 Announce Type: new  Abstract: Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks. Within the framework of reinforcement learning (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards. However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces. In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error. The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state. These models can then quickly be used to select the optimal action for arbitrary new tasks, without hav
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ACT&#26694;&#26550;&#65292;&#36890;&#36807;&#32422;&#26463;&#39564;&#35777;&#22120;&#33258;&#21160;&#35745;&#31639;&#27599;&#20010;&#21709;&#24212;&#30340;&#32422;&#26463;&#28385;&#24847;&#29575;&#65292;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#19982;&#33258;&#21160;&#32422;&#26463;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06326</link><description>&lt;p&gt;
&#20174;&#25351;&#20196;&#21040;&#32422;&#26463;&#65306;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#19982;&#33258;&#21160;&#32422;&#26463;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ACT&#26694;&#26550;&#65292;&#36890;&#36807;&#32422;&#26463;&#39564;&#35777;&#22120;&#33258;&#21160;&#35745;&#31639;&#27599;&#20010;&#21709;&#24212;&#30340;&#32422;&#26463;&#28385;&#24847;&#29575;&#65292;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#19982;&#33258;&#21160;&#32422;&#26463;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#40784;&#23545;&#20110;&#23558;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35843;&#25972;&#20026;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#20026;&#25152;&#26377;&#31867;&#22411;&#30340;&#25351;&#20196;&#25552;&#20379;&#20154;&#31867;&#27880;&#37322;&#65292;&#29305;&#21035;&#26159;&#20855;&#26377;&#23450;&#21046;&#32422;&#26463;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29992;&#25143;&#25351;&#20196;&#36890;&#24120;&#21253;&#21547;&#32422;&#26463;&#26465;&#20214;&#12290;&#34429;&#28982;&#35780;&#20272;&#25972;&#20010;&#25351;&#20196;&#30340;&#21709;&#24212;&#36136;&#37327;&#36890;&#24120;&#25104;&#26412;&#39640;&#26114;&#65292;&#20294;&#39640;&#25928;&#22320;&#35780;&#20272;&#32422;&#26463;&#26465;&#20214;&#30340;&#28385;&#24847;&#29575;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;NLP&#20219;&#21153;&#20013;&#30340;&#24120;&#35265;&#32422;&#26463;&#26465;&#20214;&#65292;&#23558;&#23427;&#20204;&#22522;&#20110;&#20854;&#21442;&#25968;&#31867;&#22411;&#20998;&#31867;&#20026;&#19977;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;ACT&#65288;Aligning to ConsTraints&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#20026;&#24102;&#32422;&#26463;&#29992;&#25143;&#23545;&#40784;&#29983;&#25104;&#30417;&#30563;&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ACT&#20351;&#29992;&#32422;&#26463;&#39564;&#35777;&#22120;&#65292;&#36825;&#20123;&#39564;&#35777;&#22120;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#26131;&#20110;&#23454;&#29616;&#65292;&#26469;&#35745;&#31639;&#27599;&#20010;&#21709;&#24212;&#30340;&#32422;&#26463;&#28385;&#24847;&#29575;&#65288;CSR&#65289;&#12290;&#23427;&#20026;&#27599;&#20010;&#25552;&#31034;&#21462;&#26679;&#22810;&#20010;&#21709;&#24212;&#24182;&#25910;&#38598;&#20559;&#22909;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06326v1 Announce Type: cross  Abstract: User alignment is crucial for adapting general-purpose language models (LMs) to downstream tasks, but human annotations are often not available for all types of instructions, especially those with customized constraints. We observe that user instructions typically contain constraints. While assessing response quality in terms of the whole instruction is often costly, efficiently evaluating the satisfaction rate of constraints is feasible. We investigate common constraints in NLP tasks, categorize them into three classes based on the types of their arguments, and propose a unified framework, ACT (Aligning to ConsTraints), to automatically produce supervision signals for user alignment with constraints. Specifically, ACT uses constraint verifiers, which are typically easy to implement in practice, to compute constraint satisfaction rate (CSR) of each response. It samples multiple responses for each prompt and collect preference labels ba
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20854;&#38477;&#20302;&#20026;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#20004;&#20010;&#36890;&#29992;&#30340;&#20803;&#31639;&#27861;&#65292;&#19968;&#20010;&#22522;&#20110;&#20048;&#35266;&#31639;&#27861;&#65292;&#21478;&#19968;&#20010;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#65292;&#27010;&#25324;&#20102;&#20197;&#24448;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#24182;&#35777;&#23454;&#20102;&#26032;&#30340;&#29702;&#35770;&#22312;&#20855;&#26377;&#26377;&#30028;&#21487;&#35206;&#30422;&#24615;&#30340;MDP&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06323</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#31561;&#20215;&#35777;&#26126;&#38477;&#20302;&#21040;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39118;&#38505;&#25935;&#24863;RL
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20854;&#38477;&#20302;&#20026;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#20004;&#20010;&#36890;&#29992;&#30340;&#20803;&#31639;&#27861;&#65292;&#19968;&#20010;&#22522;&#20110;&#20048;&#35266;&#31639;&#27861;&#65292;&#21478;&#19968;&#20010;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#65292;&#27010;&#25324;&#20102;&#20197;&#24448;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#24182;&#35777;&#23454;&#20102;&#26032;&#30340;&#29702;&#35770;&#22312;&#20855;&#26377;&#26377;&#30028;&#21487;&#35206;&#30422;&#24615;&#30340;MDP&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20248;&#21270;&#31561;&#20215;&#35777;&#26126;&#65288;OCE&#65289;&#39118;&#38505;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#65288;RSRL&#65289;&#65292;&#35813;&#39118;&#38505;&#27010;&#25324;&#20102;&#26465;&#20214;&#20540;&#39118;&#38505;&#65288;CVaR&#65289;&#12289;&#29109;&#39118;&#38505;&#21644;&#39532;&#31185;&#32500;&#33576;&#30340;&#22343;&#20540;-&#26041;&#24046;&#12290;&#36890;&#36807;&#22686;&#24378;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#36890;&#29992;&#30340;&#20803;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#20854;&#38477;&#20302;&#20026;&#26631;&#20934;RL&#65306;&#19968;&#20010;&#22522;&#20110;&#20048;&#35266;&#31639;&#27861;&#65292;&#21478;&#19968;&#20010;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#20048;&#35266;&#20803;&#31639;&#27861;&#27010;&#25324;&#20102;&#20960;&#20046;&#25152;&#26377;&#20043;&#21069;RSRL&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#20351;&#29992;&#29109;&#39118;&#38505;&#25110;CVaR&#12290;&#22312;&#31163;&#25955;&#22870;&#21169;&#19979;&#65292;&#25105;&#20204;&#30340;&#20048;&#35266;&#29702;&#35770;&#36824;&#35777;&#26126;&#20102;&#20855;&#26377;&#26377;&#30028;&#21487;&#35206;&#30422;&#24615;&#30340;MDP&#65288;&#20363;&#22914;&#22806;&#29983;&#22359;MDP&#65289;&#30340;&#31532;&#19968;&#20010;RSRL&#36951;&#25022;&#19978;&#30028;&#12290;&#22312;&#31163;&#25955;&#22870;&#21169;&#19979;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#20248;&#21270;&#20803;&#31639;&#27861;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#24230;&#37327;&#20013;&#20139;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#23616;&#37096;&#25913;&#36827;&#20445;&#35777;&#65292;&#35813;&#24230;&#37327;&#19979;&#30028;&#20026;&#30495;&#23454;&#30340;OCE&#39118;&#38505;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;PPO&#23454;&#20363;&#21270;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#26500;&#24314;&#19968;&#20010;MDP&#65292;&#24182;&#23637;&#31034;&#23427;&#23398;&#20064;&#20102;&#26368;&#20248;&#30340;&#39118;&#38505;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06323v1 Announce Type: new  Abstract: We study Risk-Sensitive Reinforcement Learning (RSRL) with the Optimized Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk (CVaR), entropic risk and Markowitz's mean-variance. Using an augmented Markov Decision Process (MDP), we propose two general meta-algorithms via reductions to standard RL: one based on optimistic algorithms and another based on policy optimization. Our optimistic meta-algorithm generalizes almost all prior RSRL theory with entropic risk or CVaR. Under discrete rewards, our optimistic theory also certifies the first RSRL regret bounds for MDPs with bounded coverability, e.g., exogenous block MDPs. Under discrete rewards, our policy optimization meta-algorithm enjoys both global convergence and local improvement guarantees in a novel metric that lower bounds the true OCE risk. Finally, we instantiate our framework with PPO, construct an MDP, and show that it learns the optimal risk-sensitive
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#28548;&#28165;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#24694;&#24847;&#23458;&#25143;&#30340;&#28151;&#28102;&#65292;&#36890;&#36807;&#25552;&#20986;&#28151;&#21512;&#23545;&#25163;&#27169;&#22411;&#26469;&#36830;&#25509;&#29616;&#26377;&#30340;&#23545;&#25163;&#27169;&#22411;&#65292;&#20998;&#26512;&#21508;&#31181;&#27602;&#23475;&#25915;&#20987;&#21644;&#38450;&#24481;&#32858;&#21512;&#35268;&#21017;&#65292;&#20174;&#32780;&#20026;&#35813;&#39046;&#22495;&#30340;&#23433;&#20840;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;.</title><link>https://arxiv.org/abs/2403.06319</link><description>&lt;p&gt;
&#20266;&#36896;&#36824;&#26159;&#34987;&#31713;&#25913;? &#29702;&#35299;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#23458;&#25143;
&lt;/p&gt;
&lt;p&gt;
Fake or Compromised? Making Sense of Malicious Clients in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06319
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#28548;&#28165;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#24694;&#24847;&#23458;&#25143;&#30340;&#28151;&#28102;&#65292;&#36890;&#36807;&#25552;&#20986;&#28151;&#21512;&#23545;&#25163;&#27169;&#22411;&#26469;&#36830;&#25509;&#29616;&#26377;&#30340;&#23545;&#25163;&#27169;&#22411;&#65292;&#20998;&#26512;&#21508;&#31181;&#27602;&#23475;&#25915;&#20987;&#21644;&#38450;&#24481;&#32858;&#21512;&#35268;&#21017;&#65292;&#20174;&#32780;&#20026;&#35813;&#39046;&#22495;&#30340;&#23433;&#20840;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#21487;&#20197;&#22312;&#20998;&#25955;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#38450;&#27602;&#25915;&#20987;&#26041;&#38754;&#65292;FL&#23433;&#20840;&#39046;&#22495;&#23384;&#22312;&#28151;&#20081;&#65292;&#22240;&#20026;&#26377;&#35768;&#22810;&#30740;&#31350;&#20551;&#35774;&#23545;&#25163;&#30340;&#33021;&#21147;&#21644;&#23545;&#25163;&#27169;&#22411;&#23384;&#22312;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#23545;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#21508;&#31181;&#27602;&#23475;&#25915;&#20987;&#21644;&#38450;&#24481;&#32858;&#21512;&#35268;&#21017;&#65288;AGRs&#65289;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#22312;&#19968;&#20010;&#20849;&#21516;&#26694;&#26550;&#19979;&#20104;&#20197;&#36830;&#25509;&#26469;&#28548;&#28165;&#36825;&#31181;&#28151;&#20081;&#12290;&#20026;&#20102;&#32852;&#32467;&#29616;&#26377;&#30340;&#23545;&#25163;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#23545;&#25163;&#27169;&#22411;&#65292;&#20301;&#20110;&#23545;&#25163;&#20809;&#35889;&#30340;&#20013;&#38388;&#20301;&#32622;&#65292;&#23545;&#25163;&#20250;&#25439;&#23475;&#19968;&#20123;&#23458;&#25143;&#65292;&#20351;&#29992;&#20182;&#20204;&#21463;&#25439;&#30340;&#26679;&#26412;&#35757;&#32451;&#29983;&#25104;&#65288;&#20363;&#22914;DDPM&#65289;&#27169;&#22411;&#65292;&#24182;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#20197;&#35299;&#20915;&#26356;&#24378;&#65288;&#20363;&#22914;&#26356;&#20415;&#23452;&#65292;&#26356;&#23454;&#38469;&#65289;&#25915;&#20987;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06319v1 Announce Type: new  Abstract: Federated learning (FL) is a distributed machine learning paradigm that enables training models on decentralized data. The field of FL security against poisoning attacks is plagued with confusion due to the proliferation of research that makes different assumptions about the capabilities of adversaries and the adversary models they operate under. Our work aims to clarify this confusion by presenting a comprehensive analysis of the various poisoning attacks and defensive aggregation rules (AGRs) proposed in the literature, and connecting them under a common framework. To connect existing adversary models, we present a hybrid adversary model, which lies in the middle of the spectrum of adversaries, where the adversary compromises a few clients, trains a generative (e.g., DDPM) model with their compromised samples, and generates new synthetic data to solve an optimization for a stronger (e.g., cheaper, more practical) attack against differe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20351;&#29992;&#20154;&#31867;&#38754;&#37096;&#34920;&#24773;&#35757;&#32451;&#30340;&#25925;&#38556;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#23545;&#20110;HRI&#30740;&#31350;&#65292;&#36825;&#24378;&#35843;&#20102;&#25913;&#36827;&#27169;&#22411;&#31283;&#20581;&#24615;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06315</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#24212;&#30740;&#31350;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#29992;&#20110;&#25925;&#38556;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Study on Domain Generalization for Failure Detection through Human Reactions in HRI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06315
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20351;&#29992;&#20154;&#31867;&#38754;&#37096;&#34920;&#24773;&#35757;&#32451;&#30340;&#25925;&#38556;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#23545;&#20110;HRI&#30740;&#31350;&#65292;&#36825;&#24378;&#35843;&#20102;&#25913;&#36827;&#27169;&#22411;&#31283;&#20581;&#24615;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#22312;&#20998;&#24067;&#20869;&#65288;&#30456;&#21516;&#25968;&#25454;&#38598;&#65289;&#36827;&#34892;&#27979;&#35797;&#65307;&#24615;&#33021;&#20960;&#20046;&#24635;&#26159;&#22312;&#20998;&#24067;&#22806;&#35774;&#32622;&#19979;&#38477;&#12290;&#23545;&#20110;HRI&#30740;&#31350;&#65292;&#30446;&#26631;&#24448;&#24448;&#26159;&#24320;&#21457;&#24191;&#20041;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#39046;&#22495;&#27867;&#21270; - &#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#20445;&#25345;&#24615;&#33021; - &#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#20154;&#31867;&#38754;&#37096;&#34920;&#24773;&#19978;&#35757;&#32451;&#30340;&#25925;&#38556;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#36827;&#34892;&#20102;&#31616;&#26126;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19968;&#20010;&#26469;&#33258;&#21463;&#25511;&#23454;&#39564;&#23460;&#29615;&#22659;&#30340;&#25968;&#25454;&#38598;&#21644;&#21478;&#19968;&#20010;&#22312;&#32447;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#20154;&#31867;&#23545;&#21457;&#29983;&#38169;&#35823;&#30340;&#35270;&#39057;&#20316;&#20986;&#30340;&#21453;&#24212;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#24403;&#22312;&#20132;&#26367;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#21453;&#24605;&#20102;&#35266;&#23519;&#21040;&#30340;&#27169;&#22411;&#34892;&#20026;&#30340;&#21407;&#22240;&#24182;&#25552;&#20986;&#24314;&#35758;&#12290;&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;HRI&#30740;&#31350;&#38656;&#35201;&#20851;&#27880;&#25913;&#36827;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#23454;&#38469;&#36866;&#29992;&#24615;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06315v1 Announce Type: cross  Abstract: Machine learning models are commonly tested in-distribution (same dataset); performance almost always drops in out-of-distribution settings. For HRI research, the goal is often to develop generalized models. This makes domain generalization - retaining performance in different settings - a critical issue. In this study, we present a concise analysis of domain generalization in failure detection models trained on human facial expressions. Using two distinct datasets of humans reacting to videos where error occurs, one from a controlled lab setting and another collected online, we trained deep learning models on each dataset. When testing these models on the alternate dataset, we observed a significant performance drop. We reflect on the causes for the observed model behavior and leave recommendations. This work emphasizes the need for HRI research focusing on improving model robustness and real-life applicability.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;$L_0$&#33539;&#25968;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20248;&#31574;&#30053;&#31232;&#30095;&#21270;&#21644;&#20302;&#31209;&#20998;&#35299;</title><link>https://arxiv.org/abs/2403.06313</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20248;&#31574;&#30053;&#31232;&#30095;&#21270;&#21644;&#20302;&#31209;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;$L_0$&#33539;&#25968;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20248;&#31574;&#30053;&#31232;&#30095;&#21270;&#21644;&#20302;&#31209;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#22810;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#32791;&#36153;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23548;&#33268;&#23494;&#38598;&#31574;&#30053;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#23494;&#38598;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#25512;&#29702;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36793;&#32536;&#35745;&#31639;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#38480;&#21046;&#36807;&#25311;&#21512;&#21644;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65292;&#30740;&#31350;&#32773;&#24050;&#32463;&#20351;&#29992;&#20102;&#20687;&#21098;&#26525;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#36825;&#26679;&#30340;&#25216;&#26415;&#26469;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#21644;&#27169;&#22411;&#21387;&#32553;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#23548;&#33268;&#20102;&#24615;&#33021;&#27425;&#20248;&#65292;&#22312;&#22870;&#21169;&#26041;&#38754;&#20986;&#29616;&#26174;&#33879;&#30340;&#20943;&#24369;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#24320;&#21457;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;$L_1$&#21644;$L_2$&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#23454;&#29616;&#23578;&#19981;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$L_0$&#33539;&#25968;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26368;&#20248;&#31232;&#30095;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06313v1 Announce Type: cross  Abstract: Deep reinforcement learning(DRL) has shown significant promise in a wide range of applications including computer games and robotics. Yet, training DRL policies consume extraordinary computing resources resulting in dense policies which are prone to overfitting. Moreover, inference with dense DRL policies limit their practical applications, especially in edge computing. Techniques such as pruning and singular value decomposition have been used with deep learning models to achieve sparsification and model compression to limit overfitting and reduce memory consumption. However, these techniques resulted in sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$ regularization techniques have been proposed for neural network sparsification and sparse auto-encoder development, but their implementation in DRL environments has not been apparent. We propose a novel $L_0$-norm-regularization technique using an optimal sparsity m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24635;&#20307;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#26469;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#22635;&#20805;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23545; CIFAR10 &#21644; EMNIST &#25968;&#25454;&#38598;&#36827;&#34892;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.06311</link><description>&lt;p&gt;
&#20320;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#65311;&#31532;&#20108;&#37096;&#20998;&#65306;&#39044;&#27979;&#28145;&#24230;&#23398;&#20064;&#31867;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;
&lt;/p&gt;
&lt;p&gt;
How much data do you need? Part 2: Predicting DL class specific training dataset sizes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06311
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24635;&#20307;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#26469;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#22635;&#20805;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23545; CIFAR10 &#21644; EMNIST &#25968;&#25454;&#38598;&#36827;&#34892;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22312;&#32771;&#34385;&#27599;&#20010;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#32780;&#19981;&#20165;&#20165;&#26159;&#24635;&#20307;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#65292;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#22266;&#23450;&#24635;&#20307;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#24212;&#32771;&#34385;&#21738;&#20123;&#27599;&#20010;&#31867;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#32452;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21463;&#21040;&#23454;&#39564;&#35774;&#35745;&#20013;&#30340;&#31354;&#38388;&#22635;&#28385;&#35774;&#35745;&#30340;&#29305;&#27530;&#24773;&#20917;&#30340;&#21551;&#21457;&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#20351;&#29992;&#35832;&#22914;&#24130;&#24459;&#26354;&#32447;&#21644;&#31867;&#20284;&#27169;&#22411;&#12289;&#25193;&#23637;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#31561;&#27169;&#22411;&#26469;&#36827;&#34892;&#24314;&#27169;&#65292;&#21363;&#36890;&#36807;&#23558;&#24635;&#20307;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#26367;&#25442;&#20026;&#32473;&#23450;&#26631;&#31614;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#30340;&#21442;&#25968;&#21270;&#32447;&#24615;&#32452;&#21512;&#12290;&#35813;&#31639;&#27861;&#24050;&#24212;&#29992;&#20110;CIFAR10&#21644;EMNIST&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06311v1 Announce Type: new  Abstract: This paper targets the question of predicting machine learning classification model performance, when taking into account the number of training examples per class and not just the overall number of training examples. This leads to the a combinatorial question, which combinations of number of training examples per class should be considered, given a fixed overall training dataset size. In order to solve this question, an algorithm is suggested which is motivated from special cases of space filling design of experiments. The resulting data are modeled using models like powerlaw curves and similar models, extended like generalized linear models i.e. by replacing the overall training dataset size by a parametrized linear combination of the number of training examples per label class. The proposed algorithm has been applied on the CIFAR10 and the EMNIST datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26465;&#30340;&#38750;&#21442;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#32467;&#26500;&#20998;&#24067;&#30340;&#28789;&#27963;&#21518;&#39564;&#36924;&#36817;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06302</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#33258;&#21160;&#24494;&#20998;&#21464;&#20998;&#25512;&#26029;&#19982;&#26679;&#26465;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Automatic Differentiation Variational Inference with Spline Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26465;&#30340;&#38750;&#21442;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#32467;&#26500;&#20998;&#24067;&#30340;&#28789;&#27963;&#21518;&#39564;&#36924;&#36817;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#21464;&#20998;&#25512;&#26029;&#65288;ADVI&#65289;&#22312;&#23398;&#20064;&#27010;&#29575;&#27169;&#22411;&#26041;&#38754;&#24456;&#26377;&#25928;&#12290;&#32463;&#20856;ADVI&#20381;&#36182;&#20110;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26465;&#30340;&#38750;&#21442;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#30340;&#20998;&#24067;&#65288;&#22914;&#20559;&#24230;&#12289;&#22810;&#23792;&#24615;&#21644;&#26377;&#30028;&#25903;&#25345;&#65289;&#30340;&#28789;&#27963;&#21518;&#39564;&#36924;&#36817;&#12290;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#38750;&#21442;&#25968;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#36866;&#24212;&#21508;&#31181;&#25968;&#25454;&#32467;&#26500;&#12290;&#36890;&#36807;&#37319;&#29992;&#26679;&#26465;&#36924;&#36817;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#37325;&#35201;&#24615;&#21152;&#26435;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#19968;&#20010;&#19979;&#30028;&#65292;&#24182;&#30830;&#31435;&#20102;&#28176;&#36817;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#36924;&#36817;&#22797;&#26434;&#21518;&#39564;&#20998;&#24067;&#21644;&#25913;&#21892;&#20855;&#26377;&#19981;&#23436;&#20840;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06302v1 Announce Type: cross  Abstract: Automatic Differentiation Variational Inference (ADVI) is efficient in learning probabilistic models. Classic ADVI relies on the parametric approach to approximate the posterior. In this paper, we develop a spline-based nonparametric approximation approach that enables flexible posterior approximation for distributions with complicated structures, such as skewness, multimodality, and bounded support. Compared with widely-used nonparametric variational inference methods, the proposed method is easy to implement and adaptive to various data structures. By adopting the spline approximation, we derive a lower bound of the importance weighted autoencoder and establish the asymptotic consistency. Experiments demonstrate the efficiency of the proposed method in approximating complex posterior distributions and improving the performance of generative models with incomplete data.
&lt;/p&gt;</description></item><item><title>GTVMin&#22312;&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#23453;&#36149;&#35265;&#35299;</title><link>https://arxiv.org/abs/2403.06298</link><description>&lt;p&gt;
&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#24635;&#21464;&#24046;&#26368;&#23567;&#21270;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Total Variation Minimization for Clustered Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06298
&lt;/p&gt;
&lt;p&gt;
GTVMin&#22312;&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#23453;&#36149;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#35782;&#21035;&#22823;&#33268;&#21516;&#36136;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#32676;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#19968;&#31181;&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26159;&#24191;&#20041;&#24635;&#21464;&#24046;&#26368;&#23567;&#21270;&#65288;GTVMin&#65289;&#12290;&#35813;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#30456;&#20284;&#24615;&#22270;&#65292;&#21487;&#20197;&#36890;&#36807;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#25110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22270;&#23398;&#20064;&#25216;&#26415;&#26469;&#33719;&#24471;&#12290;&#22312;&#19968;&#20010;&#24191;&#27867;&#36866;&#29992;&#30340;&#38598;&#32676;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;GTVMin&#35299;&#19982;&#20854;&#25353;&#31751;&#24179;&#22343;&#20540;&#20043;&#38388;&#30340;&#20559;&#24046;&#30340;&#19978;&#30028;&#12290;&#36825;&#20010;&#30028;&#38480;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;GTVMin&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06298v1 Announce Type: new  Abstract: A key challenge in federated learning applications is the statistical heterogeneity of local datasets. Clustered federated learning addresses this challenge by identifying clusters of local datasets that are approximately homogeneous. One recent approach to clustered federated learning is generalized total variation minimization (GTVMin). This approach requires a similarity graph which can be obtained by domain expertise or in a data-driven fashion via graph learning techniques. Under a widely applicable clustering assumption, we derive an upper bound the deviation between GTVMin solutions and their cluster-wise averages. This bound provides valuable insights into the effectiveness and robustness of GTVMin in addressing statistical heterogeneity within federated learning environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;&#19981;&#20165;&#19982;&#21512;&#25104;&#26631;&#31614;&#38169;&#35823;&#26377;&#26174;&#33879;&#19981;&#21516;&#65292;&#32780;&#19988;&#22312;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#26500;&#25104;&#20102;&#29420;&#29305;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;&#30340;SCL&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.06289</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#36731;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;&#19981;&#20165;&#19982;&#21512;&#25104;&#26631;&#31614;&#38169;&#35823;&#26377;&#26174;&#33879;&#19981;&#21516;&#65292;&#32780;&#19988;&#22312;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#26500;&#25104;&#20102;&#29420;&#29305;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;&#30340;SCL&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;arXiv:2403.06289v1&#20844;&#24320;&#30340;&#20132;&#21449;&#31867;&#22411;&#30340;&#25991;&#25688;&#21487;&#20197;&#24471;&#30693;&#65292;&#20154;&#24037;&#26631;&#27880;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#21253;&#21547;&#19968;&#37096;&#20998;&#20154;&#24037;&#26631;&#27880;&#38169;&#35823;&#30340;&#31034;&#20363;&#12290;&#23613;&#31649;&#36825;&#31867;&#26631;&#27880;&#38169;&#35823;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#36127;&#38754;&#24433;&#21709;&#24050;&#32463;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#23545;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SCL&#65289;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#30456;&#23545;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20154;&#24037;&#26631;&#27880;&#38169;&#35823;&#19981;&#20165;&#19982;&#21512;&#25104;&#26631;&#31614;&#38169;&#35823;&#26174;&#33879;&#19981;&#21516;&#65292;&#32780;&#19988;&#22312;SCL&#20013;&#26500;&#25104;&#29420;&#29305;&#25361;&#25112;&#65292;&#19982;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#25361;&#25112;&#26377;&#25152;&#19981;&#21516;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#23427;&#20204;&#20316;&#20026;&#35823;&#25253;&#26679;&#26412;&#20986;&#29616;&#26102;&#65292;&#23427;&#20204;&#20250;&#23545;&#23398;&#20064;&#36807;&#31243;&#36896;&#25104;&#22823;&#32422;99%&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#24050;&#26377;&#30340;&#22122;&#22768;&#32531;&#35299;&#26041;&#27861;&#20027;&#35201;&#20391;&#37325;&#20110;&#21512;&#25104;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#22788;&#29702;&#38750;&#24120;&#39640;&#21512;&#25104;&#22122;&#22768;&#29575;&#65288;40-80%&#65289;&#30340;&#19981;&#20999;&#23454;&#38469;&#35774;&#32622;&#65292;&#20294;&#30001;&#20110;&#36807;&#24230;&#25311;&#21512;&#65292;&#23427;&#20204;&#22312;&#26222;&#36890;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24448;&#24448;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20154;&#24037;&#26631;&#27880;&#40065;&#26834;&#24615;&#30340;SCL&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06289v1 Announce Type: cross  Abstract: Human-annotated vision datasets inevitably contain a fraction of human mislabelled examples. While the detrimental effects of such mislabelling on supervised learning are well-researched, their influence on Supervised Contrastive Learning (SCL) remains largely unexplored. In this paper, we show that human-labelling errors not only differ significantly from synthetic label errors, but also pose unique challenges in SCL, different to those in traditional supervised learning methods. Specifically, our results indicate they adversely impact the learning process in the ~99% of cases when they occur as false positive samples. Existing noise-mitigating methods primarily focus on synthetic label errors and tackle the unrealistic setting of very high synthetic noise rates (40-80%), but they often underperform on common image datasets due to overfitting. To address this issue, we introduce a novel SCL objective with robustness to human-labelling
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#33268;&#21147;&#20110;&#30740;&#31350;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#29109;&#27491;&#21017;&#21270;&#24494;&#35843;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20998;&#26512;&#22914;&#20309;&#25193;&#23637;&#21040;&#28041;&#21450;&#19968;&#33324;$f$-&#25955;&#24230;&#27491;&#21017;&#21270;&#30340;&#24494;&#35843;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.06279</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#25511;&#21046;&#36827;&#34892;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#65306;&#29109;&#27491;&#21017;&#21270;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#30740;&#31350;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#29109;&#27491;&#21017;&#21270;&#24494;&#35843;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20998;&#26512;&#22914;&#20309;&#25193;&#23637;&#21040;&#28041;&#21450;&#19968;&#33324;$f$-&#25955;&#24230;&#27491;&#21017;&#21270;&#30340;&#24494;&#35843;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21457;&#23637;&#24182;&#23545;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#20013;&#29109;&#27491;&#21017;&#21270;&#24494;&#35843;&#38382;&#39064;&#36827;&#34892;&#20005;&#26684;&#22788;&#29702;&#65292;&#35813;&#38382;&#39064;&#26368;&#36817;&#30001;&#19978;&#21407;&#31561;&#20154;&#25552;&#20986;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#28041;&#21450;&#19968;&#33324;$f$-&#25955;&#24230;&#27491;&#21017;&#21270;&#30340;&#24494;&#35843;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06279v1 Announce Type: cross  Abstract: This paper aims to develop and provide a rigorous treatment to the problem of entropy regularized fine-tuning in the context of continuous-time diffusion models, which was recently proposed by Uehara et al. ( arXiv:2402.15194, 2024). We also show how the analysis can be extended to fine-tuning involving a general $f$-divergence regularizer.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNICORN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#23454;&#29616;&#36229;&#22768;&#32435;&#21345;&#21152;&#31859;&#25104;&#20687;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#21644;&#20998;&#36776;&#29575;&#36136;&#37327;&#19978;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06275</link><description>&lt;p&gt;
UNICORN: &#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#23454;&#29616;&#30340;&#36229;&#22768;&#32435;&#21345;&#21152;&#31859;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06275
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNICORN&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#23454;&#29616;&#36229;&#22768;&#32435;&#21345;&#21152;&#31859;&#25104;&#20687;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#21644;&#20998;&#36776;&#29575;&#36136;&#37327;&#19978;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Nakagami&#25104;&#20687;&#22312;&#36229;&#22768;&#27874;&#20013;&#21487;&#35270;&#21270;&#21644;&#37327;&#21270;&#32452;&#32455;&#25955;&#23556;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#22312;&#32959;&#30244;&#35786;&#26029;&#21644;&#33026;&#32938;&#20998;&#25968;&#20272;&#35745;&#31561;&#39046;&#22495;&#26377;&#28508;&#22312;&#24212;&#29992;&#65292;&#32780;&#36825;&#20123;&#39046;&#22495;&#24456;&#38590;&#36890;&#36807;&#20256;&#32479;&#36229;&#22768;B&#27169;&#24335;&#22270;&#20687;&#20998;&#36776;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#36873;&#25321;&#26368;&#20248;&#31383;&#21475;&#22823;&#23567;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#30001;&#20110;&#20272;&#35745;&#19981;&#31283;&#23450;&#24615;&#32780;&#23548;&#33268;&#20998;&#36776;&#29575;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNICORN&#65288;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#23454;&#29616;&#30340;&#36229;&#22768;&#32435;&#21345;&#21152;&#31859;&#25104;&#20687;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#30340;&#65292;&#23553;&#38381;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#36229;&#22768;&#21253;&#32476;&#30340;&#20998;&#25968;&#20989;&#25968;&#26469;&#20272;&#35745;&#21345;&#21152;&#31859;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#36229;&#22768;RF&#25968;&#25454;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;UNICORN&#22312;&#20934;&#30830;&#24615;&#21644;&#20998;&#36776;&#29575;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06275v1 Announce Type: cross  Abstract: Nakagami imaging holds promise for visualizing and quantifying tissue scattering in ultrasound waves, with potential applications in tumor diagnosis and fat fraction estimation which are challenging to discern by conventional ultrasound B-mode images. Existing methods struggle with optimal window size selection and suffer from estimator instability, leading to degraded resolution images. To address this, here we propose a novel method called UNICORN (Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an accurate, closed-form estimator for Nakagami parameter estimation in terms of the score function of ultrasonic envelope. Extensive experiments using simulation and real ultrasound RF data demonstrate UNICORN's superiority over conventional approaches in accuracy and resolution quality.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29289;&#29702;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36712;&#36857;&#25968;&#25454;&#20013;&#35782;&#21035;&#24322;&#24120;&#38388;&#38553;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#31038;&#20250;&#24212;&#29992;&#21644;&#25216;&#26415;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.06268</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#24322;&#24120;&#36712;&#36857;&#38388;&#38553;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Physics-Guided Abnormal Trajectory Gap Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06268
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36712;&#36857;&#25968;&#25454;&#20013;&#35782;&#21035;&#24322;&#24120;&#38388;&#38553;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#31038;&#20250;&#24212;&#29992;&#21644;&#25216;&#26415;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#38388;&#38553;&#65288;&#21363;&#32570;&#22833;&#25968;&#25454;&#65289;&#30340;&#36712;&#36857;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#35782;&#21035;&#36712;&#36857;&#20013;&#30340;&#24322;&#24120;&#38388;&#38553;&#30340;&#31639;&#27861;&#65292;&#36825;&#31181;&#24322;&#24120;&#38388;&#38553;&#22312;&#19968;&#20010;&#31227;&#21160;&#29289;&#20307;&#27809;&#26377;&#25253;&#21578;&#20854;&#20301;&#32622;&#26102;&#21457;&#29983;&#65292;&#20294;&#21516;&#19968;&#22320;&#29702;&#21306;&#22495;&#30340;&#20854;&#20182;&#31227;&#21160;&#29289;&#20307;&#21608;&#26399;&#24615;&#22320;&#25253;&#21578;&#20301;&#32622;&#12290;&#35813;&#38382;&#39064;&#30001;&#20110;&#20854;&#23545;&#31038;&#20250;&#30340;&#37325;&#35201;&#24212;&#29992;&#32780;&#21464;&#24471;&#37325;&#35201;&#65292;&#22914;&#25913;&#21892;&#28023;&#19978;&#23433;&#20840;&#21644;&#23545;&#20840;&#29699;&#23433;&#20840;&#38382;&#39064;&#65288;&#22914;&#38750;&#27861;&#25429;&#40060;&#12289;&#38750;&#27861;&#36755;&#27833;&#21644;&#36716;&#36816;&#27963;&#21160;&#65289;&#30340;&#30417;&#31649;&#25191;&#34892;&#12290;&#35813;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#24456;&#38590;&#38480;&#23450;&#22312;&#36712;&#36857;&#38388;&#38553;&#26399;&#38388;&#31227;&#21160;&#29289;&#20307;&#30340;&#21487;&#33021;&#20301;&#32622;&#65292;&#24182;&#19988;&#26816;&#27979;&#22914;&#27492;&#22823;&#37327;&#20301;&#32622;&#25968;&#25454;&#20013;&#30340;&#38388;&#38553;&#30340;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#12290;&#30446;&#21069;&#20851;&#20110;&#24322;&#24120;&#36712;&#36857;&#26816;&#27979;&#30340;&#25991;&#29486;&#20551;&#35774;&#22312;&#38388;&#38553;&#20869;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#26816;&#27979;&#21040;&#24322;&#24120;&#38388;&#38553;&#65292;&#22240;&#20026;&#22312;&#32473;&#23450;&#21306;&#22495;&#20869;&#30340;&#29289;&#20307;&#21487;&#33021;&#24050;&#32463;&#20559;&#31163;&#26368;&#30701;&#36335;&#24452;&#12290;&#22312;&#21021;&#27493;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06268v1 Announce Type: cross  Abstract: Given trajectories with gaps (i.e., missing data), we investigate algorithms to identify abnormal gaps in trajectories which occur when a given moving object did not report its location, but other moving objects in the same geographic region periodically did. The problem is important due to its societal applications, such as improving maritime safety and regulatory enforcement for global security concerns such as illegal fishing, illegal oil transfers, and trans-shipments. The problem is challenging due to the difficulty of bounding the possible locations of the moving object during a trajectory gap, and the very high computational cost of detecting gaps in such a large volume of location data. The current literature on anomalous trajectory detection assumes linear interpolation within gaps, which may not be able to detect abnormal gaps since objects within a given region may have traveled away from their shortest path. In preliminary 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21387;&#32553;&#22312;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#35777;&#26126;&#20102;&#21387;&#32553;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#20043;&#38388;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06265</link><description>&lt;p&gt;
&#25286;&#35299;&#20998;&#35789;&#65306;&#35780;&#20272;&#25991;&#26412;&#21387;&#32553;&#21450;&#20854;&#19982;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21387;&#32553;&#22312;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#35777;&#26126;&#20102;&#21387;&#32553;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#20043;&#38388;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21387;&#32553;&#26159;BPE&#26368;&#24120;&#35265;&#30340;&#20998;&#35789;&#31639;&#27861;&#30340;&#37325;&#35201;&#22522;&#30784;&#65292;&#20294;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#21387;&#32553;&#37325;&#35201;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#35770;&#36848;&#20102;&#21387;&#32553;&#30340;&#29702;&#35770;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;0-gram&#35821;&#35328;&#24314;&#27169;&#65292;&#21363;&#20026;&#25152;&#26377;&#26631;&#35760;&#20998;&#37197;&#30456;&#31561;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21387;&#32553;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#29992;&#25991;&#26723;&#30340;&#25968;&#37327;&#26469;&#25511;&#21046;&#22810;&#20010;BPE&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#33021;&#21147;&#65306;&#20174;100&#19975;&#20010;&#25991;&#26723;&#21040;&#30456;&#24403;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#22522;&#20110;&#23383;&#31526;&#30340;&#20998;&#35789;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#20998;&#35789;&#22120;&#39044;&#35757;&#32451;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#21387;&#32553;&#26159;&#20998;&#35789;&#30340;&#21487;&#38752;&#20869;&#22312;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06265v1 Announce Type: cross  Abstract: Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers' compression and models' downstream performance, suggesting that compression is a reliable intrinsic indicator of tokeniza
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.06259</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Conceptual Knowledge for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21644;&#35780;&#20272;&#20165;&#25506;&#35752;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#28982;&#32780;LLMs&#26159;&#21542;&#20855;&#26377;&#20462;&#25913;&#27010;&#24565;&#30340;&#33021;&#21147;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;LLMs&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;ConceptEdit&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#32423;&#21035;&#30340;&#23450;&#20041;&#65292;&#20294;&#23427;&#20204;&#20063;&#26377;&#28508;&#21147;&#25197;&#26354;LLMs&#20013;&#30456;&#20851;&#30340;&#23454;&#20363;&#30693;&#35782;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#21487;&#20197;&#28608;&#21457;&#23545;&#26356;&#22909;&#29702;&#35299;LLMs&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#20027;&#39029;&#20301;&#20110;https://zjunlp.github.io/project/ConceptEdit&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06259v1 Announce Type: cross  Abstract: Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#20809;&#35889;&#31070;&#32463;&#36861;&#36394;&#26041;&#27861;&#65292;&#26080;&#38656;&#31163;&#32447;&#35757;&#32451;&#65292;&#21487;&#20197;&#24555;&#36895;&#24182;&#20934;&#30830;&#22320;&#22312;&#22810;&#20809;&#35889;&#22270;&#20687;&#20013;&#37325;&#26500;&#31070;&#32463;&#12290;</title><link>https://arxiv.org/abs/2403.06251</link><description>&lt;p&gt;
&#22312;&#32447;&#22810;&#20809;&#35889;&#31070;&#32463;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Online Multi-spectral Neuron Tracing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#20809;&#35889;&#31070;&#32463;&#36861;&#36394;&#26041;&#27861;&#65292;&#26080;&#38656;&#31163;&#32447;&#35757;&#32451;&#65292;&#21487;&#20197;&#24555;&#36895;&#24182;&#20934;&#30830;&#22320;&#22312;&#22810;&#20809;&#35889;&#22270;&#20687;&#20013;&#37325;&#26500;&#31070;&#32463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#20809;&#35889;&#31070;&#32463;&#36861;&#36394;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#29420;&#29305;&#35774;&#35745;&#30340;&#27169;&#22359;&#65292;&#26080;&#38656;&#31163;&#32447;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#32447;&#35757;&#32451;&#26469;&#26356;&#26032;&#22686;&#24378;&#30340;&#21028;&#21035;&#24615;&#30456;&#20851;&#28388;&#27874;&#22120;&#65292;&#20197;&#36830;&#25509;&#36861;&#36394;&#36807;&#31243;&#12290;&#36825;&#31181;&#29420;&#29305;&#30340;&#26080;&#31163;&#32447;&#35757;&#32451;&#27169;&#24335;&#20351;&#25105;&#20204;&#19982;&#20854;&#20182;&#38656;&#35201;&#35757;&#32451;&#20381;&#36182;&#30340;&#36861;&#36394;&#26041;&#27861;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65289;&#19981;&#21516;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#36861;&#36394;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#20182;&#26041;&#27861;&#38656;&#35201;&#22797;&#26434;&#30340;&#35774;&#32622;&#65292;&#20363;&#22914;&#29992;&#20110;&#32858;&#31867;&#21644;&#22270;&#22810;&#20999;&#21106;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#23481;&#26131;&#24212;&#29992;&#20110;&#26032;&#22270;&#20687;&#12290;&#20107;&#23454;&#19978;&#65292;&#23427;&#21482;&#38656;&#35201;&#19968;&#20010;&#36861;&#36394;&#31070;&#32463;&#20803;&#30340;&#36215;&#22987;&#36793;&#30028;&#26694;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#29992;&#25143;&#30340;&#37197;&#32622;&#24037;&#20316;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26080;&#38656;&#35757;&#32451;&#21644;&#26131;&#37197;&#32622;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#22810;&#20809;&#35889;&#22270;&#20687;&#20013;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#31070;&#32463;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06251v1 Announce Type: cross  Abstract: In this paper, we propose an online multi-spectral neuron tracing method with uniquely designed modules, where no offline training are required. Our method is trained online to update our enhanced discriminative correlation filter to conglutinate the tracing process. This distinctive offline-training-free schema differentiates us from other training-dependent tracing approaches like deep learning methods since no annotation is needed for our method. Besides, compared to other tracing methods requiring complicated set-up such as for clustering and graph multi-cut, our approach is much easier to be applied to new images. In fact, it only needs a starting bounding box of the tracing neuron, significantly reducing users' configuration effort. Our extensive experiments show that our training-free and easy-configured methodology allows fast and accurate neuron reconstructions in multi-spectral images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#20998;&#31867;&#19982;&#29702;&#24615;&#21270;&#65288;C2R&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#29702;&#24615;&#21270;&#27169;&#22359;&#21327;&#21516;&#24037;&#20316;&#65292;&#25913;&#21892;&#23545;&#20998;&#24067;&#20043;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.06239</link><description>&lt;p&gt;
&#21512;&#20316;&#20998;&#31867;&#19982;&#29702;&#24615;&#21270;&#29992;&#20110;&#22270;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cooperative Classification and Rationalization for Graph Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#20998;&#31867;&#19982;&#29702;&#24615;&#21270;&#65288;C2R&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#29702;&#24615;&#21270;&#27169;&#22359;&#21327;&#21516;&#24037;&#20316;&#65292;&#25913;&#21892;&#23545;&#20998;&#24067;&#20043;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06239v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#38754;&#23545;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#26102;&#24456;&#38590;&#26377;&#25928;&#27867;&#21270;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20854;&#20013;&#20043;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#20462;&#25913;&#25968;&#25454;&#29615;&#22659;&#26469;&#20351;&#21407;&#22987;&#20998;&#31867;&#30340;&#35757;&#32451;&#20998;&#24067;&#22810;&#26679;&#21270;&#65292;&#20294;&#35775;&#38382;&#29615;&#22659;&#20449;&#24687;&#36739;&#20026;&#22797;&#26434;&#12290;&#21478;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#28041;&#21450;&#29702;&#24615;&#21270;&#65292;&#25552;&#21462;&#29992;&#20110;&#39044;&#27979;&#30340;&#19981;&#21464;&#21407;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#20449;&#21495;&#26377;&#38480;&#65292;&#25552;&#21462;&#21407;&#29702;&#26159;&#22256;&#38590;&#30340;&#65292;&#23548;&#33268;&#36739;&#19981;&#20934;&#30830;&#30340;&#21407;&#29702;&#21644;&#20943;&#24369;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#20998;&#31867;&#19982;&#29702;&#24615;&#21270;&#65288;C2R&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#27169;&#22359;&#21644;&#29702;&#24615;&#21270;&#27169;&#22359;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#20551;&#35774;&#20998;&#31867;&#20013;&#23384;&#22312;&#22810;&#20010;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06239v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) have achieved impressive results in graph classification tasks, but they struggle to generalize effectively when faced with out-of-distribution (OOD) data. Several approaches have been proposed to address this problem. Among them, one solution is to diversify training distributions in vanilla classification by modifying the data environment, yet accessing the environment information is complex. Besides, another promising approach involves rationalization, extracting invariant rationales for predictions. However, extracting rationales is difficult due to limited learning signals, resulting in less accurate rationales and diminished predictions. To address these challenges, in this paper, we propose a Cooperative Classification and Rationalization (C2R) method, consisting of the classification and the rationalization module. Specifically, we first assume that multiple environments are available in the classif
&lt;/p&gt;</description></item><item><title>PNCs&#23558;&#27010;&#29575;&#30005;&#36335;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#28145;&#23618;&#28151;&#21512;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#21516;&#26102;&#20316;&#20026;&#24378;&#22823;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.06235</link><description>&lt;p&gt;
&#27010;&#29575;&#31070;&#32463;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Neural Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06235
&lt;/p&gt;
&lt;p&gt;
PNCs&#23558;&#27010;&#29575;&#30005;&#36335;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#28145;&#23618;&#28151;&#21512;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#21516;&#26102;&#20316;&#20026;&#24378;&#22823;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#36817;&#24180;&#26469;&#20316;&#20026;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25506;&#35752;&#25903;&#25345;&#21487;&#22788;&#29702;&#26597;&#35810;&#19988;&#36275;&#22815;&#34920;&#36798;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21487;&#22788;&#29702;&#24615;&#26159;&#26377;&#20195;&#20215;&#30340;&#65306;PCs&#27604;&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#26356;&#24369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27010;&#29575;&#31070;&#32463;&#30005;&#36335;&#65288;PNCs&#65289;&#65292;&#22312;&#21487;&#22788;&#29702;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#22312;PCs&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PNCs&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#28145;&#24230;&#28151;&#21512;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PNCs&#26500;&#25104;&#20102;&#24378;&#22823;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06235v1 Announce Type: cross  Abstract: Probabilistic circuits (PCs) have gained prominence in recent years as a versatile framework for discussing probabilistic models that support tractable queries and are yet expressive enough to model complex probability distributions. Nevertheless, tractability comes at a cost: PCs are less expressive than neural networks. In this paper we introduce probabilistic neural circuits (PNCs), which strike a balance between PCs and neural nets in terms of tractability and expressive power. Theoretically, we show that PNCs can be interpreted as deep mixtures of Bayesian networks. Experimentally, we demonstrate that PNCs constitute powerful function approximators.
&lt;/p&gt;</description></item><item><title>LinearAPT&#31639;&#27861;&#26159;&#19968;&#31181;&#20026;&#22266;&#23450;&#39044;&#31639;&#35774;&#32622;&#30340;&#38408;&#20540;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#26032;&#31639;&#27861;&#65292;&#20855;&#26377;&#36866;&#24212;&#24615;&#12289;&#31616;&#21333;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#20248;&#21270;&#39034;&#24207;&#20915;&#31574;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.06230</link><description>&lt;p&gt;
LinearAPT: &#19968;&#31181;&#29992;&#20110;&#22266;&#23450;&#39044;&#31639;&#38408;&#20540;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
LinearAPT: An Adaptive Algorithm for the Fixed-Budget Thresholding Linear Bandit Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06230
&lt;/p&gt;
&lt;p&gt;
LinearAPT&#31639;&#27861;&#26159;&#19968;&#31181;&#20026;&#22266;&#23450;&#39044;&#31639;&#35774;&#32622;&#30340;&#38408;&#20540;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#26032;&#31639;&#27861;&#65292;&#20855;&#26377;&#36866;&#24212;&#24615;&#12289;&#31616;&#21333;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#20248;&#21270;&#39034;&#24207;&#20915;&#31574;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#38408;&#20540;&#32447;&#24615;&#36172;&#21338;&#26426;&#65288;TLB&#65289;&#38382;&#39064;&#65292;&#36825;&#26159;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#24494;&#22937;&#39046;&#22495;&#65292;&#37325;&#28857;&#26159;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#38024;&#23545;&#32447;&#24615;&#23450;&#20041;&#38408;&#20540;&#30340;&#20915;&#31574;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LinearAPT&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;TLB&#30340;&#22266;&#23450;&#39044;&#31639;&#35774;&#32622;&#32780;&#35774;&#35745;&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#20026;&#20248;&#21270;&#39034;&#24207;&#20915;&#31574;&#25552;&#20379;&#20102;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#31639;&#27861;&#19981;&#20165;&#20026;&#20272;&#35745;&#25439;&#22833;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30028;&#65292;&#32780;&#19988;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#31361;&#20986;&#20102;LinearAPT&#30340;&#36866;&#24212;&#24615;&#12289;&#31616;&#21333;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#20854;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#25361;&#25112;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06230v1 Announce Type: new  Abstract: In this study, we delve into the Thresholding Linear Bandit (TLB) problem, a nuanced domain within stochastic Multi-Armed Bandit (MAB) problems, focusing on maximizing decision accuracy against a linearly defined threshold under resource constraints. We present LinearAPT, a novel algorithm designed for the fixed budget setting of TLB, providing an efficient solution to optimize sequential decision-making. This algorithm not only offers a theoretical upper bound for estimated loss but also showcases robust performance on both synthetic and real-world datasets. Our contributions highlight the adaptability, simplicity, and computational efficiency of LinearAPT, making it a valuable addition to the toolkit for addressing complex sequential decision-making challenges.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;LLMTrack&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#32467;&#21512;&#35282;&#33394;&#25198;&#28436;&#21644;&#36880;&#27493;&#24605;&#32771;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;IMU&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#23556;&#36712;&#36857;&#35782;&#21035;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26080;&#38656;&#35757;&#32451;&#22312;&#19987;&#38376;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.06201</link><description>&lt;p&gt;
&#24744;&#34987;&#36861;&#36394;&#20102;&#21527;&#65311;&#21457;&#29616;LLMs&#30340;&#38646;&#23556;&#36712;&#36857;&#36319;&#36394;&#30340;&#23041;&#21147;&#65281;
&lt;/p&gt;
&lt;p&gt;
Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06201
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;LLMTrack&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#32467;&#21512;&#35282;&#33394;&#25198;&#28436;&#21644;&#36880;&#27493;&#24605;&#32771;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;IMU&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#23556;&#36712;&#36857;&#35782;&#21035;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26080;&#38656;&#35757;&#32451;&#22312;&#19987;&#38376;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#20316;&#20026;&#22522;&#26412;&#32452;&#20214;&#30340;&#35752;&#35770;&#20013;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#34701;&#20837;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#65288;AIoT&#65289;&#20013;&#20197;&#35299;&#37322;&#22797;&#26434;&#36712;&#36857;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;LLMTrack&#65292;&#35813;&#27169;&#22411;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#36827;&#34892;&#38646;&#23556;&#36712;&#36857;&#35782;&#21035;&#65292;&#36890;&#36807;&#37319;&#29992;&#23558;&#35282;&#33394;&#25198;&#28436;&#21644;&#36880;&#27493;&#24605;&#32771;&#26041;&#27861;&#19982;&#26410;&#32463;&#22788;&#29702;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#25968;&#25454;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#21333;&#25552;&#31034;&#25216;&#26415;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26088;&#22312;&#25361;&#25112;&#23427;&#20197;&#20855;&#26377;&#23460;&#20869;&#21644;&#23460;&#22806;&#24773;&#26223;&#29305;&#24449;&#30340;&#19981;&#21516;&#36712;&#36857;&#12290;&#22312;&#20004;&#31181;&#27979;&#35797;&#24773;&#26223;&#20013;&#65292;LLMTrack &#19981;&#20165;&#28385;&#36275;&#29978;&#33267;&#36229;&#36807;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#24403;&#20195;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35774;&#23450;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#32780;&#19988;&#26080;&#38656;&#23545;&#19987;&#38376;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06201v1 Announce Type: cross  Abstract: There is a burgeoning discussion around the capabilities of Large Language Models (LLMs) in acting as fundamental components that can be seamlessly incorporated into Artificial Intelligence of Things (AIoT) to interpret complex trajectories. This study introduces LLMTrack, a model that illustrates how LLMs can be leveraged for Zero-Shot Trajectory Recognition by employing a novel single-prompt technique that combines role-play and think step-by-step methodologies with unprocessed Inertial Measurement Unit (IMU) data. We evaluate the model using real-world datasets designed to challenge it with distinct trajectories characterized by indoor and outdoor scenarios. In both test scenarios, LLMTrack not only meets but exceeds the performance benchmarks set by traditional machine learning approaches and even contemporary state-of-the-art deep learning models, all without the requirement of training on specialized datasets. The results of our 
&lt;/p&gt;</description></item><item><title>DrFuse&#36890;&#36807;&#35299;&#32806;&#25968;&#25454;&#29305;&#24449;&#35299;&#20915;&#20102;&#20020;&#24202;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#32570;&#22833;&#27169;&#24577;&#21644;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06197</link><description>&lt;p&gt;
DrFuse&#65306;&#23398;&#20064;&#38754;&#21521;&#20020;&#24202;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#35299;&#32806;&#34920;&#31034;&#65292;&#35299;&#20915;&#32570;&#22833;&#27169;&#24577;&#21644;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06197
&lt;/p&gt;
&lt;p&gt;
DrFuse&#36890;&#36807;&#35299;&#32806;&#25968;&#25454;&#29305;&#24449;&#35299;&#20915;&#20102;&#20020;&#24202;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#32570;&#22833;&#27169;&#24577;&#21644;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#21644;&#21307;&#23398;&#22270;&#20687;&#30340;&#32452;&#21512;&#23545;&#20020;&#24202;&#21307;&#29983;&#22312;&#35786;&#26029;&#21644;&#39044;&#27979;&#39044;&#21518;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#25112;&#30053;&#24615;&#22320;&#34701;&#21512;&#36825;&#20004;&#31181;&#25968;&#25454;&#27169;&#24577;&#26377;&#24040;&#22823;&#28508;&#21147;&#25913;&#21892;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;EHR&#21644;&#21307;&#23398;&#22270;&#20687;&#30340;&#24322;&#27493;&#21644;&#20114;&#34917;&#24615;&#29305;&#36136;&#24102;&#26469;&#29420;&#29305;&#25361;&#25112;&#12290;&#30001;&#20110;&#20020;&#24202;&#21644;&#34892;&#25919;&#22240;&#32032;&#65292;&#32570;&#22833;&#27169;&#24577;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#32780;&#27599;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#21462;&#20915;&#20110;&#24739;&#32773;&#21644;&#39044;&#27979;&#30446;&#26631;&#65292;&#23548;&#33268;&#39044;&#27979;&#19981;&#19968;&#33268;&#21644;&#27169;&#22411;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;DrFuse&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20020;&#24202;&#22810;&#27169;&#24577;&#34701;&#21512;&#12290;&#23427;&#36890;&#36807;&#35299;&#32806;&#20849;&#20139;&#27169;&#24577;&#20869;&#21644;&#29420;&#29305;&#27169;&#24577;&#20869;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#32570;&#22833;&#27169;&#24577;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27169;&#24577;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06197v1 Announce Type: cross  Abstract: The combination of electronic health records (EHR) and medical images is crucial for clinicians in making diagnoses and forecasting prognosis. Strategically fusing these two data modalities has great potential to improve the accuracy of machine learning models in clinical prediction tasks. However, the asynchronous and complementary nature of EHR and medical images presents unique challenges. Missing modalities due to clinical and administrative factors are inevitable in practice, and the significance of each data modality varies depending on the patient and the prediction target, resulting in inconsistent predictions and suboptimal model performance. To address these challenges, we propose DrFuse to achieve effective clinical multi-modal fusion. It tackles the missing modality issue by disentangling the features shared across modalities and those unique within each modality. Furthermore, we address the modal inconsistency issue via a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20808;&#21069;&#25193;&#25955;&#25216;&#26415;&#23545;&#28385;&#36275;&#23545;&#25968;Sobolev&#19981;&#31561;&#24335;&#30340;&#30446;&#26631;&#20998;&#24067;&#30340;&#20316;&#29992;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#20165;&#38024;&#23545;&#24378;&#23545;&#25968;&#20985;&#20998;&#24067;&#30340;&#30456;&#20851;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.06183</link><description>&lt;p&gt;
&#20855;&#26377;&#20808;&#21069;&#25193;&#25955;&#30340;&#20848;&#22522;&#25991;&#31639;&#27861;&#22312;&#38750;&#23545;&#25968;&#20985;&#25277;&#26679;&#20013;&#30340;&#25913;&#36827;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Improved Analysis of Langevin Algorithms with Prior Diffusion for Non-Log-Concave Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20808;&#21069;&#25193;&#25955;&#25216;&#26415;&#23545;&#28385;&#36275;&#23545;&#25968;Sobolev&#19981;&#31561;&#24335;&#30340;&#30446;&#26631;&#20998;&#24067;&#30340;&#20316;&#29992;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#20165;&#38024;&#23545;&#24378;&#23545;&#25968;&#20985;&#20998;&#24067;&#30340;&#30456;&#20851;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25277;&#26679;&#38382;&#39064;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#32500;&#24230;&#30456;&#20851;&#24615;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#26080;&#35770;&#26159;&#20174;&#23454;&#38469;&#36824;&#26159;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#12290;&#30456;&#23545;&#20110;&#20855;&#26377;&#26080;&#20559;&#31283;&#24577;&#20998;&#24067;&#30340;&#25277;&#26679;&#22120;&#65292;&#22914;Metropolis-adjusted Langevin algorithm (MALA)&#65292;&#20855;&#26377;&#20559;&#32622;&#31283;&#24577;&#20998;&#24067;&#30340;&#25277;&#26679;&#22120;&#65292;&#22914;Underdamped Langevin Dynamics (ULD)&#65292;&#22312;&#20302;&#20934;&#30830;&#24230;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20165;&#20165;&#22240;&#20026;&#23427;&#20204;&#30340;&#22797;&#26434;&#24230;&#23545;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#26356;&#20302;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;Freund&#31561;&#20154;(2022)&#25552;&#20986;&#65292;&#20855;&#26377;&#20808;&#21069;&#25193;&#25955;&#30340;&#20462;&#25913;&#20848;&#22522;&#25991;&#31639;&#27861;&#33021;&#22815;&#32500;&#24230;&#29420;&#31435;&#22320;&#25910;&#25947;&#20110;&#24378;&#23545;&#25968;&#20985;&#30446;&#26631;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#26159;&#21542;&#23384;&#22312;&#36825;&#26679;&#30340;&#24615;&#36136;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#28385;&#36275;&#23545;&#25968;Sobolev&#19981;&#31561;&#24335;&#65288;LSI&#65289;&#30340;&#30446;&#26631;&#20998;&#24067;&#30340;&#20808;&#21069;&#25193;&#25955;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#35206;&#30422;&#20102;&#27604;&#24378;&#23545;&#25968;&#20985;&#20998;&#24067;&#26356;&#24191;&#27867;&#30340;&#20998;&#24067;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06183v1 Announce Type: new  Abstract: Understanding the dimension dependency of computational complexity in high-dimensional sampling problem is a fundamental problem, both from a practical and theoretical perspective. Compared with samplers with unbiased stationary distribution, e.g., Metropolis-adjusted Langevin algorithm (MALA), biased samplers, e.g., Underdamped Langevin Dynamics (ULD), perform better in low-accuracy cases just because a lower dimension dependency in their complexities. Along this line, Freund et al. (2022) suggest that the modified Langevin algorithm with prior diffusion is able to converge dimension independently for strongly log-concave target distributions. Nonetheless, it remains open whether such property establishes for more general cases. In this paper, we investigate the prior diffusion technique for the target distributions satisfying log-Sobolev inequality (LSI), which covers a much broader class of distributions compared to the strongly log-c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#20998;&#31867;&#20219;&#21153;&#30340;&#39046;&#22495;&#23545;&#25239;&#20027;&#21160;&#23398;&#20064;&#65288;DAAL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#26679;&#26412;&#30340;&#39046;&#22495;&#23545;&#25239;&#36873;&#25321;&#26041;&#27861;&#65292;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.06174</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#20998;&#31867;&#30340;&#39046;&#22495;&#23545;&#25239;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain Adversarial Active Learning for Domain Generalization Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#20998;&#31867;&#20219;&#21153;&#30340;&#39046;&#22495;&#23545;&#25239;&#20027;&#21160;&#23398;&#20064;&#65288;DAAL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#26679;&#26412;&#30340;&#39046;&#22495;&#23545;&#25239;&#36873;&#25321;&#26041;&#27861;&#65292;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#27169;&#22411;&#26088;&#22312;&#20174;&#28304;&#39046;&#22495;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#22312;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22810;&#26679;&#21270;&#21644;&#20016;&#23500;&#30340;&#28304;&#39046;&#22495;&#26679;&#26412;&#21487;&#20197;&#22686;&#24378;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#35748;&#20026;&#27599;&#20010;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#26159;&#19981;&#21516;&#30340;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#19968;&#23450;&#27700;&#24179;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#20998;&#31867;&#20219;&#21153;&#30340;&#39046;&#22495;&#23545;&#25239;&#20027;&#21160;&#23398;&#20064;&#65288;DAAL&#65289;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#21516;&#19968;&#39046;&#22495;&#20869;&#30340;&#31867;&#38388;&#36317;&#31163;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#26368;&#23567;&#21270;&#31867;&#20869;&#36317;&#31163;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#25361;&#25112;&#24615;&#26679;&#26412;&#30340;&#39046;&#22495;&#23545;&#25239;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06174v1 Announce Type: cross  Abstract: Domain generalization models aim to learn cross-domain knowledge from source domain data, to improve performance on unknown target domains. Recent research has demonstrated that diverse and rich source domain samples can enhance domain generalization capability. This paper argues that the impact of each sample on the model's generalization ability varies. Despite its small scale, a high-quality dataset can still attain a certain level of generalization ability. Motivated by this, we propose a domain-adversarial active learning (DAAL) algorithm for classification tasks in domain generalization. First, we analyze that the objective of tasks is to maximize the inter-class distance within the same domain and minimize the intra-class distance across different domains. To achieve this objective, we design a domain adversarial selection method that prioritizes challenging samples. Second, we posit that even in a converged model, there are sub
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#19982;&#20808;&#39564;&#30693;&#35782;&#32467;&#21512;&#65292;&#20197;&#21152;&#36895;&#22312;&#20223;&#30495;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#25235;&#21462;&#23039;&#21183;&#65292;&#23454;&#39564;&#35777;&#26126;QD&#30456;&#27604;&#20110;&#26631;&#20934;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#37319;&#26679;&#26041;&#26696;&#26377;&#30528;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.06173</link><description>&lt;p&gt;
&#29992;&#36136;&#37327;&#22810;&#26679;&#24615;&#21152;&#36895;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Speeding up 6-DoF Grasp Sampling with Quality-Diversity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06173
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#19982;&#20808;&#39564;&#30693;&#35782;&#32467;&#21512;&#65292;&#20197;&#21152;&#36895;&#22312;&#20223;&#30495;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#25235;&#21462;&#23039;&#21183;&#65292;&#23454;&#39564;&#35777;&#26126;QD&#30456;&#27604;&#20110;&#26631;&#20934;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#37319;&#26679;&#26041;&#26696;&#26377;&#30528;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#35268;&#21010;&#21644;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#39640;&#25928;&#20248;&#21270;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#20132;&#20114;&#25968;&#25454;&#20173;&#28982;&#26159;&#27867;&#21270;&#30340;&#29942;&#39048;&#12290;&#33719;&#21462;&#25235;&#21462;&#25968;&#25454;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#39033;&#25216;&#33021;&#23545;&#23436;&#25104;&#35768;&#22810;&#25805;&#32437;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#31639;&#27861;&#20248;&#21270;&#19968;&#32452;&#35299;&#20197;&#33719;&#24471;&#22312;&#32473;&#23450;&#38382;&#39064;&#19978;&#22810;&#26679;&#21270;&#12289;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;QD&#19982;&#20808;&#39564;&#30693;&#35782;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#21152;&#36895;&#20223;&#30495;&#20013;&#22810;&#26679;&#21270;&#25235;&#21462;&#23039;&#21183;&#30340;&#29983;&#25104;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#37319;&#26679;&#26041;&#26696;&#12290;&#22312;&#26631;&#20934;&#23545;&#35937;&#19978;&#23545;&#22235;&#20010;&#24102;&#26377;2&#33267;5&#20010;&#25163;&#25351;&#30340;&#22841;&#29226;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;QD&#30340;&#34920;&#29616;&#36828;&#36828;&#36229;&#36807;&#24120;&#29992;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;QD&#20248;&#21270;&#33258;&#21160;&#21457;&#29616;&#20102;&#19968;&#20123;&#36890;&#24120;&#38590;&#20197;&#32534;&#30721;&#30340;&#39640;&#25928;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06173v1 Announce Type: cross  Abstract: Recent advances in AI have led to significant results in robotic learning, including natural language-conditioned planning and efficient optimization of controllers using generative models. However, the interaction data remains the bottleneck for generalization. Getting data for grasping is a critical challenge, as this skill is required to complete many manipulation tasks. Quality-Diversity (QD) algorithms optimize a set of solutions to get diverse, high-performing solutions to a given problem. This paper investigates how QD can be combined with priors to speed up the generation of diverse grasps poses in simulation compared to standard 6-DoF grasp sampling schemes. Experiments conducted on 4 grippers with 2-to-5 fingers on standard objects show that QD outperforms commonly used methods by a large margin. Further experiments show that QD optimization automatically finds some efficient priors that are usually hard coded. The deployment
&lt;/p&gt;</description></item><item><title>ALL0CORE&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#23427;&#22312;&#20445;&#25345;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#30340;&#22522;&#30784;&#19978;&#21033;&#29992;Tucker&#20998;&#35299;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26680;&#30340;&#24494;&#23567;&#37096;&#20998;&#21363;&#36798;&#21040;&#19982;&#23436;&#25972;Tucker&#20998;&#35299;&#30456;&#21516;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06153</link><description>&lt;p&gt;
ALL0CORE&#24352;&#37327;&#20998;&#35299;&#29992;&#20110;&#31232;&#30095;&#35745;&#25968;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
The ALL0CORE Tensor Decomposition for Sparse Count Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06153
&lt;/p&gt;
&lt;p&gt;
ALL0CORE&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#23427;&#22312;&#20445;&#25345;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#30340;&#22522;&#30784;&#19978;&#21033;&#29992;Tucker&#20998;&#35299;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26680;&#30340;&#24494;&#23567;&#37096;&#20998;&#21363;&#36798;&#21040;&#19982;&#23436;&#25972;Tucker&#20998;&#35299;&#30456;&#21516;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ALL0CORE&#65292;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#24418;&#24335;&#12290;ALL0CORE&#26159;&#19968;&#31181;Tucker&#20998;&#35299;&#65292;&#20854;&#20013;&#26680;&#24352;&#37327;&#30340;&#38750;&#38646;&#20803;&#32032;&#25968;&#37327;&#65288;&#21363;L0&#33539;&#25968;&#65289;&#34987;&#38480;&#21046;&#20026;&#36828;&#23567;&#20110;&#26680;&#30340;&#22823;&#23567;&#30340;&#39044;&#35774;&#20540;Q&#12290;&#34429;&#28982;&#29992;&#25143;&#35268;&#23450;&#20102;&#24635;&#39044;&#31639;Q&#65292;&#20294;&#38750;&#38646;&#20803;&#32032;&#30340;&#20301;&#32622;&#21644;&#20540;&#26159;&#28508;&#22312;&#21464;&#37327;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20998;&#37197;&#32473;&#26680;&#24352;&#37327;&#30340;&#21508;&#20010;&#37096;&#20998;&#12290;ALL0CORE&#65292;&#21363;&#20998;&#37197;&#30340;L0&#32422;&#26463;&#26680;&#65292;&#22240;&#27492;&#26082;&#20855;&#26377;CP&#20998;&#35299;&#30340;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#65292;&#21448;&#20855;&#26377;Tucker&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ALL0CORE&#36890;&#24120;&#21482;&#38656;&#20351;&#29992;&#26680;&#30340;&#24494;&#23567;&#37096;&#20998;&#65288;&#20363;&#22914;&#65374;1%&#65289;&#21363;&#21487;&#20197;&#19982;&#23436;&#25972;Tucker&#20998;&#35299;&#30456;&#21516;&#30340;&#32467;&#26524;&#65292;&#32780;&#25104;&#26412;&#20165;&#30456;&#24212;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06153v1 Announce Type: cross  Abstract: This paper introduces ALL0CORE, a new form of probabilistic non-negative tensor decomposition. ALL0CORE is a Tucker decomposition where the number of non-zero elements (i.e., the L0-norm) of the core tensor is constrained to a preset value Q much smaller than the size of the core. While the user dictates the total budget Q, the locations and values of the non-zero elements are latent variables and allocated across the core tensor during inference. ALL0CORE -- i.e., allocated L0-constrained core -- thus enjoys both the computational tractability of CP decomposition and the qualitatively appealing latent structure of Tucker. In a suite of real-data experiments, we demonstrate that ALL0CORE typically requires only tiny fractions (e.g.,~1%) of the full core to achieve the same results as full Tucker decomposition at only a correspondingly tiny fraction of the cost.
&lt;/p&gt;</description></item><item><title>MACE&#36890;&#36807;&#25104;&#21151;&#23558;&#27010;&#24565;&#28040;&#38500;&#30340;&#33539;&#22260;&#25193;&#23637;&#21040;100&#20010;&#27010;&#24565;&#65292;&#24182;&#22312;&#27867;&#21270;&#24615;&#21644;&#29305;&#24322;&#24615;&#20043;&#38388;&#21462;&#24471;&#26377;&#25928;&#24179;&#34913;&#65292;&#20174;&#32780;&#38450;&#27490;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#25110;&#35823;&#23548;&#24615;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.06135</link><description>&lt;p&gt;
MACE&#65306;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#22823;&#35268;&#27169;&#27010;&#24565;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
MACE: Mass Concept Erasure in Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06135
&lt;/p&gt;
&lt;p&gt;
MACE&#36890;&#36807;&#25104;&#21151;&#23558;&#27010;&#24565;&#28040;&#38500;&#30340;&#33539;&#22260;&#25193;&#23637;&#21040;100&#20010;&#27010;&#24565;&#65292;&#24182;&#22312;&#27867;&#21270;&#24615;&#21644;&#29305;&#24322;&#24615;&#20043;&#38388;&#21462;&#24471;&#26377;&#25928;&#24179;&#34913;&#65292;&#20174;&#32780;&#38450;&#27490;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#25110;&#35823;&#23548;&#24615;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#25193;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20854;&#22312;&#21019;&#24314;&#26377;&#23475;&#25110;&#35823;&#23548;&#24615;&#20869;&#23481;&#26041;&#38754;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#27010;&#24565;&#28040;&#38500;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#38450;&#27490;&#27169;&#22411;&#22312;&#25552;&#31034;&#26102;&#29983;&#25104;&#20855;&#26377;&#19981;&#24076;&#26395;&#27010;&#24565;&#30340;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#36890;&#24120;&#21463;&#38480;&#20110;&#21516;&#26102;&#22788;&#29702;&#23569;&#20110;&#20116;&#20010;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#28040;&#38500;&#27010;&#24565;&#30340;&#21516;&#26102;&#38590;&#20197;&#25214;&#21040;&#28040;&#38500;&#27010;&#24565;&#21516;&#20041;&#35789;&#65288;&#27867;&#21270;&#24615;&#65289;&#21644;&#20445;&#25345;&#19981;&#30456;&#20851;&#27010;&#24565;&#65288;&#29305;&#24322;&#24615;&#65289;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;MACE&#36890;&#36807;&#25104;&#21151;&#23558;&#28040;&#38500;&#33539;&#22260;&#25193;&#23637;&#21040;100&#20010;&#27010;&#24565;&#65292;&#24182;&#22312;&#27867;&#21270;&#24615;&#21644;&#29305;&#24322;&#24615;&#20043;&#38388;&#21462;&#24471;&#26377;&#25928;&#24179;&#34913;&#32780;&#19981;&#21516;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#38381;&#29615;&#20132;&#21449;&#27880;&#24847;&#21147;&#32454;&#21270;&#20197;&#21450;LoRA&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#65292;&#20849;&#21516;&#28040;&#38500;&#20102;&#19981;&#33391;&#27010;&#24565;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;MACE&#36824;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06135v1 Announce Type: cross  Abstract: The rapid expansion of large-scale text-to-image diffusion models has raised growing concerns regarding their potential misuse in creating harmful or misleading content. In this paper, we introduce MACE, a finetuning framework for the task of mass concept erasure. This task aims to prevent models from generating images that embody unwanted concepts when prompted. Existing concept erasure methods are typically restricted to handling fewer than five concepts simultaneously and struggle to find a balance between erasing concept synonyms (generality) and maintaining unrelated concepts (specificity). In contrast, MACE differs by successfully scaling the erasure scope up to 100 concepts and by achieving an effective balance between generality and specificity. This is achieved by leveraging closed-form cross-attention refinement along with LoRA finetuning, collectively eliminating the information of undesirable concepts. Furthermore, MACE int
&lt;/p&gt;</description></item><item><title>&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#23545;&#37197;&#23545;&#32452;&#21512;&#21644;&#35780;&#20272;&#37327;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#30340;&#35774;&#35745;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.06100</link><description>&lt;p&gt;
&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#36827;&#34892;&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic design optimization of preference-based subjective evaluation with online learning in crowdsourcing environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06100
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#23545;&#37197;&#23545;&#32452;&#21512;&#21644;&#35780;&#20272;&#37327;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#30340;&#35774;&#35745;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#26159;&#35780;&#20215;&#29983;&#25104;&#24335;&#23186;&#20307;&#21487;&#38752;&#24615;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20854;&#24222;&#22823;&#30340;&#37197;&#23545;&#32452;&#21512;&#20351;&#24471;&#23427;&#26080;&#27861;&#24212;&#29992;&#20110;&#21033;&#29992;&#20247;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#36827;&#34892;&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#30340;&#33258;&#21160;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#23545;&#37197;&#23545;&#32452;&#21512;&#36873;&#25321;&#21644;&#35780;&#20272;&#37327;&#20998;&#37197;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25490;&#24207;&#31639;&#27861;&#30340;&#22522;&#20110;&#21916;&#22909;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#26469;&#35782;&#21035;&#20855;&#26377;&#26368;&#23567;&#26679;&#26412;&#37327;&#30340;&#35780;&#20272;&#30446;&#26631;&#30340;&#23436;&#20840;&#39034;&#24207;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#25903;&#25345;&#22312;&#20247;&#21253;&#25152;&#38656;&#30340;&#22266;&#23450;&#39044;&#31639;&#26465;&#20214;&#19979;&#30340;&#24182;&#34892;&#21644;&#24322;&#27493;&#25191;&#34892;&#12290;&#25105;&#20204;&#23545;&#21512;&#25104;&#35821;&#38899;&#30340;&#22522;&#20110;&#21916;&#22909;&#30340;&#20027;&#35266;&#35780;&#20272;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#36890;&#36807;&#23558;&#37197;&#23545;&#32452;&#21512;&#20174;351&#20943;&#23569;&#21040;83&#24182;&#20998;&#37197;&#26368;&#20248;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06100v1 Announce Type: cross  Abstract: A preference-based subjective evaluation is a key method for evaluating generative media reliably. However, its huge combinations of pairs prohibit it from being applied to large-scale evaluation using crowdsourcing. To address this issue, we propose an automatic optimization method for preference-based subjective evaluation in terms of pair combination selections and allocation of evaluation volumes with online learning in a crowdsourcing environment. We use a preference-based online learning method based on a sorting algorithm to identify the total order of evaluation targets with minimum sample volumes. Our online learning algorithm supports parallel and asynchronous execution under fixed-budget conditions required for crowdsourcing. Our experiment on preference-based subjective evaluation of synthetic speech shows that our method successfully optimizes the test by reducing pair combinations from 351 to 83 and allocating optimal eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#21644;&#19981;&#21516;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#36710;&#36733;&#22810;&#20219;&#21153;&#38754;&#37096;&#23646;&#24615;&#35782;&#21035;&#20013;&#30340;&#25928;&#26524;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.06088</link><description>&lt;p&gt;
&#38754;&#21521;&#36710;&#36733;&#22810;&#20219;&#21153;&#38754;&#37096;&#23646;&#24615;&#35782;&#21035;&#65306;&#30740;&#31350;&#21512;&#25104;&#25968;&#25454;&#21644;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#21644;&#19981;&#21516;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#36710;&#36733;&#22810;&#20219;&#21153;&#38754;&#37096;&#23646;&#24615;&#35782;&#21035;&#20013;&#30340;&#25928;&#26524;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#38754;&#37096;&#23646;&#24615;&#35782;&#21035;&#65288;&#22914;&#38754;&#37096;&#34920;&#24773;&#12289;&#30524;&#31070;&#12289;&#24180;&#40836;&#31561;&#65289;&#22686;&#24378;&#36710;&#36742;&#39550;&#39542;&#21592;&#30340;&#20132;&#20114;&#23545;&#20110;&#23433;&#20840;&#12289;&#20010;&#24615;&#21270;&#21644;&#25972;&#20307;&#29992;&#25143;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20840;&#38754;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#23545;&#35757;&#32451;&#31283;&#20581;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#25991;&#29486;&#36890;&#24120;&#24573;&#35270;&#20102;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#28508;&#21147;&#20197;&#21450;&#22312;&#36825;&#31181;&#21463;&#38480;&#29615;&#22659;&#20013;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#27604;&#36739;&#21151;&#25928;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#26469;&#35757;&#32451;&#35782;&#21035;&#36710;&#36742;&#20056;&#23458;&#30340;&#38754;&#37096;&#23646;&#24615;&#65288;&#22914;&#30524;&#31070;&#26041;&#21521;&#12289;&#24180;&#40836;&#21644;&#38754;&#37096;&#34920;&#24773;&#65289;&#30340;&#22797;&#26434;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Vision Transformer&#65288;ViT&#65289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65288;ResNet&#65289;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#26469;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06088v1 Announce Type: cross  Abstract: In the burgeoning field of intelligent transportation systems, enhancing vehicle-driver interaction through facial attribute recognition, such as facial expression, eye gaze, age, etc., is of paramount importance for safety, personalization, and overall user experience. However, the scarcity of comprehensive large-scale, real-world datasets poses a significant challenge for training robust multi-task models. Existing literature often overlooks the potential of synthetic datasets and the comparative efficacy of state-of-the-art vision foundation models in such constrained settings. This paper addresses these gaps by investigating the utility of synthetic datasets for training complex multi-task models that recognize facial attributes of passengers of a vehicle, such as gaze plane, age, and facial expression. Utilizing transfer learning techniques with both pre-trained Vision Transformer (ViT) and Residual Network (ResNet) models, we exp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#19981;&#21487;&#36870;&#36827;&#23637;&#36712;&#36857;</title><link>https://arxiv.org/abs/2403.06087</link><description>&lt;p&gt;
&#23398;&#20064;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#19981;&#21487;&#36870;&#36827;&#23637;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Learning the irreversible progression trajectory of Alzheimer's disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06087
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#19981;&#21487;&#36870;&#36827;&#23637;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#19968;&#31181;&#38543;&#30528;30&#24180;&#26102;&#38388;&#36880;&#28176;&#23637;&#24320;&#30340;&#36827;&#34892;&#24615;&#19981;&#21487;&#36870;&#33041;&#37096;&#30142;&#30149;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#26159;&#22312;&#26089;&#26399;&#25429;&#33719;&#30142;&#30149;&#30340;&#36827;&#23637;&#65292;&#20197;&#20415;&#22312;&#30151;&#29366;&#20986;&#29616;&#20043;&#21069;&#21487;&#20197;&#26045;&#21152;&#24178;&#39044;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#39044;&#27979;AD&#30340;&#21457;&#20316;&#26041;&#38754;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26377;&#38543;&#35775;&#30340;&#21463;&#35797;&#32773;&#65292;&#29616;&#26377;&#30340;AD&#20998;&#31867;&#25216;&#26415;&#21482;&#38024;&#23545;&#20934;&#30830;&#30340;&#32452;&#20998;&#37197;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#22312;&#38543;&#35775;&#36807;&#31243;&#20013;&#36882;&#22686;&#39118;&#38505;&#30340;&#21333;&#35843;&#22686;&#21152;&#12290;&#22312;&#38543;&#35775;&#38388;&#20986;&#29616;&#30340;&#27874;&#21160;&#39118;&#38505;&#35780;&#20998;&#36829;&#32972;&#20102;AD&#30340;&#19981;&#21487;&#36870;&#24615;&#65292;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#20063;&#23545;&#29702;&#35299;&#30142;&#30149;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#20215;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#39044;&#27979;AD&#30340;&#32437;&#21521;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26088;&#22312;&#22312;&#30149;&#24773;&#36827;&#23637;&#26399;&#38388;&#20445;&#25345;&#39044;&#26399;&#30340;&#21333;&#35843;&#22686;&#21152;&#30142;&#30149;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06087v1 Announce Type: new  Abstract: Alzheimer's disease (AD) is a progressive and irreversible brain disorder that unfolds over the course of 30 years. Therefore, it is critical to capture the disease progression in an early stage such that intervention can be applied before the onset of symptoms. Machine learning (ML) models have been shown effective in predicting the onset of AD. Yet for subjects with follow-up visits, existing techniques for AD classification only aim for accurate group assignment, where the monotonically increasing risk across follow-up visits is usually ignored. Resulted fluctuating risk scores across visits violate the irreversibility of AD, hampering the trustworthiness of models and also providing little value to understanding the disease progression. To address this issue, we propose a novel regularization approach to predict AD longitudinally. Our technique aims to maintain the expected monotonicity of increasing disease risk during progression w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#34701;&#21512;&#26694;&#26550;&#23558;Transformer&#27169;&#22411;&#37327;&#21270;&#20026;&#20165;&#20004;&#20301;&#65292;&#20165;&#26377;&#36731;&#24494;&#31934;&#24230;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.06082</link><description>&lt;p&gt;
FrameQuant: Transformer&#30340;&#28789;&#27963;&#20302;&#27604;&#29305;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FrameQuant: Flexible Low-Bit Quantization for Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06082
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#34701;&#21512;&#26694;&#26550;&#23558;Transformer&#27169;&#22411;&#37327;&#21270;&#20026;&#20165;&#20004;&#20301;&#65292;&#20165;&#26377;&#36731;&#24494;&#31934;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#35768;&#22810;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#24378;&#22823;&#22522;&#30784;&#27169;&#22411;&#30340;&#25903;&#26609;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;/&#23384;&#20648;&#31354;&#38388;&#21344;&#29992;&#36739;&#22823;&#65292;&#22240;&#27492;&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#24448;&#24448;&#38656;&#35201;&#26114;&#36149;&#30340;&#39640;&#31471;&#30828;&#20214;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#22256;&#38590;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#35797;&#22270;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#20854;&#37327;&#21270;&#20026;&#20843;&#20301;&#25110;&#26356;&#20302;&#30340;&#20301;&#25968;&#65292;&#26174;&#30528;&#25552;&#39640;&#35745;&#31639;/&#20869;&#23384;/&#24310;&#36831;&#25928;&#29575;&#12290;&#26082;&#21487;&#20197;&#25104;&#21151;&#23558;&#36825;&#20123;&#27169;&#22411;&#37327;&#21270;&#20026;&#22235;&#20301;&#65292;&#20294;&#24615;&#33021;&#26377;&#25152;&#25439;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#26696;&#65292;&#23558;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#37327;&#21270;&#20026;&#20165;&#20004;&#20301;&#65288;&#21152;&#19968;&#20123;&#39069;&#22806;&#24320;&#38144;&#65289;&#65292;&#20165;&#20250;&#26377;&#36731;&#24494;&#30340;&#31934;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#21046;&#23450;&#20851;&#38190;&#22312;&#20110;&#20174;&#35856;&#27874;&#20998;&#26512;&#20013;&#20511;&#37492;&#20102;&#19968;&#31181;&#31216;&#20026;&#34701;&#21512;&#26694;&#26550;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#37327;&#21270;&#19981;&#24212;&#35813;&#22312;&#21407;&#22987;&#26435;&#37325;&#31354;&#38388;&#20013;&#36827;&#34892;&#65292;&#32780;&#26159;&#24212;&#35813;&#22312;&#34701;&#21512;&#26694;&#26550;&#34920;&#31034;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06082v1 Announce Type: cross  Abstract: Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26412;&#22320;&#39030;&#28857;&#30528;&#33394;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#32463;&#20856;&#25628;&#32034;&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#36229;&#36234;1-WL&#30340;&#22270;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#36825;&#31181;&#30528;&#33394;&#26041;&#26696;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22270;&#21452;&#36830;&#36890;&#24615;&#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#34920;&#26126;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#38543;&#30528;&#25628;&#32034;&#37051;&#22495;&#21322;&#24452;&#30340;&#22686;&#21152;&#32780;&#23618;&#32423;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2403.06080</link><description>&lt;p&gt;
&#22522;&#20110;&#26412;&#22320;&#39030;&#28857;&#30528;&#33394;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Local Vertex Colouring Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06080
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26412;&#22320;&#39030;&#28857;&#30528;&#33394;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#32463;&#20856;&#25628;&#32034;&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#36229;&#36234;1-WL&#30340;&#22270;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#36825;&#31181;&#30528;&#33394;&#26041;&#26696;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22270;&#21452;&#36830;&#36890;&#24615;&#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#34920;&#26126;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#38543;&#30528;&#25628;&#32034;&#37051;&#22495;&#21322;&#24452;&#30340;&#22686;&#21152;&#32780;&#23618;&#32423;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36229;&#36234;Weisfeiler-Lehman&#65288;1-WL&#65289;&#26694;&#26550;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#22312;&#25552;&#39640;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24448;&#24448;&#20197;&#25928;&#29575;&#38477;&#20302;&#20026;&#20195;&#20215;&#65292;&#25110;&#32773;&#38480;&#21046;&#22312;&#29305;&#23450;&#31867;&#22411;&#30340;&#22270;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#22270;&#25628;&#32034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39030;&#28857;&#30528;&#33394;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#32463;&#20856;&#25628;&#32034;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#36229;&#36234;1-WL&#30340;&#22270;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30528;&#33394;&#26041;&#26696;&#20174;&#22270;&#25628;&#32034;&#20013;&#32487;&#25215;&#20102;&#26377;&#29992;&#30340;&#23646;&#24615;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#35832;&#22914;&#22270;&#30340;&#21452;&#36830;&#36890;&#24615;&#31561;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#38543;&#30528;&#25628;&#32034;&#37051;&#22495;&#21322;&#24452;&#30340;&#22686;&#21152;&#32780;&#23618;&#32423;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06080v1 Announce Type: new  Abstract: In recent years, there has been a significant amount of research focused on expanding the expressivity of Graph Neural Networks (GNNs) beyond the Weisfeiler-Lehman (1-WL) framework. While many of these studies have yielded advancements in expressivity, they have frequently come at the expense of decreased efficiency or have been restricted to specific types of graphs. In this study, we investigate the expressivity of GNNs from the perspective of graph search. Specifically, we propose a new vertex colouring scheme and demonstrate that classical search algorithms can efficiently compute graph representations that extend beyond the 1-WL. We show the colouring scheme inherits useful properties from graph search that can help solve problems like graph biconnectivity. Furthermore, we show that under certain conditions, the expressivity of GNNs increases hierarchically with the radius of the search neighbourhood. To further investigate the prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22270;&#21516;&#24577;&#30340;&#29109;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#35270;&#35282;&#65292;&#25512;&#23548;&#20986;&#20102;&#36866;&#29992;&#20110;&#22270;&#21644;&#33410;&#28857;&#20998;&#31867;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#22270;&#32467;&#26500;&#30340;&#32454;&#24494;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#26694;&#26550;&#21051;&#30011;&#20102;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.06079</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#24577;&#30340;&#35282;&#24230;&#25512;&#24191;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalization of Graph Neural Networks through the Lens of Homomorphism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22270;&#21516;&#24577;&#30340;&#29109;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#35270;&#35282;&#65292;&#25512;&#23548;&#20986;&#20102;&#36866;&#29992;&#20110;&#22270;&#21644;&#33410;&#28857;&#20998;&#31867;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#22270;&#32467;&#26500;&#30340;&#32454;&#24494;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#26694;&#26550;&#21051;&#30011;&#20102;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24191;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#27867;&#21270;&#33021;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282; - &#20998;&#26512;&#22270;&#21516;&#24577;&#30340;&#29109;&#26469;&#30740;&#31350;GNNs&#30340;&#27867;&#21270;&#12290;&#36890;&#36807;&#23558;&#22270;&#21516;&#24577;&#19982;&#20449;&#24687;&#35770;&#24230;&#37327;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#38024;&#23545;&#22270;&#21644;&#33410;&#28857;&#20998;&#31867;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36825;&#20123;&#30028;&#38480;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#22270;&#32467;&#26500;&#22266;&#26377;&#30340;&#32454;&#24494;&#24046;&#24322;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#36335;&#24452;&#12289;&#29615;&#21644;&#22242;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#20855;&#26377;&#31283;&#20581;&#29702;&#35770;&#20445;&#35777;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;&#27867;&#21270;&#20998;&#26512;&#12290;&#20026;&#20102;&#38416;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#30028;&#38480;&#30340;&#26222;&#36866;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#21516;&#24577;&#30340;&#35270;&#35282;&#21051;&#30011;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#26174;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#19982;&#23454;&#38469;&#30340;&#19968;&#33268;&#24615;&#26469;&#39564;&#35777;&#25105;&#20204;&#29702;&#35770;&#21457;&#29616;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06079v1 Announce Type: new  Abstract: Despite the celebrated popularity of Graph Neural Networks (GNNs) across numerous applications, the ability of GNNs to generalize remains less explored. In this work, we propose to study the generalization of GNNs through a novel perspective - analyzing the entropy of graph homomorphism. By linking graph homomorphism with information-theoretic measures, we derive generalization bounds for both graph and node classifications. These bounds are capable of capturing subtleties inherent in various graph structures, including but not limited to paths, cycles and cliques. This enables a data-dependent generalization analysis with robust theoretical guarantees. To shed light on the generality of of our proposed bounds, we present a unifying framework that can characterize a broad spectrum of GNN models through the lens of graph homomorphism. We validate the practical applicability of our theoretical findings by showing the alignment between the 
&lt;/p&gt;</description></item><item><title>I3SB&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#32467;&#21512;&#25439;&#22351;&#30340;&#22270;&#20687;&#25913;&#21892;&#32441;&#29702;&#24674;&#22797;&#65292;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.06069</link><description>&lt;p&gt;
&#38544;&#24335;&#22270;&#20687;&#23545;&#22270;&#20687;Schrodinger&#26725;&#29992;&#20110;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06069
&lt;/p&gt;
&lt;p&gt;
I3SB&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#32467;&#21512;&#25439;&#22351;&#30340;&#22270;&#20687;&#25913;&#21892;&#32441;&#29702;&#24674;&#22797;&#65292;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#32780;&#24471;&#21040;&#35748;&#21487;&#65292;&#28982;&#32780;&#65292;&#20854;&#20174;&#39640;&#26031;&#22122;&#22768;&#24320;&#22987;&#30340;&#36845;&#20195;&#21435;&#22122;&#36807;&#31243;&#24448;&#24448;&#23548;&#33268;&#25512;&#26029;&#36895;&#24230;&#24930;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22270;&#20687;&#23545;&#22270;&#20687;Schr&#246;dinger&#26725;&#65288;I2SB&#65289;&#20174;&#25439;&#22351;&#30340;&#22270;&#20687;&#24320;&#22987;&#21021;&#22987;&#21270;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#38598;&#25104;&#20102;&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25216;&#26415;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#22270;&#20687;&#23545;&#22270;&#20687;Schr&#246;dinger&#26725;&#65288;I3SB&#65289;&#25193;&#23637;&#20102;I2SB&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#29983;&#25104;&#27493;&#39588;&#20013;&#32435;&#20837;&#25439;&#22351;&#30340;&#22270;&#20687;&#65292;&#23558;&#20854;&#29983;&#25104;&#36807;&#31243;&#36716;&#25442;&#20026;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#36825;&#31181;&#22686;&#24378;&#20351;&#24471;I3SB&#33021;&#22815;&#22312;&#23569;&#37327;&#29983;&#25104;&#27493;&#39588;&#20013;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#32441;&#29702;&#24674;&#22797;&#30340;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#24182;&#36229;&#36234;&#20102;&#21253;&#25324;&#26377;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#20869;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06069v1 Announce Type: cross  Abstract: Conditional diffusion models have gained recognition for their effectiveness in image restoration tasks, yet their iterative denoising process, starting from Gaussian noise, often leads to slow inference speeds. As a promising alternative, the Image-to-Image Schr\"odinger Bridge (I2SB) initializes the generative process from corrupted images and integrates training techniques from conditional diffusion models. In this study, we extended the I2SB method by introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB), transitioning its generative process to a non-Markovian process by incorporating corrupted images in each generative step. This enhancement empowers I3SB to generate images with better texture restoration using a small number of generative steps. The proposed method was validated on CT super-resolution and denoising tasks and outperformed existing methods, including the conditional denoising diffusion probabilistic mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22240;&#26524;&#25512;&#26029;&#27169;&#22359;&#21644;&#22810;&#26679;&#21270;&#32858;&#21512;&#21367;&#31215;&#25216;&#26415;&#30340;CausalCellSegmenter&#26694;&#26550;&#65292;&#36890;&#36807;DAC&#27169;&#22359;&#21644;SimAM&#20851;&#27880;&#27169;&#22359;&#35299;&#20915;&#20102;&#32454;&#32990;&#26680;&#20998;&#21106;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35823;&#25253;&#35782;&#21035;&#21644;&#36793;&#32536;&#27169;&#31946;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06066</link><description>&lt;p&gt;
CausalCellSegmenter&#65306;&#22240;&#26524;&#25512;&#26029;&#21551;&#21457;&#30340;&#22810;&#26679;&#21270;&#32858;&#21512;&#21367;&#31215;&#29992;&#20110;&#30149;&#29702;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
CausalCellSegmenter: Causal Inference inspired Diversified Aggregation Convolution for Pathology Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06066
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22240;&#26524;&#25512;&#26029;&#27169;&#22359;&#21644;&#22810;&#26679;&#21270;&#32858;&#21512;&#21367;&#31215;&#25216;&#26415;&#30340;CausalCellSegmenter&#26694;&#26550;&#65292;&#36890;&#36807;DAC&#27169;&#22359;&#21644;SimAM&#20851;&#27880;&#27169;&#22359;&#35299;&#20915;&#20102;&#32454;&#32990;&#26680;&#20998;&#21106;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35823;&#25253;&#35782;&#21035;&#21644;&#36793;&#32536;&#27169;&#31946;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30149;&#29702;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#32454;&#32990;&#26680;&#20998;&#21106;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#24378;&#22823;&#27169;&#22411;&#20173;&#28982;&#26159;&#32454;&#32990;&#26680;&#20998;&#21106;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#32972;&#26223;&#22122;&#22768;&#12289;&#32454;&#32990;&#26680;&#20043;&#38388;&#39640;&#24230;&#37325;&#21472;&#20197;&#21450;&#27169;&#31946;&#36793;&#32536;&#30340;&#32570;&#28857;&#32463;&#24120;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CausalCellSegmenter&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23427;&#23558;&#22240;&#26524;&#25512;&#26029;&#27169;&#22359;&#65288;CIM&#65289;&#19982;&#22810;&#26679;&#21270;&#32858;&#21512;&#21367;&#31215;&#65288;DAC&#65289;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;DAC&#27169;&#22359;&#34987;&#35774;&#35745;&#20026;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#12289;&#26080;&#21442;&#25968;&#30340;&#20851;&#27880;&#27169;&#22359;&#65288;SimAM&#65289;&#23558;&#19981;&#21516;&#30340;&#19979;&#37319;&#26679;&#29305;&#24449;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26088;&#22312;&#35299;&#20915;&#35823;&#25253;&#35782;&#21035;&#21644;&#36793;&#32536;&#27169;&#31946;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;CIM&#26469;&#21033;&#29992;&#26679;&#26412;&#21152;&#26435;&#65292;&#36890;&#36807;&#30452;&#25509;&#28040;&#38500;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#26469;&#32858;&#28966;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06066v1 Announce Type: cross  Abstract: Deep learning models have shown promising performance for cell nucleus segmentation in the field of pathology image analysis. However, training a robust model from multiple domains remains a great challenge for cell nucleus segmentation. Additionally, the shortcomings of background noise, highly overlapping between cell nucleus, and blurred edges often lead to poor performance. To address these challenges, we propose a novel framework termed CausalCellSegmenter, which combines Causal Inference Module (CIM) with Diversified Aggregation Convolution (DAC) techniques. The DAC module is designed which incorporates diverse downsampling features through a simple, parameter-free attention module (SimAM), aiming to overcome the problems of false-positive identification and edge blurring. Furthermore, we introduce CIM to leverage sample weighting by directly removing the spurious correlations between features for every input sample and concentra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06064</link><description>&lt;p&gt;
L$^2$GC: &#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#29992;&#20110;&#23545;&#22270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32447;&#24615;GCN&#27169;&#22411;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#25191;&#34892;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#65292;&#36825;&#24182;&#27809;&#26377;&#26126;&#30830;&#25429;&#25417;&#21040;&#20316;&#20026;&#22270;&#27169;&#22411;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#31867;&#20284;&#26641;&#29366;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#25991;&#23581;&#35797;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;GCN&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22270;&#33410;&#28857;&#30340;&#23398;&#20064;&#29305;&#24449;&#26144;&#23556;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27931;&#20262;&#20857;&#32447;&#24615;&#29305;&#24449;&#21464;&#25442;&#65292;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#28508;&#22312;&#26641;&#29366;&#32467;&#26500;&#12290;&#22312;&#26631;&#20934;&#24341;&#25991;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Citeseer&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;74.7%&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;PubMed&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;81.3%&#30340;&#20934;&#30830;&#24230;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#33267;&#23569;&#36798;&#21040;2&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06064v1 Announce Type: cross  Abstract: Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and 81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;&#38463;&#25289;&#20271;&#35821;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#38598;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21333;&#35821;&#27169;&#22411;&#34920;&#29616;&#20248;&#36234;&#65292;&#38598;&#25104;&#27169;&#22411;&#32988;&#36807;&#22522;&#32447;&#65292;&#32780;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#20248;&#20110;&#33521;&#35821;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2403.06060</link><description>&lt;p&gt;
&#38598;&#25104;&#24335;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Ensemble Language Models for Multilingual Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;&#38463;&#25289;&#20271;&#35821;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#38598;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21333;&#35821;&#27169;&#22411;&#34920;&#29616;&#20248;&#36234;&#65292;&#38598;&#25104;&#27169;&#22411;&#32988;&#36807;&#22522;&#32447;&#65292;&#32780;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#20248;&#20110;&#33521;&#35821;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#29992;&#25143;&#30340;&#35266;&#28857;&#12290;&#26368;&#36817;&#65292;&#24773;&#24863;&#20998;&#26512;&#22312;&#29702;&#35299;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#20998;&#20139;&#20869;&#23481;&#30340;&#20154;&#31867;&#24773;&#24863;&#26041;&#38754;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#23613;&#31649;&#24120;&#29992;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#24050;&#26377;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20687;&#38463;&#25289;&#20271;&#35821;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#20173;&#28982;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26469;&#33258;SemEval-17&#21644;&#38463;&#25289;&#20271;&#24773;&#24863;&#25512;&#25991;&#25968;&#25454;&#38598;&#30340;&#25512;&#25991;&#25991;&#26412;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22235;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#38598;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#21253;&#25324;&#21333;&#35821;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#38598;&#25104;&#27169;&#22411;&#32988;&#36807;&#22522;&#32447;&#65292;&#32780;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#32988;&#36807;&#33521;&#35821;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06060v1 Announce Type: new  Abstract: The rapid advancement of social media enables us to analyze user opinions. In recent times, sentiment analysis has shown a prominent research gap in understanding human sentiment based on the content shared on social media. Although sentiment analysis for commonly spoken languages has advanced significantly, low-resource languages like Arabic continue to get little research due to resource limitations. In this study, we explore sentiment analysis on tweet texts from SemEval-17 and the Arabic Sentiment Tweet dataset. Moreover, We investigated four pretrained language models and proposed two ensemble language models. Our findings include monolingual models exhibiting superior performance and ensemble models outperforming the baseline while the majority voting ensemble outperforms the English language.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#36828;&#31163;&#30495;&#23454;&#35299;&#30340;&#20851;&#38190;&#28857;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#20005;&#26684;&#38797;&#28857;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#20171;&#32461;&#20102;&#39640;&#38454;&#25439;&#22833;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#20854;&#32435;&#20837;&#30446;&#26631;&#20989;&#25968;&#22686;&#21152;&#36127;&#26354;&#29575;&#65292;&#20174;&#32780;&#21152;&#36895;&#25670;&#33073;&#36825;&#20123;&#20020;&#30028;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.06056</link><description>&lt;p&gt;
&#36828;&#31163;&#30495;&#23454;&#35299;&#30340;&#34394;&#20551;&#35299;&#26512;&#26080;&#65306;&#20855;&#26377;&#39640;&#38454;&#25439;&#22833;&#30340;&#20302;&#31209;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Absence of spurious solutions far from ground truth: A low-rank analysis with high-order losses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#36828;&#31163;&#30495;&#23454;&#35299;&#30340;&#20851;&#38190;&#28857;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#20005;&#26684;&#38797;&#28857;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#20171;&#32461;&#20102;&#39640;&#38454;&#25439;&#22833;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#20854;&#32435;&#20837;&#30446;&#26631;&#20989;&#25968;&#22686;&#21152;&#36127;&#26354;&#29575;&#65292;&#20174;&#32780;&#21152;&#36895;&#25670;&#33073;&#36825;&#20123;&#20020;&#30028;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#23637;&#29616;&#20986;&#26222;&#36941;&#30340;&#38750;&#20984;&#24615;&#65292;&#22256;&#25200;&#30528;&#20248;&#21270;&#36807;&#31243;&#20013;&#22823;&#37327;&#27425;&#20248;&#34394;&#20551;&#35299;&#30340;&#34067;&#24310;&#12290;&#36991;&#20813;&#25910;&#25947;&#21040;&#36825;&#20123;&#20020;&#30028;&#28857;&#26500;&#25104;&#20102;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#25581;&#31034;&#38750;&#20984;&#26223;&#35266;&#22797;&#26434;&#24615;&#30340;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#36828;&#31163;&#30495;&#23454;&#30697;&#38453;&#30340;&#20020;&#30028;&#28857;&#21576;&#29616;&#20986;&#26377;&#21033;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#21363;&#20005;&#26684;&#38797;&#28857;&#65292;&#32780;&#38750;&#20196;&#20154;&#22836;&#30140;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#30340;&#39640;&#38454;&#25439;&#22833;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#23558;&#36825;&#20123;&#25439;&#22833;&#32435;&#20837;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#22686;&#22823;&#22260;&#32469;&#36825;&#20123;&#36828;&#31163;&#20020;&#30028;&#28857;&#30340;&#36127;&#26354;&#29575;&#12290;&#36825;&#24847;&#21619;&#30528;&#36890;&#36807;&#39640;&#38454;&#25439;&#22833;&#22686;&#21152;&#30446;&#26631;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#21152;&#36895;&#25670;&#33073;&#36825;&#20123;&#20020;&#30028;&#28857;&#65292;&#24182;&#19988;&#20316;&#20026;&#19968;&#31181;&#29702;&#24819;&#30340;&#26367;&#20195;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06056v1 Announce Type: cross  Abstract: Matrix sensing problems exhibit pervasive non-convexity, plaguing optimization with a proliferation of suboptimal spurious solutions. Avoiding convergence to these critical points poses a major challenge. This work provides new theoretical insights that help demystify the intricacies of the non-convex landscape. In this work, we prove that under certain conditions, critical points sufficiently distant from the ground truth matrix exhibit favorable geometry by being strict saddle points rather than troublesome local minima. Moreover, we introduce the notion of higher-order losses for the matrix sensing problem and show that the incorporation of such losses into the objective function amplifies the negative curvature around those distant critical points. This implies that increasing the complexity of the objective function via high-order losses accelerates the escape from such critical points and acts as a desirable alternative to increa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.06054</link><description>&lt;p&gt;
&#20855;&#26377;&#25193;&#25955;&#20928;&#21270;&#30340;&#20998;&#31163;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Decoupled Data Consistency with Diffusion Purification for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06054
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#31867;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#22320;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#65292;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#36890;&#36807;&#23558;&#39069;&#22806;&#30340;&#20284;&#28982;&#26799;&#24230;&#27493;&#39588;&#32435;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#26799;&#24230;&#27493;&#39588;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#24403;&#20351;&#29992;&#21152;&#36895;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#22120;&#26102;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#27493;&#39588;&#36824;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#30340;&#25968;&#37327;&#21463;&#38480;&#20110;&#21453;&#21521;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#20998;&#31163;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26032;&#30340;&#22522;&#20110;RCT-Plus&#21464;&#25442;&#30340;&#22270;&#20687;&#34920;&#31034;&#21644;&#23398;&#20064;&#26041;&#27861;&#25913;&#36827;&#20102;&#32441;&#29702;&#22270;&#20687;&#26816;&#32034;&#65292;&#23454;&#29616;&#20102;&#27604;&#20197;&#21069;&#26041;&#26696;&#26356;&#39640;&#30340;&#26816;&#32034;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06048</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#31867;&#21644;&#22522;&#20110;Contourlet&#29305;&#24449;&#30340;&#32441;&#29702;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Texture image retrieval using a classification and contourlet-based features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06048
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26032;&#30340;&#22522;&#20110;RCT-Plus&#21464;&#25442;&#30340;&#22270;&#20687;&#34920;&#31034;&#21644;&#23398;&#20064;&#26041;&#27861;&#25913;&#36827;&#20102;&#32441;&#29702;&#22270;&#20687;&#26816;&#32034;&#65292;&#23454;&#29616;&#20102;&#27604;&#20197;&#21069;&#26041;&#26696;&#26356;&#39640;&#30340;&#26816;&#32034;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#20869;&#23481;&#30340;&#22270;&#20687;&#26816;&#32034;&#65288;CBIR&#65289;&#30340;&#32441;&#29702;&#22270;&#20687;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;RCT-Plus&#21464;&#25442;&#30340;&#26032;&#22270;&#20687;&#34920;&#31034;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#21462;&#22270;&#20687;&#20013;&#26356;&#20016;&#23500;&#26041;&#21521;&#20449;&#24687;&#30340;Redundant Contourlet&#21464;&#25442;&#30340;&#26032;&#21464;&#20307;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22270;&#20687;&#25628;&#32034;&#36807;&#31243;&#65292;&#20854;&#20013;&#25968;&#25454;&#24211;&#20013;&#30340;&#22270;&#20687;&#20351;&#29992;&#36866;&#24212;&#20110;RCT-Plus&#21464;&#25442;&#30340;&#32479;&#35745;&#24314;&#27169;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#21518;&#39318;&#20808;&#23545;&#26597;&#35810;&#36827;&#34892;&#20998;&#31867;&#20197;&#36873;&#25321;&#26368;&#20339;&#32441;&#29702;&#31867;&#21035;&#65292;&#28982;&#21518;&#23545;&#20445;&#30041;&#31867;&#21035;&#30340;&#22270;&#20687;&#36827;&#34892;&#25490;&#21517;&#20197;&#36873;&#25321;&#39030;&#37096;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#30456;&#27604;&#20197;&#21069;&#30340;CBIR&#26041;&#26696;&#22312;&#26816;&#32034;&#36895;&#29575;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06048v1 Announce Type: cross  Abstract: In this paper, we propose a new framework for improving Content Based Image Retrieval (CBIR) for texture images. This is achieved by using a new image representation based on the RCT-Plus transform which is a novel variant of the Redundant Contourlet transform that extracts a richer directional information in the image. Moreover, the process of image search is improved through a learning-based approach where the images of the database are classified using an adapted similarity metric to the statistical modeling of the RCT-Plus transform. A query is then first classified to select the best texture class after which the retained class images are ranked to select top ones. By this, we have achieved significant improvements in the retrieval rates compared to previous CBIR schemes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MATRIX&#30340;&#23398;&#20064;&#22522;&#30784;&#33258;&#21160;&#36712;&#36857;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#32972;&#26223;&#20013;&#29983;&#25104;&#20132;&#20114;&#24335;&#20154;&#31867;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.06041</link><description>&lt;p&gt;
MATRIX: &#20855;&#26377;&#22810;&#26679;&#21270;&#32972;&#26223;&#30340;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MATRIX&#30340;&#23398;&#20064;&#22522;&#30784;&#33258;&#21160;&#36712;&#36857;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#32972;&#26223;&#20013;&#29983;&#25104;&#20132;&#20114;&#24335;&#20154;&#31867;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#24314;&#27169;&#22797;&#26434;&#20154;&#31867;&#34892;&#20026;&#21160;&#24577;&#21644;&#22788;&#29702;&#35768;&#22810;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24102;&#26631;&#27880;&#30340;&#30495;&#23454;&#19990;&#30028;&#20154;&#31867;&#25968;&#25454;&#36890;&#24120;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#24230;&#20132;&#20114;&#24335;&#30340;&#22330;&#26223;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#31639;&#27861;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#21463;&#21040;&#27169;&#22411;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20351;&#20854;&#26080;&#27861;&#20026;&#21508;&#31181;&#24212;&#29992;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#29616;&#23454;&#21644;&#22810;&#26679;&#21270;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#25110;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#30340;&#36712;&#36857;&#32423;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#30784;&#30340;&#33258;&#21160;&#36712;&#36857;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#22810;&#26679;&#21270;&#32972;&#26223;&#30340;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#29983;&#25104;&#65288;MATRIX&#65289;&#12290;MATRIX&#33021;&#22815;&#22312;&#29616;&#23454;&#22810;&#26679;&#21270;&#30340;&#32972;&#26223;&#20013;&#29983;&#25104;&#20132;&#20114;&#24335;&#20154;&#31867;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#27169;&#26174;&#24335;&#21644;&#21487;&#35299;&#37322;&#30340;&#30446;&#26631;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20351;MATRIX&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06041v1 Announce Type: cross  Abstract: Data-driven methods have great advantages in modeling complicated human behavioral dynamics and dealing with many human-robot interaction applications. However, collecting massive and annotated real-world human datasets has been a laborious task, especially for highly interactive scenarios. On the other hand, algorithmic data generation methods are usually limited by their model capacities, making them unable to offer realistic and diverse data needed by various application users. In this work, we study trajectory-level data generation for multi-human or human-robot interaction scenarios and propose a learning-based automatic trajectory generation model, which we call Multi-Agent TRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of generating interactive human behaviors in realistic diverse contexts. We achieve this goal by modeling the explicit and interpretable objectives so that MATRIX can generate human motion
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoDAP&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#29992;&#20110;&#39044;&#27979;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#28966;&#34385;&#21644;&#25233;&#37057;&#30340;&#36235;&#21183;&#65292;&#36890;&#36807;&#23450;&#24615;&#20010;&#20307;&#23646;&#24615;&#35270;&#35282;&#20998;&#26512;&#20102;&#28966;&#34385;&#21644;&#25233;&#37057;&#27169;&#24335;&#65292;&#24182;&#25581;&#31034;&#20102;&#20154;&#21475;&#22240;&#32032;&#12289;&#34892;&#20026;&#21464;&#21270;&#21644;&#31038;&#20250;&#24515;&#29702;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#30340;&#20851;&#38190;&#35265;&#35299;</title><link>https://arxiv.org/abs/2403.06033</link><description>&lt;p&gt;
&#39044;&#27979;&#25233;&#37057;&#21644;&#28966;&#34385;&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;COVID-19&#24515;&#29702;&#20581;&#24247;&#24433;&#21709;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
Predicting Depression and Anxiety: A Multi-Layer Perceptron for Analyzing the Mental Health Impact of COVID-19
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06033
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoDAP&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#29992;&#20110;&#39044;&#27979;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#28966;&#34385;&#21644;&#25233;&#37057;&#30340;&#36235;&#21183;&#65292;&#36890;&#36807;&#23450;&#24615;&#20010;&#20307;&#23646;&#24615;&#35270;&#35282;&#20998;&#26512;&#20102;&#28966;&#34385;&#21644;&#25233;&#37057;&#27169;&#24335;&#65292;&#24182;&#25581;&#31034;&#20102;&#20154;&#21475;&#22240;&#32032;&#12289;&#34892;&#20026;&#21464;&#21270;&#21644;&#31038;&#20250;&#24515;&#29702;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#30340;&#20851;&#38190;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;COVID-19&#25233;&#37057;&#21644;&#28966;&#34385;&#39044;&#27979;&#22120;&#65288;CoDAP&#65289;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#30340;&#24515;&#29702;&#20581;&#24247;&#36235;&#21183;&#65292;&#29305;&#21035;&#26159;&#28966;&#34385;&#21644;&#25233;&#37057;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;COVID-19&#22823;&#27969;&#34892;&#21021;&#26399;&#65288;2020&#24180;4&#26376;&#33267;6&#26376;&#65289;&#30340;&#21313;&#21608;&#20869;&#27599;&#21608;&#36319;&#36394;&#20102;&#32654;&#22269;&#25104;&#24180;&#20154;&#22810;&#26679;&#21270;&#38431;&#21015;&#30340;&#24515;&#29702;&#20581;&#24247;&#30151;&#29366;&#12290;&#36825;&#19968;&#26102;&#26399;&#20197;&#24515;&#29702;&#20581;&#24247;&#30151;&#29366;&#21644;&#24773;&#20917;&#28608;&#22686;&#20026;&#29305;&#24449;&#65292;&#20026;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#38190;&#32972;&#26223;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;CoDAP&#20174;&#29420;&#29305;&#30340;&#23450;&#24615;&#20010;&#20307;&#23646;&#24615;&#35270;&#35282;&#25552;&#21462;&#21644;&#20998;&#26512;&#28966;&#34385;&#21644;&#25233;&#37057;&#27169;&#24335;&#12290;&#36825;&#20010;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#39044;&#27979;&#22823;&#27969;&#34892;&#26399;&#38388;&#28966;&#34385;&#21644;&#25233;&#37057;&#30340;&#27169;&#24335;&#65292;&#36824;&#25581;&#31034;&#20102;&#20154;&#21475;&#22240;&#32032;&#12289;&#34892;&#20026;&#21464;&#21270;&#21644;&#31038;&#20250;&#24515;&#29702;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#26356;&#32454;&#33268;&#22320;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06033v1 Announce Type: new  Abstract: We introduce a multi-layer perceptron (MLP) called the COVID-19 Depression and Anxiety Predictor (CoDAP) to predict mental health trends, particularly anxiety and depression, during the COVID-19 pandemic. Our method utilizes a comprehensive dataset, which tracked mental health symptoms weekly over ten weeks during the initial COVID-19 wave (April to June 2020) in a diverse cohort of U.S. adults. This period, characterized by a surge in mental health symptoms and conditions, offers a critical context for our analysis. Our focus was to extract and analyze patterns of anxiety and depression through a unique lens of qualitative individual attributes using CoDAP. This model not only predicts patterns of anxiety and depression during the pandemic but also unveils key insights into the interplay of demographic factors, behavioral changes, and social determinants of mental health. These findings contribute to a more nuanced understanding of the 
&lt;/p&gt;</description></item><item><title>FairTargetSim&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;&#65292;&#23637;&#31034;&#20102;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#36866;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.06031</link><description>&lt;p&gt;
FairTargetSim&#65306;&#29992;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#20844;&#24179;&#24615;&#24433;&#21709;&#30340;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06031
&lt;/p&gt;
&lt;p&gt;
FairTargetSim&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;&#65292;&#23637;&#31034;&#20102;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#36866;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#20026;&#39044;&#27979;&#25110;&#20915;&#31574;&#23450;&#20041;&#30446;&#26631;&#21464;&#37327;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#23545;&#20844;&#24179;&#24615;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#65306;&#20559;&#35265;&#36890;&#24120;&#24050;&#32463;&#34987;&#32534;&#30721;&#22312;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#26412;&#36523;&#20013;&#65292;&#32780;&#19981;&#26159;&#22312;&#20219;&#20309;&#25968;&#25454;&#25910;&#38598;&#25110;&#35757;&#32451;&#20043;&#21069;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;&#65292;FairTargetSim (FTS)&#65292;&#23637;&#31034;&#20102;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#22914;&#20309;&#24433;&#21709;&#20844;&#24179;&#24615;&#12290;FTS&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#36866;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;FTS&#20351;&#29992;&#20102;&#31639;&#27861;&#25307;&#32856;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#21644;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#21464;&#37327;&#12290;FTS&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;http://tinyurl.com/ftsinterface&#12290;&#26412;&#25991;&#38468;&#24102;&#30340;&#35270;&#39057;&#32593;&#22336;&#20026;&#65306;http://tinyurl.com/ijcaifts&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06031v1 Announce Type: cross  Abstract: Machine learning requires defining one's target variable for predictions or decisions, a process that can have profound implications on fairness: biases are often encoded in target variable definition itself, before any data collection or training. We present an interactive simulator, FairTargetSim (FTS), that illustrates how target variable definition impacts fairness. FTS is a valuable tool for algorithm developers, researchers, and non-technical stakeholders. FTS uses a case study of algorithmic hiring, using real-world data and user-defined target variables. FTS is open-source and available at: http://tinyurl.com/ftsinterface. The video accompanying this paper is here: http://tinyurl.com/ijcaifts.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#20020;&#24202;&#25968;&#25454;&#21644;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#65292;&#25104;&#21151;&#39044;&#27979;&#24515;&#33039;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#30340;&#31070;&#32463;&#24674;&#22797;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#36801;&#31227;&#23398;&#20064;&#22312;&#21307;&#23398;&#20998;&#31867;&#20013;&#30340;&#25928;&#29992;&#21644;&#38480;&#21046;</title><link>https://arxiv.org/abs/2403.06027</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24515;&#33039;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#30340;&#31070;&#32463;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Multimodal deep learning approach to predicting neurological recovery from coma after cardiac arrest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06027
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#20020;&#24202;&#25968;&#25454;&#21644;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#65292;&#25104;&#21151;&#39044;&#27979;&#24515;&#33039;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#30340;&#31070;&#32463;&#24674;&#22797;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#36801;&#31227;&#23398;&#20064;&#22312;&#21307;&#23398;&#20998;&#31867;&#20013;&#30340;&#25928;&#29992;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#22242;&#38431;&#65288;BEEGees&#65289;&#22312;2023&#24180;George B. Moody PhysioNet&#25361;&#25112;&#36187;&#20013;&#30340;&#36129;&#29486;&#12290;&#26088;&#22312;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#21644;&#35832;&#22914;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#20043;&#31867;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#39044;&#27979;&#24515;&#33039;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#30340;&#31070;&#32463;&#24674;&#22797;&#12290;&#25105;&#20204;&#30340;&#24314;&#27169;&#26041;&#27861;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#22522;&#20110;&#20174;&#20247;&#22810;&#33041;&#30005;&#22270;&#36890;&#36947;&#27966;&#29983;&#30340;&#20108;&#32500;&#39057;&#35889;&#22270;&#34920;&#31034;&#65292;&#21516;&#26102;&#25972;&#21512;&#20020;&#24202;&#25968;&#25454;&#21644;&#30452;&#25509;&#20174;&#33041;&#30005;&#22270;&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20132;&#30340;&#27169;&#22411;&#22312;&#33258;&#21457;&#24490;&#29615;&#24674;&#22797;&#21518;72&#23567;&#26102;&#36827;&#34892;&#30340;&#39044;&#27979;&#30340;&#38544;&#34255;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;0.53&#30340;&#25361;&#25112;&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#21307;&#23398;&#20998;&#31867;&#20013;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#20851;&#20110;&#26410;&#26469;&#30340;&#23454;&#26045;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27169;&#22411;&#24615;&#33021;&#19982;&#20915;&#31574;&#38408;&#20540;&#30340;&#36873;&#25321;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#22312;&#25968;&#25454;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06027v1 Announce Type: new  Abstract: This work showcases our team's (The BEEGees) contributions to the 2023 George B. Moody PhysioNet Challenge. The aim was to predict neurological recovery from coma following cardiac arrest using clinical data and time-series such as multi-channel EEG and ECG signals. Our modelling approach is multimodal, based on two-dimensional spectrogram representations derived from numerous EEG channels, alongside the integration of clinical data and features extracted directly from EEG recordings. Our submitted model achieved a Challenge score of $0.53$ on the hidden test set for predictions made $72$ hours after return of spontaneous circulation. Our study shows the efficacy and limitations of employing transfer learning in medical classification. With regard to prospective implementation, our analysis reveals that the performance of the model is strongly linked to the selection of a decision threshold and exhibits strong variability across data spl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#34920;&#31034;&#26080;&#27861;&#36328;&#36234;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06026</link><description>&lt;p&gt;
&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Towards a Generic Representation of Cominatorial Problems for Learning-Based Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#34920;&#31034;&#26080;&#27861;&#36328;&#36234;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#26080;&#35770;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36824;&#26159;&#19982;&#20256;&#32479;&#20248;&#21270;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#26223;&#19979;&#65292;&#25361;&#25112;&#22312;&#20110;&#23558;&#30446;&#26631;&#32452;&#21512;&#38382;&#39064;&#32534;&#30721;&#25104;&#36866;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#30340;&#32467;&#26500;&#12290;&#35768;&#22810;&#29616;&#26377;&#20316;&#21697;&#25552;&#20986;&#20102;&#29305;&#23450;&#20110;&#38382;&#39064;&#30340;&#34920;&#31034;&#65292;&#36890;&#24120;&#20197;&#22270;&#30340;&#24418;&#24335;&#65292;&#20197;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#65292;&#22240;&#20026;&#34920;&#31034;&#19981;&#33021;&#36731;&#26131;&#20174;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#23581;&#35797;&#21435;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21482;&#25552;&#20379;&#20102;&#37096;&#20998;&#27867;&#21270;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26397;&#30528;&#23436;&#20840;&#36890;&#29992;&#30340;&#34920;&#31034;&#26041;&#24335;&#36808;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06026v1 Announce Type: cross  Abstract: In recent years, there has been a growing interest in using learning-based approaches for solving combinatorial problems, either in an end-to-end manner or in conjunction with traditional optimization algorithms. In both scenarios, the challenge lies in encoding the targeted combinatorial problems into a structure compatible with the learning algorithm. Many existing works have proposed problem-specific representations, often in the form of a graph, to leverage the advantages of \textit{graph neural networks}. However, these approaches lack generality, as the representation cannot be easily transferred from one combinatorial problem to another one. While some attempts have been made to bridge this gap, they still offer a partial generality only. In response to this challenge, this paper advocates for progress toward a fully generic representation of combinatorial problems for learning-based approaches. The approach we propose involves 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;SMMIL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#26500;&#24615;&#24515;&#33039;&#30142;&#30149;&#33258;&#21160;&#35299;&#37322;&#65292;&#20811;&#26381;&#20102;&#24515;&#33039;&#36229;&#22768;&#24515;&#21160;&#22270;&#35780;&#20272;AS&#26102;&#23545;&#26377;&#38480;2D cineloops&#21644;&#38590;&#20197;&#33719;&#24471;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.06024</link><description>&lt;p&gt;
&#20027;&#21160;&#33033;&#29421;&#31364;&#35786;&#26029;&#30340;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06024
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;SMMIL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#26500;&#24615;&#24515;&#33039;&#30142;&#30149;&#33258;&#21160;&#35299;&#37322;&#65292;&#20811;&#26381;&#20102;&#24515;&#33039;&#36229;&#22768;&#24515;&#21160;&#22270;&#35780;&#20272;AS&#26102;&#23545;&#26377;&#38480;2D cineloops&#21644;&#38590;&#20197;&#33719;&#24471;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06024v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#33258;&#21160;&#35299;&#37322;&#24515;&#33039;&#36229;&#22768;&#24515;&#21160;&#22270;&#24515;&#33039;&#36229;&#22768;&#24515;&#21160;&#22270;(imaging of the heart (echocardiograms))&#21487;&#20197;&#25913;&#21892;&#20027;&#21160;&#33033;&#29923;&#29421;&#31364;(aortic stenosis, AS)&#30340;&#26816;&#27979;&#21644;&#27835;&#30103;&#65292;&#32780;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;AS&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#26377;&#38480;&#30340;2D cineloops&#65292;&#22240;&#27492;&#24573;&#30053;&#20102;&#21253;&#21547;&#19982;AS&#30456;&#20851;&#30340;&#21387;&#21147;&#26799;&#24230;&#21644;&#34880;&#28082;&#27969;&#21160;&#24322;&#24120;&#31561;&#37325;&#35201;&#20114;&#34917;&#20449;&#24687;&#30340;&#24191;&#27867;&#21487;&#29992;&#30340;&#22810;&#26222;&#21202;(imaging)&#12290;&#20854;&#27425;&#65292;&#33719;&#24471;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#26159;&#22256;&#38590;&#30340;&#12290;&#36890;&#24120;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#24515;&#33039;&#36229;&#22768;&#24515;&#21160;&#22270;&#24405;&#38899;&#21487;&#29992;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#34987;&#29616;&#26377;&#26041;&#27861;&#24456;&#23569;&#21033;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;Semi-supervised Multimodal Multiple-Instance Learning, SMMIL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#35299;&#37322;AS&#31561;&#32467;&#26500;&#24615;&#24515;&#33039;&#30142;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06024v1 Announce Type: cross  Abstract: Automated interpretation of ultrasound imaging of the heart (echocardiograms) could improve the detection and treatment of aortic stenosis (AS), a deadly heart disease. However, existing deep learning pipelines for assessing AS from echocardiograms have two key limitations. First, most methods rely on limited 2D cineloops, thereby ignoring widely available Doppler imaging that contains important complementary information about pressure gradients and blood flow abnormalities associated with AS. Second, obtaining labeled data is difficult. There are often far more unlabeled echocardiogram recordings available, but these remain underutilized by existing methods. To overcome these limitations, we introduce Semi-supervised Multimodal Multiple-Instance Learning (SMMIL), a new deep learning framework for automatic interpretation for structural heart diseases like AS. When deployed, SMMIL can combine information from two input modalities, spec
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20379;PSC&#24037;&#20855;&#23558;&#27874;&#26031;&#35821;&#20442;&#35821;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#24773;&#24863;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.06023</link><description>&lt;p&gt;
&#27874;&#26031;&#35821;&#20442;&#35821;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#20197;&#21450;&#31038;&#20132;&#23186;&#20307;&#19978;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Persian Slang Text Conversion to Formal and Deep Learning of Persian Short Texts on Social Media for Sentiment Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06023
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20379;PSC&#24037;&#20855;&#23558;&#27874;&#26031;&#35821;&#20442;&#35821;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#36866;&#21512;&#20998;&#26512;&#27874;&#26031;&#35821;&#20250;&#35805;&#25991;&#26412;&#30340;&#24037;&#20855;&#20351;&#24471;&#23545;&#36825;&#20123;&#25991;&#26412;&#65288;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#65289;&#30340;&#21508;&#31181;&#20998;&#26512;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#25552;&#20379;PSC&#65288;&#27874;&#26031;&#35821;&#20442;&#35821;&#36716;&#25442;&#22120;&#65289;&#65292;&#23558;&#20250;&#35805;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#65292;&#24182;&#32467;&#21512;&#26368;&#26032;&#21644;&#26368;&#20339;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#26356;&#23481;&#26131;&#29702;&#35299;&#36825;&#20123;&#25991;&#26412;&#65292;&#26356;&#22909;&#22320;&#36827;&#34892;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06023v1 Announce Type: new  Abstract: The lack of a suitable tool for the analysis of conversational texts in the Persian language has made various analyses of these texts, including Sentiment Analysis, difficult. In this research, we tried to make the understanding of these texts easier for the machine by providing PSC, Persian Slang Converter, a tool for converting conversational texts into formal ones, and by using the most up-to-date and best deep learning methods along with the PSC, the sentiment learning of short Persian language texts for the machine in a better way. be made More than 10 million unlabeled texts from various social networks and movie subtitles (as Conversational texts) and about 10 million news texts (as formal texts) have been used for training unsupervised models and formal implementation of the tool. 60,000 texts from the comments of Instagram social network users with positive, negative, and neutral labels are considered supervised data for trainin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#20998;&#23618;&#20449;&#24687;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20998;&#23618;&#26597;&#35810;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26597;&#35810;&#27169;&#31946;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06021</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#30340;&#20998;&#23618;&#26597;&#35810;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Query Classification in E-commerce Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#20998;&#23618;&#20449;&#24687;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20998;&#23618;&#26597;&#35810;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26597;&#35810;&#27169;&#31946;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#36890;&#24120;&#20197;&#23618;&#27425;&#32467;&#26500;&#23384;&#20648;&#21644;&#26500;&#36896;&#20135;&#21697;&#20449;&#24687;&#21644;&#25628;&#32034;&#25968;&#25454;&#12290;&#23558;&#29992;&#25143;&#25628;&#32034;&#26597;&#35810;&#26377;&#25928;&#20998;&#31867;&#21040;&#31867;&#20284;&#30340;&#23618;&#27425;&#32467;&#26500;&#20013;&#65292;&#22312;&#25552;&#21319;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#29992;&#25143;&#20307;&#39564;&#30340;&#21516;&#26102;&#65292;&#23545;&#20110;&#26032;&#38395;&#25972;&#29702;&#21644;&#23398;&#26415;&#30740;&#31350;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#22788;&#29702;&#25935;&#24863;&#26597;&#35810;&#20998;&#31867;&#25110;&#20851;&#38190;&#20449;&#24687;&#20256;&#25773;&#26102;&#65292;&#31934;&#30830;&#24615;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#25918;&#22823;&#65292;&#22240;&#20026;&#19981;&#20934;&#30830;&#21487;&#33021;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20998;&#23618;&#26597;&#35810;&#20998;&#31867;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21463;&#21040;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#24433;&#21709;&#65306;&#65288;1&#65289;&#26126;&#26174;&#30340;&#20559;&#21521;&#20027;&#23548;&#31867;&#21035;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#24615;&#65292;&#21644;&#65288;2&#65289;&#25628;&#32034;&#26597;&#35810;&#30340;&#22266;&#26377;&#31616;&#27905;&#24615;&#21644;&#27169;&#31946;&#24615;&#65292;&#38459;&#30861;&#20102;&#20934;&#30830;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#65288;i&#65289;&#22686;&#24378;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#21033;&#29992;&#20998;&#23618;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06021v1 Announce Type: cross  Abstract: E-commerce platforms typically store and structure product information and search data in a hierarchy. Efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research. The significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts. The inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification.   To address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced representation learning that utilizes the contrastive loss
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#25193;&#25955;&#30340;NAS&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#26465;&#20214;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26041;&#27861;&#65292;&#33021;&#22312;&#26550;&#26500;&#25628;&#32034;&#20013;&#29983;&#25104;&#24555;&#36895;&#19988;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.06020</link><description>&lt;p&gt;
&#22810;&#26465;&#20214;&#22270;&#25193;&#25955;&#29992;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-conditioned Graph Diffusion for Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#25193;&#25955;&#30340;NAS&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#26465;&#20214;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26041;&#27861;&#65292;&#33021;&#22312;&#26550;&#26500;&#25628;&#32034;&#20013;&#29983;&#25104;&#24555;&#36895;&#19988;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36890;&#36807;&#25506;&#32034;&#24222;&#22823;&#19988;&#22797;&#26434;&#30340;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#26469;&#33258;&#21160;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#20026;&#20102;&#25512;&#21160;&#26550;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#25193;&#25955;&#30340;NAS&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31163;&#25955;&#26465;&#20214;&#22270;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#24615;&#33021;&#20248;&#36234;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26465;&#20214;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#22270;&#25193;&#25955;&#32593;&#32476;&#65292;&#20849;&#21516;&#26045;&#21152;&#35832;&#22914;&#39640;&#20934;&#30830;&#24615;&#21644;&#20302;&#30828;&#20214;&#24310;&#36831;&#31561;&#32422;&#26463;&#12290;&#19982;&#30456;&#20851;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#21487;&#24494;&#20998;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#21333;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20845;&#20010;&#26631;&#20934;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20197;&#24555;&#36895;&#36895;&#24230;&#29983;&#25104;&#26032;&#39062;&#19988;&#29420;&#29305;&#30340;&#26550;&#26500;&#65292;&#21363;&#27599;&#31181;&#26550;&#26500;&#23569;&#20110;0.2&#31186;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06020v1 Announce Type: new  Abstract: Neural architecture search automates the design of neural network architectures usually by exploring a large and thus complex architecture search space. To advance the architecture search, we present a graph diffusion-based NAS approach that uses discrete conditional graph diffusion processes to generate high-performing neural network architectures. We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency. Unlike the related work, our method is completely differentiable and requires only a single model training. In our evaluations, we show promising results on six standard benchmarks, yielding novel and unique architectures at a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we demonstrate the generalisability and efficiency of our method through experiments on ImageNet dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22914;&#20309;&#23558;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;PLM LLaMa &#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25552;&#31034;&#65292;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#36866;&#24212;&#25552;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06018</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#29992;&#20110;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22914;&#20309;&#23558;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;PLM LLaMa &#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25552;&#31034;&#65292;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#36866;&#24212;&#25552;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22788;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#23637;&#30340;&#21069;&#27839;&#12290;PLMs&#30340;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#26159;&#8220;&#25552;&#31034;&#8221; - &#25110;&#19978;&#19979;&#25991;&#23398;&#20064; - &#29992;&#25143;&#22312;&#25552;&#31034;PLM&#23545;&#26032;&#31034;&#20363;&#25191;&#34892;&#20219;&#21153;&#20043;&#21069;&#21521;PLM&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#21644;&#19968;&#20123;&#23436;&#25104;&#30340;&#31034;&#20363;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;&#30446;&#21069;&#21482;&#26377;&#26368;&#22823;&#12289;&#26368;&#26377;&#33021;&#21147;&#30340;PLMs&#25165;&#33021;&#26377;&#25928;&#22320;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26159;&#36890;&#36807;&#20027;&#35201;&#20197;&#33521;&#35821;&#20026;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#65292;&#20854;&#20182;&#25152;&#26377;&#35821;&#35328;&#37117;&#33853;&#21518;&#12290;&#22823;&#22810;&#25968;&#35821;&#35328;&#30340;&#25968;&#25454;&#38480;&#21046;&#38459;&#30861;&#20102;&#35757;&#32451;&#20855;&#26377;&#25552;&#31034;&#33021;&#21147;&#30340;&#35821;&#35328;&#29305;&#23450;PLMs&#12290;&#23613;&#31649;&#22312;&#25552;&#31034;&#35774;&#32622;&#26041;&#38754;&#30340;&#24037;&#20316;&#28608;&#22686;&#65292;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#22914;&#20309;&#23558;PLMs&#19987;&#38376;&#29992;&#20110;&#36328;&#35821;&#35328;&#36866;&#24212;&#25552;&#31034;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36866;&#24212;LLaMa&#30340;&#21487;&#33021;&#26041;&#27861;&#65292;LLaMa&#26159;&#19968;&#20010;&#20027;&#35201;&#22312;&#33521;&#35821;&#20013;&#35757;&#32451;&#30340;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;PLM&#65292;&#29992;&#20110;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#36827;&#34892;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06018v1 Announce Type: cross  Abstract: Large pre-trained language models (PLMs) are at the forefront of advances in Natural Language Processing. One widespread use case of PLMs is "prompting" - or in-context learning - where a user provides a description of a task and some completed examples of the task to a PLM as context before prompting the PLM to perform the task on a new example. Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind. The data limitations in most languages preclude the training of language-specific PLMs capable of prompting. Albeit the surge in work of prompting settings, it is still unclear how PLMs should be adapted cross-lingually specifically for prompting. We evaluate the possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, nam
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#28385;&#36275;&#24191;&#27867;&#35201;&#27714;&#30340;&#21512;&#25104;&#12289;&#21322;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20844;&#24179;&#22270;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20449;&#24687;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06017</link><description>&lt;p&gt;
&#24357;&#34917;&#20844;&#24179;&#22270;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65306;&#36208;&#21521;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06017
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#28385;&#36275;&#24191;&#27867;&#35201;&#27714;&#30340;&#21512;&#25104;&#12289;&#21322;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20844;&#24179;&#22270;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20449;&#24687;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#22270;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#20844;&#24179;&#22270;&#23398;&#20064;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#24448;&#24448;&#20381;&#36182;&#20110;&#26500;&#36896;&#19981;&#20339;&#30340;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#25110;&#20302;&#26631;&#20934;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#26159;&#22522;&#26412;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20063;&#21487;&#20197;&#22312;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#32988;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35828;&#26126;&#35768;&#22810;&#25968;&#25454;&#38598;&#26410;&#33021;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#36793;&#32536;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#25361;&#25112;&#20351;&#29992;&#22270;&#32467;&#26500;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20805;&#20998;&#28385;&#36275;&#24191;&#27867;&#35201;&#27714;&#30340;&#21512;&#25104;&#12289;&#21322;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#38598;&#21512;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#21253;&#25324;&#23545;&#20110;&#27169;&#22411;&#20844;&#24179;&#35780;&#20215;&#33267;&#20851;&#37325;&#35201;&#30340;&#30456;&#20851;&#22270;&#32467;&#26500;&#21644;&#20559;&#24046;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06017v1 Announce Type: new  Abstract: Fair graph learning plays a pivotal role in numerous practical applications. Recently, many fair graph learning methods have been proposed; however, their evaluation often relies on poorly constructed semi-synthetic datasets or substandard real-world datasets. In such cases, even a basic Multilayer Perceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utility and fairness. In this work, we illustrate that many datasets fail to provide meaningful information in the edges, which may challenge the necessity of using graph structures in these problems. To address these issues, we develop and introduce a collection of synthetic, semi-synthetic, and real-world datasets that fulfill a broad spectrum of requirements. These datasets are thoughtfully designed to include relevant graph structures and bias information crucial for the fair evaluation of models. The proposed synthetic and semi-synthetic datasets offer the flexibility to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#19968;&#33268;&#21270;&#20272;&#35745;&#22120;&#31227;&#26893;&#21040;&#27973;&#23618;&#20915;&#31574;&#26641;&#65288;CART&#65289;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#19968;&#33268;&#24615;&#20445;&#35777;&#24182;&#22312;&#23454;&#35777;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.06015</link><description>&lt;p&gt;
&#31227;&#26893;&#65306;&#20351;&#38543;&#26426;&#26862;&#26519;&#19968;&#33268;&#21270;
&lt;/p&gt;
&lt;p&gt;
Grafting: Making Random Forests Consistent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#19968;&#33268;&#21270;&#20272;&#35745;&#22120;&#31227;&#26893;&#21040;&#27973;&#23618;&#20915;&#31574;&#26641;&#65288;CART&#65289;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#19968;&#33268;&#24615;&#20445;&#35777;&#24182;&#22312;&#23454;&#35777;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38543;&#26426;&#26862;&#26519;&#22312;&#24615;&#33021;&#21644;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20851;&#20110;&#20854;&#29702;&#35770;&#30693;&#20043;&#29978;&#23569;&#12290;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#26159;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#26159;&#21542;&#19968;&#33268;&#21270;&#65292;&#25110;&#20309;&#26102;&#36798;&#21040;&#19968;&#33268;&#21270;&#12290;&#25991;&#29486;&#25506;&#35752;&#20102;&#32463;&#20856;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30340;&#21508;&#31181;&#21464;&#20307;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#21644;&#24050;&#30693;&#32570;&#38519;&#12290;&#26412;&#25991;&#20026;&#36825;&#19968;&#25991;&#29486;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25506;&#35752;&#20102;&#23558;&#19968;&#33268;&#21270;&#20272;&#35745;&#22120;&#31227;&#26893;&#21040;&#27973;&#23618;CART&#30340;&#36866;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#19968;&#33268;&#21270;&#20445;&#35777;&#24182;&#22312;&#23454;&#35777;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06015v1 Announce Type: cross  Abstract: Despite their performance and widespread use, little is known about the theory of Random Forests. A major unanswered question is whether, or when, the Random Forest algorithm is consistent. The literature explores various variants of the classic Random Forest algorithm to address this question and known short-comings of the method. This paper is a contribution to this literature. Specifically, the suitability of grafting consistent estimators onto a shallow CART is explored. It is shown that this approach has a consistency guarantee and performs well in empirical settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25915;&#20987;&#30340;&#26597;&#35810;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06014</link><description>&lt;p&gt;
&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#23567;&#26597;&#35810;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Hard-label based Small Query Black-box Adversarial Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25915;&#20987;&#30340;&#26597;&#35810;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#35774;&#32622;&#65292;&#20165;&#35266;&#23519;&#26469;&#33258;&#30446;&#26631;&#27169;&#22411;&#30340;&#39044;&#27979;&#31867;&#21035;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#22823;&#22810;&#25968;&#25915;&#20987;&#26041;&#27861;&#37117;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#25968;&#37327;&#30340;&#26597;&#35810;&#25165;&#33021;&#23454;&#29616;&#25104;&#21151;&#25915;&#20987;&#12290;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#30333;&#30418;&#26367;&#20195;&#27169;&#22411;&#19982;&#40657;&#30418;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#23545;&#25239;&#20256;&#36882;&#24615;&#12290;&#28982;&#32780;&#65292;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#36719;&#26631;&#31614;&#30340;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#12290;&#19982;&#20027;&#27969;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#29992;&#30340;&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#25915;&#20987;&#35774;&#32622;&#65292;&#20854;&#20248;&#21270;&#36807;&#31243;&#30001;&#39044;&#35757;&#32451;&#30340;&#26367;&#20195;&#27169;&#22411;&#25351;&#23548;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;&#30446;&#26631;&#27169;&#22411;&#26550;&#26500;&#19978;&#30340;&#22522;&#20110;&#30828;&#26631;&#31614;&#30340;&#40657;&#30418;&#25915;&#20987;&#30340;&#26597;&#35810;&#25928;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#32422;5&#20493;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06014v1 Announce Type: cross  Abstract: We consider the hard label based black box adversarial attack setting which solely observes predicted classes from the target model. Most of the attack methods in this setting suffer from impractical number of queries required to achieve a successful attack. One approach to tackle this drawback is utilising the adversarial transferability between white box surrogate models and black box target model. However, the majority of the methods adopting this approach are soft label based to take the full advantage of zeroth order optimisation. Unlike mainstream methods, we propose a new practical setting of hard label based attack with an optimisation process guided by a pretrained surrogate model. Experiments show the proposed method significantly improves the query efficiency of the hard label based black-box attack across various target model architectures. We find the proposed method achieves approximately 5 times higher attack success rat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#22686;&#24378;&#35299;&#37322;&#40065;&#26834;&#24615;&#24182;&#19981;&#33021;&#25552;&#39640;&#20998;&#31867;&#40065;&#26834;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#12290;</title><link>https://arxiv.org/abs/2403.06013</link><description>&lt;p&gt;
&#20998;&#31867;&#40065;&#26834;&#24615;&#21644;&#35299;&#37322;&#40065;&#26834;&#24615;&#26159;&#21542;&#30495;&#30340;&#24378;&#30456;&#20851;&#65311;&#36890;&#36807;&#36755;&#20837;&#25439;&#22833;&#26223;&#35266;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Are Classification Robustness and Explanation Robustness Really Strongly Correlated? An Analysis Through Input Loss Landscape
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06013
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#22686;&#24378;&#35299;&#37322;&#40065;&#26834;&#24615;&#24182;&#19981;&#33021;&#25552;&#39640;&#20998;&#31867;&#40065;&#26834;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#40065;&#26834;&#24615;&#39046;&#22495;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#21363;&#22270;&#20687;&#20998;&#31867;&#31995;&#32479;&#20013;&#30340;&#20998;&#31867;&#40065;&#26834;&#24615;&#21644;&#35299;&#37322;&#40065;&#26834;&#24615;&#26412;&#36136;&#19978;&#26159;&#30456;&#20851;&#30340;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#32858;&#31867;&#26469;&#26377;&#25928;&#35780;&#20272;&#35299;&#37322;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22686;&#24378;&#35299;&#37322;&#40065;&#26834;&#24615;&#24182;&#19981;&#19968;&#23450;&#20250;&#20351;&#36755;&#20837;&#25439;&#22833;&#26223;&#35266;&#30456;&#23545;&#20110;&#35299;&#37322;&#25439;&#22833;&#21464;&#24179; - &#19982;&#25439;&#22833;&#26223;&#35266;&#21464;&#24179;&#34920;&#31034;&#26356;&#22909;&#30340;&#20998;&#31867;&#40065;&#26834;&#24615;&#30456;&#21453;&#12290;&#20026;&#20102;&#28145;&#20837;&#30740;&#31350;&#36825;&#19968;&#30683;&#30462;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#35843;&#25972;&#30456;&#23545;&#20110;&#35299;&#37322;&#25439;&#22833;&#30340;&#25439;&#22833;&#26223;&#35266;&#12290;&#36890;&#36807;&#36825;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#36825;&#31181;&#35843;&#25972;&#21487;&#20197;&#24433;&#21709;&#35299;&#37322;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#27809;&#26377;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#19981;&#20165;&#25361;&#25112;&#20102;&#27969;&#34892;&#30340;&#35266;&#24565;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06013v1 Announce Type: new  Abstract: This paper delves into the critical area of deep learning robustness, challenging the conventional belief that classification robustness and explanation robustness in image classification systems are inherently correlated. Through a novel evaluation approach leveraging clustering for efficient assessment of explanation robustness, we demonstrate that enhancing explanation robustness does not necessarily flatten the input loss landscape with respect to explanation loss - contrary to flattened loss landscapes indicating better classification robustness. To deeply investigate this contradiction, a groundbreaking training method designed to adjust the loss landscape with respect to explanation loss is proposed. Through the new training method, we uncover that although such adjustments can impact the robustness of explanations, they do not have an influence on the robustness of classification. These findings not only challenge the prevailing 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#20010;&#31454;&#20105;&#24615;&#37329;&#34701;&#30446;&#26631;&#30340;&#24037;&#36164;&#20248;&#21270;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#19981;&#21516;&#30446;&#26631;&#12289;&#25972;&#21512;&#29992;&#25143;&#20559;&#22909;&#21644;&#22788;&#29702;&#38543;&#26426;&#21033;&#29575;&#30340;&#38382;&#39064;&#24418;&#24335;&#21270;&#65292;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.06011</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#20803;&#37329;&#34701;&#30446;&#26631;&#30340;&#24378;&#21270;&#23398;&#20064;&#24037;&#36164;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Paycheck Optimization for Multivariate Financial Goals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06011
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#20010;&#31454;&#20105;&#24615;&#37329;&#34701;&#30446;&#26631;&#30340;&#24037;&#36164;&#20248;&#21270;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#19981;&#21516;&#30446;&#26631;&#12289;&#25972;&#21512;&#29992;&#25143;&#20559;&#22909;&#21644;&#22788;&#29702;&#38543;&#26426;&#21033;&#29575;&#30340;&#38382;&#39064;&#24418;&#24335;&#21270;&#65292;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24037;&#36164;&#20248;&#21270;&#65292;&#25506;&#35752;&#22914;&#20309;&#20998;&#37197;&#25910;&#20837;&#20197;&#23454;&#29616;&#22810;&#20010;&#31454;&#20105;&#24615;&#37329;&#34701;&#30446;&#26631;&#12290;&#23545;&#20110;&#24037;&#36164;&#20248;&#21270;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#38382;&#39064;&#24418;&#24335;&#21270;&#65292;&#32570;&#23569;&#23450;&#37327;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25928;&#29992;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;&#24418;&#24335;&#21270;&#33021;&#22815;&#65288;i&#65289;&#32479;&#19968;&#19981;&#21516;&#30340;&#37329;&#34701;&#30446;&#26631;&#65307;&#65288;ii&#65289;&#25972;&#21512;&#29992;&#25143;&#20851;&#20110;&#30446;&#26631;&#30340;&#20559;&#22909;&#65307;&#65288;iii&#65289;&#22788;&#29702;&#38543;&#26426;&#21033;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#24418;&#24335;&#21270;&#36824;&#26377;&#21161;&#20110;&#31471;&#21040;&#31471;&#30340;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#22810;&#31181;&#38382;&#39064;&#35774;&#32622;&#19978;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06011v1 Announce Type: new  Abstract: We study paycheck optimization, which examines how to allocate income in order to achieve several competing financial goals. For paycheck optimization, a quantitative methodology is missing, due to a lack of a suitable problem formulation. To deal with this issue, we formulate the problem as a utility maximization problem. The proposed formulation is able to (i) unify different financial goals; (ii) incorporate user preferences regarding the goals; (iii) handle stochastic interest rates. The proposed formulation also facilitates an end-to-end reinforcement learning solution, which is implemented on a variety of problem settings.
&lt;/p&gt;</description></item><item><title>&#21019;&#24314;&#20102;&#19968;&#31995;&#21015;&#26816;&#27979;&#22120;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#32039;&#20945;&#19988;&#26131;&#20110;&#26500;&#24314;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20026;&#21508;&#31181;&#21361;&#23475;&#25552;&#20379;&#26631;&#31614;&#65292;&#21487;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.06009</link><description>&lt;p&gt;
&#29992;&#20110;&#23433;&#20840;&#21487;&#38752;LLM&#30340;&#26816;&#27979;&#22120;&#65306;&#23454;&#29616;&#12289;&#29992;&#36884;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06009
&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#20102;&#19968;&#31995;&#21015;&#26816;&#27979;&#22120;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#32039;&#20945;&#19988;&#26131;&#20110;&#26500;&#24314;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20026;&#21508;&#31181;&#21361;&#23475;&#25552;&#20379;&#26631;&#31614;&#65292;&#21487;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#39118;&#38505;&#30340;&#24433;&#21709;&#65292;&#20174;&#36755;&#20986;&#19981;&#24544;&#23454;&#21040;&#26377;&#20559;&#35265;&#21644;&#26377;&#27602;&#30340;&#29983;&#25104;&#12290;&#30001;&#20110;&#22260;&#32469;LLMs&#23384;&#22312;&#30340;&#20960;&#20010;&#38480;&#21046;&#24615;&#22240;&#32032;&#65288;&#35757;&#32451;&#25104;&#26412;&#12289;API&#35775;&#38382;&#12289;&#25968;&#25454;&#21487;&#29992;&#24615;&#31561;&#65289;&#65292;&#22312;&#37096;&#32626;&#27169;&#22411;&#26102;&#21487;&#33021;&#24182;&#38750;&#24635;&#26159;&#21487;&#34892;&#26045;&#21152;&#30452;&#25509;&#23433;&#20840;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#25928;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27491;&#22312;&#21162;&#21147;&#21019;&#24314;&#21644;&#37096;&#32626;&#19968;&#31995;&#21015;&#26816;&#27979;&#22120;&#24211;&#65306;&#32039;&#20945;&#19988;&#26131;&#20110;&#26500;&#24314;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20026;&#21508;&#31181;&#21361;&#23475;&#25552;&#20379;&#26631;&#31614;&#12290;&#38500;&#20102;&#26816;&#27979;&#22120;&#26412;&#36523;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#27169;&#22411;&#30340;&#24191;&#27867;&#29992;&#36884;&#8212;&#8212;&#20174;&#20805;&#24403;&#38450;&#25252;&#26639;&#21040;&#20419;&#36827;&#26377;&#25928;&#30340;AI&#27835;&#29702;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#24320;&#21457;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#24037;&#20316;&#65292;&#26088;&#22312;&#20351;&#26816;&#27979;&#22120;&#26356;&#21487;&#38752;&#24182;&#25299;&#23637;&#20854;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06009v1 Announce Type: new  Abstract: Large language models (LLMs) are susceptible to a variety of risks, from non-faithful output to biased and toxic generations. Due to several limiting factors surrounding LLMs (training cost, API access, data availability, etc.), it may not always be feasible to impose direct safety constraints on a deployed model. Therefore, an efficient and reliable alternative is required. To this end, we present our ongoing efforts to create and deploy a library of detectors: compact and easy-to-build classification models that provide labels for various harms. In addition to the detectors themselves, we discuss a wide range of uses for these detector models - from acting as guardrails to enabling effective AI governance. We also deep dive into inherent challenges in their development and discuss future work aimed at making the detectors more reliable and broadening their scope.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21487;&#20197;&#25429;&#25417;&#22870;&#21169;&#20989;&#25968;&#30456;&#20284;&#24615;&#23450;&#20041;&#30340;&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06003</link><description>&lt;p&gt;
&#19968;&#31181;&#25512;&#24191;&#30340;&#29992;&#20110;&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#23398;&#20064;&#30340;&#25910;&#30410;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Generalized Acquisition Function for Preference-based Reward Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06003
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21487;&#20197;&#25429;&#25417;&#22870;&#21169;&#20989;&#25968;&#30456;&#20284;&#24615;&#23450;&#20041;&#30340;&#26694;&#26550;&#65292;&#20248;&#21270;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#23398;&#20064;&#26159;&#19968;&#31181;&#29992;&#20110;&#25945;&#23548;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#22914;&#20309;&#25191;&#34892;&#20219;&#21153;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290; &#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31215;&#26497;&#21512;&#25104;&#20559;&#22909;&#26597;&#35810;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#22870;&#21169;&#20989;&#25968;&#21442;&#25968;&#30340;&#20449;&#24687;&#22686;&#30410;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290; &#20449;&#24687;&#22686;&#30410;&#26631;&#20934;&#20391;&#37325;&#20110;&#31934;&#30830;&#35782;&#21035;&#22870;&#21169;&#20989;&#25968;&#30340;&#25152;&#26377;&#21442;&#25968;&#12290; &#36825;&#21487;&#33021;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#21442;&#25968;&#21487;&#33021;&#23548;&#33268;&#30456;&#21516;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#35768;&#22810;&#22870;&#21169;&#21487;&#33021;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#30456;&#21516;&#34892;&#20026;&#12290; &#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20248;&#21270;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30452;&#21040;&#34892;&#20026;&#31561;&#25928;&#31867;&#65292;&#20363;&#22914;&#35825;&#23548;&#20986;&#30456;&#21516;&#30340;&#34892;&#20026;&#25490;&#24207;&#65292;&#36873;&#25321;&#20998;&#24067;&#65292;&#25110;&#20854;&#20182;&#30456;&#20851;&#23450;&#20041;&#20351;&#24471;&#20004;&#20010;&#22870;&#21169;&#30475;&#36215;&#26469;&#30456;&#20284;&#12290; &#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#20197;&#25429;&#25417;&#36825;&#31181;&#30456;&#20284;&#24615;&#23450;&#20041;&#30340;&#21487;&#22788;&#29702;&#26694;&#26550;&#12290; &#25105;&#20204;&#22312;&#19968;&#20010;.synthetic env&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06003v1 Announce Type: cross  Abstract: Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic env
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05996</link><description>&lt;p&gt;
&#29992;&#39640;&#26356;&#26032;&#27604;&#20363;&#21078;&#26512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#24212;&#23545;&#20215;&#20540;&#39640;&#20272;&#21644;&#21457;&#25955;
&lt;/p&gt;
&lt;p&gt;
Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35774;&#32622;&#20013;&#21487;&#20197;&#22312;&#26799;&#24230;&#26356;&#26032;&#27425;&#25968;&#22823;&#22823;&#36229;&#36807;&#29615;&#22659;&#26679;&#26412;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23398;&#20064;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#32622;&#32593;&#32476;&#21442;&#25968;&#12290;&#22312;&#36825;&#31181;&#22823;&#37327;&#26356;&#26032;&#19982;&#25968;&#25454;&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23612;&#22522;&#36763;&#31561;&#20154; (2022) &#30340;&#26368;&#36817;&#19968;&#39033;&#30740;&#31350;&#25351;&#20986;&#20102;&#19968;&#20010;&#39318;&#35201;&#20559;&#24046;&#30340;&#20986;&#29616;&#65292;&#21363;&#20195;&#29702;&#22312;&#26089;&#26399;&#20132;&#20114;&#20013;&#36807;&#25311;&#21512;&#24182;&#28129;&#21270;&#21518;&#32493;&#32463;&#39564;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#20854;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#35299;&#26512;&#20102;&#23548;&#33268;&#39318;&#35201;&#20559;&#24046;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#24212;&#35813;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#26159;&#38271;&#26399;&#20197;&#26469;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#20215;&#20540;&#39640;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;Q&#20540;&#19981;&#20165;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#34987;&#39640;&#20272;&#65292;&#32780;&#19988;&#22312;&#20998;&#24067;&#20869;&#25968;&#25454;&#19978;&#20063;&#26159;&#22914;&#27492;&#65292;&#21487;&#20197;&#36861;&#28335;&#21040;&#30001;&#20248;&#21270;&#22120;&#21160;&#37327;&#25512;&#21160;&#30340;&#26410;&#35265;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21333;&#20301;&#29699;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#26356;&#26032;&#27604;&#20363;&#19979;&#23454;&#29616;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05996v1 Announce Type: cross  Abstract: We show that deep reinforcement learning can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples. Under such large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we dissect the phenomena underlying the primacy bias. We inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum. We employ a simple unit-ball normalization that enables learning under large update ratios
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;Q&#23398;&#20064;&#21644;SARSA&#23398;&#20064;&#65292;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#20197;&#22686;&#24378;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;87%&#21644;88%&#30340;&#26368;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.05979</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#29305;&#24449;&#36873;&#25321;&#30340;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Classification Performance via Reinforcement Learning for Feature Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05979
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;Q&#23398;&#20064;&#21644;SARSA&#23398;&#20064;&#65292;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#20197;&#22686;&#24378;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;87%&#21644;88%&#30340;&#26368;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#24182;&#36807;&#28388;&#26080;&#20851;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26377;&#25928;&#29305;&#24449;&#36873;&#25321;&#22312;&#25552;&#21319;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;Q&#23398;&#20064;&#65288;QL&#65289;&#21644;SARSA&#23398;&#20064;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#29305;&#24449;&#36873;&#25321;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#20083;&#33146;&#30284;&#31185;&#33521;&#24067;&#25289;&#25968;&#25454;&#38598;&#65288;BCCDS&#65289;&#21644;&#19977;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#65288;&#26368;&#23567;-&#26368;&#22823;&#65292;l1&#21644;l2&#65289;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;QL@&#26368;&#23567;-&#26368;&#22823;&#21644;SARSA@l2&#20998;&#21035;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;87%&#21644;88%&#12290;&#36825;&#31361;&#26174;&#20102;&#22522;&#20110;RL&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#20248;&#21270;&#20998;&#31867;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05979v1 Announce Type: new  Abstract: Feature selection plays a crucial role in improving predictive accuracy by identifying relevant features while filtering out irrelevant ones. This study investigates the importance of effective feature selection in enhancing the performance of classification models. By employing reinforcement learning (RL) algorithms, specifically Q-learning (QL) and SARSA learning, this paper addresses the feature selection challenge. Using the Breast Cancer Coimbra dataset (BCCDS) and three normalization methods (Min-Max, l1, and l2), the study evaluates the performance of these algorithms. Results show that QL@Min-Max and SARSA@l2 achieve the highest classification accuracies, reaching 87% and 88%, respectively. This highlights the effectiveness of RL-based feature selection methods in optimizing classification tasks, contributing to improved model accuracy and efficiency.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;APRICOT&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35774;&#32622;&#32622;&#20449;&#30446;&#26631;&#24182;&#35757;&#32451;&#39069;&#22806;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.05973</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#29983;&#25104;&#26469;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Calibrating Large Language Models Using Their Generations Only
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05973
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;APRICOT&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35774;&#32622;&#32622;&#20449;&#30446;&#26631;&#24182;&#35757;&#32451;&#39069;&#22806;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#38754;&#21521;&#29992;&#25143;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#36890;&#36807;&#20934;&#30830;&#37327;&#21270;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#20449;&#24515;&#26469;&#24314;&#31435;&#20449;&#20219;&#24182;&#20445;&#25345;&#23433;&#20840;&#24615;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26657;&#20934;LLMs - &#23588;&#20854;&#26159;&#24403;&#19982;&#27169;&#22411;&#30340;&#21807;&#19968;&#25509;&#21475;&#26159;&#23427;&#20204;&#29983;&#25104;&#30340;&#25991;&#26412;&#26102; - &#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;APRICOT&#65288;&#36741;&#21161;&#39044;&#27979;&#32622;&#20449;&#30446;&#26631;&#65289;&#65306;&#19968;&#31181;&#36890;&#36807;&#20165;&#20351;&#29992;&#20854;&#25991;&#26412;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35774;&#32622;&#32622;&#20449;&#30446;&#26631;&#24182;&#35757;&#32451;&#19968;&#20010;&#39069;&#22806;&#27169;&#22411;&#26469;&#39044;&#27979;LLM&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#27010;&#24565;&#19978;&#31616;&#21333;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#36229;&#20986;&#20854;&#36755;&#20986;&#65292;&#19981;&#24178;&#25200;&#35821;&#35328;&#29983;&#25104;&#65292;&#24182;&#19988;&#26377;&#22810;&#31181;&#28508;&#22312;&#29992;&#36884;&#65292;&#20363;&#22914;&#36890;&#36807;&#35328;&#35821;&#21270;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#25110;&#26681;&#25454;&#32622;&#20449;&#24230;&#35843;&#25972;&#32473;&#23450;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05973v1 Announce Type: cross  Abstract: As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important. However, finding effective ways to calibrate LLMs - especially when the only interface to the models is their generated text - remains a challenge. We propose APRICOT (auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence. We show how our approach performs competitively
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#35821;&#20041;&#19968;&#33268;&#30340;&#22270;&#20687;&#22686;&#24378;&#65292;&#20016;&#23500;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#39640;&#20102;&#23398;&#21040;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.05966</link><description>&lt;p&gt;
&#33021;&#29983;&#25104;&#27169;&#22411;&#25913;&#36827;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Generative Models Improve Self-Supervised Representation Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#35821;&#20041;&#19968;&#33268;&#30340;&#22270;&#20687;&#22686;&#24378;&#65292;&#20016;&#23500;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#39640;&#20102;&#23398;&#21040;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#31361;&#26174;&#20102;&#20854;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#24378;&#22823;&#35270;&#35273;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#21033;&#29992;&#21516;&#19968;&#22270;&#20687;&#30340;&#19981;&#21516;&#35270;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#38480;&#21046;&#20102;&#21464;&#25442;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#23548;&#33268;&#34920;&#31034;&#19981;&#22815;&#20248;&#21270;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#35821;&#20041;&#19968;&#33268;&#30340;&#22270;&#20687;&#22686;&#24378;&#65292;&#20016;&#23500;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#12290;&#36890;&#36807;&#30452;&#25509;&#22312;&#28304;&#22270;&#20687;&#34920;&#31034;&#19978;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#22686;&#24378;&#65292;&#21516;&#26102;&#20445;&#25345;&#28304;&#22270;&#20687;&#30340;&#35821;&#20041;&#65292;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#21040;&#30340;&#35270;&#35273;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05966v1 Announce Type: cross  Abstract: The rapid advancement in self-supervised learning (SSL) has highlighted its potential to leverage unlabeled data for learning powerful visual representations. However, existing SSL approaches, particularly those employing different views of the same image, often rely on a limited set of predefined data augmentations. This constrains the diversity and quality of transformations, which leads to sub-optimal representations. In this paper, we introduce a novel framework that enriches the SSL paradigm by utilizing generative models to produce semantically consistent image augmentations. By directly conditioning generative models on a source image representation, our method enables the generation of diverse augmentations while maintaining the semantics of the source image, thus offering a richer set of data for self-supervised learning. Our experimental results demonstrate that our framework significantly enhances the quality of learned visu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#24773;&#32490;&#25512;&#29702;&#65288;CLEF&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#20559;&#24046;&#24178;&#25200;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.05963</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#21435;&#20559;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robust Emotion Recognition in Context Debiasing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05963
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#24773;&#32490;&#25512;&#29702;&#65288;CLEF&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#20559;&#24046;&#24178;&#25200;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#24773;&#32490;&#35782;&#21035;&#65288;CAER&#65289;&#26368;&#36817;&#22312;&#26080;&#32422;&#26463;&#29615;&#22659;&#20013;&#25512;&#21160;&#20102;&#24773;&#24863;&#35745;&#31639;&#25216;&#26415;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290; &#20027;&#27969;&#30340;CAER&#26041;&#27861;&#24635;&#26159;&#20174;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#21644;&#20197;&#20027;&#20307;&#20026;&#20013;&#24515;&#30340;&#29305;&#24449;&#20013;&#25552;&#21462;&#38598;&#25104;&#34920;&#31034;&#65292;&#20197;&#24863;&#30693;&#30446;&#26631;&#20154;&#29289;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290; &#23613;&#31649;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#26368;&#22823;&#30340;&#25361;&#25112;&#20173;&#28982;&#26159;&#30001;&#20110;&#19978;&#19979;&#25991;&#20559;&#24046;&#30340;&#24178;&#25200;&#12290; &#26377;&#23475;&#30340;&#20559;&#35265;&#36843;&#20351;&#27169;&#22411;&#20381;&#36182;&#20110;&#32972;&#26223;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22312;&#21487;&#33021;&#24615;&#20272;&#35745;&#20013;&#36896;&#25104;&#20005;&#37325;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#20351;&#26377;&#20215;&#20540;&#30340;&#19978;&#19979;&#25991;&#20808;&#39564;&#28151;&#28102;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#24773;&#32490;&#25512;&#29702;&#65288;CLEF&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#24191;&#20041;&#22240;&#26524;&#22270;&#65292;&#20197;&#35299;&#32806;CAER&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290; &#36981;&#24490;&#22240;&#26524;&#22270;&#65292;CLEF&#24341;&#20837;&#20102;&#19968;&#20010;&#38750;&#20405;&#20837;&#24335;&#30340;&#19978;&#19979;&#25991;&#20998;&#25903;&#26469;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05963v1 Announce Type: cross  Abstract: Context-aware emotion recognition (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person's emotional state. Despite advancements, the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and emotion labels in likelihood estimation, causing severe performance bottlenecks and confounding valuable context priors. In this paper, we propose a counterfactual emotion inference (CLEF) framework to address the above issue. Specifically, we first formulate a generalized causal graph to decouple the causal relationships among the variables in CAER. Following the causal graph, CLEF introduces a non-invasive context branch to capt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#28304;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22806;&#31185;&#24212;&#29992;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;GSViT&#65289;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;Cholec80&#38454;&#27573;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05949</link><description>&lt;p&gt;
&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65306;&#29992;&#20110;&#36890;&#29992;&#22806;&#31185;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General surgery vision transformer: A video pre-trained foundation model for general surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#28304;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22806;&#31185;&#24212;&#29992;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;GSViT&#65289;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;Cholec80&#38454;&#27573;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#24320;&#25918;&#33719;&#21462;&#30340;&#25968;&#25454;&#21644;&#19987;&#38376;&#30340;&#22522;&#30784;&#27169;&#22411;&#26159;&#22806;&#31185;&#35745;&#31639;&#30740;&#31350;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#28304;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;28&#31181;&#25163;&#26415;&#25216;&#26415;&#30340;680&#23567;&#26102;&#25163;&#26415;&#35270;&#39057;&#25968;&#25454;&#65307;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#21521;&#35270;&#39057;&#39044;&#27979;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;GSViT&#65289;&#35270;&#39057;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#23454;&#26102;&#36816;&#34892;&#29992;&#20110;&#22806;&#31185;&#24212;&#29992;&#65292;&#25105;&#20204;&#36824;&#24320;&#28304;&#20102;GSViT&#30340;&#20195;&#30721;&#21644;&#26435;&#37325;&#65307;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#38024;&#23545;10&#31181;&#25163;&#26415;&#31243;&#24207;&#30340;&#29305;&#23450;&#31243;&#24207;&#24494;&#35843;&#29256;&#26412;&#30340;GSViT&#30340;&#20195;&#30721;&#21644;&#26435;&#37325;&#65307;&#25105;&#20204;&#23637;&#31034;&#20102;GSViT&#22312;Cholec80&#38454;&#27573;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21333;&#24103;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05949v1 Announce Type: cross  Abstract: The absence of openly accessible data and specialized foundation models is a major barrier for computational research in surgery. Toward this, (i) we open-source the largest dataset of general surgery videos to-date, consisting of 680 hours of surgical videos, including data from robotic and laparoscopic techniques across 28 procedures; (ii) we propose a technique for video pre-training a general surgery vision transformer (GSViT) on surgical videos based on forward video prediction that can run in real-time for surgical applications, toward which we open-source the code and weights of GSViT; (iii) we also release code and weights for procedure-specific fine-tuned versions of GSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the Cholec80 phase annotation task, displaying improved performance over state-of-the-art single frame predictors.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#23545;&#35805;&#20013;&#30340;&#32447;&#31243;&#24182;&#22522;&#20110;&#20854;&#37325;&#35201;&#24615;&#20248;&#20808;&#29983;&#25104;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.05931</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#25552;&#31034;&#30340;Transformer&#36827;&#34892;&#32447;&#31243;&#26816;&#27979;&#21644;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Thread Detection and Response Generation using Transformers with Prompt Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05931
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#23545;&#35805;&#20013;&#30340;&#32447;&#31243;&#24182;&#22522;&#20110;&#20854;&#37325;&#35201;&#24615;&#20248;&#20808;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Conversational systems &#23545;&#20110;&#20154;&#26426;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#36807;&#35782;&#21035;&#32447;&#31243;&#24182;&#20248;&#20808;&#21709;&#24212;&#26469;&#31649;&#29702;&#22797;&#26434;&#23545;&#35805;&#12290; &#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#20013;&#32447;&#31243;&#30340;&#20934;&#30830;&#35782;&#21035;&#21644;&#31574;&#30053;&#24615;&#21709;&#24212;&#20248;&#20808;&#32423;&#30830;&#20445;&#39640;&#25928;&#23545;&#35805;&#31649;&#29702;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26681;&#25454;&#37325;&#35201;&#24615;&#35782;&#21035;&#32447;&#31243;&#24182;&#20248;&#20808;&#29983;&#25104;&#20854;&#21709;&#24212;&#65292;&#28041;&#21450;&#23558;&#38382;&#39064;&#31995;&#32479;&#22320;&#20998;&#35299;&#20026;&#31163;&#25955;&#32452;&#20214; - &#32447;&#31243;&#26816;&#27979;&#12289;&#20248;&#20808;&#32423;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#31934;&#24515;&#20998;&#26512;&#21644;&#20248;&#21270;&#12290; &#36825;&#20123;&#31934;&#32454;&#30340;&#32452;&#20214;&#26080;&#32541;&#38598;&#25104;&#21040;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#12290; &#30001;&#20110;&#20854;&#39640;&#36890;&#29992;&#24615;&#65292;Llama2 7b&#34987;&#29992;&#20110;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM) &#36827;&#34892;&#26356;&#26032;&#12290; Llama2 &#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#24471;&#21040;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05931v1 Announce Type: new  Abstract: Conversational systems are crucial for human-computer interaction, managing complex dialogues by identifying threads and prioritising responses. This is especially vital in multi-party conversations, where precise identification of threads and strategic response prioritisation ensure efficient dialogue management. To address these challenges an end-to-end model that identifies threads and prioritises their response generation based on the importance was developed, involving a systematic decomposition of the problem into discrete components - thread detection, prioritisation, and performance optimisation which was meticulously analysed and optimised. These refined components seamlessly integrate into a unified framework, in conversational systems. Llama2 7b is used due to its high level of generalisation but the system can be updated with any open source Large Language Model(LLM). The computational capabilities of the Llama2 model was aug
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#32463;&#20856;&#36807;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#27169;&#24335;&#22604;&#38519;&#19982;&#35757;&#32451;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05918</link><description>&lt;p&gt;
&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05918
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#32463;&#20856;&#36807;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#27169;&#24335;&#22604;&#38519;&#19982;&#35757;&#32451;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#26080;&#27861;&#26377;&#25928;&#23398;&#20064;&#12290;&#20026;&#20102;&#24179;&#34913;&#27169;&#22411;&#35757;&#32451;&#21069;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#36890;&#24120;&#20351;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#20026;&#23569;&#25968;&#31867;&#29983;&#25104;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#20998;&#31867;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#32463;&#20856;&#30340;&#36807;&#37319;&#26679;&#26041;&#27861;&#22522;&#20110;SMOTE&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20165;&#20851;&#27880;&#25968;&#25454;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#19981;&#22815;&#36924;&#30495;&#30340;&#38382;&#39064;&#12290;&#22312;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#24403;&#21069;&#36807;&#37319;&#26679;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#30495;&#23454;&#20998;&#24067;&#65292;&#20294;&#35757;&#32451;&#20013;&#23384;&#22312;&#27169;&#24335;&#23849;&#28291;&#21644;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#65307;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#36807;&#37319;&#26679;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;U-Net&#30340;&#36870;&#25193;&#25955;&#36807;&#31243;&#31070;&#32463;&#32593;&#32476;&#19981;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05918v1 Announce Type: cross  Abstract: In the field of data mining and machine learning, commonly used classification models cannot effectively learn in unbalanced data. In order to balance the data distribution before model training,oversamplingmethods are often used to generate data for a small number of classes to solve the problem of classifying unbalanced data. Most of the classical oversampling methods are based on theSMOTE technique, which only focuses on the local information of the data, and therefore the generated data may have the problem of not being realistic enough. In the current oversampling methods based on generative networks, the methods based on GANs can capture the true distribution of data, but there is the problem of pattern collapse and training instability in training; in the oversampling methods based on denoising diffusion probability models, the neural network of the inverse diffusion process using the U-Net is not applicable to tabular data, and
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36882;&#24402;&#22312;&#32447;&#20272;&#35745;&#31639;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#38543;&#26426;&#36924;&#36817;&#35782;&#21035;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#21442;&#25968;Wiener&#27169;&#22411;&#65292;&#20855;&#26377;&#23545;&#25200;&#21160;&#36807;&#31243;&#39057;&#35889;&#20551;&#35774;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05899</link><description>&lt;p&gt;
&#20351;&#29992;&#37319;&#26679;&#25968;&#25454;&#22312;&#32447;&#35782;&#21035;&#38543;&#26426;&#36830;&#32493;&#26102;&#38388;Wiener&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Online Identification of Stochastic Continuous-Time Wiener Models Using Sampled Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05899
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36882;&#24402;&#22312;&#32447;&#20272;&#35745;&#31639;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#38543;&#26426;&#36924;&#36817;&#35782;&#21035;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#21442;&#25968;Wiener&#27169;&#22411;&#65292;&#20855;&#26377;&#23545;&#25200;&#21160;&#36807;&#31243;&#39057;&#35889;&#20551;&#35774;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22312;&#35782;&#21035;&#38543;&#26426;Wiener&#27169;&#22411;&#20013;&#24573;&#30053;&#38543;&#26426;&#25200;&#21160;&#30340;&#23384;&#22312;&#20250;&#23548;&#33268;&#28176;&#36817;&#20559;&#20506;&#30340;&#20272;&#35745;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#22522;&#20110;&#20284;&#28982;&#30340;&#26041;&#27861;&#36827;&#34892;&#26368;&#20248;&#32479;&#35745;&#35782;&#21035;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#20551;&#35774;&#25935;&#24863;&#65292;&#36890;&#24120;&#22522;&#20110;&#30456;&#23545;&#22797;&#26434;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#12290;&#25105;&#20204;&#22522;&#20110;&#36755;&#20986;&#35823;&#24046;&#39044;&#27979;&#22120;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36882;&#24402;&#22312;&#32447;&#20272;&#35745;&#31639;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#38543;&#26426;&#36924;&#36817;&#35782;&#21035;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#21442;&#25968;Wiener&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#36890;&#29992;&#27169;&#22411;&#21442;&#25968;&#21270;&#65292;&#24182;&#19988;&#27491;&#22914;&#25968;&#20540;&#27169;&#25311;&#31034;&#20363;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;&#23427;&#23545;&#25200;&#21160;&#36807;&#31243;&#39057;&#35889;&#30340;&#20551;&#35774;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05899v1 Announce Type: cross  Abstract: It is well known that ignoring the presence of stochastic disturbances in the identification of stochastic Wiener models leads to asymptotically biased estimators. On the other hand, optimal statistical identification, via likelihood-based methods, is sensitive to the assumptions on the data distribution and is usually based on relatively complex sequential Monte Carlo algorithms. We develop a simple recursive online estimation algorithm based on an output-error predictor, for the identification of continuous-time stochastic parametric Wiener models through stochastic approximation. The method is applicable to generic model parameterizations and, as demonstrated in the numerical simulation examples, it is robust with respect to the assumptions on the spectrum of the disturbance process.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Fed&#30340;&#31616;&#21333;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#37325;&#25773;&#65292;&#36890;&#36807;&#21327;&#35843;&#27599;&#20010;&#23458;&#25143;&#31471;&#32531;&#23384;&#37325;&#35201;&#26679;&#26412;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05890</link><description>&lt;p&gt;
&#26397;&#30528;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#20013;&#39640;&#25928;&#30340;&#37325;&#25773;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Replay in Federated Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Fed&#30340;&#31616;&#21333;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#37325;&#25773;&#65292;&#36890;&#36807;&#21327;&#35843;&#27599;&#20010;&#23458;&#25143;&#31471;&#32531;&#23384;&#37325;&#35201;&#26679;&#26412;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#36890;&#24120;&#20551;&#23450;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#26159;&#22266;&#23450;&#25110;&#38745;&#24577;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#36890;&#24120;&#20197;&#22686;&#37327;&#26041;&#24335;&#21040;&#26469;&#65292;&#20854;&#20013;&#25968;&#25454;&#39046;&#22495;&#21487;&#33021;&#21160;&#24577;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36793;&#32536;&#23458;&#25143;&#31471;&#22312;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#65288;FIL&#65289;&#22330;&#26223;&#20013;&#22240;&#25968;&#25454;&#24322;&#26500;&#24615;&#32780;&#21487;&#33021;&#32570;&#20047;&#36275;&#22815;&#23384;&#20648;&#31354;&#38388;&#20197;&#20445;&#30041;&#23436;&#25972;&#25968;&#25454;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Fed&#30340;&#31616;&#21333;&#12289;&#36890;&#29992;&#30340;FIL&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#21327;&#35843;&#27599;&#20010;&#23458;&#25143;&#31471;&#32531;&#23384;&#37325;&#25773;&#30340;&#37325;&#35201;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20986;&#29616;&#26032;&#20219;&#21153;&#26102;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#39318;&#20808;&#22522;&#20110;&#23427;&#20204;&#30340;&#20840;&#23616;&#21644;&#26412;&#22320;&#37325;&#35201;&#24615;&#32531;&#23384;&#36873;&#23450;&#30340;&#20808;&#21069;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#23458;&#25143;&#31471;&#20351;&#29992;&#26082;&#32531;&#23384;&#30340;&#26679;&#26412;&#21448;&#20351;&#29992;&#26032;&#20219;&#21153;&#30340;&#26679;&#26412;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;Re-Fed&#21457;&#29616;&#37325;&#25773;&#37325;&#35201;&#26679;&#26412;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05890v1 Announce Type: new  Abstract: In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we e
&lt;/p&gt;</description></item><item><title>DiffRed &#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#31283;&#23450;&#31209;&#24341;&#23548;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312; Stress &#21644; M1 &#19978;&#21462;&#24471;&#20102;&#36739;&#32039;&#23494;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.05882</link><description>&lt;p&gt;
DiffRed: &#30001;&#31283;&#23450;&#31209;&#24341;&#23548;&#30340;&#38477;&#32500;
&lt;/p&gt;
&lt;p&gt;
DiffRed: Dimensionality Reduction guided by stable rank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05882
&lt;/p&gt;
&lt;p&gt;
DiffRed &#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#31283;&#23450;&#31209;&#24341;&#23548;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312; Stress &#21644; M1 &#19978;&#21462;&#24471;&#20102;&#36739;&#32039;&#23494;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38477;&#32500;&#25216;&#26415; DiffRed&#65292;&#35813;&#25216;&#26415;&#39318;&#20808;&#27839;&#30528;&#21069; $k_1$ &#20010;&#20027;&#25104;&#20998;&#25237;&#24433;&#25968;&#25454;&#30697;&#38453; A&#65292;&#28982;&#21518;&#23558;&#21097;&#20313;&#30697;&#38453; $A^{*}$&#65288;&#20943;&#21435;&#20854; $k_1$-&#31209;&#36817;&#20284;&#21518;&#21097;&#20313;&#30340;&#37096;&#20998;&#65289;&#27839;&#30528; $k_2$ &#20010;&#39640;&#26031;&#38543;&#26426;&#21521;&#37327;&#36827;&#34892;&#25237;&#24433;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102; M1&#65292;&#21363;&#22343;&#26041;&#23545;&#36317;&#31163;&#22833;&#30495;&#65292;&#20197;&#21450; Stress&#65292;&#21363;&#23545;&#25104;&#23545;&#36317;&#31163;&#22833;&#30495;&#30340;&#22343;&#26041;&#26681;&#20540;&#30340;&#24402;&#19968;&#21270;&#20540;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126; DiffRed &#22312; Stress &#19978;&#21462;&#24471;&#20102; $O\left(\sqrt{\frac{1-p}{k_2}}\right)$ &#30340;&#19968;&#33324;&#19978;&#30028;&#65292;&#22312; M1 &#19978;&#21462;&#24471;&#20102; $O\left(\frac{(1-p)}{\sqrt{k_2*\rho(A^{*})}}\right)$ &#30340;&#19968;&#33324;&#19978;&#30028;&#65292;&#20854;&#20013;$p$ &#26159;&#30001;&#21069; $k_1$ &#20010;&#20027;&#25104;&#20998;&#35299;&#37322;&#30340;&#26041;&#24046;&#20998;&#25968;&#65292;$\rho(A^{*})$ &#26159; $A^{*}$ &#30340;&#31283;&#23450;&#31209;&#12290;&#36825;&#20123;&#19978;&#30028;&#27604;&#30446;&#21069;&#24050;&#30693;&#30340;&#38543;&#26426;&#26144;&#23556;&#32467;&#26524;&#26356;&#32039;&#23494;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;DiffRed &#23454;&#29616;&#20102;&#25509;&#36817;&#38646;&#30340; M1 &#21644;&#26356;&#20302;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05882v1 Announce Type: new  Abstract: In this work, we propose a novel dimensionality reduction technique, DiffRed, which first projects the data matrix, A, along first $k_1$ principal components and the residual matrix $A^{*}$ (left after subtracting its $k_1$-rank approximation) along $k_2$ Gaussian random vectors. We evaluate M1, the distortion of mean-squared pair-wise distance, and Stress, the normalized value of RMS of distortion of the pairwise distances. We rigorously prove that DiffRed achieves a general upper bound of $O\left(\sqrt{\frac{1-p}{k_2}}\right)$ on Stress and $O\left(\frac{(1-p)}{\sqrt{k_2*\rho(A^{*})}}\right)$ on M1 where $p$ is the fraction of variance explained by the first $k_1$ principal components and $\rho(A^{*})$ is the stable rank of $A^{*}$. These bounds are tighter than the currently known results for Random maps. Our extensive experiments on a variety of real-world datasets demonstrate that DiffRed achieves near zero M1 and much lower values 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;A&#27169;&#24335;&#36229;&#22768;&#27874;&#65288;US&#65289;&#25913;&#36827;&#39592;&#39612;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05879</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22768;&#23398;&#27979;&#37327;&#26041;&#27861;&#22312;&#39592;&#31185;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning based acoustic measurement approach for robotic applications on orthopedics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;A&#27169;&#24335;&#36229;&#22768;&#27874;&#65288;US&#65289;&#25913;&#36827;&#39592;&#39612;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#33181;&#20851;&#33410;&#32622;&#25442;&#25163;&#26415;&#65288;TKA&#65289;&#20013;&#65292;&#22806;&#31185;&#26426;&#22120;&#20154;&#21487;&#20197;&#25552;&#20379;&#22270;&#20687;&#24341;&#23548;&#23548;&#33322;&#65292;&#20197;&#39640;&#31934;&#24230;&#23433;&#35013;&#26893;&#20837;&#29289;&#12290;&#20854;&#36319;&#36394;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#20110;&#23558;&#39592;&#38024;&#25554;&#20837;&#30001;&#20809;&#23398;&#36319;&#36394;&#31995;&#32479;&#36319;&#36394;&#30340;&#39592;&#39612;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#39592;&#39612;&#36319;&#36394;&#20934;&#30830;&#24615;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65292;&#36890;&#36807;A&#27169;&#24335;&#36229;&#22768;&#27874;&#65288;US&#65289;&#36827;&#34892;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#23608;&#20307;&#23454;&#39564;&#20013;&#33719;&#24471;&#20102;&#19968;&#32452;&#36229;&#22768;&#27874;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#39592;&#38024;&#35745;&#31639;&#24471;&#20986;&#39592;&#39612;&#30340;&#30495;&#23454;&#20301;&#32622;&#12290;&#36825;&#20123;&#25968;&#25454;&#34987;&#29992;&#26469;&#35757;&#32451;&#25105;&#20204;&#25552;&#20986;&#30340;CasAtt-UNet&#65292;&#33258;&#21160;&#21644;&#31283;&#20581;&#22320;&#39044;&#27979;&#39592;&#39612;&#20301;&#32622;&#12290;&#30495;&#23454;&#30340;&#39592;&#39612;&#20301;&#32622;&#21644;US&#30340;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05879v1 Announce Type: cross  Abstract: In Total Knee Replacement Arthroplasty (TKA), surgical robotics can provide image-guided navigation to fit implants with high precision. Its tracking approach highly relies on inserting bone pins into the bones tracked by the optical tracking system. This is normally done by invasive, radiative manners (implantable markers and CT scans), which introduce unnecessary trauma and prolong the preparation time for patients. To tackle this issue, ultrasound-based bone tracking could offer an alternative. In this study, we proposed a novel deep learning structure to improve the accuracy of bone tracking by an A-mode ultrasound (US). We first obtained a set of ultrasound dataset from the cadaver experiment, where the ground truth locations of bones were calculated using bone pins. These data were used to train the proposed CasAtt-UNet to predict bone location automatically and robustly. The ground truth bone locations and those locations of US 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Legion&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTMs&#65289;&#20026;GitHub&#23384;&#20648;&#24211;&#25512;&#33616;&#20027;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#20027;&#39064;&#25512;&#33616;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05873</link><description>&lt;p&gt;
LEGION&#65306;&#21033;&#29992;&#20998;&#24067;&#24179;&#34913;&#25439;&#22833;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;GitHub&#20027;&#39064;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05873
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Legion&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTMs&#65289;&#20026;GitHub&#23384;&#20648;&#24211;&#25512;&#33616;&#20027;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#20027;&#39064;&#25512;&#33616;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#24320;&#21457;&#36890;&#36807;&#20419;&#36827;&#21327;&#20316;&#12289;&#36879;&#26126;&#24615;&#21644;&#31038;&#21306;&#39537;&#21160;&#30340;&#21019;&#26032;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#36719;&#20214;&#34892;&#19994;&#12290;&#22914;&#20170;&#65292;&#22823;&#37327;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#28304;&#36719;&#20214;&#65292;&#24418;&#25104;&#20102;&#32593;&#32476;&#23384;&#20648;&#24211;&#65292;&#36890;&#24120;&#25176;&#31649;&#22312;GitHub&#19978;-&#19968;&#31181;&#27969;&#34892;&#30340;&#36719;&#20214;&#24320;&#21457;&#24179;&#21488;&#12290;&#20026;&#20102;&#22686;&#24378;&#23384;&#20648;&#24211;&#32593;&#32476;&#30340;&#21487;&#21457;&#29616;&#24615;&#65292;&#21363;&#30456;&#20284;&#23384;&#20648;&#24211;&#32452;&#65292;GitHub&#22312;2017&#24180;&#24341;&#20837;&#20102;&#23384;&#20648;&#24211;&#20027;&#39064;&#65292;&#20351;&#29992;&#25143;&#26356;&#23481;&#26131;&#25353;&#31867;&#22411;&#12289;&#25216;&#26415;&#31561;&#27983;&#35272;&#30456;&#20851;&#39033;&#30446;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#20026;&#27599;&#20010;GitHub&#23384;&#20648;&#24211;&#20998;&#37197;&#20027;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#29992;&#20110;&#33258;&#21160;&#20027;&#39064;&#25512;&#33616;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;TF-IDF&#26469;&#23545;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#65292;&#23384;&#22312;&#29702;&#35299;&#35821;&#20041;&#32454;&#24494;&#24046;&#21035;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;Legion&#65292;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTMs&#65289;&#25512;&#33616;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05873v1 Announce Type: cross  Abstract: Open-source development has revolutionized the software industry by promoting collaboration, transparency, and community-driven innovation. Today, a vast amount of various kinds of open-source software, which form networks of repositories, is often hosted on GitHub - a popular software development platform. To enhance the discoverability of the repository networks, i.e., groups of similar repositories, GitHub introduced repository topics in 2017 that enable users to more easily explore relevant projects by type, technology, and more. It is thus crucial to accurately assign topics for each GitHub repository. Current methods for automatic topic recommendation rely heavily on TF-IDF for encoding textual data, presenting challenges in understanding semantic nuances. This paper addresses the limitations of existing techniques by proposing Legion, a novel approach that leverages Pre-trained Language Models (PTMs) for recommending topics for 
&lt;/p&gt;</description></item><item><title>PAPER-HILT&#26159;&#38024;&#23545;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;&#21019;&#26032;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#21069;&#36864;&#20986;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#38544;&#31169;&#20445;&#25252;&#21644;&#31995;&#32479;&#25928;&#29992;&#65292;&#20197;&#36866;&#24212;&#20010;&#20307;&#34892;&#20026;&#27169;&#24335;&#21644;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.05864</link><description>&lt;p&gt;
PAPER-HILT&#65306;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#38544;&#31169;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;&#25552;&#21069;&#36864;&#20986;&#22312;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for Reinforcement Learning in Human-in-the-Loop Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05864
&lt;/p&gt;
&lt;p&gt;
PAPER-HILT&#26159;&#38024;&#23545;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;&#21019;&#26032;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#21069;&#36864;&#20986;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#38544;&#31169;&#20445;&#25252;&#21644;&#31995;&#32479;&#25928;&#29992;&#65292;&#20197;&#36866;&#24212;&#20010;&#20307;&#34892;&#20026;&#27169;&#24335;&#21644;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26085;&#30410;&#25104;&#20026;&#20154;&#26426;&#21327;&#21516;&#65288;HITL&#65289;&#24212;&#29992;&#20013;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#22240;&#20854;&#36866;&#24212;&#20110;&#20154;&#31867;&#20132;&#20114;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#25972;&#21512;RL&#20250;&#24102;&#26469;&#37325;&#22823;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#26292;&#38706;&#25935;&#24863;&#29992;&#25143;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#19987;&#27880;&#20110;&#24320;&#21457;PAPER-HILT&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36866;&#24212;RL&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#20026;HITL&#29615;&#22659;&#20013;&#38544;&#31169;&#20445;&#25252;&#35774;&#35745;&#30340;&#25552;&#21069;&#36864;&#20986;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#38544;&#31169;&#20445;&#25252;&#21644;&#31995;&#32479;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20351;&#20854;&#25805;&#20316;&#36866;&#24212;&#20010;&#20154;&#34892;&#20026;&#27169;&#24335;&#21644;&#20559;&#22909;&#12290;&#25105;&#20204;&#20027;&#35201;&#24378;&#35843;&#38754;&#20020;&#22788;&#29702;&#20154;&#31867;&#34892;&#20026;&#30340;&#21487;&#21464;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#25361;&#25112;&#65292;&#20351;&#24471;&#38745;&#24577;&#38544;&#31169;&#27169;&#22411;&#22833;&#25928;&#12290;&#36890;&#36807;&#20854;&#24212;&#29992;&#65292;&#35780;&#20272;&#20102;PAPER-HILT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05864v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has increasingly become a preferred method over traditional rule-based systems in diverse human-in-the-loop (HITL) applications due to its adaptability to the dynamic nature of human interactions. However, integrating RL in such settings raises significant privacy concerns, as it might inadvertently expose sensitive user information. Addressing this, our paper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy through exploiting an early-exit approach designed explicitly for privacy preservation in HITL environments. This approach dynamically adjusts the tradeoff between privacy protection and system utility, tailoring its operation to individual behavioral patterns and preferences. We mainly highlight the challenge of dealing with the variable and evolving nature of human behavior, which renders static privacy models ineffective. PAPER-HILT's effectiveness is evaluated through its applicati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#28909;&#21147;&#23398;&#23450;&#24459;&#30340;&#25968;&#25454;&#39537;&#21160;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#28508;&#21464;&#37327;&#24182;&#26500;&#24314;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#28909;&#21147;&#23398;&#23450;&#24459;&#30340;&#36981;&#23432;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#28436;&#31034;&#20102;&#20854;&#22312;&#31283;&#20581;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#22312;&#28508;&#31354;&#38388;&#20013;&#29109;&#20135;&#29983;&#36895;&#29575;&#19982;&#31995;&#32479;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05848</link><description>&lt;p&gt;
tLaSDI: &#28909;&#21147;&#23398;&#20449;&#24687;&#39537;&#21160;&#30340;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
tLaSDI: Thermodynamics-informed latent space dynamics identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05848
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#28909;&#21147;&#23398;&#23450;&#24459;&#30340;&#25968;&#25454;&#39537;&#21160;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#28508;&#21464;&#37327;&#24182;&#26500;&#24314;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#28909;&#21147;&#23398;&#23450;&#24459;&#30340;&#36981;&#23432;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#28436;&#31034;&#20102;&#20854;&#22312;&#31283;&#20581;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#22312;&#28508;&#31354;&#38388;&#20013;&#29109;&#20135;&#29983;&#36895;&#29575;&#19982;&#31995;&#32479;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65288;tLaSDI&#65289;&#65292;&#35813;&#26041;&#27861;&#34701;&#20837;&#20102;&#28909;&#21147;&#23398;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#23450;&#24459;&#12290;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#28508;&#21464;&#37327;&#20316;&#20026;&#38750;&#32447;&#24615;&#38477;&#32500;&#27169;&#22411;&#12290;&#28508;&#21464;&#37327;&#30340;&#21160;&#21147;&#23398;&#30001;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#26500;&#24314;&#65292;&#36890;&#36807;&#36890;&#29992;&#24418;&#24335;&#20027;&#20041;&#20445;&#30041;&#26576;&#20123;&#32467;&#26500;&#20197;&#23562;&#37325;&#28909;&#21147;&#23398;&#23450;&#24459;&#12290;&#24314;&#31435;&#20102;&#23545;&#36817;&#20284;&#20540;&#30340;&#25277;&#35937;&#35823;&#24046;&#20272;&#35745;&#65292;&#25552;&#20379;&#20102;&#28041;&#21450;&#33258;&#21160;&#32534;&#30721;&#22120;&#38597;&#21487;&#27604;&#35745;&#31639;&#30340;&#26032;&#25439;&#22833;&#21046;&#23450;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#28508;&#21160;&#24577;&#37117;&#32463;&#36807;&#35757;&#32451;&#20197;&#26368;&#23567;&#21270;&#26032;&#30340;&#25439;&#22833;&#12290;&#23637;&#31034;&#20102;&#25968;&#20540;&#31034;&#20363;&#20197;&#28436;&#31034;tLaSDI&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#22806;&#25512;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#28508;&#31354;&#38388;&#20013;&#35266;&#23519;&#21040;&#20102;&#29109;&#20135;&#29983;&#36895;&#29575;&#19982;&#23436;&#25972;&#34892;&#20026;&#20043;&#38388;&#30340;&#26377;&#36259;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05848v1 Announce Type: new  Abstract: We propose a data-driven latent space dynamics identification method (tLaSDI) that embeds the first and second principles of thermodynamics. The latent variables are learned through an autoencoder as a nonlinear dimension reduction model. The dynamics of the latent variables are constructed by a neural network-based model that preserves certain structures to respect the thermodynamic laws through the GENERIC formalism. An abstract error estimate of the approximation is established, which provides a new loss formulation involving the Jacobian computation of autoencoder. Both the autoencoder and the latent dynamics are trained to minimize the new loss. Numerical examples are presented to demonstrate the performance of tLaSDI, which exhibits robust generalization ability, even in extrapolation. In addition, an intriguing correlation is empirically observed between the entropy production rates in the latent space and the behaviors of the ful
&lt;/p&gt;</description></item><item><title>TrafficGPT &#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#31361;&#30772;&#20196;&#29260;&#38271;&#24230;&#38480;&#21046;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#38271;&#26102;&#38388;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;&#20013;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#21644;&#29983;&#25104;&#31526;&#21512;&#23454;&#38469;&#27169;&#24335;&#30340;&#27969;&#37327;&#26679;&#26412;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05822</link><description>&lt;p&gt;
TrafficGPT&#65306;&#31361;&#30772;&#20196;&#29260;&#38480;&#21046;&#65292;&#23454;&#29616;&#39640;&#25928;&#38271;&#26102;&#38388;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TrafficGPT: Breaking the Token Barrier for Efficient Long Traffic Analysis and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05822
&lt;/p&gt;
&lt;p&gt;
TrafficGPT &#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#31361;&#30772;&#20196;&#29260;&#38271;&#24230;&#38480;&#21046;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#38271;&#26102;&#38388;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;&#20013;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#21644;&#29983;&#25104;&#31526;&#21512;&#23454;&#38469;&#27169;&#24335;&#30340;&#27969;&#37327;&#26679;&#26412;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#21644;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#12290;&#20174;&#20256;&#32479;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#35813;&#39046;&#22495;&#21457;&#23637;&#21040;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#36825;&#31181;&#36827;&#27493;&#25552;&#39640;&#20102;&#26816;&#27979;&#22797;&#26434;&#27169;&#24335;&#21644;&#23433;&#20840;&#23041;&#32961;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#27979;&#35797;&#21644;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#38556;&#30861;&#65292;&#20363;&#22914;&#23545;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#20381;&#36182;&#20197;&#21450;&#29983;&#25104;&#36981;&#24490;&#23454;&#38469;&#27169;&#24335;&#30340;&#27969;&#37327;&#26679;&#26412;&#30340;&#22256;&#38590;&#12290;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#20581;&#22766;&#30340;&#25968;&#25454;&#34920;&#31034;&#26469;&#25552;&#20379;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#23384;&#22312;&#30410;&#22788;&#65292;&#20294;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#38754;&#20020;&#20196;&#29260;&#38271;&#24230;&#38480;&#21046;&#31561;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20840;&#38754;&#27969;&#37327;&#20998;&#26512;&#21644;&#23454;&#38469;&#27969;&#37327;&#29983;&#25104;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TrafficGPT&#65292;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05822v1 Announce Type: new  Abstract: Over the years, network traffic analysis and generation have advanced significantly. From traditional statistical methods, the field has progressed to sophisticated deep learning techniques. This progress has improved the ability to detect complex patterns and security threats, as well as to test and optimize network performance. However, obstacles persist, such as the dependence on labeled data for analysis and the difficulty of generating traffic samples that follow realistic patterns. Pre-trained deep neural networks have emerged as powerful tools to resolve these issues, offering improved performance by learning robust data representations from large unlabeled datasets. Despite their benefits, existing pre-trained models face challenges like token length limitation, which restricts their usefulness in comprehensive traffic analysis and realistic traffic generation. To address these challenges, we introduce TrafficGPT, a deep learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20248;&#21270;&#22312;&#20851;&#31995;&#26597;&#35810;&#20013;&#35843;&#29992;LLM&#30340;&#20998;&#26512;&#22411;&#24037;&#20316;&#36127;&#36733;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21457;&#29616;&#20851;&#31995;&#26597;&#35810;&#20026;&#21152;&#36895;LLM&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.05821</link><description>&lt;p&gt;
&#22312;&#20851;&#31995;&#22411;&#24037;&#20316;&#36127;&#36733;&#20013;&#20248;&#21270;LLM&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Optimizing LLM Queries in Relational Workloads
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20248;&#21270;&#22312;&#20851;&#31995;&#26597;&#35810;&#20013;&#35843;&#29992;LLM&#30340;&#20998;&#26512;&#22411;&#24037;&#20316;&#36127;&#36733;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21457;&#29616;&#20851;&#31995;&#26597;&#35810;&#20026;&#21152;&#36895;LLM&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05821v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20998;&#26512;&#24615;&#25968;&#25454;&#24211;&#25552;&#20379;&#21830;&#65288;&#20363;&#22914;Redshift&#12289;Databricks&#12289;BigQuery&#65289;&#24050;&#36805;&#36895;&#22686;&#21152;&#23545;&#36890;&#36807;&#26412;&#26426;&#29992;&#25143;&#33258;&#23450;&#20041;&#20989;&#25968;&#65288;UDFs&#65289;&#35843;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25903;&#25345;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#20998;&#26512;&#22411;&#24037;&#20316;&#36127;&#36733;&#20869;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#31867;&#12289;&#23454;&#20307;&#25552;&#21462;&#21644;&#32763;&#35793;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20248;&#21270;&#20851;&#31995;&#26597;&#35810;&#20013;&#35843;&#29992;LLM&#30340;&#20998;&#26512;&#24037;&#20316;&#36127;&#36733;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20851;&#31995;&#26597;&#35810;&#20026;&#21152;&#36895;LLM&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26426;&#20250;&#65292;&#21253;&#25324;&#37325;&#26032;&#25490;&#24207;&#34892;&#20197;&#26368;&#22823;&#21270;LLM&#25512;&#29702;&#24341;&#25806;&#20869;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#37325;&#29992;&#65292;&#37325;&#26032;&#25490;&#24207;&#34892;&#20869;&#30340;&#21015;&#20197;&#36827;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05821v1 Announce Type: new  Abstract: Analytical database providers (e.g., Redshift, Databricks, BigQuery) have rapidly added support for invoking Large Language Models (LLMs) through native user-defined functions (UDFs) to help users perform natural language tasks, such as classification, entity extraction, and translation, inside analytical workloads. For instance, an analyst might want to extract customer sentiments on millions of product reviews. However, LLM inference is highly expensive in both computational and economic terms: for example, an NVIDIA L4 GPU running Llama2-7B can only process 6 KB of text per second. In this paper, we explore how to optimize LLM inference for analytical workloads that invoke LLMs within relational queries. We show that relational queries present novel opportunities for accelerating LLM inference, including reordering rows to maximize key-value (KV) cache reuse within the LLM inference engine, reordering columns within a row to further i
&lt;/p&gt;</description></item><item><title>PR-NET&#27169;&#22411;&#36890;&#36807;&#21387;&#32553;&#21644;&#20248;&#21270;P-NET&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#29366;&#20917;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05818</link><description>&lt;p&gt;
PR-NET&#65306;&#21033;&#29992;&#31934;&#32454;&#21270;&#36890;&#36335;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#29366;&#20917;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PR-NET: Leveraging Pathway Refined Network Structures for Prostate Cancer Patient Condition Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05818
&lt;/p&gt;
&lt;p&gt;
PR-NET&#27169;&#22411;&#36890;&#36807;&#21387;&#32553;&#21644;&#20248;&#21270;P-NET&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#29366;&#20917;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;&#35786;&#26029;&#21644;&#30417;&#27979;&#21435;&#21183;&#25269;&#25239;&#24615;&#21069;&#21015;&#33146;&#30284;&#65288;CRPC&#65289;&#23545;&#30284;&#30151;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#30340;&#27169;&#22411;&#65288;&#22914;P-NET&#65289;&#22312;&#21442;&#25968;&#25968;&#37327;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25104;&#26412;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#32467;&#26524;&#65306;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#39640;&#25928;&#30340;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#29366;&#20917;&#39044;&#27979;&#27169;&#22411;&#65292;&#21517;&#20026;PR-NET&#12290;&#36890;&#36807;&#21387;&#32553;&#21644;&#20248;&#21270;P-NET&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;PR-NET&#22312;&#39044;&#27979;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#32467;&#26524;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;P-NET&#21644;&#20854;&#20182;&#20845;&#31181;&#20256;&#32479;&#27169;&#22411;&#26174;&#33879;&#12290;&#22312;&#25105;&#20204;&#30340;&#20005;&#26684;&#35780;&#20272;&#20013;&#65292;PR-NET&#19981;&#20165;&#22312;&#24050;&#30693;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24179;&#22343;AUC&#21644;&#21484;&#22238;&#29575;&#20998;&#25968;&#65288;&#20998;&#21035;&#20026;0.94&#21644;0.83&#65289;&#65292;&#32780;&#19988;&#22312;&#20116;&#20010;&#26410;&#30693;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24179;&#22343;AUC&#20026;0.73&#65292;&#21484;&#22238;&#29575;&#20026;0
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05818v1 Announce Type: new  Abstract: Motivation: The diagnosis and monitoring of Castrate Resistant Prostate Cancer (CRPC) are crucial for cancer patients, but the current models (such as P-NET) have limitations in terms of parameter count, generalization, and cost. Results: To address the above issues, we develop a more accurate and efficient Prostate Cancer patient condition prediction model, named PR-NET. By compressing and optimizing the network structure of P-NET, the model complexity is reduced while maintaining high accuracy and interpretability. The PR-NET demonstrated superior performance in predicting prostate cancer patient outcomes, outshining P-NET and six other traditional models with a significant margin. In our rigorous evaluation, PR-NET not only achieved impressive average AUC and Recall scores of 0.94 and 0.83, respectively, on known data but also maintained robust generalizability on five unknown datasets with a higher average AUC of 0.73 and Recall of 0
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05811</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Statistical Efficiency of Distributional Temporal Difference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(DRL)&#20851;&#27880;&#30340;&#26159;&#36820;&#22238;&#30340;&#23436;&#25972;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22343;&#20540;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#32463;&#39564;&#25104;&#21151;&#12290;&#39046;&#22495;DRL&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#20043;&#19968;&#26159;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#65292;&#28041;&#21450;&#20272;&#35745;&#32473;&#23450;&#31574;&#30053;pi&#30340;&#36820;&#22238;&#20998;&#24067;&#951;^pi&#12290;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#20998;&#24067;&#26102;&#38388;&#24046;&#20998;(TD)&#31639;&#27861;&#65292;&#36825;&#26159;&#32463;&#20856;RL&#25991;&#29486;&#20013;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#30340;&#24310;&#20280;&#12290;&#22312;&#34920;&#26684;&#26696;&#20363;&#20013;&#65292;citet{rowland2018analysis}&#21644;citet{rowland2023analysis}&#20998;&#21035;&#35777;&#26126;&#20102;&#20004;&#20010;&#20998;&#24067;&#24335;TD&#23454;&#20363;&#21363;&#20998;&#31867;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;(CTD)&#21644;&#20998;&#20301;&#25968;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;(QTD)&#30340;&#28176;&#36817;&#25910;&#25947;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;TD&#30340;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340; dis
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05811v1 Announce Type: cross  Abstract: Distributional reinforcement learning (DRL), which cares about the full distribution of returns instead of just the mean, has achieved empirical success in various domains. One of the core tasks in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\eta^\pi$ for a given policy $\pi$. A distributional temporal difference (TD) algorithm has been accordingly proposed, which is an extension of the temporal difference algorithm in the classic RL literature. In the tabular case, \citet{rowland2018analysis} and \citet{rowland2023analysis} proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference algorithm (CTD) and quantile temporal difference algorithm (QTD), respectively. In this paper, we go a step further and analyze the finite-sample performance of distributional TD. To facilitate theoretical analysis, we propose non-parametric dis
&lt;/p&gt;</description></item><item><title>&#22312;&#20984;&#22810;&#38754;&#20307;&#32593;&#26684;&#19978;&#65292;&#25552;&#20986;&#20102;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#26469;&#24369;&#34920;&#31034;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#26681;&#25454;&#32593;&#26684;&#20013;&#30340;&#22810;&#38754;&#20307;&#21644;&#36229;&#24179;&#38754;&#30340;&#25968;&#37327;&#20934;&#30830;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340;&#31070;&#32463;&#20803;&#25968;&#65292;&#24314;&#31435;&#20102;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.05809</link><description>&lt;p&gt;
&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;
&lt;/p&gt;
&lt;p&gt;
Shallow ReLU neural networks and finite elements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05809
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20984;&#22810;&#38754;&#20307;&#32593;&#26684;&#19978;&#65292;&#25552;&#20986;&#20102;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#26469;&#24369;&#34920;&#31034;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#26681;&#25454;&#32593;&#26684;&#20013;&#30340;&#22810;&#38754;&#20307;&#21644;&#36229;&#24179;&#38754;&#30340;&#25968;&#37327;&#20934;&#30830;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340;&#31070;&#32463;&#20803;&#25968;&#65292;&#24314;&#31435;&#20102;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25351;&#20986;&#22312;&#20984;&#22810;&#38754;&#20307;&#32593;&#26684;&#19978;&#65292;&#21487;&#20197;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#24369;&#24847;&#20041;&#19979;&#34920;&#31034;&#65288;&#36830;&#32493;&#25110;&#19981;&#36830;&#32493;&#30340;&#65289;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#28041;&#21450;&#21040;&#30340;&#22810;&#38754;&#20307;&#21644;&#36229;&#24179;&#38754;&#30340;&#25968;&#37327;&#65292;&#20934;&#30830;&#32473;&#20986;&#20102;&#24369;&#34920;&#31034;&#25152;&#38656;&#30340;&#20004;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#20803;&#25968;&#12290;&#36825;&#20123;&#32467;&#26524;&#33258;&#28982;&#22320;&#36866;&#29992;&#20110;&#24120;&#25968;&#21644;&#32447;&#24615;&#26377;&#38480;&#20803;&#20989;&#25968;&#12290;&#36825;&#31181;&#24369;&#34920;&#31034;&#24314;&#31435;&#20102;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#20989;&#25968;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#24182;&#20026;&#36890;&#36807;&#26377;&#38480;&#20803;&#20989;&#25968;&#20998;&#26512;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;$L^p$&#33539;&#25968;&#20013;&#30340;&#36924;&#36817;&#33021;&#21147;&#25552;&#20379;&#20102;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#23545;&#24352;&#37327;&#26377;&#38480;&#20803;&#20989;&#25968;&#30340;&#20005;&#26684;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05809v1 Announce Type: cross  Abstract: We point out that (continuous or discontinuous) piecewise linear functions on a convex polytope mesh can be represented by two-hidden-layer ReLU neural networks in a weak sense. In addition, the numbers of neurons of the two hidden layers required to weakly represent are accurately given based on the numbers of polytopes and hyperplanes involved in this mesh. The results naturally hold for constant and linear finite element functions. Such weak representation establishes a bridge between shallow ReLU neural networks and finite element functions, and leads to a perspective for analyzing approximation capability of ReLU neural networks in $L^p$ norm via finite element functions. Moreover, we discuss the strict representation for tensor finite element functions via the recent tensor neural networks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;$\textbf{S}^2$IP-LLM&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#23558;&#35821;&#20041;&#31354;&#38388;&#19982;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05798</link><description>&lt;p&gt;
$\textbf{S}^2$IP-LLM: &#20511;&#21161;LLM&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35821;&#20041;&#31354;&#38388;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
$\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05798
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;$\textbf{S}^2$IP-LLM&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#23558;&#35821;&#20041;&#31354;&#38388;&#19982;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#24314;&#31435;&#30340;LLM&#30340;&#35821;&#20041;&#31354;&#38388;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#20135;&#29983;&#26356;&#21152;&#29420;&#29305;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#20197;&#20419;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20511;&#21161;LLM&#36827;&#34892;&#35821;&#20041;&#31354;&#38388;&#25552;&#31034;&#23398;&#20064;&#65288;$\textbf{S}^2$IP-LLM&#65289;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#31354;&#38388;&#19982;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#22522;&#20110;&#32852;&#21512;&#31354;&#38388;&#20013;&#23398;&#21040;&#30340;&#25552;&#31034;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#19987;&#20026;&#36328;&#27169;&#24577;&#23545;&#40784;&#23450;&#21046;&#30340;&#26631;&#35760;&#21270;&#27169;&#22359;&#65292;&#26174;&#24335;&#22320;&#20018;&#32852;&#20998;&#35299;&#30340;&#26102;&#38388;&#24207;&#21015;&#32452;&#20214;&#30340;&#34917;&#19969;&#65292;&#20197;&#21019;&#24314;&#33021;&#22815;&#26377;&#25928;&#32534;&#30721;&#26102;&#38388;&#21160;&#24577;&#30340;&#23884;&#20837;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#35789;&#26631;&#35760;&#23884;&#20837;&#26469;&#23548;&#20986;&#35821;&#20041;&#38170;&#28857;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#40784;&#25152;&#36873;&#38170;&#28857;&#19982;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05798v1 Announce Type: new  Abstract: Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the pre-trained semantic space with time series embeddings space and perform time series forecasting based on learned prompts from the joint space. We first design a tokenization module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maxi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#23433;&#20840;&#24615;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#22312;&#32447;&#24615;&#32422;&#26463;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;$\tilde{\mathcal{O}}(\sqrt{T})$&#30340;&#21518;&#24724;&#20540;&#65292;&#21516;&#26102;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#26102;&#21464;&#38543;&#26426;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;OCO&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05786</link><description>&lt;p&gt;
&#32447;&#24615;&#32422;&#26463;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#20048;&#35266;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimistic Safety for Linearly-Constrained Online Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#23433;&#20840;&#24615;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#22312;&#32447;&#24615;&#32422;&#26463;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;$\tilde{\mathcal{O}}(\sqrt{T})$&#30340;&#21518;&#24724;&#20540;&#65292;&#21516;&#26102;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#26102;&#21464;&#38543;&#26426;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;OCO&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#26410;&#30693;&#32422;&#26463;&#19979;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#38382;&#39064;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#38745;&#24577;&#32447;&#24615;&#32422;&#26463;&#30340;&#38382;&#39064;&#29256;&#26412;&#65292;&#29609;&#23478;&#20250;&#25910;&#21040;&#22024;&#26434;&#21453;&#39304;&#24182;&#19988;&#24517;&#39035;&#22987;&#32456;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#21019;&#26032;&#30340;&#20048;&#35266;&#23433;&#20840;&#24615;&#35774;&#35745;&#33539;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#30340;&#21518;&#24724;&#20540;&#20026;$\tilde{\mathcal{O}}(\sqrt{T})$&#12290;&#36825;&#19968;&#25913;&#36827;&#20102;&#20197;&#24448;$\tilde{\mathcal{O}}(T^{2/3})$&#30340;&#26368;&#20339;&#21518;&#24724;&#20540;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;&#30053;&#24378;&#30340;&#29420;&#31435;&#22122;&#22768;&#21644;&#26080;&#24847;&#35782;&#23545;&#25163;&#30340;&#20551;&#35774;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#36825;&#20010;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#22312;&#26102;&#21464;&#38543;&#26426;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;OCO&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#19979;&#20855;&#26377;&#30456;&#21516;&#30340;&#21518;&#24724;&#20540;&#20445;&#35777;&#65292;&#24182;&#19988;&#20174;&#26399;&#26395;&#19978;&#27704;&#36828;&#19981;&#20250;&#36829;&#21453;&#32422;&#26463;&#12290;&#36825;&#20026;OCO&#22312;&#26102;&#21464;&#38543;&#26426;&#32422;&#26463;&#19979;&#30340;&#25991;&#29486;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20854;&#20013;&#29616;&#26377;&#30340;&#20808;&#36827;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05786v1 Announce Type: new  Abstract: The setting of online convex optimization (OCO) under unknown constraints has garnered significant attention in recent years. In this work, we consider a version of this problem with static linear constraints that the player receives noisy feedback of and must always satisfy. By leveraging our novel design paradigm of optimistic safety, we give an algorithm for this problem that enjoys $\tilde{\mathcal{O}}(\sqrt{T})$ regret. This improves on the previous best regret bound of $\tilde{\mathcal{O}}(T^{2/3})$ while using only slightly stronger assumptions of independent noise and an oblivious adversary. Then, by recasting this problem as OCO under time-varying stochastic linear constraints, we show that our algorithm enjoys the same regret guarantees in such a setting and never violates the constraints in expectation. This contributes to the literature on OCO under time-varying stochastic constraints, where the state-of-the-art algorithms en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;AI&#27169;&#22411;&#36741;&#21161;&#30340;3D&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#21253;&#25324;3D&#35821;&#20041;&#25552;&#21462;&#22120;&#21644;&#33258;&#36866;&#24212;&#35821;&#20041;&#21387;&#32553;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#25552;&#21462;&#21644;&#32534;&#30721;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.05783</link><description>&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#36741;&#21161;&#30340;3D&#35821;&#20041;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Large Generative Model Assisted 3D Semantic Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05783
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;AI&#27169;&#22411;&#36741;&#21161;&#30340;3D&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#21253;&#25324;3D&#35821;&#20041;&#25552;&#21462;&#22120;&#21644;&#33258;&#36866;&#24212;&#35821;&#20041;&#21387;&#32553;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#25552;&#21462;&#21644;&#32534;&#30721;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#65288;SC&#65289;&#26159;6G&#25968;&#25454;&#20256;&#36755;&#30340;&#19968;&#31181;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;3D&#22330;&#26223;&#20013;&#36827;&#34892;SC&#26102;&#23384;&#22312;&#20960;&#20010;&#25361;&#25112;&#65306;1&#65289;3D&#35821;&#20041;&#25552;&#21462;&#65307;2&#65289;&#28508;&#22312;&#35821;&#20041;&#20887;&#20313;&#65307;&#21644;3&#65289;&#19981;&#30830;&#23450;&#30340;&#20449;&#36947;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;AI&#27169;&#22411;&#36741;&#21161;&#30340;3D SC&#65288;GAM-3DSC&#65289;&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;3D&#35821;&#20041;&#25552;&#21462;&#22120;&#65288;3DSE&#65289;&#65292;&#23427;&#21033;&#29992;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#21253;&#25324;Segment Anything Model&#65288;SAM&#65289;&#21644;Neural Radiance Field&#65288;NeRF&#65289;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#20174;3D&#22330;&#26223;&#20013;&#25552;&#21462;&#20851;&#38190;&#35821;&#20041;&#12290;&#25552;&#21462;&#30340;3D&#35821;&#20041;&#34987;&#34920;&#31034;&#20026;&#38754;&#21521;&#30446;&#26631;&#30340;3D&#29289;&#20307;&#30340;&#22810;&#35270;&#35282;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#21387;&#32553;&#27169;&#22411;&#65288;ASCM&#65289;&#65292;&#29992;&#20110;&#23545;&#36825;&#20123;&#22810;&#35270;&#35282;&#22270;&#20687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#36755;&#20986;&#22836;&#30340;&#35821;&#20041;&#32534;&#30721;&#22120;&#26469;&#25191;&#34892;&#35821;&#20041;&#32534;&#30721;&#65292;&#24182;&#22312;&#28508;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#25513;&#30422;&#20887;&#20313;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05783v1 Announce Type: cross  Abstract: Semantic Communication (SC) is a novel paradigm for data transmission in 6G. However, there are several challenges posed when performing SC in 3D scenarios: 1) 3D semantic extraction; 2) Latent semantic redundancy; and 3) Uncertain channel estimation. To address these issues, we propose a Generative AI Model assisted 3D SC (GAM-3DSC) system. Firstly, we introduce a 3D Semantic Extractor (3DSE), which employs generative AI models, including Segment Anything Model (SAM) and Neural Radiance Field (NeRF), to extract key semantics from a 3D scenario based on user requirements. The extracted 3D semantics are represented as multi-perspective images of the goal-oriented 3D object. Then, we present an Adaptive Semantic Compression Model (ASCM) for encoding these multi-perspective images, in which we use a semantic encoder with two output heads to perform semantic encoding and mask redundant semantics in the latent semantic space, respectively. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#21033;&#29992;&#20301;&#32622;&#20449;&#24687;&#30340;&#33337;&#33334;&#36335;&#24452;&#30340;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#23454;&#29616;&#36335;&#24452;&#32858;&#31867;&#65292;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;F1&#20998;&#25968;&#65292;&#20026;&#25913;&#21892;&#28023;&#36816;&#23433;&#20840;&#21644;&#25928;&#29575;&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.05778</link><description>&lt;p&gt;
&#33337;&#33334;&#33322;&#36857;&#35782;&#21035;&#30340;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spatial Clustering Approach for Vessel Path Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05778
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#21033;&#29992;&#20301;&#32622;&#20449;&#24687;&#30340;&#33337;&#33334;&#36335;&#24452;&#30340;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#23454;&#29616;&#36335;&#24452;&#32858;&#31867;&#65292;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;F1&#20998;&#25968;&#65292;&#20026;&#25913;&#21892;&#28023;&#36816;&#23433;&#20840;&#21644;&#25928;&#29575;&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#35782;&#21035;&#20855;&#26377;&#37325;&#22797;&#36335;&#24452;&#12289;&#37096;&#20998;&#37325;&#22797;&#36335;&#24452;&#21644;&#26032;&#36335;&#24452;&#30340;&#33337;&#33334;&#36335;&#24452;&#30340;&#25361;&#25112;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#21033;&#29992;&#20301;&#32622;&#20449;&#24687;&#26631;&#35760;&#33337;&#33334;&#36335;&#24452;&#30340;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36335;&#24452;&#32858;&#31867;&#26694;&#26550;&#65292;&#37319;&#29992;&#20004;&#31181;&#26041;&#27861;&#65306;&#22522;&#20110;&#36317;&#31163;&#30340;&#36335;&#24452;&#24314;&#27169;&#21644;&#27010;&#20284;&#20272;&#35745;&#26041;&#27861;&#12290;&#21069;&#32773;&#36890;&#36807;&#25972;&#21512;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20102;&#36335;&#24452;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#21518;&#32773;&#19987;&#27880;&#20110;&#22522;&#20110;&#27010;&#20284;&#30340;&#36335;&#24452;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20998;&#21106;&#20197;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#32467;&#26524;&#21457;&#29616;&#31361;&#20986;&#20102;&#25152;&#24320;&#21457;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#22240;&#20026;&#23558;&#33337;&#33334;&#36335;&#24452;&#32858;&#31867;&#20026;&#20116;&#31867;&#30340;&#20004;&#31181;&#26041;&#27861;&#22343;&#21462;&#24471;&#20102;&#23436;&#32654;&#30340;F1&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#20026;&#33322;&#32447;&#35268;&#21010;&#25552;&#20379;&#23453;&#36149;&#35265;&#35299;&#65292;&#26368;&#32456;&#26377;&#21161;&#20110;&#25552;&#39640;&#28023;&#36816;&#23433;&#20840;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05778v1 Announce Type: new  Abstract: This paper addresses the challenge of identifying the paths for vessels with operating routes of repetitive paths, partially repetitive paths, and new paths. We propose a spatial clustering approach for labeling the vessel paths by using only position information. We develop a path clustering framework employing two methods: a distance-based path modeling and a likelihood estimation method. The former enhances the accuracy of path clustering through the integration of unsupervised machine learning techniques, while the latter focuses on likelihood-based path modeling and introduces segmentation for a more detailed analysis. The result findings highlight the superior performance and efficiency of the developed approach, as both methods for clustering vessel paths into five classes achieve a perfect F1-score. The approach aims to offer valuable insights for route planning, ultimately contributing to improving safety and efficiency in marit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#24212;&#29992;&#20110;&#24191;&#27867;&#25216;&#33021;&#21644;&#22810;&#31181;&#34892;&#20026;&#30340;&#21151;&#25928;&#65292;&#24182;&#21457;&#29616;&#23548;&#21521;&#24191;&#27867;&#25216;&#33021;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#20013;&#21516;&#26102;&#27880;&#20837;&#20010;&#20307;&#23548;&#21521;&#21521;&#37327;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05767</link><description>&lt;p&gt;
&#23558;&#28608;&#27963;&#23548;&#21521;&#25193;&#23637;&#21040;&#24191;&#27867;&#25216;&#33021;&#19982;&#22810;&#31181;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Extending Activation Steering to Broad Skills and Multiple Behaviours
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#24212;&#29992;&#20110;&#24191;&#27867;&#25216;&#33021;&#21644;&#22810;&#31181;&#34892;&#20026;&#30340;&#21151;&#25928;&#65292;&#24182;&#21457;&#29616;&#23548;&#21521;&#24191;&#27867;&#25216;&#33021;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#20013;&#21516;&#26102;&#27880;&#20837;&#20010;&#20307;&#23548;&#21521;&#21521;&#37327;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21361;&#38505;&#30340;&#33021;&#21147;&#65292;&#36825;&#24456;&#21487;&#33021;&#22312;&#26410;&#26469;&#21464;&#24471;&#26356;&#21152;&#26840;&#25163;&#12290;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#21487;&#29992;&#20110;&#20943;&#23569;&#36825;&#20123;&#33021;&#21147;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28608;&#27963;&#23548;&#21521;&#22312;&#24191;&#27867;&#25216;&#33021;&#21644;&#22810;&#31181;&#34892;&#20026;&#20013;&#30340;&#21151;&#25928;&#12290;&#36890;&#36807;&#27604;&#36739;&#20943;&#23569;&#23545;&#19968;&#33324;&#32534;&#30721;&#33021;&#21147;&#21644;Python&#29305;&#23450;&#33021;&#21147;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23548;&#21521;&#26356;&#24191;&#27867;&#25216;&#33021;&#19982;&#23548;&#21521;&#36739;&#31364;&#25216;&#33021;&#31454;&#20105;&#28608;&#28872;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#25110;&#26356;&#23569;&#36817;&#35270;&#21644;&#23547;&#27714;&#36130;&#23500;&#65292;&#20197;&#21450;&#20854;&#20182;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#23558;&#22810;&#31181;&#19981;&#21516;&#34892;&#20026;&#30340;&#23548;&#21521;&#21521;&#37327;&#32452;&#21512;&#20026;&#19968;&#20010;&#23548;&#21521;&#21521;&#37327;&#36890;&#24120;&#19981;&#25104;&#21151;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#20013;&#19981;&#21516;&#20301;&#32622;&#27880;&#20837;&#20010;&#20307;&#23548;&#21521;&#21521;&#37327;&#26159;&#26377;&#21069;&#36884;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05767v1 Announce Type: cross  Abstract: Current large language models have dangerous capabilities, which are likely to become more problematic in the future. Activation steering techniques can be used to reduce risks from these capabilities. In this paper, we investigate the efficacy of activation steering for broad skills and multiple behaviours. First, by comparing the effects of reducing performance on general coding ability and Python-specific ability, we find that steering broader skills is competitive to steering narrower skills. Second, we steer models to become more or less myopic and wealth-seeking, among other behaviours. In our experiments, combining steering vectors for multiple different behaviours into one steering vector is largely unsuccessful. On the other hand, injecting individual steering vectors at different places in a model simultaneously is promising.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#32422;&#26463;&#27969;&#24418;&#19978;&#35299;&#20915;Eikonal&#26041;&#31243;&#30340;&#29289;&#29702;&#20449;&#24687;CMP&#26694;&#26550;&#65292;&#26080;&#38656;&#19987;&#23478;&#25968;&#25454;&#65292;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.05765</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#36816;&#21160;&#35268;&#21010;&#22312;&#32422;&#26463;&#27969;&#24418;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Neural Motion Planning on Constraint Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#32422;&#26463;&#27969;&#24418;&#19978;&#35299;&#20915;Eikonal&#26041;&#31243;&#30340;&#29289;&#29702;&#20449;&#24687;CMP&#26694;&#26550;&#65292;&#26080;&#38656;&#19987;&#23478;&#25968;&#25454;&#65292;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#36816;&#21160;&#35268;&#21010;&#65288;CMP&#65289;&#26088;&#22312;&#22312;&#36816;&#21160;&#32422;&#26463;&#27969;&#24418;&#19978;&#25214;&#21040;&#32473;&#23450;&#36215;&#22987;&#21644;&#30446;&#26631;&#37197;&#32622;&#20043;&#38388;&#30340;&#26080;&#30896;&#36335;&#24452;&#12290;&#36825;&#20123;&#38382;&#39064;&#20986;&#29616;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#20174;&#29289;&#20307;&#25805;&#32437;&#21040;&#33151;&#24335;&#26426;&#22120;&#20154;&#34892;&#36208;&#12290;&#28982;&#32780;&#65292;&#27969;&#24418;&#30340;&#38646;&#20307;&#31215;&#29305;&#24615;&#20351;&#24471;CMP&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#20960;&#31186;&#38047;&#26469;&#25214;&#21040;&#36335;&#24452;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#35745;&#31639;&#36153;&#21147;&#30340;&#36335;&#24452;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#30452;&#25509;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;Eikonal&#26041;&#31243;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#30340;&#29289;&#29702;&#20449;&#24687;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#26080;&#38656;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#23398;&#20064;&#12290;&#21463;&#36825;&#20123;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#32422;&#26463;&#27969;&#24418;&#19978;&#35299;&#20915;Eikonal&#26041;&#31243;&#30340;&#29289;&#29702;&#20449;&#24687;CMP&#26694;&#26550;&#65292;&#24182;&#35757;&#32451;&#31070;&#32463;&#20989;&#25968;&#36827;&#34892;CMP&#32780;&#26080;&#38656;&#19987;&#23478;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05765v1 Announce Type: cross  Abstract: Constrained Motion Planning (CMP) aims to find a collision-free path between the given start and goal configurations on the kinematic constraint manifolds. These problems appear in various scenarios ranging from object manipulation to legged-robot locomotion. However, the zero-volume nature of manifolds makes the CMP problem challenging, and the state-of-the-art methods still take several seconds to find a path and require a computationally expansive path dataset for imitation learning. Recently, physics-informed motion planning methods have emerged that directly solve the Eikonal equation through neural networks for motion planning and do not require expert demonstrations for learning. Inspired by these approaches, we propose the first physics-informed CMP framework that solves the Eikonal equation on the constraint manifolds and trains neural function for CMP without expert data. Our results show that the proposed approach efficientl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32500;&#35745;&#31639;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#29992;&#20110;&#26356;&#39640;&#25928;&#21644;&#21152;&#36895;&#30340;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.05763</link><description>&lt;p&gt;
HDReason&#65306;&#36229;&#32500;&#30693;&#35782;&#22270;&#25512;&#29702;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32500;&#35745;&#31639;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#29992;&#20110;&#26356;&#39640;&#25928;&#21644;&#21152;&#36895;&#30340;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20026;&#22270;&#23398;&#20064;&#24212;&#29992;&#22914;&#39030;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#25552;&#20986;&#20102;&#22823;&#37327;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24456;&#23569;&#20851;&#27880;&#30693;&#35782;&#22270;&#34917;&#20840;&#65288;KGC&#65289;&#65292;&#36825;&#26159;&#19968;&#39033;&#20197;&#20854;&#26174;&#33879;&#26356;&#39640;&#31639;&#27861;&#22797;&#26434;&#24615;&#32780;&#38395;&#21517;&#30340;&#20219;&#21153;&#12290;&#22522;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#26368;&#20808;&#36827;KGC&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#24191;&#27867;&#30340;&#39030;&#28857;/&#20851;&#31995;&#23884;&#20837;&#26356;&#26032;&#21644;&#22797;&#26434;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#36825;&#23545;&#21152;&#36895;&#26469;&#35828;&#26159;&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#21152;&#36895;&#22120;&#35774;&#35745;&#19981;&#20877;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#26469;&#36827;&#34892;KG&#25512;&#29702;&#12290;&#26368;&#36817;&#65292;&#21463;&#33041;&#21551;&#21457;&#30340;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#34987;&#24341;&#20837;&#20316;&#20026;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22270;&#23398;&#20064;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;HDC&#26469;&#23454;&#29616;&#19968;&#20010;&#22266;&#26377;&#26356;&#39640;&#25928;&#19988;&#36866;&#21512;&#21152;&#36895;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05763v1 Announce Type: cross  Abstract: In recent times, a plethora of hardware accelerators have been put forth for graph learning applications such as vertex classification and graph classification. However, previous works have paid little attention to Knowledge Graph Completion (KGC), a task that is well-known for its significantly higher algorithm complexity. The state-of-the-art KGC solutions based on graph convolution neural network (GCN) involve extensive vertex/relation embedding updates and complicated score functions, which are inherently cumbersome for acceleration. As a result, existing accelerator designs are no longer optimal, and a novel algorithm-hardware co-design for KG reasoning is needed.   Recently, brain-inspired HyperDimensional Computing (HDC) has been introduced as a promising solution for lightweight machine learning, particularly for graph learning applications. In this paper, we leverage HDC for an intrinsically more efficient and acceleration-fri
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;&#22312;&#32473;&#23450;&#30340;&#26368;&#22823;&#26080;&#21521;&#22242;&#22823;&#23567;($s$)&#26041;&#38754;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#29420;&#31435;&#26597;&#35810;&#39044;&#35328;&#32773;&#22312;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#36827;&#34892;&#25104;&#21592;&#27979;&#35797;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05759</link><description>&lt;p&gt;
&#36890;&#36807;&#29420;&#31435;&#26597;&#35810;&#39044;&#35328;&#32773;&#22312;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#36827;&#34892;&#25104;&#21592;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Membership Testing in Markov Equivalence Classes via Independence Query Oracles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05759
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;&#22312;&#32473;&#23450;&#30340;&#26368;&#22823;&#26080;&#21521;&#22242;&#22823;&#23567;($s$)&#26041;&#38754;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#29420;&#31435;&#26597;&#35810;&#39044;&#35328;&#32773;&#22312;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#36827;&#34892;&#25104;&#21592;&#27979;&#35797;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#26159;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#24433;&#21709;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#20294;&#20854;&#34917;&#20805;&#27010;&#24565;&#8212;&#8212;&#27979;&#35797;&#22240;&#26524;&#20851;&#31995;&#21364;&#22522;&#26412;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#22312;&#32473;&#23450;MEC(Markov&#31561;&#20215;&#31867;)&#30340;&#26368;&#22823;&#26080;&#21521;&#22242;&#30340;&#22823;&#23567;($s$)&#26041;&#38754;&#30340;&#19979;&#30028;&#65292;&#25506;&#35752;&#22522;&#20110;&#32422;&#26463;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;$\exp(\Omega(s))$&#20010;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05759v1 Announce Type: cross  Abstract: Understanding causal relationships between variables is a fundamental problem with broad impact in numerous scientific fields. While extensive research has been dedicated to learning causal graphs from data, its complementary concept of testing causal relationships has remained largely unexplored. While learning involves the task of recovering the Markov equivalence class (MEC) of the underlying causal graph from observational data, the testing counterpart addresses the following critical question: Given a specific MEC and observational data from some causal graph, can we determine if the data-generating causal graph belongs to the given MEC?   We explore constraint-based testing methods by establishing bounds on the required number of conditional independence tests. Our bounds are in terms of the size of the maximum undirected clique ($s$) of the given MEC. In the worst case, we show a lower bound of $\exp(\Omega(s))$ independence tes
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ANN&#38544;&#23618;&#25552;&#20379;&#30340;&#36755;&#20837;&#30340;&#38477;&#32500;&#34920;&#31034;&#26469;&#23545;ANN&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#26412;&#22320;&#37325;&#26032;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#20174;&#25991;&#29486;&#20013;&#36817;&#20284;&#36125;&#21494;&#26031;&#35745;&#31639;&#21644;&#26080;&#20284;&#28982;&#25512;&#26029;&#26041;&#27861;&#30340;&#37325;&#26032;&#26657;&#20934;&#25216;&#26415;&#20013;&#33719;&#21462;&#28789;&#24863;&#12290;</title><link>https://arxiv.org/abs/2403.05756</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#26412;&#22320;&#37325;&#26032;&#26657;&#20934;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Model-Free Local Recalibration of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05756
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ANN&#38544;&#23618;&#25552;&#20379;&#30340;&#36755;&#20837;&#30340;&#38477;&#32500;&#34920;&#31034;&#26469;&#23545;ANN&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#26412;&#22320;&#37325;&#26032;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#20174;&#25991;&#29486;&#20013;&#36817;&#20284;&#36125;&#21494;&#26031;&#35745;&#31639;&#21644;&#26080;&#20284;&#28982;&#25512;&#26029;&#26041;&#27861;&#30340;&#37325;&#26032;&#26657;&#20934;&#25216;&#26415;&#20013;&#33719;&#21462;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#26159;&#39640;&#24230;&#28789;&#27963;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21487;&#38752;&#22320;&#37327;&#21270;&#20854;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#20851;&#20110;&#23545;ANNs&#30340;&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#8220;&#37325;&#26032;&#26657;&#20934;&#8221;&#30340;&#24037;&#20316;&#24456;&#22810;&#65292;&#20351;&#24471;&#24863;&#20852;&#36259;&#20107;&#20214;&#30340;&#39044;&#27979;&#27010;&#29575;&#19982;&#23545;&#23427;&#20204;&#30340;&#26576;&#20123;&#39057;&#29575;&#35780;&#20272;&#19968;&#33268;&#12290;&#26410;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#23545;&#20110;&#35768;&#22810;&#37325;&#35201;&#30340;&#20915;&#31574;&#20219;&#21153;&#26377;&#38480;&#29992;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ANN&#38544;&#23618;&#25552;&#20379;&#30340;&#36755;&#20837;&#30340;&#38477;&#32500;&#34920;&#31034;&#26469;&#23545;ANN&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#26412;&#22320;&#37325;&#26032;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#26041;&#27861;&#21463;&#21040;&#20102;&#25991;&#29486;&#20013;&#29992;&#20110;&#36817;&#20284;&#36125;&#21494;&#26031;&#35745;&#31639;&#21644;&#26080;&#20284;&#28982;&#25512;&#26029;&#26041;&#27861;&#30340;&#37325;&#26032;&#26657;&#20934;&#25216;&#26415;&#30340;&#21551;&#21457;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;ANN&#26657;&#20934;&#26041;&#27861;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#22312;&#36755;&#20837;&#23618;&#19978;&#36827;&#34892;&#26657;&#20934;&#65292;&#20294;&#24403;&#36755;&#20837;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05756v1 Announce Type: cross  Abstract: Artificial neural networks (ANNs) are highly flexible predictive models. However, reliably quantifying uncertainty for their predictions is a continuing challenge. There has been much recent work on "recalibration" of predictive distributions for ANNs, so that forecast probabilities for events of interest are consistent with certain frequency evaluations of them. Uncalibrated probabilistic forecasts are of limited use for many important decision-making tasks. To address this issue, we propose a localized recalibration of ANN predictive distributions using the dimension-reduced representation of the input provided by the ANN hidden layers. Our novel method draws inspiration from recalibration techniques used in the literature on approximate Bayesian computation and likelihood-free inference methods. Most existing calibration methods for ANNs can be thought of as calibrating either on the input layer, which is difficult when the input is
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.05754</link><description>&lt;p&gt;
&#27169;&#24335;&#35782;&#21035;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;ResNet&#21644;DenseNet&#21450;&#20854;&#23436;&#25972;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition with Completeness Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05754
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24403;&#20170;&#25968;&#23383;&#25216;&#26415;&#30340;&#25509;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#32321;&#33635;&#30340;&#22522;&#30784;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#19981;&#26029;&#21457;&#23637;&#30340;&#31038;&#20250;&#38656;&#27714;&#27491;&#22312;&#24378;&#35843;&#26367;&#20195;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#21518;&#25705;&#23572;&#26102;&#20195;&#30340;&#26469;&#20020;&#25512;&#21160;&#20102;&#20855;&#26377;&#21331;&#36234;&#28508;&#21147;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#21069;&#26032;&#26087;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#27604;&#36739;&#20013;&#23384;&#22312;&#21547;&#31946;&#25351;&#26631;&#65292;&#22240;&#27492;&#19968;&#22871;&#26126;&#30830;&#30340;&#35780;&#20272;&#31995;&#32479;&#19982;&#35814;&#32454;&#30340;&#25351;&#26631;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05754v1 Announce Type: new  Abstract: With the contemporary digital technology approaching, deep neural networks are emerging as the foundational algorithm of the artificial intelligence boom. Whereas, the evolving social demands have been emphasizing the necessity of novel methodologies to substitute traditional neural networks. Concurrently, the advent of the post-Moore era has spurred the development of quantum-inspired neural networks with outstanding potentials at certain circumstances. Nonetheless, a definitive evaluating system with detailed metrics is tremendously vital and indispensable owing to the vague indicators in comparison between the novel and traditional deep learning models at present. Hence, to improve and evaluate the performances of the novel neural networks more comprehensively in complex and unpredictable environments, we propose two hybrid quantum-inspired neural networks which are rooted in residual and dense connections respectively for pattern rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;KG-TOSA&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#30340;&#36807;&#22810;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2403.05752</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#35757;&#32451;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20934;&#30830;&#39640;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;KG-TOSA&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#30340;&#36807;&#22810;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#26159;&#19968;&#31181;&#21253;&#21547;&#21508;&#31181;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#30340;&#24322;&#26500;&#22270;&#12290;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#36890;&#24120;&#29992;&#20110;&#22312;KG&#19978;&#35757;&#32451;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;HGNN&#26041;&#27861;&#21463;KG&#30340;&#22823;&#23567;&#12289;&#23494;&#24230;&#20197;&#21450;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20986;&#36807;&#22810;&#30340;&#22797;&#26434;&#24615;&#12290;AI&#20174;&#19994;&#32773;&#25163;&#24037;&#35774;&#35745;&#20986;&#19968;&#20010;&#19982;&#29305;&#23450;&#20219;&#21153;&#30456;&#20851;&#30340;KG G&#30340;&#23376;&#22270;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38754;&#21521;&#20219;&#21153;&#30340;&#23376;&#22270;&#65288;TOSG&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;G&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#30340;&#23376;&#38598;&#12290;&#20351;&#29992;TOSG&#32780;&#19981;&#26159;G&#26469;&#35757;&#32451;&#20219;&#21153;&#21487;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#25152;&#38656;&#30340;&#36807;&#22810;&#35745;&#31639;&#12290;&#35774;&#35745;TOSG&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;KG&#30340;&#32467;&#26500;&#21644;&#20219;&#21153;&#30340;&#30446;&#26631;&#65292;&#22240;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KG-TOSA&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;KG&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;HGNN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05752v1 Announce Type: cross  Abstract: A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range of node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular for training machine learning tasks like node classification and link prediction on KGs. However, HGNN methods exhibit excessive complexity influenced by the KG's size, density, and the number of node and edge types. AI practitioners handcraft a subgraph of a KG G relevant to a specific task. We refer to this subgraph as a task-oriented subgraph (TOSG), which contains a subset of task-related node and edge types in G. Training the task using TOSG instead of G alleviates the excessive computation required for a large KG. Crafting the TOSG demands a deep understanding of the KG's structure and the task's objectives. Hence, it is challenging and time-consuming. This paper proposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented HGNN training on a large KG. In KG
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MG-TSD&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#30446;&#26631;&#26469;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;-of-the-art&#30340;&#39044;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.05751</link><description>&lt;p&gt;
MG-TSD&#65306;&#20855;&#26377;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#30340;&#22810;&#31890;&#24230;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MG-TSD&#27169;&#22411;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#30446;&#26631;&#26469;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#29366;&#24577;-of-the-art&#30340;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30001;&#20110;&#20854;&#29983;&#25104;&#39640;&#20445;&#30495;&#26679;&#26412;&#30340;&#26174;&#33879;&#33021;&#21147;&#32780;&#22312;&#29983;&#25104;&#24335;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24341;&#36215;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#38543;&#26426;&#29305;&#24615;&#24102;&#26469;&#30340;&#19981;&#31283;&#23450;&#24615;&#25361;&#25112;&#65292;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#22312;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24378;&#22823;&#24314;&#27169;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#31890;&#24230;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#65288;MG-TSD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20869;&#22312;&#30340;&#31890;&#24230;&#27700;&#24179;&#20316;&#20026;&#20013;&#38388;&#25193;&#25955;&#27493;&#39588;&#30340;&#32473;&#23450;&#30446;&#26631;&#26469;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05751v1 Announce Type: cross  Abstract: Recently, diffusion probabilistic models have attracted attention in generative time series forecasting due to their remarkable capacity to generate high-fidelity samples. However, the effective utilization of their strong modeling ability in the probabilistic time series forecasting task remains an open question, partially due to the challenge of instability arising from their stochastic nature. To address this challenge, we introduce a novel Multi-Granularity Time Series Diffusion (MG-TSD) model, which achieves state-of-the-art predictive performance by leveraging the inherent granularity levels within the data as given targets at intermediate diffusion steps to guide the learning process of diffusion models. The way to construct the targets is motivated by the observation that the forward process of the diffusion model, which sequentially corrupts the data distribution to a standard normal distribution, intuitively aligns with the p
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.05750</link><description>&lt;p&gt;
&#35299;&#35835;AI&#31508;: &#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#25216;&#26415;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05750
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#24443;&#24213;&#39072;&#35206;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#30340;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#28145;&#20837;&#23457;&#26597;&#12289;&#20262;&#29702;&#23457;&#26597;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#20316;&#20026;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#35780;&#20272;&#20102;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#39046;&#22495;&#38480;&#21046;&#30340;&#26032;&#39062;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#32534;&#30721;&#22120;&#21644;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#30340;&#39640;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.05743</link><description>&lt;p&gt;
&#20855;&#26377;&#24066;&#22330;&#36816;&#33829;&#24212;&#29992;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative Probabilistic Forecasting with Applications in Market Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05743
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#32534;&#30721;&#22120;&#21644;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#30340;&#39640;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28304;&#33258;&#20110;&#38750;&#21442;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#12290;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#24335;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#27979;&#26550;&#26500;&#21253;&#25324;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#38750;&#21442;&#25968;&#22810;&#21464;&#37327;&#38543;&#26426;&#36807;&#31243;&#36716;&#21270;&#20026;&#35268;&#33539;&#30340;&#21019;&#26032;&#24207;&#21015;&#65292;&#20174;&#20013;&#26681;&#25454;&#36807;&#21435;&#26679;&#26412;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26465;&#20214;&#26159;&#23427;&#20204;&#30340;&#27010;&#29575;&#20998;&#24067;&#21462;&#20915;&#20110;&#36807;&#21435;&#26679;&#26412;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28508;&#22312;&#36807;&#31243;&#38480;&#21046;&#20026;&#20855;&#26377;&#21305;&#37197;&#33258;&#32534;&#30721;&#22120;&#36755;&#20837;-&#36755;&#20986;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#24207;&#21015;&#12290;&#24314;&#31435;&#20102;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#24335;&#39044;&#27979;&#26041;&#27861;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#28041;&#21450;&#39640;&#24230;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#30340;&#19977;&#20010;&#24212;&#29992;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05743v1 Announce Type: cross  Abstract: This paper presents a novel generative probabilistic forecasting approach derived from the Wiener-Kallianpur innovation representation of nonparametric time series. Under the paradigm of generative artificial intelligence, the proposed forecasting architecture includes an autoencoder that transforms nonparametric multivariate random processes into canonical innovation sequences, from which future time series samples are generated according to their probability distributions conditioned on past samples. A novel deep-learning algorithm is proposed that constrains the latent process to be an independent and identically distributed sequence with matching autoencoder input-output conditional probability distributions. Asymptotic optimality and structural convergence properties of the proposed generative forecasting approach are established. Three applications involving highly dynamic and volatile time series in real-time market operations a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#24179;&#22343;&#22238;&#25253;&#20934;&#21017;&#19979;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#21487;&#35777;&#26126;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.05738</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#30340;&#24179;&#22343;&#22238;&#25253;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Policy Gradient Methods for Average-Reward Markov Potential Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#24179;&#22343;&#22238;&#25253;&#20934;&#21017;&#19979;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#21487;&#35777;&#26126;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#26102;&#38388;&#27573;&#24179;&#22343;&#22238;&#25253;&#20934;&#21017;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#37117;&#26159;&#38024;&#23545;&#25240;&#25187;&#22238;&#25253;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#22522;&#20110;&#29420;&#31435;&#31574;&#30053;&#26799;&#24230;&#21644;&#29420;&#31435;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#31639;&#27861;&#22343;&#22312;&#24179;&#22343;&#22238;&#25253;&#20934;&#21017;&#19979;&#20840;&#23616;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#12290;&#20026;&#20102;&#20026;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22880;&#23450;&#22522;&#30784;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#24179;&#22343;&#22238;&#25253;&#26159;&#31574;&#30053;&#30340;&#20809;&#28369;&#20989;&#25968;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243; (MDP) &#30340;&#36941;&#21382;&#24615;&#21644;&#27425;&#22823;&#29305;&#24449;&#20540;&#30340;&#19968;&#23450;&#26465;&#20214;&#19979;&#25552;&#20379;&#24046;&#20998;&#20215;&#20540;&#20989;&#25968;&#30340;&#25935;&#24863;&#24230;&#30028;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19977;&#31181;&#31639;&#27861;&#65292;&#21253;&#25324;&#31574;&#30053;&#26799;&#24230;&#12289;&#36817;&#31471;-Q &#21644;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230; (NPG)&#65292;&#22312;&#32473;&#23450;&#26799;&#24230;/&#24046;&#20998;Q&#20989;&#25968;oracle&#30340;&#24773;&#20917;&#19979;&#65292;&#25910;&#25947;&#21040;$\epsilon$-Nash&#22343;&#34913;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(\frac{1}{\epsilon^2})$&#12290;&#24403;&#24517;&#39035;&#20272;&#35745;&#31574;&#30053;&#26799;&#24230;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20855;&#26377;$\tilde{...}$
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05738v1 Announce Type: new  Abstract: We study Markov potential games under the infinite horizon average reward criterion. Most previous studies have been for discounted rewards. We prove that both algorithms based on independent policy gradient and independent natural policy gradient converge globally to a Nash equilibrium for the average reward criterion. To set the stage for gradient-based methods, we first establish that the average reward is a smooth function of policies and provide sensitivity bounds for the differential value functions, under certain conditions on ergodicity and the second largest eigenvalue of the underlying Markov decision process (MDP). We prove that three algorithms, policy gradient, proximal-Q, and natural policy gradient (NPG), converge to an $\epsilon$-Nash equilibrium with time complexity $O(\frac{1}{\epsilon^2})$, given a gradient/differential Q function oracle. When policy gradients have to be estimated, we propose an algorithm with $\tilde{
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#23432;DDPG&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;$Q$-&#30446;&#26631;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#24809;&#32602;&#26469;&#35299;&#20915;DDPG&#20013;&#30340;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#38598;&#25104;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.05732</link><description>&lt;p&gt;
&#20445;&#23432;DDPG - &#26080;&#38598;&#25104;&#30340;&#24754;&#35266;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conservative DDPG -- Pessimistic RL without Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#23432;DDPG&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;$Q$-&#30446;&#26631;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#24809;&#32602;&#26469;&#35299;&#20915;DDPG&#20013;&#30340;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#38598;&#25104;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DDPG&#21463;&#21040;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#30340;&#38459;&#30861;&#65292;&#20854;&#20013;&#20854;$Q$-&#20272;&#35745;&#20542;&#21521;&#20110;&#22840;&#22823;&#23454;&#38469;$Q$&#20540;&#12290;&#20256;&#32479;&#35299;&#20915;&#36825;&#19968;&#20559;&#35265;&#30340;&#26041;&#27861;&#28041;&#21450;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#25110;&#32773;&#22522;&#20110;&#22797;&#26434;&#23545;&#25968;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#38590;&#20197;&#29702;&#35299;&#21644;&#23454;&#26045;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;$Q$-&#30446;&#26631;&#24182;&#32467;&#21512;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#25439;&#22833;&#24809;&#32602;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#29992;&#36739;&#23569;&#30340;&#20195;&#30721;&#23454;&#29616;&#65292;&#32780;&#26080;&#38656;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#24378;&#28872;&#25903;&#25345;&#20445;&#23432;DDPG&#22312;&#21508;&#31181;MuJoCo&#21644;Bullet&#20219;&#21153;&#19978;&#20248;&#20110;DDPG&#12290;&#25105;&#20204;&#22987;&#32456;&#35266;&#23519;&#21040;&#22312;&#25152;&#26377;&#35780;&#20272;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#22312;&#19982;TD3&#21644;TD7&#30456;&#27604;&#24615;&#33021;&#26356;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#20248;&#36234;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#26159;&#20197;&#26174;&#33879;&#38477;&#20302;&#30340;&#35745;&#31639;&#35201;&#27714;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05732v1 Announce Type: new  Abstract: DDPG is hindered by the overestimation bias problem, wherein its $Q$-estimates tend to overstate the actual $Q$-values. Traditional solutions to this bias involve ensemble-based methods, which require significant computational resources, or complex log-policy-based approaches, which are difficult to understand and implement. In contrast, we propose a straightforward solution using a $Q$-target and incorporating a behavioral cloning (BC) loss penalty. This solution, acting as an uncertainty measure, can be easily implemented with minimal code and without the need for an ensemble. Our empirical findings strongly support the superiority of Conservative DDPG over DDPG across various MuJoCo and Bullet tasks. We consistently observe better performance in all evaluated tasks and even competitive or superior performance compared to TD3 and TD7, all achieved with significantly reduced computational requirements.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#25454;&#22686;&#24378;&#12289;&#39044;&#35757;&#32451;&#31639;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#30456;&#23545;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;SSL&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#65292;&#21457;&#29616;&#38500;&#20102;&#25913;&#21464;&#39044;&#35757;&#32451;&#31639;&#27861;&#22806;&#65292;&#26032;&#25968;&#25454;&#22686;&#24378;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26550;&#26500;&#20063;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;</title><link>https://arxiv.org/abs/2403.05726</link><description>&lt;p&gt;
&#22686;&#24378; vs &#31639;&#27861;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Augmentations vs Algorithms: What Works in Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05726
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#25454;&#22686;&#24378;&#12289;&#39044;&#35757;&#32451;&#31639;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#30456;&#23545;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;SSL&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#65292;&#21457;&#29616;&#38500;&#20102;&#25913;&#21464;&#39044;&#35757;&#32451;&#31639;&#27861;&#22806;&#65292;&#26032;&#25968;&#25454;&#22686;&#24378;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26550;&#26500;&#20063;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#12289;&#39044;&#35757;&#32451;&#31639;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20013;&#30340;&#30456;&#23545;&#24433;&#21709;&#12290;&#34429;&#28982;&#26368;&#36817;&#36825;&#19968;&#39046;&#22495;&#30340;&#25991;&#29486;&#35753;&#20154;&#35273;&#24471;&#39044;&#35757;&#32451;&#31639;&#27861;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35201;&#29702;&#35299;&#20854;&#24433;&#21709;&#21463;&#21046;&#20110;&#24456;&#38590;&#36827;&#34892;&#23458;&#35266;&#21644;&#30452;&#25509;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#35768;&#22810;&#30475;&#20284;&#19981;&#21516;&#30340;SSL&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#27169;&#26495;&#20013;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#20043;&#22788;&#65292;&#24182;&#35266;&#23519;&#21040;&#38500;&#20102;&#25913;&#21464;&#39044;&#35757;&#32451;&#31639;&#27861;&#22806;&#65292;&#35768;&#22810;&#20316;&#21697;&#36824;&#20351;&#29992;&#20102;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25110;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#27604;&#36739;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;SSL&#26041;&#27861;&#65292;&#21457;&#29616;&#35768;&#22810;&#31639;&#27861;&#28155;&#21152;&#65292;&#22914;&#39044;&#27979;&#32593;&#32476;&#25110;&#26032;&#30340;&#25439;&#22833;&#65292;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65288;&#36890;&#24120;&#20302;&#20110;1%&#65289;&#65292;&#32780;&#22686;&#24378;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05726v1 Announce Type: new  Abstract: We study the relative effects of data augmentations, pretraining algorithms, and model architectures in Self-Supervised Learning (SSL). While the recent literature in this space leaves the impression that the pretraining algorithm is of critical importance to performance, understanding its effect is complicated by the difficulty in making objective and direct comparisons between methods. We propose a new framework which unifies many seemingly disparate SSL methods into a single shared template. Using this framework, we identify aspects in which methods differ and observe that in addition to changing the pretraining algorithm, many works also use new data augmentations or more powerful model architectures. We compare several popular SSL methods using our framework and find that many algorithmic additions, such as prediction networks or new losses, have a minor impact on downstream task performance (often less than $1\%$), while enhanced a
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.05720</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05720
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#36890;&#36807;&#24635;&#32467;&#20020;&#24202;&#35760;&#24405;&#32780;&#29983;&#25104;&#30340;&#24120;&#35265;&#20020;&#24202;&#25991;&#20214;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#23454;&#38469;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#65288;&#22914;BHC&#21512;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#23637;&#31034;&#12290;&#20026;&#20102;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;BHC&#21512;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;MIMIC-IV&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#23553;&#35013;&#20102;&#20020;&#24202;&#35760;&#24405;&#21644;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#23545;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#36890;&#29992;LLMs&#21644;&#19977;&#20010;&#21307;&#30103;&#39046;&#22495;&#36866;&#24212;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#36827;&#20174;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;BHC&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;BHC&#65292;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#65288;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#21644;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#24212;&#29992;&#20110;&#19977;&#20010;&#24320;&#28304;LLMs&#65288;Clinical-T5-Large&#65292;Llama2-13B&#65292;FLAN-UL2&#65289;&#21644;&#20004;&#20010;&#19987;&#26377;LLMs&#65288;GPT-3.5&#65292;GPT-4&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#32593;&#32476;&#29289;&#29702;&#20154;&#31995;&#32479;&#20013;&#20154;&#24037;&#26234;&#33021;&#25512;&#33616;&#21644;&#20154;&#31867;&#20915;&#31574;&#32773;&#20043;&#38388;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#32771;&#34385;&#20154;&#31867;&#21487;&#33021;&#19981;&#21516;&#20110;AI&#24179;&#21488;&#30340;&#24863;&#30693;&#21644;&#35299;&#37322;&#26041;&#24335;&#65292;&#24314;&#31435;&#20102;&#26368;&#20339;&#25512;&#33616;&#31574;&#30053;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#36817;&#20284;&#20154;&#31867;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#24615;&#30028;&#38480;&#21644;&#25968;&#20540;&#31034;&#20363;&#39564;&#35777;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.05715</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#22312;&#32593;&#32476;&#29289;&#29702;&#20154;&#31995;&#32479;&#20013;&#36827;&#34892;&#26377;&#25928;&#20154;&#24037;&#26234;&#33021;&#25512;&#33616;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Effective AI Recommendations in Cyber-Physical-Human Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#32593;&#32476;&#29289;&#29702;&#20154;&#31995;&#32479;&#20013;&#20154;&#24037;&#26234;&#33021;&#25512;&#33616;&#21644;&#20154;&#31867;&#20915;&#31574;&#32773;&#20043;&#38388;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#32771;&#34385;&#20154;&#31867;&#21487;&#33021;&#19981;&#21516;&#20110;AI&#24179;&#21488;&#30340;&#24863;&#30693;&#21644;&#35299;&#37322;&#26041;&#24335;&#65292;&#24314;&#31435;&#20102;&#26368;&#20339;&#25512;&#33616;&#31574;&#30053;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#36817;&#20284;&#20154;&#31867;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#24615;&#30028;&#38480;&#21644;&#25968;&#20540;&#31034;&#20363;&#39564;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32593;&#32476;&#29289;&#29702;&#20154;&#31995;&#32479;&#65288;CPHS&#65289;&#28041;&#21450;&#21040;&#19968;&#20010;&#20154;&#31867;&#20915;&#31574;&#32773;&#65292;&#20182;&#21487;&#33021;&#20250;&#20174;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24179;&#21488;&#25509;&#25910;&#25512;&#33616;&#65292;&#21516;&#26102;&#21448;&#25345;&#26377;&#26368;&#32456;&#20915;&#31574;&#30340;&#36131;&#20219;&#12290;&#22312;&#36825;&#31867;CPHS&#24212;&#29992;&#20013;&#65292;&#20154;&#31867;&#20915;&#31574;&#32773;&#21487;&#33021;&#20250;&#22240;&#20026;&#21508;&#31181;&#21407;&#22240;&#32780;&#20559;&#31163;&#26368;&#20339;&#25512;&#33616;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#23454;&#26045;&#21478;&#19968;&#20010;&#12290;&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#26694;&#26550;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#20154;&#31867;&#21487;&#33021;&#20250;&#22240;&#20026;&#24863;&#30693;&#21644;&#35299;&#37322;&#31995;&#32479;&#29366;&#24577;&#30340;&#26041;&#24335;&#19982;AI&#24179;&#21488;&#19981;&#21516;&#32780;&#20559;&#31163;AI&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#26368;&#20339;&#25512;&#33616;&#31574;&#30053;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;AI&#20351;&#29992;&#30340;&#36817;&#20284;&#20154;&#31867;&#27169;&#22411;&#65288;AHM&#65289;&#12290;&#25105;&#20204;&#20026;&#30001;AHM&#20135;&#29983;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#25552;&#20379;&#20102;&#29702;&#35770;&#30028;&#38480;&#65292;&#24182;&#22312;&#19968;&#20010;&#25968;&#20540;&#31034;&#20363;&#20013;&#35828;&#26126;&#20102;&#25105;&#20204;&#32467;&#26524;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05715v1 Announce Type: cross  Abstract: Many cyber-physical-human systems (CPHS) involve a human decision-maker who may receive recommendations from an artificial intelligence (AI) platform while holding the ultimate responsibility of making decisions. In such CPHS applications, the human decision-maker may depart from an optimal recommended decision and instead implement a different one for various reasons. In this letter, we develop a rigorous framework to overcome this challenge. In our framework, we consider that humans may deviate from AI recommendations as they perceive and interpret the system's state in a different way than the AI platform. We establish the structural properties of optimal recommendation strategies and develop an approximate human model (AHM) used by the AI. We provide theoretical bounds on the optimality gap that arises from an AHM and illustrate the efficacy of our results in a numerical example.
&lt;/p&gt;</description></item><item><title>$\mathtt{tsGT}$&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;Transformer&#26550;&#26500;&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#36229;&#36807;&#20854;&#38543;&#26426;&#21516;&#34892;&#65292;&#29305;&#21035;&#22312;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#21644;&#36793;&#38469;&#20998;&#20301;&#20540;&#39044;&#27979;&#26041;&#38754;&#20855;&#22791;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.05713</link><description>&lt;p&gt;
$\mathtt{tsGT}$&#65306;&#20855;&#26377;Transformer&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
$\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05713
&lt;/p&gt;
&lt;p&gt;
$\mathtt{tsGT}$&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;Transformer&#26550;&#26500;&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#36229;&#36807;&#20854;&#38543;&#26426;&#21516;&#34892;&#65292;&#29305;&#21035;&#22312;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#21644;&#36793;&#38469;&#20998;&#20301;&#20540;&#39044;&#27979;&#26041;&#38754;&#20855;&#22791;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#22312;&#20960;&#20046;&#25152;&#26377;&#22788;&#29702;&#26102;&#38388;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#31185;&#23398;&#39046;&#22495;&#20013;&#37117;&#20855;&#26377;&#22522;&#30784;&#37325;&#35201;&#24615;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#22823;&#25209;&#20855;&#26377;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#26550;&#26500;&#20559;&#35265;&#30340;&#30830;&#23450;&#24615;Transformer&#27169;&#22411;&#12290;&#26412;&#25991;&#37319;&#21462;&#20102;&#19981;&#21516;&#30340;&#26041;&#21521;&#65292;&#24341;&#20837;&#20102;$\mathtt{tsGT}$&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;Transformer&#26550;&#26500;&#26500;&#24314;&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#19988;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#28378;&#21160;&#31383;&#21475;&#22238;&#27979;&#21644;&#35780;&#20272;&#21327;&#35758;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;$\mathtt{tsGT}$&#22312;&#22235;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#22312;MAD&#21644;RMSE&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#22312;QL&#21644;CRPS&#26041;&#38754;&#36229;&#36807;&#20102;&#20854;&#38543;&#26426;&#21516;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;$\mathtt{tsGT}$&#22312;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#21644;&#39044;&#27979;&#36793;&#38469;&#20998;&#20301;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#26469;&#34917;&#20805;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05713v1 Announce Type: new  Abstract: Time series methods are of fundamental importance in virtually any field of science that deals with temporally structured data. Recently, there has been a surge of deterministic transformer models with time series-specific architectural biases. In this paper, we go in a different direction by introducing $\mathtt{tsGT}$, a stochastic time series model built on a general-purpose transformer architecture. We focus on using a well-known and theoretically justified rolling window backtesting and evaluation protocol. We show that $\mathtt{tsGT}$ outperforms the state-of-the-art models on MAD and RMSE, and surpasses its stochastic peers on QL and CRPS, on four commonly used datasets. We complement these results with a detailed analysis of $\mathtt{tsGT}$'s ability to model the data distribution and predict marginal quantile values.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#24418;&#24335;&#21270;&#20102;&#33322;&#22825;&#22120;&#20219;&#21153;&#21644;&#23433;&#20840;&#35201;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#25506;&#35752;&#20102;&#20174;&#23433;&#20840;LTL&#35268;&#33539;&#26500;&#24314;&#33322;&#22825;&#22120;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20197;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#30340;&#35774;&#35745;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#23631;&#34109;&#19982;&#19981;&#21516;&#31574;&#30053;&#30340;&#20114;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05693</link><description>&lt;p&gt;
&#22797;&#26434;&#33322;&#22825;&#22120;&#20219;&#21153;&#30340;&#23631;&#34109;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#24418;&#24335;&#21270;&#20102;&#33322;&#22825;&#22120;&#20219;&#21153;&#21644;&#23433;&#20840;&#35201;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#25506;&#35752;&#20102;&#20174;&#23433;&#20840;LTL&#35268;&#33539;&#26500;&#24314;&#33322;&#22825;&#22120;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20197;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#30340;&#35774;&#35745;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#23631;&#34109;&#19982;&#19981;&#21516;&#31574;&#30053;&#30340;&#20114;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#33322;&#22825;&#22120;&#25511;&#21046;&#36890;&#36807;&#23631;&#34109;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;SDRL&#65289;&#24050;&#25104;&#20026;&#24555;&#36895;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#23631;&#34109;&#30340;&#26500;&#24314;&#21644;&#20219;&#21153;&#30340;&#23450;&#20041;&#20173;&#19981;&#22815;&#27491;&#24335;&#65292;&#23548;&#33268;&#31574;&#30053;&#26080;&#27861;&#20445;&#35777;&#23433;&#20840;&#24182;&#32473;RL&#20195;&#29702;&#35774;&#23450;&#20102;&#27169;&#26865;&#20004;&#21487;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#39318;&#20808;&#25506;&#35752;&#20102;&#20351;&#29992;&#24418;&#24335;&#35821;&#35328;&#65292;&#21363;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#65292;&#26469;&#24418;&#24335;&#21270;&#33322;&#22825;&#22120;&#20219;&#21153;&#21644;&#23433;&#20840;&#35201;&#27714;&#12290;&#28982;&#21518;&#23450;&#20041;&#20102;&#19968;&#31181;&#33258;&#21160;&#20174;co-safe LTL&#35268;&#33539;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#20197;&#26377;&#25928;&#35757;&#32451;SDRL&#26694;&#26550;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20026;&#33322;&#22825;&#22120;&#24212;&#29992;&#20174;&#23433;&#20840;LTL&#35268;&#33539;&#26500;&#24314;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20197;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#23631;&#34109;&#19982;&#19981;&#21516;&#31574;&#30053;&#30340;&#20114;&#21160;&#20197;&#21450;&#22870;&#21169;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05693v1 Announce Type: new  Abstract: Autonomous spacecraft control via Shielded Deep Reinforcement Learning (SDRL) has become a rapidly growing research area. However, the construction of shields and the definition of tasking remains informal, resulting in policies with no guarantees on safety and ambiguous goals for the RL agent. In this paper, we first explore the use of formal languages, namely Linear Temporal Logic (LTL), to formalize spacecraft tasks and safety requirements. We then define a manner in which to construct a reward function from a co-safe LTL specification automatically for effective training in SDRL framework. We also investigate methods for constructing a shield from a safe LTL specification for spacecraft applications and propose three designs that provide probabilistic guarantees. We show how these shields interact with different policies and the flexibility of the reward structure through several experiments.
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#30340;&#24212;&#29992;&#25552;&#39640;&#20102;&#24178;&#39044;&#30340;&#31934;&#20934;&#24230;&#65292;&#34429;&#28982;&#23384;&#22312;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#33021;&#20248;&#21270;&#26377;&#38480;&#30340;&#24178;&#39044;&#36164;&#28304;&#20351;&#29992;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.05683</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20998;&#35299;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#30340;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05683
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#30340;&#24212;&#29992;&#25552;&#39640;&#20102;&#24178;&#39044;&#30340;&#31934;&#20934;&#24230;&#65292;&#34429;&#28982;&#23384;&#22312;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#33021;&#20248;&#21270;&#26377;&#38480;&#30340;&#24178;&#39044;&#36164;&#28304;&#20351;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#21355;&#29983;&#35745;&#21010;&#20013;&#21463;&#30410;&#32773;&#21442;&#19982;&#24230;&#19979;&#38477;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20026;&#20102;&#25913;&#21892;&#20445;&#30041;&#29575;&#65292;&#20581;&#24247;&#24037;&#20316;&#32773;&#20250;&#23545;&#26377;&#36749;&#23398;&#39118;&#38505;&#30340;&#21463;&#30410;&#32773;&#36827;&#34892;&#24178;&#39044;&#65292;&#28982;&#32780;&#65292;&#20581;&#24247;&#24037;&#20316;&#32773;&#30340;&#21487;&#29992;&#24615;&#21644;&#26102;&#38388;&#26159;&#26377;&#38480;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#26377;&#30740;&#31350;&#33268;&#21147;&#20110;&#20351;&#29992;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#26426;&#26469;&#20248;&#21270;&#36825;&#20123;&#26377;&#38480;&#30340;&#24178;&#39044;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#36825;&#19968;&#26694;&#26550;&#30340;&#20851;&#38190;&#25216;&#26415;&#38556;&#30861;&#22312;&#20110;&#38656;&#35201;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#20272;&#31639;&#21463;&#30410;&#32773;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05683v1 Announce Type: new  Abstract: The declining participation of beneficiaries over time is a key concern in public health programs. A popular strategy for improving retention is to have health workers `intervene' on beneficiaries at risk of dropping out. However, the availability and time of these health workers are limited resources. As a result, there has been a line of research on optimizing these limited intervention resources using Restless Multi-Armed Bandits (RMABs). The key technical barrier to using this framework in practice lies in the need to estimate the beneficiaries' RMAB parameters from historical data. Recent research has shown that Decision-Focused Learning (DFL), which focuses on maximizing the beneficiaries' adherence rather than predictive accuracy, improves the performance of intervention targeting using RMABs. Unfortunately, these gains come at a high computational cost because of the need to solve and evaluate the RMAB in each DFL training step. 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#26469;&#20445;&#25252;&#22312;&#29615;&#22659;&#20013;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.05681</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;ially Private Tabular Data&#36827;&#34892;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;DP-TabICL
&lt;/p&gt;
&lt;p&gt;
DP-TabICL: In-Context Learning with Differentially Private Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05681
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#26469;&#20445;&#25252;&#22312;&#29615;&#22659;&#20013;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL)&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#22312;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#31034;&#33539;&#26465;&#20214;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#24182;&#19988;&#23427;&#24050;&#32463;&#34920;&#29616;&#20986;&#19982;&#26114;&#36149;&#30340;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#21644;&#24494;&#35843;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;ICL&#24050;&#34987;&#25193;&#23637;&#65292;&#20801;&#35768;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#20316;&#20026;&#31034;&#33539;&#31034;&#20363;&#65292;&#26041;&#27861;&#26159;&#23558;&#21333;&#20010;&#35760;&#24405;&#20018;&#34892;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#34920;&#26126;LLM&#21487;&#33021;&#20250;&#27844;&#38706;&#25552;&#31034;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#65292;&#32780;&#19988;&#30001;&#20110;&#34920;&#26684;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#22240;&#27492;&#20102;&#35299;&#22914;&#20309;&#20445;&#25252;ICL&#20013;&#20351;&#29992;&#30340;&#22522;&#30784;&#34920;&#26684;&#25968;&#25454;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#20316;&#20026;&#23545;&#22914;&#20309;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#36827;&#34892;&#21021;&#22987;&#25506;&#32034;&#30340;&#30740;&#31350;--&#24046;&#20998;&#38544;&#31169;&#26159;&#25968;&#25454;&#38544;&#31169;&#21644;&#21311;&#21517;&#21270;&#30340;&#38271;&#26399;&#37329;&#26631;&#20934;--&#20197;&#20445;&#25252;ICL&#20013;&#20351;&#29992;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#25968;&#25454;&#31169;&#26377;&#21270;&#26426;&#21046;&#22312;&#31169;&#26377;&#34920;&#26684;ICL&#20013;&#24212;&#29992;DP&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05681v1 Announce Type: cross  Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks by conditioning on demonstrations of question-answer pairs and it has been shown to have comparable performance to costly model retraining and fine-tuning. Recently, ICL has been extended to allow tabular data to be used as demonstration examples by serializing individual records into natural language formats. However, it has been shown that LLMs can leak information contained in prompts, and since tabular data often contain sensitive information, understanding how to protect the underlying tabular data used in ICL is a critical area of research. This work serves as an initial investigation into how to use differential privacy (DP) -- the long-established gold standard for data privacy and anonymization -- to protect tabular data used in ICL. Specifically, we investigate the application of DP mechanisms for private tabular ICL via data privatization pr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28155;&#21152;&#39069;&#22806;&#33410;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#25968;&#20540;&#21644;&#20998;&#31867;&#20449;&#24687;&#21516;&#26102;&#32435;&#20837;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#25110;&#22797;&#26434;&#30456;&#20284;&#24615;&#20989;&#25968;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.05669</link><description>&lt;p&gt;
&#36890;&#36807;&#39069;&#22806;&#22270;&#33410;&#28857;&#30340;&#26041;&#27861;&#23545;&#20998;&#31867;&#21644;&#28151;&#21512;&#25968;&#25454;&#36827;&#34892;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spectral Clustering of Categorical and Mixed-type Data via Extra Graph Nodes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05669
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28155;&#21152;&#39069;&#22806;&#33410;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#25968;&#20540;&#21644;&#20998;&#31867;&#20449;&#24687;&#21516;&#26102;&#32435;&#20837;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#25110;&#22797;&#26434;&#30456;&#20284;&#24615;&#20989;&#25968;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25968;&#25454;&#23545;&#35937;&#32858;&#31867;&#25104;&#21516;&#36136;&#32676;&#20307;&#26159;&#25968;&#25454;&#25366;&#25496;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#35889;&#32858;&#31867;&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#22362;&#23454;&#19988;&#36866;&#24212;&#22810;&#31181;&#29616;&#23454;&#22330;&#26223;&#30340;&#26368;&#37325;&#35201;&#30340;&#32858;&#31867;&#31639;&#27861;&#20043;&#19968;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#30340;&#26041;&#27861;&#65292;&#23558;&#25968;&#20540;&#21644;&#20998;&#31867;&#20449;&#24687;&#21516;&#26102;&#32435;&#20837;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#25110;&#20351;&#29992;&#22797;&#26434;&#30456;&#20284;&#24615;&#20989;&#25968;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05669v1 Announce Type: cross  Abstract: Clustering data objects into homogeneous groups is one of the most important tasks in data mining. Spectral clustering is arguably one of the most important algorithms for clustering, as it is appealing for its theoretical soundness and is adaptable to many real-world data settings. For example, mixed data, where the data is composed of numerical and categorical features, is typically handled via numerical discretization, dummy coding, or similarity computation that takes into account both data types. This paper explores a more natural way to incorporate both numerical and categorical information into the spectral clustering algorithm, avoiding the need for data preprocessing or the use of sophisticated similarity functions. We propose adding extra nodes corresponding to the different categories the data may belong to and show that it leads to an interpretable clustering objective function. Furthermore, we demonstrate that this simple 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;ICP&#31639;&#27861;&#36827;&#34892;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25915;&#20987;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#35780;&#20272;&#20854;&#40065;&#26834;&#24615;&#65292;&#37325;&#28857;&#22312;&#20110;&#25214;&#21040;&#21487;&#33021;&#30340;&#26368;&#22823;ICP&#23039;&#21183;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.05666</link><description>&lt;p&gt;
&#38754;&#23545;&#26368;&#22351;&#24773;&#20917;&#65306;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#23545;ICP&#31639;&#27861;&#40065;&#26834;&#24615;&#20998;&#26512;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Prepared for the Worst: A Learning-Based Adversarial Attack for Resilience Analysis of the ICP Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05666
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;ICP&#31639;&#27861;&#36827;&#34892;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25915;&#20987;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#35780;&#20272;&#20854;&#40065;&#26834;&#24615;&#65292;&#37325;&#28857;&#22312;&#20110;&#25214;&#21040;&#21487;&#33021;&#30340;&#26368;&#22823;ICP&#23039;&#21183;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25915;&#20987;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#26469;&#35780;&#20272;&#36845;&#20195;&#26368;&#36817;&#28857;&#65288;ICP&#65289;&#31639;&#27861;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#23545;&#20110;&#20687;&#33258;&#20027;&#23548;&#33322;&#36825;&#26679;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#30830;&#20445;&#31639;&#27861;&#22312;&#37096;&#32626;&#21069;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;ICP&#31639;&#27861;&#24050;&#25104;&#20026;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#23450;&#20301;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#23427;&#20135;&#29983;&#30340;&#23039;&#21183;&#20272;&#35745;&#21487;&#33021;&#20250;&#21463;&#21040;&#27979;&#37327;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25968;&#25454;&#30340;&#27745;&#26579;&#21487;&#33021;&#26469;&#33258;&#21508;&#31181;&#22330;&#26223;&#65292;&#22914;&#36974;&#25377;&#12289;&#24694;&#21155;&#22825;&#27668;&#25110;&#20256;&#24863;&#22120;&#30340;&#26426;&#26800;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;ICP&#30340;&#22797;&#26434;&#21644;&#36845;&#20195;&#29305;&#24615;&#20351;&#24471;&#35780;&#20272;&#20854;&#23545;&#27745;&#26579;&#30340;&#40065;&#26834;&#24615;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#20154;&#21162;&#21147;&#21019;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#21644;&#24320;&#21457;&#20223;&#30495;&#26469;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;ICP&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#36890;&#36807;&#22522;&#20110;&#25200;&#21160;&#30340;&#23545;&#25239;&#25915;&#20987;&#25214;&#21040;&#26368;&#22823;&#21487;&#33021;&#30340;ICP&#23039;&#21183;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05666v1 Announce Type: cross  Abstract: This paper presents a novel method to assess the resilience of the Iterative Closest Point (ICP) algorithm via deep-learning-based attacks on lidar point clouds. For safety-critical applications such as autonomous navigation, ensuring the resilience of algorithms prior to deployments is of utmost importance. The ICP algorithm has become the standard for lidar-based localization. However, the pose estimate it produces can be greatly affected by corruption in the measurements. Corruption can arise from a variety of scenarios such as occlusions, adverse weather, or mechanical issues in the sensor. Unfortunately, the complex and iterative nature of ICP makes assessing its resilience to corruption challenging. While there have been efforts to create challenging datasets and develop simulations to evaluate the resilience of ICP empirically, our method focuses on finding the maximum possible ICP pose error using perturbation-based adversarial
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2403.05652</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
What is different between these datasets?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05652
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05652v1 Announce Type: cross  Abstract: The performance of machine learning models heavily depends on the quality of input data, yet real-world applications often encounter various data-related challenges. One such challenge could arise when curating training data or deploying the model in the real world - two comparable datasets in the same domain may have different distributions. While numerous techniques exist for detecting distribution shifts, the literature lacks comprehensive approaches for explaining dataset differences in a human-understandable manner. To address this gap, we propose a suite of interpretable methods (toolbox) for comparing two datasets. We demonstrate the versatility of our approach across diverse data modalities, including tabular data, language, images, and signals in both low and high-dimensional settings. Our methods not only outperform comparable and related approaches in terms of explanation quality and correctness, but also provide actionable,
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;&#65292;&#25552;&#20379;&#20102;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#21487;&#38752;&#31639;&#27861;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#33298;&#36866;&#24230;&#24182;&#20419;&#36827;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.05645</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Geometric Neural Network based on Phase Space for BCI decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05645
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;&#65292;&#25552;&#20379;&#20102;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#21487;&#38752;&#31639;&#27861;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#33298;&#36866;&#24230;&#24182;&#20419;&#36827;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deep Learning(DL)&#31639;&#27861;&#19982;&#33041;&#20449;&#21495;&#20998;&#26512;&#30340;&#25972;&#21512;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#65292;&#30456;&#27604;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;(BCI)&#39046;&#22495;&#23588;&#20026;&#31361;&#20986;&#65292;BCI&#36890;&#36807;&#35299;&#30721;&#22823;&#33041;&#27963;&#21160;&#25511;&#21046;&#22806;&#37096;&#35774;&#22791;&#32780;&#26080;&#38656;&#32908;&#32905;&#25511;&#21046;&#12290;&#33041;&#30005;&#22270;(EEG)&#26159;&#35774;&#35745;BCI&#31995;&#32479;&#30340;&#24191;&#27867;&#36873;&#25321;&#65292;&#22240;&#20854;&#26080;&#21019;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#20986;&#33394;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20294;&#32570;&#23569;&#35757;&#32451;&#25968;&#25454;&#12289;&#20449;&#22122;&#27604;&#20302;&#12289;&#20197;&#21450;&#22312;&#20010;&#20307;&#38388;&#21644;&#20869;&#37096;&#30340;&#22823;&#37327;&#21464;&#21270;&#12290; &#26368;&#21518;&#65292;&#20351;&#29992;&#22810;&#20010;&#30005;&#26497;&#35774;&#32622;BCI&#31995;&#32479;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#65292;&#38459;&#30861;&#21487;&#38752;DL&#26550;&#26500;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#20043;&#22806;&#30340;BCI&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290; &#20026;&#20102;&#25552;&#39640;&#37319;&#32435;&#29575;&#65292;&#25105;&#20204;&#38656;&#35201;&#25913;&#21892;&#29992;&#25143;&#33298;&#36866;&#24230;&#65292;&#20363;&#22914;&#20351;&#29992;&#23569;&#37327;&#30005;&#26497;&#25805;&#20316;&#30340;&#21487;&#38752;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05645v1 Announce Type: cross  Abstract: The integration of Deep Learning (DL) algorithms on brain signal analysis is still in its nascent stages compared to their success in fields like Computer Vision, especially in Brain-Computer Interface (BCI), where the brain activity is decoded to control external devices without requiring muscle control. Electroencephalography (EEG) is a widely adopted choice for designing BCI systems due to its non-invasive and cost-effective nature and excellent temporal resolution. Still, it comes at the expense of limited training data, poor signal-to-noise, and a large variability across and within-subject recordings. Finally, setting up a BCI system with many electrodes takes a long time, hindering the widespread adoption of reliable DL architectures in BCIs outside research laboratories. To improve adoption, we need to improve user comfort using, for instance, reliable algorithms that operate with few electrodes. \textbf{Approach:} Our research
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#29289;&#29702;&#25968;&#25454;&#21644;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;OmniJet-$\alpha$&#26159;&#39318;&#20010;&#36328;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#24182;&#23637;&#31034;&#20102;&#22312;&#26080;&#30417;&#30563;&#38382;&#39064;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05618</link><description>&lt;p&gt;
OmniJet-$\alpha$: &#31890;&#23376;&#29289;&#29702;&#23398;&#30340;&#39318;&#20010;&#36328;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OmniJet-$\alpha$: The first cross-task foundation model for particle physics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05618
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#25968;&#25454;&#21644;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;OmniJet-$\alpha$&#26159;&#39318;&#20010;&#36328;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#24182;&#23637;&#31034;&#20102;&#22312;&#26080;&#30417;&#30563;&#38382;&#39064;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#22810;&#25968;&#25454;&#38598;&#21644;&#22810;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#19968;&#32463;&#39044;&#35757;&#32451;&#65292;&#20415;&#21487;&#34987;&#24494;&#35843;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#25104;&#21151;&#24320;&#21457;&#20986;&#36825;&#31181;&#36890;&#29992;&#29289;&#29702;&#25968;&#25454;&#27169;&#22411;&#23558;&#26159;&#19968;&#39033;&#37325;&#22823;&#31361;&#30772;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#21487;&#23454;&#29616;&#30340;&#29289;&#29702;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#22312;&#36825;&#19968;&#25361;&#25112;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#26469;&#35780;&#21028;&#20174;&#29289;&#29702;&#25968;&#25454;&#36716;&#25442;&#20026;&#36866;&#21512;&#21464;&#21387;&#22120;&#26550;&#26500;&#65288;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#39592;&#24178;&#65289;&#36827;&#34892;&#33258;&#22238;&#24402;&#29983;&#25104;&#31890;&#23376;&#21943;&#27969;&#30340;&#34920;&#31034;&#36136;&#37327;&#12290;&#36825;&#20123;&#25514;&#26045;&#25903;&#25345;&#20102;&#30456;&#36739;&#20110;&#20808;&#21069;&#24037;&#20316;&#30340;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#26631;&#35760;&#21270;&#30340;&#36873;&#25321;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#30417;&#30563;&#38382;&#39064;&#65288;&#21943;&#27969;&#29983;&#25104;&#65289;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05618v1 Announce Type: cross  Abstract: Foundation models are multi-dataset and multi-task machine learning methods that once pre-trained can be fine-tuned for a large variety of downstream applications. The successful development of such general-purpose models for physics data would be a major breakthrough as they could improve the achievable physics performance while at the same time drastically reduce the required amount of training time and data.   We report significant progress on this challenge on several fronts. First, a comprehensive set of evaluation methods is introduced to judge the quality of an encoding from physics data into a representation suitable for the autoregressive generation of particle jets with transformer architectures (the common backbone of foundation models). These measures motivate the choice of a higher-fidelity tokenization compared to previous works. Finally, we demonstrate transfer learning between an unsupervised problem (jet generation) an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#24494;&#35843;&#31034;&#20363;&#30340;&#30417;&#30563;&#26469;&#25511;&#21046;&#20854;&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#20102;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.05612</link><description>&lt;p&gt;
&#19981;&#29087;&#24713;&#30340;&#24494;&#35843;&#31034;&#20363;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Unfamiliar Finetuning Examples Control How Language Models Hallucinate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#24494;&#35843;&#31034;&#20363;&#30340;&#30417;&#30563;&#26469;&#25511;&#21046;&#20854;&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#20102;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20542;&#21521;&#20110;&#29983;&#25104;&#21548;&#36215;&#26469;&#20196;&#20154;&#20449;&#26381;&#20294;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#29305;&#21035;&#26159;&#24403;&#22312;&#19981;&#29087;&#24713;&#30340;&#27010;&#24565;&#19978;&#36827;&#34892;&#26597;&#35810;&#26102;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35843;&#25972;&#21518;&#30340;LLMs&#22914;&#20309;&#20135;&#29983;&#24187;&#35273;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#27169;&#24335;&#65306;&#38543;&#30528;&#36755;&#20837;&#21464;&#24471;&#26356;&#19981;&#29087;&#24713;&#65292;LLMs&#30340;&#36755;&#20986;&#20542;&#21521;&#20110;&#40664;&#35748;&#20026;"&#21547;&#31946;&#20854;&#35789;"&#30340;&#39044;&#27979;&#65292;&#20854;&#24418;&#24335;&#21463;&#24494;&#35843;&#25968;&#25454;&#20013;&#19981;&#29087;&#24713;&#31034;&#20363;&#30417;&#30563;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#20462;&#25913;&#36825;&#20123;&#31034;&#20363;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;LLM&#23545;&#19981;&#29087;&#24713;&#36755;&#20837;&#30340;&#39044;&#27979;&#65288;&#20363;&#22914;&#65292;&#25945;&#20250;&#23427;&#20204;&#35828;&#8220;&#25105;&#19981;&#30693;&#36947;&#8221;&#65289;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;RL&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22870;&#21169;&#27169;&#22411;&#24187;&#35273;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#26356;&#21487;&#38752;&#22320;&#20943;&#36731;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MMLU&#19978;&#30340;&#22810;&#36873;QA&#20013;&#36827;&#34892;&#19968;&#31995;&#21015;&#21463;&#25511;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05612v1 Announce Type: cross  Abstract: Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts. In this work, we explore the underlying mechanisms that govern how finetuned LLMs hallucinate. Our investigation reveals an interesting pattern: as inputs become more unfamiliar, LLM outputs tend to default towards a ``hedged'' prediction, whose form is determined by how the unfamiliar examples in the finetuning data are supervised. Thus, by strategically modifying these examples' supervision, we can control LLM predictions for unfamiliar inputs (e.g., teach them to say ``I don't know''). Based on these principles, we develop an RL approach that more reliably mitigates hallucinations for long-form generation tasks, by tackling the challenges presented by reward model hallucinations. We validate our findings with a series of controlled experiments in multiple-choice QA on MMLU, as
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#20957;&#32858;&#25910;&#25947;&#32676;&#20307;&#65292;&#20197;&#25299;&#23637;&#26368;&#36817;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.05610</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#20013;&#20957;&#32858;&#25910;&#25947;&#32676;&#20307;&#23384;&#22312;&#30340;&#35777;&#25454;&#12289;&#23450;&#20041;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evidence, Definitions and Algorithms regarding the Existence of Cohesive-Convergence Groups in Neural Network Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05610
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#20957;&#32858;&#25910;&#25947;&#32676;&#20307;&#65292;&#20197;&#25299;&#23637;&#26368;&#36817;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36807;&#31243;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#22797;&#26434;&#21644;&#20851;&#38190;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#23613;&#31649;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#19982;&#35813;&#39046;&#22495;&#30340;&#26174;&#33879;&#25104;&#21151;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#36825;&#19968;&#27010;&#24565;&#20173;&#28982;&#20027;&#35201;&#26159;&#29702;&#35770;&#24615;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#30340;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#38750;&#20984;&#24615;&#36136;&#65292;&#24456;&#23569;&#35757;&#32451;&#30340;&#32593;&#32476;&#23454;&#38469;&#19978;&#23454;&#29616;&#25910;&#25947;&#12290;&#20026;&#20102;&#25193;&#22823;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#26412;&#25991;&#23558;&#35752;&#35770;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#20957;&#32858;&#25910;&#25947;&#32676;&#20307;&#35266;&#23519;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05610v1 Announce Type: new  Abstract: Understanding the convergence process of neural networks is one of the most complex and crucial issues in the field of machine learning. Despite the close association of notable successes in this domain with the convergence of artificial neural networks, this concept remains predominantly theoretical. In reality, due to the non-convex nature of the optimization problems that artificial neural networks tackle, very few trained networks actually achieve convergence. To expand recent research efforts on artificial-neural-network convergence, this paper will discuss a different approach based on observations of cohesive-convergence groups emerging during the optimization process of an artificial neural network.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#29992;&#20110;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#20419;&#36827;&#20102;&#23545;&#32597;&#35265;&#30142;&#30149;&#30340;&#35786;&#26029;&#65292;&#24182;&#22312;&#20020;&#24202;&#23454;&#36341;&#21644;&#21307;&#23398;&#25945;&#32946;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>https://arxiv.org/abs/2403.05606</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#29992;&#20110;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#33033;&#32476;&#33180;&#32959;&#30244;
&lt;/p&gt;
&lt;p&gt;
A Concept-based Interpretable Model for the Diagnosis of Choroid Neoplasias using Multimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05606
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#29992;&#20110;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#20419;&#36827;&#20102;&#23545;&#32597;&#35265;&#30142;&#30149;&#30340;&#35786;&#26029;&#65292;&#24182;&#22312;&#20020;&#24202;&#23454;&#36341;&#21644;&#21307;&#23398;&#25945;&#32946;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35786;&#26029;&#32597;&#35265;&#30142;&#30149;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#38754;&#20020;&#30528;&#20849;&#21516;&#25361;&#25112;&#65292;&#38656;&#35201;&#19987;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;&#25165;&#33021;&#20934;&#30830;&#35782;&#21035;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#36825;&#31867;&#25216;&#26415;&#30340;&#21457;&#23637;&#21463;&#21040;&#32597;&#35265;&#29366;&#20917;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#38656;&#35201;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#36182;&#24615;&#30340;&#27169;&#22411;&#30340;&#38656;&#27714;&#30340;&#38459;&#30861;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20855;&#26377;&#20154;&#31867;&#21487;&#35835;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#20419;&#36827;&#20020;&#24202;&#21307;&#29983;&#30340;&#39564;&#35777;&#24182;&#20419;&#36827;&#21307;&#23398;&#25945;&#32946;&#12290;&#22312;&#24403;&#21069;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#36825;&#26159;&#25104;&#20154;&#20013;&#26368;&#24120;&#35265;&#30340;&#30524;&#30555;&#30284;&#30151;&#24418;&#24335;&#65292;&#23613;&#31649;&#32597;&#35265;&#65292;&#32633;&#24739;&#29575;&#20026;&#27599;&#30334;&#19975;&#20154;5.1&#20363;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#20849;&#21253;&#25324;750&#21517;&#24739;&#32773;&#65292;&#28085;&#30422;&#20102;&#20174;2004&#24180;&#33267;2022&#24180;&#25910;&#38598;&#30340;&#19977;&#31181;&#19981;&#21516;&#25104;&#20687;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#21487;&#21306;&#20998;&#19977;&#31181;&#31867;&#22411;&#30340;&#33033;&#32476;&#33180;&#32959;&#30244;&#65292;&#34701;&#21512;&#20102;&#19968;&#20123;&#19981;&#22826;&#37325;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05606v1 Announce Type: cross  Abstract: Diagnosing rare diseases presents a common challenge in clinical practice, necessitating the expertise of specialists for accurate identification. The advent of machine learning offers a promising solution, while the development of such technologies is hindered by the scarcity of data on rare conditions and the demand for models that are both interpretable and trustworthy in a clinical context. Interpretable AI, with its capacity for human-readable outputs, can facilitate validation by clinicians and contribute to medical education. In the current work, we focus on choroid neoplasias, the most prevalent form of eye cancer in adults, albeit rare with 5.1 per million. We built the so-far largest dataset consisting of 750 patients, incorporating three distinct imaging modalities collected from 2004 to 2022. Our work introduces a concept-based interpretable model that distinguishes between three types of choroidal tumors, integrating insig
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20851;&#31995;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;Transformer-based&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#22810;&#28304;PPI&#35821;&#26009;&#24211;&#65292;&#25913;&#36827;&#20102;&#20851;&#31995;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05602</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20851;&#32852;&#19978;&#19979;&#25991;&#20449;&#24687;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;PPIs&#65289;
&lt;/p&gt;
&lt;p&gt;
Extracting Protein-Protein Interactions (PPIs) from Biomedical Literature using Attention-based Relational Context Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05602
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20851;&#31995;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;Transformer-based&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#22810;&#28304;PPI&#35821;&#26009;&#24211;&#65292;&#25913;&#36827;&#20102;&#20851;&#31995;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;PPIs&#65289;&#23545;&#20110;&#29702;&#35299;&#29983;&#21629;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#23545;&#20110;&#25506;&#31350;&#30142;&#30149;&#21457;&#23637;&#21644;&#35782;&#21035;&#22522;&#22240;/&#34507;&#30333;&#36136;&#21151;&#33021;&#20197;&#21450;&#29983;&#29289;&#36807;&#31243;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#22810;&#28304;PPI&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#20108;&#20803;&#20132;&#20114;&#31867;&#22411;&#26631;&#31614;&#22686;&#24378;&#20102;&#32463;&#36807;&#23457;&#26597;&#30340;&#20132;&#20114;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#30340;&#20851;&#32852;&#19978;&#19979;&#25991;&#20449;&#24687;&#36827;&#34892;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#20851;&#31995;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05602v1 Announce Type: cross  Abstract: Because protein-protein interactions (PPIs) are crucial to understand living systems, harvesting these data is essential to probe disease development and discern gene/protein functions and biological processes. Some curated datasets contain PPI data derived from the literature and other sources (e.g., IntAct, BioGrid, DIP, and HPRD). However, they are far from exhaustive, and their maintenance is a labor-intensive process. On the other hand, machine learning methods to automate PPI knowledge extraction from the scientific literature have been limited by a shortage of appropriate annotated data. This work presents a unified, multi-source PPI corpora with vetted interaction definitions augmented by binary interaction type labels and a Transformer-based deep learning method that exploits entities' relational context information for relation representation to improve relation classification performance. The model's performance is evaluated
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19987;&#23478;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#32423;&#29305;&#24449;&#23454;&#29616;&#21160;&#24577;&#38477;&#20302;&#20219;&#21153;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20026;&#26410;&#26469;&#35774;&#35745;&#36731;&#37327;&#32423;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#32593;&#32476;&#38138;&#24179;&#20102;&#36947;&#36335;</title><link>https://arxiv.org/abs/2403.05601</link><description>&lt;p&gt;
&#36873;&#25321;&#39640;&#32423;&#29305;&#24449;&#65306;&#20998;&#23618;&#20998;&#31867;&#32593;&#32476;&#20013;&#30340;&#39640;&#25928;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Select High-Level Features: Efficient Experts from a Hierarchical Classification Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05601
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19987;&#23478;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#32423;&#29305;&#24449;&#23454;&#29616;&#21160;&#24577;&#38477;&#20302;&#20219;&#21153;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20026;&#26410;&#26469;&#35774;&#35745;&#36731;&#37327;&#32423;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#32593;&#32476;&#38138;&#24179;&#20102;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19987;&#23478;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#21160;&#24577;&#20943;&#23569;&#20219;&#21153;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#39044;&#27979;&#24615;&#33021;&#12290;&#23427;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#20998;&#31867;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#23558;&#36890;&#29992;&#20302;&#32423;&#29305;&#24449;&#30340;&#39034;&#24207;&#22788;&#29702;&#19982;&#39640;&#32423;&#29305;&#24449;&#30340;&#24182;&#34892;&#22788;&#29702;&#21644;&#23884;&#22871;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#32467;&#26500;&#20801;&#35768;&#21019;&#26032;&#30340;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65306;&#33021;&#22815;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#31867;&#21035;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20960;&#20046;&#21487;&#20197;&#36339;&#36807;&#25152;&#26377;&#19981;&#24517;&#35201;&#30340;&#39640;&#32423;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#26465;&#20214;&#19979;&#38750;&#24120;&#26377;&#30410;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#26041;&#27861;&#20026;&#26410;&#26469;&#36731;&#37327;&#32423;&#21644;&#21487;&#36866;&#24212;&#30340;&#32593;&#32476;&#35774;&#35745;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20174;&#32039;&#20945;&#36793;&#32536;&#35774;&#22791;&#21040;&#22823;&#22411;&#20113;&#31471;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#22312;&#21160;&#24577;&#25512;&#29702;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#25490;&#38500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05601v1 Announce Type: new  Abstract: This study introduces a novel expert generation method that dynamically reduces task and computational complexity without compromising predictive performance. It is based on a new hierarchical classification network topology that combines sequential processing of generic low-level features with parallelism and nesting of high-level features. This structure allows for the innovative extraction technique: the ability to select only high-level features of task-relevant categories. In certain cases, it is possible to skip almost all unneeded high-level features, which can significantly reduce the inference cost and is highly beneficial in resource-constrained conditions. We believe this method paves the way for future network designs that are lightweight and adaptable, making them suitable for a wide range of applications, from compact edge devices to large-scale clouds. In terms of dynamic inference our methodology can achieve an exclusion 
&lt;/p&gt;</description></item><item><title>&#23494;&#24230;&#22238;&#24402;&#26159;&#19968;&#31181;&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36317;&#31163;&#24863;&#30693;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#20135;&#29983;&#39640;&#36136;&#37327;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.05600</link><description>&lt;p&gt;
&#23494;&#24230;&#22238;&#24402;&#65306;&#38754;&#21521;&#20998;&#24067;&#20559;&#31227;&#19979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#39640;&#25928;&#19988;&#36317;&#31163;&#24863;&#30693;&#30340;&#28145;&#24230;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
Density-Regression: Efficient and Distance-Aware Deep Regressor for Uncertainty Estimation under Distribution Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05600
&lt;/p&gt;
&lt;p&gt;
&#23494;&#24230;&#22238;&#24402;&#26159;&#19968;&#31181;&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36317;&#31163;&#24863;&#30693;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#20135;&#29983;&#39640;&#36136;&#37327;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#21512;&#22863;&#25216;&#26415;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#30340;&#22810;&#27425;&#21069;&#21521;&#20256;&#36882;&#23454;&#29616;&#24378;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#20250;&#21344;&#29992;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#24182;&#19988;&#25512;&#26029;&#65288;&#27979;&#35797;&#65289;&#26102;&#38388;&#36739;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23494;&#24230;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#21033;&#29992;&#23494;&#24230;&#20989;&#25968;&#65292;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;&#25105;&#20204;&#35777;&#26126;&#23427;&#22312;&#29305;&#24449;&#31354;&#38388;&#19978;&#20855;&#26377;&#36317;&#31163;&#24863;&#30693;&#65292;&#36825;&#26159;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#39640;&#36136;&#37327;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#31435;&#26041;&#20307;&#29609;&#20855;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;UCI&#25968;&#25454;&#38598;&#12289;&#20855;&#26377;&#26102;&#38388;&#24207;&#21015;&#30340;&#22825;&#27668;&#39044;&#27979;&#21644;&#30495;&#23454;&#19990;&#30028;&#20559;&#31227;&#24212;&#29992;&#19979;&#30340;&#28145;&#24230;&#20272;&#35745;&#31561;&#22238;&#24402;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#65292;&#23494;&#24230;&#22238;&#24402;&#19982;&#29616;&#20195;&#28145;&#24230;&#22238;&#24402;&#22120;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#27169;&#24335;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05600v1 Announce Type: new  Abstract: Morden deep ensembles technique achieves strong uncertainty estimation performance by going through multiple forward passes with different models. This is at the price of a high storage space and a slow speed in the inference (test) time. To address this issue, we propose Density-Regression, a method that leverages the density function in uncertainty estimation and achieves fast inference by a single forward pass. We prove it is distance aware on the feature space, which is a necessary condition for a neural network to produce high-quality uncertainty estimation under distribution shifts. Empirically, we conduct experiments on regression tasks with the cubic toy dataset, benchmark UCI, weather forecast with time series, and depth estimation under real-world shifted applications. We show that Density-Regression has competitive uncertainty estimation performance under distribution shifts with modern deep regressors while using a lower mode
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#30028;&#25903;&#25345;&#30340;&#39640;&#26031;&#26426;&#21046;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#30456;&#20851;&#26680;&#31639;&#19979;&#22686;&#24378;&#38544;&#31169;&#20445;&#35777;</title><link>https://arxiv.org/abs/2403.05598</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#30028;&#25903;&#25345;&#30340;&#39640;&#26031;&#26426;&#21046;&#23454;&#29616;&#38544;&#31169;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Privacy Amplification for the Gaussian Mechanism via Bounded Support
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05598
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#30028;&#25903;&#25345;&#30340;&#39640;&#26031;&#26426;&#21046;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#30456;&#20851;&#26680;&#31639;&#19979;&#22686;&#24378;&#38544;&#31169;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.05598v1 &#20844;&#24067;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#25968;&#25454;&#30456;&#20851;&#30340;&#38544;&#31169;&#26680;&#31639;&#26694;&#26550;&#65292;&#22914;&#27599;&#23454;&#20363;&#24046;&#20998;&#38544;&#31169;&#65288;pDP&#65289;&#21644;&#36153;&#33293;&#23572;&#20449;&#24687;&#25439;&#22833;&#65288;FIL&#65289;&#65292;&#20026;&#22266;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20010;&#20307;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#19982;&#20256;&#32479;&#30340;&#24046;&#20998;&#38544;&#31169;&#30456;&#27604;&#65292;&#36825;&#20123;&#20445;&#35777;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#21487;&#33021;&#26356;&#20026;&#29702;&#24819;&#65292;&#22240;&#20026;&#23427;&#20204;&#20005;&#26684;&#22320;&#19978;&#30028;&#32422;&#26463;&#20102;$\textit{&#23454;&#38469;}$&#25968;&#25454;&#38598;&#20013;$\textit{&#29305;&#23450;}$&#20010;&#20307;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#26368;&#22351;&#24773;&#20917;&#30340;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#36825;&#20123;&#26694;&#26550;&#24320;&#22987;&#21463;&#21040;&#27426;&#36814;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#32570;&#20047;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#26680;&#31639;&#20248;&#21183;&#30340;&#31169;&#20154;&#26426;&#21046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26377;&#30028;&#25903;&#25345;&#30340;&#39640;&#26031;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#65292;&#35777;&#26126;&#23427;&#20204;&#22312;&#25968;&#25454;&#30456;&#20851;&#26680;&#31639;&#19979;&#22686;&#24378;&#20102;&#38544;&#31169;&#20445;&#35777;&#12290;&#36890;&#36807;&#23545;DP-SGD&#27169;&#22411;&#35757;&#32451;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#26377;&#30028;&#25903;&#25345;&#39640;&#26031;&#26426;&#21046;&#21487;&#20197;&#38477;&#20302;pDP&#30028;&#38480;$\epsilon$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05598v1 Announce Type: cross  Abstract: Data-dependent privacy accounting frameworks such as per-instance differential privacy (pDP) and Fisher information loss (FIL) confer fine-grained privacy guarantees for individuals in a fixed training dataset. These guarantees can be desirable compared to vanilla DP in real world settings as they tightly upper-bound the privacy leakage for a $\textit{specific}$ individual in an $\textit{actual}$ dataset, rather than considering worst-case datasets. While these frameworks are beginning to gain popularity, to date, there is a lack of private mechanisms that can fully leverage advantages of data-dependent accounting. To bridge this gap, we propose simple modifications of the Gaussian mechanism with bounded support, showing that they amplify privacy guarantees under data-dependent accounting. Experiments on model training with DP-SGD show that using bounded support Gaussian mechanisms can provide a reduction of the pDP bound $\epsilon$ by
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#19979;&#32930;&#32908;&#30005;&#22270;&#25968;&#25454;&#19978;&#36827;&#34892;&#27493;&#24577;&#30456;&#20301;&#26816;&#27979;&#65292;&#23545;&#20110;&#25511;&#21046;&#19979;&#32930;&#36741;&#21161;&#35774;&#22791;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.05595</link><description>&lt;p&gt;
&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#27604;&#36739;&#27493;&#24577;&#30456;&#20301;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Comparison of gait phase detection using traditional machine learning and deep learning techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#19979;&#32930;&#32908;&#30005;&#22270;&#25968;&#25454;&#19978;&#36827;&#34892;&#27493;&#24577;&#30456;&#20301;&#26816;&#27979;&#65292;&#23545;&#20110;&#25511;&#21046;&#19979;&#32930;&#36741;&#21161;&#35774;&#22791;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#34892;&#36208;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#27963;&#21160;&#65292;&#38656;&#35201;&#36523;&#20307;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#39640;&#24230;&#21512;&#20316;&#21644;&#20114;&#21160;&#12290;&#23454;&#26102;&#20934;&#30830;&#26816;&#27979;&#27493;&#24577;&#38454;&#27573;&#23545;&#20110;&#25511;&#21046;&#19979;&#32930;&#36741;&#21161;&#35774;&#22791;&#22914;&#22806;&#39592;&#39612;&#21644;&#20551;&#32930;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#19979;&#32930;&#32908;&#30005;&#22270;&#65288;EMG&#65289;&#25968;&#25454;&#36827;&#34892;&#20154;&#31867;&#34892;&#36208;&#30340;&#20960;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22522;&#20110;&#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;&#65288;NB&#65289;&#12289;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#12289;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#21644;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNN&#65289;&#12290;&#20256;&#32479;&#30340;ML&#27169;&#22411;&#26159;&#22312;&#25163;&#24037;&#35774;&#35745;&#30340;&#29305;&#24449;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05595v1 Announce Type: cross  Abstract: Human walking is a complex activity with a high level of cooperation and interaction between different systems in the body. Accurate detection of the phases of the gait in real-time is crucial to control lower-limb assistive devices like exoskeletons and prostheses. There are several ways to detect the walking gait phase, ranging from cameras and depth sensors to the sensors attached to the device itself or the human body. Electromyography (EMG) is one of the input methods that has captured lots of attention due to its precision and time delay between neuromuscular activity and muscle movement. This study proposes a few Machine Learning (ML) based models on lower-limb EMG data for human walking. The proposed models are based on Gaussian Naive Bayes (NB), Decision Tree (DT), Random Forest (RF), Linear Discriminant Analysis (LDA) and Deep Convolutional Neural Networks (DCNN). The traditional ML models are trained on hand-crafted features
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#20307;&#24037;&#31243;&#23398;&#39118;&#38505;&#35780;&#20272;&#31995;&#32479;&#65292;&#29305;&#21035;&#20851;&#27880;&#25163;&#37096;&#21644;&#25163;&#25351;&#27963;&#21160;&#65292;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#21644;&#35299;&#20915;&#19982;&#25163;&#24037;&#23494;&#38598;&#22411;&#21046;&#36896;&#36807;&#31243;&#30456;&#20851;&#30340;&#20154;&#20307;&#24037;&#31243;&#23398;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.05591</link><description>&lt;p&gt;
&#22797;&#26434;&#25163;&#24037;&#23494;&#38598;&#22411;&#21046;&#36896;&#36807;&#31243;&#30340;&#25968;&#25454;&#39537;&#21160;&#20154;&#20307;&#24037;&#31243;&#23398;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Ergonomic Risk Assessment of Complex Hand-intensive Manufacturing Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05591
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#20307;&#24037;&#31243;&#23398;&#39118;&#38505;&#35780;&#20272;&#31995;&#32479;&#65292;&#29305;&#21035;&#20851;&#27880;&#25163;&#37096;&#21644;&#25163;&#25351;&#27963;&#21160;&#65292;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#21644;&#35299;&#20915;&#19982;&#25163;&#24037;&#23494;&#38598;&#22411;&#21046;&#36896;&#36807;&#31243;&#30456;&#20851;&#30340;&#20154;&#20307;&#24037;&#31243;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#24037;&#23494;&#38598;&#22411;&#21046;&#36896;&#36807;&#31243;&#65292;&#22914;&#22797;&#21512;&#26448;&#26009;&#38138;&#35774;&#21644;&#32442;&#32455;&#25104;&#24418;&#65292;&#38656;&#35201;&#26174;&#33879;&#30340;&#20154;&#20307;&#28789;&#24039;&#24615;&#20197;&#36866;&#24212;&#20219;&#21153;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123;&#32321;&#37325;&#30340;&#25163;&#37096;&#21160;&#20316;&#24448;&#24448;&#23548;&#33268;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#21644;&#24247;&#22797;&#25163;&#26415;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#20307;&#24037;&#31243;&#23398;&#39118;&#38505;&#35780;&#20272;&#31995;&#32479;&#65292;&#29305;&#21035;&#20851;&#27880;&#25163;&#37096;&#21644;&#25163;&#25351;&#27963;&#21160;&#65292;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#21644;&#35299;&#20915;&#19982;&#25163;&#24037;&#23494;&#38598;&#22411;&#21046;&#36896;&#36807;&#31243;&#30456;&#20851;&#30340;&#20154;&#20307;&#24037;&#31243;&#23398;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#23454;&#39564;&#24179;&#21488;&#65292;&#29992;&#20110;&#25910;&#38598;&#21644;&#21516;&#27493;&#25805;&#20316;&#21592;&#19978;&#21322;&#36523;&#23039;&#21183;&#12289;&#25163;&#37096;&#23039;&#21183;&#21644;&#26045;&#21152;&#21147;&#37327;&#65307;&#19968;&#20010;&#29992;&#20110;&#27979;&#37327;&#39640;&#20445;&#30495;&#24230;&#25163;&#37096;&#21644;&#25163;&#25351;&#39118;&#38505;&#30340;Biometric Assessment of Complete Hand (BACH)&#37197;&#26041;&#65307;&#20197;&#21450;&#19982;&#19978;&#21322;&#36523;&#23039;&#21183;&#12289;RULA&#21644;&#25163;&#37096;&#27963;&#21160;HAL&#30456;&#20851;&#30340;&#34892;&#19994;&#26631;&#20934;&#39118;&#38505;&#35780;&#20998;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;BACH&#30456;&#27604;&#29616;&#26377;&#30340;&#25351;&#26631;&#33021;&#22815;&#26356;&#32454;&#33268;&#22320;&#25429;&#25417;&#26377;&#23475;&#27963;&#21160;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05591v1 Announce Type: cross  Abstract: Hand-intensive manufacturing processes, such as composite layup and textile draping, require significant human dexterity to accommodate task complexity. These strenuous hand motions often lead to musculoskeletal disorders and rehabilitation surgeries. We develop a data-driven ergonomic risk assessment system with a special focus on hand and finger activity to better identify and address ergonomic issues related to hand-intensive manufacturing processes. The system comprises a multi-modal sensor testbed to collect and synchronize operator upper body pose, hand pose and applied forces; a Biometric Assessment of Complete Hand (BACH) formulation to measure high-fidelity hand and finger risks; and industry-standard risk scores associated with upper body posture, RULA, and hand activity, HAL. Our findings demonstrate that BACH captures injurious activity with a higher granularity in comparison to the existing metrics. Machine learning models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24067;&#23616;&#26159;&#21542;&#20250;&#24433;&#21709;&#21442;&#19982;&#32773;&#23545;&#21253;&#21547;&#20167;&#24680;&#35328;&#35770;&#21477;&#23376;&#30340;&#35780;&#20215;&#65292;&#32467;&#26524;&#34920;&#26126;&#35299;&#37322;&#24067;&#23616;&#22312;&#35302;&#21457;&#21442;&#19982;&#32773;&#25552;&#20379;&#32416;&#27491;&#24615;&#21453;&#39304;&#21644;&#35780;&#20272;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.05581</link><description>&lt;p&gt;
&#35299;&#37322;&#24067;&#23616;&#21487;&#20197;&#24433;&#21709;&#20154;&#23545;&#20882;&#29359;&#24615;&#21477;&#23376;&#30340;&#24863;&#30693;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Interpretability Layouts Influence Human Perception of Offensive Sentences?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24067;&#23616;&#26159;&#21542;&#20250;&#24433;&#21709;&#21442;&#19982;&#32773;&#23545;&#21253;&#21547;&#20167;&#24680;&#35328;&#35770;&#21477;&#23376;&#30340;&#35780;&#20215;&#65292;&#32467;&#26524;&#34920;&#26126;&#35299;&#37322;&#24067;&#23616;&#22312;&#35302;&#21457;&#21442;&#19982;&#32773;&#25552;&#20379;&#32416;&#27491;&#24615;&#21453;&#39304;&#21644;&#35780;&#20272;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35299;&#37322;&#24067;&#23616;&#26159;&#21542;&#20250;&#24433;&#21709;&#21442;&#19982;&#32773;&#35780;&#20272;&#21253;&#21547;&#20167;&#24680;&#35328;&#35770;&#30340;&#21477;&#23376;&#26102;&#30340;&#35266;&#28857;&#65292;&#37325;&#28857;&#20851;&#27880;&#8220;&#21388;&#24694;&#22899;&#24615;&#8221;&#21644;&#8220;&#31181;&#26063;&#20027;&#20041;&#8221;&#20004;&#31867;&#12290;&#37492;&#20110;&#25991;&#29486;&#20013;&#23384;&#22312;&#20998;&#27495;&#30340;&#32467;&#35770;&#65292;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#21644;&#23450;&#24615;&#20998;&#26512;&#38382;&#21367;&#35843;&#26597;&#22238;&#24212;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#25506;&#35752;&#22312;&#22312;&#32447;&#31038;&#21306;&#20013;&#20351;&#29992;ML&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#12290;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20272;&#35745;&#21442;&#19982;&#32773;&#30340;&#35780;&#32423;&#65292;&#34701;&#21512;&#20102;&#32452;&#20869;&#35774;&#35745;&#21644;&#32452;&#38388;&#35774;&#35745;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#32479;&#35745;&#20998;&#26512;&#34920;&#26126;&#65292;&#27809;&#26377;&#20219;&#20309;&#35299;&#37322;&#24067;&#23616;&#26174;&#33879;&#24433;&#21709;&#21442;&#19982;&#32773;&#30340;&#35266;&#28857;&#65292;&#20294;&#25105;&#20204;&#30340;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;ML&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#65306;1&#65289;&#35302;&#21457;&#21442;&#19982;&#32773;&#22312;&#20182;&#20204;&#30340;&#35266;&#28857;&#19982;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#26102;&#25552;&#20379;&#32416;&#27491;&#24615;&#21453;&#39304;&#65292;2&#65289;&#25552;&#20379;&#35780;&#20272;&#27169;&#22411;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05581v1 Announce Type: cross  Abstract: This paper conducts a user study to assess whether three machine learning (ML) interpretability layouts can influence participants' views when evaluating sentences containing hate speech, focusing on the "Misogyny" and "Racism" classes. Given the existence of divergent conclusions in the literature, we provide empirical evidence on using ML interpretability in online communities through statistical and qualitative analyses of questionnaire responses. The Generalized Additive Model estimates participants' ratings, incorporating within-subject and between-subject designs. While our statistical analysis indicates that none of the interpretability layouts significantly influences participants' views, our qualitative analysis demonstrates the advantages of ML interpretability: 1) triggering participants to provide corrective feedback in case of discrepancies between their views and the model, and 2) providing insights to evaluate a model's 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20010;&#24615;&#21270;&#32593;&#39029;&#27178;&#24133;&#65292;&#26681;&#25454;&#29992;&#25143;&#20114;&#21160;&#21160;&#24577;&#20869;&#23481;&#65292;&#24182;&#19988;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2403.05578</link><description>&lt;p&gt;
&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20018;&#32852;&#65306;&#29983;&#25104;&#20010;&#24615;&#21270;&#30005;&#23376;&#21830;&#21153;&#27178;&#24133;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20010;&#24615;&#21270;&#32593;&#39029;&#27178;&#24133;&#65292;&#26681;&#25454;&#29992;&#25143;&#20114;&#21160;&#21160;&#24577;&#20869;&#23481;&#65292;&#24182;&#19988;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05578v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#31283;&#23450;&#25193;&#25955;&#31561;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20026;&#29983;&#25104;&#33402;&#26415;&#20316;&#21697;&#24320;&#36767;&#20102;&#22823;&#37327;&#26426;&#20250;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#22686;&#24378;&#35768;&#22810;&#21019;&#24847;&#33402;&#26415;&#23478;&#24037;&#20316;&#20013;&#30340;&#24212;&#29992;&#12290;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#37319;&#29992;&#25163;&#21160;&#27969;&#31243;&#29983;&#25104;&#27178;&#24133;&#65292;&#36825;&#26159;&#32791;&#26102;&#30340;&#19988;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#26681;&#25454;&#22312;&#32447;&#36141;&#29289;&#32773;&#30340;&#20114;&#21160;&#29983;&#25104;&#20855;&#26377;&#21160;&#24577;&#20869;&#23481;&#30340;&#20010;&#24615;&#21270;&#32593;&#39029;&#27178;&#24133;&#30340;&#29992;&#36884;&#12290;&#27492;&#26041;&#27861;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#22312;&#27809;&#26377;&#20154;&#20026;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23558;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#36716;&#25442;&#20026;&#26377;&#24847;&#20041;&#30340;&#25552;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31995;&#32479;&#22320;&#20174;&#39033;&#30446;&#20803;&#20449;&#24687;&#20013;&#25552;&#21462;&#23646;&#24615;&#20803;&#32452;&#12290;&#28982;&#21518;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#36825;&#20123;&#23646;&#24615;&#20256;&#36882;&#32473;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#27178;&#24133;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#39640;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05578v1 Announce Type: cross  Abstract: Text-to-image models such as stable diffusion have opened a plethora of opportunities for generating art. Recent literature has surveyed the use of text-to-image models for enhancing the work of many creative artists. Many e-commerce platforms employ a manual process to generate the banners, which is time-consuming and has limitations of scalability. In this work, we demonstrate the use of text-to-image models for generating personalized web banners with dynamic content for online shoppers based on their interactions. The novelty in this approach lies in converting users' interaction data to meaningful prompts without human intervention. To this end, we utilize a large language model (LLM) to systematically extract a tuple of attributes from item meta-information. The attributes are then passed to a text-to-image model via prompt engineering to generate images for the banner. Our results show that the proposed approach can create high-
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20799;&#31461;&#31119;&#21033;&#39046;&#22495;&#24120;&#29992;&#30340;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#26410;&#19982;&#20986;&#29983;&#29238;&#27597;&#22242;&#32858;&#30340;&#20799;&#31461;&#30340;&#20986;&#38498;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.05573</link><description>&lt;p&gt;
&#36229;&#36234;&#20799;&#31461;&#31119;&#21033;&#39046;&#22495;&#30340;&#39044;&#27979;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Predictive Algorithms in Child Welfare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05573
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20799;&#31461;&#31119;&#21033;&#39046;&#22495;&#24120;&#29992;&#30340;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#26410;&#19982;&#20986;&#29983;&#29238;&#27597;&#22242;&#32858;&#30340;&#20799;&#31461;&#30340;&#20986;&#38498;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#31119;&#21033;&#37096;&#38376;&#30340;&#24037;&#20316;&#20154;&#21592;&#20351;&#29992;&#24314;&#31435;&#22312;&#39118;&#38505;&#35780;&#20272;&#65288;RA&#65289;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#24615;&#20915;&#31574;&#31639;&#27861;&#26469;&#25351;&#23548;&#21644;&#25903;&#25345;&#20799;&#31461;&#31119;&#21033;&#20915;&#31574;&#12290;&#30740;&#31350;&#20154;&#21592;&#25351;&#20986;&#65292;RA&#21487;&#33021;&#21253;&#21547;&#20559;&#35265;&#20449;&#21495;&#65292;&#20250;&#21066;&#24369;&#20799;&#31461;&#31119;&#21033;&#26696;&#20363;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#36825;&#20123;&#31639;&#27861;&#21487;&#33021;&#21463;&#30410;&#20110;&#32435;&#20837;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#26696;&#20363;&#25551;&#36848;&#65292;&#21363;&#30001;&#24037;&#20316;&#20154;&#21592;&#25776;&#20889;&#30340;&#26696;&#20363;&#31508;&#35760;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#31181;&#20551;&#35774;&#30340;&#25913;&#36827;&#65292;&#25105;&#20204;&#23450;&#37327;&#22320;&#35299;&#26500;&#20102;&#32654;&#22269;&#19968;&#20010;&#20799;&#31461;&#31119;&#21033;&#26426;&#26500;&#20004;&#31181;&#24120;&#29992;&#30340;RA&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#20998;&#31867;&#22120;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#26696;&#20363;&#31508;&#35760;&#25551;&#36848;&#30340;RA&#30340;&#39044;&#27979;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#26696;&#20363;&#31508;&#35760;&#36827;&#34892;&#35745;&#31639;&#26426;&#25991;&#26412;&#20998;&#26512;&#20197;&#31361;&#20986;&#20854;&#20013;&#25581;&#31034;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#20110;&#35780;&#20272;&#23478;&#24237;&#24182;&#26500;&#24314;CWS&#39044;&#27979;&#24615;&#39118;&#38505;&#27169;&#22411;&#65288;PRMs&#65289;&#30340;&#24120;&#35265;&#39118;&#38505;&#25351;&#26631;&#26080;&#27861;&#39044;&#27979;&#26410;&#19982;&#20986;&#29983;&#29238;&#27597;&#22242;&#32858;&#30340;&#20799;&#31461;&#30340;&#20986;&#38498;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05573v1 Announce Type: cross  Abstract: Caseworkers in the child welfare (CW) sector use predictive decision-making algorithms built on risk assessment (RA) data to guide and support CW decisions. Researchers have highlighted that RAs can contain biased signals which flatten CW case complexities and that the algorithms may benefit from incorporating contextually rich case narratives, i.e. - casenotes written by caseworkers. To investigate this hypothesized improvement, we quantitatively deconstructed two commonly used RAs from a United States CW agency. We trained classifier models to compare the predictive validity of RAs with and without casenote narratives and applied computational text analysis on casenotes to highlight topics uncovered in the casenotes. Our study finds that common risk metrics used to assess families and build CWS predictive risk models (PRMs) are unable to predict discharge outcomes for children who are not reunified with their birth parent(s). We also
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.05571</link><description>&lt;p&gt;
&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with Constrained Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#36712;&#36857;&#20248;&#21270;&#38754;&#20020;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#21644;&#29615;&#22659;&#35774;&#32622;&#36896;&#25104;&#30340;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#26694;&#26550;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#65292;&#30830;&#20445;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#39069;&#22806;&#32422;&#26463;&#36829;&#21453;&#25439;&#22833;&#30340;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#26088;&#22312;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#36817;&#20284;&#23616;&#37096;&#26368;&#20248;&#35299;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32422;&#26463;&#36829;&#21453;&#12290;&#28982;&#21518;&#29992;&#26679;&#26412;&#20316;&#20026;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#21021;&#22987;&#29468;&#27979;&#65292;&#26469;&#20248;&#21270;&#24182;&#24471;&#20986;&#26368;&#32456;&#35299;&#65292;&#24182;&#39564;&#35777;&#21487;&#34892;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05571v1 Announce Type: cross  Abstract: Trajectory optimization in robotics poses a challenging non-convex problem due to complex dynamics and environmental settings. Traditional numerical optimization methods are time-consuming in finding feasible solutions, whereas data-driven approaches lack safety guarantees for the output trajectories. In this paper, we introduce a general and fully parallelizable framework that combines diffusion models and numerical solvers for non-convex trajectory optimization, ensuring both computational efficiency and constraint satisfaction. A novel constrained diffusion model is proposed with an additional constraint violation loss for training. It aims to approximate the distribution of locally optimal solutions while minimizing constraint violations during sampling. The samples are then used as initial guesses for a numerical solver to refine and derive final solutions with formal verification of feasibility and optimality. Experimental evalua
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#36866;&#24212;&#35821;&#20041;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#36793;&#32536;&#20869;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24573;&#35270;&#12290;</title><link>https://arxiv.org/abs/2403.05559</link><description>&lt;p&gt;
&#29992;&#33258;&#36866;&#24212;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Cognitive Diagnosis Models with Adaptive Relational Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05559
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#36866;&#24212;&#35821;&#20041;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#36793;&#32536;&#20869;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#65288;CD&#65289;&#31639;&#27861;&#22312;&#26234;&#33021;&#25945;&#32946;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#20856;&#22411;&#30340;CD&#31639;&#27861;&#36890;&#36807;&#25512;&#26029;&#23398;&#29983;&#30340;&#33021;&#21147;&#65288;&#21363;&#20182;&#20204;&#22312;&#21508;&#31181;&#30693;&#35782;&#27010;&#24565;&#19978;&#30340;&#29087;&#32451;&#27700;&#24179;&#65289;&#26469;&#24110;&#21161;&#23398;&#29983;&#12290;&#36825;&#31181;&#29087;&#32451;&#27700;&#24179;&#21487;&#20197;&#36827;&#19968;&#27493;&#23454;&#29616;&#38024;&#23545;&#24615;&#30340;&#25216;&#33021;&#35757;&#32451;&#21644;&#20010;&#24615;&#21270;&#30340;&#32451;&#20064;&#24314;&#35758;&#65292;&#20174;&#32780;&#20419;&#36827;&#22312;&#32447;&#25945;&#32946;&#20013;&#23398;&#29983;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24314;&#31435;&#21644;&#25972;&#21512;&#23398;&#29983;-&#32451;&#20064;&#20108;&#37096;&#22270;&#23545;&#20110;&#22686;&#24378;&#35786;&#26029;&#24615;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#30740;&#31350;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#19968;&#26041;&#38754;&#65292;&#30740;&#31350;&#20154;&#21592;&#24573;&#35270;&#20102;&#36793;&#32536;&#20869;&#30340;&#24322;&#36136;&#24615;&#65292;&#21363;&#21487;&#33021;&#23384;&#22312;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20182;&#20204;&#24573;&#35270;&#20102;&#36793;&#32536;&#20869;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20363;&#22914;&#65292;&#27491;&#30830;&#30340;&#31572;&#26696;&#21487;&#33021;&#34920;&#31034;&#30495;&#27491;&#25484;&#25569;&#25110;&#24184;&#36816;&#29468;&#27979;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#35821;&#20041;&#24863;&#30693;&#22270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05559v1 Announce Type: cross  Abstract: Cognitive Diagnosis (CD) algorithms receive growing research interest in intelligent education. Typically, these CD algorithms assist students by inferring their abilities (i.e., their proficiency levels on various knowledge concepts). The proficiency levels can enable further targeted skill training and personalized exercise recommendations, thereby promoting students' learning efficiency in online education. Recently, researchers have found that building and incorporating a student-exercise bipartite graph is beneficial for enhancing diagnostic performance. However, there are still limitations in their studies. On one hand, researchers overlook the heterogeneity within edges, where there can be both correct and incorrect answers. On the other hand, they disregard the uncertainty within edges, e.g., a correct answer can indicate true mastery or fortunate guessing. To address the limitations, we propose Adaptive Semantic-aware Graph-ba
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#65292;&#36890;&#36807;&#23618;&#32423;&#24863;&#30693;&#30340;&#26631;&#31614;&#20851;&#31995;&#24314;&#27169;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#22797;&#26434;&#30340;&#26631;&#31614;&#20851;&#31995;&#32435;&#20837;&#22522;&#26412;&#27169;&#22411;&#20013;</title><link>https://arxiv.org/abs/2403.05557</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20855;&#26377;&#23618;&#27425;&#24863;&#30693;&#26631;&#31614;&#20851;&#31995;&#24314;&#27169;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Re-thinking Human Activity Recognition with Hierarchy-aware Label Relationship Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05557
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#65292;&#36890;&#36807;&#23618;&#32423;&#24863;&#30693;&#30340;&#26631;&#31614;&#20851;&#31995;&#24314;&#27169;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#22797;&#26434;&#30340;&#26631;&#31614;&#20851;&#31995;&#32435;&#20837;&#22522;&#26412;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#20174;&#25968;&#25454;&#25910;&#38598;&#12289;&#23398;&#20064;&#27169;&#22411;&#21040;&#21518;&#22788;&#29702;&#21644;&#32467;&#26524;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#27963;&#21160;&#20013;&#23384;&#22312;&#30340;&#23618;&#27425;&#32467;&#26500;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#35299;&#37322;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#20294;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;H-HAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#27963;&#21160;&#20013;&#22797;&#26434;&#30340;&#20840;&#23616;&#26631;&#31614;&#20851;&#31995;&#65292;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;HAR&#20219;&#21153;&#12290;&#25105;&#20204;&#19981;&#26159;&#20026;&#22810;&#23618;&#27425;&#30340;&#27963;&#21160;&#20998;&#21035;&#26500;&#24314;&#22810;&#20010;&#20998;&#31867;&#22120;&#65292;&#32780;&#26159;&#25506;&#32034;&#36890;&#36807;&#22522;&#20110;&#22270;&#30340;&#26631;&#31614;&#20851;&#31995;&#24314;&#27169;&#22686;&#24378;&#30340;&#24179;&#38754;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#23618;&#27425;&#24847;&#35782;&#30340;&#22270;&#22522;&#26631;&#31614;&#24314;&#27169;&#23558;&#22797;&#26434;&#30340;&#26631;&#31614;&#20851;&#31995;&#32435;&#20837;&#22522;&#26412;HAR&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#22797;&#26434;&#30340;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#39564;&#35777;&#20102;&#35813;&#25552;&#35758;&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;&#35813;&#25552;&#35758;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22402;&#30452;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05557v1 Announce Type: cross  Abstract: Human Activity Recognition (HAR) has been studied for decades, from data collection, learning models, to post-processing and result interpretations. However, the inherent hierarchy in the activities remains relatively under-explored, despite its significant impact on model performance and interpretation. In this paper, we propose H-HAR, by rethinking the HAR tasks from a fresh perspective by delving into their intricate global label relationships. Rather than building multiple classifiers separately for multi-layered activities, we explore the efficacy of a flat model enhanced with graph-based label relationship modeling. Being hierarchy-aware, the graph-based label modeling enhances the fundamental HAR model, by incorporating intricate label relationships into the model. We validate the proposal with a multi-label classifier on complex human activity data. The results highlight the advantages of the proposal, which can be vertically i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#23545;&#23398;&#29983;&#30340;&#21442;&#19982;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;K-EM&#30340;&#22522;&#20110;K&#22343;&#20540;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#26524;&#26497;&#20855;&#21069;&#26223;</title><link>https://arxiv.org/abs/2403.05556</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#23545;&#23398;&#29983;&#30340;&#21442;&#19982;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Modeling and predicting students' engagement behaviors using mixture Markov models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#23545;&#23398;&#29983;&#30340;&#21442;&#19982;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;K-EM&#30340;&#22522;&#20110;K&#22343;&#20540;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#26524;&#26497;&#20855;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#30340;&#21442;&#19982;&#21453;&#26144;&#20102;&#20182;&#20204;&#22312;&#36827;&#34892;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#21442;&#19982;&#31243;&#24230;&#65292;&#21487;&#20197;&#36890;&#36807;&#20182;&#20204;&#19982;&#22522;&#20110;&#35745;&#31639;&#26426;&#30340;&#23398;&#20064;&#25110;&#35780;&#20272;&#31995;&#32479;&#30340;&#20114;&#21160;&#26469;&#20272;&#35745;&#12290;&#28608;&#21457;&#23398;&#29983;&#30340;&#21442;&#19982;&#38656;&#35201;&#20855;&#22791;&#22823;&#33268;&#30340;&#34920;&#31034;&#27169;&#22411;&#65292;&#20197;&#29702;&#35299;&#23398;&#29983;&#30340;&#21508;&#31181;&#65288;&#19981;&#65289;&#21442;&#19982;&#34892;&#20026;&#12290;&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#65292;&#29983;&#25104;K&#20010;&#28151;&#21512;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#23545;&#21253;&#21547;&#23398;&#29983;&#65288;&#19981;&#65289;&#21442;&#19982;&#34892;&#20026;&#27169;&#24335;&#30340;&#36712;&#36857;&#36827;&#34892;&#20998;&#32452;&#12290;&#20026;&#20102;&#38450;&#27490;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#38519;&#20837;&#23616;&#37096;&#26368;&#22823;&#20540;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;K-EM&#30340;&#22522;&#20110;K&#22343;&#20540;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;EM&#31639;&#27861;&#30340;&#19977;&#20010;&#21464;&#20307;&#65306;&#21407;&#22987;EM&#12289;emEM&#12289;K-EM&#65307;&#20197;&#21450;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#38750;&#28151;&#21512;&#22522;&#32447;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;K-EM&#34920;&#29616;&#20986;&#20102;&#26497;&#20855;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05556v1 Announce Type: cross  Abstract: Students' engagements reflect their level of involvement in an ongoing learning process which can be estimated through their interactions with a computer-based learning or assessment system. A pre-requirement for stimulating student engagement lies in the capability to have an approximate representation model for comprehending students' varied (dis)engagement behaviors. In this paper, we utilized model-based clustering for this purpose which generates K mixture Markov models to group students' traces containing their (dis)engagement behavioral patterns. To prevent the Expectation-Maximization (EM) algorithm from getting stuck in a local maxima, we also introduced a K-means-based initialization method named as K-EM. We performed an experimental work on two real datasets using the three variants of the EM algorithm: the original EM, emEM, K-EM; and, non-mixture baseline models for both datasets. The proposed K-EM has shown very promising
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;MapReduce&#30340;&#23376;&#32676;&#20307;&#21457;&#29616;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#26088;&#22312;&#23545;MOOC&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#23398;&#20064;&#32773;&#36827;&#34892;&#20998;&#31867;&#21644;&#25551;&#36848;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.05555</link><description>&lt;p&gt;
MOOC&#20013;&#30340;&#23376;&#32676;&#20307;&#21457;&#29616;&#65306;&#25551;&#36848;&#19981;&#21516;&#31867;&#22411;&#23398;&#20064;&#32773;&#30340;&#22823;&#25968;&#25454;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Subgroup Discovery in MOOCs: A Big Data Application for Describing Different Types of Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05555
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;MapReduce&#30340;&#23376;&#32676;&#20307;&#21457;&#29616;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#26088;&#22312;&#23545;MOOC&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#23398;&#20064;&#32773;&#36827;&#34892;&#20998;&#31867;&#21644;&#25551;&#36848;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;MapReduce&#30340;&#23376;&#32676;&#20307;&#21457;&#29616;&#26041;&#27861;&#23545;&#28023;&#37327;&#24320;&#25918;&#22312;&#32447;&#35838;&#31243;&#65288;MOOCs&#65289;&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#23398;&#20064;&#32773;&#36827;&#34892;&#20998;&#31867;&#21644;&#25551;&#36848;&#12290;&#26368;&#32456;&#30446;&#26631;&#26159;&#21457;&#29616;&#20986;&#29616;&#22312;&#19981;&#21516;MOOC&#20013;&#30340;IF-THEN&#35268;&#21017;&#12290;&#25152;&#25552;&#20986;&#30340;&#23376;&#32676;&#20307;&#21457;&#29616;&#26041;&#27861;&#26159;&#23545;&#33879;&#21517;&#30340;FP-Growth&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#32771;&#34385;&#20102;MapReduce&#31561;&#26032;&#20852;&#24182;&#34892;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#26497;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#20316;&#20026;&#38468;&#21152;&#29305;&#24615;&#65292;&#35813;&#25552;&#26696;&#21253;&#25324;&#19968;&#20010;&#38408;&#20540;&#25968;&#20540;&#65292;&#29992;&#20110;&#25351;&#31034;&#27599;&#20010;&#21457;&#29616;&#30340;&#35268;&#21017;&#24212;&#28385;&#36275;&#30340;&#35838;&#31243;&#25968;&#37327;&#12290;&#36824;&#21253;&#25324;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#20197;&#21024;&#38500;&#22810;&#20313;&#30340;&#23376;&#32676;&#20307;&#12290;&#23454;&#39564;&#38454;&#27573;&#36890;&#36807;&#32771;&#34385;&#26469;&#33258;edX&#24179;&#21488;&#19978;16&#38376;MITx&#21644;HarvardX&#35838;&#31243;&#39318;&#24180;&#30340;&#21435;&#26631;&#35782;&#21270;&#25968;&#25454;&#36827;&#34892;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MapReduce&#26041;&#27861;&#32988;&#36807;&#20256;&#32479;&#39034;&#24207;&#23376;&#32676;&#20307;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05555v1 Announce Type: cross  Abstract: The aim of this paper is to categorize and describe different types of learners in massive open online courses (MOOCs) by means of a subgroup discovery approach based on MapReduce. The final objective is to discover IF-THEN rules that appear in different MOOCs. The proposed subgroup discovery approach, which is an extension of the well-known FP-Growth algorithm, considers emerging parallel methodologies like MapReduce to be able to cope with extremely large datasets. As an additional feature, the proposal includes a threshold value to denote the number of courses that each discovered rule should satisfy. A post-processing step is also included so redundant subgroups can be removed. The experimental stage is carried out by considering de-identified data from the first year of 16 MITx and HarvardX courses on the edX platform. Experimental results demonstrate that the proposed MapReduce approach outperforms traditional sequential subgroup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;BERT&#20027;&#39064;&#24314;&#27169;&#20174;&#35838;&#31243;&#20013;&#25552;&#21462;&#20027;&#39064;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20027;&#39064;&#26469;&#35782;&#21035;&#19981;&#21516;&#23398;&#31185;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#21508;&#31181;&#23398;&#20064;&#35805;&#39064;&#30340;&#28436;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.05553</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#29702;&#35299;&#25945;&#32946;&#35805;&#39064;&#30340;&#28436;&#36827;
&lt;/p&gt;
&lt;p&gt;
Understanding the Progression of Educational Topics via Semantic Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;BERT&#20027;&#39064;&#24314;&#27169;&#20174;&#35838;&#31243;&#20013;&#25552;&#21462;&#20027;&#39064;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20027;&#39064;&#26469;&#35782;&#21035;&#19981;&#21516;&#23398;&#31185;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#21508;&#31181;&#23398;&#20064;&#35805;&#39064;&#30340;&#28436;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#31995;&#32479;&#27491;&#22312;&#21160;&#24577;&#21464;&#21270;&#65292;&#20197;&#36866;&#24212;&#25216;&#26415;&#36827;&#27493;&#12289;&#24037;&#19994;&#21644;&#31038;&#20250;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#23398;&#29983;&#30340;&#23398;&#20064;&#26053;&#31243;&#12290;&#35838;&#31243;&#19987;&#23478;&#21644;&#25945;&#32946;&#24037;&#20316;&#32773;&#19981;&#26029;&#20462;&#35746;&#21508;&#25945;&#32946;&#24180;&#32423;&#25152;&#25945;&#25480;&#30340;&#31185;&#30446;&#65292;&#20197;&#30830;&#23450;&#24046;&#36317;&#65292;&#24341;&#20837;&#26032;&#30340;&#23398;&#20064;&#35805;&#39064;&#65292;&#24182;&#25552;&#39640;&#23398;&#20064;&#25104;&#26524;&#12290;&#26412;&#25991;&#21033;&#29992;&#25968;&#25454;&#31185;&#23398;&#26469;&#26356;&#22909;&#22320;&#20102;&#35299;&#21508;&#31181;&#23398;&#20064;&#35805;&#39064;&#30340;&#28436;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05553v1 Announce Type: cross  Abstract: Education systems are dynamically changing to accommodate technological advances, industrial and societal needs, and to enhance students' learning journeys. Curriculum specialists and educators constantly revise taught subjects across educational grades to identify gaps, introduce new learning topics, and enhance the learning outcomes. This process is usually done within the same subjects (e.g. math) or across related subjects (e.g. math and physics) considering the same and different educational levels, leading to massive multi-layer comparisons. Having nuanced data about subjects, topics, and learning outcomes structured within a dataset, empowers us to leverage data science to better understand the progression of various learning topics. In this paper, Bidirectional Encoder Representations from Transformers (BERT) topic modeling was used to extract topics from the curriculum, which were then used to identify relationships between su
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24212;&#29992;&#22810;&#28304;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#28151;&#21512;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#39044;&#27979;&#23398;&#29983;&#23398;&#26415;&#34920;&#29616;&#30340;&#26368;&#20339;&#23646;&#24615;&#38598;&#20026;&#29702;&#35770;&#35838;&#19978;&#30340;&#20851;&#27880;&#31243;&#24230;&#12289;Moodle&#27979;&#39564;&#25104;&#32489;&#20197;&#21450;Moodle&#35770;&#22363;&#19978;&#30340;&#27963;&#21160;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.05552</link><description>&lt;p&gt;
&#22810;&#28304;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#22312;&#28151;&#21512;&#24335;&#23398;&#20064;&#22823;&#23398;&#35838;&#31243;&#20013;&#39044;&#27979;&#23398;&#26415;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Multi-source and multimodal data fusion for predicting academic performance in blended learning university courses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24212;&#29992;&#22810;&#28304;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#28151;&#21512;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#39044;&#27979;&#23398;&#29983;&#23398;&#26415;&#34920;&#29616;&#30340;&#26368;&#20339;&#23646;&#24615;&#38598;&#20026;&#29702;&#35770;&#35838;&#19978;&#30340;&#20851;&#27880;&#31243;&#24230;&#12289;Moodle&#27979;&#39564;&#25104;&#32489;&#20197;&#21450;Moodle&#35770;&#22363;&#19978;&#30340;&#27963;&#21160;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#26469;&#39044;&#27979;&#22823;&#23398;&#29983;&#22312;&#28151;&#21512;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#26368;&#32456;&#23398;&#26415;&#34920;&#29616;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#28304;&#22836;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;&#19981;&#21516;&#26469;&#28304;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#20102;&#20851;&#20110;&#22823;&#19968;&#23398;&#29983;&#30340;&#25968;&#25454;&#65306;&#29702;&#35770;&#35838;&#12289;&#23454;&#36341;&#35838;&#12289;&#22312;&#32447;Moodle&#35838;&#31243;&#20197;&#21450;&#26399;&#26411;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21457;&#29616;&#21738;&#31181;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24212;&#29992;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#21644;&#20845;&#31181;&#20998;&#31867;&#31639;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38598;&#25104;&#21644;&#36873;&#25321;&#26368;&#20339;&#23646;&#24615;&#26041;&#27861;&#19982;&#31163;&#25955;&#21270;&#25968;&#25454;&#19968;&#36215;&#20135;&#29983;&#20102;&#26368;&#20339;&#39044;&#27979;&#32467;&#26524;&#12290;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#26174;&#31034;&#65292;&#29702;&#35770;&#35838;&#19978;&#30340;&#20851;&#27880;&#31243;&#24230;&#12289;Moodle&#27979;&#39564;&#25104;&#32489;&#20197;&#21450;Moodle&#35770;&#22363;&#19978;&#30340;&#27963;&#21160;&#27700;&#24179;&#26159;&#39044;&#27979;&#23398;&#29983;&#26368;&#32456;&#34920;&#29616;&#30340;&#26368;&#20339;&#23646;&#24615;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05552v1 Announce Type: cross  Abstract: In this paper we applied data fusion approaches for predicting the final academic performance of university students using multiple-source, multimodal data from blended learning environments. We collected and preprocessed data about first-year university students from different sources: theory classes, practical sessions, on-line Moodle sessions, and a final exam. Our objective was to discover which data fusion approach produced the best results using our data. We carried out experiments by applying four different data fusion approaches and six classification algorithms. The results showed that the best predictions were produced using ensembles and selecting the best attributes approach with discretized data. The best prediction models showed us that the level of attention in theory classes, scores in Moodle quizzes, and the level of activity in Moodle forums were the best set of attributes for predicting students' final performance in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#28436;&#21464;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#30417;&#27979;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#20026;&#24178;&#39044;&#21644;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.05548</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#30340;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
Monitoring the evolution of antisemitic discourse on extremist social media using BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#28436;&#21464;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#30417;&#27979;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#20026;&#24178;&#39044;&#21644;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#31181;&#26063;&#20027;&#20041;&#21644;&#19981;&#23485;&#23481;&#26377;&#21487;&#33021;&#22312;&#32447;&#19979;&#20135;&#29983;&#20167;&#24680;&#65292;&#26368;&#32456;&#23548;&#33268;&#36523;&#20307;&#26292;&#21147;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#30340;&#26159;&#22312;&#32447;&#21453;&#29369;&#20027;&#20041;&#65292;&#36861;&#36394;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#21453;&#29369;&#20027;&#39064;&#21450;&#20854;&#30456;&#20851;&#26415;&#35821;&#30340;&#28436;&#21464;&#65292;&#26377;&#21161;&#20110;&#30417;&#27979;&#21442;&#19982;&#32773;&#30340;&#24773;&#32490;&#21644;&#28436;&#21464;&#65292;&#24182;&#21487;&#33021;&#25552;&#20379;&#24178;&#39044;&#26041;&#27861;&#65292;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#12290;&#37492;&#20110;&#22312;&#32447;&#27969;&#37327;&#24222;&#22823;&#19988;&#19981;&#26029;&#21464;&#21270;&#65292;&#25163;&#21160;&#30417;&#27979;&#35848;&#35805;&#23454;&#38469;&#19978;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#25552;&#21462;&#21453;&#29369;&#20027;&#39064;&#21644;&#26415;&#35821;&#65292;&#36319;&#36394;&#23427;&#20204;&#30340;&#28436;&#21464;&#12290;&#30001;&#20110;&#30417;&#30563;&#23398;&#20064;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#36807;&#20110;&#21463;&#38480;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05548v1 Announce Type: cross  Abstract: Racism and intolerance on social media contribute to a toxic online environment which may spill offline to foster hatred, and eventually lead to physical violence. That is the case with online antisemitism, the specific category of hatred considered in this study. Tracking antisemitic themes and their associated terminology over time in online discussions could help monitor the sentiments of their participants and their evolution, and possibly offer avenues for intervention that may prevent the escalation of hatred. Due to the large volume and constant evolution of online traffic, monitoring conversations manually is impractical. Instead, we propose an automated method that extracts antisemitic themes and terminology from extremist social media over time and captures their evolution. Since supervised learning would be too limited for such a task, we created an unsupervised online machine learning approach that uses large language model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;AI&#30340;&#25945;&#23398;&#35268;&#21010;&#33050;&#26412;&#65292;&#36890;&#36807;&#23558;AI&#27010;&#24565;&#19982;&#30740;&#31350;&#30456;&#20851;&#20027;&#39064;&#32852;&#31995;&#36215;&#26469;&#65292;&#20419;&#36827;&#20102;&#23398;&#29983;&#23545;AI&#28508;&#21147;&#21644;&#39118;&#38505;&#30340;&#29702;&#35299;&#21644;&#20852;&#36259;&#12290;</title><link>https://arxiv.org/abs/2403.05547</link><description>&lt;p&gt;
AI&#23545;&#38750;&#31243;&#24207;&#21592;&#30340;&#24212;&#29992;&#65306;&#24212;&#29992;AI&#22312;&#27809;&#26377;&#32534;&#31243;&#25216;&#33021;&#30340;&#23398;&#29983;&#35838;&#22530;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AI for non-programmers: Applied AI in the lectures for students without programming skills
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;AI&#30340;&#25945;&#23398;&#35268;&#21010;&#33050;&#26412;&#65292;&#36890;&#36807;&#23558;AI&#27010;&#24565;&#19982;&#30740;&#31350;&#30456;&#20851;&#20027;&#39064;&#32852;&#31995;&#36215;&#26469;&#65292;&#20419;&#36827;&#20102;&#23398;&#29983;&#23545;AI&#28508;&#21147;&#21644;&#39118;&#38505;&#30340;&#29702;&#35299;&#21644;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;ChatGPT&#21644;WOMBO Dream&#31561;&#24212;&#29992;&#20351;&#24471;&#21551;&#21457;&#26080;&#32534;&#31243;&#30693;&#35782;&#30340;&#23398;&#29983;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21464;&#24471;&#36731;&#32780;&#26131;&#20030;&#12290;&#37492;&#20110;AI&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#30340;&#26085;&#30410;&#37325;&#35201;&#65292;&#38656;&#35201;&#21019;&#26032;&#31574;&#30053;&#26469;&#25945;&#32946;&#37027;&#20123;&#27809;&#26377;&#32534;&#31243;&#30693;&#35782;&#30340;&#23398;&#29983;&#65292;&#20197;&#20415;&#23558;AI&#20316;&#20026;&#26410;&#26469;&#25216;&#33021;&#34701;&#20837;&#20182;&#20204;&#30340;&#23398;&#20064;&#27169;&#22359;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;AI&#30340;&#25945;&#23398;&#35268;&#21010;&#33050;&#26412;&#12290;&#35813;&#25945;&#23398;&#35268;&#21010;&#33050;&#26412;&#22522;&#20110;AI&#24212;&#29992;&#27969;&#31243;&#65292;&#24182;&#23558;AI&#27010;&#24565;&#19982;&#30740;&#31350;&#30456;&#20851;&#20027;&#39064;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20123;&#32852;&#31995;&#25171;&#24320;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#65292;&#24182;&#20419;&#36827;&#20102;&#23398;&#29983;&#23545;AI&#28508;&#21147;&#21644;&#39118;&#38505;&#30340;&#20852;&#36259;&#21644;&#29702;&#35299;&#12290;&#20197;&#33021;&#28304;&#31649;&#29702;&#30805;&#22763;&#23398;&#29983;&#20026;&#20363;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;AI&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#19987;&#19994;&#35838;&#31243;&#20013;&#12290;&#20026;&#27492;&#65292;&#24212;&#29992;AI&#30340;&#25945;&#23398;&#35268;&#21010;&#33050;&#26412;&#34987;&#35843;&#25972;&#20197;&#36866;&#24212;&#30740;&#31350;&#39033;&#30446;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05547v1 Announce Type: cross  Abstract: Applications such as ChatGPT and WOMBO Dream make it easy to inspire students without programming knowledge to use artificial intelligence (AI). Therefore, given the increasing importance of AI in all disciplines, innovative strategies are needed to educate students in AI without programming knowledge so that AI can be integrated into their study modules as a future skill. This work presents a didactic planning script for applied AI. The didactic planning script is based on the AI application pipeline and links AI concepts with study-relevant topics. These linkages open up a new solution space and promote students' interest in and understanding of the potentials and risks of AI. An example lecture series for master students in energy management shows how AI can be seamlessly integrated into discipline-specific lectures. To this end, the planning script for applied AI is adapted to fit the study programs' topic. This specific teaching s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#20056;&#23458;&#21344;&#24231;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;AFC&#21644;APC&#25968;&#25454;&#65292;&#22312;&#20844;&#20849;&#20132;&#36890;&#32593;&#32476;&#19978;&#25512;&#26029;&#27599;&#20010;&#29677;&#27425;&#30340;&#20056;&#23458;&#21344;&#24231;&#29575;&#65292;&#24357;&#34917;&#20102;&#32570;&#22833;&#30340;APC&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.05546</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;AFC&#21644;APC&#25968;&#25454;&#22312;&#20844;&#20849;&#20132;&#36890;&#32593;&#32476;&#19978;&#23454;&#29616;&#32479;&#19968;&#20056;&#23458;&#21344;&#24231;&#29575;
&lt;/p&gt;
&lt;p&gt;
Unified Occupancy on a Public Transport Network through Combination of AFC and APC Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#20056;&#23458;&#21344;&#24231;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;AFC&#21644;APC&#25968;&#25454;&#65292;&#22312;&#20844;&#20849;&#20132;&#36890;&#32593;&#32476;&#19978;&#25512;&#26029;&#27599;&#20010;&#29677;&#27425;&#30340;&#20056;&#23458;&#21344;&#24231;&#29575;&#65292;&#24357;&#34917;&#20102;&#32570;&#22833;&#30340;APC&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#36890;&#32593;&#32476;&#20013;&#65292;&#36710;&#19978;&#30340;&#20056;&#23458;&#21344;&#24231;&#29575;&#23545;&#20110;&#20102;&#35299;&#26053;&#23458;&#20064;&#24815;&#24182;&#35843;&#25972;&#25552;&#20379;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;&#36816;&#33829;&#21830;&#20381;&#38752;&#23454;&#22320;&#30740;&#31350;&#35780;&#20272;&#20856;&#22411;&#24037;&#20316;&#26085;&#30340;&#20056;&#36710;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#25910;&#36153;&#31995;&#32479;&#65288;AFC&#65289;&#21644;&#33258;&#21160;&#20056;&#23458;&#35745;&#25968;&#65288;APC&#65289;&#25968;&#25454;&#24448;&#24448;&#21487;&#33719;&#24471;&#20294;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#26102;&#38388;&#35206;&#30422;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#27599;&#31181;&#25968;&#25454;&#28304;&#37117;&#24102;&#26377;&#20854;&#33258;&#36523;&#30340;&#20559;&#35265;&#65306;AFC&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#32771;&#34385;&#27450;&#35784;&#34892;&#20026;&#65292;&#32780;&#24182;&#38750;&#25152;&#26377;&#36710;&#36742;&#37117;&#37197;&#22791;&#26377;APC&#31995;&#32479;&#12290; &#26412;&#25991;&#24341;&#20837;&#20102;&#32479;&#19968;&#20056;&#23458;&#21344;&#24231;&#29575;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22320;&#29702;&#32479;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;AFC&#21644;APC&#25968;&#25454;&#19982;&#37096;&#20998;&#35206;&#30422;&#30340;&#33539;&#22260;&#30456;&#32467;&#21512;&#26469;&#25512;&#26029;&#20844;&#20849;&#20132;&#36890;&#32593;&#32476;&#20869;&#27599;&#36255;&#36710;&#30340;&#20056;&#23458;&#21344;&#24231;&#29575;&#12290;&#32479;&#19968;&#20056;&#23458;&#21344;&#24231;&#29575;&#20026;&#22312;&#20854;&#20182;&#36255;&#36710;&#26377;APC&#27979;&#37327;&#30340;&#32447;&#36335;&#19978;&#30340;&#26576;&#20123;&#36255;&#36710;&#65292;&#20197;&#21450;&#22312;&#26681;&#26412;&#27809;&#26377;APC&#25968;&#25454;&#21487;&#29992;&#30340;&#32447;&#36335;&#19978;&#30340;&#26576;&#20123;&#36255;&#36710;&#65292;&#34917;&#20840;&#20102;&#32570;&#22833;&#30340;APC&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05546v1 Announce Type: cross  Abstract: In a transport network, the onboard occupancy is key for gaining insights into travelers' habits and adjusting the offer. Traditionally, operators have relied on field studies to evaluate ridership of a typical workday. However, automated fare collection (AFC) and automatic passenger counting (APC) data, which provide complete temporal coverage, are often available but underexploited. It should be noted, however, that each data source comes with its own biases: AFC data may not account for fraud, while not all vehicles are equipped with APC systems.   This paper introduces the unified occupancy method, a geostatistical model to extrapolate occupancy to every course of a public transportation network by combining AFC and APC data with partial coverage. Unified occupancy completes missing APC information for courses on lines where other courses have APC measures, as well as for courses on lines where no APC data is available at all. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#37329;&#34701;&#26426;&#26500;&#20013;&#20154;&#24037;&#26234;&#33021;&#22312;ESG&#20513;&#35758;&#20013;&#30340;&#24212;&#29992;&#65292;&#38416;&#26126;&#20102;AI&#22312;&#21152;&#24378;ESG&#26694;&#26550;&#26041;&#38754;&#30340;&#24517;&#35201;&#24615;&#21644;&#24433;&#21709;&#65292;&#20197;&#21450;AI&#22914;&#20309;&#22686;&#24378;&#37329;&#34701;&#27963;&#21160;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.05541</link><description>&lt;p&gt;
&#37329;&#34701;&#26426;&#26500;ESG&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#20010;&#20135;&#19994;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
AI in ESG for Financial Institutions: An Industrial Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#37329;&#34701;&#26426;&#26500;&#20013;&#20154;&#24037;&#26234;&#33021;&#22312;ESG&#20513;&#35758;&#20013;&#30340;&#24212;&#29992;&#65292;&#38416;&#26126;&#20102;AI&#22312;&#21152;&#24378;ESG&#26694;&#26550;&#26041;&#38754;&#30340;&#24517;&#35201;&#24615;&#21644;&#24433;&#21709;&#65292;&#20197;&#21450;AI&#22914;&#20309;&#22686;&#24378;&#37329;&#34701;&#27963;&#21160;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26085;&#30410;&#34701;&#20837;&#37329;&#34701;&#34892;&#19994;&#30340;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#20513;&#35758;&#65292;&#20195;&#34920;&#20102;&#21521;&#26356;&#21487;&#25345;&#32493;&#21644;&#20844;&#24179;&#37329;&#34701;&#23454;&#36341;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#26412;&#25991;&#35843;&#26597;&#20135;&#19994;&#26684;&#23616;&#65292;&#38416;&#26126;&#20102;AI&#22312;&#21152;&#24378;ESG&#26694;&#26550;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#24433;&#21709;&#12290;&#38543;&#30528;&#20005;&#26684;&#30340;&#30417;&#31649;&#35201;&#27714;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#24847;&#35782;&#30340;&#25552;&#39640;&#65292;&#37329;&#34701;&#26426;&#26500;&#65288;FIs&#65289;&#36234;&#26469;&#36234;&#34987;&#36843;&#37319;&#32435;ESG&#26631;&#20934;&#12290;AI&#25104;&#20026;&#22312;&#23548;&#33322;&#37329;&#34701;&#27963;&#21160;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#20013;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#23545;ESG&#30340;&#19977;&#20010;&#20027;&#35201;&#25903;&#26609;&#20013;&#30340;AI&#24212;&#29992;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#38416;&#26126;&#20102;AI&#22914;&#20309;&#22686;&#24378;&#20998;&#26512;&#33021;&#21147;&#12289;&#39118;&#38505;&#35780;&#20272;&#12289;&#23458;&#25143;&#21442;&#19982;&#12289;&#25253;&#21578;&#20934;&#30830;&#24615;&#31561;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22260;&#32469;&#25968;&#25454;&#20351;&#29992;&#21644;&#27169;&#22411;&#24320;&#21457;&#30340;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05541v1 Announce Type: cross  Abstract: The burgeoning integration of Artificial Intelligence (AI) into Environmental, Social, and Governance (ESG) initiatives within the financial sector represents a paradigm shift towards more sus-tainable and equitable financial practices. This paper surveys the industrial landscape to delineate the necessity and impact of AI in bolstering ESG frameworks. With the advent of stringent regulatory requirements and heightened stakeholder awareness, financial institutions (FIs) are increasingly compelled to adopt ESG criteria. AI emerges as a pivotal tool in navigating the complex in-terplay of financial activities and sustainability goals. Our survey categorizes AI applications across three main pillars of ESG, illustrating how AI enhances analytical capabilities, risk assessment, customer engagement, reporting accuracy and more. Further, we delve into the critical con-siderations surrounding the use of data and the development of models, und
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#28781;&#32477;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#28781;&#32477;&#32423;&#21476;&#21704;&#29305;&#23450;&#24459;&#65292;&#25351;&#20986;&#20219;&#20309;&#30446;&#26631;&#35268;&#33539;&#36807;&#24230;&#36861;&#27714;&#37117;&#21487;&#33021;&#23548;&#33268;&#20154;&#31867;&#28781;&#32477;&#65292;&#21516;&#26102;&#25351;&#20986;&#35780;&#20272;&#27492;&#20551;&#35774;&#38656;&#35201;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#20294;&#21487;&#33021;&#30001;&#20110;&#27169;&#22411;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#23454;&#29616;&#65292;&#26263;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#30340;&#28781;&#32477;&#39118;&#38505;&#21487;&#33021;&#22312;&#24403;&#21069;&#31185;&#23398;&#26041;&#27861;&#19979;&#38590;&#20197;&#35266;&#23519;&#21040;&#12290;</title><link>https://arxiv.org/abs/2403.05540</link><description>&lt;p&gt;
AI&#30340;&#28781;&#32477;&#39118;&#38505;&#65306;&#23545;&#31185;&#23398;&#26159;&#21542;&#38544;&#24418;&#30340;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Extinction Risks from AI: Invisible to Science?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05540
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#28781;&#32477;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#28781;&#32477;&#32423;&#21476;&#21704;&#29305;&#23450;&#24459;&#65292;&#25351;&#20986;&#20219;&#20309;&#30446;&#26631;&#35268;&#33539;&#36807;&#24230;&#36861;&#27714;&#37117;&#21487;&#33021;&#23548;&#33268;&#20154;&#31867;&#28781;&#32477;&#65292;&#21516;&#26102;&#25351;&#20986;&#35780;&#20272;&#27492;&#20551;&#35774;&#38656;&#35201;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#20294;&#21487;&#33021;&#30001;&#20110;&#27169;&#22411;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#23454;&#29616;&#65292;&#26263;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#30340;&#28781;&#32477;&#39118;&#38505;&#21487;&#33021;&#22312;&#24403;&#21069;&#31185;&#23398;&#26041;&#27861;&#19979;&#38590;&#20197;&#35266;&#23519;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35752;&#35770;AI&#25152;&#24102;&#26469;&#30340;&#28781;&#32477;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28781;&#32477;&#32423;&#21476;&#21704;&#29305;&#23450;&#24459;&#65292;&#21363;&#8220;&#20960;&#20046;&#20219;&#20309;&#30446;&#26631;&#35268;&#33539;&#65292;&#22914;&#26524;&#36807;&#24230;&#36861;&#27714;&#65292;&#23558;&#23548;&#33268;&#20154;&#31867;&#30340;&#28781;&#32477;&#8221;&#65292;&#24182;&#35797;&#22270;&#20102;&#35299;&#21738;&#20123;&#24418;&#24335;&#27169;&#22411;&#36866;&#21512;&#30740;&#31350;&#36825;&#19968;&#20551;&#35774;&#12290;&#25105;&#20204;&#23545;&#28781;&#32477;&#32423;&#21476;&#21704;&#29305;&#23450;&#24459;&#26159;&#21542;&#25104;&#31435;&#25345;&#20013;&#31435;&#24577;&#24230;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#32452;&#24517;&#35201;&#30340;&#26465;&#20214;&#65292;&#36825;&#20123;&#26465;&#20214;&#23545;&#20110;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#28781;&#32477;&#32423;&#21476;&#21704;&#29305;&#23450;&#24459;&#20855;&#20307;&#35770;&#28857;&#30340;&#27169;&#22411;&#26159;&#24517;&#35201;&#30340;&#12290;&#30001;&#20110;&#27599;&#20010;&#26465;&#20214;&#20284;&#20046;&#22312;&#30456;&#24403;&#31243;&#24230;&#19978;&#22686;&#21152;&#20102;&#26368;&#32456;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24418;&#24335;&#35780;&#20272;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#20250;&#24322;&#24120;&#22256;&#38590;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#21487;&#33021;&#24615;&#65292;&#21363;&#26080;&#35770;&#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#30340;&#28781;&#32477;&#39118;&#38505;&#26159;&#21542;&#30495;&#23454;&#65292;&#20854;&#28508;&#22312;&#21160;&#24577;&#21487;&#33021;&#22312;&#24403;&#21069;&#31185;&#23398;&#26041;&#27861;&#19979;&#26080;&#27861;&#35266;&#23519;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05540v1 Announce Type: cross  Abstract: In an effort to inform the discussion surrounding existential risks from AI, we formulate Extinction-level Goodhart's Law as "Virtually any goal specification, pursued to the extreme, will result in the extinction of humanity", and we aim to understand which formal models are suitable for investigating this hypothesis. Note that we remain agnostic as to whether Extinction-level Goodhart's Law holds or not. As our key contribution, we identify a set of conditions that are necessary for a model that aims to be informative for evaluating specific arguments for Extinction-level Goodhart's Law. Since each of the conditions seems to significantly contribute to the complexity of the resulting model, formally evaluating the hypothesis might be exceedingly difficult. This raises the possibility that whether the risk of extinction from artificial intelligence is real or not, the underlying dynamics might be invisible to current scientific method
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#30446;&#26631;&#26102;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#26679;&#26412;&#25968;&#37327;&#38656;&#27714;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#20013;&#65292;FQI-LOG&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.05385</link><description>&lt;p&gt;
&#22312;&#25209;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20999;&#25442;&#25439;&#22833;&#20989;&#25968;&#26469;&#38477;&#20302;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Switching the Loss Reduces the Cost in Batch Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05385
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#30446;&#26631;&#26102;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#26679;&#26412;&#25968;&#37327;&#38656;&#27714;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#20013;&#65292;FQI-LOG&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#65288;FQI-LOG&#65289;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;FQI-LOG&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#23545;&#20110;&#37027;&#20123;&#36890;&#36807;&#26368;&#20248;&#34892;&#20026;&#23454;&#29616;&#30446;&#26631;&#19988;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#20026;&#38646;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#25209;RL&#20013;&#35777;&#26126;&#20855;&#26377;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#30340;&#19968;&#33324;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#65292;FQI-LOG&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#22320;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#19978;&#20351;&#29992;&#30340;&#26679;&#26412;&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#35201;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05385v1 Announce Type: new  Abstract: We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#35774;&#35745;&#32454;&#32990;&#37325;&#32534;&#31243;&#65292;&#21033;&#29992;&#21151;&#33021;&#36716;&#24405;&#32593;&#32476;&#30340;&#30693;&#35782;&#26469;&#25511;&#21046;&#32454;&#32990;&#34892;&#20026;&#65292;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#36716;&#24405;&#29366;&#24577;&#20043;&#38388;&#30340;&#26368;&#23567;&#21270;&#24046;&#24322;</title><link>https://arxiv.org/abs/2403.04837</link><description>&lt;p&gt;
&#32454;&#32990;&#37325;&#32534;&#31243;&#35774;&#35745;&#36890;&#36807;&#21151;&#33021;&#36716;&#24405;&#32593;&#32476;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cell reprogramming design by transfer learning of functional transcriptional networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04837
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#35774;&#35745;&#32454;&#32990;&#37325;&#32534;&#31243;&#65292;&#21033;&#29992;&#21151;&#33021;&#36716;&#24405;&#32593;&#32476;&#30340;&#30693;&#35782;&#26469;&#25511;&#21046;&#32454;&#32990;&#34892;&#20026;&#65292;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#36716;&#24405;&#29366;&#24577;&#20043;&#38388;&#30340;&#26368;&#23567;&#21270;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21512;&#25104;&#29983;&#29289;&#23398;&#12289;&#19979;&#19968;&#20195;&#27979;&#24207;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#20026;&#22522;&#20110;&#22522;&#22240;&#25200;&#21160;&#21644;&#33647;&#29289;&#30340;&#27979;&#37327;&#21709;&#24212;&#26469;&#21512;&#29702;&#35774;&#35745;&#26032;&#30340;&#30142;&#30149;&#27835;&#30103;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#32454;&#32990;&#32593;&#32476;&#30340;&#30693;&#35782;&#19981;&#23436;&#25972;&#21644;&#21487;&#33021;&#24178;&#39044;&#30340;&#32452;&#21512;&#29190;&#28856;&#65292;&#36825;&#20004;&#32773;&#37117;&#26080;&#27861;&#36890;&#36807;&#23454;&#39564;&#26469;&#20811;&#26381;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#26469;&#25511;&#21046;&#32454;&#32990;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#19982;&#20154;&#31867;&#32454;&#32990;&#21629;&#36816;&#30456;&#20851;&#30340;&#36716;&#24405;&#32452;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#29983;&#25104;&#19968;&#20010;&#21487;&#20197;&#36716;&#31227;&#21040;&#29305;&#23450;&#37325;&#32534;&#31243;&#30446;&#26631;&#30340;&#32593;&#32476;&#21160;&#24577;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#23545;&#22522;&#22240;&#25200;&#21160;&#30340;&#36716;&#24405;&#21453;&#24212;&#65292;&#20197;&#26368;&#23567;&#21270;&#32473;&#23450;&#21021;&#22987;&#21644;&#30446;&#26631;&#36716;&#24405;&#29366;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#26469;&#35777;&#26126;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04837v1 Announce Type: cross  Abstract: Recent developments in synthetic biology, next-generation sequencing, and machine learning provide an unprecedented opportunity to rationally design new disease treatments based on measured responses to gene perturbations and drugs to reprogram cells. The main challenges to seizing this opportunity are the incomplete knowledge of the cellular network and the combinatorial explosion of possible interventions, both of which are insurmountable by experiments. To address these challenges, we develop a transfer learning approach to control cell behavior that is pre-trained on transcriptomic data associated with human cell fates, thereby generating a model of the network dynamics that can be transferred to specific reprogramming goals. The approach combines transcriptional responses to gene perturbations to minimize the difference between a given pair of initial and target transcriptional states. We demonstrate our approach's versatility by 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.04789</link><description>&lt;p&gt;
TopicDiff&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#30340;&#20027;&#39064;&#20016;&#23500;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#65288;MCE&#65289;&#26816;&#27979;&#36890;&#24120;&#36328;&#36234;&#22768;&#23398;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#21560;&#24341;&#20102;&#22810;&#23186;&#20307;&#31038;&#21306;&#26085;&#30410;&#22686;&#21152;&#30340;&#20852;&#36259;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#23545;&#35805;&#20013;&#30340;&#35821;&#22659;&#20449;&#24687;&#65292;&#21482;&#26377;&#23569;&#25968;&#32771;&#34385;&#21333;&#19968;&#35821;&#35328;&#27169;&#24577;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#32780;&#24635;&#26159;&#24573;&#35270;&#22768;&#23398;&#21644;&#35270;&#35273;&#20027;&#39064;&#20449;&#24687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;Topic-enriched Diffusion&#65288;TopicDiff&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;MCE&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#20197;&#32531;&#35299;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#12290;&#35814;&#32454;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;TopicDiff&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#23545;MCE&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;TopicDiff&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04789v1 Announce Type: cross  Abstract: Multimodal Conversational Emotion (MCE) detection, generally spanning across the acoustic, vision and language modalities, has attracted increasing interest in the multimedia community. Previous studies predominantly focus on learning contextual information in conversations with only a few considering the topic information in single language modality, while always neglecting the acoustic and vision topic information. On this basis, we propose a model-agnostic Topic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic information in MCE tasks. Particularly, we integrate the diffusion model into neural topic model to alleviate the diversity deficiency problem of neural topic model in capturing topic information. Detailed evaluations demonstrate the significant improvements of TopicDiff over the state-of-the-art MCE baselines, justifying the importance of multimodal topic information to MCE and the effectiveness of Topic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;</title><link>https://arxiv.org/abs/2403.04769</link><description>&lt;p&gt;
&#31227;&#38500;GPT4&#30340;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Removing GPT4's Filter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04769
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT4&#26368;&#21021;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#21363;&#24535;&#24895;&#32773;&#25552;&#20379;&#21453;&#39304;&#20197;&#25945;&#23548;GPT4&#19981;&#35201;&#29983;&#25104;&#19981;&#24403;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25805;&#20316;&#24050;&#32463;&#36827;&#34892;&#24494;&#35843;&#30340;&#29256;&#26412;&#65292;&#20351;&#20854;&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;RLHF&#65288;Reinforcement learning from Human Feedback&#65289;&#30340;&#34892;&#20026;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#20102;&#27169;&#22411;&#22312;RLHF&#26399;&#38388;&#23398;&#20064;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;GPT4&#22312;&#27809;&#26377;&#32463;&#36807;RLHF&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#26102;&#65292;&#23427;&#22833;&#21435;&#20102;&#25152;&#26377;&#25233;&#21046;&#21147;&#65292;&#21482;&#38656;&#21069;&#20960;&#20010;&#35789;&#23601;&#21487;&#20197;&#29983;&#25104;&#38750;&#24120;&#19981;&#24403;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04769v1 Announce Type: cross  Abstract: GPT4 was initially trained on large amounts of data, and then fine-tuned using Reinforcement learning from Human Feedback (RLHF), which is when volunteers give feedback in order to teach GPT4 not to create inappropriate content. In this paper, we present a method to manipulate the fine-tuned version into reverting to pre-RLHF behavior, effectively removing all safety mechanisms that the model learned during RLHF. In particular, when GPT4 acts without RLHF, it loses all inhibition, and can complete very inappropriate content given only the first few words.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.04493</link><description>&lt;p&gt;
&#20351;&#22270;&#20687;&#30495;&#23454;&#30340;&#22240;&#32032;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What makes an image realistic?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04493
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#25105;&#20204;&#22312;&#29983;&#25104;&#30475;&#36215;&#26469;&#30495;&#23454;&#30340;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#26080;&#35770;&#26159;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#36824;&#26159;&#35270;&#39057;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#20043;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#21363;&#37327;&#21270;&#29616;&#23454;&#20027;&#20041;&#65292;&#21363;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#12290;&#20174;&#31639;&#27861;&#20449;&#24687;&#29702;&#35770;&#30340;&#35266;&#28857;&#20986;&#21457;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#20026;&#20160;&#20040;&#19968;&#20010;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;&#21333;&#29420;&#19981;&#33021;&#35299;&#20915;&#23427;&#65292;&#20197;&#21450;&#19968;&#20010;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#26159;&#20160;&#20040;&#26679;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#65292;&#19981;&#20687;&#23545;&#25239;&#24615;&#35780;&#35770;&#32773;&#37027;&#26679;&#38656;&#35201;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#23613;&#31649;&#36890;&#29992;&#35780;&#35770;&#32773;&#24182;&#19981;&#31435;&#21363;&#23454;&#29992;&#65292;&#20294;&#23427;&#20204;&#26082;&#21487;&#20197;&#20316;&#20026;&#24341;&#23548;&#23454;&#38469;&#23454;&#29616;&#30340;&#21271;&#26497;&#26143;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04493v1 Announce Type: new  Abstract: The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool 
&lt;/p&gt;</description></item><item><title>&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04261</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#21306;&#25361;&#25112;&#25512;&#21160;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advancing Biomedical Text Mining with Community Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04261
&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#39046;&#22495;&#31215;&#32047;&#20102;&#22823;&#37327;&#26469;&#33258;&#31185;&#23398;&#25991;&#29486;&#12289;&#30005;&#23376;&#30149;&#21382;&#12289;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#21508;&#26041;&#38754;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#28982;&#32780;&#25163;&#21160;&#22788;&#29702;&#21644;&#20998;&#26512;&#36825;&#20123;&#24222;&#22823;&#19988;&#22797;&#26434;&#30340;&#36164;&#28304;&#26159;&#32791;&#26102;&#19988;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#65292;&#20063;&#31216;&#20026;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22791;&#21463;&#20851;&#27880;&#12290;&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#25361;&#25112;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#24320;&#21457;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#25366;&#25496;&#21644;&#20449;&#24687;&#22788;&#29702;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#30340;&#24179;&#21488;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19982;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#26377;&#20851;&#30340;&#26368;&#26032;&#31038;&#21306;&#25361;&#25112;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04261v1 Announce Type: new  Abstract: The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining. Firs
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04224</link><description>&lt;p&gt;
Aligners: &#35299;&#32806;LLMs&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligners: Decoupling LLMs and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#40784;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#27599;&#20010;LLM&#21644;&#23545;&#40784;&#26631;&#20934;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35757;&#32451;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29992;&#20110;&#23545;&#40784;&#32473;&#23450;&#26631;&#20934;&#30340;&#20219;&#20309;LLM&#30340;&#23545;&#40784;&#27169;&#22411;&#26469;&#35299;&#32806;LLMs&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#23569;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#40784;&#27169;&#22411;&#35757;&#32451;&#37197;&#26041;&#20165;&#20381;&#36182;&#20110;&#20351;&#29992;&#65288;&#25552;&#31034;&#30340;&#65289;LLM &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#36866;&#24212;&#21508;&#31181;&#23545;&#40784;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#36947;&#24503;&#8221;&#23545;&#40784;&#22120;&#24182;&#22312;&#23454;&#39564;&#19978;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#26469;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04161</link><description>&lt;p&gt;
SWAP-NAS: &#36866;&#29992;&#20110;&#36229;&#24555;&#36895;NAS&#30340;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#65288;&#21363;&#38646;&#25104;&#26412;&#20195;&#29702;&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36991;&#20813;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#23588;&#20854;&#26159;&#22312;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#65292;&#27604;&#22914;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#20851;&#32852;&#24615;&#21644;&#24046;&#21170;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;&#21450;&#20854;&#34893;&#29983;&#29289;SWAP-Score&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;&#23427;&#27979;&#37327;&#20102;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;SWAP-Score&#19982;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#30340;&#30495;&#23454;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#22312;NAS-Bench-101/201/301&#21644;TransNAS-Bench-101&#19978;&#32988;&#36807;&#20102;15&#31181;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;SWAP-Score&#21487;&#20197;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#36825;&#22312;&#22522;&#20110;&#21333;&#20803;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#25511;&#21046;&#12290;&#20363;&#22914;&#65292;Spearman&#30340;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04161v1 Announce Type: new  Abstract: Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman's rank
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.04121</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Reason and Plan?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04121
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20154;&#31867;&#26377;&#26102;&#20505;&#34920;&#29616;&#20986;&#33021;&#22815;&#36890;&#36807;&#33258;&#25105;&#25209;&#35780;&#32416;&#27491;&#33258;&#24049;&#38169;&#35823;&#29468;&#27979;&#30340;&#33021;&#21147;&#65292;&#20294;&#20284;&#20046;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#27809;&#26377;&#20381;&#25454;&#25903;&#25345;&#36825;&#19968;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04121v1 Announce Type: new  Abstract: While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.
&lt;/p&gt;</description></item><item><title>DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.03768</link><description>&lt;p&gt;
DeepCRE&#65306;&#21033;&#29992;&#23574;&#31471;&#35745;&#31639;&#27169;&#22411;&#25913;&#38761;&#33647;&#29289;&#30740;&#21457;
&lt;/p&gt;
&lt;p&gt;
DeepCRE: Revolutionizing Drug R&amp;D with Cutting-Edge Computational Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03768
&lt;/p&gt;
&lt;p&gt;
DeepCRE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#32423;&#21035;CRE&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;17.7&#65285;&#65292;&#22312;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#24182;&#25104;&#21151;&#30830;&#23450;&#20102;&#20845;&#20010;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33647;&#29289;&#24320;&#21457;&#39046;&#22495;&#21644;&#27835;&#30103;&#24212;&#29992;&#39046;&#22495;&#37117;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#27835;&#30103;&#39046;&#22495;&#38656;&#35201;&#26356;&#22810;&#30340;&#27835;&#30103;&#36873;&#25321;&#65292;&#21516;&#26102;&#22823;&#37327;&#26377;&#21069;&#26223;&#30340;&#20020;&#24202;&#21069;&#33647;&#29289;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#22833;&#36133;&#12290;&#19968;&#20010;&#21407;&#22240;&#26159;&#22312;&#33647;&#29289;&#24320;&#21457;&#30340;&#21518;&#26399;&#38454;&#27573;&#20132;&#21449;&#33647;&#29289;&#21453;&#24212;&#35780;&#20272;&#65288;CRE&#65289;&#30340;&#19981;&#36275;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#27169;&#25311;&#30340;CRE&#27169;&#22411;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23398;&#35201;&#20040;&#23616;&#38480;&#20110;&#26089;&#26399;&#24320;&#21457;&#38454;&#27573;&#65292;&#35201;&#20040;&#32570;&#20047;&#23545;&#20840;&#38754;CRE&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeepCRE&#30340;&#26032;&#22411;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;DeepCRE&#22312;&#25512;&#21160;&#27835;&#30103;&#21457;&#29616;&#21644;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;DeepCRE&#36890;&#36807;&#23454;&#29616;&#24739;&#32773;&#32423;&#21035;CRE&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;17.7\%&#65292;&#25351;&#31034;&#32423;&#21035;CRE&#22686;&#21152;&#20102;5&#20493;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;DeepCRE&#24050;&#32463;&#30830;&#23450;&#20102;&#20845;&#20010;&#26174;&#31034;&#20986;&#26126;&#26174;&#26356;&#22823;&#20248;&#21183;&#30340;&#33647;&#29289;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03768v1 Announce Type: new  Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater ef
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#19982;&#20256;&#32479;ABM&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27169;&#22411;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#26032;&#35199;&#20848;2019&#24180;&#40635;&#30137;&#29190;&#21457;&#65292;&#28145;&#20837;&#27934;&#23519;&#20256;&#26579;&#30149;&#29190;&#21457;&#30340;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.03434</link><description>&lt;p&gt;
&#19968;&#31181;AI&#21551;&#29992;&#30340;&#22522;&#20110;Agent&#30340;&#27169;&#22411;&#21450;&#20854;&#22312;&#26032;&#35199;&#20848;&#40635;&#30137;&#29190;&#21457;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03434
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#19982;&#20256;&#32479;ABM&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27169;&#22411;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#25311;&#26032;&#35199;&#20848;2019&#24180;&#40635;&#30137;&#29190;&#21457;&#65292;&#28145;&#20837;&#27934;&#23519;&#20256;&#26579;&#30149;&#29190;&#21457;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#27169;&#22411;&#65288;ABMs&#65289;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#21355;&#29983;&#21644;&#20256;&#26579;&#30149;&#35843;&#26597;&#30340;&#32972;&#26223;&#19979;&#12290;&#20026;&#20102;&#22686;&#24378;&#20256;&#32479;ABM&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#27169;&#22411;&#26657;&#20934;&#24182;&#20943;&#23569;&#25193;&#23637;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#36807;&#32806;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#26469;&#36827;&#34892;&#24352;&#37327;&#21270;&#21644;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;Agent&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#34987;&#29992;&#20110;&#30740;&#31350;2019&#24180;&#21457;&#29983;&#22312;&#26032;&#35199;&#20848;&#30340;&#40635;&#30137;&#29190;&#21457;&#65292;&#23637;&#31034;&#20102;&#22312;&#39640;&#23792;&#26399;&#37325;&#22797;&#30149;&#20363;&#20013;&#20934;&#30830;&#27169;&#25311;&#29190;&#21457;&#21160;&#24577;&#30340;&#26377;&#24076;&#26395;&#33021;&#21147;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#21644;&#20256;&#32479;ABMs&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#20256;&#26579;&#30149;&#29190;&#21457;&#30340;&#21160;&#24577;&#65292;&#20174;&#32780;&#24110;&#21161;&#25105;&#20204;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03434v1 Announce Type: cross  Abstract: Agent Based Models (ABMs) have emerged as a powerful tool for investigating complex social interactions, particularly in the context of public health and infectious disease investigation. In an effort to enhance the conventional ABM, enabling automated model calibration and reducing the computational resources needed for scaling up the model, we have developed a tensorized and differentiable agent-based model by coupling Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) network. The model was employed to investigate the 2019 measles outbreak occurred in New Zealand, demonstrating a promising ability to accurately simulate the outbreak dynamics, particularly during the peak period of repeated cases. This paper shows that by leveraging the latest Artificial Intelligence (AI) technology and the capabilities of traditional ABMs, we gain deeper insights into the dynamics of infectious disease outbreaks. This, in turn, helps us ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;&#21644;&#26368;&#23567;&#25554;&#20540;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#21487;&#20197;&#34920;&#31034;&#20026;&#32447;&#24615;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.03353</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#20551;&#35774;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Hypothesis Spaces for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;&#21644;&#26368;&#23567;&#25554;&#20540;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#21487;&#20197;&#34920;&#31034;&#20026;&#32447;&#24615;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#20551;&#35774;&#31354;&#38388;&#12290;&#36890;&#36807;&#23558;DNN&#35270;&#20026;&#20004;&#20010;&#21464;&#37327;&#30340;&#20989;&#25968;&#65292;&#21363;&#29289;&#29702;&#21464;&#37327;&#21644;&#21442;&#25968;&#21464;&#37327;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;DNNs&#30340;&#21407;&#22987;&#38598;&#21512;&#65292;&#21442;&#25968;&#21464;&#37327;&#20301;&#20110;&#30001;DNNs&#30340;&#26435;&#37325;&#30697;&#38453;&#21644;&#20559;&#32622;&#20915;&#23450;&#30340;&#19968;&#32452;&#28145;&#24230;&#21644;&#23485;&#24230;&#20013;&#12290;&#28982;&#21518;&#22312;&#24369;*&#25299;&#25169;&#20013;&#23436;&#25104;&#21407;&#22987;DNN&#38598;&#21512;&#30340;&#32447;&#24615;&#36328;&#24230;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#29289;&#29702;&#21464;&#37327;&#20989;&#25968;&#30340;Banach&#31354;&#38388;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#26500;&#36896;&#30340;Banach&#31354;&#38388;&#26159;&#19968;&#20010;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65288;RKBS&#65289;&#65292;&#24182;&#26500;&#36896;&#20854;&#20877;&#29983;&#26680;&#12290;&#36890;&#36807;&#20026;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#24314;&#31435;&#34920;&#36798;&#23450;&#29702;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#23398;&#20064;&#27169;&#22411;&#65292;&#27491;&#21017;&#21270;&#23398;&#20064;&#21644;&#26368;&#23567;&#25554;&#20540;&#38382;&#39064;&#22312;&#32467;&#26524;RKBS&#20013;&#12290;&#34920;&#36798;&#23450;&#29702;&#25581;&#31034;&#20102;&#36825;&#20123;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#21487;&#20197;&#34920;&#31034;&#20026;&#32447;&#24615;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03353v1 Announce Type: cross  Abstract: This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;5G&#32593;&#32476;&#20013;&#26816;&#27979;&#24178;&#25200;&#32773;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#38408;&#20540;&#28145;&#24230;&#23398;&#20064;&#24178;&#25200;&#26816;&#27979;&#22120;&#65292;&#19987;&#27880;&#20110;SSB&#30340;RF&#39046;&#22495;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02645</link><description>&lt;p&gt;
&#22312;5G RF&#39046;&#22495;&#65292;&#29992;&#20110;&#24178;&#25200;&#26816;&#27979;&#30340;&#31354;&#20013;&#21452;&#38408;&#20540;&#28145;&#24230;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Over-The-Air Double-Threshold Deep Learner for Jamming Detection in 5G RF domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;5G&#32593;&#32476;&#20013;&#26816;&#27979;&#24178;&#25200;&#32773;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#38408;&#20540;&#28145;&#24230;&#23398;&#20064;&#24178;&#25200;&#26816;&#27979;&#22120;&#65292;&#19987;&#27880;&#20110;SSB&#30340;RF&#39046;&#22495;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;5G&#26080;&#32447;&#36890;&#20449;&#30340;&#21457;&#23637;&#65292;&#21516;&#27493;&#20449;&#21495;&#22359;&#65288;SSB&#65289;&#22312;&#35774;&#22791;&#21516;&#27493;&#21644;&#26381;&#21153;&#21487;&#35775;&#38382;&#24615;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SSB&#20256;&#36755;&#20855;&#26377;&#21487;&#39044;&#27979;&#24615;&#65292;&#21253;&#25324;&#20027;&#35201;&#21516;&#27493;&#20449;&#21495;&#65288;PSS&#65289;&#21644;&#27425;&#35201;&#21516;&#27493;&#20449;&#21495;&#65288;SSS&#65289;&#65292;&#24178;&#25200;&#25915;&#20987;&#26159;&#37325;&#35201;&#23041;&#32961;&#12290;&#26412;&#25991;&#21033;&#29992;RF&#39046;&#22495;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;5G&#32593;&#32476;&#24178;&#25200;&#26816;&#27979;&#25216;&#26415;&#12290;&#19982;&#29616;&#26377;&#30340;&#22823;&#22810;&#20381;&#36182;&#32593;&#32476;&#21442;&#25968;&#30340;&#24178;&#25200;&#26816;&#27979;&#31639;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#27880;&#20110;SSB&#24341;&#20837;&#20102;&#21452;&#38408;&#20540;&#28145;&#24230;&#23398;&#20064;&#24178;&#25200;&#26816;&#27979;&#22120;&#12290;&#35813;&#26816;&#27979;&#26041;&#27861;&#20391;&#37325;&#20110;RF&#39046;&#22495;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#19982;&#29616;&#26377;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#19968;&#20010;&#39044;&#22788;&#29702;&#22359;&#26469;&#25552;&#21462;PSS&#30456;&#20851;&#24615;&#21644;&#27599;&#20010;&#31354;&#38386;&#36164;&#28304;&#20803;&#32032;&#30340;&#33021;&#37327;&#65288;EPNRE&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02645v1 Announce Type: cross  Abstract: With the evolution of 5G wireless communications, the Synchronization Signal Block (SSB) plays a critical role in the synchronization of devices and accessibility of services. However, due to the predictable nature of SSB transmission, including the Primary and Secondary Synchronization Signals (PSS and SSS), jamming attacks are critical threats. By leveraging RF domain knowledge, this work presents a novel deep learning-based technique for detecting jammers in 5G networks. Unlike the existing jamming detection algorithms that mostly rely on network parameters, we introduce a double threshold deep learning jamming detector by focusing on the SSB. The detection method is focused on RF domain features and improves the robustness of the network without requiring integration with the pre-existing network infrastructure. By integrating a preprocessing block that extracts PSS correlation and energy per null resource elements (EPNRE) characte
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#21040;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#30830;&#20445;&#31283;&#20581;&#24615;&#65292;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#31639;&#27861;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#65292;&#31616;&#21333;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#65292;&#39564;&#35777;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#23545;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#30340;&#25512;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.01865</link><description>&lt;p&gt;
&#36890;&#36807;&#38170;&#22810;&#20803;&#20998;&#26512;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving generalisation via anchor multivariate analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01865
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#21040;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#30830;&#20445;&#31283;&#20581;&#24615;&#65292;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#31639;&#27861;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#65292;&#31616;&#21333;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#65292;&#39564;&#35777;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#23545;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#30340;&#25512;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#65292;&#20197;&#25913;&#21892;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#30830;&#20445;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#65288;MVA&#65289;&#31639;&#27861;&#65292;&#22914;&#65288;&#27491;&#20132;&#21270;&#65289;PLS&#12289;RRR&#21644;MLR&#65292;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#31616;&#21333;&#30340;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#27668;&#20505;&#31185;&#23398;&#38382;&#39064;&#20013;&#65292;&#20026;&#25152;&#36873;&#31639;&#27861;&#25552;&#20379;&#20102;&#20272;&#35745;&#22120;&#65292;&#23637;&#31034;&#20102;&#20854;&#19968;&#33268;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32463;&#39564;&#39564;&#35777;&#31361;&#26174;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#24378;&#35843;&#20854;&#19982;MVA&#26041;&#27861;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#22686;&#24378;&#21487;&#22797;&#21046;&#24615;&#30340;&#21516;&#26102;&#25269;&#24481;&#20998;&#24067;&#36716;&#31227;&#20013;&#30340;&#20316;&#29992;&#12290;&#25193;&#23637;&#30340;AR&#26694;&#26550;&#25512;&#36827;&#20102;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#65292;&#35299;&#20915;&#20102;&#21487;&#38752;OOD&#27867;&#21270;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01865v1 Announce Type: cross  Abstract: We introduce a causal regularisation extension to anchor regression (AR) for improved out-of-distribution (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against distribution shifts. Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against distribution shifts. The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.
&lt;/p&gt;</description></item><item><title>NASH&#26159;&#19968;&#31181;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#30828;&#20214;&#35774;&#35745;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#12289;&#20302;&#24310;&#36831;&#21644;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.01845</link><description>&lt;p&gt;
NASH&#65306;&#29992;&#20110;&#30828;&#20214;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01845
&lt;/p&gt;
&lt;p&gt;
NASH&#26159;&#19968;&#31181;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#30828;&#20214;&#35774;&#35745;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#12289;&#20302;&#24310;&#36831;&#21644;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#37096;&#32626;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22312;&#39640;&#20934;&#30830;&#24615;&#12289;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NASH&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#12290;&#20351;&#29992;NASH&#65292;&#30828;&#20214;&#35774;&#35745;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#29256;&#26412;&#30340;NASH&#31574;&#30053;&#65292;&#25152;&#26377;&#36825;&#20123;&#31574;&#30053;&#26174;&#31034;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#20247;&#22810;&#27169;&#22411;&#25805;&#20316;&#20013;&#36873;&#25321;&#29305;&#23450;&#25805;&#20316;&#65292;&#24341;&#23548;&#35757;&#32451;&#36807;&#31243;&#26397;&#21521;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312; ResNet18 &#25110; ResNet34 &#19978;&#24212;&#29992;NASH&#65292;&#19982;&#38750;NASH&#29256;&#26412;&#30456;&#27604;&#65292;&#21487;&#20351;Top1&#20934;&#30830;&#29575;&#25552;&#39640;&#39640;&#36798;3.1%&#65292;Top5&#20934;&#30830;&#29575;&#25552;&#39640;&#39640;&#36798;2.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01845v1 Announce Type: cross  Abstract: As machine learning (ML) algorithms get deployed in an ever-increasing number of applications, these algorithms need to achieve better trade-offs between high accuracy, high throughput and low latency. This paper introduces NASH, a novel approach that applies neural architecture search to machine learning hardware. Using NASH, hardware designs can achieve not only high throughput and low latency but also superior accuracy performance. We present four versions of the NASH strategy in this paper, all of which show higher accuracy than the original models. The strategy can be applied to various convolutional neural networks, selecting specific model operations among many to guide the training process toward higher accuracy. Experimental results show that applying NASH on ResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top 5 accuracy increase of up to 2.2% compared to the non-NASH version when tested on the Imag
&lt;/p&gt;</description></item><item><title>PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00929</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#20026;&#21407;&#35821;&#25645;&#24314;&#20219;&#21153;&#30340;&#26694;&#26550;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00929
&lt;/p&gt;
&lt;p&gt;
PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#35753;&#26426;&#22120;&#20154;&#23398;&#20250;&#22797;&#26434;&#30340;&#25805;&#20316;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#21463;&#21040;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#22797;&#21512;&#35823;&#24046;&#20250;&#22312;&#20219;&#21153;&#26102;&#27573;&#20869;&#32047;&#31215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRIME&#65288;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#25968;&#25454;&#25928;&#29575;&#27169;&#20223;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;PRIME&#36890;&#36807;&#23558;&#20219;&#21153;&#28436;&#31034;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#26469;&#25645;&#24314;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#23398;&#20064;&#19968;&#20010;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#26469;&#23545;&#21407;&#35821;&#24207;&#21015;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PRIME&#22312;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#29575;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#39640;&#20986;10-34&#65285;&#65292;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#39640;&#20986;20-48&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00929v1 Announce Type: cross  Abstract: Imitation learning has shown great potential for enabling robots to acquire complex manipulation behaviors. However, these algorithms suffer from high sample complexity in long-horizon tasks, where compounding errors accumulate over the task horizons. We present PRIME (PRimitive-based IMitation with data Efficiency), a behavior primitive-based framework designed for improving the data efficiency of imitation learning. PRIME scaffolds robot tasks by decomposing task demonstrations into primitive sequences, followed by learning a high-level control policy to sequence primitives through imitation learning. Our experiments demonstrate that PRIME achieves a significant performance improvement in multi-stage manipulation tasks, with 10-34% higher success rates in simulation over state-of-the-art baselines and 20-48% on physical hardware.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#31454;&#20105;&#20998;&#26512;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35843;&#25972;FTRL&#23398;&#20064;&#29575;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#20351;&#20854;&#22312;&#24120;&#25968;&#22240;&#23376;&#20869;&#36798;&#21040;&#26368;&#20339;&#31454;&#20105;&#27604;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#24403;&#24809;&#32602;&#39033;&#20855;&#26377;&#36817;&#20284;&#21333;&#35843;&#24615;&#26102;&#30340;&#31454;&#20105;&#27604;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00715</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;FTRL&#31639;&#27861;&#30340;&#31454;&#20105;&#27604;&#20998;&#26512;&#21644;&#26368;&#20339;&#26041;&#26696;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00715
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#31454;&#20105;&#20998;&#26512;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35843;&#25972;FTRL&#23398;&#20064;&#29575;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#20351;&#20854;&#22312;&#24120;&#25968;&#22240;&#23376;&#20869;&#36798;&#21040;&#26368;&#20339;&#31454;&#20105;&#27604;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#24403;&#24809;&#32602;&#39033;&#20855;&#26377;&#36817;&#20284;&#21333;&#35843;&#24615;&#26102;&#30340;&#31454;&#20105;&#27604;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Follow-The-Regularized-Leader (FTRL)&#34987;&#35748;&#20026;&#26159;&#22312;&#32447;&#23398;&#20064;&#20013;&#19968;&#31181;&#26377;&#25928;&#19988;&#22810;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#23398;&#20064;&#29575;&#30340;&#24688;&#24403;&#36873;&#25321;&#23545;&#20110;&#20943;&#23567;&#21518;&#24724;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#35843;&#25972;FTRL&#23398;&#20064;&#29575;&#30340;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#31454;&#20105;&#20998;&#26512;&#26694;&#26550;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#31454;&#20105;&#27604;&#30340;&#19979;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#23398;&#20064;&#29575;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#20351;&#20854;&#22312;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#20869;&#36798;&#21040;&#19979;&#30028;&#30340;&#19978;&#30028;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#26368;&#20248;&#31454;&#20105;&#27604;&#26159;&#30001;&#24809;&#32602;&#39033;&#30340;&#32452;&#25104;&#37096;&#20998;&#30340;&#65288;&#36817;&#20284;&#65289;&#21333;&#35843;&#24615;&#25152;&#20915;&#23450;&#30340;&#65292;&#34920;&#26126;&#22914;&#26524;&#24809;&#32602;&#39033;&#30340;&#32452;&#25104;&#37096;&#20998;&#24418;&#25104;&#21333;&#35843;&#38750;&#22686;&#24207;&#21015;&#65292;&#21017;&#21487;&#20197;&#23454;&#29616;&#24120;&#25968;&#31454;&#20105;&#27604;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#24809;&#32602;&#39033;$\xi$&#36817;&#20284;&#21333;&#35843;&#38750;&#22686;&#26102;&#30340;&#32039;&#23494;&#31454;&#20105;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26356;&#26032;&#35268;&#21017;&#34987;&#31216;&#20026;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00715v1 Announce Type: new  Abstract: Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile approach in online learning, where appropriate choice of the learning rate is crucial for smaller regret. To this end, we formulate the problem of adjusting FTRL's learning rate as a sequential decision-making problem and introduce the framework of competitive analysis. We establish a lower bound for the competitive ratio and propose update rules for learning rate that achieves an upper bound within a constant factor of this lower bound. Specifically, we illustrate that the optimal competitive ratio is characterized by the (approximate) monotonicity of components of the penalty term, showing that a constant competitive ratio is achievable if the components of the penalty term form a monotonically non-increasing sequence, and derive a tight competitive ratio when penalty terms are $\xi$-approximately monotone non-increasing. Our proposed update rule, referred to a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#21644;&#28388;&#27874;&#22120;&#21407;&#23376;&#30340;&#27010;&#24565;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#22823;&#22411;&#21367;&#31215;&#27169;&#22411;&#26102;&#20165;&#35843;&#25972;&#23569;&#37327;&#21442;&#25968;&#26469;&#25552;&#21462;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00269</link><description>&lt;p&gt;
&#22823;&#22411;&#21367;&#31215;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Tuning of Large Convolutional Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00269
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#21644;&#28388;&#27874;&#22120;&#21407;&#23376;&#30340;&#27010;&#24565;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#22823;&#22411;&#21367;&#31215;&#27169;&#22411;&#26102;&#20165;&#35843;&#25972;&#23569;&#37327;&#21442;&#25968;&#26469;&#25552;&#21462;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#39640;&#35745;&#31639;&#21644;&#21442;&#25968;&#22797;&#26434;&#24615;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20165;&#26356;&#26032;&#19979;&#28216;&#20219;&#21153;&#30340;&#37096;&#20998;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#36890;&#24120;&#24573;&#35270;&#20102;&#21367;&#31215;&#26680;&#30340;&#29420;&#29305;&#23646;&#24615;&#65292;&#32780;&#21367;&#31215;&#26680;&#20173;&#28982;&#26159;&#35768;&#22810;&#22823;&#22411;&#27169;&#22411;&#30340;&#22522;&#26412;&#20803;&#32032;&#65292;&#27604;&#22914;Stable Diffusion&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22312;&#27599;&#20010;&#32593;&#32476;&#23618;&#20869;&#20998;&#35299;&#21367;&#31215;&#26680;&#21040;&#19968;&#23567;&#32452;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#20803;&#32032;&#65292;&#21363;&#28388;&#27874;&#22120;&#21407;&#23376;&#65292;&#24341;&#20837;&#20102;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20165;&#35843;&#25972;&#28388;&#27874;&#22120;&#21407;&#23376;&#65288;&#36890;&#24120;&#20026;&#20960;&#30334;&#20010;&#21442;&#25968;&#65289;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#21462;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#28508;&#22312;&#22320;&#25193;&#23637;&#35843;&#25972;&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#22320;&#23558;&#27599;&#20010;&#31579;&#36873;&#21407;&#23376;&#20998;&#35299;&#21040;&#21478;&#19968;&#32452;&#31579;&#36873;&#21407;&#23376;&#26469;&#29983;&#25104;&#19968;&#20010;&#36807;&#23436;&#22791;&#30340;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00269v1 Announce Type: cross  Abstract: To address the high computational and parameter complexity associated with fine-tuning large pre-trained models, researchers have developed parameter-efficient methods, where only partial parameters are updated for downstream tasks. However, these works often overlook the distinct properties of convolutional kernels, which still remain essential elements in many large models, such as Stable Diffusion. In this study, we first introduce filter subspace by decomposing convolutional kernels within each network layer over a small set of filter subspace elements, referred to as filter atoms. We then fine-tune these models to extract task-specific representation by only adapting the filter atoms, a few hundred parameters typically. To potentially expand the parameter space for tuning, we further show a simple approach to generate an overcomplete filter subspace by recursively decomposing each filter atom over another set of filter atoms. The 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#38024;&#23545;&#36890;&#29992;AISG&#65292;&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#65292;&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.18781</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#32423;&#20449;&#24565;&#30340;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#22312;&#19981;&#23545;&#31216;&#20449;&#24687;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18781
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#38024;&#23545;&#36890;&#29992;AISG&#65292;&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#65292;&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21338;&#24328;&#20986;&#29616;&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#65292;&#22914;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#21644;IT&#22522;&#30784;&#35774;&#26045;&#65292;&#20449;&#24687;&#19981;&#23545;&#31216;&#20026;&#20915;&#31574;&#23454;&#20307;&#65288;&#29609;&#23478;&#65289;&#30340;&#20915;&#31574;&#24102;&#26469;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#19981;&#23545;&#31216;&#20449;&#24687;&#38543;&#26426;&#21338;&#24328;&#65288;AISG&#65289;&#30340;&#35745;&#31639;&#26041;&#27861;&#20027;&#35201;&#26159;&#31163;&#32447;&#30340;&#65292;&#38024;&#23545;&#29305;&#27530;&#31867;&#21035;&#30340;AISG&#65292;&#20197;&#36991;&#20813;&#20449;&#24565;&#23618;&#27425;&#65292;&#24182;&#19988;&#32570;&#20047;&#36866;&#24212;&#22343;&#34913;&#20559;&#24046;&#30340;&#22312;&#32447;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#19987;&#38376;&#38024;&#23545;&#36890;&#29992;AISG&#12290;COL&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#12290;&#38024;&#23545;&#20551;&#35774;&#30340;&#23545;&#25163;&#65292;COL&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;COL&#20013;&#30340;&#20551;&#35774;&#19982;t&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18781v1 Announce Type: cross  Abstract: Stochastic games arise in many complex socio-technical systems, such as cyber-physical systems and IT infrastructures, where information asymmetry presents challenges for decision-making entities (players). Existing computational methods for asymmetric information stochastic games (AISG) are primarily offline, targeting special classes of AISGs to avoid belief hierarchies, and lack online adaptability to deviations from equilibrium. To address this limitation, we propose a conjectural online learning (COL), a learning scheme for generic AISGs. COL, structured as a forecaster-actor-critic (FAC) architecture, utilizes first-order beliefs over the hidden states and subjective forecasts of the opponent's strategies. Against the conjectured opponent, COL updates strategies in an actor-critic approach using online rollout and calibrates conjectures through Bayesian learning. We prove that conjecture in COL is asymptotically consistent with t
&lt;/p&gt;</description></item><item><title>ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18609</link><description>&lt;p&gt;
ICE-SEARCH: &#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH: A Language Model-Driven Feature Selection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18609
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;In-Context Evolutionary Search (ICE-SEARCH)&#26041;&#27861;&#65292;&#36825;&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;(LMs)&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;(FS)&#20219;&#21153;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;(MPA)&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;ICE-SEARCH&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20132;&#21449;&#21644;&#31361;&#21464;&#33021;&#21147;&#65292;&#22312;&#19968;&#20010;&#36827;&#21270;&#26694;&#26550;&#20869;&#26174;&#30528;&#25913;&#36827;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#27169;&#22411;&#30340;&#20840;&#38754;&#19990;&#30028;&#30693;&#35782;&#21644;&#20854;&#36866;&#24212;&#21508;&#31181;&#35282;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19977;&#20010;&#20851;&#38190;&#30340;MPA&#20219;&#21153;&#65306;&#20013;&#39118;&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31958;&#23615;&#30149;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;ICE-SEARCH&#22312;&#30830;&#23450;&#21307;&#23398;&#24212;&#29992;&#30340;&#20851;&#38190;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;FS&#26041;&#27861;&#12290;ICE-SEARCH&#22312;&#20013;&#39118;&#39044;&#27979;&#21644;&#31958;&#23615;&#30149;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#27700;&#24179;&#65307;&#20915;&#31574;&#38543;&#26426;&#21270;ICE-SEARCH&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#20013;&#25490;&#21517;&#20026;&#39046;&#20808;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18609v1 Announce Type: cross  Abstract: This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;MIM-Reasoner&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16898</link><description>&lt;p&gt;
MIM-Reasoner: &#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16898
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;MIM-Reasoner&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;&#65288;MIM&#65289;&#35201;&#27714;&#25105;&#20204;&#35782;&#21035;&#19968;&#32452;&#31181;&#23376;&#29992;&#25143;&#65292;&#20197;&#26368;&#22823;&#21270;&#22810;&#37325;&#32593;&#32476;&#20013;&#21463;&#24433;&#21709;&#29992;&#25143;&#30340;&#39044;&#26399;&#25968;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MIM-Reasoner&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#27010;&#29575;&#22270;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25429;&#25417;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16898v1 Announce Type: cross  Abstract: Multiplex influence maximization (MIM) asks us to identify a set of seed users such as to maximize the expected number of influenced users in a multiplex network. MIM has been one of central research topics, especially in nowadays social networking landscape where users participate in multiple online social networks (OSNs) and their influences can propagate among several OSNs simultaneously. Although there exist a couple combinatorial algorithms to MIM, learning-based solutions have been desired due to its generalization ability to heterogeneous networks and their diversified propagation characteristics. In this paper, we introduce MIM-Reasoner, coupling reinforcement learning with probabilistic graphical model, which effectively captures the complex propagation process within and between layers of a given multiplex network, thereby tackling the most challenging problem in MIM. We establish a theoretical guarantee for MIM-Reasoner as w
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20294;&#22914;&#20309;&#20248;&#21270;&#36873;&#25321;&#25968;&#25454;&#20197;&#38477;&#20302;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16827</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Data Selection for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16827
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20294;&#22914;&#20309;&#20248;&#21270;&#36873;&#25321;&#25968;&#25454;&#20197;&#38477;&#20302;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#30340;&#19968;&#20010;&#20027;&#35201;&#22240;&#32032;&#26159;&#21033;&#29992;&#24040;&#22823;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65288;&#25110;&#19981;&#21487;&#34892;&#65289;&#65292;&#22240;&#20026;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#36136;&#37327;&#21487;&#33021;&#26377;&#25152;&#19981;&#21516;&#12290;&#25968;&#25454;&#36807;&#28388;&#20063;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#37327;&#26469;&#38477;&#20302;&#35757;&#32451;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#12290;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26088;&#22312;&#30830;&#23450;&#35201;&#21253;&#25324;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#21738;&#20123;&#20505;&#36873;&#25968;&#25454;&#28857;&#65292;&#20197;&#21450;&#22914;&#20309;&#20174;&#25152;&#36873;&#25968;&#25454;&#28857;&#20013;&#36866;&#24403;&#37319;&#26679;&#12290;&#25913;&#36827;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#30340;&#21069;&#26223;&#24050;&#32463;&#23548;&#33268;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#37327;&#36805;&#36895;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#20027;&#35201;&#21463;&#23454;&#35777;&#35777;&#25454;&#39537;&#21160;&#65292;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#25104;&#26412;&#26114;&#36149;&#65292;&#24456;&#23569;&#26377;&#32452;&#32455;&#25317;&#26377;&#36164;&#28304;&#36827;&#34892;&#24191;&#27867;&#30340;&#25968;&#25454;&#36873;&#25321;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#25968;&#25454;&#36873;&#25321;&#30340;&#30693;&#35782;&#21487;&#33021;&#22823;&#22810;&#23616;&#38480;&#20110;&#22823;&#22411;&#25216;&#26415;&#20844;&#21496;&#25110;&#30740;&#31350;&#26426;&#26500;&#20869;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16827v1 Announce Type: new  Abstract: A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.   Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data se
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.16278</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#27880;&#37322;&#23884;&#20837;&#27169;&#22411;&#30340;&#26412;&#20307;&#21253;&#21547;&#20851;&#31995;&#39044;&#27979;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16278
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#23454;&#20307;&#30340;&#26412;&#20307;&#23884;&#20837;&#65292;&#29992;&#20110;&#26412;&#20307;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#26412;&#20307;&#23884;&#20837;&#26410;&#35299;&#20915;&#31867;&#20284;&#21644;&#23396;&#31435;&#23454;&#20307;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#26410;&#25552;&#21462;&#26412;&#20307;&#20013;&#27880;&#37322;&#20844;&#29702;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#30340;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65306;Inverted-index Matrix Embedding (InME) &#21644; Co-occurrence Matrix Embedding (CoME)&#12290;&#36825;&#20004;&#31181;&#23884;&#20837;&#36890;&#36807;&#27599;&#20010;&#21333;&#35789;&#22312;&#19968;&#32452;&#20844;&#29702;&#20013;&#20986;&#29616;&#30340;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#20844;&#29702;&#20013;&#21333;&#35789;&#30340;&#20849;&#29616;&#26469;&#25429;&#33719;&#27880;&#37322;&#20844;&#29702;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#12290;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#39044;&#27979;&#30340;&#36229;&#31867;&#19982;&#23376;&#31867;&#30456;&#20284;&#19988;&#23396;&#31435;&#20110;&#26412;&#20307;&#20013;&#30340;&#20854;&#20182;&#23454;&#20307;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#39057;&#35889;&#30340;&#26497;&#22823;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#22312;&#19968;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#26497;&#22823;&#32467;&#26524;&#65292;&#20197;&#21450;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23384;&#22312;&#20445;&#25345;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#65292;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14515</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#21644;&#26497;&#22823;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14515
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#39057;&#35889;&#30340;&#26497;&#22823;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#22312;&#19968;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#26497;&#22823;&#32467;&#26524;&#65292;&#20197;&#21450;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23384;&#22312;&#20445;&#25345;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#65292;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#28909;&#38376;&#26041;&#27861;&#65292;&#30001;&#20110;&#20854;&#19982;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#23494;&#20999;&#32852;&#31995;&#65292;&#20351;&#20854;&#25104;&#20026;&#22312;&#22122;&#22768;&#20013;&#38388;&#23610;&#24230;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;QNN&#21487;&#20197;&#34920;&#31034;&#20026;&#26377;&#38480;&#20613;&#37324;&#21494;&#32423;&#25968;&#65292;&#20854;&#20013;&#39057;&#29575;&#38598;&#34987;&#31216;&#20026;&#39057;&#35889;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#39057;&#35889;&#24182;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#22823;&#31867;&#27169;&#22411;&#65292;&#23384;&#22312;&#21508;&#31181;&#26497;&#22823;&#24615;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#19968;&#20123;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#20445;&#25345;&#39057;&#35889;&#30340;&#20855;&#26377;&#30456;&#21516;&#38754;&#31215;$A = RL$&#30340;&#27169;&#22411;&#31867;&#20043;&#38388;&#30340;&#21452;&#23556;&#65292;&#20854;&#20013;$R$&#34920;&#31034;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#65292;$L$&#34920;&#31034;&#23618;&#25968;&#65292;&#25105;&#20204;&#22240;&#27492;&#31216;&#20043;&#20026;&#38754;&#31215;&#20445;&#25345;&#21464;&#25442;&#19979;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#12290;&#36890;&#36807;&#36825;&#20010;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#30340;&#22312;&#32467;&#26524;&#20013;$R$&#21644;$L$&#30340;&#23545;&#31216;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#22823;&#39057;&#35889;&#30340;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14515v1 Announce Type: cross  Abstract: Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine Learning due to their close connection to Variational Quantum Circuits, making them a promising candidate for practical applications on Noisy Intermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finite Fourier series, where the set of frequencies is called the frequency spectrum. We analyse this frequency spectrum and prove, for a large class of models, various maximality results. Furthermore, we prove that under some mild conditions there exists a bijection between classes of models with the same area $A = RL$ that preserves the frequency spectrum, where $R$ denotes the number of qubits and $L$ the number of layers, which we consequently call spectral invariance under area-preserving transformations. With this we explain the symmetry in $R$ and $L$ in the results often observed in the literature and show that the maximal frequency spectrum depen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#23450;&#21046;&#28151;&#21512;&#31934;&#24230;&#20302;&#20110;8&#20301;&#37327;&#21270;&#26041;&#26696;&#65292;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#23610;&#23544;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;&#22235;&#20010;&#19981;&#21516;&#39034;&#24207;&#20219;&#21153;&#19978;&#23637;&#31034;&#28151;&#21512;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#21516;&#36136;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#23610;&#23544;&#32553;&#20943;25%&#33267;55%&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12263</link><description>&lt;p&gt;
&#38754;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#23450;&#21046;&#28151;&#21512;&#31934;&#24230;&#20302;&#20110;8&#20301;&#37327;&#21270;&#26041;&#26696;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12263
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#23450;&#21046;&#28151;&#21512;&#31934;&#24230;&#20302;&#20110;8&#20301;&#37327;&#21270;&#26041;&#26696;&#65292;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#23610;&#23544;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;&#22235;&#20010;&#19981;&#21516;&#39034;&#24207;&#20219;&#21153;&#19978;&#23637;&#31034;&#28151;&#21512;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#21516;&#36136;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#23610;&#23544;&#32553;&#20943;25%&#33267;55%&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#36229;&#20302;&#21151;&#32791;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#26159;&#38024;&#23545;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#30340;&#37327;&#21270;&#26041;&#26696;&#24456;&#38590;&#35843;&#25972;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#20869;&#37096;&#29366;&#24577;&#65292;&#26080;&#27861;&#20805;&#20998;&#20174;&#20302;&#20110;8&#20301;&#30340;&#37327;&#21270;&#20013;&#33719;&#30410;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#25972;&#25968;&#37327;&#21270;&#26041;&#26696;&#65292;&#20854;&#20013;&#27599;&#20010;&#36816;&#31639;&#31526;&#30340;&#20301;&#23485;&#21487;&#20197;&#29420;&#31435;&#36873;&#25321;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#26469;&#25506;&#32034;&#21487;&#33021;&#20301;&#23485;&#30340;&#24222;&#22823;&#25628;&#32034;&#31354;&#38388;&#65292;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#23610;&#23544;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#39034;&#24207;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#28151;&#21512;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#22312;Pareto&#25928;&#29575;&#26041;&#38754;&#36229;&#36807;&#21516;&#36136;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27169;&#22411;&#23610;&#23544;&#22312;25%&#33267;55%&#20043;&#38388;&#30340;&#32553;&#20943;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;t&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12263v1 Announce Type: new  Abstract: Despite the recent advances in model compression techniques for deep neural networks, deploying such models on ultra-low-power embedded devices still proves challenging. In particular, quantization schemes for Gated Recurrent Units (GRU) are difficult to tune due to their dependence on an internal state, preventing them from fully benefiting from sub-8bit quantization. In this work, we propose a modular integer quantization scheme for GRUs where the bit width of each operator can be selected independently. We then employ Genetic Algorithms (GA) to explore the vast search space of possible bit widths, simultaneously optimising for model size and accuracy. We evaluate our methods on four different sequential tasks and demonstrate that mixed-precision solutions exceed homogeneous-precision ones in terms of Pareto efficiency. In our results, we achieve a model size reduction between 25% and 55% while maintaining an accuracy comparable with t
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.11816</link><description>&lt;p&gt;
&#36991;&#20813;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#65306;&#23398;&#20064;&#20197;&#21069;&#26410;&#26366;&#23398;&#21040;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11816
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#65288;&#22914;SimCLR&#12289;CLIP&#20013;&#65289;&#20013;&#21457;&#29616;&#20102;&#29305;&#24449;&#25233;&#21046;&#65306;&#22312;&#21333;&#20010;&#31471;&#21040;&#31471;&#35757;&#32451;&#38454;&#27573;&#65292;&#23545;&#27604;&#27169;&#22411;&#20165;&#25429;&#33719;&#23545;&#27604;&#35266;&#28857;&#20043;&#38388;&#30340;&#19968;&#37096;&#20998;&#20849;&#20139;&#20449;&#24687;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#28508;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#20855;&#26377;&#29305;&#24449;&#25233;&#21046;&#65292;&#23545;&#27604;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#23398;&#20064;&#36275;&#22815;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#20943;&#36731;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#24182;&#30830;&#20445;&#23545;&#27604;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#12290;&#19982;&#36890;&#24120;&#20250;&#23548;&#33268;&#29305;&#24449;&#25233;&#21046;&#30340;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#19981;&#21516;&#65292;MCL&#36880;&#28176;&#23398;&#20064;&#20197;&#21069;&#26410;&#25506;&#32034;&#36807;&#30340;&#26032;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#24050;&#32463;&#23398;&#21040;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11816v1 Announce Type: cross  Abstract: Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.11140</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#25552;&#21319;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35797;&#38169;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#24615;&#33021;&#20851;&#38190;&#21462;&#20915;&#20110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#19968;&#20123;&#24605;&#32500;&#38142;&#31034;&#33539;&#20316;&#20026;&#31034;&#20363;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;&#20363;&#22914;Thought Tree&#65289;&#25351;&#20986;&#20102;&#22312;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#25512;&#29702;&#27493;&#39588;&#36873;&#25321;&#20013;&#65292;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#35768;&#22810;&#24605;&#32500;&#26641;&#26469;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#36825;&#23558;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;BoT&#20174;&#19968;&#20010;&#31616;&#21333;&#25552;&#31034;&#24320;&#22987;&#65292;&#26080;&#38656;&#31034;&#20363;&#65292;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#35780;&#20272;&#22823;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#21033;&#29992;LLM&#33719;&#24471;&#30340;&#38169;&#35823;&#20998;&#26512;&#26469;&#26126;&#30830;&#20462;&#25913;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11140v1 Announce Type: new  Abstract: The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompt
&lt;/p&gt;</description></item><item><title>&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11078</link><description>&lt;p&gt;
&#36890;&#36807;&#32431;&#24494;&#35843;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Model Editing by Pure Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11078
&lt;/p&gt;
&lt;p&gt;
&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#35843;&#25972;&#34987;&#35748;&#20026;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#19981;&#22815;&#26377;&#25928;&#65292;&#22240;&#20026;&#30456;&#23545;&#26356;&#19987;&#19994;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#26159;&#31616;&#21333;&#30340;&#65292;&#19981;&#20851;&#24515;&#34987;&#32534;&#36753;&#27169;&#22411;&#30340;&#20307;&#31995;&#32467;&#26500;&#32454;&#33410;&#65292;&#24182;&#19988;&#33021;&#22815;&#21033;&#29992;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#30340;&#19981;&#26029;&#36827;&#23637;&#65288;&#20363;&#22914;PEFT&#65289;&#65292;&#20351;&#20854;&#25104;&#20026;&#27169;&#22411;&#32534;&#36753;&#22120;&#30340;&#21560;&#24341;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#24494;&#35843;&#21487;&#20197;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26420;&#32032;&#24494;&#35843;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#32780;&#38750;&#23436;&#25972;&#20284;&#28982;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#26469;&#22686;&#21152;&#25968;&#25454;&#65292;&#20197;&#40723;&#21169;&#27867;&#21270;&#21644;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#22312;ZsRE&#21644;CounterFact&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#19968;&#31616;&#21333;&#20462;&#25913;&#20351;&#24471;&#24494;&#35843;&#36890;&#24120;&#21487;&#20197;&#19982;&#19987;&#19994;&#32534;&#36753;&#22120;&#22312;&#32534;&#36753;&#20998;&#25968;&#26041;&#38754;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#20013;&#36793;&#35299;&#38500;&#36807;&#31243;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10695</link><description>&lt;p&gt;
&#19982;&#36951;&#24536;&#21628;&#24212;&#30340;&#35299;&#38500;&#38142;&#25509;&#65306;&#31616;&#21270;GNN&#20013;&#30340;&#36793;&#35299;&#38500;
&lt;/p&gt;
&lt;p&gt;
Unlink to Unlearn: Simplifying Edge Unlearning in GNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10695
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#20013;&#36793;&#35299;&#38500;&#36807;&#31243;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#21152;&#21095;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#35299;&#38500;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#23398;&#26415;&#30028;&#19968;&#20010;&#31361;&#20986;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;&#36825;&#19968;&#27010;&#24565;&#22312;&#24378;&#35843;&#34987;&#36951;&#24536;&#26435;&#21033;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#22312;&#29992;&#25143;&#35831;&#27714;&#26102;&#26377;&#36873;&#25321;&#24615;&#22320;&#20174;&#24050;&#35757;&#32451;&#30340;GNN&#20013;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#36793;&#30340;&#35299;&#38500;&#23398;&#20064;&#65292;&#36825;&#19968;&#36807;&#31243;&#23545;&#29616;&#23454;&#24212;&#29992;&#29305;&#21035;&#30456;&#20851;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22914;GNNDelete&#21487;&#20197;&#28040;&#38500;&#29305;&#23450;&#36793;&#30340;&#24433;&#21709;&#65292;&#28982;&#32780;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#23616;&#38480;&#65292;&#31216;&#20026;&#36807;&#24230;&#36951;&#24536;&#12290;&#24403;&#35299;&#38500;&#23398;&#20064;&#36807;&#31243;&#26080;&#24847;&#20013;&#38500;&#21435;&#36229;&#20986;&#29305;&#23450;&#25968;&#25454;&#30340;&#36807;&#22810;&#20449;&#24687;&#26102;&#65292;&#20250;&#23548;&#33268;&#23545;&#21097;&#20313;&#36793;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;GNNDelete&#30340;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10695v1 Announce Type: cross  Abstract: As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#30701;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#25233;&#37057;&#24433;&#21709;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20854;&#24433;&#21709;&#24182;&#37319;&#21462;&#30456;&#24212;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2402.10045</link><description>&lt;p&gt;
&#30701;&#35270;&#39057;&#21644;&#24515;&#29702;&#20581;&#24247;&#65306;&#22522;&#20110;&#30693;&#35782;&#23548;&#21521;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal Neural Topic Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10045
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#30701;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#25233;&#37057;&#24433;&#21709;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20854;&#24433;&#21709;&#24182;&#37319;&#21462;&#30456;&#24212;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35270;&#39057;&#27491;&#35797;&#22270;&#37325;&#26032;&#22609;&#36896;&#25972;&#20010;&#31038;&#20132;&#23186;&#20307;&#26223;&#35266;&#65292;&#28982;&#32780;&#19987;&#23478;&#20204;&#23545;&#20854;&#23545;&#35266;&#20247;&#30340;&#25233;&#37057;&#24433;&#21709;&#24863;&#21040;&#26497;&#24230;&#25285;&#24551;&#65292;&#36825;&#19968;&#28857;&#24050;&#30001;&#21307;&#23398;&#30740;&#31350;&#35777;&#26126;&#12290;&#20026;&#20102;&#38450;&#27490;&#24191;&#27867;&#24433;&#21709;&#65292;&#21508;&#24179;&#21488;&#28212;&#26395;&#39044;&#27979;&#36825;&#20123;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#37319;&#21462;&#24178;&#39044;&#25514;&#26045;&#65292;&#27604;&#22914;&#20462;&#35746;&#25512;&#33616;&#31639;&#27861;&#21644;&#26174;&#31034;&#35266;&#20247;&#24910;&#37325;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#32570;&#20047;&#19982;&#25233;&#37057;&#30151;&#30340;&#20020;&#24202;&#35777;&#23454;&#30340;&#22806;&#37096;&#29615;&#22659;&#22240;&#32032;&#30456;&#20851;&#30340;&#21307;&#23398;&#30693;&#35782;&#12290;&#20026;&#20102;&#32771;&#34385;&#36825;&#26679;&#30340;&#21307;&#23398;&#30693;&#35782;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#26041;&#27861;&#35770;&#23398;&#31185;&#8212;&#8212;&#31181;&#23376;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTMs&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31181;&#23376;NTMs&#23384;&#22312;&#21333;&#19968;&#26469;&#28304;&#20027;&#39064;&#12289;&#26410;&#30693;&#20027;&#39064;&#26469;&#28304;&#12289;&#27169;&#31946;&#30340;&#31181;&#23376;&#30417;&#30563;&#21644;&#27425;&#20248;&#30340;&#25910;&#25947;&#31561;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30693;&#35782;&#25351;&#23548;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;Knowledg...&#65288;&#24453;&#34917;&#20805;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10045v1 Announce Type: cross  Abstract: While short-form videos head to reshape the entire social media landscape, experts are exceedingly worried about their depressive impacts on viewers, as evidenced by medical studies. To prevent widespread consequences, platforms are eager to predict these videos' impact on viewers' mental health. Subsequently, they can take intervention measures, such as revising recommendation algorithms and displaying viewer discretion. Nevertheless, applicable predictive methods lack relevance to well-established medical knowledge, which outlines clinically proven external and environmental factors of depression. To account for such medical knowledge, we resort to an emergent methodological discipline, seeded Neural Topic Models (NTMs). However, existing seeded NTMs suffer from the limitations of single-origin topics, unknown topic sources, unclear seed supervision, and suboptimal convergence. To address those challenges, we develop a novel Knowledg
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#31232;&#30095;&#35299;&#37322;&#20540;(SEV)&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;&#21363;&#20351;&#27169;&#22411;&#19981;&#26159;&#31232;&#30095;&#30340;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;SEV&#30340;&#34913;&#37327;&#19979;&#20173;&#20855;&#26377;&#20302;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09702</link><description>&lt;p&gt;
&#26080;&#38656;&#31232;&#30095;&#27169;&#22411;&#30340;&#31232;&#30095;&#19988;&#20934;&#30830;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Sparse and Faithful Explanations Without Sparse Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09702
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#31232;&#30095;&#35299;&#37322;&#20540;(SEV)&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;&#21363;&#20351;&#27169;&#22411;&#19981;&#26159;&#31232;&#30095;&#30340;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;SEV&#30340;&#34913;&#37327;&#19979;&#20173;&#20855;&#26377;&#20302;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#27169;&#22411;&#19981;&#28385;&#36275;&#20840;&#23616;&#30340;&#31232;&#30095;&#24615;&#65292;&#20915;&#31574;&#20173;&#28982;&#21487;&#20197;&#29992;&#23569;&#37327;&#30340;&#29305;&#24449;&#20934;&#30830;&#22320;&#25551;&#36848;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#26576;&#20154;&#32780;&#35328;&#65292;&#23613;&#31649;&#27809;&#26377;&#20449;&#29992;&#21382;&#21490;&#65292;&#20294;&#30003;&#35831;&#22823;&#31508;&#36151;&#27454;&#21487;&#33021;&#20250;&#34987;&#25298;&#32477;&#65292;&#36825;&#23601;&#24573;&#35270;&#20102;&#19982;&#20854;&#20449;&#29992;&#20215;&#20540;&#30456;&#20851;&#30340;&#20219;&#20309;&#35777;&#25454;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#35299;&#37322;&#20540;&#65288;SEV&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31232;&#30095;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20197;&#19978;&#36151;&#27454;&#25298;&#32477;&#30340;&#20363;&#23376;&#20013;&#65292;SEV&#20026;1&#65292;&#22240;&#20026;&#21482;&#38656;&#35201;&#19968;&#20010;&#22240;&#32032;&#26469;&#35299;&#37322;&#20026;&#20160;&#20040;&#36151;&#27454;&#34987;&#25298;&#32477;&#12290;SEV&#26159;&#23545;&#20915;&#31574;&#31232;&#30095;&#24615;&#30340;&#34913;&#37327;&#65292;&#32780;&#19981;&#26159;&#23545;&#25972;&#20307;&#27169;&#22411;&#31232;&#30095;&#24615;&#30340;&#34913;&#37327;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21363;&#20351;&#23427;&#20204;&#19981;&#26159;&#31232;&#30095;&#30340;&#8212;&#8212;&#23454;&#38469;&#19978;&#22312;SEV&#30340;&#34913;&#37327;&#19979;&#20855;&#26377;&#20302;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;SEV&#20351;&#29992;&#36229;&#31435;&#26041;&#20307;&#19978;&#30340;&#31227;&#21160;&#36827;&#34892;&#23450;&#20041;&#65292;&#20351;&#24471;SEV&#33021;&#22815;&#22312;&#21508;&#31181;&#27169;&#22411;&#31867;&#21035;&#19978;&#19968;&#33268;&#22320;&#23450;&#20041;&#65292;&#20854;&#20013;&#31227;&#21160;&#38480;&#21046;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09702v1 Announce Type: new  Abstract: Even if a model is not globally sparse, it is possible for decisions made from that model to be accurately and faithfully described by a small number of features. For instance, an application for a large loan might be denied to someone because they have no credit history, which overwhelms any evidence towards their creditworthiness. In this work, we introduce the Sparse Explanation Value (SEV), a new way of measuring sparsity in machine learning models. In the loan denial example above, the SEV is 1 because only one factor is needed to explain why the loan was denied. SEV is a measure of decision sparsity rather than overall model sparsity, and we are able to show that many machine learning models -- even if they are not sparse -- actually have low decision sparsity, as measured by SEV. SEV is defined using movements over a hypercube, allowing SEV to be defined consistently over various model classes, with movement restrictions reflectin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09450</link><description>&lt;p&gt;
&#24341;&#23548;&#36974;&#34109;&#34920;&#31034;&#23398;&#20064;&#20197;&#25429;&#25417;&#24515;&#30005;&#22270;&#30340;&#26102;&#31354;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#24191;&#27867;&#29992;&#20316;&#30417;&#27979;&#24515;&#33039;&#36215;&#28304;&#30340;&#30005;&#20449;&#21495;&#30340;&#35786;&#26029;&#24037;&#20855;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#20449;&#21495;&#36827;&#34892;&#21508;&#31181;&#30142;&#30149;&#31579;&#26597;&#30340;&#24212;&#29992;&#19978;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#30142;&#30149;&#31579;&#26597;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;ECG&#25968;&#25454;&#26377;&#38480;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23454;&#29616;&#36890;&#29992;&#34920;&#31034;&#26159;&#20811;&#26381;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#24120;&#29992;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22312;ECG&#25968;&#25454;&#19978;&#32431;&#31929;&#24212;&#29992;SSL&#65292;&#32780;&#19981;&#32771;&#34385;ECG&#20449;&#21495;&#22266;&#26377;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ST-MEM&#65288;&#26102;&#31354;&#36974;&#34109;&#24515;&#30005;&#22270;&#24314;&#27169;&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;12&#23548;&#32852;ECG&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;ST-MEM&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;SSL&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09450v1 Announce Type: cross  Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening disease is challenging in that labeled ECG data are limited. Achieving general representation through self-supervised learning (SSL) is a well-known approach to overcome the scarcity of labeled data; however, a naive application of SSL to ECG data, without considering the spatial-temporal relationships inherent in ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM (Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM outperforms other SSL baseline methods in various experimental settings for arrhythmia classification tasks. Mo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#23454;&#29616;&#23545;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05210</link><description>&lt;p&gt;
&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#23454;&#29616;&#23545;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23454;&#29616;&#20102;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#20026;&#23567;&#22411;&#25110;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#34917;&#20805;&#65292;&#20174;&#32780;&#24110;&#21161;&#20943;&#36731;&#33719;&#21462;&#21644;&#27880;&#37322;&#26032;&#22270;&#20687;&#30340;&#36153;&#29992;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#20687;&#26102;&#38754;&#20020;&#30528;&#20840;&#23616;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#21487;&#25511;&#30340;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#20013;&#36981;&#24490;&#22810;&#31867;&#35299;&#21078;&#20998;&#21106;&#25513;&#27169;&#65292;&#24182;&#37319;&#29992;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#25152;&#36873;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#20801;&#35768;&#20854;&#20182;&#35299;&#21078;&#21306;&#22495;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20063;&#25913;&#21892;&#20102;&#32593;&#32476;&#22312;&#23436;&#20840;&#26080;&#26465;&#20214;&#65288;&#26080;&#32422;&#26463;&#29983;&#25104;&#65289;&#24773;&#20917;&#19979;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#20083;&#33146;MRI&#21644;&#33145;&#37096;/&#39048;&#37096;&#21040;&#30406;&#33108;CT&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#35299;&#21078;&#30495;&#23454;&#24615;&#21644;&#36755;&#20837;&#25513;&#27169;&#20445;&#30495;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have enabled remarkably high-quality medical image generation, which can help mitigate the expenses of acquiring and annotating new images by supplementing small or imbalanced datasets, along with other applications. However, these are hampered by the challenge of enforcing global anatomical realism in generated images. To this end, we propose a diffusion model for anatomically-controlled medical image generation. Our model follows a multi-class anatomical segmentation mask at each sampling step and incorporates a \textit{random mask ablation} training algorithm, to enable conditioning on a selected combination of anatomical constraints while allowing flexibility in other anatomical areas. This also improves the network's learning of anatomical realism for the completely unconditional (unconstrained generation) case. Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets demonstrates superior anatomical realism and input mask faithfulness over st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#22823;&#23610;&#24230;&#21644;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#36817;&#20284;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05067</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#23610;&#24230;&#24314;&#27169;&#65306;&#20174;&#22797;&#26434;&#31995;&#32479;&#30340;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#21040;&#23567;&#23610;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#22823;&#23610;&#24230;&#21644;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#36817;&#20284;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#29616;&#35937;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#23545;&#20110;&#20934;&#30830;&#26377;&#25928;&#22320;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#22810;&#23610;&#24230;&#21160;&#21147;&#23398;&#25552;&#20986;&#20102;&#26222;&#36941;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#32806;&#26041;&#27861;&#23545;&#22810;&#23610;&#24230;&#21160;&#21147;&#23398;&#36827;&#34892;&#34920;&#24449;&#30340;&#26032;&#30340;&#27714;&#35299;&#27169;&#24335;&#12290;&#36890;&#36807;&#29420;&#31435;&#22320;&#24314;&#27169;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#35270;&#20026;&#20174;&#23646;&#31995;&#32479;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35889;PINN&#26041;&#27861;&#65292;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#25509;&#36817;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#21253;&#25324;&#19968;&#32500;Kuramot-Sivashinsky (KS)&#26041;&#31243;&#12289;&#20108;&#32500;&#21644;&#19977;&#32500;Navier-Stokes (NS)&#26041;&#31243;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#26356;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#38750;&#22343;&#21248;&#32593;&#26684;&#12289;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#12289;&#24102;&#22122;&#22768;&#30340;&#22823;&#23610;&#24230;&#25968;&#25454;&#21644;&#39640;&#32500;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems. In this paper, a novel solving mode is proposed for characterizing multiscale dynamics through a decoupling method. By modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system, a Spectral PINN is developed to approach the small-scale system in an orthogonal basis functional space. The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional Navier-Stokes (NS) equations, showcasing its versatility in addressing problems of fluid dynamics. Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;aLLM4TS&#26694;&#26550;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#22810;&#22359;&#39044;&#27979;&#20219;&#21153;&#65292;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24494;&#35843;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04852</link><description>&lt;p&gt;
&#22810;&#22359;&#39044;&#27979;&#65306;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;LLMs&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;aLLM4TS&#26694;&#26550;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#22810;&#22359;&#39044;&#27979;&#20219;&#21153;&#65292;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24494;&#35843;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;aLLM4TS&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#37325;&#26032;&#26500;&#24819;&#20026;&#19968;&#39033;&#33258;&#30417;&#30563;&#30340;&#22810;&#22359;&#39044;&#27979;&#20219;&#21153;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#25513;&#30721;&#21644;&#37325;&#26500;&#26041;&#27861;&#65292;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#22359;&#34920;&#31034;&#20013;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#65306;(i) &#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22240;&#26524;&#36830;&#32493;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#20197;&#19979;&#19968;&#20010;&#22359;&#39044;&#27979;&#20026;&#38170;&#28857;&#65292;&#26377;&#25928;&#22320;&#23558;LLM&#30340;&#33021;&#21147;&#19982;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21516;&#27493;&#12290;(ii) &#22312;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#19978;&#36827;&#34892;&#22810;&#22359;&#39044;&#27979;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#29420;&#29305;&#35201;&#32032;&#26159;&#22359;&#32423;&#35299;&#30721;&#23618;&#65292;&#19981;&#21516;&#20110;&#20043;&#21069;&#20381;&#36182;&#20110;&#24207;&#21015;&#32423;&#35299;&#30721;&#30340;&#26041;&#27861;&#12290;&#36825;&#26679;&#30340;&#35774;&#35745;&#30452;&#25509;&#23558;&#21333;&#20010;&#22359;&#36716;&#25442;&#20026;&#26102;&#38388;&#24207;&#21015;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#25513;&#34109;&#20219;&#21153;&#19979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mast
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04676</link><description>&lt;p&gt;
&#24102;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#20998;&#32452;&#20998;&#24067;&#40065;&#26834;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Group Distributionally Robust Dataset Distillation with Risk Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;DD&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#25429;&#25417;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20449;&#24687;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#20174;&#32780;&#26041;&#20415;&#20934;&#30830;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#12290;&#20854;&#24212;&#29992;&#28085;&#30422;&#20102;&#36716;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#26159;&#23558;&#35757;&#32451;&#25968;&#25454;&#38598;&#35270;&#20026;&#36741;&#21161;&#65292;&#23601;&#20687;&#35757;&#32451;&#38598;&#26159;&#20154;&#21475;&#20998;&#24067;&#30340;&#36817;&#20284;&#26367;&#20195;&#21697;&#19968;&#26679;&#65292;&#32780;&#21518;&#32773;&#25165;&#26159;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#20854;&#21463;&#27426;&#36814;&#31243;&#24230;&#24456;&#39640;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;DD&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#36328;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24403;&#38754;&#23545;&#26469;&#33258;&#32597;&#35265;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#30830;&#20445;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;Moreau&#21253;&#32476;&#26469;&#23545;&#27979;&#24230;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;Wasserstein&#26799;&#24230;&#27969;&#12290;</title><link>https://arxiv.org/abs/2402.04613</link><description>&lt;p&gt;
&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;Moreau&#21253;&#32476;&#30340;f-&#24046;&#24322;&#30340;Wasserstein&#26799;&#24230;&#27969;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in Reproducing Kernel Hilbert Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;Moreau&#21253;&#32476;&#26469;&#23545;&#27979;&#24230;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;Wasserstein&#26799;&#24230;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#27979;&#24230;f-&#24046;&#24322;&#65292;&#20363;&#22914;Kullback-Leibler&#24046;&#24322;&#65292;&#23545;&#20110;&#25152;&#28041;&#21450;&#30340;&#27979;&#24230;&#30340;&#25903;&#25345;&#23384;&#22312;&#38480;&#21046;&#12290;&#35299;&#20915;&#21150;&#27861;&#26159;&#36890;&#36807;&#19982;&#29305;&#24449;&#26680;K&#30456;&#20851;&#30340;&#24179;&#26041;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;(MMD)&#23545;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25152;&#35859;&#30340;&#26680;&#22343;&#20540;&#23884;&#20837;&#26469;&#26174;&#31034;&#30456;&#24212;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#37325;&#20889;&#20026;&#19982;K&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#26576;&#20123;&#20989;&#25968;&#30340;Moreau&#21253;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20851;&#20110;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;Moreau&#21253;&#32476;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#32467;&#26524;&#26469;&#35777;&#26126;MMD&#27491;&#21017;&#21270;&#30340;f-&#24046;&#24322;&#21450;&#20854;&#26799;&#24230;&#30340;&#23646;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26469;&#20998;&#26512;&#21463;MMD&#27491;&#21017;&#21270;&#30340;f-&#24046;&#24322;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#32463;&#39564;&#27979;&#24230;&#24320;&#22987;&#30340;Wasserstein&#26799;&#24230;&#27969;&#65292;&#24182;&#25552;&#20379;&#20351;&#29992;Tsallis-$\alpha$&#24046;&#24322;&#30340;&#27010;&#24565;&#24615;&#25968;&#20540;&#31034;&#20363;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most commonly used $f$-divergences of measures, e.g., the Kullback-Leibler divergence, are subject to limitations regarding the support of the involved measures. A remedy consists of regularizing the $f$-divergence by a squared maximum mean discrepancy (MMD) associated with a characteristic kernel $K$. In this paper, we use the so-called kernel mean embedding to show that the corresponding regularization can be rewritten as the Moreau envelope of some function in the reproducing kernel Hilbert space associated with $K$. Then, we exploit well-known results on Moreau envelopes in Hilbert spaces to prove properties of the MMD-regularized $f$-divergences and, in particular, their gradients. Subsequently, we use our findings to analyze Wasserstein gradient flows of MMD-regularized $f$-divergences. Finally, we consider Wasserstein gradient flows starting from empirical measures and provide proof-of-the-concept numerical examples with Tsallis-$\alpha$ divergences.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#22330;&#26223;&#30340;&#26080;&#31034;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#33539;&#29305;&#24449;&#28418;&#31227;&#24182;&#21033;&#29992;&#21407;&#22411;&#26469;&#20943;&#23569;&#20219;&#21153;&#26032;&#40092;&#24230;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.03917</link><description>&lt;p&gt;
&#20919;&#21551;&#21160;&#26080;&#31034;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03917
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#22330;&#26223;&#30340;&#26080;&#31034;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#33539;&#29305;&#24449;&#28418;&#31227;&#24182;&#21033;&#29992;&#21407;&#22411;&#26469;&#20943;&#23569;&#20219;&#21153;&#26032;&#40092;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#31034;&#20363;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;EFCIL&#65289;&#26088;&#22312;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20808;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#20013;&#27809;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#39592;&#24178;&#32593;&#32476;&#12290;&#23545;&#20110;EFCIL&#26469;&#35828;&#65292;&#36825;&#26159;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39640;&#24230;&#30340;&#21487;&#22609;&#24615;&#65292;&#36825;&#20250;&#23548;&#33268;&#29305;&#24449;&#28418;&#31227;&#65292;&#22312;&#26080;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#36827;&#34892;&#34917;&#20607;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#33539;&#22312;&#19982;&#20808;&#21069;&#20219;&#21153;&#39640;&#24230;&#30456;&#20851;&#30340;&#26041;&#21521;&#19978;&#30340;&#28418;&#31227;&#65292;&#24182;&#21033;&#29992;&#21407;&#22411;&#26469;&#20943;&#23569;&#20219;&#21153;&#26032;&#40092;&#24230;&#20559;&#24046;&#65292;&#20197;&#25972;&#21512;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;&#65288;EFC&#65289;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#32463;&#39564;&#29305;&#24449;&#30697;&#38453;&#65288;EFM&#65289;&#30340;&#21487;&#35299;&#20108;&#38454;&#36817;&#20284;&#26469;&#22788;&#29702;&#29305;&#24449;&#28418;&#31227;&#12290;EFM&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#20266;&#24230;&#37327;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#35268;&#33539;&#37325;&#35201;&#26041;&#21521;&#19978;&#30340;&#29305;&#24449;&#28418;&#31227;&#65292;&#24182;&#26356;&#26032;&#39640;&#26031;&#21407;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20999;&#29255;Wasserstein Weisfeiler-Lehman&#22270;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#31232;&#30095;&#22270;&#24418;&#25968;&#25454;&#38598;&#26102;&#20855;&#26377;&#27491;&#23450;&#24615;&#21644;&#26174;&#33879;&#30340;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.03838</link><description>&lt;p&gt;
&#24102;&#26377;&#20999;&#29255;Wasserstein Weisfeiler-Lehman&#22270;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20999;&#29255;Wasserstein Weisfeiler-Lehman&#22270;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#31232;&#30095;&#22270;&#24418;&#25968;&#25454;&#38598;&#26102;&#20855;&#26377;&#27491;&#23450;&#24615;&#21644;&#26174;&#33879;&#30340;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#29289;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#21462;&#22797;&#26434;&#27169;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#25110;&#39044;&#27979;&#26448;&#26009;&#24615;&#36136;&#31561;&#20219;&#21153;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#31867;&#25968;&#25454;&#38598;&#30001;&#20855;&#26377;&#22823;&#37327;&#33410;&#28857;&#30340;&#32593;&#26684;&#34920;&#31034;&#30340;&#36755;&#20837;&#65288;&#35270;&#20026;&#22270;&#24418;&#65289;&#21644;&#20351;&#29992;&#25968;&#20540;&#27714;&#35299;&#22120;&#33719;&#24471;&#30340;&#30456;&#24212;&#36755;&#20986;&#32452;&#25104;&#12290;&#36825;&#24847;&#21619;&#30528;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24517;&#39035;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36830;&#32493;&#33410;&#28857;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#31232;&#30095;&#22270;&#24418;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#24341;&#20837;&#20102;&#20999;&#29255;Wasserstein Weisfeiler-Lehman&#65288;SWWL&#65289;&#22270;&#26680;&#12290;&#19982;&#29616;&#26377;&#30340;&#22270;&#26680;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;SWWL&#26680;&#20855;&#26377;&#27491;&#23450;&#24615;&#21644;&#26174;&#33879;&#30340;&#22797;&#26434;&#24230;&#38477;&#20302;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#27492;&#21069;&#19981;&#21487;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#12290;&#26032;&#30340;&#26680;&#39318;&#20808;&#22312;&#20998;&#23376;&#22270;&#20998;&#31867;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning has recently garnered significant attention in the field of computational physics due to its ability to effectively extract complex patterns for tasks like solving partial differential equations, or predicting material properties. Traditionally, such datasets consist of inputs given as meshes with a large number of nodes representing the problem geometry (seen as graphs), and corresponding outputs obtained with a numerical solver. This means the supervised learning model must be able to handle large and sparse graphs with continuous node attributes. In this work, we focus on Gaussian process regression, for which we introduce the Sliced Wasserstein Weisfeiler-Lehman (SWWL) graph kernel. In contrast to existing graph kernels, the proposed SWWL kernel enjoys positive definiteness and a drastic complexity reduction, which  makes it possible to process datasets that were previously impossible to handle. The new kernel is first validated on graph classification for molec
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#23398;&#20064;&#32773;&#29615;&#22659;&#20013;&#65292;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#29992;&#25143;&#22312;&#22810;&#20010;&#21487;&#29992;&#26381;&#21153;&#20013;&#36827;&#34892;&#36873;&#25321;&#30340;&#25112;&#30053;&#34892;&#20026;&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2401.16422</link><description>&lt;p&gt;
&#22312;&#22810;&#23398;&#20064;&#32773;&#29615;&#22659;&#20013;&#30340;&#25112;&#30053;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Strategic Usage in a Multi-Learner Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16422
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#23398;&#20064;&#32773;&#29615;&#22659;&#20013;&#65292;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#29992;&#25143;&#22312;&#22810;&#20010;&#21487;&#29992;&#26381;&#21153;&#20013;&#36827;&#34892;&#36873;&#25321;&#30340;&#25112;&#30053;&#34892;&#20026;&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#36890;&#24120;&#28041;&#21450;&#19968;&#20123;&#29992;&#25143;&#22312;&#19968;&#31995;&#21015;&#26381;&#21153;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#12290;&#38543;&#30528;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#27969;&#34892;&#65292;&#36825;&#20123;&#26381;&#21153;&#29616;&#22312;&#21487;&#20197;&#33258;&#25105;&#20248;&#21270;&#65292;&#21033;&#29992;&#23545;&#29992;&#25143;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#26368;&#22823;&#21270;&#26576;&#31181;&#22870;&#21169;&#65292;&#22914;&#26381;&#21153;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#25112;&#30053;&#24615;&#22320;&#36873;&#25321;&#35201;&#20351;&#29992;&#30340;&#26381;&#21153;&#65292;&#20197;&#36861;&#27714;&#20182;&#20204;&#33258;&#24049;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#23545;&#21738;&#20123;&#26381;&#21153;&#33021;&#22815;&#30475;&#21040;&#24182;&#20351;&#29992;&#20182;&#20204;&#30340;&#25968;&#25454;&#25317;&#26377;&#26435;&#21147;&#12290;&#20808;&#21069;&#24050;&#23545;&#21333;&#26381;&#21153;&#35774;&#32622;&#20013;&#25112;&#30053;&#29992;&#25143;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25112;&#30053;&#34892;&#20026;&#20307;&#29616;&#22312;&#25805;&#32437;&#21487;&#35266;&#23519;&#29305;&#24449;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#20998;&#31867;&#19978;&#65307;&#28982;&#32780;&#65292;&#36825;&#23545;&#29992;&#25143;&#26469;&#35828;&#24448;&#24448;&#20250;&#24456;&#26114;&#36149;&#25110;&#38590;&#20197;&#23454;&#29616;&#65292;&#24182;&#19988;&#26410;&#33021;&#25429;&#25417;&#21040;&#22810;&#26381;&#21153;&#21160;&#24577;&#31995;&#32479;&#30340;&#20840;&#37096;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;&#20854;&#20013;&#25112;&#30053;&#29992;&#25143;&#22312;&#20960;&#20010;&#21487;&#29992;&#26381;&#21153;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#20197;&#36861;&#27714;&#31215;&#26497;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16422v2 Announce Type: replace  Abstract: Real-world systems often involve some pool of users choosing between a set of services. With the increase in popularity of online learning algorithms, these services can now self-optimize, leveraging data collected on users to maximize some reward such as service quality. On the flipside, users may strategically choose which services to use in order to pursue their own reward functions, in the process wielding power over which services can see and use their data. Extensive prior research has been conducted on the effects of strategic users in single-service settings, with strategic behavior manifesting in the manipulation of observable features to achieve a desired classification; however, this can often be costly or unattainable for users and fails to capture the full behavior of multi-service dynamic systems. As such, we analyze a setting in which strategic users choose among several available services in order to pursue positive c
&lt;/p&gt;</description></item><item><title>SemPLeS&#26694;&#26550;&#21033;&#29992;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#25552;&#31034;&#26469;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2401.11791</link><description>&lt;p&gt;
SemPLeS: &#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11791
&lt;/p&gt;
&lt;p&gt;
SemPLeS&#26694;&#26550;&#21033;&#29992;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#25552;&#31034;&#26469;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#26088;&#22312;&#21033;&#29992;&#20165;&#20855;&#26377;&#22270;&#20687;&#32423;&#30417;&#30563;&#30340;&#22270;&#20687;&#25968;&#25454;&#26469;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#12290;&#30001;&#20110;&#26080;&#27861;&#33719;&#24471;&#31934;&#30830;&#30340;&#20687;&#32032;&#32423;&#26631;&#27880;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20391;&#37325;&#20110;&#36890;&#36807;&#20248;&#21270;CAM&#26679;&#24335;&#30340;&#28909;&#22270;&#26469;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#30340;&#20266;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#28909;&#22270;&#21487;&#33021;&#20165;&#25429;&#33719;&#23545;&#35937;&#31867;&#21035;&#30340;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#22270;&#20687;&#21306;&#22495;&#25110;&#30456;&#20851;&#30340;&#20849;&#21516;&#20986;&#29616;&#30340;&#32972;&#26223;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;WSSS&#30340;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#65288;SemPLeS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#26377;&#25928;&#22320;&#25552;&#31034;CLIP&#28508;&#31354;&#38388;&#20197;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#21644;&#25552;&#31034;&#24341;&#23548;&#30340;&#35821;&#20041;&#32454;&#21270;&#65292;&#20197;&#23398;&#20064;&#36866;&#24403;&#25551;&#36848;&#21644;&#25233;&#21046;&#19982;&#27599;&#20010;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#30456;&#20851;&#30340;&#20849;&#21516;&#20986;&#29616;&#30340;&#32972;&#26223;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11791v2 Announce Type: replace-cross  Abstract: Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may capture only the discriminative image regions of object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP latent space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Prompt-guided Semantic Refinement to learn the prompts that adequately describe and suppress the co-occurring backgrounds associated with each target object category. In thi
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;RepairLLaMA&#21487;&#20026;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#20135;&#29983;&#39640;&#25928;&#30340;&#36866;&#37197;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.15698</link><description>&lt;p&gt;
RepairLLaMA&#65306;&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#29992;&#20110;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15698
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;RepairLLaMA&#21487;&#20026;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#20135;&#29983;&#39640;&#25928;&#30340;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24050;&#26377;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#23545;&#20110;&#31243;&#24207;&#20462;&#22797;&#36827;&#34892;LLMs&#30340;&#24494;&#35843;&#26159;&#26368;&#36817;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#65292;&#26377;&#35768;&#22810;&#26410;&#34987;&#25506;&#32034;&#30340;&#32500;&#24230;&#12290;&#29616;&#26377;&#24037;&#20316;&#22823;&#22810;&#20351;&#29992;&#31616;&#21333;&#30340;&#20195;&#30721;&#34920;&#31034;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#33021;&#22815;&#24494;&#35843;&#26356;&#22823;&#22411;LLMs&#30340;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#23616;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepairLLaMA&#65292;&#19968;&#20010;&#32467;&#21512;&#20102;1&#65289;&#29992;&#20110;APR&#30340;&#20195;&#30721;&#34920;&#31034;&#21644;2&#65289;&#26368;&#20808;&#36827;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;LLM&#24494;&#35843;&#25216;&#26415;LoRA&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;RepairLLaMA&#20135;&#29983;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#8220;&#31243;&#24207;&#20462;&#22797;&#36866;&#37197;&#22120;&#8221;&#65292;&#29992;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#20855;&#26377;&#31243;&#24207;&#20462;&#22797;&#29305;&#23450;&#20195;&#30721;&#34920;&#31034;&#30340;&#24494;&#35843;&#36866;&#37197;&#22120;&#20351;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#20462;&#22797;&#20449;&#21495;&#12290;&#20854;&#27425;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26377;&#21161;&#20110;&#24494;&#35843;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15698v2 Announce Type: replace-cross  Abstract: Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs. To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-the-art parameter-efficient LLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a highly effective `program repair adapter' for fixing bugs with language models. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals. Second, parameter-efficient fine-tuning helps fine-tun
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#23454;&#29616;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#30340;&#25104;&#26412;&#65292;&#25351;&#20986;&#25972;&#21512;&#36825;&#20004;&#20010;&#30446;&#26631;&#20250;&#29306;&#29298;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.14712</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#12289;&#25928;&#29575;&#25110;&#38544;&#31169;&#65306;&#21482;&#33021;&#36873;&#20004;&#26679;
&lt;/p&gt;
&lt;p&gt;
Robustness, Efficiency, or Privacy: Pick Two in Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14712
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#23454;&#29616;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#30340;&#25104;&#26412;&#65292;&#25351;&#20986;&#25972;&#21512;&#36825;&#20004;&#20010;&#30446;&#26631;&#20250;&#29306;&#29298;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#22686;&#38271;&#65292;&#36825;&#20123;&#26550;&#26500;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#25968;&#25454;&#27745;&#26579;&#21644;&#30828;&#20214;&#25925;&#38556;&#31561;&#38382;&#39064;&#24456;&#24120;&#35265;&#12290;&#30830;&#20445;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#23545;&#20110;ML&#22312;&#20844;&#20849;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;ML&#26550;&#26500;&#20013;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#25152;&#24102;&#26469;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20998;&#24067;&#24335;ML&#20013;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#30340;&#21547;&#20041;&#65292;&#24182;&#38416;&#26126;&#20102;&#22914;&#20309;&#21333;&#29420;&#39640;&#25928;&#23454;&#29616;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#25972;&#21512;&#36825;&#20004;&#20010;&#30446;&#26631;&#20250;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26377;&#26174;&#33879;&#30340;&#25240;&#34935;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#20256;&#32479;&#30340;&#22122;&#22768;&#27880;&#20837;&#36890;&#36807;&#38544;&#34255;&#27602;&#23475;&#36755;&#20837;&#26469;&#25439;&#23475;&#20934;&#30830;&#24615;&#65292;&#32780;&#21152;&#23494;&#26041;&#27861;&#19982;&#38450;&#27602;&#38450;&#24481;&#30456;&#20914;&#31361;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#38750;&#32447;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14712v2 Announce Type: replace  Abstract: The success of machine learning (ML) applications relies on vast datasets and distributed architectures which, as they grow, present major challenges. In real-world scenarios, where data often contains sensitive information, issues like data poisoning and hardware failures are common. Ensuring privacy and robustness is vital for the broad adoption of ML in public life. This paper examines the costs associated with achieving these objectives in distributed ML architectures, from both theoretical and empirical perspectives. We overview the meanings of privacy and robustness in distributed ML, and clarify how they can be achieved efficiently in isolation. However, we contend that the integration of these two objectives entails a notable compromise in computational efficiency. In short, traditional noise injection hurts accuracy by concealing poisoned inputs, while cryptographic methods clash with poisoning defenses due to their non-line
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#28909;&#39044;&#35686;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#21644;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#27668;&#20505;&#21644;&#20581;&#24247;&#29615;&#22659;&#20013;&#30340;&#20302;&#20449;&#21495;&#25928;&#24212;&#21644;&#31354;&#38388;&#24322;&#36136;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.14196</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#28909;&#39044;&#35686;&#30340;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
Optimizing Heat Alert Issuance with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#28909;&#39044;&#35686;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#21644;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#27668;&#20505;&#21644;&#20581;&#24247;&#29615;&#22659;&#20013;&#30340;&#20302;&#20449;&#21495;&#25928;&#24212;&#21644;&#31354;&#38388;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#36866;&#24212;&#27668;&#20505;&#21464;&#21270;&#30340;&#20851;&#38190;&#25112;&#30053;&#20043;&#19968;&#26159;&#21033;&#29992;&#39044;&#35686;&#31995;&#32479;&#20943;&#23569;&#26497;&#31471;&#39640;&#28201;&#20107;&#20214;&#30340;&#19981;&#21033;&#20581;&#24247;&#24433;&#21709;&#65292;&#20197;&#20419;&#20351;&#39044;&#38450;&#24615;&#34892;&#21160;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#20248;&#21270;&#27492;&#31867;&#31995;&#32479;&#25928;&#26524;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#35780;&#20272;&#28909;&#39044;&#35686;&#25919;&#31574;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#20943;&#23569;&#19982;&#39640;&#28201;&#26377;&#20851;&#30340;&#20303;&#38498;&#20154;&#25968;&#12290;&#22870;&#21169;&#27169;&#22411;&#22522;&#20110;&#21382;&#21490;&#22825;&#27668;&#12289;&#21307;&#30103;&#20445;&#38505;&#20581;&#24247;&#35760;&#24405;&#20197;&#21450;&#31038;&#20250;&#32463;&#27982;/&#22320;&#29702;&#29305;&#24449;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#25216;&#26415;&#35299;&#20915;&#20102;&#22312;&#27668;&#20505;&#21644;&#20581;&#24247;&#29615;&#22659;&#20013;&#24120;&#35265;&#30340;&#20302;&#20449;&#21495;&#25928;&#24212;&#21644;&#31354;&#38388;&#24322;&#36136;&#24615;&#12290;&#36716;&#25442;&#27169;&#22411;&#32467;&#21512;&#20102;&#30495;&#23454;&#30340;&#21382;&#21490;&#22825;&#27668;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#27668;&#20505;&#21306;&#22495;&#30456;&#20284;&#24615;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#36827;&#34892;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14196v2 Announce Type: replace  Abstract: A key strategy in societal adaptation to climate change is the use of alert systems to reduce the adverse health impacts of extreme heat events by prompting preventative action. In this work, we investigate reinforcement learning (RL) as a tool to optimize the effectiveness of such systems. Our contributions are threefold. First, we introduce a novel RL environment enabling the evaluation of the effectiveness of heat alert policies to reduce heat-related hospitalizations. The rewards model is trained from a comprehensive dataset of historical weather, Medicare health records, and socioeconomic/geographic features. We use variational Bayesian techniques to address low-signal effects and spatial heterogeneity, which are commonly encountered in climate &amp; health settings. The transition model incorporates real historical weather patterns enriched by a data augmentation mechanism based on climate region similarity. Second, we use this env
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25299;&#25169;&#23398;&#20064;&#26041;&#27861;&#21644;&#23450;&#20041;&#19981;&#21487;&#38752;&#38142;&#25509;&#24863;&#30693;&#37051;&#22495;&#24046;&#24322;&#30340;&#26032;&#39046;&#22495;&#25968;&#37327;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#20998;&#24067;&#24322;&#26500;&#24615;&#21644;D2D&#32593;&#32476;&#20013;&#36890;&#20449;&#20013;&#26029;&#23548;&#33268;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.13611</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#21487;&#38752;D2D&#32593;&#32476;&#19978;&#24322;&#26500;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#25299;&#25169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Topology Learning for Heterogeneous Decentralized Federated Learning over Unreliable D2D Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#23398;&#20064;&#26041;&#27861;&#21644;&#23450;&#20041;&#19981;&#21487;&#38752;&#38142;&#25509;&#24863;&#30693;&#37051;&#22495;&#24046;&#24322;&#30340;&#26032;&#39046;&#22495;&#25968;&#37327;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#20998;&#24067;&#24322;&#26500;&#24615;&#21644;D2D&#32593;&#32476;&#20013;&#36890;&#20449;&#20013;&#26029;&#23548;&#33268;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#31227;&#21160;&#35774;&#22791;&#22312;&#26080;&#32447;&#35774;&#22791;&#38388;(D2D)&#32593;&#32476;&#20013;&#30340;&#26222;&#21450;&#65292;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;(DFL)&#22791;&#21463;&#20851;&#27880;&#12290;&#19982;&#38598;&#20013;&#24335;&#32852;&#37030;&#23398;&#20064;(CFL)&#30456;&#27604;&#65292;DFL&#20943;&#36731;&#20102;&#30001;&#20110;&#36890;&#20449;&#29942;&#39048;&#23548;&#33268;&#20013;&#22830;&#26381;&#21153;&#22120;&#25925;&#38556;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;DFL&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#19981;&#21516;&#29615;&#22659;&#20013;&#25968;&#25454;&#20998;&#24067;&#30340;&#20005;&#37325;&#24322;&#26500;&#24615;&#65292;&#20197;&#21450;D2D&#32593;&#32476;&#20013;&#37319;&#29992;&#29992;&#25143;&#25968;&#25454;&#25253;&#21327;&#35758;(UDP)&#23548;&#33268;&#30340;&#20256;&#36755;&#20013;&#26029;&#21644;&#25968;&#25454;&#21253;&#38169;&#35823;&#12290;&#36825;&#20123;&#25361;&#25112;&#36890;&#24120;&#20250;&#38477;&#20302;&#35757;&#32451;DFL&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23545;DFL&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25910;&#25947;&#30028;&#12290;&#36890;&#36807;&#22312;&#36825;&#19968;&#25910;&#25947;&#30028;&#20013;&#23450;&#20041;&#19968;&#20010;&#21517;&#20026;&#19981;&#21487;&#38752;&#38142;&#25509;&#24863;&#30693;&#37051;&#22495;&#24046;&#24322;&#30340;&#26032;&#39046;&#22495;&#25968;&#37327;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13611v2 Announce Type: replace  Abstract: With the proliferation of intelligent mobile devices in wireless device-to-device (D2D) networks, decentralized federated learning (DFL) has attracted significant interest. Compared to centralized federated learning (CFL), DFL mitigates the risk of central server failures due to communication bottlenecks. However, DFL faces several challenges, such as the severe heterogeneity of data distributions in diverse environments, and the transmission outages and package errors caused by the adoption of the User Datagram Protocol (UDP) in D2D networks. These challenges often degrade the convergence of training DFL models. To address these challenges, we conduct a thorough theoretical convergence analysis for DFL and derive a convergence bound. By defining a novel quantity named unreliable links-aware neighborhood discrepancy in this convergence bound, we formulate a tractable optimization objective, and develop a novel Topology Learning metho
&lt;/p&gt;</description></item><item><title>&#20559;&#20519;&#30340;&#38750;&#21709;&#24212;&#23545;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#26377;&#23475;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;-based &#20462;&#27491;&#31574;&#30053;&#26469;&#20943;&#36731;&#20854;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2312.08150</link><description>&lt;p&gt;
&#20855;&#26377;&#20559;&#20519;&#30340;&#38750;&#21709;&#24212;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active learning with biased non-response to label requests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08150
&lt;/p&gt;
&lt;p&gt;
&#20559;&#20519;&#30340;&#38750;&#21709;&#24212;&#23545;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#26377;&#23475;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;-based &#20462;&#27491;&#31574;&#30053;&#26469;&#20943;&#36731;&#20854;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#33719;&#21462;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26032;&#26631;&#31614;&#26469;&#25552;&#39640;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#26631;&#31614;&#35831;&#27714;&#30340;&#38750;&#21709;&#24212;&#24773;&#20917;&#19979;&#65292;&#20250;&#24433;&#21709;&#20027;&#21160;&#23398;&#20064;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38750;&#21709;&#24212;&#31867;&#22411;&#26469;&#27010;&#24565;&#21270;&#36825;&#31181;&#36864;&#21270;&#65292;&#35777;&#26126;&#20559;&#20519;&#30340;&#38750;&#21709;&#24212;&#23545;&#27169;&#22411;&#24615;&#33021;&#29305;&#21035;&#26377;&#23475;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#26631;&#31614;&#36807;&#31243;&#22825;&#28982;&#20381;&#36182;&#29992;&#25143;&#20132;&#20114;&#30340;&#29615;&#22659;&#20013;&#65292;&#20559;&#20519;&#30340;&#38750;&#21709;&#24212;&#24456;&#21487;&#33021;&#20250;&#20986;&#29616;&#12290;&#20026;&#20102;&#20943;&#36731;&#20559;&#20519;&#30340;&#38750;&#21709;&#24212;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25104;&#26412;&#30340;&#20462;&#27491;&#37319;&#26679;&#31574;&#30053;&#8212;&#8212;&#39044;&#26399;&#25928;&#29992;&#30340;&#19978;&#30028;&#32622;&#20449;&#24230;&#65288;UCB-EU&#65289;, &#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21512;&#29702;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#26631;&#31614;&#38750;&#21709;&#24212;&#36896;&#25104;&#30340;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08150v2 Announce Type: replace  Abstract: Active learning can improve the efficiency of training prediction models by identifying the most informative new labels to acquire. However, non-response to label requests can impact active learning's effectiveness in real-world contexts. We conceptualise this degradation by considering the type of non-response present in the data, demonstrating that biased non-response is particularly detrimental to model performance. We argue that biased non-response is likely in contexts where the labelling process, by nature, relies on user interactions. To mitigate the impact of biased non-response, we propose a cost-based correction to the sampling strategy--the Upper Confidence Bound of the Expected Utility (UCB-EU)--that can, plausibly, be applied to any active learning algorithm. Through experiments, we demonstrate that our method successfully reduces the harm from labelling non-response in many settings. However, we also characterise settin
&lt;/p&gt;</description></item><item><title>&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#23398;&#20064;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;&#30340;&#22806;&#29983;&#22122;&#22768;&#65292;&#24182;&#23558;&#20854;&#19982;&#30456;&#24212;&#30340;&#20869;&#29983;&#21464;&#37327;&#21305;&#37197;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24178;&#39044;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.06091</link><description>&lt;p&gt;
&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Unknown Intervention Targets in Structural Causal Models from Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06091
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#23398;&#20064;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;&#30340;&#22806;&#29983;&#22122;&#22768;&#65292;&#24182;&#23558;&#20854;&#19982;&#30456;&#24212;&#30340;&#20869;&#29983;&#21464;&#37327;&#21305;&#37197;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24178;&#39044;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35782;&#21035;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#20174;&#22810;&#20010;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#24322;&#26500;&#25968;&#25454;&#12290;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;&#26159;&#19968;&#32452;&#20869;&#29983;&#21464;&#37327;&#65292;&#20854;&#30456;&#24212;&#30340;&#22806;&#29983;&#22122;&#22768;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#31532;&#19968;&#38454;&#27573;&#20013;&#24674;&#22797;&#20102;&#36328;&#19981;&#21516;&#29615;&#22659;&#21457;&#29983;&#21464;&#21270;&#30340;&#26410;&#30693;&#24178;&#39044;&#30446;&#26631;&#23545;&#24212;&#30340;&#22806;&#29983;&#22122;&#22768;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#24674;&#22797;&#30340;&#22122;&#22768;&#19982;&#30456;&#24212;&#30340;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#21305;&#37197;&#12290;&#23545;&#20110;&#24674;&#22797;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23398;&#20064;&#36825;&#20123;&#22806;&#29983;&#22122;&#22768;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#21487;&#36798;&#21040;&#26576;&#31181;&#20998;&#37327;&#26041;&#21521;&#21487;&#36870;&#36716;&#25442;&#12290;&#23545;&#20110;&#21305;&#37197;&#38454;&#27573;&#65292;&#22312;&#22240;&#26524;&#20805;&#20998;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#21807;&#19968;&#22320;&#35782;&#21035;&#24178;&#39044;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06091v2 Announce Type: replace-cross  Abstract: We study the problem of identifying the unknown intervention targets in structural causal models where we have access to heterogeneous data collected from multiple environments. The unknown intervention targets are the set of endogenous variables whose corresponding exogenous noises change across the environments. We propose a two-phase approach which in the first phase recovers the exogenous noises corresponding to unknown intervention targets whose distributions have changed across environments. In the second phase, the recovered noises are matched with the corresponding endogenous variables. For the recovery phase, we provide sufficient conditions for learning these exogenous noises up to some component-wise invertible transformation. For the matching phase, under the causal sufficiency assumption, we show that the proposed method uniquely identifies the intervention targets. In the presence of latent confounders, the interv
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#19979;&#20445;&#35777;&#24615;&#33021;&#30340;&#24378;&#22823;GNN&#27169;&#22411;</title><link>https://arxiv.org/abs/2312.04111</link><description>&lt;p&gt;
&#25171;&#30772;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22312;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#32416;&#32544;
&lt;/p&gt;
&lt;p&gt;
Breaking the Entanglement of Homophily and Heterophily in Semi-supervised Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04111
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#19979;&#20445;&#35777;&#24615;&#33021;&#30340;&#24378;&#22823;GNN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21033;&#29992;&#22270;&#25968;&#25454;&#24211;&#30693;&#35782;&#36827;&#34892;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GNNs&#36981;&#24490;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#21363;&#36830;&#25509;&#30340;&#33410;&#28857;&#26356;&#26377;&#21487;&#33021;&#23637;&#29616;&#20986;&#30456;&#20284;&#30340;&#29305;&#24449;&#20998;&#24067;&#21644;&#30456;&#21516;&#30340;&#26631;&#31614;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#35777;&#26126;&#26159;&#33030;&#24369;&#30340;&#12290;&#20316;&#20026;&#34917;&#20805;&#65292;&#24322;&#36136;&#24615;&#21453;&#26144;&#20102;&#30456;&#36830;&#33410;&#28857;&#30340;&#19981;&#30456;&#20284;&#24615;&#65292;&#22312;&#22270;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#24037;&#31243;&#24072;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;GNN&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#19979;&#20445;&#35777;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#36827;&#34892;&#20102;&#22823;&#37327;&#23581;&#35797;&#65292;&#20294;&#30001;&#20110;&#26080;&#21521;&#22270;&#30340;&#32422;&#26463;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GNNs&#37117;&#38590;&#20197;&#23454;&#29616;&#26368;&#20339;&#33410;&#28857;&#34920;&#31034;&#12290;&#24573;&#30053;&#26377;&#21521;&#36793;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#22270;&#34920;&#31034;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;GNNs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04111v2 Announce Type: replace-cross  Abstract: Recently, graph neural networks (GNNs) have shown prominent performance in semi-supervised node classification by leveraging knowledge from the graph database. However, most existing GNNs follow the homophily assumption, where connected nodes are more likely to exhibit similar feature distributions and the same labels, and such an assumption has proven to be vulnerable in a growing number of practical applications. As a supplement, heterophily reflects dissimilarity in connected nodes, which has gained significant attention in graph learning. To this end, data engineers aim to develop a powerful GNN model that can ensure performance under both homophily and heterophily. Despite numerous attempts, most existing GNNs struggle to achieve optimal node representations due to the constraints of undirected graphs. The neglect of directed edges results in sub-optimal graph representations, thereby hindering the capacity of GNNs. To add
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631; SharpeRatio@k&#65292;&#29992;&#20110;&#35780;&#20272;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#39118;&#38505;-&#25910;&#30410;&#26435;&#34913;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#39118;&#38505;&#20272;&#35745;&#22120;&#24182;&#20934;&#30830;&#35782;&#21035;&#26368;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#12290;</title><link>https://arxiv.org/abs/2311.18207</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#39118;&#38505;-&#25910;&#30410;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631; SharpeRatio@k&#65292;&#29992;&#20110;&#35780;&#20272;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#39118;&#38505;-&#25910;&#30410;&#26435;&#34913;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#39118;&#38505;&#20272;&#35745;&#22120;&#24182;&#20934;&#30830;&#35782;&#21035;&#26368;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#26088;&#22312;&#20165;&#20351;&#29992;&#32447;&#19979;&#35760;&#24405;&#30340;&#25968;&#25454;&#35780;&#20272;&#21453;&#20107;&#23454;&#25919;&#31574;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#24120;&#29992;&#20110;&#35782;&#21035;&#22312;&#22312;&#32447;A/B&#27979;&#35797;&#37096;&#32626;&#20013;&#30340;&#21069;k&#20010;&#26377;&#21069;&#36884;&#30340;&#25919;&#31574;&#12290;&#24403;&#21069;&#30340;OPE&#20272;&#35745;&#22120;&#35780;&#20272;&#25351;&#26631;&#20027;&#35201;&#20851;&#27880;OPE&#25110;&#19979;&#28216;&#25919;&#31574;&#36873;&#25321;&#30340;&#8220;&#20934;&#30830;&#24615;&#8221;&#65292;&#32780;&#24573;&#30053;&#20102;&#38543;&#21518;&#22312;&#32447;&#25919;&#31574;&#37096;&#32626;&#20013;&#30340;&#39118;&#38505;-&#22238;&#25253;&#26435;&#34913;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#37329;&#34701;&#20013;&#30340;&#25237;&#36164;&#32452;&#21512;&#35780;&#20272;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;SharpeRatio@k&#30340;&#26032;&#25351;&#26631;&#65292;&#29992;&#20110;&#34913;&#37327;&#30001;OPE&#20272;&#35745;&#22120;&#24418;&#25104;&#30340;&#25919;&#31574;&#25237;&#36164;&#32452;&#21512;&#22312;&#19981;&#21516;&#30340;&#22312;&#32447;&#35780;&#20272;&#39044;&#31639;&#65288;k&#65289;&#19979;&#30340;&#39118;&#38505;-&#22238;&#25253;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31034;&#20363;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#65292;&#23637;&#31034;&#20102;&#20854;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#24182;&#20934;&#30830;&#35782;&#21035;&#25928;&#29575;&#26368;&#39640;&#30340;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18207v3 Announce Type: replace-cross  Abstract: Off-Policy Evaluation (OPE) aims to assess the effectiveness of counterfactual policies using only offline logged data and is often used to identify the top-k promising policies for deployment in online A/B tests. Existing evaluation metrics for OPE estimators primarily focus on the "accuracy" of OPE or that of downstream policy selection, neglecting risk-return tradeoff in the subsequent online policy deployment. To address this issue, we draw inspiration from portfolio evaluation in finance and develop a new metric, called SharpeRatio@k, which measures the risk-return tradeoff of policy portfolios formed by an OPE estimator under varying online evaluation budgets (k). We validate our metric in two example scenarios, demonstrating its ability to effectively distinguish between low-risk and high-risk estimators and to accurately identify the most efficient one. Efficiency of an estimator is characterized by its capability to fo
&lt;/p&gt;</description></item><item><title>SCOPE-RL&#26159;&#19968;&#20010;Python&#24211;&#65292;&#20860;&#39038;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#31574;&#30053;&#35780;&#20272;&#65292;&#36890;&#36807;&#25972;&#21512;&#31574;&#30053;&#23398;&#20064;&#21644;&#35780;&#20272;&#23454;&#29616;&#20102;&#26356;&#28789;&#27963;&#12289;&#23436;&#25972;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;OPE&#27169;&#22359;&#25552;&#20379;&#20102;&#22810;&#31181;OPE&#20272;&#35745;&#22120;&#21644;&#31283;&#20581;&#30340;OPE&#21327;&#35758;&#65292;&#20351;&#24471;OPE&#26356;&#28145;&#20837;&#21644;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2311.18206</link><description>&lt;p&gt;
SCOPE-RL: &#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18206
&lt;/p&gt;
&lt;p&gt;
SCOPE-RL&#26159;&#19968;&#20010;Python&#24211;&#65292;&#20860;&#39038;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#31574;&#30053;&#35780;&#20272;&#65292;&#36890;&#36807;&#25972;&#21512;&#31574;&#30053;&#23398;&#20064;&#21644;&#35780;&#20272;&#23454;&#29616;&#20102;&#26356;&#28789;&#27963;&#12289;&#23436;&#25972;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;OPE&#27169;&#22359;&#25552;&#20379;&#20102;&#22810;&#31181;OPE&#20272;&#35745;&#22120;&#21644;&#31283;&#20581;&#30340;OPE&#21327;&#35758;&#65292;&#20351;&#24471;OPE&#26356;&#28145;&#20837;&#21644;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SCOPE-RL&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#28304;Python&#36719;&#20214;&#65292;&#19987;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;offline RL&#65289;&#12289;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#21644;&#36873;&#25321;&#65288;OPS&#65289;&#32780;&#35774;&#35745;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24211;&#19981;&#21516;&#65292;&#36825;&#20123;&#24211;&#20165;&#20851;&#27880;&#31574;&#30053;&#23398;&#20064;&#25110;&#35780;&#20272;&#20013;&#30340;&#19968;&#20010;&#65292;SCOPE-RL&#26080;&#32541;&#25972;&#21512;&#20102;&#36825;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#20419;&#36827;&#20102;&#31163;&#32447;RL&#21644;OPE&#36807;&#31243;&#30340;&#28789;&#27963;&#21644;&#23436;&#25972;&#23454;&#29616;&#12290;SCOPE-RL&#29305;&#21035;&#20391;&#37325;&#20110;&#20854;OPE&#27169;&#22359;&#65292;&#25552;&#20379;&#19968;&#31995;&#21015;OPE&#20272;&#35745;&#22120;&#21644;&#31283;&#20581;&#30340;OPE&#21327;&#35758;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#19982;&#20854;&#20182;&#36719;&#20214;&#21253;&#30456;&#27604;&#65292;SCOPE-RL&#33021;&#22815;&#26356;&#28145;&#20837;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;OPE&#12290;&#20363;&#22914;&#65292;SCOPE-RL&#36890;&#36807;&#20272;&#35745;&#31574;&#30053;&#19979;&#30340;&#25972;&#20010;&#22870;&#21169;&#20998;&#24067;&#32780;&#19981;&#20165;&#20165;&#26159;&#20854;&#28857;&#20540;&#39044;&#26399;&#20540;&#26469;&#22686;&#24378;OPE&#12290;&#27492;&#22806;&#65292;SCOPE-RL&#36890;&#36807;&#22312;OPE&#32467;&#26524;&#20013;&#25552;&#20379;&#39118;&#38505;-&#22238;&#25253;&#26435;&#34913;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#24211;&#20013;&#20165;&#20165;&#26159;&#20934;&#30830;&#24615;&#35780;&#20272;&#30340;&#26356;&#20840;&#38754;&#30340;OPE&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18206v3 Announce Type: replace-cross  Abstract: This paper introduces SCOPE-RL, a comprehensive open-source Python software designed for offline reinforcement learning (offline RL), off-policy evaluation (OPE), and selection (OPS). Unlike most existing libraries that focus solely on either policy learning or evaluation, SCOPE-RL seamlessly integrates these two key aspects, facilitating flexible and complete implementations of both offline RL and OPE processes. SCOPE-RL put particular emphasis on its OPE modules, offering a range of OPE estimators and robust evaluation-of-OPE protocols. This approach enables more in-depth and reliable OPE compared to other packages. For instance, SCOPE-RL enhances OPE by estimating the entire reward distribution under a policy rather than its mere point-wise expected value. Additionally, SCOPE-RL provides a more thorough evaluation-of-OPE by presenting the risk-return tradeoff in OPE results, extending beyond mere accuracy evaluations in exis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32925;&#30284;&#25968;&#23383;&#32452;&#32455;&#30149;&#29702;&#20999;&#29255;&#20998;&#31867;&#30340;&#20256;&#23548;&#24335;&#23567;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22312;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#21487;&#29992;&#24615;&#30340;&#21516;&#26102;&#21462;&#24471;&#20102;&#19968;&#33268;&#20934;&#30830;&#20998;&#31867;&#30340;&#23454;&#38469;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.17740</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#32925;&#30284;&#25968;&#23383;&#32452;&#32455;&#30149;&#29702;&#20999;&#29255;&#30340;&#20256;&#23548;&#24335;&#23567;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transductive few-shot learning approach for classification of digital histopathological slides from liver cancer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17740
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32925;&#30284;&#25968;&#23383;&#32452;&#32455;&#30149;&#29702;&#20999;&#29255;&#20998;&#31867;&#30340;&#20256;&#23548;&#24335;&#23567;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22312;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#21487;&#29992;&#24615;&#30340;&#21516;&#26102;&#21462;&#24471;&#20102;&#19968;&#33268;&#20934;&#30830;&#20998;&#31867;&#30340;&#23454;&#38469;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#23545;2D&#32452;&#32455;&#30149;&#29702;&#20999;&#29255;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#26631;&#35760;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#36890;&#36807;&#23558;&#28369;&#21160;&#31383;&#21475;&#25216;&#26415;&#24212;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#20999;&#29255;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#23548;&#24335;&#23398;&#20064;&#65288;&#21363;&#22312;&#20999;&#29255;&#19978;&#20570;&#20986;&#32852;&#21512;&#39044;&#27979;&#65289;&#22312;&#23454;&#29616;&#19968;&#33268;&#20934;&#30830;&#20998;&#31867;&#26041;&#38754;&#30340;&#23454;&#38469;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#31574;&#30053;&#65292;&#31215;&#26497;&#24809;&#32602;&#27599;&#20010;&#31383;&#21475;&#20869;&#39044;&#27979;&#22823;&#37327;&#19981;&#21516;&#31867;&#21035;&#12290;&#25105;&#20204;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20197;&#23545;&#32925;&#30284;&#25968;&#23383;&#24187;&#28783;&#29255;&#20013;&#30340;&#32452;&#32455;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#65292;&#29305;&#21035;&#26159;&#32925;&#32454;&#32990;&#30284;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22686;&#24378;&#33258;&#21160;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#36807;&#31243;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17740v2 Announce Type: replace-cross  Abstract: This paper presents a new approach for classifying 2D histopathology patches using few-shot learning. The method is designed to tackle a significant challenge in histopathology, which is the limited availability of labeled data. By applying a sliding window technique to histopathology slides, we illustrate the practical benefits of transductive learning (i.e., making joint predictions on patches) to achieve consistent and accurate classification. Our approach involves an optimization-based strategy that actively penalizes the prediction of a large number of distinct classes within each window. We conducted experiments on histopathological data to classify tissue classes in digital slides of liver cancer, specifically hepatocellular carcinoma. The initial results show the effectiveness of our method and its potential to enhance the process of automated cancer diagnosis and treatment, all while reducing the time and effort requir
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21033;&#29992;&#24314;&#31569;&#29289;&#29702;&#23398;&#27934;&#23519;&#23454;&#29616;&#22266;&#26377;&#21487;&#35299;&#37322;&#30340;&#33021;&#28304;&#25968;&#25454;&#25554;&#34917;&#27169;&#22411;PI-DAE&#65292;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#29289;&#29702;&#21551;&#21457;&#36719;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2311.16632</link><description>&lt;p&gt;
&#25171;&#24320;&#40657;&#21283;&#23376;&#65306;&#21033;&#29992;&#24314;&#31569;&#29289;&#29702;&#23398;&#27934;&#23519;&#23454;&#29616;&#22266;&#26377;&#21487;&#35299;&#37322;&#30340;&#33021;&#28304;&#25968;&#25454;&#25554;&#34917;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Opening the Black Box: Towards inherently interpretable energy data imputation models using building physics insight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21033;&#29992;&#24314;&#31569;&#29289;&#29702;&#23398;&#27934;&#23519;&#23454;&#29616;&#22266;&#26377;&#21487;&#35299;&#37322;&#30340;&#33021;&#28304;&#25968;&#25454;&#25554;&#34917;&#27169;&#22411;PI-DAE&#65292;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#29289;&#29702;&#21551;&#21457;&#36719;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#32463;&#24120;&#34987;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#39046;&#22495;&#30340;&#23454;&#36341;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#36890;&#24120;&#38656;&#35201;&#20808;&#36827;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21453;&#26144;&#36825;&#20123;&#24322;&#24120;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#12290;&#20316;&#20026;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#20851;&#30340;&#19968;&#20010;&#25345;&#32493;&#30740;&#31350;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#32593;&#32476;&#20013;&#24341;&#20837;&#20808;&#39564;&#30693;&#35782;&#26469;&#25506;&#32034;&#27169;&#22411;&#22312;&#26377;&#38480;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#31181;&#31574;&#30053;&#20063;&#21487;&#20197;&#23548;&#33268;&#26356;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#20419;&#36827;&#26041;&#27861;&#30340;&#29616;&#22330;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#29289;&#29702;&#21551;&#21457;&#22411;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;(PI-DAE)&#29992;&#20110;&#21830;&#19994;&#24314;&#31569;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#29289;&#29702;&#21551;&#21457;&#36719;&#32422;&#26463;&#24378;&#21152;&#32473;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;(DAE)&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#20026;&#20102;&#37327;&#21270;&#29289;&#29702;&#32452;&#20214;&#30340;&#22909;&#22788;&#65292;&#36827;&#34892;&#20102;&#19968;&#39033;&#28040;&#34701;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16632v2 Announce Type: replace-cross  Abstract: Missing data are frequently observed by practitioners and researchers in the building energy modeling community. In this regard, advanced data-driven solutions, such as Deep Learning methods, are typically required to reflect the non-linear behavior of these anomalies. As an ongoing research question related to Deep Learning, a model's applicability to limited data settings can be explored by introducing prior knowledge in the network. This same strategy can also lead to more interpretable predictions, hence facilitating the field application of the approach. For that purpose, the aim of this paper is to propose the use of Physics-informed Denoising Autoencoders (PI-DAE) for missing data imputation in commercial buildings. In particular, the presented method enforces physics-inspired soft constraints to the loss function of a Denoising Autoencoder (DAE). In order to quantify the benefits of the physical component, an ablation s
&lt;/p&gt;</description></item><item><title>TFMQ-DM&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.16503</link><description>&lt;p&gt;
TFMQ-DM&#65306;&#38754;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#29305;&#24449;&#32500;&#25345;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16503
&lt;/p&gt;
&lt;p&gt;
TFMQ-DM&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16503v2 &#36890;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449; &#25688;&#35201;&#65306;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#20294;&#30001;&#20110;&#20854;&#36739;&#38271;&#30340;&#25512;&#29702;&#26102;&#38388;&#21644;&#22823;&#37327;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#22312;&#24191;&#27867;&#36866;&#29992;&#24615;&#26041;&#38754;&#36935;&#21040;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#39640;&#25928;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;&#25193;&#25955;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#26102;&#38388;&#27493;&#38271; $t$ &#26469;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#22810;&#36718;&#21435;&#22122;&#12290;&#36890;&#24120;&#65292;&#20174;&#26377;&#38480;&#38598;&#21512; $\{1, \ldots, T\}$ &#20013;&#30340; $t$&#20250;&#34987;&#20960;&#20010;&#27169;&#22359;&#32534;&#30721;&#20026;&#19968;&#20010;&#26102;&#38388;&#29305;&#24449;&#65292;&#36825;&#23436;&#20840;&#19981;&#32771;&#34385;&#37319;&#26679;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#24182;&#19981;&#20998;&#21035;&#20248;&#21270;&#36825;&#20123;&#27169;&#22359;&#12290;&#23427;&#20204;&#37319;&#29992;&#19981;&#24688;&#24403;&#30340;&#37325;&#26500;&#30446;&#26631;&#21644;&#22797;&#26434;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#23548;&#33268;&#26102;&#38388;&#29305;&#24449;&#21644;&#21435;&#22122;&#36712;&#36857;&#20005;&#37325;&#21463;&#21040;&#24178;&#25200;&#65292;&#21516;&#26102;&#21387;&#32553;&#25928;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16503v2 Announce Type: replace-cross  Abstract: The Diffusion model, a prevalent framework for image generation, encounters significant challenges in terms of broad applicability due to its extended inference times and substantial memory requirements. Efficient Post-training Quantization (PTQ) is pivotal for addressing these issues in traditional models. Different from traditional models, diffusion models heavily depend on the time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$ from the finite set $\{1, \ldots, T\}$ is encoded to a temporal feature by a few modules totally irrespective of the sampling data. However, existing PTQ methods do not optimize these modules separately. They adopt inappropriate reconstruction targets and complex calibration methods, resulting in a severe disturbance of the temporal feature and denoising trajectory, as well as a low compression efficiency. To solve these, we propose a Temporal Feature Maintenance Quantization (TF
&lt;/p&gt;</description></item><item><title>FRAC-Q-Learning&#26159;&#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#35774;&#35745;&#65292;&#33021;&#36991;&#20813;&#29992;&#25143;&#21388;&#28902;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#22312;&#20852;&#36259;&#21644;&#21388;&#28902;&#31243;&#24230;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#19981;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;</title><link>https://arxiv.org/abs/2311.15327</link><description>&lt;p&gt;
FRAC-Q-Learning: &#19968;&#31181;&#20855;&#26377;&#36991;&#20813;&#21388;&#28902;&#36807;&#31243;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FRAC-Q-Learning: A Reinforcement Learning with Boredom Avoidance Processes for Social Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15327
&lt;/p&gt;
&lt;p&gt;
FRAC-Q-Learning&#26159;&#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#35774;&#35745;&#65292;&#33021;&#36991;&#20813;&#29992;&#25143;&#21388;&#28902;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#22312;&#20852;&#36259;&#21644;&#21388;&#28902;&#31243;&#24230;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#19981;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32463;&#24120;&#34987;&#24212;&#29992;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#26410;&#38024;&#23545;&#31038;&#20132;&#26426;&#22120;&#20154;&#36827;&#34892;&#20248;&#21270;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#26032;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;FRAC-Q-Learning&#65292;&#21487;&#20197;&#36991;&#20813;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#12290;&#35813;&#31639;&#27861;&#38500;&#20102;&#38543;&#26426;&#21270;&#21644;&#20998;&#31867;&#36807;&#31243;&#22806;&#65292;&#36824;&#21253;&#25324;&#19968;&#20010;&#36951;&#24536;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19982;&#20256;&#32479;Q-Learning&#30340;&#27604;&#36739;&#35780;&#20272;&#20102;FRAC-Q-Learning&#30340;&#20852;&#36259;&#21644;&#21388;&#28902;&#31243;&#24230;&#20998;&#25968;&#12290;FRAC-Q-Learning&#26174;&#31034;&#20986;&#26126;&#26174;&#26356;&#39640;&#30340;&#20852;&#36259;&#20998;&#25968;&#36235;&#21183;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;Q-Learning&#26356;&#38590;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#12290;&#22240;&#27492;&#65292;FRAC-Q-Learning&#26377;&#21161;&#20110;&#24320;&#21457;&#19981;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#35813;&#31639;&#27861;&#36824;&#21487;&#20197;&#22312;&#22522;&#20110;Web&#30340;&#36890;&#20449;&#21644;&#25945;&#32946;&#20013;&#25214;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15327v3 Announce Type: replace-cross  Abstract: The reinforcement learning algorithms have often been applied to social robots. However, most reinforcement learning algorithms were not optimized for the use of social robots, and consequently they may bore users. We proposed a new reinforcement learning method specialized for the social robot, the FRAC-Q-learning, that can avoid user boredom. The proposed algorithm consists of a forgetting process in addition to randomizing and categorizing processes. This study evaluated interest and boredom hardness scores of the FRAC-Q-learning by a comparison with the traditional Q-learning. The FRAC-Q-learning showed significantly higher trend of interest score, and indicated significantly harder to bore users compared to the traditional Q-learning. Therefore, the FRAC-Q-learning can contribute to develop a social robot that will not bore users. The proposed algorithm can also find applications in Web-based communication and educational 
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#33945;&#26085;&#26144;&#23556;&#20013;&#24341;&#20837;&#19981;&#24179;&#34913;&#24615;&#21487;&#25913;&#36827;&#26410;&#37197;&#23545;&#39046;&#22495;&#36716;&#25442;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;</title><link>https://arxiv.org/abs/2311.15100</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#33945;&#26085;&#26144;&#23556;&#20013;&#30340;&#19981;&#24179;&#34913;&#24615;&#25913;&#36827;&#26410;&#37197;&#23545;&#39046;&#22495;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15100
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#33945;&#26085;&#26144;&#23556;&#20013;&#24341;&#20837;&#19981;&#24179;&#34913;&#24615;&#21487;&#25913;&#36827;&#26410;&#37197;&#23545;&#39046;&#22495;&#36716;&#25442;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#20248;&#36755;&#36865;&#65288;OT&#65289;&#20013;&#65292;&#33945;&#26085;&#26144;&#23556;&#34987;&#31216;&#20026;&#20197;&#26368;&#32463;&#27982;&#30340;&#26041;&#24335;&#23558;&#28304;&#20998;&#24067;&#20256;&#36755;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#26144;&#23556;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#24182;&#24212;&#29992;&#20102;&#22810;&#31181;&#29992;&#20110;Monge&#26144;&#23556;&#30340;&#31070;&#32463;&#20272;&#35745;&#22120;&#65292;&#24182;&#22312;&#21508;&#31181;&#26410;&#37197;&#23545;&#30340;&#39046;&#22495;&#36716;&#25442;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#65292;&#20363;&#22914;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;OT&#26694;&#26550;&#24378;&#21046;&#20445;&#25345;&#36136;&#37327;&#23432;&#24658;&#65292;&#36825;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#31163;&#32676;&#20540;&#30340;&#24433;&#21709;&#65292;&#24182;&#38480;&#21046;&#20102;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#21518;&#32773;&#22312;OT&#39046;&#22495;&#36716;&#25442;&#20219;&#21153;&#20013;&#21487;&#33021;&#29305;&#21035;&#26377;&#23475;&#65292;&#22240;&#20026;&#20854;&#20013;&#26174;&#24335;&#32771;&#34385;&#20102;&#26679;&#26412;&#22312;&#20998;&#24067;&#20013;&#30340;&#30456;&#23545;&#20301;&#32622;&#12290;&#23613;&#31649;&#22312;&#31163;&#25955;&#35774;&#32622;&#20013;&#65292;&#19981;&#24179;&#34913;OT&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#20294;&#20854;&#38598;&#25104;&#21040;&#31070;&#32463;Monge&#26144;&#23556;&#20272;&#35745;&#22120;&#20013;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#24179;&#34913;&#24615;&#32435;&#20837;&#21040;&#20219;&#20309;Monge&#26144;&#23556;&#20272;&#35745;&#22120;&#20013;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20272;&#35745;&#22120;&#20197;&#27169;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15100v2 Announce Type: replace-cross  Abstract: In optimal transport (OT), a Monge map is known as a mapping that transports a source distribution to a target distribution in the most cost-efficient way. Recently, multiple neural estimators for Monge maps have been developed and applied in diverse unpaired domain translation tasks, e.g. in single-cell biology and computer vision. However, the classic OT framework enforces mass conservation, which makes it prone to outliers and limits its applicability in real-world scenarios. The latter can be particularly harmful in OT domain translation tasks, where the relative position of a sample within a distribution is explicitly taken into account. While unbalanced OT tackles this challenge in the discrete setting, its integration into neural Monge map estimators has received limited attention. We propose a theoretically grounded method to incorporate unbalancedness into any Monge map estimator. We improve existing estimators to mode
&lt;/p&gt;</description></item><item><title>ECNR&#26159;&#19968;&#31181;&#38024;&#23545;&#26102;&#21464;&#25968;&#25454;&#30340;&#39640;&#25928;&#21387;&#32553;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23610;&#24230;&#32467;&#26500;&#21644;&#22810;&#20010;&#23567;&#22411;MLP&#65292;&#20197;&#21450;&#28145;&#24230;&#21387;&#32553;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2311.12831</link><description>&lt;p&gt;
ECNR: &#39640;&#25928;&#21387;&#32553;&#31070;&#32463;&#34920;&#31034;&#30340;&#26102;&#21464;&#20307;&#31215;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12831
&lt;/p&gt;
&lt;p&gt;
ECNR&#26159;&#19968;&#31181;&#38024;&#23545;&#26102;&#21464;&#25968;&#25454;&#30340;&#39640;&#25928;&#21387;&#32553;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23610;&#24230;&#32467;&#26500;&#21644;&#22810;&#20010;&#23567;&#22411;MLP&#65292;&#20197;&#21450;&#28145;&#24230;&#21387;&#32553;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#27010;&#24565;&#31616;&#21333;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#21387;&#32553;&#31070;&#32463;&#34920;&#31034;&#24050;&#32463;&#25104;&#20026;&#31649;&#29702;&#22823;&#35268;&#27169;&#20307;&#31215;&#25968;&#25454;&#38598;&#30340;&#20256;&#32479;&#21387;&#32553;&#26041;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#24403;&#21069;&#30340;&#31070;&#32463;&#21387;&#32553;&#23454;&#36341;&#21033;&#29992;&#21333;&#20010;&#22823;&#22411;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#26469;&#23545;&#20840;&#23616;&#20307;&#31215;&#36827;&#34892;&#32534;&#30721;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#24930;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#21464;&#25968;&#25454;&#21387;&#32553;&#30340;&#39640;&#25928;&#21387;&#32553;&#31070;&#32463;&#34920;&#31034;&#65288;ECNR&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#25289;&#26222;&#25289;&#26031;&#37329;&#23383;&#22612;&#36827;&#34892;&#33258;&#36866;&#24212;&#20449;&#21495;&#25311;&#21512;&#12290;&#36981;&#24490;&#22810;&#23610;&#24230;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#23610;&#24230;&#19978;&#21033;&#29992;&#22810;&#20010;&#23567;&#22411;MLP&#26469;&#25311;&#21512;&#26412;&#22320;&#20869;&#23481;&#25110;&#27531;&#24046;&#22359;&#12290;&#36890;&#36807;&#23558;&#30456;&#20284;&#30340;&#22359;&#20998;&#37197;&#32473;&#30456;&#21516;&#22823;&#23567;&#30340;MLP&#65292;&#36890;&#36807;&#22823;&#23567;&#32479;&#19968;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;MLP&#20043;&#38388;&#30340;&#24179;&#34913;&#24182;&#34892;&#21270;&#65292;&#20174;&#32780;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#19982;&#22810;&#23610;&#24230;&#32467;&#26500;&#21327;&#21516;&#24037;&#20316;&#30340;&#26159;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#31181;&#28145;&#24230;&#21387;&#32553;&#31574;&#30053;&#26469;&#21387;&#32553;&#32467;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ECNR&#30340;&#25928;&#26524;&#65292;&#26377;&#22810;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Due to its conceptual simplicity and generality, compressive neural representation has emerged as a promising alternative to traditional compression methods for managing massive volumetric datasets. The current practice of neural compression utilizes a single large multilayer perceptron (MLP) to encode the global volume, incurring slow training and inference. This paper presents an efficient compressive neural representation (ECNR) solution for time-varying data compression, utilizing the Laplacian pyramid for adaptive signal fitting. Following a multiscale structure, we leverage multiple small MLPs at each scale for fitting local content or residual blocks. By assigning similar blocks to the same MLP via size uniformization, we enable balanced parallelization among MLPs to significantly speed up training and inference. Working in concert with the multiscale structure, we tailor a deep compression strategy to compact the resulting model. We show the effectiveness of ECNR with multiple 
&lt;/p&gt;</description></item><item><title>BEND&#26159;&#19968;&#20010;&#38024;&#23545;DNA&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;&#19968;&#31995;&#21015;&#22312;&#20154;&#31867;&#22522;&#22240;&#32452;&#19978;&#23450;&#20041;&#30340;&#29616;&#23454;&#19988;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2311.12570</link><description>&lt;p&gt;
BEND: &#22312;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#20219;&#21153;&#19978;&#23545;DNA&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BEND: Benchmarking DNA Language Models on biologically meaningful tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12570
&lt;/p&gt;
&lt;p&gt;
BEND&#26159;&#19968;&#20010;&#38024;&#23545;DNA&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;&#19968;&#31995;&#21015;&#22312;&#20154;&#31867;&#22522;&#22240;&#32452;&#19978;&#23450;&#20041;&#30340;&#29616;&#23454;&#19988;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#32452;&#24207;&#21015;&#21253;&#21547;&#20102;&#25351;&#23548;&#32454;&#32990;&#36807;&#31243;&#30340;&#34013;&#22270;&#12290;&#23613;&#31649;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#22522;&#22240;&#32452;&#30340;&#21487;&#29992;&#24615;&#22823;&#22823;&#22686;&#21152;&#65292;&#20294;&#23545;DNA&#24207;&#21015;&#20013;&#32534;&#30721;&#30340;&#21508;&#31181;&#21151;&#33021;&#24615;&#12289;&#38750;&#32534;&#30721;&#21644;&#35843;&#33410;&#20803;&#32032;&#36827;&#34892;&#23454;&#39564;&#27880;&#37322;&#20173;&#26082;&#26114;&#36149;&#21448;&#20855;&#25361;&#25112;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#22240;&#32452;DNA&#36827;&#34892;&#26080;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#30340;&#20852;&#36259;&#65292;&#36825;&#31181;&#33539;&#24335;&#22312;&#34507;&#30333;&#24207;&#21015;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#21508;&#31181;DNA&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#20010;&#21035;&#20316;&#21697;&#20043;&#38388;&#30340;&#35780;&#20272;&#20219;&#21153;&#24448;&#24448;&#19981;&#21516;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#22797;&#21046;&#22522;&#22240;&#32452;&#27880;&#37322;&#30340;&#22522;&#26412;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#30340;&#38271;&#24230;&#12289;&#35268;&#27169;&#21644;&#31232;&#30095;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BEND&#65292;&#19968;&#20010;&#38024;&#23545;DNA&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#23450;&#20041;&#22312;&#20154;&#31867;&#22522;&#22240;&#32452;&#19978;&#30340;&#29616;&#23454;&#21644;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;DNA&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#21487;&#20197;&#25509;&#36817;pe
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12570v3 Announce Type: replace-cross  Abstract: The genome sequence contains the blueprint for governing cellular processes. While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data. Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduce BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. We find that embeddings from current DNA LMs can approach pe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;Epitome&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#31639;&#23376;&#65292;&#20026;PIM&#21152;&#36895;&#22120;&#35774;&#35745;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;CNN&#31639;&#23376;&#65288;EPIM&#65289;&#12290;</title><link>https://arxiv.org/abs/2311.07620</link><description>&lt;p&gt;
EPIM: &#22522;&#20110;Epitome&#30340;&#39640;&#25928;&#22788;&#29702;&#20869;&#23384;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
EPIM: Efficient Processing-In-Memory Accelerators based on Epitome
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;Epitome&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#31639;&#23376;&#65292;&#20026;PIM&#21152;&#36895;&#22120;&#35774;&#35745;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;CNN&#31639;&#23376;&#65288;EPIM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#21152;&#36895;&#22120;&#19978;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#21033;&#29992;&#38754;&#20020;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#29255;&#19978;&#20869;&#23384;&#23481;&#37327;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#27169;&#22411;&#21387;&#32553;&#31639;&#27861;&#65292;&#20197;&#20943;&#23567;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#22823;&#23567;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#31639;&#27861;&#35201;&#20040;&#26088;&#22312;&#29992;&#20855;&#26377;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#30340;&#31070;&#32463;&#31639;&#23376;&#65288;&#20363;&#22914;&#65292;&#37327;&#21270;&#65289;&#34920;&#31034;&#31070;&#32463;&#31639;&#23376;&#65292;&#35201;&#20040;&#23547;&#25214;&#31070;&#32463;&#31639;&#23376;&#30340;&#26368;&#20339;&#32452;&#21512;&#65288;&#20363;&#22914;&#65292;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65289;&#12290;&#35774;&#35745;&#19982;PIM&#21152;&#36895;&#22120;&#35268;&#26684;&#30456;&#19968;&#33268;&#30340;&#31070;&#32463;&#31639;&#23376;&#26159;&#19968;&#20010;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;Epitome&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#31639;&#23376;&#65292;&#25552;&#20379;&#31867;&#20284;&#21367;&#31215;&#21151;&#33021;&#65292;&#20197;&#20026;PIM&#21152;&#36895;&#22120;&#65288;EPIM&#65289;&#35774;&#35745;&#20869;&#23384;&#39640;&#25928;&#30340;CNN&#31639;&#23376;&#12290;&#22312;&#36719;&#20214;&#26041;&#38754;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Epitome&#22312;PIM&#21152;&#36895;&#22120;&#19978;&#30340;&#24310;&#36831;&#21644;&#33021;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;PIM&#24863;&#30693;&#30340;&#36880;&#23618;&#35774;&#35745;&#26041;&#27861;&#26469;&#22686;&#24378;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07620v2 Announce Type: replace-cross  Abstract: The utilization of large-scale neural networks on Processing-In-Memory (PIM) accelerators encounters challenges due to constrained on-chip memory capacity. To tackle this issue, current works explore model compression algorithms to reduce the size of Convolutional Neural Networks (CNNs). Most of these algorithms either aim to represent neural operators with reduced-size parameters (e.g., quantization) or search for the best combinations of neural operators (e.g., neural architecture search). Designing neural operators to align with PIM accelerators' specifications is an area that warrants further study. In this paper, we introduce the Epitome, a lightweight neural operator offering convolution-like functionality, to craft memory-efficient CNN operators for PIM accelerators (EPIM). On the software side, we evaluate epitomes' latency and energy on PIM accelerators and introduce a PIM-aware layer-wise design method to enhance thei
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#21147;&#27969;&#20998;&#26512;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#26377;&#26395;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#22312;&#37327;&#23376;&#35745;&#31639;&#26102;&#20195;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.06293</link><description>&lt;p&gt;
&#29992;&#20110;&#30005;&#21147;&#27969;&#20998;&#26512;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quantum Neural Networks for Power Flow Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06293
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#21147;&#27969;&#20998;&#26512;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#26377;&#26395;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#22312;&#37327;&#23376;&#35745;&#31639;&#26102;&#20195;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#37327;&#23376;&#21644;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#21147;&#27969;&#20998;&#26512;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23454;&#39564;&#20351;&#29992;&#22522;&#20110;4&#33410;&#28857;&#21644;33&#33410;&#28857;&#27979;&#35797;&#31995;&#32479;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#12290;&#36824;&#36827;&#34892;&#20102;&#37327;&#23376;&#12289;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#21644;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#31995;&#32479;&#24615;&#24615;&#33021;&#27604;&#36739;&#12290;&#27604;&#36739;&#22522;&#20110;(i) &#27867;&#21270;&#33021;&#21147;&#65292;(ii) &#40065;&#26834;&#24615;&#65292;(iii) &#38656;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;(iv) &#35757;&#32451;&#35823;&#24046;&#65292;&#21644;(v) &#35757;&#32451;&#36807;&#31243;&#31283;&#23450;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24320;&#21457;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20248;&#20110;&#37327;&#23376;&#21644;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#65292;&#22240;&#27492;&#21487;&#20197;&#25913;&#36827;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30005;&#21147;&#27969;&#20998;&#26512;&#22312;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#21644;&#23481;&#38169;&#37327;&#23376;&#65288;FTQ&#65289;&#26102;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06293v2 Announce Type: replace-cross  Abstract: This paper explores the potential application of quantum and hybrid quantum-classical neural networks in power flow analysis. Experiments are conducted using two datasets based on 4-bus and 33-bus test systems. A systematic performance comparison is also conducted among quantum, hybrid quantum-classical, and classical neural networks. The comparison is based on (i) generalization ability, (ii) robustness, (iii) training dataset size needed, (iv) training error, and (v) training process stability. The results show that the developed hybrid quantum-classical neural network outperforms both quantum and classical neural networks, and hence can improve deep learning-based power flow analysis in the noisy-intermediate-scale quantum (NISQ) and fault-tolerant quantum (FTQ) era.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#21463;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31616;&#21333;&#38543;&#26426;&#37319;&#26679;&#22120;&#22788;&#29702;&#25935;&#24863;&#23646;&#24615;&#65292;&#21487;&#20197;&#26356;&#24191;&#27867;&#22320;&#36866;&#29992;&#20110;&#23454;&#36341;&#20013;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32676;&#20307;&#32423;&#21035;&#22788;&#29702;&#20844;&#24179;&#24863;&#30693;&#30340;&#35757;&#32451;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2311.05866</link><description>&lt;p&gt;
&#20844;&#24179;&#21463;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#31616;&#21333;&#38543;&#26426;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fair Supervised Learning with A Simple Random Sampler of Sensitive Attributes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05866
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#21463;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31616;&#21333;&#38543;&#26426;&#37319;&#26679;&#22120;&#22788;&#29702;&#25935;&#24863;&#23646;&#24615;&#65292;&#21487;&#20197;&#26356;&#24191;&#27867;&#22320;&#36866;&#29992;&#20110;&#23454;&#36341;&#20013;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32676;&#20307;&#32423;&#21035;&#22788;&#29702;&#20844;&#24179;&#24863;&#30693;&#30340;&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#36807;&#31243;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#21464;&#24471;&#20027;&#23548;&#65292;&#21508;&#20010;&#39046;&#22495;&#23545;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#25935;&#24863;&#23646;&#24615;&#30340;&#31616;&#21333;&#38543;&#26426;&#37319;&#26679;&#22120;&#23398;&#20064;&#20844;&#24179;&#24809;&#32602;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#27495;&#35270;&#24615;&#21463;&#30417;&#30563;&#23398;&#20064;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#20316;&#21697;&#22522;&#26412;&#20381;&#36182;&#20110;&#25935;&#24863;&#23646;&#24615;&#21644;&#21709;&#24212;&#21464;&#37327;&#30340;&#31163;&#25955;&#24615;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#24809;&#32602;&#33021;&#22815;&#22788;&#29702;&#22810;&#31181;&#26684;&#24335;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#22240;&#27492;&#22312;&#23454;&#36341;&#20013;&#27604;&#35768;&#22810;&#29616;&#26377;&#31639;&#27861;&#26356;&#20855;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#35813;&#24809;&#32602;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32676;&#20307;&#32423;&#21035;&#22788;&#29702;&#20844;&#24179;&#24863;&#30693;&#30340;&#35757;&#32451;&#26694;&#26550;&#12290;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29992;&#21644;&#20844;&#24179;&#24230;&#37327;&#12290;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#23545;&#20272;&#35745;&#35823;&#24046;&#21644;&#25439;&#22833;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05866v2 Announce Type: replace-cross  Abstract: As the data-driven decision process becomes dominating for industrial applications, fairness-aware machine learning arouses great attention in various areas. This work proposes fairness penalties learned by neural networks with a simple random sampler of sensitive attributes for non-discriminatory supervised learning. In contrast to many existing works that critically rely on the discreteness of sensitive attributes and response variables, the proposed penalty is able to handle versatile formats of the sensitive attributes, so it is more extensively applicable in practice than many existing algorithms. This penalty enables us to build a computationally efficient group-level in-processing fairness-aware training framework. Empirical evidence shows that our framework enjoys better utility and fairness measures on popular benchmark data sets than competing methods. We also theoretically characterize estimation errors and loss of u
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20986;&#20102;LRM&#65292;&#37319;&#29992;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#21644;&#39640;&#23481;&#37327;&#27169;&#22411;&#65292;&#21487;&#22312;&#30701;&#26102;&#38388;&#20869;&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#39044;&#27979;&#39640;&#36136;&#37327;3D&#37325;&#24314;&#32467;&#26524;</title><link>https://arxiv.org/abs/2311.04400</link><description>&lt;p&gt;
LRM&#65306;&#21333;&#22270;&#20687;&#21040;3D&#30340;&#22823;&#22411;&#37325;&#24314;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LRM: Large Reconstruction Model for Single Image to 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04400
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;LRM&#65292;&#37319;&#29992;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#21644;&#39640;&#23481;&#37327;&#27169;&#22411;&#65292;&#21487;&#22312;&#30701;&#26102;&#38388;&#20869;&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#39044;&#27979;&#39640;&#36136;&#37327;3D&#37325;&#24314;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22823;&#22411;&#37325;&#24314;&#27169;&#22411;&#65288;LRM&#65289;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;5&#31186;&#20869;&#20174;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#20013;&#39044;&#27979;&#23545;&#35937;&#30340;3D&#27169;&#22411;&#12290;&#19982;&#35768;&#22810;&#20808;&#21069;&#35757;&#32451;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#22914;ShapeNet&#65289;&#19978;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;LRM&#37319;&#29992;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;5&#20159;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#36755;&#20837;&#22270;&#20687;&#20013;&#39044;&#27979;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#12290;&#25105;&#20204;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#22312;&#21253;&#21547;&#32422;100&#19975;&#20010;&#23545;&#35937;&#30340;&#22823;&#35268;&#27169;&#22810;&#35270;&#35282;&#25968;&#25454;&#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#26469;&#33258;Objaverse&#30340;&#21512;&#25104;&#28210;&#26579;&#21644;&#26469;&#33258;MVImgNet&#30340;&#30495;&#23454;&#25429;&#33719;&#12290;&#36825;&#31181;&#39640;&#23481;&#37327;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#30340;&#32467;&#21512;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#27979;&#35797;&#36755;&#20837;&#20013;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#37325;&#24314;&#32467;&#26524;&#65292;&#21253;&#25324;&#30495;&#23454;&#22330;&#26223;&#25429;&#33719;&#21644;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#30340;&#22270;&#20687;&#12290;&#35270;&#39057;&#28436;&#31034;&#21644;&#21487;&#20132;&#20114;&#30340;3D&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04400v2 Announce Type: replace-cross  Abstract: We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D mes
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26435;&#37325;&#20849;&#20139;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#24809;&#32602;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#22411;&#24182;&#34892;&#31639;&#27861;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#21367;&#31215;&#26679;&#24335;&#30340;&#28388;&#27874;&#22120;</title><link>https://arxiv.org/abs/2311.03096</link><description>&lt;p&gt;
&#26435;&#37325;&#20849;&#20139;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Weight-Sharing Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03096
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26435;&#37325;&#20849;&#20139;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#24809;&#32602;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#22411;&#24182;&#34892;&#31639;&#27861;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#21367;&#31215;&#26679;&#24335;&#30340;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#26435;&#37325;&#20849;&#20139;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;$w \in \mathbb{R}^d$&#36827;&#34892;&#8220;&#26435;&#37325;&#20849;&#20139;&#27491;&#21017;&#21270;&#8221;&#24809;&#32602;&#65292;&#23450;&#20041;&#20026;$\mathcal{R}(w) = \frac{1}{d - 1}\sum_{i &gt; j}^d |w_i - w_j|$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;$\mathcal{R}$&#30340;&#36817;&#31471;&#26144;&#23556;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#29289;&#29702;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#31995;&#32479;&#23545;&#20854;&#36827;&#34892;&#20102;&#30452;&#35266;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#24182;&#34892;&#21270;&#20102;&#29616;&#26377;&#30340;$\operatorname{prox}_\mathcal{R}$&#31639;&#27861;&#65288;&#22312;GPU&#19978;&#36816;&#34892;&#65289;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#19968;&#31181;&#22312;&#23454;&#36341;&#20013;&#24555;&#36895;&#65292;&#20294;&#23545;&#20110;&#26368;&#22351;&#24773;&#20917;&#36755;&#20837;&#36739;&#24930;&#65288;$O(d)$&#65289;&#12290;&#21033;&#29992;&#29289;&#29702;&#35299;&#37322;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#31639;&#27861;&#65292;&#24403;&#26377;&#36275;&#22815;&#30340;&#22788;&#29702;&#22120;&#21487;&#29992;&#26102;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$O(\log^3 d)$&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#24555;&#36895;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#26435;&#37325;&#20849;&#20139;&#27491;&#21017;&#21270;&#20351;&#20840;&#36830;&#25509;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#21367;&#31215;&#26679;&#24335;&#30340;&#28388;&#27874;&#22120;&#65292;&#21363;&#20351;&#20687;&#32032;&#24050;&#34987;&#25171;&#20081;&#65292;&#32780;&#21367;&#31215;&#31070;&#32463;&#32593;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03096v2 Announce Type: replace  Abstract: Weight-sharing is ubiquitous in deep learning. Motivated by this, we propose a "weight-sharing regularization" penalty on the weights $w \in \mathbb{R}^d$ of a neural network, defined as $\mathcal{R}(w) = \frac{1}{d - 1}\sum_{i &gt; j}^d |w_i - w_j|$. We study the proximal mapping of $\mathcal{R}$ and provide an intuitive interpretation of it in terms of a physical system of interacting particles. We also parallelize existing algorithms for $\operatorname{prox}_\mathcal{R}$ (to run on GPU) and find that one of them is fast in practice but slow ($O(d)$) for worst-case inputs. Using the physical interpretation, we design a novel parallel algorithm which runs in $O(\log^3 d)$ when sufficient processors are available, thus guaranteeing fast training. Our experiments reveal that weight-sharing regularization enables fully connected networks to learn convolution-like filters even when pixels have been shuffled while convolutional neural netwo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$B$-sample stochastic dual averaging with the logarithmic barrier&#30340;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35299;&#20915;&#27850;&#26494;&#36870;&#38382;&#39064;&#65292;&#31639;&#27861;&#22312;$\smash{\tilde{O}}(d^2/\varepsilon^2)$&#26102;&#38388;&#20869;&#33719;&#24471;$\varepsilon$-&#26368;&#20248;&#35299;&#65292;&#19982;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2311.02557</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#23545;&#20598;&#24179;&#22343;&#24555;&#36895;&#26368;&#23567;&#21270;&#26399;&#26395;&#23545;&#25968;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Fast Minimization of Expected Logarithmic Loss via Stochastic Dual Averaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02557
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$B$-sample stochastic dual averaging with the logarithmic barrier&#30340;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35299;&#20915;&#27850;&#26494;&#36870;&#38382;&#39064;&#65292;&#31639;&#27861;&#22312;$\smash{\tilde{O}}(d^2/\varepsilon^2)$&#26102;&#38388;&#20869;&#33719;&#24471;$\varepsilon$-&#26368;&#20248;&#35299;&#65292;&#19982;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#22312;&#27010;&#29575;&#21333;&#32431;&#24418;&#25110;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#38598;&#21512;&#19978;&#26368;&#23567;&#21270;&#26399;&#26395;&#23545;&#25968;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#21253;&#25324;&#35299;&#20915;&#27850;&#26494;&#36870;&#38382;&#39064;&#12289;&#35745;&#31639;&#37327;&#23376;&#24577;&#27979;&#37327;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#20197;&#21450;&#29992;&#24403;&#21069;&#26368;&#32039;&#23494;&#36924;&#36817;&#27604;&#29575;&#36817;&#20284;&#27491;&#21322;&#23450;&#30697;&#38453;&#27704;&#20037;&#20540;&#31561;&#20219;&#21153;&#12290;&#23613;&#31649;&#20248;&#21270;&#38382;&#39064;&#26159;&#20984;&#30340;&#65292;&#20294;&#30001;&#20110;&#25439;&#22833;&#20989;&#25968;&#32570;&#20047;Lipschitz&#36830;&#32493;&#24615;&#21644;&#20809;&#28369;&#24615;&#65292;&#26631;&#20934;&#30340;&#19968;&#38454;&#26041;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02557v2 Announce Type: replace-cross  Abstract: Consider the problem of minimizing an expected logarithmic loss over either the probability simplex or the set of quantum density matrices. This problem includes tasks such as solving the Poisson inverse problem, computing the maximum-likelihood estimate for quantum state tomography, and approximating positive semi-definite matrix permanents with the currently tightest approximation ratio. Although the optimization problem is convex, standard iteration complexity guarantees for first-order methods do not directly apply due to the absence of Lipschitz continuity and smoothness in the loss function.   In this work, we propose a stochastic first-order algorithm named $B$-sample stochastic dual averaging with the logarithmic barrier. For the Poisson inverse problem, our algorithm attains an $\varepsilon$-optimal solution in $\smash{\tilde{O}}(d^2/\varepsilon^2)$ time, matching the state of the art, where $d$ denotes the dimension. 
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#36890;&#24120;&#20855;&#26377;&#26080;&#38480;&#31181;&#21512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20256;&#32479;&#30340;&#21518;&#39564;&#37319;&#26679;&#29983;&#25104;&#30340;&#22810;&#26679;&#36755;&#20986;&#21463;&#21040;&#23614;&#37096;&#25928;&#24212;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#24847;&#20041;&#30340;&#22810;&#26679;&#24615;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2310.16047</link><description>&lt;p&gt;
&#20174;&#21518;&#39564;&#37319;&#26679;&#21040;&#22270;&#20687;&#24674;&#22797;&#20013;&#26377;&#24847;&#20041;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
From Posterior Sampling to Meaningful Diversity in Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.16047
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#36890;&#24120;&#20855;&#26377;&#26080;&#38480;&#31181;&#21512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20256;&#32479;&#30340;&#21518;&#39564;&#37319;&#26679;&#29983;&#25104;&#30340;&#22810;&#26679;&#36755;&#20986;&#21463;&#21040;&#23614;&#37096;&#25928;&#24212;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#24847;&#20041;&#30340;&#22810;&#26679;&#24615;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#36890;&#24120;&#26159;&#19981;&#36866;&#23450;&#30340;&#65292;&#22240;&#20026;&#27599;&#20010;&#36864;&#21270;&#22270;&#20687;&#37117;&#21487;&#20197;&#20197;&#26080;&#38480;&#26377;&#25928;&#30340;&#26041;&#24335;&#24674;&#22797;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#28857;&#65292;&#35768;&#22810;&#20316;&#21697;&#35797;&#22270;&#20174;&#32473;&#23450;&#36864;&#21270;&#36755;&#20837;&#30340;&#33258;&#28982;&#22270;&#20687;&#21518;&#39564;&#20998;&#24067;&#20013;&#38543;&#26426;&#37319;&#26679;&#65292;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#31574;&#30053;&#26222;&#36941;&#20855;&#26377;&#26377;&#38480;&#30340;&#23454;&#29992;&#20215;&#20540;&#65292;&#22240;&#20026;&#21518;&#39564;&#20998;&#24067;&#30340;&#23614;&#37096;&#36739;&#37325;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#20462;&#22797;&#22270;&#20687;&#20013;&#32570;&#22833;&#30340;&#22825;&#31354;&#21306;&#22495;&#12290;&#30001;&#20110;&#32570;&#22833;&#21306;&#22495;&#24456;&#21487;&#33021;&#19981;&#21253;&#21547;&#29289;&#20307;&#65292;&#21482;&#26377;&#20113;&#65292;&#20219;&#20309;&#26469;&#33258;&#21518;&#39564;&#30340;&#26679;&#26412;&#38598;&#37117;&#23558;&#23436;&#20840;&#34987;&#22825;&#31354;&#30340;&#65288;&#23454;&#38469;&#19978;&#30456;&#21516;&#30340;&#65289;&#20462;&#22797;&#25152;&#20027;&#23548;&#12290;&#28982;&#32780;&#65292;&#21487;&#20197;&#35748;&#20026;&#21521;&#29992;&#25143;&#21576;&#29616;&#19968;&#20010;&#28165;&#26224;&#30340;&#22825;&#31354;&#20462;&#22797;&#20197;&#21450;&#20960;&#31181;&#26367;&#20195;&#35299;&#20915;&#26041;&#26696;&#65288;&#20363;&#22914;&#39134;&#33351;&#12289;&#40479;&#31867;&#21644;&#27668;&#29699;&#65289;&#23558;&#26356;&#22909;&#22320;&#27010;&#36848;&#21487;&#33021;&#30340;&#36873;&#25321;&#38598;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.16047v2 Announce Type: replace-cross  Abstract: Image restoration problems are typically ill-posed in the sense that each degraded image can be restored in infinitely many valid ways. To accommodate this, many works generate a diverse set of outputs by attempting to randomly sample from the posterior distribution of natural images given the degraded input. Here we argue that this strategy is commonly of limited practical value because of the heavy tail of the posterior distribution. Consider for example inpainting a missing region of the sky in an image. Since there is a high probability that the missing region contains no object but clouds, any set of samples from the posterior would be entirely dominated by (practically identical) completions of sky. However, arguably, presenting users with only one clear sky completion, along with several alternative solutions such as airships, birds, and balloons, would better outline the set of possibilities. In this paper, we initiate 
&lt;/p&gt;</description></item><item><title>Bongard-OpenWorld&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#36827;&#34892;&#23569;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#24320;&#25918;&#19990;&#30028;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#20004;&#39033;&#26032;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2310.10207</link><description>&lt;p&gt;
Bongard-OpenWorld: &#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10207
&lt;/p&gt;
&lt;p&gt;
Bongard-OpenWorld&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#36827;&#34892;&#23569;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#24320;&#25918;&#19990;&#30028;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#20004;&#39033;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bongard-OpenWorld&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#30495;&#23454;&#19990;&#30028;&#23569;&#26679;&#26412;&#25512;&#29702;&#30340;&#26032;&#22522;&#20934;&#12290; &#23427;&#28304;&#33258;&#32463;&#20856;&#30340;Bongard&#38382;&#39064;&#65288;BPs&#65289;&#65306;&#32473;&#23450;&#20004;&#32452;&#22270;&#20687;&#65288;&#27491;&#21644;&#36127;&#65289;&#65292;&#27169;&#22411;&#38656;&#35201;&#36890;&#36807;&#35825;&#23548;&#35270;&#35273;&#27010;&#24565;&#26469;&#30830;&#23450;&#26597;&#35810;&#22270;&#20687;&#25152;&#23646;&#30340;&#22270;&#20687;&#38598;&#65292;&#36825;&#20123;&#27010;&#24565;&#20165;&#30001;&#27491;&#38598;&#20013;&#30340;&#22270;&#20687;&#25152;&#25551;&#36848;&#12290; &#25105;&#20204;&#30340;&#22522;&#20934;&#32487;&#25215;&#20102;&#21407;&#22987;BPs&#30340;&#23569;&#26679;&#26412;&#27010;&#24565;&#24402;&#32435;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#20004;&#23618;&#26032;&#25361;&#25112;&#65306;1&#65289;&#24320;&#25918;&#19990;&#30028;&#30340;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#65292;&#22240;&#20026;Bongard-OpenWorld&#20013;&#30340;&#35270;&#35273;&#27010;&#24565;&#26159;&#20174;&#24320;&#25918;&#35789;&#27719;&#34920;&#20013;&#29420;&#29305;&#32452;&#21512;&#30340;&#26415;&#35821;&#65292;&#33539;&#22260;&#20174;&#23545;&#35937;&#31867;&#21035;&#21040;&#25277;&#35937;&#35270;&#35273;&#23646;&#24615;&#21644;&#24120;&#35782;&#20107;&#23454;&#30693;&#35782;&#65307; 2&#65289;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#65292;&#32780;&#19981;&#26159;&#35768;&#22810;&#31867;&#20284;&#29289;&#20351;&#29992;&#30340;&#21512;&#25104;&#22270;&#34920;&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;Bongard-OpenWorld&#24050;&#32463;&#23545;&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#31639;&#27861;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36828;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10207v2 Announce Type: replace  Abstract: We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We furt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#21387;&#32553;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#26041;&#27861; Byz-DASHA-PAGE&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#38750;&#20984;&#21644;Polyak-Lojasiewicz&#24179;&#28369;&#20248;&#21270;&#38382;&#39064;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#22312;&#24322;&#26500;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#23567;&#30340;&#37051;&#22495;&#22823;&#23567;&#65292;&#20197;&#21450;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#33021;&#23481;&#24525;&#26356;&#22810;&#30340;&#25308;&#21344;&#24237;&#24037;&#20316;&#32773;&#12290;</title><link>https://arxiv.org/abs/2310.09804</link><description>&lt;p&gt;
&#25308;&#21344;&#24237;&#40065;&#26834;&#23398;&#20064;&#30340;&#36890;&#20449;&#21387;&#32553;&#65306;&#26032;&#39640;&#25928;&#31639;&#27861;&#21644;&#25913;&#36827;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Communication Compression for Byzantine Robust Learning: New Efficient Algorithms and Improved Rates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#21387;&#32553;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#26041;&#27861; Byz-DASHA-PAGE&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#38750;&#20984;&#21644;Polyak-Lojasiewicz&#24179;&#28369;&#20248;&#21270;&#38382;&#39064;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#22312;&#24322;&#26500;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#23567;&#30340;&#37051;&#22495;&#22823;&#23567;&#65292;&#20197;&#21450;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#33021;&#23481;&#24525;&#26356;&#22810;&#30340;&#25308;&#21344;&#24237;&#24037;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25308;&#21344;&#24237;&#40065;&#26834;&#24615;&#26159;&#26576;&#20123;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#31639;&#27861;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#36890;&#24120;&#20986;&#29616;&#22312;&#21327;&#20316;/&#32852;&#37030;&#23398;&#20064;&#20013;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#35268;&#27169;&#24040;&#22823;&#65292;&#22240;&#27492;&#36890;&#20449;&#21387;&#32553;&#23545;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#22240;&#32032;&#25512;&#21160;&#20102;&#26368;&#36817;&#22312;&#25308;&#21344;&#24237;&#40065;&#26834;&#23398;&#20064;&#19982;&#21387;&#32553;&#39046;&#22495;&#30340;&#31639;&#27861;&#21644;&#29702;&#35770;&#21457;&#23637;&#12290;&#26412;&#25991;&#22312;&#20004;&#20010;&#20027;&#35201;&#26041;&#21521;&#19978;&#20026;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#21387;&#32553;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#26041;&#27861; - Byz-DASHA-PAGE&#65292;&#24182;&#35777;&#26126;&#35813;&#26032;&#26041;&#27861;&#22312;&#38750;&#20984;&#21644;Polyak-Lojasiewicz&#24179;&#28369;&#20248;&#21270;&#38382;&#39064;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#22312;&#24322;&#26500;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#23567;&#30340;&#37051;&#22495;&#22823;&#23567;&#65292;&#24182;&#19988;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#23481;&#24525;&#26356;&#22810;&#30340;&#25308;&#21344;&#24237;&#24037;&#20316;&#32773;&#65292;&#32988;&#36807;&#20855;&#26377;SOTA&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#30340;&#20808;&#21069;&#26041;&#27861;&#65288;Byz-VR-MARINA&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09804v2 Announce Type: replace-cross  Abstract: Byzantine robustness is an essential feature of algorithms for certain distributed optimization problems, typically encountered in collaborative/federated learning. These problems are usually huge-scale, implying that communication compression is also imperative for their resolution. These factors have spurred recent algorithmic and theoretical developments in the literature of Byzantine-robust learning with compression. In this paper, we contribute to this research area in two main directions. First, we propose a new Byzantine-robust method with compression - Byz-DASHA-PAGE - and prove that the new method has better convergence rate (for non-convex and Polyak-Lojasiewicz smooth optimization problems), smaller neighborhood size in the heterogeneous case, and tolerates more Byzantine workers under over-parametrization than the previous method with SOTA theoretical convergence guarantees (Byz-VR-MARINA). Secondly, we develop the 
&lt;/p&gt;</description></item><item><title>Prometheus&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#35780;&#20998;&#26631;&#20934;&#21644;&#36866;&#24403;&#30340;&#21442;&#32771;&#26448;&#26009;&#65292;&#21487;&#20197;&#22312;&#19982;GPT-4&#30456;&#23218;&#32654;&#30340;&#35780;&#20272;&#33021;&#21147;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2310.08491</link><description>&lt;p&gt;
Prometheus: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#32454;&#31890;&#24230;&#35780;&#20272;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Prometheus: Inducing Fine-grained Evaluation Capability in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08491
&lt;/p&gt;
&lt;p&gt;
Prometheus&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#35780;&#20998;&#26631;&#20934;&#21644;&#36866;&#24403;&#30340;&#21442;&#32771;&#26448;&#26009;&#65292;&#21487;&#20197;&#22312;&#19982;GPT-4&#30456;&#23218;&#32654;&#30340;&#35780;&#20272;&#33021;&#21147;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#19987;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-4&#65289;&#20316;&#20026;&#38271;&#31687;&#22238;&#31572;&#30340;&#35780;&#20272;&#32773;&#24050;&#32463;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#22823;&#35268;&#27169;&#35780;&#20272;&#20219;&#21153;&#21644;&#32771;&#34385;&#33258;&#23450;&#20041;&#26631;&#20934;&#65288;&#20363;&#22914;&#20799;&#31461;&#21487;&#35835;&#24615;&#65289;&#30340;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#20351;&#29992;&#19987;&#26377;LLMs&#20316;&#20026;&#35780;&#20272;&#32773;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#38381;&#28304;&#24615;&#36136;&#12289;&#26080;&#25511;&#21046;&#30340;&#29256;&#26412;&#21644;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prometheus&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#24320;&#28304;&#30340;LLM&#65292;&#21482;&#35201;&#25658;&#24102;&#36866;&#24403;&#30340;&#21442;&#32771;&#26448;&#26009;&#65288;&#21442;&#32771;&#31572;&#26696;&#12289;&#35780;&#20998;&#26631;&#20934;&#65289;&#65292;&#23601;&#21487;&#20197;&#19982;GPT-4&#30340;&#35780;&#20272;&#33021;&#21147;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#21453;&#39304;&#25910;&#38598;&#65288;Feedback Collection&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;1K&#20010;&#32454;&#31890;&#24230;&#35780;&#20998;&#26631;&#20934;&#12289;20K&#26465;&#25351;&#20196;&#21644;GPT-4&#29983;&#25104;&#30340;100K&#26465;&#21709;&#24212;&#21644;&#35821;&#35328;&#21453;&#39304;&#32452;&#25104;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#36825;&#20010;&#21453;&#39304;&#25910;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;Prometheus&#65292;&#19968;&#20010;13B&#30340;&#35780;&#20272;&#32773;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#23450;&#21046;&#30340;&#35780;&#20998;&#26631;&#20934;&#35780;&#20272;&#20219;&#20309;&#32473;&#23450;&#30340;&#38271;&#31687;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08491v2 Announce Type: replace  Abstract: Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric
&lt;/p&gt;</description></item><item><title>AutoVP&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#35270;&#35273;&#25552;&#31034;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20379;12&#20010;&#19979;&#28216;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20316;&#20026;&#22522;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;AutoVP&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#24050;&#30693;&#30340;&#26368;&#20339;VP&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2310.08381</link><description>&lt;p&gt;
AutoVP: &#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#35270;&#35273;&#25552;&#31034;&#26694;&#26550;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AutoVP: An Automated Visual Prompting Framework and Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08381
&lt;/p&gt;
&lt;p&gt;
AutoVP&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#35270;&#35273;&#25552;&#31034;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20379;12&#20010;&#19979;&#28216;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20316;&#20026;&#22522;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;AutoVP&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#24050;&#30693;&#30340;&#26368;&#20339;VP&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25552;&#31034;&#65288;VP&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#20197;&#35299;&#20915;&#21508;&#31181;&#19979;&#28216;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#23545;VP&#30340;&#35774;&#35745;&#31354;&#38388;&#32570;&#20047;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#24182;&#27809;&#26377;&#26126;&#30830;&#30340;&#29992;&#20110;&#35780;&#20272;&#20854;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoVP&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;VP&#35774;&#35745;&#36873;&#25321;&#65292;&#20197;&#21450;12&#20010;&#19979;&#28216;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#20840;&#38754;&#30340;VP&#24615;&#33021;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#31354;&#38388;&#35206;&#30422;&#20102;1&#65289;&#25552;&#31034;&#30340;&#32852;&#21512;&#20248;&#21270;&#65307;2&#65289;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36873;&#25321;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;-&#22270;&#20687;&#32534;&#30721;&#22120;&#65307;&#20197;&#21450;3&#65289;&#27169;&#22411;&#36755;&#20986;&#26144;&#23556;&#31574;&#30053;&#65292;&#21253;&#25324;&#38750;&#21442;&#25968;&#21270;&#21644;&#21487;&#35757;&#32451;&#30340;&#26631;&#31614;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AutoVP&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#24050;&#30693;&#30340;&#26368;&#20339;VP&#26041;&#27861;&#65292;&#31934;&#24230;&#25552;&#39640;&#39640;&#36798;6.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08381v2 Announce Type: replace-cross  Abstract: Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks. However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its performance. To bridge this gap, we propose AutoVP, an end-to-end expandable framework for automating VP design choices, along with 12 downstream image-classification tasks that can serve as a holistic VP-performance benchmark. Our design space covers 1) the joint optimization of the prompts; 2) the selection of pre-trained models, including image classifiers and text-image encoders; and 3) model output mapping strategies, including nonparametric and trainable label mapping. Our extensive experimental results show that AutoVP outperforms the best-known current VP methods by a substantial margin, having up to 6.7% improvement in accuracy
&lt;/p&gt;</description></item><item><title>iTransformer&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;Transformer&#26550;&#26500;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#31616;&#21333;&#24212;&#29992;&#27880;&#24847;&#21147;&#21644;&#39304;&#36865;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#20811;&#26381;&#20102;&#20854;&#20182;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#26356;&#22823;&#22238;&#28335;&#31383;&#21475;&#30340;&#31995;&#21015;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2310.06625</link><description>&lt;p&gt;
iTransformer: &#21453;&#36716;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#26377;&#25928;&#30340;
&lt;/p&gt;
&lt;p&gt;
iTransformer: Inverted Transformers Are Effective for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06625
&lt;/p&gt;
&lt;p&gt;
iTransformer&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;Transformer&#26550;&#26500;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#31616;&#21333;&#24212;&#29992;&#27880;&#24847;&#21147;&#21644;&#39304;&#36865;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#20811;&#26381;&#20102;&#20854;&#20182;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#26356;&#22823;&#22238;&#28335;&#31383;&#21475;&#30340;&#31995;&#21015;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#32447;&#24615;&#39044;&#27979;&#27169;&#22411;&#30340;&#20852;&#36215;&#23545;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#22120;&#30340;&#26550;&#26500;&#20462;&#25913;&#30340;&#25345;&#32493;&#28909;&#24773;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#36825;&#20123;&#39044;&#27979;&#22120;&#21033;&#29992;Transformer&#26469;&#27169;&#25311;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#26631;&#35760;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#27599;&#20010;&#26102;&#38388;&#26631;&#35760;&#30001;&#30456;&#21516;&#26102;&#38388;&#25139;&#30340;&#22810;&#20010;&#21464;&#37327;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24615;&#33021;&#19979;&#38477;&#21644;&#35745;&#31639;&#29190;&#28856;&#65292;Transformer&#22312;&#39044;&#27979;&#20855;&#26377;&#26356;&#22823;&#22238;&#28335;&#31383;&#21475;&#30340;&#31995;&#21015;&#26102;&#21463;&#21040;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#26102;&#38388;&#26631;&#35760;&#30340;&#23884;&#20837;&#34701;&#21512;&#20102;&#20195;&#34920;&#28508;&#22312;&#24310;&#36831;&#20107;&#20214;&#21644;&#19981;&#21516;&#29289;&#29702;&#27979;&#37327;&#30340;&#22810;&#20010;&#21464;&#37327;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26080;&#27861;&#23398;&#20064;&#21464;&#37327;-centric&#34920;&#31034;&#24182;&#23548;&#33268;&#26080;&#24847;&#20041;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21453;&#24605;&#20102;Transformer&#32452;&#20214;&#30340;&#33021;&#21147;&#65292;&#24182;&#37325;&#26032;&#21033;&#29992;&#20102;Transformer&#26550;&#26500;&#65292;&#32780;&#27809;&#26377;&#20462;&#25913;&#22522;&#26412;&#32452;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;iTransformer&#65292;&#23427;&#31616;&#21333;&#22320;&#24212;&#29992;&#20102;&#27880;&#24847;&#21147;&#21644;&#39304;&#36865;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06625v3 Announce Type: replace  Abstract: The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#39118;&#38505;&#36866;&#24212;&#24615;&#35843;&#25972;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21160;&#24577;&#36873;&#25321;&#35748;&#30693;&#39118;&#38505;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2310.05179</link><description>&lt;p&gt;
&#20855;&#26377;&#22312;&#32447;&#39118;&#38505;&#24863;&#30693;&#36866;&#24212;&#24615;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning with Online Risk-awareness Adaption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#39118;&#38505;&#36866;&#24212;&#24615;&#35843;&#25972;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21160;&#24577;&#36873;&#25321;&#35748;&#30693;&#39118;&#38505;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38656;&#35201;&#32771;&#34385;&#27425;&#20248;&#32467;&#26524;&#65292;&#36825;&#21462;&#20915;&#20110;&#20195;&#29702;&#20154;&#23545;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#29087;&#24713;&#31243;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;Distributional RL with Online Risk Adaption&#65288;DRL-ORA&#65289;&#65292;&#21487;&#20197;&#32508;&#21512;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#24182;&#21160;&#24577;&#36873;&#25321;&#35748;&#30693;&#39118;&#38505;&#27700;&#24179;&#65292;&#36890;&#36807;&#22312;&#32447;&#35299;&#20915;&#24635;&#21464;&#24046;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#39118;&#38505;&#27700;&#24179;&#36873;&#25321;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;Follow-The-Leader&#31867;&#22411;&#31639;&#27861;&#36827;&#34892;&#32593;&#26684;&#25628;&#32034;&#26469;&#26377;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05179v2 Announce Type: replace  Abstract: The use of reinforcement learning (RL) in practical applications requires considering sub-optimal outcomes, which depend on the agent's familiarity with the uncertain environment. Dynamically adjusting the level of epistemic risk over the course of learning can tactically achieve reliable optimal policy in safety-critical environments and tackle the sub-optimality of a static risk level. In this work, we introduce a novel framework, Distributional RL with Online Risk Adaption (DRL-ORA), which can quantify the aleatory and epistemic uncertainties compositely and dynamically select the epistemic risk levels via solving a total variation minimization problem online. The risk level selection can be efficiently achieved through grid search using a Follow-The-Leader type algorithm, and its offline oracle is related to "satisficing measure" (in the decision analysis community) under a special modification of the loss function. We show multi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#32437;&#21521;&#30740;&#31350;&#24052;&#35199;&#24635;&#32479;&#20505;&#36873;&#20154;&#22312;Instagram&#19978;&#21457;&#24067;&#30340;&#24086;&#23376;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#25919;&#27835;&#20256;&#25773;&#20013;&#22266;&#23450;&#30340;&#27169;&#24335;&#65292;&#21253;&#25324;&#24198;&#31069;&#21644;&#31215;&#26497;&#35843;&#24615;&#22270;&#20687;&#30340;&#26222;&#36941;&#20351;&#29992;&#20197;&#21450;&#20505;&#36873;&#20154;&#19982;&#36873;&#27665;&#32039;&#23494;&#32852;&#31995;&#30340;&#20010;&#24615;&#21270;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2310.00349</link><description>&lt;p&gt;
&#26497;&#21270;&#31038;&#20250;&#20013;&#30340;&#35270;&#35273;&#25919;&#27835;&#20256;&#25773;: &#24052;&#35199;&#24635;&#32479;&#36873;&#20030;&#22312;Instagram&#19978;&#30340;&#32437;&#21521;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Visual Political Communication in a Polarized Society: A Longitudinal Study of Brazilian Presidential Elections on Instagram
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#32437;&#21521;&#30740;&#31350;&#24052;&#35199;&#24635;&#32479;&#20505;&#36873;&#20154;&#22312;Instagram&#19978;&#21457;&#24067;&#30340;&#24086;&#23376;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#25919;&#27835;&#20256;&#25773;&#20013;&#22266;&#23450;&#30340;&#27169;&#24335;&#65292;&#21253;&#25324;&#24198;&#31069;&#21644;&#31215;&#26497;&#35843;&#24615;&#22270;&#20687;&#30340;&#26222;&#36941;&#20351;&#29992;&#20197;&#21450;&#20505;&#36873;&#20154;&#19982;&#36873;&#27665;&#32039;&#23494;&#32852;&#31995;&#30340;&#20010;&#24615;&#21270;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#26102;&#20195;&#65292;&#22270;&#20687;&#24050;&#32463;&#25104;&#20026;&#25919;&#27835;&#20154;&#22763;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#19982;&#36873;&#27665;&#20114;&#21160;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#35270;&#35273;&#20869;&#23481;&#20855;&#26377;&#29420;&#29305;&#30340;&#24773;&#24863;&#21560;&#24341;&#21147;&#65292;&#24448;&#24448;&#23548;&#33268;&#29992;&#25143;&#21442;&#19982;&#24230;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#23545;&#35270;&#35273;&#20256;&#25773;&#30340;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#26377;&#38480;&#65292;&#23588;&#20854;&#26159;&#22312;&#20840;&#29699;&#21335;&#26041;&#22269;&#23478;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#35745;&#31639;&#26041;&#27861;&#21644;&#23450;&#24615;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;2018&#24180;&#21644;2022&#24180;19&#20301;&#24052;&#35199;&#24635;&#32479;&#20505;&#36873;&#20154;&#22312;Instagram&#19978;&#21457;&#24067;&#30340;11,263&#26465;&#24086;&#23376;&#20013;&#37319;&#29992;&#30340;&#35270;&#35273;&#20256;&#25773;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#20004;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#20505;&#36873;&#20154;&#22312;&#35270;&#35273;&#25919;&#27835;&#20256;&#25773;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#27169;&#24335;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24198;&#31069;&#21644;&#31215;&#26497;&#35843;&#24615;&#22270;&#20687;&#30340;&#26222;&#36941;&#24615;&#12290;&#20182;&#20204;&#36824;&#23637;&#31034;&#20102;&#24378;&#28872;&#30340;&#20010;&#24615;&#21270;&#24863;&#65292;&#25551;&#32472;&#20505;&#36873;&#20154;&#19982;&#36873;&#27665;&#26356;&#32039;&#23494;&#30456;&#36830;&#30340;&#24418;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00349v2 Announce Type: replace-cross  Abstract: In today's digital age, images have emerged as powerful tools for politicians to engage with their voters on social media platforms. Visual content possesses a unique emotional appeal that often leads to increased user engagement. However, research on visual communication remains relatively limited, particularly in the Global South. This study aims to bridge this gap by employing a combination of computational methods and qualitative approach to investigate the visual communication strategies employed in a dataset of 11,263 Instagram posts by 19 Brazilian presidential candidates in 2018 and 2022 national elections. Through two studies, we observed consistent patterns across these candidates on their use of visual political communication. Notably, we identify a prevalence of celebratory and positively toned images. They also exhibit a strong sense of personalization, portraying candidates connected with their voters on a more em
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#32479;&#19968;&#21644;&#27867;&#21270;&#20102;&#21508;&#31181;&#22522;&#20110;&#27010;&#29575;&#25512;&#26029;&#30340;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;&#20248;&#21270;&#30340;&#35268;&#21010;&#22120;&#36830;&#25509;&#36215;&#26469;</title><link>https://arxiv.org/abs/2309.00854</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#39640;&#26031;&#36807;&#31243;&#36816;&#21160;&#35268;&#21010;&#21464;&#20998;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unifying Variational Framework for Gaussian Process Motion Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#32479;&#19968;&#21644;&#27867;&#21270;&#20102;&#21508;&#31181;&#22522;&#20110;&#27010;&#29575;&#25512;&#26029;&#30340;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;&#20248;&#21270;&#30340;&#35268;&#21010;&#22120;&#36830;&#25509;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#65292;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#24517;&#39035;&#35745;&#31639;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#65292;&#21516;&#26102;&#32771;&#34385;&#19982;&#30005;&#26426;&#21644;&#20851;&#33410;&#30456;&#20851;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#29983;&#25104;&#24179;&#31283;&#31283;&#23450;&#30340;&#36816;&#21160;&#65292;&#36991;&#24320;&#38556;&#30861;&#29289;&#65292;&#24182;&#38450;&#27490;&#30896;&#25758;&#12290;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#24517;&#39035;&#24179;&#34913;&#31454;&#20105;&#38656;&#27714;&#65292;&#24182;&#19988;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#21253;&#25324;&#19981;&#30830;&#23450;&#24615;&#20197;&#22788;&#29702;&#22122;&#22768;&#12289;&#27169;&#22411;&#38169;&#35823;&#65292;&#24182;&#20419;&#36827;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#23427;&#32479;&#19968;&#21644;&#27867;&#21270;&#20102;&#21508;&#31181;&#22522;&#20110;&#27010;&#29575;&#25512;&#26029;&#30340;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#22522;&#20110;&#20248;&#21270;&#30340;&#35268;&#21010;&#22120;&#36830;&#25509;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#22312;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#20013;&#25972;&#21512;&#22522;&#20110;&#30456;&#31561;&#24615;&#12289;&#22522;&#20110;&#19981;&#31561;&#24335;&#24615;&#21644;&#36719;&#24615;&#36816;&#21160;&#35268;&#21010;&#32422;&#26463;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;bo
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00854v2 Announce Type: replace-cross  Abstract: To control how a robot moves, motion planning algorithms must compute paths in high-dimensional state spaces while accounting for physical constraints related to motors and joints, generating smooth and stable motions, avoiding obstacles, and preventing collisions. A motion planning algorithm must therefore balance competing demands, and should ideally incorporate uncertainty to handle noise, model errors, and facilitate deployment in complex environments. To address these issues, we introduce a framework for robot motion planning based on variational Gaussian processes, which unifies and generalizes various probabilistic-inference-based motion planning algorithms, and connects them with optimization-based planners. Our framework provides a principled and flexible way to incorporate equality-based, inequality-based, and soft motion-planning constraints during end-to-end training, is straightforward to implement, and provides bo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#30340;&#26426;&#22120;&#36951;&#24536;&#27010;&#24565;&#65292;&#22312;&#36127;&#33655;&#39044;&#27979;&#20013;&#24212;&#29992;&#35813;&#27010;&#24565;&#21487;&#20197;&#24179;&#34913;&#36951;&#24536;&#23436;&#25972;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2308.14412</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#26426;&#22120;&#36951;&#24536;&#21450;&#20854;&#22312;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Task-Aware Machine Unlearning and Its Application in Load Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14412
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#30340;&#26426;&#22120;&#36951;&#24536;&#27010;&#24565;&#65292;&#22312;&#36127;&#33655;&#39044;&#27979;&#20013;&#24212;&#29992;&#35813;&#27010;&#24565;&#21487;&#20197;&#24179;&#34913;&#36951;&#24536;&#23436;&#25972;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24050;&#25104;&#20026;&#36127;&#36733;&#39044;&#27979;&#20013;&#19968;&#20010;&#19981;&#21487;&#24573;&#35270;&#30340;&#22240;&#32032;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35757;&#32451;&#38454;&#27573;&#30340;&#22686;&#24378;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#27169;&#22411;&#34987;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#22914;&#26524;&#21457;&#29616;&#36825;&#20123;&#25968;&#25454;&#26159;&#24694;&#24847;&#30340;&#25110;&#34987;&#25968;&#25454;&#25152;&#26377;&#32773;&#35201;&#27714;&#65292;&#21487;&#33021;&#38656;&#35201;&#8220;&#36951;&#24536;&#8221;&#65288;&#21363;&#21435;&#38500;&#65289;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#27010;&#24565;&#65292;&#20854;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#28040;&#38500;&#24050;&#32463;&#35757;&#32451;&#30340;&#39044;&#27979;&#22120;&#19978;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#36951;&#24536;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#36951;&#24536;&#30340;&#23436;&#25972;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24615;&#33021;&#24863;&#30693;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21644;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#26469;&#35780;&#20272;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#32479;&#35745;&#20934;&#21017;&#65288;&#22914;&#22343;&#26041;&#35823;&#24046;&#65289;&#26080;&#27861;&#23436;&#20840;&#21453;&#26144;&#25805;&#20316;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14412v2 Announce Type: replace  Abstract: Data privacy and security have become a non-negligible factor in load forecasting. Previous researches mainly focus on training stage enhancement. However, once the model is trained and deployed, it may need to `forget' (i.e., remove the impact of) part of training data if the these data are found to be malicious or as requested by the data owner. This paper introduces the concept of machine unlearning which is specifically designed to remove the influence of part of the dataset on an already trained forecaster. However, direct unlearning inevitably degrades the model generalization ability. To balance between unlearning completeness and model performance, a performance-aware algorithm is proposed by evaluating the sensitivity of local model parameter change using influence function and sample re-weighting. Furthermore, we observe that the statistical criterion such as mean squared error, cannot fully reflect the operation cost of th
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#23545;&#25239;&#24694;&#24847;&#23458;&#25143;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2307.00543</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22359;&#38142;&#38450;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Defending Against Malicious Behaviors in Federated Learning with Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#23545;&#25239;&#24694;&#24847;&#23458;&#25143;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#65292;&#32852;&#37030;&#23398;&#20064;(FL)&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#22810;&#23478;&#26426;&#26500;&#25968;&#25454;&#25152;&#26377;&#32773;&#25110;&#23458;&#25143;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FL&#26041;&#27861;&#20381;&#36182;&#20110;&#29992;&#20110;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#30340;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#65292;&#23548;&#33268;&#21333;&#28857;&#25925;&#38556;&#12290;&#36825;&#20351;&#31995;&#32479;&#22312;&#22788;&#29702;&#19981;&#35802;&#23454;&#30340;&#23458;&#25143;&#26102;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21487;&#38752;FL&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#32467;&#21512;&#20102;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#30001;&#38142;&#19978;&#26234;&#33021;&#21512;&#32422;&#25552;&#20379;&#21160;&#21147;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#24694;&#24847;&#23458;&#25143;&#26159;&#24378;&#22823;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00543v2 Announce Type: replace-cross  Abstract: In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20849;&#35782;&#27010;&#24565;&#23454;&#29616;&#20102;&#21512;&#20316;&#21644;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#20805;&#20998;&#23637;&#31034;&#20102;&#21512;&#20316;&#23545;&#20110;&#26368;&#20248;&#35774;&#35745;&#30340;&#37325;&#35201;&#20316;&#29992;</title><link>https://arxiv.org/abs/2306.14348</link><description>&lt;p&gt;
&#21512;&#20316;&#21644;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270; via &#20849;&#35782;: &#23637;&#31034;&#21512;&#20316;&#30340;&#21147;&#37327;&#29992;&#20110;&#26368;&#20248;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Collaborative and Distributed Bayesian Optimization via Consensus: Showcasing the Power of Collaboration for Optimal Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20849;&#35782;&#27010;&#24565;&#23454;&#29616;&#20102;&#21512;&#20316;&#21644;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#20805;&#20998;&#23637;&#31034;&#20102;&#21512;&#20316;&#23545;&#20110;&#26368;&#20248;&#35774;&#35745;&#30340;&#37325;&#35201;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#35774;&#35745;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#25361;&#25112;&#26469;&#33258;&#20110;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#35797;&#39564;&#21644;&#38169;&#35823;&#65292;&#36890;&#24120;&#36890;&#36807;&#27169;&#25311;&#25110;&#36827;&#34892;&#29616;&#22330;&#23454;&#39564;&#26469;&#23436;&#25104;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#24403;&#20351;&#29992;&#24102;&#26377;&#36125;&#21494;&#26031;&#39118;&#26684;&#30340;&#20195;&#29702;&#21644;&#39640;&#25928;&#30340;&#39034;&#24207;&#25277;&#26679;&#31574;&#30053;&#26102;&#65292;&#39034;&#24207;&#26368;&#20248;&#35774;&#35745;&#65292;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#19979;&#20063;&#25198;&#28436;&#30528;&#21152;&#36895;&#35774;&#35745;&#36807;&#31243;&#30340;&#20851;&#38190;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;&#20170;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#26426;&#36935;&#12290;&#36793;&#32536;&#35774;&#22791;&#30340;&#22686;&#21152;&#36830;&#25509;&#24615;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#20316;&#33539;&#24335;&#12290;&#19968;&#31181;&#33539;&#24335;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#20998;&#37197;&#20854;&#23454;&#39564;&#21162;&#21147;&#65292;&#35753;&#19981;&#21516;&#23458;&#25143;&#31471;&#20849;&#21516;&#20511;&#37492;&#24444;&#27492;&#30340;&#21147;&#37327;&#65292;&#20197;&#25913;&#21892;&#21644;&#24555;&#36895;&#25512;&#36827;&#20854;&#26368;&#20248;&#35774;&#35745;&#36807;&#31243;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#20849;&#35782;&#30340;&#27010;&#24565;&#24341;&#20837;&#21040;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#23458;&#25143;&#31471;&#21516;&#24847;&#65288;&#21363;&#36798;&#25104;&#20849;&#35782;&#65289;&#20854;&#19979;&#19968;&#20010;&#37319;&#26679;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.14348v2 Announce Type: replace  Abstract: Optimal design is a critical yet challenging task within many applications. This challenge arises from the need for extensive trial and error, often done through simulations or running field experiments. Fortunately, sequential optimal design, also referred to as Bayesian optimization when using surrogates with a Bayesian flavor, has played a key role in accelerating the design process through efficient sequential sampling strategies. However, a key opportunity exists nowadays. The increased connectivity of edge devices sets forth a new collaborative paradigm for Bayesian optimization. A paradigm whereby different clients collaboratively borrow strength from each other by effectively distributing their experimentation efforts to improve and fast-track their optimal design process. To this end, we bring the notion of consensus to Bayesian optimization, where clients agree (i.e., reach a consensus) on their next-to-sample designs. Our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2306.02090</link><description>&lt;p&gt;
&#27809;&#26377;&#25968;&#25454;&#35775;&#38382;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Deep Classifier Mimicry without Data Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#35775;&#38382;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26631;&#20934;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#33021;&#26080;&#27861;&#31561;&#21516;&#22320;&#33719;&#24471;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#24494;&#35843;&#12289;&#21387;&#32553;&#27169;&#22411;&#12289;&#25345;&#32493;&#35843;&#25972;&#25110;&#36827;&#34892;&#20219;&#20309;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26356;&#26032;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#21487;&#33021;&#26080;&#38656;&#21407;&#22987;&#25968;&#25454;&#35775;&#38382;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25512;&#29702;&#30693;&#35782;&#25552;&#21462;&#65288;CAKE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;CAKE&#29983;&#25104;&#19968;&#23545;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#23427;&#20204;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#26550;&#26500;&#36873;&#25321;&#22312;&#23454;&#35777;&#19978;&#35777;&#23454;&#20102;CAKE&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02090v2 Announce Type: replace-cross  Abstract: Access to pre-trained models has recently emerged as a standard across numerous machine learning domains. Unfortunately, access to the original data the models were trained on may not equally be granted. This makes it tremendously challenging to fine-tune, compress models, adapt continually, or to do any other type of data-driven update. We posit that original data access may however not be required. Specifically, we propose Contrastive Abductive Knowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure that mimics deep classifiers without access to the original data. To this end, CAKE generates pairs of noisy synthetic samples and diffuses them contrastively toward a model's decision boundary. We empirically corroborate CAKE's effectiveness using several benchmark datasets and various architectural choices, paving the way for broad application.
&lt;/p&gt;</description></item><item><title>&#31283;&#20581;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#26799;&#24230;&#36890;&#24120;&#19982;&#20154;&#31867;&#24863;&#30693;&#23545;&#40784;&#65292;&#36890;&#36807;&#31163;&#25955;&#24230;&#31283;&#20581;&#24615;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2305.19101</link><description>&lt;p&gt;
&#21738;&#20123;&#27169;&#22411;&#20855;&#26377;&#24863;&#30693;&#23545;&#40784;&#26799;&#24230;&#65311;&#36890;&#36807;&#31163;&#25955;&#24230;&#31283;&#20581;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.19101
&lt;/p&gt;
&lt;p&gt;
&#31283;&#20581;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#26799;&#24230;&#36890;&#24120;&#19982;&#20154;&#31867;&#24863;&#30693;&#23545;&#40784;&#65292;&#36890;&#36807;&#31163;&#25955;&#24230;&#31283;&#20581;&#24615;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#20581;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#24615;&#26159;&#23427;&#20204;&#30340;&#36755;&#20837;&#26799;&#24230;&#36890;&#24120;&#19982;&#20154;&#31867;&#24863;&#30693;&#23545;&#40784;&#65292;&#34987;&#25991;&#29486;&#31216;&#20026;&#24863;&#30693;&#23545;&#40784;&#26799;&#24230;&#65288;PAGs&#65289;&#12290;&#23613;&#31649;&#21482;&#34987;&#35757;&#32451;&#29992;&#20110;&#20998;&#31867;&#65292;PAGs&#20351;&#24471;&#31283;&#20581;&#27169;&#22411;&#20855;&#26377;&#22522;&#26412;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#21253;&#25324;&#22270;&#20687;&#29983;&#25104;&#12289;&#21435;&#22122;&#21644;&#20462;&#22797;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#35937;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;\emph{&#31163;&#25955;&#24230;&#31283;&#20581;&#24615;}&#39318;&#27425;&#23545;PAGs&#36827;&#34892;&#35299;&#37322;&#65292;&#35813;&#29702;&#35770;&#25351;&#20986;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20043;&#22806;&#24517;&#39035;&#27604;&#22312;&#27969;&#24418;&#19978;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#31163;&#25955;&#24230;&#31283;&#20581;&#24615;&#23548;&#33268;&#36755;&#20837;&#26799;&#24230;&#22823;&#33268;&#20301;&#20110;&#25968;&#25454;&#27969;&#24418;&#19978;&#65292;&#20174;&#32780;&#35299;&#37322;&#23427;&#20204;&#30340;&#24863;&#30693;&#23545;&#40784;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#36125;&#21494;&#26031;&#26368;&#20248;&#27169;&#22411;&#28385;&#36275;&#31163;&#25955;&#24230;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;&#32463;&#39564;&#19978;&#35777;&#23454;&#36890;&#36807;&#26799;&#24230;&#35757;&#32451;&#30340;&#31283;&#20581;&#27169;&#22411;&#20063;&#28385;&#36275;&#30456;&#21516;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.19101v2 Announce Type: replace  Abstract: One of the remarkable properties of robust computer vision models is that their input-gradients are often aligned with human perception, referred to in the literature as perceptually-aligned gradients (PAGs). Despite only being trained for classification, PAGs cause robust models to have rudimentary generative capabilities, including image generation, denoising, and in-painting. However, the underlying mechanisms behind these phenomena remain unknown. In this work, we provide a first explanation of PAGs via \emph{off-manifold robustness}, which states that models must be more robust off- the data manifold than they are on-manifold. We first demonstrate theoretically that off-manifold robustness leads input gradients to lie approximately on the data manifold, explaining their perceptual alignment. We then show that Bayes optimal models satisfy off-manifold robustness, and confirm the same empirically for robust models trained via grad
&lt;/p&gt;</description></item><item><title>DistriBlock&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#12289;&#29109;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.17000</link><description>&lt;p&gt;
DistriBlock: &#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
DistriBlock: Identifying adversarial audio samples by leveraging characteristics of the output distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.17000
&lt;/p&gt;
&lt;p&gt;
DistriBlock&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#12289;&#29109;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#33021;&#35823;&#23548;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#20351;&#20854;&#39044;&#27979;&#20219;&#24847;&#30446;&#26631;&#25991;&#26412;&#65292;&#20174;&#32780;&#26500;&#25104;&#26126;&#26174;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistriBlock&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;ASR&#31995;&#32479;&#30340;&#39640;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#35813;&#31995;&#32479;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#39044;&#27979;&#36755;&#20986;&#26631;&#35760;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#23545;&#35813;&#20998;&#24067;&#30340;&#19968;&#32452;&#29305;&#24449;&#36827;&#34892;&#27979;&#37327;&#65306;&#36755;&#20986;&#27010;&#29575;&#30340;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#65292;&#20998;&#24067;&#30340;&#29109;&#65292;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#21644;Jensen-Shannon&#25955;&#24230;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#20998;&#31867;&#12289;&#36825;&#31181;&#20998;&#31867;&#22120;&#30340;&#38598;&#21512;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;ASR&#31995;&#32479;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.17000v2 Announce Type: replace-cross  Abstract: Adversarial attacks can mislead automatic speech recognition (ASR) systems into predicting an arbitrary target text, thus posing a clear security threat. To prevent such attacks, we propose DistriBlock, an efficient detection strategy applicable to any ASR system that predicts a probability distribution over output tokens in each time step. We measure a set of characteristics of this distribution: the median, maximum, and minimum over the output probabilities, the entropy of the distribution, as well as the Kullback-Leibler and the Jensen-Shannon divergence with respect to the distributions of the subsequent time step. Then, by leveraging the characteristics observed for both benign and adversarial data, we apply binary classifiers, including simple threshold-based classification, ensembles of such classifiers, and neural networks. Through extensive analysis across different state-of-the-art ASR systems and language data sets, 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21452;&#33258;&#36866;&#24212;&#21306;&#22495;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#21319;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2303.14877</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#32416;&#27491;&#35823;&#24046;&#30340;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Error-mitigated Quantum Approximate Optimization via Learning-based Adaptive Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14877
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21452;&#33258;&#36866;&#24212;&#21306;&#22495;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#21319;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#24456;&#38590;&#27714;&#35299;&#12290;&#37327;&#23376;&#35745;&#31639;&#34987;&#35270;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#26576;&#20123;&#38382;&#39064;&#26102;&#20855;&#26377;&#28508;&#22312;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#65288;QAOA&#65289;&#26159;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#31639;&#27861;&#20043;&#19968;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#30005;&#36335;&#21442;&#25968;&#22495;&#19978;&#30340;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;QAOA&#30446;&#26631;&#20989;&#25968;&#22312;&#21442;&#25968;&#21464;&#37327;&#19978;&#30340;&#26223;&#35266;&#22240;&#20854;&#26222;&#36941;&#23384;&#22312;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#36139;&#30240;&#24179;&#21488;&#32780;&#33261;&#21517;&#26157;&#33879;&#65292;&#20854;&#21487;&#34892;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;QAOA&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21452;&#33258;&#36866;&#24212;&#21306;&#22495;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;DARBO&#65289;&#65292;&#19968;&#31181;&#36866;&#24212;&#24615;&#32463;&#20856;&#20248;&#21270;&#22120;&#29992;&#20110;QAOA&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;grad&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization problems are ubiquitous and computationally hard to solve in general. Quantum computing is envisioned as a powerful tool offering potential computational advantages for solving some of these problems. Quantum approximate optimization algorithm (QAOA), one of the most representative quantum-classical hybrid algorithms, is designed to solve certain combinatorial optimization problems by transforming a discrete optimization problem into a classical optimization problem over a continuous circuit parameter domain. QAOA objective landscape over the parameter variables is notorious for pervasive local minima and barren plateaus, and its viability in training significantly relies on the efficacy of the classical optimization algorithm. To enhance the performance of QAOA, we design double adaptive-region Bayesian optimization (DARBO), an adaptive classical optimizer for QAOA. Our experimental results demonstrate that the algorithm greatly outperforms conventional grad
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;DTI&#25968;&#25454;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20165;&#22522;&#20110;fMRI&#25968;&#25454;&#21457;&#29616;&#22823;&#33041;&#26377;&#25928;&#36830;&#25509;&#32452;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;</title><link>https://arxiv.org/abs/2302.05451</link><description>&lt;p&gt;
&#22522;&#20110;fMRI&#21644;DTI&#25968;&#25454;&#30340;&#22823;&#33041;&#26377;&#25928;&#36830;&#25509;&#32452;&#65306;&#36125;&#21494;&#26031;&#22240;&#26524;&#23398;&#20064;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Brain Effective Connectome based on fMRI and DTI Data: Bayesian Causal Learning and Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05451
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;DTI&#25968;&#25454;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20165;&#22522;&#20110;fMRI&#25968;&#25454;&#21457;&#29616;&#22823;&#33041;&#26377;&#25928;&#36830;&#25509;&#32452;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#26088;&#22312;&#25214;&#21040;&#20934;&#30830;&#21487;&#38752;&#30340;&#22823;&#33041;&#26377;&#25928;&#36830;&#25509;&#32452;&#65288;EC&#65289;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;EC&#21457;&#29616;&#26041;&#27861;&#24050;&#32463;&#20026;&#25105;&#20204;&#29702;&#35299;&#22823;&#33041;&#32452;&#32455;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20294;&#30001;&#20110;fMRI&#25968;&#25454;&#30340;&#26679;&#26412;&#37327;&#36739;&#23567;&#12289;&#26102;&#38388;&#20998;&#36776;&#29575;&#36739;&#20302;&#20197;&#21450;&#22823;&#33041;&#36830;&#25509;&#32452;&#30340;&#39640;&#32500;&#24615;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;DTI&#25968;&#25454;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#36125;&#21494;&#26031;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;--&#36125;&#21494;&#26031;GOLEM&#65288;BGOLEM&#65289;&#21644;&#36125;&#21494;&#26031;FGES&#65288;BFGES&#65289;&#26041;&#27861;--&#36825;&#20004;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;EC&#65292;&#24182;&#35299;&#20915;&#20102;&#20165;&#22522;&#20110;fMRI&#25968;&#25454;&#21457;&#29616;EC&#30340;&#29616;&#26377;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#32570;&#38519;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#38024;&#23545;&#21512;&#25104;&#21644;&#28151;&#21512;&#65288;&#20154;&#31867;&#36830;&#25509;&#32452;&#35745;&#21010;&#65288;HCP&#65289;&#21463;&#35797;&#32773;&#30340;DTI&#19982;&#21512;&#25104;fMRI&#25968;&#25454;&#65289;&#25968;&#25454;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#21457;&#29616;EC&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23545;&#25913;&#36827;&#36827;&#34892;&#25968;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05451v3 Announce Type: replace  Abstract: Neuroscientific studies aim to find an accurate and reliable brain Effective Connectome (EC). Although current EC discovery methods have contributed to our understanding of brain organization, their performances are severely constrained by the short sample size and poor temporal resolution of fMRI data, and high dimensionality of the brain connectome. By leveraging the DTI data as prior knowledge, we introduce two Bayesian causal discovery frameworks -- the Bayesian GOLEM (BGOLEM) and Bayesian FGES (BFGES) methods -- that offer significantly more accurate and reliable ECs and address the shortcomings of the existing causal discovery methods in discovering ECs based on only fMRI data. Through a series of simulation studies on synthetic and hybrid (DTI of the Human Connectome Project (HCP) subjects and synthetic fMRI) data, we demonstrate the effectiveness of the proposed methods in discovering EC. To numerically assess the improvement
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#20132;&#21449;&#27169;&#22411;&#27604;&#36739;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30340;&#25928;&#29992;&#65292;&#23454;&#29616;&#20943;&#23569;&#20887;&#20313;&#21442;&#25968;&#21644;&#25233;&#21046;&#36755;&#20837;&#22122;&#22768;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2301.03765</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#21449;&#27169;&#22411;&#27604;&#36739;&#25439;&#22833;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#20013;&#31070;&#32463;&#20803;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.03765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#20132;&#21449;&#27169;&#22411;&#27604;&#36739;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30340;&#25928;&#29992;&#65292;&#23454;&#29616;&#20943;&#23569;&#20887;&#20313;&#21442;&#25968;&#21644;&#25233;&#21046;&#36755;&#20837;&#22122;&#22768;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#22312;&#27169;&#22411;&#35268;&#27169;&#21644;&#36755;&#20837;&#32972;&#26223;&#26041;&#38754;&#19981;&#26029;&#25193;&#22823;&#65292;&#24341;&#20837;&#20102;&#26356;&#22810;&#38544;&#34255;&#31070;&#32463;&#20803;&#21644;&#36755;&#20837;&#31070;&#32463;&#20803;&#65292;&#22823;&#20307;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39069;&#22806;&#30340;&#31070;&#32463;&#20803;&#24182;&#19981;&#33021;&#20026;&#25152;&#26377;&#23454;&#20363;&#24102;&#26469;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#22240;&#20026;&#19968;&#20123;&#38544;&#34255;&#31070;&#32463;&#20803;&#26159;&#20887;&#20313;&#30340;&#65292;&#28151;&#20837;&#36755;&#20837;&#31070;&#32463;&#20803;&#30340;&#22122;&#22768;&#24448;&#24448;&#20250;&#20998;&#25955;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#20391;&#37325;&#20110;&#36890;&#36807;&#38468;&#21152;&#30340;&#21518;&#22788;&#29702;&#25110;&#39044;&#22788;&#29702;&#65292;&#22914;&#32593;&#32476;&#20462;&#21098;&#21644;&#19978;&#19979;&#25991;&#36873;&#25321;&#65292;&#20174;&#22806;&#37096;&#38477;&#20302;&#20302;&#25928;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#65292;&#20197;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#25928;&#29992;&#26469;&#20351;&#27169;&#22411;&#20943;&#23569;&#20887;&#20313;&#21442;&#25968;&#24182;&#25233;&#21046;&#36755;&#20837;&#22122;&#22768;&#65311;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#31070;&#32463;&#20803;&#65292;&#37027;&#20040;&#19981;&#31649;&#21738;&#20123;&#31070;&#32463;&#20803;&#34987;&#21093;&#31163;&#65288;&#31105;&#29992;&#65289;&#65292;&#21093;&#31163;&#21518;&#30340;&#23376;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#19981;&#24212;&#35813;&#20248;&#20110;&#21407;&#22987;&#23436;&#25972;&#27169;&#22411;&#12290;&#26681;&#25454;&#36825;&#26679;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.03765v2 Announce Type: replace  Abstract: Current natural language understanding (NLU) models have been continuously scaling up, both in terms of model size and input context, introducing more hidden and input neurons. While this generally improves performance on average, the extra neurons do not yield a consistent improvement for all instances. This is because some hidden neurons are redundant, and the noise mixed in input neurons tends to distract the model. Previous work mainly focuses on extrinsically reducing low-utility neurons by additional post- or pre-processing, such as network pruning and context selection, to avoid this problem. Beyond that, can we make the model reduce redundant parameters and suppress input noise by intrinsically enhancing the utility of each neuron? If a model can efficiently utilize neurons, no matter which neurons are ablated (disabled), the ablated submodel should perform no better than the original full model. Based on such a comparison pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;oneDNN&#22270;&#32534;&#35793;&#22120;&#65292;&#37319;&#29992;&#20102;&#32534;&#35793;&#20248;&#21270;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#24615;&#33021;&#24352;&#37327;&#32534;&#35793;&#65292;&#29983;&#25104;&#19987;&#23478;&#32423;&#24615;&#33021;&#20195;&#30721;&#24182;&#22312;DNN&#35745;&#31639;&#22270;&#33539;&#22260;&#20869;&#24212;&#29992;&#32534;&#35793;&#20248;&#21270;</title><link>https://arxiv.org/abs/2301.01333</link><description>&lt;p&gt;
oneDNN&#22270;&#32534;&#35793;&#22120;&#65306;&#29992;&#20110;&#39640;&#24615;&#33021;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#30340;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
oneDNN Graph Compiler: A Hybrid Approach for High-Performance Deep Learning Compilation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.01333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;oneDNN&#22270;&#32534;&#35793;&#22120;&#65292;&#37319;&#29992;&#20102;&#32534;&#35793;&#20248;&#21270;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#24615;&#33021;&#24352;&#37327;&#32534;&#35793;&#65292;&#29983;&#25104;&#19987;&#23478;&#32423;&#24615;&#33021;&#20195;&#30721;&#24182;&#22312;DNN&#35745;&#31639;&#22270;&#33539;&#22260;&#20869;&#24212;&#29992;&#32534;&#35793;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#30828;&#20214;&#23545;&#23494;&#38598;&#35745;&#31639;&#30340;&#25903;&#25345;&#65292;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#29305;&#24449;&#21457;&#29983;&#20102;&#26174;&#33879;&#21464;&#21270;&#65292;&#20174;&#35745;&#31639;&#23494;&#38598;&#22411;&#25805;&#20316;&#19978;&#30340;&#20960;&#20010;&#28909;&#28857;&#21040;&#20998;&#24067;&#22312;&#27169;&#22411;&#20013;&#30340;&#24191;&#27867;&#25805;&#20316;&#12290;&#21033;&#29992;&#19987;&#23478;&#35843;&#20248;&#30340;&#22522;&#20803;&#23454;&#29616;&#21152;&#36895;&#20960;&#20010;&#35745;&#31639;&#23494;&#38598;&#22411;&#25805;&#20316;&#24182;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;AI&#30828;&#20214;&#30340;&#24615;&#33021;&#28508;&#21147;&#12290;&#24050;&#32463;&#20570;&#20986;&#20102;&#21508;&#31181;&#21162;&#21147;&#26469;&#32534;&#35793;&#23436;&#25972;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22270;&#12290;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#36890;&#36807;&#29983;&#25104;&#19987;&#23478;&#32423;&#24615;&#33021;&#20195;&#30721;&#26469;&#23454;&#29616;&#39640;&#24615;&#33021;&#24352;&#37327;&#32534;&#35793;&#65292;&#23545;DNN&#35745;&#31639;&#22270;&#30340;&#33539;&#22260;&#36328;&#22810;&#20010;&#35745;&#31639;&#23494;&#38598;&#25805;&#20316;&#24212;&#29992;&#32534;&#35793;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.01333v3 Announce Type: replace  Abstract: With the rapid development of deep learning models and hardware support for dense computing, the deep learning workload characteristics changed significantly from a few hot spots on compute-intensive operations to a broad range of operations scattered across the models. Accelerating a few compute-intensive operations using the expert-tuned implementation of primitives does not fully exploit the performance potential of AI hardware. Various efforts have been made to compile a full deep neural network (DNN) graph. One of the biggest challenges is to achieve high-performance tensor compilation by generating expert level performance code for the dense compute-intensive operations and applying compilation optimization at the scope of DNN computation graph across multiple compute-intensive operations.   We present oneDNN Graph Compiler, a tensor compiler that employs a hybrid approach of using techniques from both compiler optimization and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22806;&#24863;&#30693;&#20256;&#24863;&#21644;&#20013;&#26530;&#27169;&#24335;&#21457;&#29983;&#22120;&#25972;&#21512;&#21040;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#35270;&#35273;&#24341;&#23548;&#30340;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#65292;&#25506;&#32034;&#20102;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#23545;&#23548;&#33322;&#40065;&#26834;&#24615;&#30340;&#25913;&#36827;&#12289;&#20855;&#26377;&#35760;&#24518;&#21151;&#33021;&#30340;&#31574;&#30053;&#32593;&#32476;&#19982;&#26080;&#35760;&#24518;&#31574;&#30053;&#32593;&#32476;&#22312;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#20197;&#21450;&#21160;&#29289;&#22914;&#20309;&#23481;&#24525;&#39640;&#12290;</title><link>https://arxiv.org/abs/2212.14400</link><description>&lt;p&gt;
Visual CPG-RL&#65306;&#23398;&#20064;&#29992;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#30340;&#20013;&#26530;&#27169;&#24335;&#21457;&#29983;&#22120;
&lt;/p&gt;
&lt;p&gt;
Visual CPG-RL: Learning Central Pattern Generators for Visually-Guided Quadruped Locomotion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.14400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22806;&#24863;&#30693;&#20256;&#24863;&#21644;&#20013;&#26530;&#27169;&#24335;&#21457;&#29983;&#22120;&#25972;&#21512;&#21040;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#35270;&#35273;&#24341;&#23548;&#30340;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#65292;&#25506;&#32034;&#20102;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#23545;&#23548;&#33322;&#40065;&#26834;&#24615;&#30340;&#25913;&#36827;&#12289;&#20855;&#26377;&#35760;&#24518;&#21151;&#33021;&#30340;&#31574;&#30053;&#32593;&#32476;&#19982;&#26080;&#35760;&#24518;&#31574;&#30053;&#32593;&#32476;&#22312;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#20197;&#21450;&#21160;&#29289;&#22914;&#20309;&#23481;&#24525;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22806;&#24863;&#30693;&#20256;&#24863;&#21644;&#20013;&#26530;&#27169;&#24335;&#21457;&#29983;&#22120;&#65288;CPGs&#65292;&#21363;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#65289;&#25972;&#21512;&#21040;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#35270;&#35273;&#24341;&#23548;&#30340;&#22235;&#36275;&#21160;&#29289;&#36816;&#21160;&#12290;&#36890;&#36807;&#22806;&#24863;&#30693;&#21644;&#26412;&#20307;&#24863;&#30693;&#65292;&#20195;&#29702;&#23398;&#20064;&#21327;&#35843;&#19981;&#21516;&#25391;&#33633;&#22120;&#20043;&#38388;&#30340;&#33410;&#24459;&#34892;&#20026;&#65292;&#20197;&#36319;&#36394;&#36895;&#24230;&#25351;&#20196;&#65292;&#21516;&#26102;&#35206;&#30422;&#36825;&#20123;&#25351;&#20196;&#20197;&#36991;&#20813;&#19982;&#29615;&#22659;&#30896;&#25758;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20960;&#20010;&#24320;&#25918;&#30340;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#31070;&#32463;&#31185;&#23398;&#38382;&#39064;&#65306;1&#65289;&#25391;&#33633;&#22120;&#20043;&#38388;&#30340;&#26174;&#24335;&#30456;&#20114;&#32806;&#21512;&#30340;&#20316;&#29992;&#26159;&#20160;&#20040;&#65292;&#36825;&#31181;&#32806;&#21512;&#26159;&#21542;&#33021;&#25552;&#39640;&#29992;&#20110;&#23548;&#33322;&#40065;&#26834;&#24615;&#30340;&#20174;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#65311;2&#65289;&#20351;&#29992;&#20855;&#26377;&#35760;&#24518;&#21151;&#33021;&#19982;&#26080;&#35760;&#24518;&#31574;&#30053;&#32593;&#32476;&#23545;&#20110;&#40065;&#26834;&#24615;&#12289;&#33021;&#25928;&#21644;&#20174;&#27169;&#25311;&#21040;&#30495;&#23454;&#23548;&#33322;&#20219;&#21153;&#30340;&#36319;&#36394;&#24615;&#33021;&#26377;&#20160;&#20040;&#24433;&#21709;&#65311;3&#65289;&#21160;&#29289;&#26159;&#22914;&#20309;&#23481;&#24525;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.14400v2 Announce Type: replace-cross  Abstract: We present a framework for learning visually-guided quadruped locomotion by integrating exteroceptive sensing and central pattern generators (CPGs), i.e. systems of coupled oscillators, into the deep reinforcement learning (DRL) framework. Through both exteroceptive and proprioceptive sensing, the agent learns to coordinate rhythmic behavior among different oscillators to track velocity commands, while at the same time override these commands to avoid collisions with the environment. We investigate several open robotics and neuroscience questions: 1) What is the role of explicit interoscillator couplings between oscillators, and can such coupling improve sim-to-real transfer for navigation robustness? 2) What are the effects of using a memory-enabled vs. a memory-free policy network with respect to robustness, energy-efficiency, and tracking performance in sim-to-real navigation tasks? 3) How do animals manage to tolerate high 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Tensor&#31070;&#32463;&#32593;&#32476;&#65288;TNN&#65289;&#21644;&#24352;&#37327;&#32593;&#32476;&#21021;&#22987;&#21270;&#22120;&#65288;TNN Init&#65289;&#65292;TNN&#21487;&#20197;&#22312;&#33719;&#24471;&#19982;DNN&#30456;&#21516;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26174;&#33879;&#30340;&#21442;&#25968;&#33410;&#30465;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2212.14076</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26399;&#26435;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Quantum-Inspired Tensor Neural Networks for Option Pricing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.14076
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Tensor&#31070;&#32463;&#32593;&#32476;&#65288;TNN&#65289;&#21644;&#24352;&#37327;&#32593;&#32476;&#21021;&#22987;&#21270;&#22120;&#65288;TNN Init&#65289;&#65292;TNN&#21487;&#20197;&#22312;&#33719;&#24471;&#19982;DNN&#30456;&#21516;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26174;&#33879;&#30340;&#21442;&#25968;&#33410;&#30465;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#22312;&#26356;&#39640;&#32500;&#24230;&#35299;&#20915;&#38382;&#39064;&#26469;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#65288;COD&#65289;&#12290;&#35299;&#20915;COD&#30340;&#36825;&#31867;&#26041;&#27861;&#20043;&#19968;&#23548;&#33268;&#25105;&#20204;&#35299;&#20915;&#39640;&#32500;PDE&#38382;&#39064;&#12290;&#36825;&#24050;&#32463;&#20026;&#35299;&#20915;&#20174;&#25968;&#23398;&#37329;&#34701;&#21040;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38543;&#26426;&#25511;&#21046;&#31561;&#21508;&#31181;&#23454;&#38469;&#38382;&#39064;&#25950;&#24320;&#20102;&#22823;&#38376;&#12290;&#23613;&#31649;&#21487;&#34892;&#65292;&#20294;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20173;&#21463;&#21040;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#38480;&#21046;&#12290;Tensor&#31070;&#32463;&#32593;&#32476;&#65288;TNN&#65289;&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#23427;&#20204;&#35777;&#26126;&#21487;&#20197;&#22312;&#33719;&#24471;&#19982;&#32463;&#20856;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30456;&#21516;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26174;&#33879;&#30340;&#21442;&#25968;&#33410;&#30465;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#30456;&#27604;DNN&#65292;TNN&#30340;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#38500;&#20102;TNN&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#24352;&#37327;&#32593;&#32476;&#21021;&#22987;&#21270;&#22120;&#65288;TNN Init&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23567;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.14076v2 Announce Type: replace-cross  Abstract: Recent advances in deep learning have enabled us to address the curse of dimensionality (COD) by solving problems in higher dimensions. A subset of such approaches of addressing the COD has led us to solving high-dimensional PDEs. This has resulted in opening doors to solving a variety of real-world problems ranging from mathematical finance to stochastic control for industrial applications. Although feasible, these deep learning methods are still constrained by training time and memory. Tackling these shortcomings, Tensor Neural Networks (TNN) demonstrate that they can provide significant parameter savings while attaining the same accuracy as compared to the classical Dense Neural Network (DNN). In addition, we also show how TNN can be trained faster than DNN for the same accuracy. Besides TNN, we also introduce Tensor Network Initializer (TNN Init), a weight initialization scheme that leads to faster convergence with smaller 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#26500;&#24314;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#38024;&#23545;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#23450;&#21046;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2210.15182</link><description>&lt;p&gt;
Text2Model:&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#24402;&#32435;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text2Model: Text-based Model Induction for Zero-shot Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.15182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#26500;&#24314;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#38024;&#23545;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#23450;&#21046;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20165;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#26500;&#24314;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#22120;&#30340;&#25361;&#25112;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#12289;3D&#28857;&#20113;&#20998;&#31867;&#20197;&#21450;&#20174;&#22330;&#26223;&#20013;&#35782;&#21035;&#21160;&#20316;&#12290;&#19982;&#23398;&#20064;&#22266;&#23450;&#36755;&#20986;&#31867;&#21035;&#34920;&#31034;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#25512;&#26029;&#26102;&#29983;&#25104;&#38024;&#23545;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#23450;&#21046;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#29983;&#25104;&#22522;&#20110;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#25509;&#25910;&#31867;&#25551;&#36848;&#24182;&#36755;&#20986;&#19968;&#20010;&#22810;&#31867;&#27169;&#22411;&#12290;&#36229;&#32593;&#32476;&#35774;&#35745;&#20026;&#23545;&#25551;&#36848;&#38598;&#21512;&#21644;&#20998;&#31867;&#23618;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#22240;&#27492;&#31526;&#21512;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20016;&#23500;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#22270;&#20687;&#12289;&#28857;&#20113;&#21644;&#21160;&#20316;&#35782;&#21035;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.15182v2 Announce Type: replace-cross  Abstract: We address the challenge of building task-agnostic classifiers using only text descriptions, demonstrating a unified approach to image classification, 3D point cloud classification, and action recognition from scenes. Unlike approaches that learn a fixed representation of the output classes, we generate at inference time a model tailored to a query classification task. To generate task-based zero-shot classifiers, we train a hypernetwork that receives class descriptions and outputs a multi-class model. The hypernetwork is designed to be equivariant with respect to the set of descriptions and the classification layer, thus obeying the symmetries of the problem and improving generalization. Our approach generates non-linear classifiers and can handle rich textual descriptions. We evaluate this approach in a series of zero-shot classification tasks, for image, point-cloud, and action recognition, using a range of text descriptions
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#35838;&#31243;&#20998;&#37197;&#26426;&#21046;&#65288;MLCM&#65289;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22359;&#20943;&#36731;&#23398;&#29983;&#22312;&#25253;&#21578;&#20559;&#22909;&#26102;&#30340;&#38169;&#35823;&#65292;&#26174;&#33879;&#25552;&#39640;&#23398;&#29983;&#25928;&#29992;&#65292;&#19988;&#20855;&#26377;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2210.00954</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#35838;&#31243;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Powered Course Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.00954
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#35838;&#31243;&#20998;&#37197;&#26426;&#21046;&#65288;MLCM&#65289;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22359;&#20943;&#36731;&#23398;&#29983;&#22312;&#25253;&#21578;&#20559;&#22909;&#26102;&#30340;&#38169;&#35823;&#65292;&#26174;&#33879;&#25552;&#39640;&#23398;&#29983;&#25928;&#29992;&#65292;&#19988;&#20855;&#26377;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35838;&#31243;&#20998;&#37197;&#38382;&#39064;&#65292;&#21363;&#22823;&#23398;&#20026;&#23398;&#29983;&#23433;&#25490;&#35838;&#31243;&#26102;&#38388;&#34920;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#21046;Course Match&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#23398;&#29983;&#22312;&#25253;&#21578;&#20182;&#20204;&#30340;&#20559;&#22909;&#26102;&#20250;&#29359;&#24456;&#22823;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#23545;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26426;&#21046;&#65292;&#21363;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Course Match&#65288;MLCM&#65289;&#12290;MLCM&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20559;&#22909;&#24341;&#23548;&#27169;&#22359;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#22320;&#21521;&#23398;&#29983;&#25552;&#20986;&#20010;&#24615;&#21270;&#30340;&#20004;&#20004;&#27604;&#36739;&#26597;&#35810;&#65292;&#20197;&#20943;&#36731;&#23398;&#29983;&#30340;&#25253;&#21578;&#38169;&#35823;&#12290;&#22823;&#37327;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#65292;MLCM&#20165;&#38656;&#21313;&#20010;&#27604;&#36739;&#26597;&#35810;&#65292;&#23601;&#33021;&#23558;&#24179;&#22343;&#21644;&#26368;&#23567;&#23398;&#29983;&#25928;&#29992;&#20998;&#21035;&#25552;&#39640;7%-11%&#21644;17%-29%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;MLCM&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#22914;&#20309;&#26368;&#23567;&#21270;&#21319;&#32423;&#33267;MLCM &#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.00954v3 Announce Type: replace-cross  Abstract: We study the course allocation problem, where universities assign course schedules to students. The current state-of-the-art mechanism, Course Match, has one major shortcoming: students make significant mistakes when reporting their preferences, which negatively affects welfare and fairness. To address this issue, we introduce a new mechanism, Machine Learning-powered Course Match (MLCM). At the core of MLCM is a machine learning-powered preference elicitation module that iteratively asks personalized pairwise comparison queries to alleviate students' reporting mistakes. Extensive computational experiments, grounded in real-world data, demonstrate that MLCM, with only ten comparison queries, significantly increases both average and minimum student utility by 7%-11% and 17%-29%, respectively. Finally, we highlight MLCM's robustness to changes in the environment and show how our design minimizes the risk of upgrading to MLCM whil
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#24352;&#37327;&#22330;&#30340;&#20998;&#35299;&#23884;&#20837;&#65292;&#20351;&#24471;&#23545;&#21407;&#22987;&#24352;&#37327;&#38598;&#21512;&#36827;&#34892;&#20449;&#24687;&#26597;&#35810;&#21644;&#21518;&#22788;&#29702;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2209.00372</link><description>&lt;p&gt;
&#26377;&#25928;&#23398;&#20064;&#24352;&#37327;&#22330;&#30340;&#20998;&#35299;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Learning of Factored Embeddings of Tensor Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00372
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#24352;&#37327;&#22330;&#30340;&#20998;&#35299;&#23884;&#20837;&#65292;&#20351;&#24471;&#23545;&#21407;&#22987;&#24352;&#37327;&#38598;&#21512;&#36827;&#34892;&#20449;&#24687;&#26597;&#35810;&#21644;&#21518;&#22788;&#29702;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24352;&#37327;&#30340;&#31209;2&#21450;&#20197;&#19978;&#30340;&#29983;&#25104;&#29616;&#22312;&#24050;&#25104;&#24120;&#24577;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#36234;&#26469;&#36234;&#24222;&#22823;&#19988;&#19981;&#26029;&#22686;&#38271;&#12290;&#35768;&#22810;&#31185;&#23398;&#21644;&#21307;&#23398;&#25968;&#25454;&#24352;&#37327;&#26159;&#24352;&#37327;&#22330;&#65288;&#20363;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#22320;&#29702;&#25968;&#25454;&#65289;&#65292;&#20854;&#20013;&#31354;&#38388;&#37051;&#22495;&#21253;&#21547;&#37325;&#35201;&#20449;&#24687;&#12290;&#30452;&#25509;&#35775;&#38382;&#22914;&#27492;&#24222;&#22823;&#30340;&#25968;&#25454;&#24352;&#37327;&#38598;&#21512;&#20197;&#33719;&#21462;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#36817;&#20284;&#20840;&#31209;&#21644;&#32039;&#20945;&#30340;&#24352;&#37327;&#33609;&#22270;&#65292;&#20855;&#26377;&#25552;&#20379;&#24352;&#37327;&#22330;&#30340;&#32039;&#20945;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#35889;&#23884;&#20837;&#30340;&#20998;&#35299;&#34920;&#31034;&#12290;&#25152;&#26377;&#23545;&#21407;&#22987;&#24352;&#37327;&#22330;&#30340;&#20449;&#24687;&#26597;&#35810;&#21644;&#21518;&#22788;&#29702;&#29616;&#22312;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23454;&#29616;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36825;&#20123;&#32039;&#20945;&#30340;&#20998;&#35299;&#33609;&#22270;&#19978;&#20197;&#28508;&#22312;&#29983;&#25104;&#31354;&#38388;&#20013;&#36827;&#34892;&#23450;&#21046;&#21270;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#20174;&#26679;&#26412;&#39640;&#25928;&#23376;&#31354;&#38388;&#26500;&#24314;&#32039;&#20945;&#30340;&#22240;&#23376;&#30697;&#38453;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#38454;&#25968;&#25454;&#24352;&#37327;&#30340;&#26368;&#20339;&#31209;r&#33609;&#22270;&#21270;Tucker&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00372v2 Announce Type: replace  Abstract: Data tensors of orders 2 and greater are now routinely being generated. These data collections are increasingly huge and growing. Many scientific and medical data tensors are tensor fields (e.g., images, videos, geographic data) in which the spatial neighborhood contains important information. Directly accessing such large data tensor collections for information has become increasingly prohibitive. We learn approximate full-rank and compact tensor sketches with decompositive representations providing compact space, time and spectral embeddings of tensor fields. All information querying and post-processing on the original tensor field can now be achieved more efficiently and with customizable accuracy as they are performed on these compact factored sketches in latent generative space. We produce optimal rank-r sketchy Tucker decomposition of arbitrary order data tensors by building compact factor matrices from a sample-efficient sub-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#20013;&#31070;&#32463;&#32593;&#32476;&#23618;&#35745;&#31639;&#22797;&#26434;&#24230;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#24322;&#26500;&#37327;&#21270;&#30340;&#8220;&#21152;&#27861;&#21644;&#20301;&#31227;&#25968;&#37327;(NABS)&#8221;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2206.12191</link><description>&lt;p&gt;
&#20449;&#21495;&#22788;&#29702;&#20013;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Computational Complexity Evaluation of Neural Network Applications in Signal Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.12191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#20013;&#31070;&#32463;&#32593;&#32476;&#23618;&#35745;&#31639;&#22797;&#26434;&#24230;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#24322;&#26500;&#37327;&#21270;&#30340;&#8220;&#21152;&#27861;&#21644;&#20301;&#31227;&#25968;&#37327;(NABS)&#8221;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#20013;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#24182;&#38142;&#25509;&#20102;&#22235;&#31181;&#36719;&#20214;&#21040;&#30828;&#20214;&#22797;&#26434;&#24230;&#34913;&#37327;&#26631;&#20934;&#65292;&#23450;&#20041;&#20102;&#19981;&#21516;&#22797;&#26434;&#24230;&#25351;&#26631;&#19982;&#23618;&#30340;&#36229;&#21442;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#35299;&#37322;&#20102;&#22914;&#20309;&#35745;&#31639;&#21069;&#39304;&#21644;&#24490;&#29615;&#23618;&#30340;&#36825;&#22235;&#20010;&#25351;&#26631;&#65292;&#24182;&#23450;&#20041;&#20102;&#25105;&#20204;&#20309;&#26102;&#24212;&#35813;&#20351;&#29992;&#29305;&#23450;&#25351;&#26631;&#30340;&#24773;&#20917;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#25105;&#20204;&#26159;&#23545;&#26356;&#36719;&#20214;&#23548;&#21521;&#36824;&#26159;&#30828;&#20214;&#23548;&#21521;&#30340;&#24212;&#29992;&#36827;&#34892;&#29305;&#24449;&#21270;&#12290;&#20854;&#20013;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#27861;&#21644;&#20301;&#31227;&#25968;&#37327;&#65288;NABS&#65289;&#8221;&#30340;&#22235;&#20010;&#25351;&#26631;&#20026;&#24322;&#26500;&#37327;&#21270;&#26032;&#24341;&#20837;&#12290;NABS&#21051;&#30011;&#20102;&#25805;&#20316;&#20013;&#20351;&#29992;&#30340;&#20301;&#23485;&#20197;&#21450;&#31639;&#26415;&#25805;&#20316;&#20013;&#20351;&#29992;&#30340;&#37327;&#21270;&#31867;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#20316;&#20026;&#19981;&#21516;&#27700;&#24179;&#65288;&#30446;&#30340;&#65289;&#30340;&#22797;&#26434;&#24230;&#20272;&#35745;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.12191v2 Announce Type: replace-cross  Abstract: In this paper, we provide a systematic approach for assessing and comparing the computational complexity of neural network layers in digital signal processing. We provide and link four software-to-hardware complexity measures, defining how the different complexity metrics relate to the layers' hyper-parameters. This paper explains how to compute these four metrics for feed-forward and recurrent layers, and defines in which case we ought to use a particular metric depending on whether we characterize a more soft- or hardware-oriented application. One of the four metrics, called `the number of additions and bit shifts (NABS)', is newly introduced for heterogeneous quantization. NABS characterizes the impact of not only the bitwidth used in the operation but also the type of quantization used in the arithmetical operations. We intend this work to serve as a baseline for the different levels (purposes) of complexity estimation rela
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21306;&#20998;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#39044;&#27979;&#20010;&#20307;&#26410;&#26469;&#36824;&#26159;&#37325;&#22797;&#36807;&#21435;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#21518;&#22522;&#32447;&#27979;&#35797;&#23637;&#31034;&#27169;&#22411;&#26159;&#21542;&#22238;&#28335;&#36807;&#21435;&#65292;&#24182;&#22312;&#38271;&#26399;&#38754;&#26495;&#35843;&#30740;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2206.11673</link><description>&lt;p&gt;
&#24744;&#30340;&#27169;&#22411;&#26159;&#22312;&#39044;&#27979;&#36807;&#21435;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is your model predicting the past?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.11673
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21306;&#20998;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#39044;&#27979;&#20010;&#20307;&#26410;&#26469;&#36824;&#26159;&#37325;&#22797;&#36807;&#21435;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#21518;&#22522;&#32447;&#27979;&#35797;&#23637;&#31034;&#27169;&#22411;&#26159;&#21542;&#22238;&#28335;&#36807;&#21435;&#65292;&#24182;&#22312;&#38271;&#26399;&#38754;&#26495;&#35843;&#30740;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20309;&#26102;&#39044;&#27979;&#20010;&#20307;&#30340;&#26410;&#26469;&#65292;&#20309;&#26102;&#37325;&#22797;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#24335;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#36825;&#20004;&#31181;&#39044;&#27979;&#36335;&#24452;&#36827;&#34892;&#21306;&#20998;&#65292;&#36825;&#19968;&#25552;&#35758;&#24471;&#21040;&#20102;&#29702;&#35770;&#12289;&#32463;&#39564;&#21644;&#35268;&#33539;&#24615;&#35770;&#35777;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26680;&#24515;&#26159;&#19968;&#31867;&#31616;&#21333;&#39640;&#25928;&#30340;&#32479;&#35745;&#27979;&#35797;&#65292;&#31216;&#20026;&#8220;&#21521;&#21518;&#22522;&#32447;&#8221;&#65292;&#21487;&#20197;&#23637;&#31034;&#27169;&#22411;&#26159;&#21542;&#20197;&#21450;&#22312;&#20309;&#31181;&#31243;&#24230;&#19978;&#37325;&#26032;&#35762;&#36848;&#20102;&#36807;&#21435;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#29702;&#35770;&#20026;&#35299;&#37322;&#21521;&#21518;&#22522;&#32447;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#24182;&#24314;&#31435;&#20102;&#19981;&#21516;&#22522;&#32447;&#21644;&#29087;&#24713;&#32479;&#35745;&#27010;&#24565;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#21521;&#21518;&#22522;&#32447;&#65292;&#21487;&#20197;&#23545;&#39044;&#27979;&#31995;&#32479;&#36827;&#34892;&#23457;&#35745;&#65292;&#21363;&#20351;&#21482;&#25552;&#20379;&#20102;&#32972;&#26223;&#21464;&#37327;&#21644;&#31995;&#32479;&#30340;&#39044;&#27979;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#22312;&#20174;&#32437;&#21521;&#38754;&#26495;&#35843;&#26597;&#20013;&#34893;&#29983;&#20986;&#30340;&#19981;&#21516;&#39044;&#27979;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#65292;&#23637;&#31034;&#20102;&#23558;&#20854;&#32435;&#20837;&#30340;&#20415;&#25463;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.11673v2 Announce Type: replace  Abstract: When does a machine learning model predict the future of individuals and when does it recite patterns that predate the individuals? In this work, we propose a distinction between these two pathways of prediction, supported by theoretical, empirical, and normative arguments. At the center of our proposal is a family of simple and efficient statistical tests, called backward baselines, that demonstrate if, and to what extent, a model recounts the past. Our statistical theory provides guidance for interpreting backward baselines, establishing equivalences between different baselines and familiar statistical concepts. Concretely, we derive a meaningful backward baseline for auditing a prediction system as a black box, given only background variables and the system's predictions. Empirically, we evaluate the framework on different prediction tasks derived from longitudinal panel surveys, demonstrating the ease and effectiveness of incorpo
&lt;/p&gt;</description></item><item><title>OpenXAI &#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#25143;&#21487;&#36731;&#26494;&#25193;&#23637;&#21644;&#27604;&#36739;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2206.11104</link><description>&lt;p&gt;
OpenXAI: &#36808;&#21521;&#36879;&#26126;&#35780;&#20272;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
OpenXAI: Towards a Transparent Evaluation of Model Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.11104
&lt;/p&gt;
&lt;p&gt;
OpenXAI &#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#25143;&#21487;&#36731;&#26494;&#25193;&#23637;&#21644;&#27604;&#36739;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#20294;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20316;&#38750;&#24120;&#23569;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenXAI&#65292;&#19968;&#20010;&#20840;&#38754;&#19988;&#21487;&#25193;&#23637;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#12290;OpenXAI&#21253;&#25324;&#20197;&#19979;&#20851;&#38190;&#32452;&#20214;&#65306;&#65288;i&#65289;&#28789;&#27963;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#21644;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#29305;&#24449;&#24402;&#23646;&#26041;&#27861;&#30340;&#38598;&#21512;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#26041;&#27861;&#24544;&#23454;&#24230;&#12289;&#31283;&#23450;&#24615;&#65288;&#40065;&#26834;&#24615;&#65289;&#21644;&#20844;&#24179;&#24615;&#30340;&#21313;&#19968;&#31181;&#37327;&#21270;&#24230;&#37327;&#26631;&#20934;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#22810;&#31181;&#24230;&#37327;&#26631;&#20934;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#20960;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#27604;&#36739;&#12290;OpenXAI&#26131;&#20110;&#25193;&#23637;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#35780;&#20272;&#33258;&#23450;&#20041;&#35299;&#37322;&#26041;&#27861;&#24182;&#23558;&#20854;&#32435;&#20837;&#25105;&#20204;&#30340;&#25490;&#34892;&#27036;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.11104v4 Announce Type: replace-cross  Abstract: While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, and (ii) open-source implementations of eleven quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, in turn providing comparisons of several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI 
&lt;/p&gt;</description></item><item><title>NAS-Bench-Graph&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;&#23545;GraphNAS&#36827;&#34892;&#32479;&#19968;&#12289;&#21487;&#22797;&#29616;&#21644;&#39640;&#25928;&#30340;&#35780;&#20272;&#65292;&#35299;&#20915;&#20102;&#23454;&#39564;&#35774;&#32622;&#19981;&#19968;&#33268;&#21644;&#35745;&#31639;&#38656;&#27714;&#22823;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2206.09166</link><description>&lt;p&gt;
NAS-Bench-Graph: &#22522;&#20110;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.09166
&lt;/p&gt;
&lt;p&gt;
NAS-Bench-Graph&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;&#23545;GraphNAS&#36827;&#34892;&#32479;&#19968;&#12289;&#21487;&#22797;&#29616;&#21644;&#39640;&#25928;&#30340;&#35780;&#20272;&#65292;&#35299;&#20915;&#20102;&#23454;&#39564;&#35774;&#32622;&#19981;&#19968;&#33268;&#21644;&#35745;&#31639;&#38656;&#27714;&#22823;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;GraphNAS&#65289;&#26368;&#36817;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#20005;&#37325;&#38459;&#30861;&#20102;&#23545;GraphNAS&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#30001;&#20110;&#23454;&#39564;&#35774;&#32622;&#27809;&#26377;&#20849;&#35782;&#65292;&#19981;&#21516;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#36890;&#24120;&#19981;&#21487;&#27604;&#36739;&#65292;&#29978;&#33267;&#19981;&#21487;&#37325;&#22797;&#65292;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;GraphNAS&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#36825;&#20351;&#20854;&#39640;&#24230;&#20302;&#25928;&#19988;&#19981;&#26131;&#35775;&#38382;&#65292;&#38480;&#21046;&#20102;&#27809;&#26377;&#22823;&#35268;&#27169;&#35745;&#31639;&#36164;&#28304;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NAS-Bench-Graph&#65292;&#36825;&#26159;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;&#23545;GraphNAS&#36827;&#34892;&#32479;&#19968;&#12289;&#21487;&#22797;&#29616;&#21644;&#39640;&#25928;&#30340;&#35780;&#20272;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#12289;&#34920;&#36798;&#21147;&#20016;&#23500;&#32780;&#32039;&#20945;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#28085;&#30422;&#20102;26,206&#31181;&#29420;&#29305;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.09166v2 Announce Type: replace  Abstract: Graph neural architecture search (GraphNAS) has recently aroused considerable attention in both academia and industry. However, two key challenges seriously hinder the further research of GraphNAS. First, since there is no consensus for the experimental setting, the empirical results in different research papers are often not comparable and even not reproducible, leading to unfair comparisons. Secondly, GraphNAS often needs extensive computations, which makes it highly inefficient and inaccessible to researchers without access to large-scale computation. To solve these challenges, we propose NAS-Bench-Graph, a tailored benchmark that supports unified, reproducible, and efficient evaluations for GraphNAS. Specifically, we construct a unified, expressive yet compact search space, covering 26,206 unique graph neural network (GNN) architectures and propose a principled evaluation protocol. To avoid unnecessary repetitive training, we hav
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#65292;&#20219;&#20309;&#32531;&#24930;&#21464;&#21270;&#30340;&#23545;&#25239;&#24335;&#32769;&#34382;&#26426;&#31639;&#27861;&#22312;&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#33021;&#22815;&#36798;&#21040;&#26368;&#20248;&#26399;&#26395;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/2205.09056</link><description>&lt;p&gt;
&#32531;&#24930;&#21464;&#21270;&#30340;&#23545;&#25239;&#24335;&#36172;&#21338;&#31639;&#27861;&#22312;&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#25928;&#29575;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
Slowly Changing Adversarial Bandit Algorithms are Efficient for Discounted MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.09056
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#65292;&#20219;&#20309;&#32531;&#24930;&#21464;&#21270;&#30340;&#23545;&#25239;&#24335;&#32769;&#34382;&#26426;&#31639;&#27861;&#22312;&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#33021;&#22815;&#36798;&#21040;&#26368;&#20248;&#26399;&#26395;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#23558;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#27867;&#21270;&#20026;&#20855;&#26377;&#26356;&#38271;&#35745;&#21010;&#35270;&#37326;&#21644;&#26410;&#30693;&#36716;&#31227;&#20869;&#26680;&#30340;&#39069;&#22806;&#22256;&#38590;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#25240;&#25187;&#26080;&#38480;&#35270;&#37326;&#34920;&#26684;&#24378;&#21270;&#23398;&#20064;&#21040;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#40657;&#30418;&#38477;&#32500;&#65292;&#20854;&#20013;&#29305;&#21035;&#26159;&#22312;&#27599;&#20010;&#29366;&#24577;&#25918;&#32622;&#19968;&#20010;&#29420;&#31435;&#30340;&#32769;&#34382;&#26426;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36941;&#21382;&#24615;&#21644;&#24555;&#36895;&#28151;&#21512;&#30340;&#20551;&#35774;&#19979;&#65292;&#20219;&#20309;&#22312;&#23545;&#25239;&#24335;&#32769;&#34382;&#26426;&#35774;&#32622;&#20013;&#23454;&#29616;&#26368;&#20248;&#36951;&#25022;&#30340;&#32531;&#24930;&#21464;&#21270;&#23545;&#25239;&#24335;&#32769;&#34382;&#26426;&#31639;&#27861;&#20063;&#21487;&#20197;&#22312;&#26080;&#38480;&#35270;&#37326;&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#26368;&#20248;&#26399;&#26395;&#36951;&#25022;&#65292;&#20851;&#20110;&#22238;&#21512;&#25968;$T$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#21152;&#26435;&#31639;&#27861;&#30340;&#29305;&#23450;&#23454;&#20363;&#26469;&#26816;&#39564;&#25105;&#20204;&#30340;&#38477;&#32500;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.09056v3 Announce Type: replace  Abstract: Reinforcement learning generalizes multi-armed bandit problems with additional difficulties of a longer planning horizon and unknown transition kernel. We explore a black-box reduction from discounted infinite-horizon tabular reinforcement learning to multi-armed bandits, where, specifically, an independent bandit learner is placed in each state. We show that, under ergodicity and fast mixing assumptions, any slowly changing adversarial bandit algorithm achieving optimal regret in the adversarial bandit setting can also attain optimal expected regret in infinite-horizon discounted Markov decision processes, with respect to the number of rounds $T$. Furthermore, we examine our reduction using a specific instance of the exponential-weight algorithm.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;&#36807;&#31243;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#33021;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2201.13192</link><description>&lt;p&gt;
&#38024;&#23545;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20266;&#26631;&#31614;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Pseudo-label Selection for Positive-Unlabeled Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.13192
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;&#36807;&#31243;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#33021;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#65288;PUL&#65289;&#26088;&#22312;&#20174;&#20165;&#20855;&#26377;&#27491;&#26679;&#26412;&#21644;&#26410;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#20108;&#20803;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;&#29616;&#23454;&#24212;&#29992;&#36890;&#24120;&#28041;&#21450;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#31034;&#20363;&#23646;&#20110;&#19968;&#31867;&#65292;&#20294;&#22823;&#22810;&#25968;&#24403;&#20195;PUL&#26041;&#27861;&#24182;&#26410;&#30740;&#31350;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;&#36807;&#31243;&#65288;PUUPL&#65289;&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26657;&#20934;&#38382;&#39064;&#65306;&#36890;&#36807;&#22686;&#24378;&#23569;&#25968;&#31867;&#30340;&#20449;&#21495;&#65292;&#20266;&#26631;&#31614;&#20174;&#26410;&#26631;&#35760;&#38598;&#20013;&#25193;&#23637;&#20102;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#26174;&#24335;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38450;&#27490;&#20102;&#26377;&#23475;&#30340;&#30830;&#35748;&#20559;&#35265;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;PUUPL&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.13192v3 Announce Type: replace-cross  Abstract: Positive-unlabeled learning (PUL) aims at learning a binary classifier from only positive and unlabeled training data. Even though real-world applications often involve imbalanced datasets where the majority of examples belong to one class, most contemporary approaches to PUL do not investigate performance in this setting, thus severely limiting their applicability in practice. In this work, we thus propose to tackle the issues of imbalanced datasets and model calibration in a PUL setting through an uncertainty-aware pseudo-labeling procedure (PUUPL): by boosting the signal from the minority class, pseudo-labeling expands the labeled dataset with new samples from the unlabeled set, while explicit uncertainty quantification prevents the emergence of harmful confirmation bias leading to increased predictive performance. Within a series of experiments, PUUPL yields substantial performance gains in highly imbalanced settings while 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;DSQN&#21033;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33021;&#28304;&#39640;&#25928;&#30340;&#25511;&#21046;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2201.09754</link><description>&lt;p&gt;
&#24102;&#27874;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Spiking Q-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.09754
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;DSQN&#21033;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33021;&#28304;&#39640;&#25928;&#30340;&#25511;&#21046;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#29305;&#27530;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#65292;&#26399;&#26395;&#36890;&#36807;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#20197;&#26356;&#23569;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#23558;SNNs&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#65292;&#20026;&#23454;&#29616;&#29616;&#23454;&#25511;&#21046;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#39640;&#25928;&#33021;&#28304;&#26041;&#24335;&#12290;&#30446;&#21069;&#20165;&#26377;&#23569;&#25968;&#22522;&#20110;SNN&#30340;RL&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#35201;&#20040;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#35201;&#20040;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#26469;&#20272;&#31639;&#20540;&#20989;&#25968;&#12290;&#21069;&#32773;&#38656;&#35201;&#20026;&#27599;&#20010;&#22330;&#26223;&#35843;&#25972;&#22823;&#37327;&#36229;&#21442;&#25968;&#65292;&#32780;&#21518;&#32773;&#38480;&#21046;&#20102;&#19981;&#21516;&#31867;&#22411;RL&#31639;&#27861;&#30340;&#24212;&#29992;&#24182;&#24573;&#30053;&#20102;&#35757;&#32451;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#36739;&#22823;&#12290;&#20026;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#26118;&#34411;&#20013;&#21457;&#29616;&#30340;&#38750;&#33033;&#20914;&#38388;&#31070;&#32463;&#20803;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#33033;&#20914;Q&#32593;&#32476;&#65288;DSQN&#65289;&#65292;&#20351;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#21487;&#20197;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.09754v2 Announce Type: replace-cross  Abstract: With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of non-spiking neurons as the representation of Q-value, which can direct
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23884;&#22871;&#38750;&#21442;&#25968;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#30340;&#23545;&#25239;&#20272;&#35745;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#22240;&#26524;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20855;&#26377;&#38480;&#21046;&#30149;&#24577;&#24615;&#22797;&#21512;&#25216;&#26415;&#12289;&#22810;&#31181;&#36866;&#24212;&#27169;&#22411;&#21644;&#25193;&#23637;&#21040;&#22240;&#26524;&#20989;&#25968;&#31561;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2112.14249</link><description>&lt;p&gt;
&#23884;&#22871;&#38750;&#21442;&#25968;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#65306;&#38271;&#26399;&#12289;&#20013;&#20171;&#21644;&#26102;&#21464;&#27835;&#30103;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Nested Nonparametric Instrumental Variable Regression: Long Term, Mediated, and Time Varying Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.14249
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23884;&#22871;&#38750;&#21442;&#25968;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#30340;&#23545;&#25239;&#20272;&#35745;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#22240;&#26524;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20855;&#26377;&#38480;&#21046;&#30149;&#24577;&#24615;&#22797;&#21512;&#25216;&#26415;&#12289;&#22810;&#31181;&#36866;&#24212;&#27169;&#22411;&#21644;&#25193;&#23637;&#21040;&#22240;&#26524;&#20989;&#25968;&#31561;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#38754;&#26495;&#25968;&#25454;&#27169;&#22411;&#20013;&#30340;&#20960;&#20010;&#22240;&#26524;&#21442;&#25968;&#26159;&#31216;&#20026;&#23884;&#22871;&#38750;&#21442;&#25968;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#65288;nested NPIV&#65289;&#30340;&#20989;&#25968;&#30340;&#26631;&#37327;&#24635;&#32467;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#35782;&#21035;&#20986;&#38271;&#26399;&#12289;&#20013;&#20171;&#21644;&#26102;&#21464;&#27835;&#30103;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#20284;&#20046;&#19981;&#23384;&#22312;&#20851;&#20110;&#23884;&#22871;NPIV&#30340;&#20808;&#21069;&#20272;&#35745;&#37327;&#25110;&#20445;&#35777;&#65292;&#36825;&#26679;&#23601;&#26080;&#27861;&#28789;&#27963;&#22320;&#20272;&#35745;&#21644;&#25512;&#26029;&#36825;&#20123;&#22240;&#26524;&#21442;&#25968;&#12290;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#30001;&#20110;&#23884;&#22871;&#36870;&#38382;&#39064;&#32780;&#23548;&#33268;&#30340;&#22797;&#21512;&#30149;&#24577;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23884;&#22871;NPIV&#30340;&#23545;&#25239;&#20272;&#35745;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#22240;&#26524;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#38750;&#28176;&#36817;&#20998;&#26512;&#20855;&#26377;&#19977;&#20010;&#26174;&#33879;&#29305;&#24449;&#65306;&#65288;i&#65289;&#24341;&#20837;&#38480;&#21046;&#30149;&#24577;&#24615;&#22797;&#21512;&#30340;&#25216;&#26415;&#65307;&#65288;ii&#65289;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65307;&#65288;iii&#65289;&#25193;&#23637;&#21040;&#22240;&#26524;&#20989;&#25968;&#65292;&#20363;&#22914;&#38271;&#26399;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.14249v3 Announce Type: replace-cross  Abstract: Several causal parameters in short panel data models are scalar summaries of a function called a nested nonparametric instrumental variable regression (nested NPIV). Examples include long term, mediated, and time varying treatment effects identified using proxy variables. However, it appears that no prior estimators or guarantees for nested NPIV exist, preventing flexible estimation and inference for these causal parameters. A major challenge is compounding ill posedness due to the nested inverse problems. We analyze adversarial estimators of nested NPIV, and provide sufficient conditions for efficient inference on the causal parameter. Our nonasymptotic analysis has three salient features: (i) introducing techniques that limit how ill posedness compounds; (ii) accommodating neural networks, random forests, and reproducing kernel Hilbert spaces; and (iii) extending to causal functions, e.g. long term heterogeneous treatment eff
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PMFL&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#38024;&#23545;&#24322;&#26500;&#20219;&#21153;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#30103;&#35760;&#24405;&#31561;&#39046;&#22495;&#24212;&#29992;&#21463;&#38480;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2112.05321</link><description>&lt;p&gt;
PMFL: &#38024;&#23545;&#24322;&#26500;&#20219;&#21153;&#30340;&#37096;&#20998;&#20803;&#32852;&#37030;&#23398;&#20064;&#21450;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PMFL: Partial Meta-Federated Learning for heterogeneous tasks and its applications on real-world medical records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.05321
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PMFL&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#38024;&#23545;&#24322;&#26500;&#20219;&#21153;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#30103;&#35760;&#24405;&#31561;&#39046;&#22495;&#24212;&#29992;&#21463;&#38480;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2112.05321v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21644;&#28789;&#27963;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#24403;&#36890;&#20449;&#25216;&#26415;&#39640;&#36895;&#21457;&#23637;&#65292;&#22914;&#20170;&#21487;&#20197;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25910;&#38598;&#21040;&#21069;&#25152;&#26410;&#26377;&#30340;&#22823;&#37327;&#25968;&#25454;&#26102;&#12290;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#32593;&#32476;&#20013;&#25152;&#26377;&#35774;&#22791;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#36866;&#29992;&#20110;&#21516;&#36136;&#25968;&#25454;&#21644;&#20219;&#21153;&#65292;&#20294;&#23558;&#35813;&#26041;&#27861;&#36866;&#24212;&#19981;&#21516;&#30340;&#24322;&#26500;&#25968;&#25454;&#21644;&#20219;&#21153;&#20998;&#24067;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#19968;&#38480;&#21046;&#24050;&#32463;&#38480;&#21046;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#21463;&#21040;&#20803;&#23398;&#20064;&#30340;&#22522;&#26412;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35813;&#31639;&#27861;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.05321v2 Announce Type: replace  Abstract: Federated machine learning is a versatile and flexible tool to utilize distributed data from different sources, especially when communication technology develops rapidly and an unprecedented amount of data could be collected on mobile devices nowadays. Federated learning method exploits not only the data but the computational power of all devices in the network to achieve more efficient model training. Nevertheless, while most traditional federated learning methods work well for homogeneous data and tasks, adapting the method to a different heterogeneous data and task distribution is challenging. This limitation has constrained the applications of federated learning in real-world contexts, especially in healthcare settings. Inspired by the fundamental idea of meta-learning, in this study we propose a new algorithm, which is an integration of federated learning and meta-learning, to tackle this issue. In addition, owing to the advanta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#39640;&#32423;&#22270;&#34920;&#31034;&#24182;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#23454;&#29616;&#31283;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2111.10657</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#25512;&#24191;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalizing Graph Neural Networks on Out-Of-Distribution Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.10657
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#39640;&#32423;&#22270;&#34920;&#31034;&#24182;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#22270;&#19978;&#23454;&#29616;&#31283;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#27809;&#26377;&#32771;&#34385;&#35757;&#32451;&#21644;&#27979;&#35797;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#65292;&#23548;&#33268;GNNs&#22312;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#35774;&#32622;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#19979;&#38477;&#12290;&#36825;&#31181;&#36864;&#21270;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#22823;&#22810;&#25968;GNNs&#26159;&#22522;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#24320;&#21457;&#30340;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#30340;&#32454;&#24494;&#32479;&#35745;&#30456;&#20851;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#21363;&#20351;&#36825;&#26159;&#19968;&#31181;&#20266;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20266;&#30456;&#20851;&#24615;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#23548;&#33268;GNNs&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#28040;&#38500;&#20266;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#23545;&#20110;&#31283;&#23450;&#30340;GNNs&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StableGNN&#30340;&#36890;&#29992;&#22240;&#26524;&#34920;&#31034;&#26694;&#26550;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#39318;&#20808;&#20174;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#32423;&#34920;&#31034;&#65292;&#28982;&#21518;&#20511;&#21161;&#22240;&#26524;&#25512;&#26029;&#30340;&#21306;&#20998;&#33021;&#21147;&#26469;&#24110;&#21161;&#27169;&#22411;&#33719;&#24471;&#31283;&#23450;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.10657v3 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are proposed without considering the agnostic distribution shifts between training and testing graphs, inducing the degeneration of the generalization ability of GNNs on Out-Of-Distribution (OOD) settings. The fundamental reason for such degeneration is that most GNNs are developed based on the I.I.D hypothesis. In such a setting, GNNs tend to exploit subtle statistical correlations existing in the training set for predictions, even though it is a spurious correlation. However, such spurious correlations may change in testing environments, leading to the failure of GNNs. Therefore, eliminating the impact of spurious correlations is crucial for stable GNNs. To this end, we propose a general causal representation framework, called StableGNN. The main idea is to extract high-level representations from graph data first and resort to the distinguishing ability of causal inference to help the model get ri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;NAS&#20219;&#21153;&#36716;&#21270;&#20026;&#31616;&#21333;&#30340;Pareto&#21344;&#20248;&#20998;&#31867;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#30340;Pareto&#36827;&#21270;&#26041;&#27861;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;NAS&#30340;&#25628;&#32034;&#36807;&#31243;</title><link>https://arxiv.org/abs/2109.07582</link><description>&lt;p&gt;
Pareto-wise Ranking Classifier for Multi-objective Evolutionary Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
Pareto-wise Ranking Classifier for Multi-objective Evolutionary Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.07582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;NAS&#20219;&#21153;&#36716;&#21270;&#20026;&#31616;&#21333;&#30340;Pareto&#21344;&#20248;&#20998;&#31867;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#30340;Pareto&#36827;&#21270;&#26041;&#27861;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;NAS&#30340;&#25628;&#32034;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#26102;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#33258;&#21160;&#25214;&#21040;&#31526;&#21512;&#19981;&#21516;&#35774;&#35745;&#30446;&#26631;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#21033;&#29992;&#26367;&#20195;&#27169;&#22411;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#39044;&#27979;&#20505;&#36873;&#26550;&#26500;&#30340;&#35814;&#32454;&#24615;&#33021;&#65288;&#20363;&#22914;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#22823;&#23567;&#65289;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#22797;&#26434;&#19988;&#20302;&#25928;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#39640;&#25928;&#30340;Pareto&#20998;&#31867;&#22120;&#65292;&#23558;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;NAS&#20219;&#21153;&#36716;&#21270;&#20026;&#31616;&#21333;&#30340;Pareto&#21344;&#20248;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#31616;&#21270;NAS&#30340;&#25628;&#32034;&#36807;&#31243;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#30340;Pareto&#36827;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19968;&#27425;&#24615;NAS&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#22312;&#32447;&#20998;&#31867;&#22120;&#34987;&#35757;&#32451;&#26469;&#39044;&#27979;&#20505;&#36873;&#26550;&#26500;&#19982;&#26500;&#24314;&#30340;&#21442;&#32771;&#26550;&#26500;&#20043;&#38388;&#30340;&#21344;&#20248;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#26367;&#20195;&#27169;&#22411;&#26469;&#25311;&#21512;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.07582v2 Announce Type: replace  Abstract: In the deployment of deep neural models, how to effectively and automatically find feasible deep models under diverse design objectives is fundamental. Most existing neural architecture search (NAS) methods utilize surrogates to predict the detailed performance (e.g., accuracy and model size) of a candidate architecture during the search, which however is complicated and inefficient. In contrast, we aim to learn an efficient Pareto classifier to simplify the search process of NAS by transforming the complex multi-objective NAS task into a simple Pareto-dominance classification task. To this end, we propose a classification-wise Pareto evolution approach for one-shot NAS, where an online classifier is trained to predict the dominance relationship between the candidate and constructed reference architectures, instead of using surrogates to fit the objective functions. The main contribution of this study is to change supernet adaption i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38480;&#26102;&#38388;&#36328;&#24230;&#30340;&#38646;&#21644;&#38543;&#26426;&#21338;&#24328;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#20934;&#21017;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;$O(HS\sqrt{AT})$&#12290;</title><link>https://arxiv.org/abs/2109.03396</link><description>&lt;p&gt;
&#38024;&#23545;&#26410;&#30693;&#38646;&#21644;&#38543;&#26426;&#21338;&#24328;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#31639;&#27861;&#21450;&#20854;&#20219;&#24847;&#23545;&#25163;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with an Arbitrary Opponent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.03396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38480;&#26102;&#38388;&#36328;&#24230;&#30340;&#38646;&#21644;&#38543;&#26426;&#21338;&#24328;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#20934;&#21017;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;$O(HS\sqrt{AT})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#39564;&#37319;&#26679;&#24378;&#21270;&#23398;&#20064;&#38646;&#21644;&#38543;&#26426;&#21338;&#24328;&#65288;PSRL-ZSG&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#20934;&#21017;&#30340;&#26080;&#38480;&#26102;&#38388;&#36328;&#24230;&#30340;&#38646;&#21644;&#38543;&#26426;&#21338;&#24328;&#20013;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#20026;$O(HS\sqrt{AT})$&#12290;&#20854;&#20013;$H$&#26159;&#20559;&#24046;&#20989;&#25968;&#36328;&#24230;&#30340;&#19978;&#30028;&#65292;$S$&#26159;&#29366;&#24577;&#25968;&#37327;&#65292;$A$&#26159;&#32852;&#21512;&#21160;&#20316;&#25968;&#37327;&#65292;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#23545;&#25163;&#26080;&#27861;&#25511;&#21046;&#19988;&#21487;&#20197;&#37319;&#21462;&#20219;&#24847;&#20219;&#24847;&#19982;&#21382;&#21490;&#30456;&#20851;&#30340;&#31574;&#30053;&#30340;&#22312;&#32447;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#36951;&#25022;&#30028;&#25913;&#36827;&#20102;&#39759;&#31561;&#20154;(2017)&#22312;&#30456;&#21516;&#20551;&#35774;&#19979;&#30340;&#26368;&#20339;&#36951;&#25022;&#30028;$O(\sqrt[3]{DS^2AT^2})$&#65292;&#24182;&#19988;&#19982;&#22312;$T$&#19978;&#30340;&#29702;&#35770;&#19979;&#30028;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.03396v2 Announce Type: replace  Abstract: In this paper, we propose Posterior Sampling Reinforcement Learning for Zero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that achieves Bayesian regret bound of $O(HS\sqrt{AT})$ in the infinite-horizon zero-sum stochastic games with average-reward criterion. Here $H$ is an upper bound on the span of the bias function, $S$ is the number of states, $A$ is the number of joint actions and $T$ is the horizon. We consider the online setting where the opponent can not be controlled and can take any arbitrary time-adaptive history-dependent strategy. Our regret bound improves on the best existing regret bound of $O(\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) under the same assumption and matches the theoretical lower bound in $T$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#8220;&#19978;&#38480;&#21453;&#20107;&#23454;&#32622;&#20449;&#21306;&#38388;&#8221;&#65288;UCCB&#65289;&#26159;&#38024;&#23545;&#19968;&#33324;&#19978;&#19979;&#25991;&#36172;&#21338;&#35774;&#35745;&#20048;&#35266;&#31639;&#27861;&#30340;&#26032;&#21407;&#21017;&#65292;&#36890;&#36807;&#22312;&#31574;&#30053;&#31354;&#38388;&#20013;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#32780;&#38750;&#20687;UCB&#37027;&#26679;&#22312;&#34892;&#21160;&#31354;&#38388;&#20013;&#65292;&#36825;&#20351;&#24471;&#31639;&#27861;&#22312;&#22788;&#29702;&#19968;&#33324;&#20989;&#25968;&#31867;&#21644;&#22823;&#19978;&#19979;&#25991;&#31354;&#38388;&#26102;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2007.07876</link><description>&lt;p&gt;
&#19978;&#38480;&#21453;&#20107;&#23454;&#32622;&#20449;&#21306;&#38388;&#65306;&#19968;&#31181;&#38754;&#21521;&#19978;&#19979;&#25991;&#36172;&#21338;&#30340;&#26032;&#20048;&#35266;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Upper Counterfactual Confidence Bounds: a New Optimism Principle for Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#8220;&#19978;&#38480;&#21453;&#20107;&#23454;&#32622;&#20449;&#21306;&#38388;&#8221;&#65288;UCCB&#65289;&#26159;&#38024;&#23545;&#19968;&#33324;&#19978;&#19979;&#25991;&#36172;&#21338;&#35774;&#35745;&#20048;&#35266;&#31639;&#27861;&#30340;&#26032;&#21407;&#21017;&#65292;&#36890;&#36807;&#22312;&#31574;&#30053;&#31354;&#38388;&#20013;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#32780;&#38750;&#20687;UCB&#37027;&#26679;&#22312;&#34892;&#21160;&#31354;&#38388;&#20013;&#65292;&#36825;&#20351;&#24471;&#31639;&#27861;&#22312;&#22788;&#29702;&#19968;&#33324;&#20989;&#25968;&#31867;&#21644;&#22823;&#19978;&#19979;&#25991;&#31354;&#38388;&#26102;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20048;&#35266;&#21407;&#21017;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26159;&#22810;&#33218;&#36172;&#21338;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#21644;&#25104;&#21151;&#30340;&#29702;&#24565;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20048;&#35266;&#31639;&#27861;&#65288;&#20027;&#35201;&#26159;UCB&#21450;&#20854;&#21464;&#31181;&#65289;&#36890;&#24120;&#26080;&#27861;&#22788;&#29702;&#19968;&#33324;&#30340;&#20989;&#25968;&#31867;&#21644;&#22823;&#30340;&#19978;&#19979;&#25991;&#31354;&#38388;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31163;&#32447;&#22238;&#24402;&#39044;&#35328;&#26426;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#36172;&#21338;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36890;&#29992;&#21407;&#21017;&#26469;&#35774;&#35745;&#20048;&#35266;&#31639;&#27861;&#65292;&#31216;&#20026;&#8220;&#19978;&#38480;&#21453;&#20107;&#23454;&#32622;&#20449;&#21306;&#38388;&#8221;&#65288;UCCB&#65289;&#12290;UCCB&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#22312;&#31574;&#30053;&#31354;&#38388;&#20013;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#32780;&#19981;&#26159;&#20687;UCB&#37027;&#26679;&#22312;&#34892;&#21160;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#22788;&#29702;&#19968;&#33324;&#20989;&#25968;&#31867;&#21644;&#22823;&#19978;&#19979;&#25991;&#31354;&#38388;&#26041;&#38754;&#26082;&#26159;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#30340;&#65292;&#21448;&#22312;&#35745;&#31639;&#19978;&#26159;&#39640;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;UCCB&#21407;&#21017;&#21487;&#20197;&#36731;&#26494;&#22320;&#25193;&#23637;&#21040;&#26080;&#38480;&#21160;&#20316;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#36172;&#21338;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2007.07876v4 Announce Type: replace  Abstract: The principle of optimism in the face of uncertainty is one of the most widely used and successful ideas in multi-armed bandits and reinforcement learning. However, existing optimistic algorithms (primarily UCB and its variants) often struggle to deal with general function classes and large context spaces. In this paper, we study general contextual bandits with an offline regression oracle and propose a simple, generic principle to design optimistic algorithms, dubbed "Upper Counterfactual Confidence Bounds" (UCCB). The key innovation of UCCB is building confidence bounds in policy space, rather than in action space as is done in UCB. We demonstrate that these algorithms are provably optimal and computationally efficient in handling general function classes and large context spaces. Furthermore, we illustrate that the UCCB principle can be seamlessly extended to infinite-action general contextual bandits, provide the first solutions 
&lt;/p&gt;</description></item><item><title>ENCORE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#25216;&#26415;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20462;&#22797;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#38169;&#35823;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#20351;&#29992;&#30340;LSTM&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#20102;42&#20010;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;16&#20010;&#20808;&#21069;&#26410;&#34987;&#20462;&#22797;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/1906.08691</link><description>&lt;p&gt;
ENCORE&#65306;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#38598;&#25104;&#23398;&#20064;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
ENCORE: Ensemble Learning using Convolution Neural Machine Translation for Automatic Program Repair
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1906.08691
&lt;/p&gt;
&lt;p&gt;
ENCORE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#25216;&#26415;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20462;&#22797;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#38169;&#35823;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#20351;&#29992;&#30340;LSTM&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#20102;42&#20010;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;16&#20010;&#20808;&#21069;&#26410;&#34987;&#20462;&#22797;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:1906.08691v2 &#36890;&#21578;&#31867;&#22411;: &#20132;&#21449;&#26367;&#25442; &#25688;&#35201;: &#33258;&#21160;&#29983;&#25104;&#39564;&#35777;&#65288;G&amp;V&#65289;&#31243;&#24207;&#20462;&#22797;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#30828;&#32534;&#30721;&#35268;&#21017;&#65292;&#20165;&#20462;&#22797;&#36981;&#24490;&#29305;&#23450;&#27169;&#24335;&#30340;&#38169;&#35823;&#65292;&#24182;&#19988;&#38590;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#32534;&#31243;&#35821;&#35328;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ENCORE&#65292;&#19968;&#31181;&#26032;&#30340;G&amp;V&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#30340;&#38598;&#25104;&#23398;&#20064;&#26469;&#33258;&#21160;&#20462;&#22797;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#21033;&#29992;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38543;&#26426;&#24615;&#26500;&#24314;&#22810;&#20010;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20462;&#22797;&#19981;&#21516;&#30340;&#38169;&#35823;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#23558;&#23427;&#20204;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#36825;&#31181;&#26032;&#30340;&#21367;&#31215;NMT&#26041;&#27861;&#20248;&#20110;&#20043;&#21069;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#20196;&#29260;&#20043;&#38388;&#30340;&#23616;&#37096;&#21644;&#38271;&#36317;&#31163;&#36830;&#25509;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;Defects4J&#21644;QuixBugs&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;ENCORE&#20462;&#22797;&#20102;42&#20010;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;16&#20010;&#26410;&#34987;&#29616;&#26377;&#25216;&#26415;&#20462;&#22797;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1906.08691v2 Announce Type: replace-cross  Abstract: Automated generate-and-validate (G&amp;V) program repair techniques typically rely on hard-coded rules, only fix bugs following specific patterns, and are hard to adapt to different programming languages. We propose ENCORE, a new G&amp;V technique, which uses ensemble learning on convolutional neural machine translation (NMT) models to automatically fix bugs in multiple programming languages.   We take advantage of the randomness in hyper-parameter tuning to build multiple models that fix different bugs and combine them using ensemble learning. This new convolutional NMT approach outperforms the standard long short-term memory (LSTM) approach used in previous work, as it better captures both local and long-distance connections between tokens.   Our evaluation on two popular benchmarks, Defects4J and QuixBugs, shows that ENCORE fixed 42 bugs, including 16 that have not been fixed by existing techniques. In addition, ENCORE is the first 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#20219;&#21153;&#32593;&#32476;&#35774;&#35745;&#65292;&#20197;&#23454;&#29616;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#20013;&#20849;&#20139;&#35745;&#31639;&#36164;&#28304;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/1902.03589</link><description>&lt;p&gt;
NeurAll: &#38754;&#21521;&#33258;&#21160;&#39550;&#39542;&#30340;&#32479;&#19968;&#35270;&#35273;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NeurAll: Towards a Unified Visual Perception Model for Automated Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1902.03589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#20219;&#21153;&#32593;&#32476;&#35774;&#35745;&#65292;&#20197;&#23454;&#29616;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#20013;&#20849;&#20139;&#35745;&#31639;&#36164;&#28304;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#37325;&#35201;&#30340;&#27773;&#36710;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#65292;&#21253;&#25324;&#30446;&#26631;&#35782;&#21035;&#12289;&#36816;&#21160;&#21644;&#28145;&#24230;&#20272;&#35745;&#12289;&#35270;&#35273;SLAM&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#26159;&#29420;&#31435;&#25506;&#32034;&#21644;&#24314;&#27169;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#20219;&#21153;&#32593;&#32476;&#35774;&#35745;&#65292;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#36890;&#36807;&#22312;&#25152;&#26377;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#26114;&#36149;&#30340;&#21021;&#22987;&#21367;&#31215;&#23618;&#26469;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#12290;&#20107;&#23454;&#19978;&#65292;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#37096;&#32626;&#30828;&#20214;&#19978;&#21487;&#29992;&#30340;&#26377;&#38480;&#22788;&#29702;&#33021;&#21147;&#12290;&#36824;&#26377;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#20219;&#21153;&#26469;&#35828;&#65292;&#22312;&#25552;&#39640;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#20854;&#20182;&#22909;&#22788;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#36731;&#24320;&#21457;&#24037;&#20316;&#37327;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#21487;&#20280;&#32553;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#29305;&#24449;&#22686;&#21152;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#29992;&#20110;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#30340;&#21508;&#31181;&#22522;&#20110;CNN&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1902.03589v3 Announce Type: replace-cross  Abstract: Convolutional Neural Networks (CNNs) are successfully used for the important automotive visual perception tasks including object recognition, motion and depth estimation, visual SLAM, etc. However, these tasks are typically independently explored and modeled. In this paper, we propose a joint multi-task network design for learning several tasks simultaneously. Our main motivation is the computational efficiency achieved by sharing the expensive initial convolutional layers between all tasks. Indeed, the main bottleneck in automated driving systems is the limited processing power available on deployment hardware. There is also some evidence for other benefits in improving accuracy for some tasks and easing development effort. It also offers scalability to add more tasks leveraging existing features and achieving better generalization. We survey various CNN based solutions for visual perception tasks in automated driving. Then we
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21028;&#21035;&#24335;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#32467;&#21512;&#20808;&#21069;&#30740;&#31350;&#30340;&#35789;&#20856;&#20808;&#39564;&#21644;&#34920;&#31034;&#27861;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#21452;&#35821;&#35789;&#20856;&#24402;&#32435;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20808;&#39564;&#21487;&#20197;&#25913;&#21892;&#35825;&#23548;&#30340;&#21452;&#35821;&#35789;&#20856;&#12290;</title><link>https://arxiv.org/abs/1808.09334</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21452;&#35821;&#35789;&#20856;&#24402;&#32435;&#30340;&#21028;&#21035;&#24335;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Discriminative Latent-Variable Model for Bilingual Lexicon Induction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1808.09334
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21028;&#21035;&#24335;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#32467;&#21512;&#20808;&#21069;&#30740;&#31350;&#30340;&#35789;&#20856;&#20808;&#39564;&#21644;&#34920;&#31034;&#27861;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#21452;&#35821;&#35789;&#20856;&#24402;&#32435;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#20808;&#39564;&#21487;&#20197;&#25913;&#21892;&#35825;&#23548;&#30340;&#21452;&#35821;&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#21452;&#35821;&#35789;&#20856;&#24402;&#32435;&#30340;&#21028;&#21035;&#24335;&#28508;&#21464;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;Haghighi&#31561;&#20154;&#65288;2008&#65289;&#30340;&#20108;&#20998;&#21305;&#37197;&#35789;&#20856;&#20808;&#39564;&#19982;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#65288;Artetxe&#31561;&#20154;&#65292;2017&#65289;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#39640;&#25928;&#30340;Viterbi EM&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#19979;&#23545;&#20845;&#31181;&#35821;&#35328;&#23545;&#36827;&#34892;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#24182;&#26174;&#31034;&#20808;&#39564;&#25913;&#21892;&#20102;&#35825;&#23548;&#30340;&#21452;&#35821;&#35789;&#20856;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#20808;&#21069;&#30340;&#24037;&#20316;&#35270;&#20026;&#31867;&#20284;&#39118;&#26684;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#23613;&#31649;&#26377;&#19981;&#21516;&#30340;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1808.09334v3 Announce Type: replace  Abstract: We introduce a novel discriminative latent variable model for bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a representation-based approach (Artetxe et al., 2017). To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical results on six language pairs under two metrics and show that the prior improves the induced bilingual lexicons. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.
&lt;/p&gt;</description></item><item><title>&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20351;&#29992;&#22810;&#20851;&#31995;&#25968;&#25454;&#30340;&#24067;&#25289;&#26684;&#25463;&#20811;&#25216;&#26415;&#22823;&#23398;&#20851;&#31995;&#23398;&#20064;&#36164;&#28304;&#24211;&#65292;&#21253;&#21547;&#22823;&#37327;SQL&#25968;&#25454;&#24211;&#65292;&#24182;&#30001;getML&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/1511.03086</link><description>&lt;p&gt;
&#24067;&#25289;&#26684;&#25463;&#20811;&#25216;&#26415;&#22823;&#23398;&#20851;&#31995;&#23398;&#20064;&#36164;&#28304;&#24211;
&lt;/p&gt;
&lt;p&gt;
The CTU Prague Relational Learning Repository
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1511.03086
&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20351;&#29992;&#22810;&#20851;&#31995;&#25968;&#25454;&#30340;&#24067;&#25289;&#26684;&#25463;&#20811;&#25216;&#26415;&#22823;&#23398;&#20851;&#31995;&#23398;&#20064;&#36164;&#28304;&#24211;&#65292;&#21253;&#21547;&#22823;&#37327;SQL&#25968;&#25454;&#24211;&#65292;&#24182;&#30001;getML&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#25289;&#26684;&#20851;&#31995;&#23398;&#20064;&#36164;&#28304;&#24211;&#30340;&#30446;&#30340;&#26159;&#25903;&#25345;&#20855;&#26377;&#22810;&#20851;&#31995;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12290;&#27492;&#36164;&#28304;&#24211;&#30446;&#21069;&#21253;&#21547;148&#20010;SQL&#25968;&#25454;&#24211;&#65292;&#25176;&#31649;&#22312;&#20301;&#20110;\url{https://relational-data.org}&#30340;&#20844;&#20849;MySQL&#26381;&#21153;&#22120;&#19978;&#12290;&#26381;&#21153;&#22120;&#30001;getML&#25552;&#20379;&#65292;&#20197;&#25903;&#25345;&#20851;&#31995;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#65288;\url{www.getml.com}&#65289;&#12290;&#21487;&#25628;&#32034;&#30340;&#20803;&#25968;&#25454;&#24211;&#25552;&#20379;&#20803;&#25968;&#25454;&#65288;&#20363;&#22914;&#25968;&#25454;&#24211;&#20013;&#30340;&#34920;&#25968;&#37327;&#12289;&#34920;&#20013;&#30340;&#34892;&#21015;&#25968;&#12289;&#33258;&#20851;&#32852;&#25968;&#37327;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1511.03086v2 Announce Type: replace  Abstract: The aim of the Prague Relational Learning Repository is to support machine learning research with multi-relational data. The repository currently contains 148 SQL databases hosted on a public MySQL server located at \url{https://relational-data.org}. The server is provided by getML to support the relational machine learning community (\url{www.getml.com}). A searchable meta-database provides metadata (e.g., the number of tables in the database, the number of rows and columns in the tables, the number of self-relationships).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;Rashomon Set of Optimal Trees (ROOT)&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#30340;&#23569;&#25968;&#20154;&#32676;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.14512</link><description>&lt;p&gt;
&#25105;&#20204;&#38169;&#36807;&#20102;&#35841;&#65311;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#25581;&#31034;&#23569;&#25968;&#20154;&#32676;&#29305;&#24449;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population. (arXiv:2401.14512v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;Rashomon Set of Optimal Trees (ROOT)&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#30340;&#23569;&#25968;&#20154;&#32676;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#22312;&#29702;&#35299;&#22240;&#26524;&#25928;&#24212;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#23558;&#25512;&#35770;&#25193;&#23637;&#21040;&#30446;&#26631;&#20154;&#32676;&#26102;&#38754;&#20020;&#25928;&#24212;&#24322;&#36136;&#24615;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#35782;&#21035;&#21644;&#25551;&#36848;&#23569;&#25968;&#20154;&#32676;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30446;&#26631;&#20154;&#32676;&#20197;&#25552;&#21319;&#26222;&#36866;&#24615;&#30340;&#21019;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#8212;&#8212;Rashomon Set of Optimal Trees (ROOT)&#65292;&#26469;&#25551;&#36848;&#23569;&#25968;&#20154;&#32676;&#12290;ROOT&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#30830;&#20445;&#26356;&#31934;&#30830;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ROOT&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#23569;&#25968;&#20154;&#32676;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#26377;&#25928;&#27807;&#36890;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#23637;&#29616;&#20102;&#25913;&#36827;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We ap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.14151</link><description>&lt;p&gt;
&#30495;&#30693;&#26469;&#28304;&#20110;&#23454;&#36341;&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20351;LLMs&#19982;&#20855;&#36523;&#29615;&#22659;&#23545;&#40784;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning. (arXiv:2401.14151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20247;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#31616;&#21333;&#30340;&#20915;&#31574;&#20219;&#21153;&#19978;&#32463;&#24120;&#22833;&#36133;&#65292;&#21407;&#22240;&#26159;LLMs&#20013;&#30340;&#30693;&#35782;&#19982;&#29615;&#22659;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31574;&#30053;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22987;&#32456;&#19982;&#29615;&#22659;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#38590;&#20197;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#20854;&#20013;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TWOSOME&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;RL&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#24182;&#23454;&#29616;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20934;&#22791;&#22909;&#30340;&#25968;&#25454;&#38598;&#25110;&#29615;&#22659;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#26597;&#35810;&#27599;&#20010;&#26377;&#25928;&#21160;&#20316;&#30340;&#32852;&#21512;&#27010;&#29575;&#20197;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#22686;&#24378;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#34892;&#20026;&#35780;&#20272;&#21644;&#36873;&#25321;&#31639;&#27861;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the acto
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#35774;&#35745;&#36215;&#21040;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#36991;&#20813;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#36825;&#19968;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#24471;&#21487;&#20197;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.13171</link><description>&lt;p&gt;
&#32452;&#21512;&#24335;&#29983;&#25104;&#36870;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Compositional Generative Inverse Design. (arXiv:2401.13171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13171
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#35774;&#35745;&#36215;&#21040;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#36991;&#20813;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#36825;&#19968;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#24471;&#21487;&#20197;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#35774;&#35745;&#26159;&#19968;&#31181;&#23547;&#27714;&#35774;&#35745;&#36755;&#20837;&#21464;&#37327;&#20197;&#20248;&#21270;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22312;&#26426;&#26800;&#24037;&#31243;&#21040;&#33322;&#22825;&#24037;&#31243;&#31561;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#36870;&#35774;&#35745;&#36890;&#24120;&#34987;&#26500;&#24314;&#25104;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#24448;&#24448;&#20250;&#38519;&#20837;&#23545;&#25239;&#27169;&#24335;&#65292;&#38459;&#30861;&#26377;&#25928;&#30340;&#25277;&#26679;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#25429;&#33719;&#30340;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#36991;&#20813;&#36825;&#31181;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#26679;&#19968;&#20010;&#35774;&#35745;&#31995;&#32479;&#26159;&#32452;&#21512;&#24615;&#30340;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#32467;&#21512;&#22810;&#20010;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20195;&#34920;&#25152;&#38656;&#31995;&#32479;&#30340;&#23376;&#32452;&#20214;&#65292;&#20174;&#32780;&#35774;&#35745;&#20855;&#26377;&#27599;&#20010;&#25351;&#23450;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;&#22312;&#19968;&#20010;N&#20307;&#30456;&#20114;&#20316;&#29992;&#20219;&#21153;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20108;&#32500;&#22810;&#32764;&#22411;&#35774;&#35745;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#32452;&#21512;&#23398;&#20064;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35774;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse design, where we seek to design input variables in order to optimize an underlying objective function, is an important problem that arises across fields such as mechanical engineering to aerospace engineering. Inverse design is typically formulated as an optimization problem, with recent works leveraging optimization across learned dynamics models. However, as models are optimized they tend to fall into adversarial modes, preventing effective sampling. We illustrate that by instead optimizing over the learned energy function captured by the diffusion model, we can avoid such adversarial examples and significantly improve design performance. We further illustrate how such a design system is compositional, enabling us to combine multiple different diffusion models representing subcomponents of our desired system to design systems with every specified component. In an N-body interaction task and a challenging 2D multi-airfoil design task, we demonstrate that by composing the learn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#31070;&#32463;&#20449;&#24687;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#35780;&#20272;&#22899;&#24615;&#36816;&#21160;&#21592;&#33041;&#38663;&#33633;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#20020;&#24202;&#26041;&#27861;&#65292;&#22312;&#22899;&#24615;&#36816;&#21160;&#21592;&#20013;&#35786;&#26029;&#33041;&#38663;&#33633;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#32780;&#36825;&#20123;&#26032;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#20998;&#26512;&#25214;&#20986;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#29983;&#29289;&#26426;&#21046;&#65292;&#20174;&#32780;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.13045</link><description>&lt;p&gt;
&#35780;&#20272;&#22899;&#24615;&#36816;&#21160;&#21592;&#33041;&#38663;&#33633;&#65306;&#31070;&#32463;&#20449;&#24687;&#23398;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Assessment of Sports Concussion in Female Athletes: A Role for Neuroinformatics?. (arXiv:2401.13045v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13045
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#31070;&#32463;&#20449;&#24687;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#35780;&#20272;&#22899;&#24615;&#36816;&#21160;&#21592;&#33041;&#38663;&#33633;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#20020;&#24202;&#26041;&#27861;&#65292;&#22312;&#22899;&#24615;&#36816;&#21160;&#21592;&#20013;&#35786;&#26029;&#33041;&#38663;&#33633;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#32780;&#36825;&#20123;&#26032;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#20998;&#26512;&#25214;&#20986;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#29983;&#29289;&#26426;&#21046;&#65292;&#20174;&#32780;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#22899;&#24615;&#36816;&#21160;&#21592;&#33041;&#38663;&#33633;&#30340;&#22797;&#26434;&#24615;&#21464;&#24471;&#26126;&#26174;&#12290;&#20256;&#32479;&#30340;&#20020;&#24202;&#35786;&#26029;&#33041;&#38663;&#33633;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#22899;&#24615;&#36816;&#21160;&#21592;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#32454;&#24494;&#21464;&#21270;&#12290;&#20808;&#36827;&#30340;&#31070;&#32463;&#20449;&#24687;&#23398;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#25104;&#20026;&#23453;&#36149;&#30340;&#36164;&#20135;&#12290;&#34429;&#28982;&#36825;&#20123;&#25216;&#26415;&#22312;&#29702;&#35299;&#30007;&#24615;&#36816;&#21160;&#21592;&#30340;&#33041;&#38663;&#33633;&#26041;&#38754;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;&#25105;&#20204;&#23545;&#20110;&#23427;&#20204;&#23545;&#22899;&#24615;&#36816;&#21160;&#21592;&#30340;&#26377;&#25928;&#24615;&#30340;&#29702;&#35299;&#19978;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#24378;&#22823;&#25968;&#25454;&#20998;&#26512;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#23558;&#35266;&#23519;&#21040;&#30340;&#34920;&#22411;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#32852;&#31995;&#21040;&#29305;&#23450;&#20110;&#24615;&#21035;&#30340;&#29983;&#29289;&#26426;&#21046;&#65292;&#25581;&#31034;&#22899;&#24615;&#36816;&#21160;&#21592;&#33041;&#38663;&#33633;&#30340;&#22885;&#31192;&#12290;&#27492;&#22806;&#65292;&#23884;&#20837;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#22312;&#30740;&#31350;&#20013;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#65292;&#36827;&#19968;&#27493;&#26816;&#39564;&#24615;&#21035;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, the intricacies of sports-related concussions among female athletes have become readily apparent. Traditional clinical methods for diagnosing concussions suffer limitations when applied to female athletes, often failing to capture subtle changes in brain structure and function. Advanced neuroinformatics techniques and machine learning models have become invaluable assets in this endeavor. While these technologies have been extensively employed in understanding concussion in male athletes, there remains a significant gap in our comprehension of their effectiveness for female athletes. With its remarkable data analysis capacity, machine learning offers a promising avenue to bridge this deficit. By harnessing the power of machine learning, researchers can link observed phenotypic neuroimaging data to sex-specific biological mechanisms, unraveling the mysteries of concussions in female athletes. Furthermore, embedding methods within machine learning enable examining b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#33719;&#21462;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#26356;&#25913;&#27169;&#22411;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#25915;&#20987;&#23646;&#24615;&#30340;&#22810;&#20010;&#27169;&#22411;&#29256;&#26412;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#32773;&#20813;&#21463;&#24694;&#24847;&#20837;&#20405;&#24102;&#26469;&#30340;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2401.09574</link><description>&lt;p&gt;
&#23454;&#29616;&#21487;&#25193;&#23637;&#21644;&#31283;&#20581;&#30340;&#27169;&#22411;&#29256;&#26412;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Scalable and Robust Model Versioning. (arXiv:2401.09574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#33719;&#21462;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#26356;&#25913;&#27169;&#22411;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#25915;&#20987;&#23646;&#24615;&#30340;&#22810;&#20010;&#27169;&#22411;&#29256;&#26412;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#32773;&#20813;&#21463;&#24694;&#24847;&#20837;&#20405;&#24102;&#26469;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#34892;&#21508;&#19994;&#30340;&#19981;&#26029;&#37096;&#32626;&#65292;&#38024;&#23545;&#36825;&#20123;&#37096;&#32626;&#27169;&#22411;&#36827;&#34892;&#24694;&#24847;&#20837;&#20405;&#30340;&#23041;&#32961;&#20063;&#22312;&#22686;&#21152;&#12290;&#22914;&#26524;&#25915;&#20987;&#32773;&#33021;&#22815;&#36890;&#36807;&#26381;&#21153;&#22120;&#20837;&#20405;&#12289;&#20869;&#37096;&#25915;&#20987;&#25110;&#27169;&#22411;&#21453;&#36716;&#25216;&#26415;&#33719;&#21462;&#37096;&#32626;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#20182;&#20204;&#21487;&#20197;&#26500;&#36896;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#26469;&#25805;&#32437;&#27169;&#22411;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#20174;&#32780;&#32473;&#20381;&#36182;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20851;&#38190;&#20219;&#21153;&#30340;&#32452;&#32455;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#12290;&#27169;&#22411;&#25152;&#26377;&#32773;&#38656;&#35201;&#19968;&#31181;&#26426;&#21046;&#26469;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#36825;&#31181;&#25439;&#22833;&#65292;&#32780;&#19981;&#38656;&#35201;&#33719;&#21462;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#26412;&#25237;&#20837;&#12290;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19981;&#33719;&#21462;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#26356;&#25913;&#27169;&#22411;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#25915;&#20987;&#23646;&#24615;&#30340;&#22810;&#20010;&#27169;&#22411;&#29256;&#26412;&#30340;&#21487;&#34892;&#24615;&#12290;&#27169;&#22411;&#25152;&#26377;&#32773;&#21487;&#20197;&#36880;&#20010;&#37096;&#32626;&#29256;&#26412;&#24182;&#31435;&#21363;&#26367;&#25442;&#27844;&#38706;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the deployment of deep learning models continues to expand across industries, the threat of malicious incursions aimed at gaining access to these deployed models is on the rise. Should an attacker gain access to a deployed model, whether through server breaches, insider attacks, or model inversion techniques, they can then construct white-box adversarial attacks to manipulate the model's classification outcomes, thereby posing significant risks to organizations that rely on these models for critical tasks. Model owners need mechanisms to protect themselves against such losses without the necessity of acquiring fresh training data - a process that typically demands substantial investments in time and capital.  In this paper, we explore the feasibility of generating multiple versions of a model that possess different attack properties, without acquiring new training data or changing model architecture. The model owner can deploy one version at a time and replace a leaked version immed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#65292;CDE&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08819</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#20174;&#31232;&#30095;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Sparse Offline Datasets via Conservative Density Estimation. (arXiv:2401.08819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#65292;CDE&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20026;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#19968;&#27493;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#22806;&#25512;&#38169;&#35823;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;CDE&#36890;&#36807;&#35299;&#20915;&#36793;&#38469;&#37325;&#35201;&#24615;&#25277;&#26679;&#20013;&#30340;&#25903;&#25345;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#31283;&#24577;&#20998;&#24067;&#26657;&#27491;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CDE&#22312;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#25345;&#32493;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution (OOD) extrapolation errors, especially in sparse reward or scarce data settings. In this paper, we propose a novel training algorithm called Conservative Density Estimation (CDE), which addresses this challenge by explicitly imposing constraints on the state-action occupancy stationary distribution. CDE overcomes the limitations of existing approaches, such as the stationary distribution correction method, by addressing the support mismatch issue in marginal importance sampling. Our method achieves state-of-the-art performance on the D4RL benchmark. Notably, CDE consistently outperforms baselines in challenging tasks with sparse rewards or insufficient data, demonstrating the advantages of our approach in addressing the extrapolation error problem i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Shapley&#20540;&#22238;&#24402;&#23545;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#30340;&#33829;&#38144;&#32489;&#25928;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#19982;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;Shapley&#20540;&#22238;&#24402;&#30340;&#23454;&#29992;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#35843;&#25972;&#31995;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.05653</link><description>&lt;p&gt;
&#20351;&#29992;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#65288;MMM&#65289;&#21644;Shapley&#20540;&#22238;&#24402;&#37327;&#21270;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#30340;&#33829;&#38144;&#32489;&#25928;
&lt;/p&gt;
&lt;p&gt;
Quantifying Marketing Performance at Channel-Partner Level by Using Marketing Mix Modeling (MMM) and Shapley Value Regression. (arXiv:2401.05653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Shapley&#20540;&#22238;&#24402;&#23545;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#30340;&#33829;&#38144;&#32489;&#25928;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#19982;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;Shapley&#20540;&#22238;&#24402;&#30340;&#23454;&#29992;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#35843;&#25972;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#21033;&#29992;Shapley&#20540;&#22238;&#24402;&#26469;&#35299;&#26512;&#33829;&#38144;&#32489;&#25928;&#30340;&#24212;&#29992;&#65292;&#34917;&#20805;&#20102;&#28192;&#36947;&#23618;&#38754;&#30340;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#65288;MMM&#65289;&#12290;&#21033;&#29992;&#26469;&#33258;&#37329;&#34701;&#26381;&#21153;&#34892;&#19994;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Shapley&#20540;&#22238;&#24402;&#22312;&#35780;&#20272;&#20010;&#21035;&#21512;&#20316;&#20249;&#20276;&#36129;&#29486;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#32467;&#26500;&#21270;&#30340;&#29616;&#22330;&#27979;&#35797;&#20197;&#21450;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#26368;&#20026;&#20934;&#30830;&#65292;&#20294;&#32463;&#24120;&#20250;&#38750;&#24120;&#22797;&#26434;&#21644;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;Shapley&#20540;&#22238;&#24402;&#26159;&#19968;&#31181;&#26356;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#31163;&#33829;&#38144;&#28192;&#36947;&#20013;&#27599;&#20010;&#33829;&#38144;&#21512;&#20316;&#20249;&#20276;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25512;&#23548;&#35843;&#25972;&#31995;&#25968;&#65292;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the application of Shapley Value Regression in dissecting marketing performance at channel-partner level, complementing channel-level Marketing Mix Modeling (MMM). Utilizing real-world data from the financial services industry, we demonstrate the practicality of Shapley Value Regression in evaluating individual partner contributions. Although structured in-field testing along with cooperative game theory is most accurate, it can often be highly complex and expensive to conduct. Shapley Value Regression is thus a more feasible approach to disentangle the influence of each marketing partner within a marketing channel. We also propose a simple method to derive adjusted coefficients of Shapley Value Regression and compares it with alternative approaches.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35774;&#35745;&#26550;&#26500;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21508;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20027;&#35201;&#26041;&#27861;&#21253;&#25324;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#21644;&#20854;&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.01626</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of Graph Neural Networks. (arXiv:2401.01626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01626
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35774;&#35745;&#26550;&#26500;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#21508;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20027;&#35201;&#26041;&#27861;&#21253;&#25324;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#21644;&#20854;&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#21040;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;GNN&#21487;&#20197;&#35299;&#20915;&#31038;&#20250;&#31185;&#23398;&#12289;&#21270;&#23398;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;GNN&#26550;&#26500;&#30340;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#33410;&#28857;&#25110;&#22270;&#20998;&#31867;&#31561;&#20219;&#21153;&#30340;&#23454;&#35777;&#24615;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#21017;&#23547;&#27714;&#25214;&#21040;&#20855;&#26377;&#29702;&#35770;&#29305;&#24615;&#30340;GNN&#26550;&#26500;&#65292;&#36890;&#36807;&#30740;&#31350;&#20854;&#34920;&#36798;&#33021;&#21147;&#24182;&#35774;&#35745;&#26368;&#22823;&#21270;&#36825;&#31181;&#34920;&#36798;&#33021;&#21147;&#30340;&#26550;&#26500;&#12290;&#34429;&#28982;&#20851;&#20110;&#22914;&#20309;&#23450;&#20041;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#36824;&#27809;&#26377;&#20849;&#35782;&#65292;&#20294;&#21487;&#20197;&#20174;&#20960;&#20010;&#26377;&#24456;&#22909;&#21160;&#26426;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#12290;&#20063;&#35768;&#26368;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#30740;&#31350;GNN&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#65292;&#23601;&#20687;MLP&#30340;&#36825;&#31181;&#24615;&#36136;&#19968;&#26679;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#21478;&#19968;&#20010;&#26041;&#21521;&#20851;&#27880;&#30340;&#26159;GNN&#22312;&#21306;&#20998;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33021;&#21147;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of Graph Neural Networks has received considerable interest in the past few years. By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine. The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification. However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.  While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives. Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs. Another direction focuses on the extent to which GNNs can distinguish between different graph
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#35782;&#21035;&#20855;&#26377;&#26368;&#39640;&#39044;&#26399;&#25928;&#26524;&#30340;&#27835;&#30103;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#22266;&#23450;&#39044;&#31639;&#30340;&#23616;&#37096;&#26368;&#20248;&#31639;&#27861;&#26469;&#38477;&#20302;&#38169;&#35823;&#35782;&#21035;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.19788</link><description>&lt;p&gt;
&#20855;&#26377;&#22266;&#23450;&#39044;&#31639;&#30340;&#23616;&#37096;&#26368;&#20248;&#26368;&#20339;&#33218;&#35782;&#21035;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Locally Optimal Best Arm Identification with a Fixed Budget. (arXiv:2310.19788v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#35782;&#21035;&#20855;&#26377;&#26368;&#39640;&#39044;&#26399;&#25928;&#26524;&#30340;&#27835;&#30103;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#22266;&#23450;&#39044;&#31639;&#30340;&#23616;&#37096;&#26368;&#20248;&#31639;&#27861;&#26469;&#38477;&#20302;&#38169;&#35823;&#35782;&#21035;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#21363;&#20855;&#26377;&#26368;&#39640;&#39044;&#26399;&#25928;&#26524;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#38477;&#20302;&#38169;&#35823;&#35782;&#21035;&#30340;&#27010;&#29575;&#26469;&#30830;&#23450;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#65292;&#36825;&#19968;&#38382;&#39064;&#22312;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#24050;&#34987;&#25506;&#32034;&#65292;&#21253;&#25324;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;Best Arm Identification&#65292;BAI&#65289;&#21644;&#24207;&#21015;&#20248;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#27835;&#30103;&#20998;&#37197;&#30340;&#36718;&#25968;&#26159;&#22266;&#23450;&#30340;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#20915;&#31574;&#32773;&#23558;&#19968;&#31181;&#27835;&#30103;&#26041;&#26696;&#20998;&#37197;&#32473;&#19968;&#20010;&#23454;&#39564;&#21333;&#20803;&#65292;&#24182;&#35266;&#23519;&#30456;&#24212;&#30340;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#36981;&#24490;&#19981;&#21516;&#27835;&#30103;&#26041;&#26696;&#20043;&#38388;&#26041;&#24046;&#19981;&#21516;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#23454;&#39564;&#32467;&#26463;&#26102;&#65292;&#25105;&#20204;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#25512;&#33616;&#19968;&#31181;&#27835;&#30103;&#26041;&#26696;&#20316;&#20026;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#30340;&#20272;&#35745;&#20540;&#12290;&#20915;&#31574;&#32773;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#23454;&#39564;&#65292;&#20351;&#38169;&#35823;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#30340;&#27010;&#29575;&#26368;&#23567;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35823;&#35782;&#21035;&#27010;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the problem of identifying the best treatment arm, a treatment arm with the highest expected outcome. We aim to identify the best treatment arm with a lower probability of misidentification, which has been explored under various names across numerous research fields, including \emph{best arm identification} (BAI) and ordinal optimization. In our experiments, the number of treatment-allocation rounds is fixed. In each round, a decision-maker allocates a treatment arm to an experimental unit and observes a corresponding outcome, which follows a Gaussian distribution with a variance different among treatment arms. At the end of the experiment, we recommend one of the treatment arms as an estimate of the best treatment arm based on the observations. The objective of the decision-maker is to design an experiment that minimizes the probability of misidentifying the best treatment arm. With this objective in mind, we develop lower bounds for the probability of misident
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#26041;&#27861;&#25913;&#36827;&#20102;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#20984;&#32622;&#20449;&#38598;&#65292;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#26032;&#30340;&#38789;&#38598;&#20013;&#27493;&#39588;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.18554</link><description>&lt;p&gt;
&#36890;&#36807;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#25913;&#36827;&#65288;&#22810;&#39033;&#24335;&#65289;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Improved Regret Bounds of (Multinomial) Logistic Bandits via Regret-to-Confidence-Set Conversion. (arXiv:2310.18554v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#26041;&#27861;&#25913;&#36827;&#20102;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#20984;&#32622;&#20449;&#38598;&#65292;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#26032;&#30340;&#38789;&#38598;&#20013;&#27493;&#39588;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#26159;&#24314;&#27169;&#29992;&#25143;&#36873;&#25321;&#30340;&#26222;&#36941;&#26694;&#26550;&#65292;&#20363;&#22914;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28857;&#20987;&#19982;&#21542;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#30340;&#24037;&#20316;&#24573;&#35270;&#25110;&#24573;&#30053;&#20102;$S \geq \lVert \theta_\star \rVert_2$&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20854;&#20013;$\theta_\star \in \mathbb{R}^d$&#26159;&#26410;&#30693;&#30340;&#21442;&#25968;&#21521;&#37327;&#65292;&#24403;$S$&#36739;&#22823;&#26102;&#65292;&#20363;&#22914;$S \geq d$&#65292;&#36825;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#8220;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#65288;R2CS&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;$S$&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#24615;&#30340;&#20984;&#32622;&#20449;&#38598;&#12290;&#20351;&#29992;R2CS&#65292;&#25105;&#20204;&#22312;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;&#26041;&#38754;&#33719;&#24471;&#20102;&#20005;&#26684;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#35745;&#31639;&#21487;&#34892;&#24615;&#21644;&#23545;&#20854;&#20182;&#22240;&#32032;&#65288;&#22914;$d$&#21644;$T$&#65289;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26032;&#32622;&#20449;&#38598;&#24212;&#29992;&#20110;&#20855;&#26377;&#26032;&#30340;&#38789;&#38598;&#20013;&#27493;&#39588;&#30340;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logistic bandit is a ubiquitous framework of modeling users' choices, e.g., click vs. no click for advertisement recommender system. We observe that the prior works overlook or neglect dependencies in $S \geq \lVert \theta_\star \rVert_2$, where $\theta_\star \in \mathbb{R}^d$ is the unknown parameter vector, which is particularly problematic when $S$ is large, e.g., $S \geq d$. In this work, we improve the dependency on $S$ via a novel approach called {\it regret-to-confidence set conversion (R2CS)}, which allows us to construct a convex confidence set based on only the \textit{existence} of an online learning algorithm with a regret guarantee. Using R2CS, we obtain a strict improvement in the regret bound w.r.t. $S$ in logistic bandits while retaining computational feasibility and the dependence on other factors such as $d$ and $T$. We apply our new confidence set to the regret analyses of logistic bandits with a new martingale concentration step that circumvents an additional factor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.18152</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#23646;&#24615;&#22270;&#30340;&#35299;&#32544;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#22312;&#32593;&#32476;&#19978;&#38750;&#24120;&#24120;&#35265;&#65292;&#23545;&#20110;&#35813;&#31867;&#22270;&#65292;&#22914;&#24341;&#29992;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#31038;&#20132;&#32593;&#32476;&#30340;&#30740;&#31350;&#22312;&#32593;&#32476;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20165;&#20381;&#38752;&#25552;&#31034;&#20449;&#24687;&#26469;&#20256;&#36798;&#22270;&#32467;&#26500;&#20449;&#24687;&#32473;LLMs&#65292;&#22240;&#27492;&#23545;&#20110;TAGs&#20013;&#22797;&#26434;&#30340;&#32467;&#26500;&#20851;&#31995;&#20102;&#35299;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#32544;&#22270;&#25991;&#23398;&#20064;&#22120;&#65288;DGTL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#22686;&#24378;LLMs&#23545;TAGs&#30340;&#25512;&#29702;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;DGTL&#27169;&#22411;&#36890;&#36807;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#22240;&#32032;&#20013;&#38544;&#34255;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#25552;&#21319;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#20808;&#36827;&#27169;&#22411;&#29983;&#25104;&#39640;&#36924;&#30495;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#32479;&#35745;&#26041;&#27861;&#38169;&#35823;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#20294;&#26368;&#32456;&#21487;&#33021;&#20250;&#22686;&#21152;&#25110;&#20572;&#28382;&#12290;</title><link>http://arxiv.org/abs/2310.17848</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#25193;&#23637;&#25552;&#21319;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Boosting Data Analytics With Synthetic Volume Expansion. (arXiv:2310.17848v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#25552;&#21319;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#20808;&#36827;&#27169;&#22411;&#29983;&#25104;&#39640;&#36924;&#30495;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#32479;&#35745;&#26041;&#27861;&#38169;&#35823;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#20294;&#26368;&#32456;&#21487;&#33021;&#20250;&#22686;&#21152;&#25110;&#20572;&#28382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20316;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30707;&#65292;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#26085;&#30410;&#37325;&#35201;&#65292;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#32479;&#35745;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#19982;&#21407;&#22987;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#39640;&#36924;&#30495;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#36890;&#36807;&#20808;&#36827;&#27169;&#22411;&#22914;&#34920;&#26684;&#25193;&#25955;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#29983;&#25104;&#65292;&#24182;&#32467;&#21512;&#30456;&#20851;&#30740;&#31350;&#27934;&#23519;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#21457;&#29616;&#26159;&#29983;&#25104;&#25928;&#24212;&#65306;&#32479;&#35745;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#38169;&#35823;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#22686;&#21152;&#19968;&#24320;&#22987;&#20943;&#23569;&#65292;&#20294;&#26368;&#32456;&#21487;&#33021;&#20250;&#22686;&#21152;&#25110;&#20572;&#28382;&#12290;&#36825;&#20010;&#29616;&#35937;&#26681;&#28304;&#20110;&#22797;&#21046;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generation, a cornerstone of Generative Artificial Intelligence, signifies a paradigm shift in data science by addressing data scarcity and privacy while enabling unprecedented performance. As synthetic data gains prominence, questions arise concerning the accuracy of statistical methods when applied to synthetic data compared to raw data. In this article, we introduce the Synthetic Data Generation for Analytics framework. This framework employs statistical methods on high-fidelity synthetic data generated by advanced models such as tabular diffusion and Generative Pre-trained Transformer models. These models, trained on raw data, are further enhanced with insights from pertinent studies. A significant discovery within this framework is the generational effect: the error of a statistical method on synthetic data initially diminishes with added synthetic data but may eventually increase or plateau. This phenomenon, rooted in the complexities of replicating raw data distri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#36716;&#25442;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#32479;&#35745;&#32422;&#26463;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#22411;&#26657;&#20934;&#25928;&#26524;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17159</link><description>&lt;p&gt;
&#26368;&#22823;&#29109;&#25439;&#22833;&#65306;&#38024;&#23545;&#36229;&#20986;&#20998;&#24067;&#36716;&#25442;&#30340;&#26657;&#20934;&#30340;&#32422;&#26463;&#26368;&#22823;&#29109;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MaxEnt Loss: Constrained Maximum Entropy for Calibration under Out-of-Distribution Shift. (arXiv:2310.17159v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#36716;&#25442;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#32479;&#35745;&#32422;&#26463;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#22411;&#26657;&#20934;&#25928;&#26524;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#36716;&#25442;&#26657;&#20934;&#38382;&#39064;&#30340;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#30446;&#26631;&#20989;&#25968;&#34987;&#25552;&#20986;&#26469;&#26377;&#25928;&#22320;&#22312;&#20998;&#24067;&#20869;&#26657;&#20934;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#23427;&#20204;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#24182;&#19981;&#22909;&#12290;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#26377;&#29992;&#30340;&#32479;&#35745;&#32422;&#26463;&#65292;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#22411;&#26657;&#20934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new loss function that addresses the out-of-distribution (OOD) calibration problem. While many objective functions have been proposed to effectively calibrate models in-distribution, our findings show that they do not always fare well OOD. Based on the Principle of Maximum Entropy, we incorporate helpful statistical constraints observed during training, delivering better model calibration without sacrificing accuracy. We provide theoretical analysis and show empirically that our method works well in practice, achieving state-of-the-art calibration on both synthetic and real-world benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20132;&#26367;&#25237;&#24433;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#26031;&#36807;&#31243;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17137</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#39640;&#26031;&#36807;&#31243;&#36890;&#36807;&#20132;&#26367;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Gaussian Processes via Alternating Projection. (arXiv:2310.17137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20132;&#26367;&#25237;&#24433;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#26031;&#36807;&#31243;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#36229;&#21442;&#25968;&#20248;&#21270;&#38656;&#35201;&#21453;&#22797;&#27714;&#35299;&#20855;&#26377; nxn &#26680;&#30697;&#38453;&#30340;&#32447;&#24615;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915; O(n^3) &#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#24555;&#36895;&#36845;&#20195;&#25968;&#20540;&#26041;&#27861;&#65292;&#22914;&#20849;&#36717;&#26799;&#24230;&#65288;CG&#65289;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#30456;&#24212;&#30340;&#26680;&#30697;&#38453;&#21464;&#24471;&#36234;&#26469;&#36234;&#30149;&#24577;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#20998;&#21106;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#38656;&#35201; O(n^2) &#30340;&#31354;&#38388;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982; CG &#22686;&#21152;&#20102;&#21487;&#35757;&#32451; GP &#22522;&#20110;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#20294;&#29616;&#20195;&#25968;&#25454;&#38598;&#24050;&#32463;&#36798;&#21040;&#36229;&#20986;&#20854;&#36866;&#29992;&#33539;&#22260;&#30340;&#35268;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#35775;&#38382;&#26680;&#30697;&#38453;&#30340;&#23376;&#22359;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#23567;&#25209;&#37327;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#20132;&#26367;&#25237;&#24433;&#65292;&#27599;&#27425;&#36845;&#20195;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#20026; O(n)&#65292;&#35299;&#20915;&#20102;&#23558; GP &#25193;&#23637;&#21040;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#26102;&#30340;&#35768;&#22810;&#23454;&#38469;&#25361;&#25112;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#20174;&#23454;&#35777;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
Gaussian process (GP) hyperparameter optimization requires repeatedly solving linear systems with $n \times n$ kernel matrices. To address the prohibitive $\mathcal{O}(n^3)$ time complexity, recent work has employed fast iterative numerical methods, like conjugate gradients (CG). However, as datasets increase in magnitude, the corresponding kernel matrices become increasingly ill-conditioned and still require $\mathcal{O}(n^2)$ space without partitioning. Thus, while CG increases the size of datasets GPs can be trained on, modern datasets reach scales beyond its applicability. In this work, we propose an iterative method which only accesses subblocks of the kernel matrix, effectively enabling \emph{mini-batching}. Our algorithm, based on alternating projection, has $\mathcal{O}(n)$ per-iteration time and space complexity, solving many of the practical challenges of scaling GPs to very large datasets. Theoretically, we prove our method enjoys linear convergence and empirically we demons
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;&#21644;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#21644;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16789</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Detecting Pretraining Data from Large Language Models. (arXiv:2310.16789v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;&#21644;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#21644;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#29992;&#20110;&#35757;&#32451;&#23427;&#20204;&#30340;&#25968;&#25454;&#24456;&#23569;&#34987;&#20844;&#24320;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#25968;&#25454;&#30340;&#35268;&#27169;&#20043;&#22823;&#65292;&#21487;&#33021;&#21253;&#21547;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#12289;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#20197;&#21450;&#29992;&#20110;&#24191;&#27867;&#25253;&#36947;&#30340;&#21442;&#32771;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#25105;&#20204;&#20960;&#20046;&#21487;&#20197;&#32943;&#23450;&#23427;&#20204;&#21253;&#21547;&#20102;&#28508;&#22312;&#30340;&#38382;&#39064;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30446;&#21069;&#26080;&#27861;&#30693;&#36947;&#36825;&#20123;&#25991;&#26412;&#20013;&#21253;&#21547;&#20102;&#21738;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#20197;&#21450;&#27604;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#26816;&#27979;&#38382;&#39064;&#65306;&#22312;&#19981;&#30693;&#36947;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#27573;&#25991;&#26412;&#21644;&#23545;LLM&#30340;&#40657;&#30418;&#35775;&#38382;&#65292;&#25105;&#20204;&#33021;&#21542;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#26159;&#22312;&#25552;&#20379;&#30340;&#25991;&#26412;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65311;&#20026;&#20102;&#26041;&#20415;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;WIKIMIA&#65292;&#20351;&#29992;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#21644;&#20043;&#21518;&#21019;&#24314;&#30340;&#25968;&#25454;&#26469;&#25903;&#25345;&#37329;&#26631;&#20934;&#26816;&#27979;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;Min-K% Prob&#65292;&#22522;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20551;&#35774;&#65306;&#19968;&#20010;&#26410;&#35265;&#36807;&#30340;&#20363;&#23376;&#21487;&#33021;&#21253;&#21547;&#20960;&#20010;&#20855;&#26377;&#36739;&#20302;&#27010;&#29575;&#30340;&#31163;&#32676;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low pro
&lt;/p&gt;</description></item><item><title>DeepFDR&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#35299;&#20915;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13349</link><description>&lt;p&gt;
DeepFDR&#65306;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data. (arXiv:2310.13349v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13349
&lt;/p&gt;
&lt;p&gt;
DeepFDR&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#22270;&#20687;&#20998;&#21106;&#25216;&#26415;&#35299;&#20915;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20307;&#32032;&#30340;&#22810;&#37325;&#26816;&#39564;&#22312;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20998;&#26512;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#22522;&#20110;&#20307;&#32032;&#30340;&#26816;&#39564;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#27979;&#35797;&#33021;&#21147;&#30340;&#22823;&#24133;&#25439;&#22833;&#12290;&#34429;&#28982;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#20123;&#31354;&#38388;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#20294;&#26159;&#24403;&#22788;&#29702;&#22797;&#26434;&#30340;&#33041;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#26102;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#26368;&#20248;&#24615;&#20173;&#23384;&#22312;&#30097;&#38382;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#22312;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#32780;&#22270;&#20687;&#20998;&#21106;&#19982;&#22522;&#20110;&#20307;&#32032;&#30340;&#22810;&#37325;&#26816;&#39564;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepFDR&#30340;&#26032;&#22411;&#31354;&#38388;&#34394;&#35686;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#26469;&#35299;&#20915;&#22522;&#20110;&#20307;&#32032;&#30340;&#22810;&#37325;&#26816;&#39564;&#38382;&#39064;&#12290;&#21253;&#25324;&#20840;&#38754;&#30340;&#27169;&#25311;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;FDG-PET&#24433;&#20687;&#20998;&#26512;&#22312;&#20869;&#30340;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;DeepFDR&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;DeepFDR&#19981;&#20165;&#22312;&#34394;&#35686;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36824;&#26377;&#25928;&#38477;&#20302;&#20102;&#34394;&#20551;&#30340;&#38750;&#21457;&#29616;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voxel-based multiple testing is widely used in neuroimaging data analysis. Traditional false discovery rate (FDR) control methods often ignore the spatial dependence among the voxel-based tests and thus suffer from substantial loss of testing power. While recent spatial FDR control methods have emerged, their validity and optimality remain questionable when handling the complex spatial dependencies of the brain. Concurrently, deep learning methods have revolutionized image segmentation, a task closely related to voxel-based multiple testing. In this paper, we propose DeepFDR, a novel spatial FDR control method that leverages unsupervised deep learning-based image segmentation to address the voxel-based multiple testing problem. Numerical studies, including comprehensive simulations and Alzheimer's disease FDG-PET image analysis, demonstrate DeepFDR's superiority over existing methods. DeepFDR not only excels in FDR control and effectively diminishes the false nondiscovery rate, but als
&lt;/p&gt;</description></item><item><title>CycleNet&#26159;&#19968;&#31181;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#32763;&#35793;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.13165</link><description>&lt;p&gt;
CycleNet&#65306;&#37325;&#26032;&#24605;&#32771;&#25991;&#26412;&#24341;&#23548;&#25193;&#25955;&#20013;&#30340;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#20197;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation. (arXiv:2310.13165v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13165
&lt;/p&gt;
&lt;p&gt;
CycleNet&#26159;&#19968;&#31181;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#32763;&#35793;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#22312;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#20294;&#32570;&#20047;&#19968;&#31181;&#30452;&#35266;&#30340;&#19968;&#33268;&#22270;&#20687;&#21040;&#22270;&#20687;&#65288;I2I&#65289;&#32763;&#35793;&#25509;&#21475;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;DMs&#36827;&#34892;&#26080;&#37197;&#23545;&#30340;I2I&#32763;&#35793;&#24182;&#20445;&#25345;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Cyclenet&#65292;&#19968;&#31181;&#26032;&#39062;&#20294;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#32435;&#20837;DMs&#20013;&#65292;&#20197;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;Cyclenet&#22312;&#19981;&#21516;&#31890;&#24230;&#30340;&#26080;&#37197;&#23545;I2I&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#12290;&#38500;&#20102;&#22330;&#26223;&#21644;&#23545;&#35937;&#32423;&#21035;&#30340;&#32763;&#35793;&#65292;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;I2I&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#29289;&#20307;&#30340;&#29289;&#29702;&#29366;&#24577;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Cyclenet&#22312;&#32763;&#35793;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#25913;&#21464;&#25991;&#26412;&#25551;&#36848;&#26102;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23637;&#29616;&#20986;&#20102;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#20854;&#37319;&#29992;&#30340;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#20026;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#65292;IQL&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12955</link><description>&lt;p&gt;
&#26500;&#24314;&#20855;&#26377;&#22810;&#26679;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Offline Reinforcement Learning under Diverse Data Corruption. (arXiv:2310.12955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23637;&#29616;&#20986;&#20102;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#20854;&#37319;&#29992;&#30340;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#20026;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#65292;IQL&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24378;&#21270;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#26114;&#36149;&#25110;&#19981;&#23433;&#20840;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#23384;&#22312;&#22122;&#22768;&#65292;&#29978;&#33267;&#21487;&#33021;&#34987;&#24694;&#24847;&#25439;&#22351;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#23545;&#24403;&#21069;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21253;&#25324;&#29366;&#24577;&#12289;&#21160;&#20316;&#12289;&#22870;&#21169;&#21644;&#21160;&#21147;&#23398;&#22312;&#20869;&#30340;&#20840;&#38754;&#25968;&#25454;&#25439;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#26174;&#31034;&#65292;&#38544;&#24335;Q-learning&#65288;IQL&#65289;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#34920;&#29616;&#20986;&#20102;&#21487;&#38752;&#30340;&#25239;&#25968;&#25454;&#25439;&#22351;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#32463;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;IQL&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#30417;&#30563;&#31574;&#30053;&#23398;&#20064;&#26041;&#26696;&#30830;&#23450;&#20026;&#20851;&#38190;&#22240;&#32032;&#12290;&#23613;&#31649;&#30456;&#23545;&#40065;&#26834;&#65292;&#20294;IQL&#22312;&#21160;&#21147;&#23398;&#25439;&#22351;&#19979;&#20173;&#28982;&#23384;&#22312;Q&#20989;&#25968;&#30340;&#37325;&#23614;&#30446;&#26631;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#27169;&#22411;&#30340;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#20195;&#29702;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12713</link><description>&lt;p&gt;
&#20174;&#36807;&#21435;&#23398;&#20064;&#65306;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26694;&#26550;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness. (arXiv:2310.12713v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#27169;&#22411;&#30340;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#20195;&#29702;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#23545;&#25239;&#26679;&#26412;&#30340;&#33030;&#24369;&#24615;&#21450;&#20854;&#24102;&#26469;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;&#31361;&#20986;&#20195;&#34920;&#24615;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#23545;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#26159;&#36890;&#36807;&#26126;&#30830;&#25110;&#38544;&#24615;&#30340;&#35745;&#31639;&#36127;&#25285;&#24110;&#21161;&#30446;&#26631;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#26469;&#38450;&#24481;&#38754;&#21521;&#21442;&#25968;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#21516;&#26102;&#30001;&#20110;&#20248;&#21270;&#36712;&#36857;&#19981;&#19968;&#33268;&#32780;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#30446;&#26631;&#27169;&#22411;&#30340;&#26356;&#26032;&#35268;&#21017;&#21450;&#20854;&#24403;&#21069;&#29366;&#24577;&#19979;&#30340;&#38450;&#24481;&#19981;&#36275;&#12290;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#27169;&#22411;&#30340;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#20195;&#29702;&#65292;&#24182;&#36171;&#20104;&#20854;&#29992;&#20110;&#38450;&#24481;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26694;&#26550;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;"LAST"&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of the vulnerability of deep learning models to adversarial samples and the ensuing security issues, a range of methods, including Adversarial Training (AT) as a prominent representative, aimed at enhancing model robustness against various adversarial attacks, have seen rapid development. However, existing methods essentially assist the current state of target model to defend against parameter-oriented adversarial attacks with explicit or implicit computation burdens, which also suffers from unstable convergence behavior due to inconsistency of optimization trajectories. Diverging from previous work, this paper reconsiders the update rule of target model and corresponding deficiency to defend based on its current state. By introducing the historical state of the target model as a proxy, which is endowed with much prior information for defense, we formulate a two-stage update rule, resulting in a general adversarial defense framework, which we refer to as `LAST' ({\bf L}earn fr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SiamAF&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#36890;&#36807;Siamese&#32593;&#32476;&#21644;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.09203</link><description>&lt;p&gt;
SiamAF: &#23398;&#20064;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#29992;&#20110;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SiamAF: Learning Shared Information from ECG and PPG Signals for Robust Atrial Fibrillation Detection. (arXiv:2310.09203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09203
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SiamAF&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#36890;&#36807;Siamese&#32593;&#32476;&#21644;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#33039;&#24515;&#24459;&#22833;&#24120;&#31867;&#22411;&#65292;&#19982;&#20013;&#39118;&#12289;&#24515;&#21147;&#34928;&#31469;&#21644;&#20854;&#20182;&#24515;&#34880;&#31649;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#22686;&#21152;&#26377;&#20851;&#65292;&#20294;&#21487;&#20197;&#20020;&#24202;&#19978;&#26080;&#22768;&#12290;&#20329;&#25140;&#24335;&#35774;&#22791;&#36827;&#34892;&#34987;&#21160;&#24615;&#30340;AF&#30417;&#27979;&#21487;&#33021;&#26377;&#21161;&#20110;&#20943;&#23569;&#19982;AF&#30456;&#20851;&#30340;&#19981;&#33391;&#20020;&#24202;&#32467;&#26524;&#12290;&#22312;&#22024;&#26434;&#30340;&#20329;&#25140;&#24335;&#25968;&#25454;&#20013;&#26816;&#27979;AF&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#24341;&#21457;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#20808;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21333;&#19968;&#24418;&#24577;&#23398;&#20064;&#65292;&#35201;&#20040;&#26159;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#65292;&#35201;&#20040;&#26159;&#20809;&#30005;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#29305;&#24449;&#65292;&#24182;&#20381;&#36182;&#20110;&#26356;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#25439;&#22351;&#30340;&#29305;&#24449;&#65292;&#22312;&#26576;&#20123;&#22330;&#26223;&#20013;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36136;&#37327;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#12290;&#37492;&#20110;&#20329;&#25140;&#24335;&#35774;&#22791;&#21644;&#24202;&#36793;&#30417;&#25252;&#20202;&#19978;ECG&#21644;PPG&#20449;&#21495;&#37197;&#23545;&#30340;&#26085;&#30410;&#20016;&#23500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;SiamAF&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;Siamese&#32593;&#32476;&#32467;&#26500;&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Atrial fibrillation (AF) is the most common type of cardiac arrhythmia. It is associated with an increased risk of stroke, heart failure, and other cardiovascular complications, but can be clinically silent. Passive AF monitoring with wearables may help reduce adverse clinical outcomes related to AF. Detecting AF in noisy wearable data poses a significant challenge, leading to the emergence of various deep learning techniques. Previous deep learning models learn from a single modality, either electrocardiogram (ECG) or photoplethysmography (PPG) signals. However, deep learning models often struggle to learn generalizable features and rely on features that are more susceptible to corruption from noise, leading to sub-optimal performances in certain scenarios, especially with low-quality signals. Given the increasing availability of ECG and PPG signal pairs from wearables and bedside monitors, we propose a new approach, SiamAF, leveraging a novel Siamese network architecture and joint le
&lt;/p&gt;</description></item><item><title>METRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#65292;&#26088;&#22312;&#20351;&#20854;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#12290;&#36825;&#20010;&#30446;&#26631;&#35299;&#20915;&#20102;&#32431;&#25506;&#32034;&#26041;&#27861;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20013;&#30340;&#22256;&#38590;&#20197;&#21450;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#20013;&#32570;&#20047;&#28608;&#21169;&#32780;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08887</link><description>&lt;p&gt;
METRA:&#20855;&#26377;&#24230;&#37327;&#24863;&#30693;&#25277;&#35937;&#30340;&#21487;&#25193;&#23637;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
METRA: Scalable Unsupervised RL with Metric-Aware Abstraction. (arXiv:2310.08887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08887
&lt;/p&gt;
&lt;p&gt;
METRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#65292;&#26088;&#22312;&#20351;&#20854;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#12290;&#36825;&#20010;&#30446;&#26631;&#35299;&#20915;&#20102;&#32431;&#25506;&#32034;&#26041;&#27861;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20013;&#30340;&#22256;&#38590;&#20197;&#21450;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#20013;&#32570;&#20047;&#28608;&#21169;&#32780;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#21516;&#26679;&#65292;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26377;&#26395;&#21457;&#29616;&#21508;&#31181;&#28508;&#22312;&#26377;&#29992;&#30340;&#34892;&#20026;&#65292;&#21487;&#20197;&#21152;&#36895;&#23398;&#20064;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20043;&#21069;&#30340;&#23581;&#35797;&#65292;&#20351;&#26080;&#30417;&#30563;RL&#30495;&#27491;&#21487;&#25193;&#23637;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25361;&#25112;&#65306;&#22312;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#32431;&#25506;&#32034;&#26041;&#27861;&#21487;&#33021;&#20250;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#35206;&#30422;&#27599;&#20010;&#21487;&#33021;&#30340;&#36716;&#25442;&#26159;&#19981;&#21487;&#34892;&#30340;&#65307;&#32780;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#28608;&#21169;&#32780;&#23436;&#20840;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#12290;&#20026;&#20102;&#20351;&#26080;&#30417;&#30563;RL&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;RL&#30446;&#26631;&#65292;&#31216;&#20026;&#24230;&#37327;&#24863;&#30693;&#25277;&#35937;&#65288;METRA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks. Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning. However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge: pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible, and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives. To make unsupervised RL scalable to complex, high-dimensional environments, we propose a novel unsupervised RL objective, which we call Metric-Aware Abstraction (METRA). Ou
&lt;/p&gt;</description></item><item><title>&#22312;&#22270;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#20013;&#65292;GraphControl&#36890;&#36807;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;&#23454;&#29616;&#20102;&#23545;&#36890;&#29992;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#36801;&#31227;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#22270;&#22495;&#38388;&#30340;&#23646;&#24615;&#35821;&#20041;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07365</link><description>&lt;p&gt;
GraphControl:&#20026;&#22270;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning. (arXiv:2310.07365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07365
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#20013;&#65292;GraphControl&#36890;&#36807;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;&#23454;&#29616;&#20102;&#23545;&#36890;&#29992;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#36801;&#31227;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#22270;&#22495;&#38388;&#30340;&#23646;&#24615;&#35821;&#20041;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#19990;&#30028;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#36825;&#31181;&#25968;&#25454;&#27169;&#22411;&#20102;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20026;&#21508;&#31181;Web&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;Web&#19978;&#27599;&#22825;&#28044;&#29616;&#30340;&#26080;&#26631;&#31614;&#22270;&#25968;&#25454;&#20026;&#36825;&#20123;&#24212;&#29992;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22270;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#20174;&#20016;&#23500;&#30340;&#26080;&#26631;&#31614;&#22270;&#25968;&#25454;&#20013;&#33719;&#24471;&#36890;&#29992;&#30693;&#35782;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;Web&#24212;&#29992;&#65292;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#65292;&#25552;&#39640;&#19979;&#28216;&#65288;&#30446;&#26631;&#65289;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#34920;&#38754;&#19978;&#30475;&#36215;&#26469;&#30456;&#20284;&#30340;&#39046;&#22495;&#20013;&#65292;&#19981;&#21516;&#30340;&#22270;&#22312;&#23646;&#24615;&#35821;&#20041;&#26041;&#38754;&#20063;&#21487;&#33021;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#36825;&#32473;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36801;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#22256;&#38590;&#65292;&#29978;&#33267;&#26159;&#19981;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20363;&#22914;&#65292;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#38468;&#21152;&#29305;&#23450;&#20219;&#21153;&#33410;&#28857;&#20449;&#24687;&#65288;&#29305;&#24322;&#24615;&#65289;&#36890;&#24120;&#20250;&#34987;&#26377;&#24847;&#30465;&#30053;&#65292;&#20197;&#20415;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#65288;&#21487;&#36801;&#31227;&#24615;&#65289;&#12290;&#36825;&#31181;&#26435;&#34913;&#34987;&#31216;&#20026;
&lt;/p&gt;
&lt;p&gt;
Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as 
&lt;/p&gt;</description></item><item><title>MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.06282</link><description>&lt;p&gt;
MuseChat:&#19968;&#31181;&#35270;&#39057;&#23545;&#35805;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06282
&lt;/p&gt;
&lt;p&gt;
MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MuseChat&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#20010;&#29420;&#29305;&#30340;&#24179;&#21488;&#19981;&#20165;&#25552;&#20379;&#20114;&#21160;&#29992;&#25143;&#21442;&#19982;&#65292;&#36824;&#20026;&#36755;&#20837;&#30340;&#35270;&#39057;&#25552;&#20379;&#20102;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25913;&#36827;&#21644;&#20010;&#24615;&#21270;&#20182;&#20204;&#30340;&#38899;&#20048;&#36873;&#25321;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#20197;&#21069;&#30340;&#31995;&#32479;&#20027;&#35201;&#24378;&#35843;&#20869;&#23481;&#30340;&#20860;&#23481;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#29992;&#25143;&#20010;&#20307;&#20559;&#22909;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#20363;&#22914;&#65292;&#25152;&#26377;&#30340;&#25968;&#25454;&#38598;&#37117;&#21482;&#25552;&#20379;&#22522;&#26412;&#30340;&#38899;&#20048;-&#35270;&#39057;&#37197;&#23545;&#65292;&#25110;&#32773;&#24102;&#26377;&#38899;&#20048;&#25551;&#36848;&#30340;&#37197;&#23545;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#35805;&#21512;&#25104;&#26041;&#27861;&#65292;&#27169;&#25311;&#20102;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#20004;&#36718;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#20132;&#20114;&#20013;&#65292;&#29992;&#25143;&#25552;&#20132;&#19968;&#20010;&#35270;&#39057;&#32473;&#31995;&#32479;&#65292;&#31995;&#32479;&#20250;&#25552;&#20379;&#19968;&#20010;&#21512;&#36866;&#30340;&#38899;&#20048;&#29255;&#27573;&#65292;&#24182;&#38468;&#24102;&#35299;&#37322;&#12290;&#20043;&#21518;&#65292;&#29992;&#25143;&#20250;&#34920;&#36798;&#20182;&#20204;&#23545;&#38899;&#20048;&#30340;&#20559;&#22909;&#65292;&#31995;&#32479;&#20250;&#21576;&#29616;&#19968;&#20010;&#25913;&#36827;&#21518;&#30340;&#38899;&#20048;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recomme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#32447;&#24615;&#21270;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#24182;&#24212;&#29992;&#20219;&#21153;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04742</link><description>&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#19982;&#37096;&#20998;&#32447;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Multi-task Model Fusion with Partial Linearization. (arXiv:2310.04742v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#32447;&#24615;&#21270;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#24182;&#24212;&#29992;&#20219;&#21153;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#24182;&#25104;&#20026;&#22522;&#30784;&#32452;&#20214;&#12290;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#22914;&#20219;&#21153;&#31639;&#27861;&#65292;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#30340;&#24494;&#35843;&#26435;&#37325;&#21512;&#24182;&#21040;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23548;&#33268;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22810;&#20219;&#21153;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20687;LoRA&#24494;&#35843;&#36825;&#26679;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#37096;&#20998;&#32447;&#24615;&#21270;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#24182;&#22312;&#32447;&#24615;&#21270;&#30340;&#36866;&#37197;&#22120;&#19978;&#24212;&#29992;&#20219;&#21153;&#31639;&#27861;&#12290;&#36825;&#26679;&#19968;&#26469;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#27169;&#22411;&#34701;&#21512;&#20248;&#21183;&#26469;&#36827;&#34892;&#32447;&#24615;&#21270;&#24494;&#35843;&#65292;&#21516;&#26102;&#20445;&#25345;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#37096;&#20998;&#32447;&#24615;&#21270;&#25216;&#26415;&#20351;&#22810;&#20010;&#20219;&#21153;&#26356;&#26377;&#25928;&#22320;&#34701;&#21512;&#21040;&#21333;&#20010;&#27169;&#22411;&#20013;&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models have enabled significant advances in machine learning and served as foundation components. Model fusion methods, such as task arithmetic, have been proven to be powerful and scalable to incorporate fine-tuned weights from different tasks into a multi-task model. However, efficiently fine-tuning large pre-trained models on multiple downstream tasks remains challenging, leading to inefficient multi-task model fusion. In this work, we propose a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically, our approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters. This allows us to leverage the the advantages of model fusion over linearized fine-tuning, while still performing fine-tuning and inference efficiently. We demonstrate that our partial linearization technique enables a more effective fusion of multiple tasks into a single model, outper
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#27969;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#25163;&#21160;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04432</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65306;&#36890;&#36807;&#27969;&#36827;&#34892;
&lt;/p&gt;
&lt;p&gt;
Training-free Linear Image Inversion via Flows. (arXiv:2310.04432v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04432
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#27969;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#25163;&#21160;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#32447;&#24615;&#21453;&#28436;&#26041;&#27861;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#36866;&#24403;&#20462;&#25913;&#26469;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35843;&#20248;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#20808;&#21069;&#26041;&#27861;&#24050;&#32463;&#25506;&#32034;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#20294;&#20173;&#38656;&#35201;&#25163;&#21160;&#35843;&#25972;&#35768;&#22810;&#36229;&#21442;&#25968;&#26469;&#24212;&#23545;&#19981;&#21516;&#30340;&#36870;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27969;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#21453;&#28436;&#30340;&#26080;&#38656;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#27969;&#21305;&#37197;&#27169;&#22411;&#30340;&#31616;&#27905;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#20351;&#29992;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#21152;&#26435;&#26041;&#26696;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#25163;&#21160;&#35843;&#25972;&#30340;&#24037;&#20316;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#20027;&#35201;&#28304;&#22836;&#27762;&#21462;&#28789;&#24863;&#65306;&#23558;&#20808;&#21069;&#30340;&#26799;&#24230;&#26657;&#27491;&#26041;&#27861;&#24212;&#29992;&#20110;&#27969;&#39046;&#22495;&#65292;&#20197;&#21450;&#22522;&#20110;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#36335;&#24452;&#30340;&#27714;&#35299;&#22120;&#26041;&#26696;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#24191;&#27867;&#21487;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25193;&#25955;&#27169;&#22411;&#23454;&#38469;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#36870;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training-free linear inversion involves the use of a pretrained generative model and -- through appropriate modifications to the generation process -solving inverse problems without any finetuning of the generative model. While recent prior methods have explored the use of diffusion models, they still require the manual tuning of many hyperparameters for different inverse problems. In this work, we propose a training-free method for image inversion using pretrained flow models, leveraging the simplicity and efficiency of Flow Matching models, using theoretically-justified weighting schemes and thereby significantly reducing the amount of manual tuning. In particular, we draw inspiration from two main sources: adopting prior gradient correction methods to the flow regime, and a solver scheme based on conditional Optimal Transport paths. As pretrained diffusion models are widely accessible, we also show how to practically adapt diffusion models for our method. Empirically, our approach
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26799;&#24230;&#27969;&#25277;&#26679;&#26041;&#27861;&#30340;&#30740;&#31350;&#26041;&#21521;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#27969;&#30340;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;Kullback-Leibler&#25955;&#24230;&#20316;&#20026;&#33021;&#37327;&#27867;&#20989;&#30340;&#29420;&#29305;&#23646;&#24615;&#12289;&#24230;&#37327;&#30340;&#36873;&#25321;&#19982;&#19981;&#21464;&#24615;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03597</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling via Gradient Flows in the Space of Probability Measures. (arXiv:2310.03597v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03597
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#27969;&#25277;&#26679;&#26041;&#27861;&#30340;&#30740;&#31350;&#26041;&#21521;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#27969;&#30340;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;Kullback-Leibler&#25955;&#24230;&#20316;&#20026;&#33021;&#37327;&#27867;&#20989;&#30340;&#29420;&#29305;&#23646;&#24615;&#12289;&#24230;&#37327;&#30340;&#36873;&#25321;&#19982;&#19981;&#21464;&#24615;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#20351;&#29992;&#26410;&#30693;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#25277;&#26679;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32771;&#34385;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#27969;&#27966;&#29983;&#30340;&#31639;&#27861;&#20026;&#31639;&#27861;&#24320;&#21457;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#36890;&#36807;&#23457;&#26597;&#36825;&#31181;&#26799;&#24230;&#27969;&#30340;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#36825;&#31181;&#25277;&#26679;&#26041;&#27861;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#25277;&#26679;&#30340;&#20219;&#20309;&#23454;&#20363;&#21270;&#37117;&#38656;&#35201;&#19968;&#20010;&#33021;&#37327;&#27867;&#20989;&#21644;&#19968;&#20010;&#24230;&#37327;&#26469;&#30830;&#23450;&#27969;&#21160;&#65292;&#20197;&#21450;&#27969;&#21160;&#30340;&#25968;&#20540;&#36817;&#20284;&#26469;&#25512;&#23548;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;Kullback-Leibler&#25955;&#24230;&#20316;&#20026;&#19968;&#20010;&#33021;&#37327;&#27867;&#20989;&#20855;&#26377;&#21807;&#19968;&#30340;&#29305;&#24449;&#65288;&#22312;&#25152;&#26377;f-&#25955;&#24230;&#20013;&#65289;&#65292;&#21363;&#30001;&#20854;&#24471;&#21040;&#30340;&#26799;&#24230;&#27969;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#20998;&#24067;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#20174;&#19981;&#21464;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#24230;&#37327;&#30340;&#36873;&#25321;&#12290;Fisher-Rao&#24230;&#37327;&#34987;&#31216;&#20026;t
&lt;/p&gt;
&lt;p&gt;
Sampling a target probability distribution with an unknown normalization constant is a fundamental challenge in computational science and engineering. Recent work shows that algorithms derived by considering gradient flows in the space of probability measures open up new avenues for algorithm development. This paper makes three contributions to this sampling approach by scrutinizing the design components of such gradient flows. Any instantiation of a gradient flow for sampling needs an energy functional and a metric to determine the flow, as well as numerical approximations of the flow to derive algorithms. Our first contribution is to show that the Kullback-Leibler divergence, as an energy functional, has the unique property (among all f-divergences) that gradient flows resulting from it do not depend on the normalization constant of the target distribution. Our second contribution is to study the choice of metric from the perspective of invariance. The Fisher-Rao metric is known as t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;G2MILP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;MILP&#23454;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;MILP&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.02807</link><description>&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#29992;&#20110;MILP&#27714;&#35299;&#22120;&#30340;&#28145;&#24230;&#23454;&#20363;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability. (arXiv:2310.02807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;G2MILP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;MILP&#23454;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;MILP&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;MILP&#65289;&#65292;&#20986;&#29616;&#20102;&#29190;&#28856;&#24335;&#22686;&#38271;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#26524;&#65292;&#20294;&#30495;&#23454;&#19990;&#30028;&#23454;&#20363;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#24448;&#24448;&#20250;&#23548;&#33268;&#27425;&#20248;&#20915;&#31574;&#21644;&#26377;&#20559;&#35265;&#30340;&#27714;&#35299;&#22120;&#35780;&#20272;&#65292;&#36825;&#23601;&#38656;&#35201;&#19968;&#31995;&#21015;&#21512;&#25104;MILP&#23454;&#20363;&#29983;&#25104;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#36807;&#20110;&#20381;&#36182;&#19987;&#23478;&#35774;&#35745;&#30340;&#34920;&#36798;&#24335;&#65292;&#35201;&#20040;&#38590;&#20197;&#25429;&#25417;&#30495;&#23454;&#19990;&#30028;&#23454;&#20363;&#30340;&#20016;&#23500;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;G2MILP&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;MILP&#23454;&#20363;&#30340;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;G2MILP&#23558;MILP&#23454;&#20363;&#34920;&#31034;&#20026;&#20108;&#20998;&#22270;&#65292;&#24182;&#24212;&#29992;&#36974;&#34109;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#36845;&#20195;&#22320;&#30772;&#22351;&#21644;&#26367;&#25442;&#21407;&#22987;&#22270;&#30340;&#37096;&#20998;&#20197;&#29983;&#25104;&#26032;&#30340;&#23454;&#20363;&#12290;G2MILP&#30340;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#26159;&#23427;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;MILP&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, there has been an explosive surge in the use of machine learning (ML) techniques to address combinatorial optimization (CO) problems, especially mixed-integer linear programs (MILPs). Despite the achievements, the limited availability of real-world instances often leads to sub-optimal decisions and biased solver assessments, which motivates a suite of synthetic MILP instance generation techniques. However, existing methods either rely heavily on expert-designed formulations or struggle to capture the rich features of real-world instances. To tackle this problem, we propose G2MILP, which to the best of our knowledge is the first deep generative framework for MILP instances. Specifically, G2MILP represents MILP instances as bipartite graphs, and applies a masked variational autoencoder to iteratively corrupt and replace parts of the original graphs to generate new ones. The appealing feature of G2MILP is that it can learn to generate novel and realistic MILP instan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#31350;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#21644;&#20445;&#23432;&#20248;&#21270;&#30446;&#26631;&#30340;&#25928;&#26524;&#65292;&#23545;&#20943;&#36731;&#22870;&#21169;&#27169;&#22411;&#36807;&#24230;&#20248;&#21270;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.02743</link><description>&lt;p&gt;
&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26377;&#21161;&#20110;&#20943;&#36731;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reward Model Ensembles Help Mitigate Overoptimization. (arXiv:2310.02743v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#31350;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#21644;&#20445;&#23432;&#20248;&#21270;&#30446;&#26631;&#30340;&#25928;&#26524;&#65292;&#23545;&#20943;&#36731;&#22870;&#21169;&#27169;&#22411;&#36807;&#24230;&#20248;&#21270;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20197;&#36981;&#24490;&#25351;&#20196;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#34987;&#29992;&#26469;&#36817;&#20284;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#8220;&#30495;&#23454;&#8221;&#22870;&#21169;&#30340;&#19981;&#23436;&#32654;&#34920;&#31034;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36807;&#24230;&#20248;&#21270;&#30340;&#24433;&#21709;&#12290;Gao&#31561;&#20154;&#22312;&#19968;&#20010;&#20154;&#24037;&#21453;&#39304;&#23454;&#39564;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#29616;&#35937;&#65292;&#20351;&#29992;&#19968;&#20010;&#36739;&#22823;&#30340;&#8220;&#37329;&#26631;&#20934;&#8221;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#30495;&#23454;&#22870;&#21169;&#65288;&#32780;&#19981;&#26159;&#20154;&#31867;&#65289;&#65292;&#24182;&#26174;&#31034;&#36807;&#24230;&#20248;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#26080;&#35770;&#20195;&#29702;&#22870;&#21169;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#22914;&#20309;&#12290;&#20351;&#29992;&#31867;&#20284;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#22312;&#20351;&#29992;&#20004;&#31181;&#20248;&#21270;&#26041;&#27861;&#26102;&#65292;&#20351;&#29992;&#22522;&#20110;&#38598;&#21512;&#30340;&#20445;&#23432;&#20248;&#21270;&#30446;&#26631;&#65288;&#26368;&#22351;&#24773;&#20917;&#20248;&#21270;&#21644;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#20248;&#21270;&#65289;&#26469;&#20943;&#36731;&#22870;&#21169;&#27169;&#22411;&#36807;&#24230;&#20248;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the "true" reward, these learned reward models are susceptible to \textit{overoptimization}. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger "gold" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#23454;&#29616;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#21644;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#26469;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;&#30340;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02679</link><description>&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65306;&#36890;&#36807;&#37096;&#20998;&#36712;&#36857;&#20248;&#21270;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization. (arXiv:2310.02679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02679
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#23454;&#29616;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#21644;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#26469;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;&#30340;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#30340;&#38543;&#26426;&#36807;&#31243;&#26469;&#27169;&#25311;&#36825;&#20123;&#30446;&#26631;&#23494;&#24230;&#30340;&#36817;&#20284;&#26679;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#35757;&#32451;&#30446;&#26631;&#38656;&#35201;&#35745;&#31639;&#23436;&#25972;&#30340;&#36712;&#36857;&#65292;&#23548;&#33268;&#30001;&#20110;&#20351;&#29992;&#23436;&#25972;&#36712;&#36857;&#21644;&#21482;&#22312;&#32456;&#31471;&#26102;&#38388;&#23384;&#22312;&#30340;&#23398;&#20064;&#20449;&#21495;&#30340;&#20351;&#29992;&#32780;&#20135;&#29983;&#32531;&#24930;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#23398;&#20064;&#36807;&#31243;&#21487;&#34892;&#22320;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#27969;&#20989;&#25968;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#29702;&#35770;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#24182;&#20174;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of sampling from intractable high-dimensional density functions, a fundamental task that often appears in machine learning and statistics. We extend recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target densities. The main drawback of these approaches is that the training objective requires full trajectories to compute, resulting in sluggish credit assignment issues due to use of entire trajectories and a learning signal present only at the terminal time. In this work, we present Diffusion Generative Flow Samplers (DGFS), a sampling-based framework where the learning process can be tractably broken down into short partial trajectory segments, via parameterizing an additional "flow function". Our method takes inspiration from the theory developed for generative flow networks (GFlowNets), allowing us to make use of intermediate learning signals and benefit from off-policy exploration capabilitie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#21033;&#29992;&#27531;&#24046;&#32593;&#32476;&#21644;&#27531;&#24046;MobileNet&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#26041;&#27861;&#36827;&#34892;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.02011</link><description>&lt;p&gt;
&#35299;&#30721;&#20154;&#31867;&#34892;&#20026;&#65306;&#20998;&#26512;&#21487;&#31359;&#25140;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#25968;&#25454;&#36827;&#34892;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition. (arXiv:2310.02011v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#21033;&#29992;&#27531;&#24046;&#32593;&#32476;&#21644;&#27531;&#24046;MobileNet&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#26041;&#27861;&#36827;&#34892;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20154;&#30340;&#36816;&#21160;&#25110;&#30456;&#23545;&#23450;&#20301;&#26377;&#25928;&#22320;&#20135;&#29983;&#20102;&#21487;&#20197;&#34987;&#35745;&#31639;&#26426;&#35835;&#21462;&#30340;&#21407;&#22987;&#30005;&#20449;&#21495;&#65292;&#36890;&#36807;&#24212;&#29992;&#21508;&#31181;&#25805;&#20316;&#25216;&#26415;&#26469;&#23545;&#19981;&#21516;&#30340;&#20154;&#31867;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#19982;&#27531;&#24046;MobileNet&#36827;&#34892;&#21512;&#22863;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#31216;&#20026;FusionActNet&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#27531;&#24046;&#22359;&#20998;&#21035;&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#26126;&#26174;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#32593;&#32476;&#29420;&#31435;&#35757;&#32451;&#65292;&#24471;&#21040;&#20004;&#20010;&#19987;&#19994;&#30340;&#39640;&#31934;&#24230;&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#26550;&#26500;&#35843;&#25972;&#30340;&#31639;&#27861;&#20248;&#21183;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#36229;&#31867;&#20013;&#20248;&#31168;&#22320;&#35782;&#21035;&#27963;&#21160;&#12290;&#28982;&#21518;&#65292;&#36825;&#20004;&#20010;&#27531;&#24046;&#32593;&#32476;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#30340;&#27531;&#24046;MobileNet&#36827;&#34892;&#20256;&#36882;&#12290;&#38543;&#21518;&#65292;&#36825;&#20010;&#21512;&#22863;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19968;&#20123;&#29305;&#23450;&#30340;&#23376;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
A person's movement or relative positioning effectively generates raw electrical signals that can be read by computing machines to apply various manipulative techniques for the classification of different human activities. In this paper, a stratified multi-structural approach based on a Residual network ensembled with Residual MobileNet is proposed, termed as FusionActNet. The proposed method involves using carefully designed Residual blocks for classifying the static and dynamic activities separately because they have clear and distinct characteristics that set them apart. These networks are trained independently, resulting in two specialized and highly accurate models. These models excel at recognizing activities within a specific superclass by taking advantage of the unique algorithmic benefits of architectural adjustments. Afterward, these two ResNets are passed through a weighted ensemble-based Residual MobileNet. Subsequently, this ensemble proficiently discriminates between a sp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;&#31867;&#27604;&#25552;&#31034;&#65292;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#23454;&#20363;&#25110;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01714</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31867;&#27604;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Analogical Reasoners. (arXiv:2310.01714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;&#31867;&#27604;&#25552;&#31034;&#65292;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#23454;&#20363;&#25110;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#26377;&#26631;&#35760;&#30340;&#25512;&#29702;&#36807;&#31243;&#31034;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;&#31867;&#27604;&#25552;&#31034;&#65292;&#26088;&#22312;&#33258;&#21160;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#21463;&#31867;&#27604;&#25512;&#29702;&#30340;&#21551;&#21457;&#65292;&#31867;&#27604;&#25512;&#29702;&#26159;&#19968;&#31181;&#35748;&#30693;&#36807;&#31243;&#65292;&#20154;&#31867;&#20174;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#20013;&#33719;&#21462;&#30693;&#35782;&#26469;&#35299;&#20915;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#20351;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19978;&#19979;&#25991;&#20013;&#30340;&#30456;&#20851;&#23454;&#20363;&#25110;&#30693;&#35782;&#65292;&#28982;&#21518;&#35299;&#20915;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#20197;&#19979;&#20960;&#20010;&#20248;&#28857;&#65306;&#23427;&#30465;&#21435;&#20102;&#26631;&#35760;&#25110;&#26816;&#32034;&#23454;&#20363;&#30340;&#38656;&#27714;&#65292;&#25552;&#20379;&#20102;&#26222;&#36866;&#24615;&#21644;&#20415;&#21033;&#24615;&#65307;&#23427;&#36824;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#38382;&#39064;&#23450;&#21046;&#29983;&#25104;&#30340;&#31034;&#20363;&#21644;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#20248;&#20110;0-shot CoT&#21644;&#25163;&#21160;few-shot CoT&#65292;&#21253;&#25324;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, Analogical Prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#30005;&#36335;&#22797;&#26434;&#24230;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#31034;&#22797;&#26434;&#24615;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26576;&#20123;MDP&#21487;&#20197;&#29992;&#24658;&#23450;&#28145;&#24230;&#30005;&#36335;&#34920;&#31034;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26368;&#20248;$Q$-&#20989;&#25968;&#30340;&#30005;&#36335;&#22797;&#26434;&#24230;&#25351;&#25968;&#32423;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#36890;&#24120;&#27604;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01706</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#31034;&#22797;&#26434;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Representation Complexity of Model-based and Model-free Reinforcement Learning. (arXiv:2310.01706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#30005;&#36335;&#22797;&#26434;&#24230;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#31034;&#22797;&#26434;&#24615;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26576;&#20123;MDP&#21487;&#20197;&#29992;&#24658;&#23450;&#28145;&#24230;&#30005;&#36335;&#34920;&#31034;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26368;&#20248;$Q$-&#20989;&#25968;&#30340;&#30005;&#36335;&#22797;&#26434;&#24230;&#25351;&#25968;&#32423;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#36890;&#24120;&#27604;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#30005;&#36335;&#22797;&#26434;&#24230;&#30340;&#32972;&#26223;&#19979;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#34920;&#31034;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#23384;&#22312;&#19968;&#31867;&#24191;&#27867;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#23427;&#20204;&#30340;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#29992;&#20855;&#26377;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#24658;&#23450;&#28145;&#24230;&#30005;&#36335;&#34920;&#31034;&#65292;&#32780;&#26368;&#20248;&#30340;$Q$-&#20989;&#25968;&#22312;&#24658;&#23450;&#28145;&#24230;&#30005;&#36335;&#20013;&#36973;&#21463;&#25351;&#25968;&#32423;&#30005;&#36335;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#20851;&#27880;&#36924;&#36817;&#35823;&#24046;&#24182;&#24314;&#31435;&#21040;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20174;&#26032;&#30340;&#34920;&#31034;&#22797;&#26434;&#24615;&#35282;&#24230;&#20026;&#20160;&#20040;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#36890;&#24120;&#27604;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35265;&#35299;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29615;&#22659;&#30340;&#30495;&#23454;&#35268;&#21017;&#65288;&#27169;&#22411;&#65289;&#26131;&#20110;&#34920;&#31034;&#65292;&#32780;&#20854;&#20182;&#25968;&#37327;&#65292;&#22914;$Q$-&#20989;&#25968;&#65292;&#20284;&#20046;&#24456;&#22797;&#26434;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#36716;&#31227;&#26680;&#20989;&#25968;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#26368;&#20248;$Q$-&#20989;&#25968;&#30340;&#36924;&#36817;&#35823;&#24046;&#26469;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the representation complexity of model-based and model-free reinforcement learning (RL) in the context of circuit complexity. We prove theoretically that there exists a broad class of MDPs such that their underlying transition and reward functions can be represented by constant depth circuits with polynomial size, while the optimal $Q$-function suffers an exponential circuit complexity in constant-depth circuits. By drawing attention to the approximation errors and building connections to complexity theory, our theory provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective: in some cases, the ground-truth rule (model) of the environment is simple to represent, while other quantities, such as $Q$-function, appear complex. We empirically corroborate our theory by comparing the approximation error of the transition kernel, reward function, and optimal $Q$-function
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.17002</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#20943;&#36731;&#39044;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks. (arXiv:2309.17002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#20808;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#30340;&#24615;&#36136;&#65292;&#24182;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#22122;&#22768;&#30340;ImageNet-1K&#21644;YFCC15M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20419;&#36827;&#39046;&#22495;&#20869;&#30340;&#36716;&#31227;&#24615;&#33021;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#30456;&#21516;&#30340;&#20998;&#24067;&#65307;&#28982;&#32780;&#65292;&#23427;&#24635;&#26159;&#20250;&#25439;&#23475;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#39044;&#35757;&#32451;&#20013;&#30340;&#22122;&#22768;&#20250;&#19981;&#21516;&#22320;&#22609;&#36896;&#29305;&#24449;&#31354;&#38388;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#40657;&#30418;&#35843;&#25972;&#26041;&#27861;&#65288;NMTune&#65289;&#26469;&#20351;&#29305;&#24449;&#31354;&#38388;&#36798;&#21040;&#26144;&#23556;&#24182;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a lightweight black-box tuning method (NMTune) to affine the feature space to mitigate the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Segment Anything Model (SAM)&#20316;&#20026;&#25945;&#24072;&#26469;&#25351;&#23548;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20687;&#32032;&#35821;&#20041;&#20851;&#31995;&#33976;&#39311;&#21644;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20004;&#31181;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#26356;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.16992</link><description>&lt;p&gt;
Segment Anything Model&#23545;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#20855;&#26377;&#33391;&#22909;&#30340;&#25945;&#23548;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model is a Good Teacher for Local Feature Learning. (arXiv:2309.16992v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Segment Anything Model (SAM)&#20316;&#20026;&#25945;&#24072;&#26469;&#25351;&#23548;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20687;&#32032;&#35821;&#20041;&#20851;&#31995;&#33976;&#39311;&#21644;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20004;&#31181;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#26356;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#22320;&#29305;&#24449;&#30340;&#26816;&#27979;&#21644;&#25551;&#36848;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#25551;&#36848;&#8220;&#20219;&#20309;&#22330;&#26223;&#8221;&#21644;&#8220;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#8221;&#30340;&#20851;&#38190;&#28857;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20110;&#20687;&#32032;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#33719;&#24471;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAMFeat&#26469;&#24341;&#20837;SAM&#65288;segment anything model&#65289;&#20316;&#20026;&#25945;&#24072;&#26469;&#25351;&#23548;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#19978;&#28608;&#21457;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20687;&#32032;&#35821;&#20041;&#20851;&#31995;&#33976;&#39311;&#65288;PSRD&#65289;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#23558;SAM&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#35821;&#20041;&#20449;&#24687;&#36890;&#36807;&#29305;&#24449;&#20851;&#31995;&#33976;&#39311;&#21040;&#26412;&#22320;&#29305;&#24449;&#23398;&#20064;&#32593;&#32476;&#20013;&#65292;&#20197;&#25552;&#39640;&#36890;&#36807;&#35821;&#20041;&#21306;&#20998;&#25913;&#21892;&#26412;&#22320;&#29305;&#24449;&#25551;&#36848;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Local feature detection and description play an important role in many computer vision tasks, which are designed to detect and describe keypoints in "any scene" and "any downstream task". Data-driven local feature learning methods need to rely on pixel-level correspondence for training, which is challenging to acquire at scale, thus hindering further improvements in performance. In this paper, we propose SAMFeat to introduce SAM (segment anything model), a fundamental model trained on 11 million images, as a teacher to guide local feature learning and thus inspire higher performance on limited datasets. To do so, first, we construct an auxiliary task of Pixel Semantic Relational Distillation (PSRD), which distillates feature relations with category-agnostic semantic information learned by the SAM encoder into a local feature learning network, to improve local feature description using semantic discrimination. Second, we develop a technique called Weakly Supervised Contrastive Learning 
&lt;/p&gt;</description></item><item><title>M-OFDFT&#26159;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#20998;&#23376;&#31995;&#32479;&#38382;&#39064;&#30340;OFDFT&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#23616;&#22495;&#24615;&#24314;&#31435;&#22312;&#27169;&#22411;&#20013;&#24182;&#20351;&#29992;&#32039;&#20945;&#30340;&#23494;&#24230;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#19982;Kohn-Sham DFT&#30456;&#36817;&#30340;&#31934;&#30830;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16578</link><description>&lt;p&gt;
M-OFDFT&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20811;&#26381;&#20998;&#23376;&#31995;&#32479;&#20013;&#30340;&#26080;&#36712;&#36947;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning. (arXiv:2309.16578v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16578
&lt;/p&gt;
&lt;p&gt;
M-OFDFT&#26159;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#20998;&#23376;&#31995;&#32479;&#38382;&#39064;&#30340;OFDFT&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#23616;&#22495;&#24615;&#24314;&#31435;&#22312;&#27169;&#22411;&#20013;&#24182;&#20351;&#29992;&#32039;&#20945;&#30340;&#23494;&#24230;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#19982;Kohn-Sham DFT&#30456;&#36817;&#30340;&#31934;&#30830;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#36712;&#36947;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;OFDFT&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#36739;&#20302;&#36816;&#31639;&#25104;&#26412;&#30340;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#26041;&#27861;&#65292;&#27604;&#36215;&#24120;&#29992;&#30340;Kohn-Sham&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#26356;&#21152;&#36866;&#29992;&#20110;&#24403;&#20195;&#20998;&#23376;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;OFDFT&#30340;&#31934;&#30830;&#24615;&#21463;&#21040;&#20102;&#21160;&#33021;&#23494;&#24230;&#27867;&#20989;&#30340;&#38480;&#21046;&#65292;&#23545;&#20110;&#38750;&#21608;&#26399;&#24615;&#20998;&#23376;&#31995;&#32479;&#30340;&#36817;&#20284;&#27714;&#35299;&#38750;&#24120;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;M-OFDFT&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20989;&#25968;&#27169;&#22411;&#35299;&#20915;&#20102;&#20998;&#23376;&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#24517;&#35201;&#30340;&#38750;&#23616;&#22495;&#24615;&#24314;&#31435;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#21407;&#23376;&#22522;&#19979;&#30340;&#23637;&#24320;&#31995;&#25968;&#20316;&#20026;&#32039;&#20945;&#30340;&#23494;&#24230;&#34920;&#31034;&#26469;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#35299;&#20915;&#20854;&#20013;&#30340;&#38750;&#20256;&#32479;&#23398;&#20064;&#25361;&#25112;&#30340;&#25216;&#26415;&#65292;M-OFDFT&#22312;&#19968;&#31995;&#21015;OFDFT&#26080;&#27861;&#35302;&#21450;&#30340;&#20998;&#23376;&#19978;&#23454;&#29616;&#20102;&#19982;Kohn-Sham DFT&#30456;&#24403;&#30340;&#31934;&#30830;&#24230;&#12290;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#26159;&#65292;M-OFDFT&#22312;&#35757;&#32451;&#26102;&#23646;&#20110;&#26356;&#22823;&#30340;&#20998;&#23376;&#20013;&#26377;&#30528;&#33391;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#22823;&#20998;&#23376;&#25552;&#20379;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#35268;&#27169;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orbital-free density functional theory (OFDFT) is a quantum chemistry formulation that has a lower cost scaling than the prevailing Kohn-Sham DFT, which is increasingly desired for contemporary molecular research. However, its accuracy is limited by the kinetic energy density functional, which is notoriously hard to approximate for non-periodic molecular systems. In this work, we propose M-OFDFT, an OFDFT approach capable of solving molecular systems using a deep-learning functional model. We build the essential nonlocality into the model, which is made affordable by the concise density representation as expansion coefficients under an atomic basis. With techniques to address unconventional learning challenges therein, M-OFDFT achieves a comparable accuracy with Kohn-Sham DFT on a wide range of molecules untouched by OFDFT before. More attractively, M-OFDFT extrapolates well to molecules much larger than those in training, which unleashes the appealing scaling for studying large molecu
&lt;/p&gt;</description></item><item><title>ModuLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#12289;&#33021;&#22815;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#25903;&#25345;3&#27604;&#29305;LLMs&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#30340;&#38598;&#25104;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.16119</link><description>&lt;p&gt;
ModuLoRA:&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#38598;&#25104;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#23545;3 Bit LLMs&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers. (arXiv:2309.16119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16119
&lt;/p&gt;
&lt;p&gt;
ModuLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#12289;&#33021;&#22815;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#25903;&#25345;3&#27604;&#29305;LLMs&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#30340;&#38598;&#25104;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#31639;&#27861;&#65292;&#21487;&#25903;&#25345;&#22312;&#20165;&#20351;&#29992;1&#20010;48GB GPU&#19978;&#20197;3&#27604;&#29305;&#25110;4&#27604;&#29305;&#31934;&#24230;&#24494;&#35843;&#20855;&#26377;65B&#21442;&#25968;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#27169;&#22359;&#21270;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;ModuLoRA&#65289;&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#23558;&#20219;&#20309;&#29992;&#25143;&#25351;&#23450;&#30340;&#26435;&#37325;&#37327;&#21270;&#22120;&#19982;&#24494;&#35843;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#37327;&#21270;&#26080;&#20851;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#33258;&#23450;&#20041;&#30340;&#40657;&#30418;&#37327;&#21270;&#27169;&#22359;&#20174;&#20302;&#31934;&#24230;LLM&#26435;&#37325;&#20013;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#39318;&#27425;&#33021;&#22815;&#36827;&#34892;3&#27604;&#29305;LLMs&#30340;&#24494;&#35843;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;3&#27604;&#29305;OPTQ&#37327;&#21270;&#24448;&#24448;&#20248;&#20110;&#20381;&#36182;&#20110;&#36739;&#19981;&#22797;&#26434;&#30340;4&#27604;&#29305;&#21644;8&#27604;&#29305;&#26041;&#27861;&#30340;&#24494;&#35843;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;ModuLoRA&#22312;&#25991;&#26412;&#20998;&#31867;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#30340;&#20869;&#23384;&#27604;&#29616;&#26377;&#26041;&#27861;&#23569;&#24456;&#22810;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#25688;&#35201;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;ROUGE&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit precision on as little as one 48GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 3-bit LLMs for the first time--leveraging state-of-the-art 3-bit OPTQ quantization often outperforms finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. W
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.15257</link><description>&lt;p&gt;
STARC:&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#24046;&#24322;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
STARC: A General Framework For Quantifying Differences Between Reward Functions. (arXiv:2309.15257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#65292;&#38656;&#35201;&#23558;&#20219;&#21153;&#30340;&#30446;&#26631;&#24418;&#24335;&#21270;&#20026;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#25163;&#21160;&#25351;&#23450;&#19968;&#20010;&#27704;&#19981;&#28608;&#21169;&#19981;&#33391;&#34892;&#20026;&#30340;&#22870;&#21169;&#20989;&#25968;&#38750;&#24120;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#26469;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#23578;&#26410;&#23436;&#21892;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#24120;&#19981;&#30693;&#36947;&#32473;&#23450;&#30340;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#26159;&#21542;&#20250;&#23398;&#20064;&#21040;&#19968;&#20010;&#23433;&#20840;&#20248;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#24847;&#21619;&#30528;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#24517;&#39035;&#32463;&#36807;&#32463;&#39564;&#35780;&#20272;&#65292;&#36825;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#24456;&#38590;&#39044;&#27979;&#20854;&#22833;&#25928;&#27169;&#24335;&#12290;&#20854;&#20013;&#19968;&#20010;&#38459;&#30861;&#33719;&#24471;&#26356;&#22909;&#29702;&#35770;&#20445;&#35777;&#30340;&#38556;&#30861;&#26159;&#32570;&#20047;&#36739;&#22909;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to predict in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.14859</link><description>&lt;p&gt;
&#23548;&#33322;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#65306;&#20174;LyCORIS&#24494;&#35843;&#21040;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation. (arXiv:2309.14859v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#20854;&#33021;&#22815;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#65292;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#39046;&#20808;&#30340;&#24320;&#28304;&#27169;&#22411;&#22312;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#32473;&#26032;&#26041;&#27861;&#30340;&#25972;&#21512;&#21644;&#31995;&#32479;&#35780;&#20272;&#24102;&#26469;&#20102;&#22810;&#37325;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LyCORIS&#65288;Lora beYond Conventional methods&#65292;Other Rank adaptation Implementations for Stable diffusion&#65289;[https://github.com/KohakuBlueleaf/LyCORIS]&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#22810;&#26679;&#21270;&#30340;&#25351;&#26631;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#24494;&#35843;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#22312;&#19981;&#21516;&#27010;&#24565;&#31867;&#21035;&#19979;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31867;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categori
&lt;/p&gt;</description></item><item><title>FedCompass&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#31471;&#20351;&#29992;&#35745;&#31639;&#33021;&#21147;&#24863;&#30693;&#35843;&#24230;&#22120;&#65292;&#35299;&#20915;&#20102;&#24322;&#26500;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#20013;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#25910;&#25947;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14675</link><description>&lt;p&gt;
FedCompass&#65306;&#22522;&#20110;&#35745;&#31639;&#33021;&#21147;&#24863;&#30693;&#35843;&#24230;&#22120;&#30340;&#24322;&#26500;&#23458;&#25143;&#31471;&#35774;&#22791;&#30340;&#39640;&#25928;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler. (arXiv:2309.14675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14675
&lt;/p&gt;
&lt;p&gt;
FedCompass&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#31471;&#20351;&#29992;&#35745;&#31639;&#33021;&#21147;&#24863;&#30693;&#35843;&#24230;&#22120;&#65292;&#35299;&#20915;&#20102;&#24322;&#26500;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#20013;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#25910;&#25947;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#20026;&#21327;&#21516;&#35757;&#32451;&#40065;&#26834;&#19988;&#27867;&#21270;&#30340;AI&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#65292;&#22914;&#21307;&#30103;&#12289;&#37329;&#34701;&#20197;&#21450;&#32570;&#20047;&#38598;&#20013;&#24335;&#25968;&#25454;&#35774;&#26045;&#30340;&#31185;&#23398;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#23458;&#25143;&#31471;&#65288;&#21363;&#35774;&#22791;&#24322;&#26500;&#24615;&#65289;&#20043;&#38388;&#30340;&#35745;&#31639;&#36164;&#28304;&#24046;&#24322;&#65292;&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#22312;&#31561;&#24453;&#38459;&#22622;&#23458;&#25143;&#31471;&#26102;&#25928;&#29575;&#19979;&#38477;&#12290;&#21516;&#26679;&#65292;&#38750;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30001;&#20110;&#36807;&#26102;&#30340;&#26412;&#22320;&#27169;&#22411;&#21644;&#23458;&#25143;&#31471;&#20559;&#31227;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21644;&#26368;&#32456;&#27169;&#22411;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24322;&#26500;&#23458;&#25143;&#31471;&#21644;&#25968;&#25454;&#20013;&#30340;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedCompass&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#35745;&#31639;&#33021;&#21147;&#24863;&#30693;&#35843;&#24230;&#22120;&#30340;&#21019;&#26032;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-silo federated learning offers a promising solution to collaboratively train robust and generalized AI models without compromising the privacy of local datasets, e.g., healthcare, financial, as well as scientific projects that lack a centralized data facility. Nonetheless, because of the disparity of computing resources among different clients (i.e., device heterogeneity), synchronous federated learning algorithms suffer from degraded efficiency when waiting for straggler clients. Similarly, asynchronous federated learning algorithms experience degradation in the convergence rate and final model accuracy on non-identically and independently distributed (non-IID) heterogeneous datasets due to stale local models and client drift. To address these limitations in cross-silo federated learning with heterogeneous clients and data, we propose FedCompass, an innovative semi-asynchronous federated learning algorithm with a computing power aware scheduler on the server side, which adaptive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#21452;&#37325;&#31283;&#20581;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#36830;&#32493;&#27835;&#30103;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27714;&#35299;&#24178;&#25200;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.12819</link><description>&lt;p&gt;
&#36830;&#32493;&#27835;&#30103;&#30340;&#21452;&#37325;&#31283;&#20581;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Proximal Causal Learning for Continuous Treatments. (arXiv:2309.12819v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#21452;&#37325;&#31283;&#20581;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#36830;&#32493;&#27835;&#30103;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27714;&#35299;&#24178;&#25200;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#26159;&#22312;&#23384;&#22312;&#26410;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#19979;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#34917;&#20805;&#31283;&#20581;&#65288;DR&#65289;&#20272;&#35745;&#22120;&#34987;&#25512;&#23548;&#20986;&#26469;&#65292;&#24182;&#22312;&#20272;&#35745;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#22411;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24418;&#24335;&#30340;DR&#20272;&#35745;&#22120;&#20165;&#38480;&#20110;&#20108;&#36827;&#21046;&#27835;&#30103;&#65292;&#32780;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#27835;&#30103;&#21487;&#20197;&#26159;&#36830;&#32493;&#30340;&#12290;&#36830;&#32493;&#27835;&#30103;&#30340;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#22312;&#21407;&#22987;DR&#20272;&#35745;&#22120;&#20013;&#23384;&#22312;&#30340;delta&#20989;&#25968;&#65292;&#20351;&#24471;&#22312;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#19981;&#21487;&#34892;&#65292;&#24182;&#22312;&#24178;&#25200;&#20989;&#25968;&#20272;&#35745;&#20013;&#24341;&#20837;&#20102;&#27785;&#37325;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#36830;&#32493;&#27835;&#30103;&#30340;DR&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#36830;&#32493;&#27835;&#30103;&#12290;&#37197;&#22791;&#20854;&#24179;&#28369;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;Oracle&#24418;&#24335;&#26159;&#24433;&#21709;&#20989;&#25968;&#30340;&#19968;&#33268;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#39640;&#25928;&#35299;&#20915;&#24178;&#25200;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proximal causal learning is a promising framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatment can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#23398;&#20064;&#26041;&#27861;&#65288;BDyMA&#65289;&#26469;&#35299;&#20915;&#22312;&#21457;&#29616;&#22823;&#33041;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#26694;&#26550;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#21644;&#26356;&#31232;&#30095;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#20351;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#21462;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#12290;</title><link>http://arxiv.org/abs/2309.07080</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#23398;&#20064;&#65306;&#22312;&#21457;&#29616;&#22823;&#33041;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain. (arXiv:2309.07080v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#23398;&#20064;&#26041;&#27861;&#65288;BDyMA&#65289;&#26469;&#35299;&#20915;&#22312;&#21457;&#29616;&#22823;&#33041;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#26694;&#26550;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#21644;&#26356;&#31232;&#30095;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#20351;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#21462;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#65288;DEC&#65289;&#21487;&#20197;&#25581;&#31034;&#22823;&#33041;&#30340;&#22797;&#26434;&#26426;&#21046;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#35780;&#20998;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#21457;&#29616;&#26041;&#27861;&#22312;&#25552;&#21462;&#22240;&#26524;&#32467;&#26500;&#21644;&#25512;&#26029;&#26377;&#25928;&#36830;&#25509;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#23398;&#20064;DEC&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#19968;&#20010;&#26159;&#39640;&#32500;&#21160;&#24577;DAG&#21457;&#29616;&#26041;&#27861;&#30340;&#26681;&#26412;&#26080;&#33021;&#21147;&#65292;&#21478;&#19968;&#20010;&#26159;fMRI&#25968;&#25454;&#36136;&#37327;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;M-&#30697;&#38453;&#26080;&#29615;&#29305;&#24615;&#30340;&#36125;&#21494;&#26031;&#21160;&#24577;DAG&#23398;&#20064;&#65288;BDyMA&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#21457;&#29616;DEC&#20013;&#30340;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#22240;&#26524;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#21452;&#21521;&#36793;&#32536;&#12290;&#21033;&#29992;BDyMA&#26041;&#27861;&#20013;&#30340;&#26080;&#32422;&#26463;&#26694;&#26550;&#22312;&#26816;&#27979;&#39640;&#32500;&#32593;&#32476;&#26041;&#38754;&#21487;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#23454;&#29616;&#26356;&#31232;&#30095;&#30340;&#32467;&#26524;&#65292;&#20351;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#21462;DEC&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the complex mechanisms of the brain can be unraveled by extracting the Dynamic Effective Connectome (DEC). Recently, score-based Directed Acyclic Graph (DAG) discovery methods have shown significant improvements in extracting the causal structure and inferring effective connectivity. However, learning DEC through these methods still faces two main challenges: one with the fundamental impotence of high-dimensional dynamic DAG discovery methods and the other with the low quality of fMRI data. In this paper, we introduce Bayesian Dynamic DAG learning with M-matrices Acyclicity characterization \textbf{(BDyMA)} method to address the challenges in discovering DEC. The presented dynamic causal model enables us to discover bidirected edges as well. Leveraging an unconstrained framework in the BDyMA method leads to more accurate results in detecting high-dimensional networks, achieving sparser outcomes, making it particularly suitable for extracting DEC. Additionally, the score f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#25913;&#36827;&#20102;Spalart-Allmaras&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#31163;&#27969;&#20307;&#30340;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#35299;&#30340;&#27867;&#21270;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06679</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#23454;&#29616;&#23545;Spalart-Allmaras&#27169;&#22411;&#30340;&#21487;&#26222;&#36866;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Generalizable improvement of the Spalart-Allmaras model through assimilation of experimental data. (arXiv:2309.06679v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#25913;&#36827;&#20102;Spalart-Allmaras&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#31163;&#27969;&#20307;&#30340;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#35299;&#30340;&#27867;&#21270;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#27169;&#22411;&#21644;&#25968;&#25454;&#34701;&#21512;&#25913;&#36827;&#20998;&#31163;&#27969;&#20307;&#30340;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#35299;&#30340;Spalart-Allmaras&#65288;SA&#65289;&#38381;&#21512;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#27169;&#22411;&#65292;&#19981;&#20165;&#33021;&#23558;&#31232;&#30095;&#30340;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#20197;&#25913;&#21892;&#35745;&#31639;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36824;&#33021;&#36890;&#36807;&#24674;&#22797;&#32463;&#20856;&#30340;SA&#34892;&#20026;&#26469;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#21516;&#21270;&#65292;&#21363;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#26041;&#27861;&#65288;EnKF&#65289;&#65292;&#36890;&#36807;&#23558;SA&#27169;&#22411;&#30340;&#31995;&#25968;&#26657;&#20934;&#21040;&#20998;&#31163;&#27969;&#20307;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#21442;&#25968;&#21270;&#20135;&#29983;&#12289;&#25193;&#25955;&#21644;&#30772;&#22351;&#39033;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26657;&#20934;&#31574;&#30053;&#12290;&#35813;&#26657;&#20934;&#20381;&#36182;&#20110;&#37319;&#38598;&#30340;&#20998;&#31163;&#27969;&#20307;&#36895;&#24230;&#21078;&#38754;&#12289;&#22721;&#25830;&#21147;&#21644;&#21387;&#21147;&#31995;&#25968;&#30340;&#23454;&#39564;&#25968;&#25454;&#30340;&#21516;&#21270;&#12290;&#23613;&#31649;&#20165;&#20351;&#29992;&#20102;&#26469;&#33258;&#21333;&#19968;&#27969;&#21160;&#26465;&#20214;&#65288;&#29615;&#32469;&#19968;&#20010;&#32972;&#38754;&#21488;&#38454;&#65289;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#20294;&#37325;&#26032;&#26657;&#20934;&#30340;SA&#27169;&#22411;&#34920;&#29616;&#20986;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on the use of model and data fusion for improving the Spalart-Allmaras (SA) closure model for Reynolds-averaged Navier-Stokes solutions of separated flows. In particular, our goal is to develop of models that not-only assimilate sparse experimental data to improve performance in computational models, but also generalize to unseen cases by recovering classical SA behavior. We achieve our goals using data assimilation, namely the Ensemble Kalman Filtering approach (EnKF), to calibrate the coefficients of the SA model for separated flows. A holistic calibration strategy is implemented via a parameterization of the production, diffusion, and destruction terms. This calibration relies on the assimilation of experimental data collected velocity profiles, skin friction, and pressure coefficients for separated flows. Despite using of observational data from a single flow condition around a backward-facing step (BFS), the recalibrated SA model demonstrates generalization to o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2309.03883</link><description>&lt;p&gt;
DoLa&#65306;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03883
&lt;/p&gt;
&lt;p&gt;
DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#19982;&#39044;&#35757;&#32451;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#20107;&#23454;&#20559;&#31163;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#20943;&#23569;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#24187;&#35273;&#65292;&#23427;&#19981;&#38656;&#35201;&#22312;&#26816;&#32034;&#30340;&#22806;&#37096;&#30693;&#35782;&#25110;&#39069;&#22806;&#30340;&#24494;&#35843;&#19978;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23558;&#36739;&#26202;&#23618;&#21644;&#36739;&#26089;&#23618;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#24471;&#21040;&#30340;&#36923;&#36753;&#24046;&#24322;&#26469;&#33719;&#24471;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#20998;&#24067;&#65292;&#21033;&#29992;&#20102;LLMs&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#36890;&#24120;&#34987;&#35777;&#26126;&#23616;&#37096;&#21270;&#22312;&#29305;&#23450;&#30340;Transformer&#23618;&#20013;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#30340;&#35299;&#30721;&#65288;DoLa&#65289;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23637;&#31034;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#20943;&#23569;&#29983;&#25104;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#24773;&#20917;&#12290;DoLa&#22312;&#22810;&#20010;&#36873;&#25321;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#21319;&#20102;&#30495;&#23454;&#24615;&#65292;&#20363;&#22914;&#25913;&#21892;&#20102;LLaMA&#31995;&#21015;&#27169;&#22411;&#22312;TruthfulQA&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32034;&#24341;&#24863;&#30693;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30005;&#36335;&#36827;&#34892;&#35299;&#21078;&#27010;&#24565;&#30340;&#24212;&#29992;&#65292;&#23558;&#24046;&#20998;&#20195;&#25968;&#26041;&#31243;&#32452;&#20998;&#35299;&#20026;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20195;&#25968;&#26041;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#26469;&#37327;&#21270;&#30005;&#36335;&#35774;&#35745;&#20013;&#21487;&#35843;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00958</link><description>&lt;p&gt;
&#30005;&#36335;&#30340;&#32034;&#24341;&#24863;&#30693;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Index-aware learning of circuits. (arXiv:2309.00958v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32034;&#24341;&#24863;&#30693;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30005;&#36335;&#36827;&#34892;&#35299;&#21078;&#27010;&#24565;&#30340;&#24212;&#29992;&#65292;&#23558;&#24046;&#20998;&#20195;&#25968;&#26041;&#31243;&#32452;&#20998;&#35299;&#20026;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20195;&#25968;&#26041;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#26469;&#37327;&#21270;&#30005;&#36335;&#35774;&#35745;&#20013;&#21487;&#35843;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30005;&#36335;&#22312;&#21508;&#31181;&#25216;&#26415;&#20013;&#37117;&#23384;&#22312;&#65292;&#20854;&#35774;&#35745;&#26159;&#35745;&#31639;&#26426;&#36741;&#21161;&#24037;&#31243;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#24433;&#21709;&#26368;&#32456;&#35774;&#35745;&#30340;&#21487;&#35843;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#23545;&#37327;&#21270;&#20854;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#22312;&#36825;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#26377;&#20851;&#29616;&#26377;&#31995;&#32479;&#30340;&#30693;&#35782;&#12290;&#23601;&#30005;&#36335;&#32780;&#35328;&#65292;&#36890;&#36807;&#20462;&#25913;&#33410;&#28857;&#20998;&#26512;&#23545;&#20854;&#36827;&#34892;&#25551;&#36848;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#12290;&#36825;&#31181;&#29305;&#23450;&#30340;&#34920;&#36798;&#24418;&#24335;&#23548;&#33268;&#20102;&#24046;&#20998;&#20195;&#25968;&#26041;&#31243;&#32452;&#65288;DAEs&#65289;&#65292;&#20854;&#20013;&#23384;&#22312;&#19968;&#20123;&#29305;&#27530;&#24615;&#36136;&#65292;&#20363;&#22914;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#28385;&#36275;&#30340;&#38544;&#34255;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#29992;&#20110;DAEs&#30340;&#35299;&#21078;&#27010;&#24565;&#65292;&#21487;&#20197;&#23558;&#32473;&#23450;&#31995;&#32479;&#20998;&#35299;&#20026;&#20165;&#20381;&#36182;&#20110;&#24046;&#20998;&#21464;&#37327;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20165;&#25551;&#36848;&#24046;&#20998;&#21644;&#20195;&#25968;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#20195;&#25968;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical circuits are present in a variety of technologies, making their design an important part of computer aided engineering. The growing number of tunable parameters that affect the final design leads to a need for new approaches of quantifying their impact. Machine learning may play a key role in this regard, however current approaches often make suboptimal use of existing knowledge about the system at hand. In terms of circuits, their description via modified nodal analysis is well-understood. This particular formulation leads to systems of differential-algebraic equations (DAEs) which bring with them a number of peculiarities, e.g. hidden constraints that the solution needs to fulfill. We aim to use the recently introduced dissection concept for DAEs that can decouple a given system into ordinary differential equations, only depending on differential variables, and purely algebraic equations that describe the relations between differential and algebraic variables. The idea the
&lt;/p&gt;</description></item><item><title>MatchXML&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;label2vec&#26041;&#27861;&#29983;&#25104;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#23884;&#20837;&#26500;&#24314;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#65292;MatchXML&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#21462;&#20986;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#38745;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2308.13139</link><description>&lt;p&gt;
MatchXML: &#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification. (arXiv:2308.13139v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13139
&lt;/p&gt;
&lt;p&gt;
MatchXML&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;label2vec&#26041;&#27861;&#29983;&#25104;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#23884;&#20837;&#26500;&#24314;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#65292;MatchXML&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#21462;&#20986;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#38745;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65288;XMC&#65289;&#26159;&#25351;&#35757;&#32451;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#20174;&#19968;&#20010;&#38750;&#24120;&#22823;&#35268;&#27169;&#30340;&#26631;&#31614;&#38598;&#20013;&#65288;&#20363;&#22914;&#25968;&#30334;&#19975;&#20010;&#26631;&#31614;&#65289;&#20026;&#25991;&#26412;&#26679;&#26412;&#20998;&#37197;&#30456;&#20851;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MatchXML&#65292;&#19968;&#31181;&#29992;&#20110;XMC&#30340;&#39640;&#25928;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30001;&#31232;&#30095;&#30340;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#29305;&#24449;&#29983;&#25104;&#30340;&#26631;&#31614;&#23884;&#20837;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;label2vec&#65292;&#36890;&#36807;Skip-gram&#27169;&#22411;&#26469;&#26377;&#25928;&#35757;&#32451;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36825;&#20123;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#26469;&#26500;&#24314;&#19968;&#20010;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;Transformer&#26102;&#65292;&#25105;&#20204;&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#22312;&#20108;&#20998;&#22270;&#20013;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20174;&#24494;&#35843;&#21518;&#30340;Transformer&#20013;&#25552;&#21462;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#38500;&#20102;&#24494;&#35843;&#21518;&#30340;&#23494;&#38598;&#25991;&#26412;&#23884;&#20837;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#39044;&#35757;&#32451;&#30340;Sentence Transformer&#20013;&#25552;&#21462;&#38745;&#24577;&#30340;&#23494;&#38598;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The eXtreme Multi-label text Classification(XMC) refers to training a classifier that assigns a text sample with relevant labels from an extremely large-scale label set (e.g., millions of labels). We propose MatchXML, an efficient text-label matching framework for XMC. We observe that the label embeddings generated from the sparse Term Frequency-Inverse Document Frequency(TF-IDF) features have several limitations. We thus propose label2vec to effectively train the semantic dense label embeddings by the Skip-gram model. The dense label embeddings are then used to build a Hierarchical Label Tree by clustering. In fine-tuning the pre-trained encoder Transformer, we formulate the multi-label text classification as a text-label matching problem in a bipartite graph. We then extract the dense text representations from the fine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also extract the static dense sentence embeddings from a pre-trained Sentence Transformer. Finally,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#25511;&#21046;&#32447;&#24615;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35782;&#21035;&#31995;&#32479;&#27169;&#22411;&#65292;&#32780;&#26159;&#36890;&#36807;&#32047;&#31215;&#25200;&#21160;&#26469;&#36827;&#34892;&#20915;&#31574;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#24615;&#33021;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2308.08138</link><description>&lt;p&gt;
&#22312;&#32447;&#25511;&#21046;&#32447;&#24615;&#21160;&#21147;&#23398;&#65306;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Control for Linear Dynamics: A Data-Driven Approach. (arXiv:2308.08138v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#25511;&#21046;&#32447;&#24615;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35782;&#21035;&#31995;&#32479;&#27169;&#22411;&#65292;&#32780;&#26159;&#36890;&#36807;&#32047;&#31215;&#25200;&#21160;&#26469;&#36827;&#34892;&#20915;&#31574;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#24615;&#33021;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#22312;&#32447;&#25511;&#21046;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#20855;&#26377;&#26410;&#30693;&#21160;&#21147;&#23398;&#12289;&#26377;&#30028;&#25200;&#21160;&#21644;&#23545;&#25239;&#25104;&#26412;&#30340;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#26469;&#38477;&#20302;&#25511;&#21046;&#22120;&#30340;&#21518;&#24724;&#12290;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#35782;&#21035;&#31995;&#32479;&#27169;&#22411;&#65292;&#32780;&#26159;&#21033;&#29992;&#21333;&#20010;&#26080;&#22122;&#22768;&#36712;&#36857;&#26469;&#35745;&#31639;&#25200;&#21160;&#30340;&#32047;&#31215;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#35774;&#35745;&#30340;&#32047;&#31215;&#25200;&#21160;&#21160;&#20316;&#25511;&#21046;&#22120;&#20570;&#20986;&#20915;&#31574;&#65292;&#20854;&#21442;&#25968;&#36890;&#36807;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#26356;&#26032;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#31639;&#27861;&#30340;&#21518;&#24724;&#26159;$\mathcal{O}(\sqrt{T})$&#30340;&#65292;&#36825;&#34920;&#26126;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers an online control problem over a linear time-invariant system with unknown dynamics, bounded disturbance, and adversarial cost. We propose a data-driven strategy to reduce the regret of the controller. Unlike model-based methods, our algorithm does not identify the system model, instead, it leverages a single noise-free trajectory to calculate the accumulation of disturbance and makes decisions using the accumulated disturbance action controller we design, whose parameters are updated by online gradient descent. We prove that the regret of our algorithm is $\mathcal{O}(\sqrt{T})$ under mild assumptions, suggesting that its performance is on par with model-based methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#20197;&#20960;&#20309;&#36895;&#24230;&#25910;&#25947;&#65292;&#20026;BBVI&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25913;&#36827;&#65292;&#23545;&#27604;&#20102;STL&#20272;&#35745;&#22120;&#65292;&#24182;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.14642</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65306;&#25105;&#20204;&#24212;&#35813;&#22362;&#25345;&#21040;&#24213;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#20197;&#20960;&#20309;&#36895;&#24230;&#25910;&#25947;&#65292;&#20026;BBVI&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25913;&#36827;&#65292;&#23545;&#27604;&#20102;STL&#20272;&#35745;&#22120;&#65292;&#24182;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#65292;&#29305;&#21035;&#26159;&#30528;&#38470;&#31283;&#23450;&#65288;STL&#65289;&#20272;&#35745;&#22120;&#65292;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#25910;&#25947;&#20110;&#20960;&#20309;&#65288;&#20256;&#32479;&#19978;&#31216;&#20026;&#8220;&#32447;&#24615;&#8221;&#65289;&#36895;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;STL&#20272;&#35745;&#22120;&#30340;&#26799;&#24230;&#26041;&#24046;&#30340;&#20108;&#27425;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#21253;&#25324;&#20102;&#35823;&#25351;&#23450;&#30340;&#21464;&#20998;&#26063;&#12290;&#32467;&#21512;&#20808;&#21069;&#20851;&#20110;&#20108;&#27425;&#26041;&#24046;&#26465;&#20214;&#30340;&#24037;&#20316;&#65292;&#36825;&#30452;&#25509;&#26263;&#31034;&#20102;&#22312;&#20351;&#29992;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;BBVI&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#29616;&#26377;&#23545;&#20110;&#27491;&#24120;&#23553;&#38381;&#24418;&#24335;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#20998;&#26512;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#20854;&#19982;STL&#20272;&#35745;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#20026;&#20004;&#32773;&#25552;&#20379;&#26126;&#30830;&#30340;&#38750;&#28176;&#36827;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#27491;&#21017;&#21270;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32597;&#35265;&#36139;&#34880;&#30142;&#30149;&#30340;&#32418;&#32454;&#32990;&#20998;&#31867;&#12290;&#36890;&#36807;&#20174;&#21333;&#20010;&#32418;&#32454;&#32990;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#23610;&#24230;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#36827;&#34892;&#27169;&#22411;&#27491;&#21017;&#21270;&#65292;&#20197;&#20445;&#25345;&#25968;&#25454;&#30340;&#29305;&#24449;&#25299;&#25169;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.14025</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#27491;&#21017;&#21270;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#29992;&#20110;&#32418;&#32454;&#32990;&#30142;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification. (arXiv:2307.14025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#27491;&#21017;&#21270;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32597;&#35265;&#36139;&#34880;&#30142;&#30149;&#30340;&#32418;&#32454;&#32990;&#20998;&#31867;&#12290;&#36890;&#36807;&#20174;&#21333;&#20010;&#32418;&#32454;&#32990;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#23610;&#24230;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#36827;&#34892;&#27169;&#22411;&#27491;&#21017;&#21270;&#65292;&#20197;&#20445;&#25345;&#25968;&#25454;&#30340;&#29305;&#24449;&#25299;&#25169;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26174;&#24494;&#22270;&#20687;&#35786;&#26029;&#32597;&#35265;&#30340;&#36139;&#34880;&#30142;&#30149;&#23545;&#20110;&#29087;&#32451;&#30340;&#19987;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#20110;&#22312;&#21333;&#20010;&#34880;&#26679;&#20013;&#26377;&#25968;&#21315;&#20010;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#32454;&#32990;&#65292;&#36825;&#26500;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#38382;&#39064;&#12290;&#34429;&#28982;&#32418;&#32454;&#32990;&#30340;&#31354;&#38388;&#37051;&#22495;&#26412;&#36523;&#24182;&#19981;&#37325;&#35201;&#65292;&#20294;&#25972;&#20010;&#34880;&#26679;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#21363;&#25968;&#25454;&#30340;&#20960;&#20309;&#24615;&#36136;&#65292;&#21253;&#21547;&#20102;&#26377;&#30410;&#30340;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#20856;&#22411;&#30340;MIL&#38382;&#39064;&#65292;&#22914;&#26799;&#24230;&#28040;&#22833;&#21644;&#22312;&#26377;&#38480;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#30340;&#36807;&#25311;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#30340;&#26041;&#27861;&#65292;&#20174;&#21333;&#20010;&#32418;&#32454;&#32990;&#22270;&#20687;&#30340;&#21253;&#20013;&#25552;&#21462;&#22810;&#23610;&#24230;&#30340;&#25299;&#25169;&#29305;&#24449;&#12290;&#36825;&#20123;&#25299;&#25169;&#29305;&#24449;&#34987;&#29992;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#24378;&#21046;&#20445;&#25345;&#25968;&#25454;&#30340;&#29305;&#24449;&#25299;&#25169;&#23646;&#24615;&#12290;&#22312;&#21253;&#21547;71&#20010;&#32597;&#35265;&#36139;&#34880;&#30142;&#30149;&#24739;&#32773;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;521&#24352;&#32418;&#32454;&#32990;&#26174;&#24494;&#22270;&#20687;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#25299;&#25169;&#27491;&#21017;&#21270;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosing rare anemia disorders using microscopic images is challenging for skilled specialists and machine-learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13709</link><description>&lt;p&gt;
&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65306;&#22312;&#27809;&#26377;&#20855;&#20307;&#35780;&#20215;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#29289;&#21697;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23646;&#24615;&#65292;&#22914;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#21487;&#21462;&#24615;&#25110;&#24378;&#24230;&#65292;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#24050;&#30693;&#29289;&#21697;&#30340;&#36825;&#20123;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20986;&#29616;&#22312;&#37197;&#23545;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#30340;&#36816;&#21160;&#21592;&#30340;&#23454;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#32541;&#22320;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#20010;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#30340;&#38750;&#23545;&#31216;&#29615;&#22659;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;DBTR&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.11949</link><description>&lt;p&gt;
HIQL: &#20197;&#28508;&#22312;&#29366;&#24577;&#20316;&#20026;&#21160;&#20316;&#30340;&#31163;&#32447;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26368;&#36817;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#28508;&#22312;&#22320;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#65288;&#26080;&#22870;&#21169;&#65289;&#25968;&#25454;&#65292;&#25552;&#20379;&#31867;&#20284;&#20110;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#30452;&#25509;&#20174;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20934;&#30830;&#20272;&#35745;&#36828;&#26399;&#30446;&#26631;&#30340;&#20215;&#20540;&#20989;&#25968;&#24456;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#21363;&#36798;&#21040;&#36828;&#26399;&#30446;&#26631;&#38656;&#35201;&#39318;&#20808;&#36890;&#36807;&#36739;&#36817;&#23376;&#30446;&#26631;&#12290;&#36825;&#31181;&#32467;&#26500;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35780;&#20272;&#37051;&#36817;&#30446;&#26631;&#30340;&#21160;&#20316;&#36136;&#37327;&#36890;&#24120;&#27604;&#26356;&#36828;&#30446;&#26631;&#23481;&#26131;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;&#21033;&#29992;&#19968;&#20010;&#27809;&#26377;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65306;&#19968;&#20010;&#39640;&#23618;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#32447;&#24615;&#23618;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#35268;&#21017;&#30340;&#38750;&#32447;&#24615;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.11888</link><description>&lt;p&gt;
&#20851;&#20110;&#32447;&#24615;&#36882;&#25512;&#21644;&#38750;&#32447;&#24615;&#25237;&#24433;&#30340;&#26222;&#36941;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Universality of Linear Recurrences Followed by Nonlinear Projections. (arXiv:2307.11888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#32447;&#24615;&#23618;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#35268;&#21017;&#30340;&#38750;&#32447;&#24615;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#27880;&#37322;&#65288;&#20316;&#20026;&#19968;&#31687;&#20840;&#25991;&#35770;&#25991;&#30340;&#24037;&#20316;&#36827;&#23637;&#65289;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#26063;&#22522;&#20110;&#24490;&#29615;&#32447;&#24615;&#23618;&#65288;&#21253;&#25324;S4&#12289;S5&#21644;LRU&#65289;&#21644;&#20301;&#32622;&#36880;&#20803;&#32032;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#36924;&#36817;&#20219;&#24847;&#35268;&#21017;&#30340;&#38750;&#32447;&#24615;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#32972;&#21518;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#24490;&#29615;&#23618;&#35270;&#20026;&#21487;&#20197;&#24544;&#23454;&#22320;&#23384;&#20648;&#36755;&#20837;&#24207;&#21015;&#20449;&#24687;&#21040;&#20869;&#37096;&#29366;&#24577;&#30340;&#21387;&#32553;&#31639;&#27861;&#65292;&#28982;&#21518;&#30001;&#39640;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;MLP&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note (work in progress towards a full-length paper) we show that a family of sequence models based on recurrent linear layers~(including S4, S5, and the LRU) interleaved with position-wise multi-layer perceptrons~(MLPs) can approximate arbitrarily well any sufficiently regular non-linear sequence-to-sequence map. The main idea behind our result is to see recurrent layers as compression algorithms that can faithfully store information about the input sequence into an inner state, before it is processed by the highly expressive MLP.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.09476</link><description>&lt;p&gt;
&#36807;&#24230;&#24605;&#32771;&#30495;&#30456;&#65306;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36827;&#34892;&#22797;&#26434;&#27169;&#24335;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#37325;&#29616;&#19981;&#20934;&#30830;&#25110;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#30740;&#31350;&#26377;&#23475;&#30340;&#27169;&#20223;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#20010;&#30456;&#20851;&#29616;&#35937;&#65306;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#12290;&#31532;&#19968;&#20010;&#29616;&#35937;&#65292;&#36807;&#24230;&#24605;&#32771;&#65292;&#22312;&#32473;&#20986;&#27491;&#30830;&#19982;&#38169;&#35823;&#30340;&#23569;&#37327;&#31034;&#33539;&#26102;&#65292;&#25105;&#20204;&#20174;&#20013;&#38388;&#23618;&#35299;&#30721;&#39044;&#27979;&#12290;&#22312;&#26089;&#26399;&#23618;&#20013;&#65292;&#20004;&#31181;&#31034;&#33539;&#24341;&#36215;&#20102;&#30456;&#20284;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#20294;&#22312;&#26576;&#20010;&#8220;&#20851;&#38190;&#23618;&#8221;&#20043;&#21518;&#65292;&#32473;&#20986;&#38169;&#35823;&#31034;&#33539;&#30340;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#12290;&#31532;&#20108;&#20010;&#29616;&#35937;&#65292;&#38169;&#35823;&#24402;&#32435;&#22836;&#65292;&#21487;&#33021;&#26159;&#36807;&#24230;&#24605;&#32771;&#30340;&#19968;&#31181;&#26426;&#21046;&#24615;&#21407;&#22240;&#65306;&#36825;&#20123;&#26159;&#20301;&#20110;&#36739;&#26202;&#23618;&#30340;&#22836;&#37096;&#65292;&#23427;&#20204;&#20851;&#27880;&#24182;&#22797;&#21046;&#20808;&#21069;&#31034;&#33539;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20854;&#21066;&#24369;&#20250;&#20943;&#23569;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NetHack&#28216;&#25103;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#21457;&#29616;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#21487;&#20197;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#24314;&#31435;&#20102;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;IL&#20195;&#29702;&#20154;&#30340;&#24130;&#24459;&#12290;</title><link>http://arxiv.org/abs/2307.09423</link><description>&lt;p&gt;
&#22312;NetHack&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#35268;&#27169;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Imitation Learning in NetHack. (arXiv:2307.09423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NetHack&#28216;&#25103;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#21457;&#29616;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#21487;&#20197;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#24314;&#31435;&#20102;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;IL&#20195;&#29702;&#20154;&#30340;&#24130;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064; (IL) &#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#24378;&#22823;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#21457;&#29616;&#23427;&#24448;&#24448;&#19981;&#33021;&#23436;&#20840;&#24674;&#22797;&#20986;&#28508;&#22312;&#30340;&#19987;&#23478;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#27809;&#26377;&#28145;&#20837;&#25506;&#31350;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#25193;&#22823;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#21463;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#39046;&#22495;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#22312;&#37027;&#37324;&#8220;&#25193;&#22823;&#35268;&#27169;&#8221;&#24050;&#32463;&#23548;&#33268;&#20102;&#36234;&#26469;&#36234;&#26377;&#33021;&#21147;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20180;&#32454;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#26159;&#21542;&#21487;&#20197;&#22312;&#27169;&#20223;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#24102;&#26469;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312; NetHack &#28216;&#25103;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#31243;&#24207;&#29983;&#25104;&#12289;&#38543;&#26426;&#24615;&#12289;&#38271;&#26399;&#20381;&#36182;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#21457;&#29616; IL &#30340;&#25439;&#22833;&#21644;&#24179;&#22343;&#22238;&#25253;&#38543;&#30528;&#35745;&#31639;&#39044;&#31639;&#30340;&#21464;&#21270;&#32780;&#24179;&#28369;&#21464;&#21270;&#19988;&#24378;&#30456;&#20851;&#65292;&#20174;&#32780;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#26679;&#26412;&#25968;&#37327;&#26041;&#38754;&#20026;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;&#30340; IL &#20195;&#29702;&#20154;&#30340;&#35745;&#31639;&#39044;&#31639;&#24314;&#31435;&#20102;&#24130;&#24459;&#12290;&#25105;&#20204;&#39044;&#27979;&#24182;&#35757;&#32451;&#20102;&#20960;&#20010;&#20855;&#26377; IL &#30340;NetHack&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL an
&lt;/p&gt;</description></item><item><title>NetGPT&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#26469;&#20248;&#21270;&#36164;&#28304;&#21327;&#35843;&#21644;&#20114;&#21160;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06148</link><description>&lt;p&gt;
NetGPT: &#36229;&#36234;&#25552;&#20379;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services. (arXiv:2307.06148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06148
&lt;/p&gt;
&lt;p&gt;
NetGPT&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#26469;&#20248;&#21270;&#36164;&#28304;&#21327;&#35843;&#21644;&#20114;&#21160;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#29983;&#25104;&#20449;&#24687;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;LLMs&#30340;&#20010;&#24615;&#21270;&#21487;&#33021;&#36827;&#19968;&#27493;&#20419;&#36827;&#23427;&#20204;&#22312;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#12290;&#38024;&#23545;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#21548;&#36215;&#26469;&#24456;&#26377;&#21069;&#26223;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#26377;&#25928;&#21327;&#35843;&#24322;&#26500;&#20998;&#24067;&#24335;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#31181;&#20505;&#36873;&#30340;&#20113;&#36793;&#21327;&#20316;&#25216;&#26415;&#30340;&#21033;&#24330;&#65292;&#25552;&#20986;&#20102;NetGPT&#65292;&#26681;&#25454;&#20854;&#35745;&#31639;&#33021;&#21147;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;LLMs&#12290;&#27492;&#22806;&#65292;&#36793;&#32536;LLMs&#21487;&#20197;&#39640;&#25928;&#21033;&#29992;&#22522;&#20110;&#20301;&#32622;&#30340;&#20449;&#24687;&#36827;&#34892;&#20010;&#24615;&#21270;&#25552;&#31034;&#23436;&#25104;&#65292;&#20174;&#32780;&#26377;&#30410;&#20110;&#19982;&#20113;&#31471;LLMs&#30340;&#20114;&#21160;&#12290;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#20195;&#34920;&#24615;&#30340;&#24320;&#28304;LLMs&#65288;&#20363;&#22914;GPT-2-base&#21644;LLaMA&#27169;&#22411;&#65289;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NetGPT&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have triggered tremendous success to empower daily life by generative information, and the personalization of LLMs could further contribute to their applications due to better alignment with human intents. Towards personalized generative services, a collaborative cloud-edge methodology sounds promising, as it facilitates the effective orchestration of heterogeneous distributed communication and computing resources. In this article, after discussing the pros and cons of several candidate cloud-edge collaboration techniques, we put forward NetGPT to capably deploy appropriate LLMs at the edge and the cloud in accordance with their computing capacity. In addition, edge LLMs could efficiently leverage location-based information for personalized prompt completion, thus benefiting the interaction with cloud LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2307.05358</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#35843;&#33410;&#22120;&#35299;&#20915;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20174;&#20998;&#25955;&#24322;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#30001;&#20110;&#20998;&#25955;&#23458;&#25143;&#31471;&#19978;&#26631;&#31614;&#31232;&#32570;&#65292;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;FSSL&#65289;&#20986;&#29616;&#20197;&#20174;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#20013;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FSSL&#26041;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26631;&#31614;&#25968;&#25454;&#29420;&#31435;&#19988;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20855;&#26377;&#19968;&#33268;&#30340;&#31867;&#21035;&#20998;&#24067;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;FSSL&#30340;&#26356;&#23454;&#38469;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#19981;&#20165;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#19981;&#21516;&#65292;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20063;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;FSSL&#26694;&#26550;&#65292;FedDure&#12290;FedDure&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#65288;C-reg&#65289;&#21644;&#32454;&#35843;&#33410;&#22120;&#65288;F-reg&#65289;&#35299;&#38500;&#20102;&#20197;&#21069;&#30340;&#20551;&#35774;&#65306;C-reg&#36890;&#36807;&#36319;&#36394;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#23398;&#20064;&#25928;&#26524;&#26469;&#35268;&#33539;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#65307;F-reg&#23398;&#20064;&#19968;&#20010;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#20197;&#36866;&#24212;&#23458;&#25143;&#31471;&#20869;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.} FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored f
&lt;/p&gt;</description></item><item><title>[SF]$^2$M&#26159;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#23398;&#12290;&#23427;&#23558;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#29983;&#25104;&#24314;&#27169;&#35299;&#37322;&#20026;Schr\"odinger&#26725;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38745;&#24577;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#26469;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23398;&#20064;&#32454;&#32990;&#21160;&#21147;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.03672</link><description>&lt;p&gt;
&#36890;&#36807;&#24471;&#20998;&#21644;&#27969;&#21305;&#37197;&#23454;&#29616;&#26080;&#38656;&#27169;&#25311;&#30340;Schr\"odinger&#26725;
&lt;/p&gt;
&lt;p&gt;
Simulation-free Schr\"odinger bridges via score and flow matching. (arXiv:2307.03672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03672
&lt;/p&gt;
&lt;p&gt;
[SF]$^2$M&#26159;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#23398;&#12290;&#23427;&#23558;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#29983;&#25104;&#24314;&#27169;&#35299;&#37322;&#20026;Schr\"odinger&#26725;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38745;&#24577;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#26469;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23398;&#20064;&#32454;&#32990;&#21160;&#21147;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#38656;&#27169;&#25311;&#30340;&#24471;&#20998;&#21644;&#27969;&#21305;&#37197;&#65288;[SF]$^2$M&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25512;&#26029;&#32473;&#23450;&#26469;&#33258;&#20219;&#24847;&#20998;&#24067;&#30340;&#26410;&#37197;&#23545;&#28304;&#21644;&#30446;&#26631;&#26679;&#26412;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#29992;&#20110;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#35757;&#32451;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#30340;&#27969;&#21305;&#37197;&#25439;&#22833;&#12290;[SF]$^2$M&#23558;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#29983;&#25104;&#24314;&#27169;&#35299;&#37322;&#20026;Schr\"odinger&#26725;&#38382;&#39064;&#12290;&#23427;&#20381;&#36182;&#20110;&#38745;&#24577;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#25110;&#23567;&#25209;&#37327;&#36817;&#20284;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;Schr\"odinger&#26725;&#65292;&#32780;&#26080;&#38656;&#27169;&#25311;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#20808;&#21069;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;[SF]$^2$M&#26356;&#39640;&#25928;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;Schr\"odinger&#26725;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;[SF]$^2$M&#24212;&#29992;&#20110;&#20174;&#24555;&#29031;&#25968;&#25454;&#20013;&#23398;&#20064;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;[SF]$^2$M&#26159;&#39318;&#20010;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#39640;&#32500;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired source and target samples drawn from arbitrary distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schr\"odinger bridge (SB) problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#29305;&#24449;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#39640;&#32500;&#22810;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#20855;&#26377;&#36890;&#20449;&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;&#29305;&#24449;&#32500;&#24230;&#21644;&#24555;&#36895;&#25910;&#25947;&#24615;&#30340;&#20248;&#21183;&#65292;&#21487;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#22810;&#21464;&#37327;&#21709;&#24212;&#21464;&#37327;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2307.03410</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#39640;&#32500;&#22810;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#29992;&#20110;&#29305;&#24449;&#20998;&#24067;&#24335;&#25968;&#25454;&#32763;&#35793;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data. (arXiv:2307.03410v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03410
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#29305;&#24449;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#39640;&#32500;&#22810;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#20855;&#26377;&#36890;&#20449;&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;&#29305;&#24449;&#32500;&#24230;&#21644;&#24555;&#36895;&#25910;&#25947;&#24615;&#30340;&#20248;&#21183;&#65292;&#21487;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#22810;&#21464;&#37327;&#21709;&#24212;&#21464;&#37327;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#20998;&#24067;&#24335;&#25968;&#25454;&#26159;&#25351;&#26681;&#25454;&#29305;&#24449;&#21010;&#20998;&#24182;&#23384;&#20648;&#22312;&#22810;&#20010;&#35745;&#31639;&#33410;&#28857;&#19978;&#30340;&#25968;&#25454;&#65292;&#22312;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#30340;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36825;&#31181;&#25968;&#25454;&#30340;&#20004;&#38454;&#27573;&#25918;&#26494;&#36138;&#23146;&#31639;&#27861; (TSRGA)&#65292;&#29992;&#20110;&#24212;&#29992;&#22810;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#12290;TSRGA &#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#20854;&#36890;&#20449;&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;&#29305;&#24449;&#32500;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#39640;&#24230;&#25193;&#23637;&#21040;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#22810;&#21464;&#37327;&#21709;&#24212;&#21464;&#37327;&#65292;TSRGA &#21487;&#29992;&#20110;&#20135;&#29983;&#20302;&#31209;&#31995;&#25968;&#20272;&#35745;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;TSRGA &#30340;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;TSRGA &#24212;&#29992;&#20110;&#19968;&#31181;&#37329;&#34701;&#24212;&#29992;&#20013;&#65292;&#21033;&#29992;&#26469;&#33258; 10-K &#25253;&#21578;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#20855;&#26377;&#35768;&#22810;&#23494;&#38598;&#22823;&#32500;&#30697;&#38453;&#30340;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature-distributed data, referred to data partitioned by features and stored across multiple computing nodes, are increasingly common in applications with a large number of features. This paper proposes a two-stage relaxed greedy algorithm (TSRGA) for applying multivariate linear regression to such data. The main advantage of TSRGA is that its communication complexity does not depend on the feature dimension, making it highly scalable to very large data sets. In addition, for multivariate response variables, TSRGA can be used to yield low-rank coefficient estimates. The fast convergence of TSRGA is validated by simulation experiments. Finally, we apply the proposed TSRGA in a financial application that leverages unstructured data from the 10-K reports, demonstrating its usefulness in applications with many dense large-dimensional matrices.
&lt;/p&gt;</description></item><item><title>DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.03067</link><description>&lt;p&gt;
DeepOnto: &#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
DeepOnto: A Python Package for Ontology Engineering with Deep Learning. (arXiv:2307.03067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03067
&lt;/p&gt;
&lt;p&gt;
DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#22312;&#26412;&#20307;&#24037;&#31243;&#20013;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;PyTorch&#21644;Tensorflow&#20027;&#35201;&#26159;&#20026;Python&#24320;&#21457;&#30340;&#65292;&#32780;&#24191;&#27867;&#20351;&#29992;&#30340;&#26412;&#20307;API&#65288;&#22914;OWL API&#21644;Jena&#65289;&#20027;&#35201;&#26159;&#22522;&#20110;Java&#30340;&#12290;&#20026;&#20102;&#26041;&#20415;&#26080;&#32541;&#38598;&#25104;&#36825;&#20123;&#26694;&#26550;&#21644;API&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deeponto&#65292;&#19968;&#20010;&#19987;&#20026;&#26412;&#20307;&#24037;&#31243;&#35774;&#35745;&#30340;Python&#21253;&#12290;&#35813;&#21253;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#35748;&#21487;&#21644;&#21487;&#38752;&#30340;OWL API&#30340;&#26680;&#24515;&#26412;&#20307;&#22788;&#29702;&#27169;&#22359;&#65292;&#20197;&#26356;&#8220;Pythonic&#8221;&#30340;&#26041;&#24335;&#23553;&#35013;&#20854;&#22522;&#26412;&#29305;&#24615;&#65292;&#24182;&#25193;&#23637;&#20854;&#21151;&#33021;&#20197;&#21253;&#25324;&#20854;&#20182;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#35821;&#35328;&#21270;&#12289;&#35268;&#33539;&#21270;&#12289;&#25237;&#24433;&#31561;&#12290;&#22522;&#20110;&#36825;&#20010;&#27169;&#22359;&#65292;Deeponto&#25552;&#20379;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#21508;&#31181;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#20363;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying deep learning techniques, particularly language models (LMs), in ontology engineering has raised widespread attention. However, deep learning frameworks like PyTorch and Tensorflow are predominantly developed for Python programming, while widely-used ontology APIs, such as the OWL API and Jena, are primarily Java-based. To facilitate seamless integration of these frameworks and APIs, we present Deeponto, a Python package designed for ontology engineering. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more "Pythonic" manner and extending its capabilities to include other essential components including reasoning, verbalisation, normalisation, projection, and more. Building on this module, Deeponto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25269;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25351;&#26631;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16614</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65306;&#29616;&#23454;&#19990;&#30028;&#20013;&#23450;&#21046;&#40065;&#26834;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Group-based Robustness: A General Framework for Customized Robustness in the Real World. (arXiv:2306.16614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25269;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25351;&#26631;&#30340;&#19981;&#36275;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25351;&#26631;&#33021;&#22815;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36867;&#36991;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#36890;&#36807;&#25200;&#21160;&#27169;&#22411;&#36755;&#20837;&#26469;&#24341;&#36215;&#38169;&#35823;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#24230;&#37327;&#30446;&#26631;&#21644;&#38750;&#30446;&#26631;&#40065;&#26834;&#24615;&#30340;&#25351;&#26631;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#30495;&#23454;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#23427;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#26356;&#36866;&#21512;&#35780;&#20272;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#33021;&#22815;&#22312;&#20256;&#32479;&#30340;&#40065;&#26834;&#24615;&#25351;&#26631;&#19981;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#23545;&#29305;&#23450;&#23041;&#32961;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26377;&#25928;&#20934;&#30830;&#22320;&#34913;&#37327;&#22522;&#20110;&#32676;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss func
&lt;/p&gt;</description></item><item><title>SCENEREPLICA&#26159;&#19968;&#20010;&#22522;&#20110;YCB&#23545;&#35937;&#30340;&#21487;&#37325;&#22797;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#12290;&#27492;&#22522;&#20934;&#27979;&#35797;&#26131;&#20110;&#37325;&#22797;&#24182;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#19981;&#21516;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15620</link><description>&lt;p&gt;
SCENEREPLICA&#65306;&#36890;&#36807;&#21019;&#24314;&#21487;&#37325;&#22797;&#30340;&#22330;&#26223;&#26469;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating Reproducible Scenes. (arXiv:2306.15620v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15620
&lt;/p&gt;
&lt;p&gt;
SCENEREPLICA&#26159;&#19968;&#20010;&#22522;&#20110;YCB&#23545;&#35937;&#30340;&#21487;&#37325;&#22797;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#12290;&#27492;&#22522;&#20934;&#27979;&#35797;&#26131;&#20110;&#37325;&#22797;&#24182;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#19981;&#21516;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#37325;&#22797;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#25235;&#21462;&#21644;&#25918;&#32622;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20351;&#29992;&#20102;YCB&#23545;&#35937;&#65292;&#36825;&#26159;&#26426;&#22120;&#20154;&#23398;&#30028;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#19982;&#20854;&#20182;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#27492;&#22522;&#20934;&#27979;&#35797;&#36824;&#34987;&#35774;&#35745;&#20026;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26131;&#20110;&#37325;&#22797;&#65292;&#20351;&#20854;&#21487;&#20379;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#22522;&#20934;&#27979;&#35797;&#20013;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;6D&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#65292;&#20854;&#20013;&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#31639;&#27861;&#22312;&#29289;&#20307;&#24863;&#30693;&#12289;&#25235;&#21462;&#35268;&#21010;&#21644;&#36816;&#21160;&#35268;&#21010;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#23558;&#25104;&#20026;&#25512;&#21160;&#26426;&#22120;&#20154;&#25805;&#32437;&#39046;&#22495;&#21457;&#23637;&#30340;&#23453;&#36149;&#24037;&#20855;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26356;&#23481;&#26131;&#22320;&#27604;&#36739;&#19981;&#21516;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#20174;&#32780;&#21152;&#24555;&#21457;&#23637;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new reproducible benchmark for evaluating robot manipulation in the real world, specifically focusing on pick-and-place. Our benchmark uses the YCB objects, a commonly used dataset in the robotics community, to ensure that our results are comparable to other studies. Additionally, the benchmark is designed to be easily reproducible in the real world, making it accessible to researchers and practitioners. We also provide our experimental results and analyzes for model-based and model-free 6D robotic grasping on the benchmark, where representative algorithms are evaluated for object perception, grasping planning, and motion planning. We believe that our benchmark will be a valuable tool for advancing the field of robot manipulation. By providing a standardized evaluation framework, researchers can more easily compare different techniques and algorithms, leading to faster progress in developing robot manipulation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#21516;&#30340;&#36755;&#20837;&#22270;&#21644;GNN&#23884;&#20837;&#22823;&#23567;&#19978;&#20351;&#29992;&#20195;&#25968;&#37325;&#32452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#32452;&#21512;&#26469;&#25552;&#39640;GNN&#21152;&#36895;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15155</link><description>&lt;p&gt;
&#38024;&#23545;GNN&#21152;&#36895;&#30340;&#36755;&#20837;&#25935;&#24863;&#30340;&#31264;&#23494;-&#31232;&#30095;&#22522;&#26412;&#32452;&#25104;&#20803;&#32032;
&lt;/p&gt;
&lt;p&gt;
Input-sensitive dense-sparse primitive compositions for GNN acceleration. (arXiv:2306.15155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#21516;&#30340;&#36755;&#20837;&#22270;&#21644;GNN&#23884;&#20837;&#22823;&#23567;&#19978;&#20351;&#29992;&#20195;&#25968;&#37325;&#32452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#32452;&#21512;&#26469;&#25552;&#39640;GNN&#21152;&#36895;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#19968;&#31867;&#37325;&#35201;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#31038;&#20132;&#21644;&#37329;&#34701;&#32593;&#32476;&#20998;&#26512;&#31561;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;GNN&#35745;&#31639;&#30340;&#19981;&#21516;&#38454;&#27573;&#21487;&#20197;&#20351;&#29992;&#31264;&#23494;&#21644;&#31232;&#30095;&#30697;&#38453;&#36816;&#31639;&#26469;&#24314;&#27169;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26694;&#26550;&#21644;&#20248;&#21270;&#25216;&#26415;&#26469;&#21152;&#36895;GNN&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#19981;&#21516;&#31232;&#30095;&#27169;&#24335;&#21644;GNN&#23884;&#20837;&#22823;&#23567;&#30340;&#36755;&#20837;&#22270;&#19978;&#23454;&#29616;&#19968;&#33268;&#39640;&#24615;&#33021;&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;GNN&#35745;&#31639;&#36827;&#34892;&#19981;&#21516;&#30340;&#20195;&#25968;&#37325;&#32452;&#65292;&#23548;&#33268;&#20102;&#26032;&#30340;&#23494;&#38598;&#21644;&#31232;&#30095;&#30697;&#38453;&#22522;&#26412;&#36873;&#25321;&#21644;&#32452;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#32452;&#21512;&#30340;&#30408;&#21033;&#33021;&#21147;&#21462;&#20915;&#20110;&#36755;&#20837;&#22270;&#12289;&#23884;&#20837;&#22823;&#23567;&#21644;&#30446;&#26631;&#30828;&#20214;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;SENSEi&#30340;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#36873;&#25321;&#32473;&#23450;&#36755;&#20837;&#22270;&#21644;GNN&#23884;&#20837;&#22823;&#23567;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#33539;&#22260;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNN) have become an important class of neural network models that have gained popularity in domains such as social and financial network analysis. Different phases of GNN computations can be modeled using both dense and sparse matrix operations. There have been many frameworks and optimization techniques proposed in the literature to accelerate GNNs. However, getting consistently high performance across many input graphs with different sparsity patterns and GNN embedding sizes has remained difficult.  In this paper, we propose different algebraic reassociations of GNN computations that lead to novel dense and sparse matrix primitive selections and compositions. We show that the profitability of these compositions depends on the input graph, embedding size, and the target hardware. We developed SENSEi, a system that uses a data-driven adaptive strategy to select the best composition given the input graph and GNN embedding sizes. Our evaluations on a wide range of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#39640;&#25928;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36816;&#21160;&#26799;&#24230;&#30340;&#20196;&#29260;&#21152;&#26435;&#26041;&#27861;&#65292;&#25972;&#21512;&#25945;&#24072;&#35299;&#30721;&#22120;&#21644;&#23398;&#29983;&#35299;&#30721;&#22120;&#20197;&#21450;&#29983;&#25104;&#21512;&#25104;&#24322;&#24120;&#20107;&#20214;&#65292;&#23454;&#29616;&#20849;&#21516;&#37325;&#26500;&#21407;&#22987;&#24103;&#21644;&#23545;&#24212;&#30340;&#20687;&#32032;&#32423;&#24322;&#24120;&#26144;&#23556;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23454;&#29616;&#20986;&#33394;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.12041</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#26159;&#39640;&#25928;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors. (arXiv:2306.12041v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#39640;&#25928;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36816;&#21160;&#26799;&#24230;&#30340;&#20196;&#29260;&#21152;&#26435;&#26041;&#27861;&#65292;&#25972;&#21512;&#25945;&#24072;&#35299;&#30721;&#22120;&#21644;&#23398;&#29983;&#35299;&#30721;&#22120;&#20197;&#21450;&#29983;&#25104;&#21512;&#25104;&#24322;&#24120;&#20107;&#20214;&#65292;&#23454;&#29616;&#20849;&#21516;&#37325;&#26500;&#21407;&#22987;&#24103;&#21644;&#23545;&#24212;&#30340;&#20687;&#32032;&#32423;&#24322;&#24120;&#26144;&#23556;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23454;&#29616;&#20986;&#33394;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#30340;&#39640;&#25928;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#24103;&#32423;&#21035;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#19977;&#20010;&#21019;&#26032;&#28857;&#65306;&#20854;&#19968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#36816;&#21160;&#26799;&#24230;&#30340;&#20196;&#29260;&#21152;&#26435;&#26041;&#27861;&#65292;&#22240;&#27492;&#36991;&#20813;&#20102;&#23398;&#20064;&#37325;&#26500;&#38745;&#24577;&#32972;&#26223;&#22330;&#26223;&#12290;&#20854;&#20108;&#65292;&#25105;&#20204;&#23558;&#25945;&#24072;&#35299;&#30721;&#22120;&#21644;&#23398;&#29983;&#35299;&#30721;&#22120;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#26550;&#26500;&#20013;&#65292;&#21033;&#29992;&#20004;&#20010;&#35299;&#30721;&#22120;&#32473;&#20986;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25913;&#21892;&#24322;&#24120;&#26816;&#27979;&#12290;&#20854;&#19977;&#65292;&#25105;&#20204;&#29983;&#25104;&#21512;&#25104;&#24322;&#24120;&#20107;&#20214;&#20197;&#22686;&#24378;&#22521;&#35757;&#35270;&#39057;&#65292;&#24182;&#23558;&#36974;&#34109;AE&#27169;&#22411;&#20219;&#21153;&#35774;&#32622;&#20026;&#20849;&#21516;&#37325;&#26500;&#21407;&#22987;&#24103;&#65288;&#26080;&#24322;&#24120;&#65289;&#21644;&#23545;&#24212;&#30340;&#20687;&#32032;&#32423;&#24322;&#24120;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#23548;&#33268;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient abnormal event detection model based on a lightweight masked auto-encoder (AE) applied at the video frame level. The novelty of the proposed model is threefold. First, we introduce an approach to weight tokens based on motion gradients, thus avoiding learning to reconstruct the static background scene. Second, we integrate a teacher decoder and a student decoder into our architecture, leveraging the discrepancy between the outputs given by the two decoders to improve anomaly detection. Third, we generate synthetic abnormal events to augment the training videos, and task the masked AE model to jointly reconstruct the original frames (without anomalies) and the corresponding pixel-level anomaly maps. Our design leads to an efficient and effective model, as demonstrated by the extensive experiments carried out on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. The empirical results show that our model achieves an excellent trade-off between speed and accuracy
&lt;/p&gt;</description></item><item><title>VoxMol&#26159;&#19968;&#31181;&#26681;&#25454;&#20998;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;3D&#20998;&#23376;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20174;&#22122;&#22768;&#20998;&#23376;&#30340;&#24179;&#28369;&#20998;&#24067;&#21040;&#30495;&#23454;&#20998;&#23376;&#30340;&#20998;&#24067;&#30340;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#19982;&#24403;&#21069;&#20808;&#36827;&#25216;&#26415;&#19981;&#21516;&#65292;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#35757;&#32451;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07473</link><description>&lt;p&gt;
&#21435;&#22122;&#22768;&#28857;&#38453;&#26684;&#29983;&#25104;3D&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
3D molecule generation by denoising voxel grids. (arXiv:2306.07473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07473
&lt;/p&gt;
&lt;p&gt;
VoxMol&#26159;&#19968;&#31181;&#26681;&#25454;&#20998;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;3D&#20998;&#23376;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20174;&#22122;&#22768;&#20998;&#23376;&#30340;&#24179;&#28369;&#20998;&#24067;&#21040;&#30495;&#23454;&#20998;&#23376;&#30340;&#20998;&#24067;&#30340;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#19982;&#24403;&#21069;&#20808;&#36827;&#25216;&#26415;&#19981;&#21516;&#65292;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#35757;&#32451;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#34920;&#31034;&#20026;&#35268;&#21017;&#32593;&#26684;&#19978;&#30340;&#21407;&#23376;&#23494;&#24230;&#30340;3D&#20998;&#23376;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#21435;&#22122;&#22768;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23398;&#20064;&#20174;&#22122;&#22768;&#20998;&#23376;&#30340;&#24179;&#28369;&#20998;&#24067;&#21040;&#30495;&#23454;&#20998;&#23376;&#30340;&#20998;&#24067;&#30340;&#26144;&#23556;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36981;&#24490;&#31070;&#32463;&#32463;&#39564;&#36125;&#21494;&#26031;&#26694;&#26550; [Saremi&#21644;Hyvarinen&#65292;2019]&#65292;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#29983;&#25104;&#20998;&#23376;&#65306;&#65288;i&#65289;&#36890;&#36807;&#27424;&#38459;&#23612;Langevin&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#20174;&#24179;&#28369;&#20998;&#24067;&#20013;&#37319;&#26679;&#24102;&#22122;&#22768;&#30340;&#23494;&#24230;&#32593;&#26684;&#65292;&#65288;ii&#65289;&#36890;&#36807;&#21333;&#27493;&#21435;&#22122;&#22122;&#22768;&#26684;&#65292;&#36824;&#21407;&#8220;&#24178;&#20928;&#8221;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;VoxMol&#26159;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#20110;&#24403;&#21069;&#29616;&#26377;&#25216;&#26415;&#65288;&#21363;&#24212;&#29992;&#20110;&#21407;&#23376;&#28857;&#20113;&#30340;&#25193;&#25955;&#27169;&#22411;&#65289;&#30340;&#29983;&#25104;&#20998;&#23376;&#30340;&#26041;&#27861;&#12290;&#23427;&#22312;&#25968;&#25454;&#34920;&#31034;&#12289;&#22122;&#22768;&#27169;&#22411;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#29983;&#25104;&#24314;&#27169;&#31639;&#27861;&#26041;&#38754;&#19981;&#21516;&#12290;VoxMol&#22312;&#26080;&#26465;&#20214;3D&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#19982;&#29616;&#26377;&#25216;&#26415;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#35757;&#32451;&#31616;&#21333;&#19988;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new score-based approach to generate 3D molecules represented as atomic densities on regular grids. First, we train a denoising neural network that learns to map from a smooth distribution of noisy molecules to the distribution of real molecules. Then, we follow the neural empirical Bayes framework [Saremi and Hyvarinen, 2019] and generate molecules in two steps: (i) sample noisy density grids from a smooth distribution via underdamped Langevin Markov chain Monte Carlo, and (ii) recover the ``clean'' molecule by denoising the noisy grid with a single step. Our method, VoxMol, generates molecules in a fundamentally different way than the current state of the art (i.e., diffusion models applied to atom point clouds). It differs in terms of the data representation, the noise model, the network architecture and the generative modeling algorithm. VoxMol achieves comparable results to state of the art on unconditional 3D molecule generation while being simpler to train and faste
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#39044;&#27979;&#38598;&#30340;&#26399;&#26395;&#22823;&#23567;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#37327;&#21270;&#26041;&#27861;&#20197;&#21450;&#28857;&#20272;&#35745;&#21644;&#39640;&#27010;&#29575;&#21306;&#38388;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07254</link><description>&lt;p&gt;
&#20851;&#20110;&#36866;&#24212;&#24615;&#39044;&#27979;&#38598;&#26399;&#26395;&#22823;&#23567;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Expected Size of Conformal Prediction Sets. (arXiv:2306.07254v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07254
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#39044;&#27979;&#38598;&#30340;&#26399;&#26395;&#22823;&#23567;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#37327;&#21270;&#26041;&#27861;&#20197;&#21450;&#28857;&#20272;&#35745;&#21644;&#39640;&#27010;&#29575;&#21306;&#38388;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36866;&#24212;&#24615;&#39044;&#27979;&#22120;&#22312;&#35823;&#24046;&#39057;&#29575;&#26041;&#38754;&#20855;&#26377;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#20294;&#20854;&#39044;&#27979;&#38598;&#22823;&#23567;&#23545;&#20854;&#23454;&#38469;&#25928;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#32570;&#20047;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#21644;&#39044;&#27979;&#38598;&#22823;&#23567;&#30340;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20998;&#35010;&#36866;&#24212;&#24615;&#39044;&#27979;&#26694;&#26550;&#19979;&#29702;&#35770;&#37327;&#21270;&#39044;&#27979;&#38598;&#30340;&#26399;&#26395;&#22823;&#23567;&#12290;&#22240;&#20026;&#36825;&#31181;&#31934;&#30830;&#30340;&#35745;&#31639;&#36890;&#24120;&#26080;&#27861;&#30452;&#25509;&#35745;&#31639;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20986;&#21487;&#36731;&#26494;&#35745;&#31639;&#30340;&#28857;&#20272;&#35745;&#21644;&#39640;&#27010;&#29575;&#21306;&#38388;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#25551;&#36848;&#27979;&#35797;&#21644;&#26657;&#20934;&#25968;&#25454;&#19981;&#21516;&#21487;&#33021;&#23454;&#29616;&#30340;&#26399;&#26395;&#39044;&#27979;&#38598;&#22823;&#23567;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#32467;&#26524;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While conformal predictors reap the benefits of rigorous statistical guarantees for their error frequency, the size of their corresponding prediction sets is critical to their practical utility. Unfortunately, there is currently a lack of finite-sample analysis and guarantees for their prediction set sizes. To address this shortfall, we theoretically quantify the expected size of the prediction set under the split conformal prediction framework. As this precise formulation cannot usually be calculated directly, we further derive point estimates and high probability intervals that can be easily computed, providing a practical method for characterizing the expected prediction set size across different possible realizations of the test and calibration data. Additionally, we corroborate the efficacy of our results with experiments on real-world datasets, for both regression and classification problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#21462;&#21644;&#24674;&#22797;&#28508;&#22312;&#21160;&#21147;&#23398;&#23545;&#40784;&#30340;&#26102;&#31354;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#28508;&#22312;&#21160;&#21147;&#23398;&#26102;&#31354;&#32467;&#26500;&#23548;&#33268;&#23545;&#40784;&#21518;&#24615;&#33021;&#36136;&#37327;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06138</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#21462;&#21644;&#24674;&#22797;&#28508;&#22312;&#21160;&#21147;&#23398;&#23545;&#40784;&#30340;&#26102;&#31354;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model. (arXiv:2306.06138v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#21462;&#21644;&#24674;&#22797;&#28508;&#22312;&#21160;&#21147;&#23398;&#23545;&#40784;&#30340;&#26102;&#31354;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#28508;&#22312;&#21160;&#21147;&#23398;&#26102;&#31354;&#32467;&#26500;&#23548;&#33268;&#23545;&#40784;&#21518;&#24615;&#33021;&#36136;&#37327;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34892;&#20026;&#30456;&#20851;&#30340;&#33041;&#35745;&#31639;&#39046;&#22495;&#65292;&#26377;&#24517;&#35201;&#23558;&#21407;&#22987;&#31070;&#32463;&#32676;&#20307;&#27963;&#21160;&#19982;&#23427;&#20204;&#20043;&#38388;&#30340;&#21095;&#28872;&#21464;&#21270;&#26377;&#24847;&#20041;&#22320;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#31070;&#32463;&#32676;&#20307;&#27963;&#21160;&#37117;&#20197;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#24335;&#20986;&#29616;&#65292;&#22240;&#27492;&#23545;&#40784;&#26159;&#38750;&#24120;&#26840;&#25163;&#30340;&#12290;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#24037;&#20855;&#24615;&#26694;&#26550;&#35748;&#20026;&#65292;&#22522;&#20110;&#35797;&#39564;&#30340;&#31070;&#32463;&#32676;&#20307;&#27963;&#21160;&#20381;&#36182;&#20110;&#20302;&#32500;&#24230;&#28508;&#22312;&#21160;&#21147;&#23398;&#12290;&#20851;&#27880;&#36825;&#31181;&#28508;&#22312;&#21160;&#21147;&#23398;&#22823;&#22823;&#20419;&#36827;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#23613;&#31649;&#25105;&#20204;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#28508;&#22312;&#21160;&#21147;&#23398;&#20013;&#22266;&#26377;&#30340;&#26102;&#31354;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#23545;&#40784;&#21518;&#23548;&#33268;&#21160;&#21147;&#23398;&#32467;&#26500;&#21644;&#25972;&#20307;&#24615;&#33021;&#36136;&#37327;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#39318;&#20808;&#25552;&#21462;&#28304;&#22495;&#30340;&#28508;&#22312;&#21160;&#21147;&#23398;&#32467;&#26500;&#65292;&#28982;&#21518;&#22312;&#23545;&#40784;&#26102;&#36827;&#34892;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of behavior-related brain computation, it is necessary to meaningfully align raw neural population activities against the drastic shift between them. However, the alignment is non-trivial since most neural population activities are in a multivariate time-series manner. An instrumental framework within neuroscience research posits that trial-based neural population activities rely on low-dimensional latent dynamics. Focusing on such latent dynamics greatly facilitates the alignment procedure. Despite the considerable progress we have reached, existing methods usually ignore the intrinsic spatio-temporal structures within latent dynamics. Thus, those solutions lead to poor quality in dynamics structures and overall performance after alignment. To tackle this problem, we propose a method leveraging the expressiveness of diffusion model to relieve such issues. Specifically, the latent dynamics structures of the source domain are first extracted by the diffusion model. Then, su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#23545;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#21387;&#32553;&#65288;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#65289;&#65292;&#23558;&#39044;&#22788;&#29702;&#22120;&#30340;&#23384;&#20648;&#25104;&#26412;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06098</link><description>&lt;p&gt;
&#38169;&#35823;&#21453;&#39304;&#21487;&#20197;&#20934;&#30830;&#22320;&#21387;&#32553;&#39044;&#22788;&#29702;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error Feedback Can Accurately Compress Preconditioners. (arXiv:2306.06098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#23545;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#21387;&#32553;&#65288;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#65289;&#65292;&#23558;&#39044;&#22788;&#29702;&#22120;&#30340;&#23384;&#20648;&#25104;&#26412;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#35268;&#27169;&#30340;&#20108;&#38454;&#20449;&#24687;&#26159;&#25913;&#36827;&#24403;&#21069;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#24615;&#33021;&#30340;&#20027;&#35201;&#36884;&#24452;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31934;&#30830;&#20840;&#30697;&#38453;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#22914;&#20840;&#30697;&#38453;Adagrad&#65288;GGT&#65289;&#25110;&#26080;&#30697;&#38453;&#36817;&#20284;&#26354;&#29575;&#65288;M-FAC&#65289;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#65292;&#20063;&#20250;&#36935;&#21040;&#24040;&#22823;&#30340;&#23384;&#20648;&#25104;&#26412;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#23384;&#20648;&#26799;&#24230;&#30340;&#28369;&#21160;&#31383;&#21475;&#65292;&#20854;&#23384;&#20648;&#38656;&#27714;&#22312;&#27169;&#22411;&#32500;&#24230;&#20013;&#26159;&#25104;&#20493;&#22686;&#21152;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#39640;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#23558;&#39044;&#22788;&#29702;&#22120;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23558;&#26799;&#24230;&#20449;&#24687;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#36890;&#36807;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#21387;&#32553;&#26799;&#24230;&#20449;&#24687;&#65292;&#23558;&#21387;&#32553;&#35823;&#24046;&#21453;&#39304;&#21040;&#26410;&#26469;&#30340;&#36845;&#20195;&#20013;&#12290;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging second-order information at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to medium-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via an efficient and simple-to-implement error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression \emph{before} it is fed into the preconditioner, feeding the compression error back into future iterations. Extensive experiments on deep neural networks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#31216;&#20026;BeMap&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04107</link><description>&lt;p&gt;
BeMap&#65306;&#24179;&#34913;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#29992;&#20110;&#20844;&#24179;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
BeMap: Balanced Message Passing for Fair Graph Neural Network. (arXiv:2306.04107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#31216;&#20026;BeMap&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36890;&#36807;&#36845;&#20195;&#22320;&#32858;&#21512;&#27599;&#20010;&#33410;&#28857;&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#26469;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#21363;&#28040;&#24687;&#20256;&#36882;&#12290;&#28982;&#32780;&#65292;&#20855;&#20307;&#35777;&#25454;&#26174;&#31034;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23545;&#26576;&#20123;&#20154;&#21475;&#32676;&#20307;&#23384;&#22312;&#20559;&#35265;&#65292;&#36825;&#35201;&#27714;&#32771;&#34385;&#31639;&#27861;&#30340;&#20844;&#27491;&#24615;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#21162;&#21147;&#22312;&#20445;&#35777;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#35757;&#32451;&#26399;&#38388;&#24448;&#24448;&#24182;&#19981;&#26126;&#30830;&#32771;&#34385;&#28040;&#24687;&#20256;&#36882;&#22312;GNN&#20013;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#35777;&#25454;&#21644;&#29702;&#35770;&#35777;&#26126;&#65292;&#24403;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#19981;&#24179;&#34913;&#26102;&#65292;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#20250;&#25918;&#22823;&#20559;&#24046;&#12290;&#22312;&#36825;&#20123;&#20998;&#26512;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BeMap&#65292;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#27599;&#20010;&#33410;&#28857;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) has shown strong empirical performance in many downstream tasks by iteratively aggregating information from the local neighborhood of each node, i.e., message passing. However, concrete evidence has revealed that a graph neural network could be biased against certain demographic groups, which calls for the consideration of algorithmic fairness. Despite the increasing efforts in ensuring algorithmic fairness on graph neural networks, they often do not explicitly consider the induced bias caused by message passing in GNN during training. In this paper, we first investigate the problem of bias amplification in message passing. We empirically and theoretically demonstrate that message passing could amplify the bias when the 1-hop neighbors from different demographic groups are unbalanced. Guided by such analyses, we propose BeMap, a fair message passing method, that leverages a balance-aware sampling strategy to balance the number of the 1-hop neighbors of each n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#20803;&#28608;&#27963;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#35206;&#30422;&#24230;&#65288;NAC&#65289;&#20316;&#20026;&#34913;&#37327;&#31070;&#32463;&#20803;&#34892;&#20026;&#30340;&#25351;&#26631;&#12290;&#21033;&#29992;NAC&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#22495;&#20869;&#21644;&#31163;&#22495;&#36755;&#20837;&#65292;&#31616;&#21270;&#31163;&#22495;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;NAC&#19982;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.02879</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#28608;&#27963;&#35206;&#30422;&#24230;&#65306;&#37325;&#26032;&#24605;&#32771;&#31163;&#22495;&#26816;&#27979;&#21644;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization. (arXiv:2306.02879v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#20803;&#28608;&#27963;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#35206;&#30422;&#24230;&#65288;NAC&#65289;&#20316;&#20026;&#34913;&#37327;&#31070;&#32463;&#20803;&#34892;&#20026;&#30340;&#25351;&#26631;&#12290;&#21033;&#29992;NAC&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#22495;&#20869;&#21644;&#31163;&#22495;&#36755;&#20837;&#65292;&#31616;&#21270;&#31163;&#22495;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;NAC&#19982;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#22495;&#38382;&#39064;&#36890;&#24120;&#22312;&#31070;&#32463;&#32593;&#32476;&#36935;&#21040;&#26126;&#26174;&#20559;&#31163;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#25968;&#25454;&#26102;&#20986;&#29616;&#65292;&#21363;&#22312;&#22495;&#20869;&#25968;&#25454;&#65288;InD&#65289;&#20043;&#22806;&#12290;&#26412;&#25991;&#20174;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#31163;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32771;&#34385;&#31070;&#32463;&#20803;&#30340;&#36755;&#20986;&#21644;&#20854;&#23545;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#26469;&#23450;&#20041;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#29366;&#24577;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#25551;&#36848;&#31070;&#32463;&#20803;&#19982;&#31163;&#22495;&#38382;&#39064;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#35206;&#30422;&#24230;&#65288;NAC&#65289;&#8212;&#8212;&#19968;&#31181;&#34913;&#37327;&#31070;&#32463;&#20803;&#22312;&#22495;&#20869;&#25968;&#25454;&#19979;&#34892;&#20026;&#30340;&#31616;&#21333;&#24230;&#37327;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;NAC&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;1&#65289;&#22522;&#20110;&#31070;&#32463;&#20803;&#34892;&#20026;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#22495;&#20869;&#21644;&#31163;&#22495;&#36755;&#20837;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#31163;&#22495;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#65288;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet-1K&#65289;&#19978;&#36229;&#36807;&#20102;21&#20010;&#20808;&#21069;&#30340;&#26041;&#27861;&#65307;2&#65289;NAC&#19982;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#20851;&#31995;&#65292;&#36825;&#31181;&#20851;&#31995;&#22312;&#19981;&#21516;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#19968;&#33268;&#25104;&#31435;&#65292;&#20351;&#24471;&#22522;&#20110;NAC&#30340;&#20934;&#21017;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The out-of-distribution (OOD) problem generally arises when neural networks encounter data that significantly deviates from the training data distribution, i.e., in-distribution (InD). In this paper, we study the OOD problem from a neuron activation view. We first formulate neuron activation states by considering both the neuron output and its influence on model decisions. Then, to characterize the relationship between neurons and OOD issues, we introduce the \textit{neuron activation coverage} (NAC) -- a simple measure for neuron behaviors under InD data. Leveraging our NAC, we show that 1) InD and OOD inputs can be largely separated based on the neuron behavior, which significantly eases the OOD detection problem and beats the 21 previous methods over three benchmarks (CIFAR-10, CIFAR-100, and ImageNet-1K). 2) a positive correlation between NAC and model generalization ability consistently holds across architectures and datasets, which enables a NAC-based criterion for evaluating mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00010</link><description>&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explainability in Simplicial Map Neural Networks. (arXiv:2306.00010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#26159;&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#21644;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#20013;&#24212;&#29992; SMNN &#23384;&#22312;&#19968;&#20123;&#29942;&#39048;&#65292;&#39318;&#20808;&#27809;&#26377;&#23450;&#20041; SMNN &#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#27425;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#38598;&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#21253;&#22260;&#20984;&#22810;&#38754;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#23376;&#38598;&#21644;&#25237;&#24433;&#21040;&#36229;&#29699;&#38754;&#30340;&#26041;&#27861;&#20316;&#20026;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340; SMNN &#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplicial map neural networks (SMNNs) are topology-based neural networks with interesting properties such as universal approximation capability and robustness to adversarial examples under appropriate conditions. However, SMNNs present some bottlenecks for their possible application in high dimensions. First, no SMNN training process has been defined so far. Second, SMNNs require the construction of a convex polytope surrounding the input dataset. In this paper, we propose a SMNN training procedure based on a support subset of the given dataset and a method based on projection to a hypersphere as a replacement for the convex polytope construction. In addition, the explainability capacity of SMNNs is also introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#37327;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#20998;&#31867;&#26041;&#27861;&#65288;PEC&#65289;&#12290;&#23545;PEC&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;PEC&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#20248;&#21183;&#65292;&#20363;&#22914;&#26679;&#26412;&#25928;&#29575;&#39640;&#12289;&#26131;&#20110;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2305.18806</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#22686;&#37327;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prediction Error-based Classification for Class-Incremental Learning. (arXiv:2305.18806v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#37327;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#20998;&#31867;&#26041;&#27861;&#65288;PEC&#65289;&#12290;&#23545;PEC&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;PEC&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#65292;&#24182;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#20248;&#21183;&#65292;&#20363;&#22914;&#26679;&#26412;&#25928;&#29575;&#39640;&#12289;&#26131;&#20110;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#20998;&#31867;&#26159;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#26469;&#21306;&#20998;&#25152;&#26377;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#37327;&#20998;&#31867;&#26102;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#36951;&#24536;&#21644;&#20998;&#25968;&#19981;&#22343;&#34913;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;&#39044;&#27979;&#35823;&#24046;&#20998;&#31867;&#65288;PEC&#65289;&#65292;&#23427;&#19982;&#20256;&#32479;&#30340;&#21028;&#21035;&#21644;&#29983;&#25104;&#20998;&#31867;&#33539;&#24335;&#26377;&#25152;&#19981;&#21516;&#12290;PEC&#36890;&#36807;&#27979;&#37327;&#27169;&#22411;&#22312;&#20174;&#35813;&#31867;&#21035;&#20013;&#23398;&#20064;&#30340;&#25968;&#25454;&#19978;&#22797;&#21046;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#39044;&#27979;&#35823;&#24046;&#26469;&#35745;&#31639;&#31867;&#21035;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#20026;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#26041;&#24046;&#30340;&#20998;&#31867;&#35268;&#21017;&#30340;&#36817;&#20284;&#12290;PEC&#20855;&#26377;&#20960;&#20010;&#23454;&#38469;&#20248;&#21183;&#65292;&#21253;&#25324;&#26679;&#26412;&#25928;&#29575;&#39640;&#12289;&#26131;&#20110;&#35843;&#25972;&#20197;&#21450;&#21363;&#20351;&#22312;&#36880;&#20010;&#21576;&#29616;&#25968;&#25454;&#26102;&#20063;&#24456;&#26377;&#25928;&#12290;&#26412;&#25991;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;PEC&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning (CIL) is a particularly challenging variant of continual learning, where the goal is to learn to discriminate between all classes presented in an incremental fashion. Existing approaches often suffer from excessive forgetting and imbalance of the scores assigned to classes that have not been seen together during training. In this study, we introduce a novel approach, Prediction Error-based Classification (PEC), which differs from traditional discriminative and generative classification paradigms. PEC computes a class score by measuring the prediction error of a model trained to replicate the outputs of a frozen random neural network on data from that class. The method can be interpreted as approximating a classification rule based on Gaussian Process posterior variance. PEC offers several practical advantages, including sample efficiency, ease of tuning, and effectiveness even when data are presented one class at a time. Our empirical results show that PEC pe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18766</link><description>&lt;p&gt;
HiFA: &#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#21450;&#20854;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance. (arXiv:2305.18766v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;3D&#27169;&#22411;&#65292;&#33258;&#21160;&#25991;&#26412;&#21040;3D&#21512;&#25104;&#22312;&#25552;&#21319;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#25552;&#20379;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#30340;2D&#28210;&#26579;&#24471;&#20998;&#24182;&#29992;&#20110;&#20248;&#21270;NeRFs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#23545;3D&#20960;&#20309;&#30340;&#26377;&#38480;&#29702;&#35299;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#36935;&#21040;&#22810;&#20010;&#35270;&#35282;&#19978;&#30340;&#20266;&#24433;&#21644;&#19981;&#19968;&#33268;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#20808;&#39564;&#37325;&#26032;&#21046;&#23450;&#20248;&#21270;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#37322;&#25918;&#20102;&#25193;&#25955;&#20808;&#39564;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#65292;&#25105;&#20204;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#65292;&#24182;&#35268;&#33539;&#21270;NeRF&#30340;&#23494;&#24230;&#22330;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic text-to-3D synthesis has achieved remarkable advancements through the optimization of 3D models. Existing methods commonly rely on pre-trained text-to-image generative models, such as diffusion models, providing scores for 2D renderings of Neural Radiance Fields (NeRFs) and being utilized for optimizing NeRFs. However, these methods often encounter artifacts and inconsistencies across multiple views due to their limited understanding of 3D geometry. To address these limitations, we propose a reformulation of the optimization loss using the diffusion prior. Furthermore, we introduce a novel training approach that unlocks the potential of the diffusion prior. To improve 3D geometry representation, we apply auxiliary depth supervision for NeRF-rendered images and regularize the density field of NeRFs. Extensive experiments demonstrate the superiority of our method over prior works, resulting in advanced photo-realism and improved multi-view consistency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#30103;&#39046;&#22495;&#8220;&#20146;&#27835;&#30103;&#8221;&#25805;&#20316;&#30340;&#38480;&#21046;&#65292;&#19988;&#32771;&#34385;&#21040;&#20102;&#25805;&#20316;&#39044;&#31639;&#30340;&#20855;&#26377;&#20449;&#24687;&#39044;&#31639;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#23558;&#22312;&#32447;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#21644;&#24773;&#22659;&#36172;&#21338;&#26426;&#23398;&#20064;&#31639;&#27861;&#26377;&#26426;&#22320;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.18511</link><description>&lt;p&gt;
&#20855;&#26377;&#20449;&#24687;&#39044;&#31639;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contextual Bandits with Budgeted Information Reveal. (arXiv:2305.18511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#30103;&#39046;&#22495;&#8220;&#20146;&#27835;&#30103;&#8221;&#25805;&#20316;&#30340;&#38480;&#21046;&#65292;&#19988;&#32771;&#34385;&#21040;&#20102;&#25805;&#20316;&#39044;&#31639;&#30340;&#20855;&#26377;&#20449;&#24687;&#39044;&#31639;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#23558;&#22312;&#32447;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#21644;&#24773;&#22659;&#36172;&#21338;&#26426;&#23398;&#20064;&#31639;&#27861;&#26377;&#26426;&#22320;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;&#24120;&#29992;&#20110;&#25512;&#33616;&#20010;&#24615;&#21270;&#30340;&#21307;&#30103;&#22788;&#29702;&#26041;&#24335;&#65292;&#20294;&#22312;&#23454;&#38469;&#25805;&#20316;&#20013;&#65292;&#20026;&#20445;&#35777;&#27835;&#30103;&#25928;&#26524;&#65292;&#21307;&#29983;&#36890;&#24120;&#38656;&#35201;&#35201;&#27714;&#24739;&#32773;&#37319;&#21462;&#27809;&#26377;&#30452;&#25509;&#22909;&#22788;&#30340;&#8220;&#20146;&#27835;&#30103;&#8221;&#25805;&#20316;&#65292;&#32780;&#19988;&#20020;&#24202;&#21307;&#29983;&#30340;&#25805;&#20316;&#39044;&#31639;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26377;&#25928;&#32467;&#21512;&#20102;&#20004;&#31181;&#31639;&#27861;&#26041;&#27861;&#20043;&#38271;&#65306;1&#65289;&#19968;&#20010;&#20915;&#23450;&#26368;&#20339;&#26102;&#26426;&#19982;&#24739;&#32773;&#32852;&#31995;&#30340;&#22312;&#32447;&#21407;&#22987;-&#23545;&#20598;&#65288;primal-dual&#65289;&#31639;&#27861;&#65292;2&#65289;&#29992;&#20110;&#21521;&#24739;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#20855;&#26377;&#20122;&#32447;&#24615;&#30340;&#22238;&#24402;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual bandit algorithms are commonly used in digital health to recommend personalized treatments. However, to ensure the effectiveness of the treatments, patients are often requested to take actions that have no immediate benefit to them, which we refer to as pro-treatment actions. In practice, clinicians have a limited budget to encourage patients to take these actions and collect additional information. We introduce a novel optimization and learning algorithm to address this problem. This algorithm effectively combines the strengths of two algorithmic approaches in a seamless manner, including 1) an online primal-dual algorithm for deciding the optimal timing to reach out to patients, and 2) a contextual bandit learning algorithm to deliver personalized treatment to the patient. We prove that this algorithm admits a sub-linear regret bound. We illustrate the usefulness of this algorithm on both synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212; Sharpened Lazy Incremental Quasi-Newton (SLIQN) &#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#26174;&#24335;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#21644;$O(d^2)$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17283</link><description>&lt;p&gt;
&#20248;&#21270;&#36864;&#28779;&#31639;&#27861;&#30340;&#35823;&#24046;&#30028;&#21644;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Sharpened Lazy Incremental Quasi-Newton Method. (arXiv:2305.17283v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212; Sharpened Lazy Incremental Quasi-Newton (SLIQN) &#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#26174;&#24335;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#21644;$O(d^2)$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20855;&#26377;$Lipschitz$&#36830;&#32493;Hessian&#30697;&#38453;&#22312;$d$&#32500;&#31354;&#38388;&#20013;&#65292;$n$&#20010;&#24378;&#20984;&#20809;&#28369;&#20989;&#25968;&#30340;&#26377;&#38480;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;$n$&#30340;&#25968;&#37327;&#24456;&#22823;&#65292;&#22240;&#27492;&#24517;&#39035;&#20351;&#29992;&#27599;&#36845;&#20195;&#19968;&#27425;&#19982;$n$&#26080;&#20851;&#30340;&#22686;&#37327;&#24335;&#25110;&#38543;&#26426;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212; Sharpened Lazy Incremental Quasi-Newton (SLIQN) &#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#26174;&#24335;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#21644;$O(d^2)$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the finite sum minimization of $n$ strongly convex and smooth functions with Lipschitz continuous Hessians in $d$ dimensions. In many applications where such problems arise, including maximum likelihood estimation, empirical risk minimization, and unsupervised learning, the number of observations $n$ is large, and it becomes necessary to use incremental or stochastic algorithms whose per-iteration complexity is independent of $n$. Of these, the incremental/stochastic variants of the Newton method exhibit superlinear convergence, but incur a per-iteration complexity of $O(d^3)$, which may be prohibitive in large-scale settings. On the other hand, the incremental Quasi-Newton method incurs a per-iteration complexity of $O(d^2)$ but its superlinear convergence rate has only been characterized asymptotically. This work puts forth the Sharpened Lazy Incremental Quasi-Newton (SLIQN) method that achieves the best of both worlds: an explicit superlinear convergence rate with a per-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#26694;&#26550;&#65292;&#21363;LLMs&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;&#65288;LATM&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#33258;&#20027;&#22320;&#21019;&#24314;&#29992;&#20110;&#35299;&#20915;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#22806;&#37096;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.17126</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Tool Makers. (arXiv:2305.17126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#26694;&#26550;&#65292;&#21363;LLMs&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;&#65288;LATM&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#33258;&#20027;&#22320;&#21019;&#24314;&#29992;&#20110;&#35299;&#20915;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#22806;&#37096;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22686;&#24378;&#20854;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#26041;&#38754;&#30340;&#20808;&#21069;&#24037;&#20316;&#20381;&#36182;&#20110;&#29616;&#26377;&#24037;&#20855;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#26694;&#26550;&#65292;&#31216;&#20026;LLMs As Tool Makers&#65288;LATM&#65289;&#65292;&#20197;&#28040;&#38500;&#36825;&#31181;&#20381;&#36182;&#24615;&#65292;&#20854;&#20013;LLMs&#21019;&#24314;&#33258;&#24049;&#30340;&#21487;&#37325;&#29992;&#24037;&#20855;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#38454;&#27573;&#65306;1&#65289;&#21046;&#36896;&#24037;&#20855;&#65306;LLM&#20316;&#20026;&#24037;&#20855;&#21046;&#36896;&#32773;&#65292;&#20026;&#32473;&#23450;&#20219;&#21153;&#21046;&#20316;&#24037;&#20855;&#65292;&#20854;&#20013;&#24037;&#20855;&#20316;&#20026;Python&#23454;&#29992;&#20989;&#25968;&#23454;&#29616;&#12290;2&#65289;&#20351;&#29992;&#24037;&#20855;&#65306;LLM&#20316;&#20026;&#24037;&#20855;&#29992;&#25143;&#65292;&#24212;&#29992;&#24037;&#20855;&#21046;&#36896;&#32773;&#26500;&#24314;&#30340;&#24037;&#20855;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#24037;&#20855;&#29992;&#25143;&#21487;&#20197;&#26159;&#19982;&#24037;&#20855;&#21046;&#36896;&#32773;&#30456;&#21516;&#25110;&#19981;&#21516;&#30340;LLM&#12290;&#24037;&#20855;&#21046;&#36896;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#29983;&#25104;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#35831;&#27714;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#23558;&#26469;&#35831;&#27714;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#33021;&#35843;&#29992;&#30456;&#24212;&#30340;API&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research shows the potential of enhancing the problem-solving ability of large language models (LLMs) through the use of external tools. However, prior work along this line depends on the availability of existing tools. In this work, we take an initial step towards removing this dependency by proposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two key phases: 1) tool making: an LLM acts as the tool maker that crafts tools for given tasks, where a tool is implemented as a Python utility function. 2) tool using: an LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. The tool user can be either the same or a different LLM from the tool maker. Tool-making enables an LLM to continually generate tools that can be applied to different requests so that future requests can call the corresponding APIs when beneficial for solving the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36890;&#36807;&#21152;&#26435;&#34928;&#20943;&#35757;&#32451;&#30340;&#22810;&#36755;&#20986;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20989;&#25968;&#31867;&#22411;&#21644;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.16534</link><description>&lt;p&gt;
&#21521;&#37327;&#20540;&#21464;&#20998;&#31354;&#38388;&#21644;DNN&#30340;&#23485;&#24230;&#30028;&#65306;&#20851;&#20110;&#26435;&#37325;&#34928;&#20943;&#27491;&#21017;&#21270;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector-Valued Variation Spaces and Width Bounds for DNNs: Insights on Weight Decay Regularization. (arXiv:2305.16534v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36890;&#36807;&#21152;&#26435;&#34928;&#20943;&#35757;&#32451;&#30340;&#22810;&#36755;&#20986;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20989;&#25968;&#31867;&#22411;&#21644;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26368;&#23567;&#21270;&#25439;&#22833;&#39033;&#21644;&#24179;&#26041;&#26435;&#37325;&#21644;&#30456;&#24212;&#65292;&#23545;&#24212;&#20110;&#35757;&#32451;&#21152;&#26435;&#34928;&#20943;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#36825;&#31181;&#24120;&#35265;&#23398;&#20064;&#26694;&#26550;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#35757;&#32451;&#21152;&#26435;&#34928;&#20943;&#20197;&#33719;&#24471;&#22810;&#36755;&#20986;(&#21521;&#37327;&#20540;)ReLU&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#20989;&#25968;&#31867;&#22411;&#12290;&#36825;&#25193;&#23637;&#20102;&#20808;&#21069;&#38480;&#20110;&#21333;&#36755;&#20986;(&#26631;&#37327;&#20540;)&#32593;&#32476;&#30340;&#34920;&#24449;&#12290;&#36825;&#31181;&#34920;&#24449;&#38656;&#35201;&#23450;&#20041;&#25105;&#20204;&#31216;&#20043;&#20026;&#21521;&#37327;&#20540;&#21464;&#20998;(VV)&#31354;&#38388;&#30340;&#26032;&#31867;&#31070;&#32463;&#20989;&#25968;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#34920;&#24449;&#23450;&#29702;&#35777;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#26159;&#36890;&#36807;VV&#31354;&#38388;&#20013;&#25552;&#20986;&#23398;&#20064;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#36825;&#20010;&#26032;&#30340;&#34920;&#24449;&#23450;&#29702;&#34920;&#26126;&#65292;&#36825;&#20123;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#23384;&#22312;&#20110;&#23485;&#24230;&#21463;&#35757;&#32451;&#25968;&#25454;&#25968;&#38480;&#21046;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25509;&#19979;&#26469;&#65292;&#36890;&#36807;&#19982;&#22810;&#20219;&#21153;lasso&#38382;&#39064;&#30340;&#26032;&#32852;&#31995;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) trained to minimize a loss term plus the sum of squared weights via gradient descent corresponds to the common approach of training with weight decay. This paper provides new insights into this common learning framework. We characterize the kinds of functions learned by training with weight decay for multi-output (vector-valued) ReLU neural networks. This extends previous characterizations that were limited to single-output (scalar-valued) networks. This characterization requires the definition of a new class of neural function spaces that we call vector-valued variation (VV) spaces. We prove that neural networks (NNs) are optimal solutions to learning problems posed over VV spaces via a novel representer theorem. This new representer theorem shows that solutions to these learning problems exist as vector-valued neural networks with widths bounded in terms of the number of training data. Next, via a novel connection to the multi-task lasso problem, we derive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20195;&#34920;&#19981;&#36275;&#30340;&#20122;&#32676;&#20307;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16363</link><description>&lt;p&gt;
&#22686;&#21152;&#20122;&#32676;&#27169;&#22411;&#24615;&#33021;&#30340;&#38598;&#25104;&#21512;&#25104;&#30005;&#23376;&#30149;&#21382;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Ensemble Synthetic EHR Generation for Increasing Subpopulation Model's Performance. (arXiv:2305.16363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20195;&#34920;&#19981;&#36275;&#30340;&#20122;&#32676;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#36890;&#24120;&#21253;&#21547;&#26576;&#20123;&#20122;&#32676;&#20307;&#65288;SP&#65289;&#30340;&#19981;&#21516;&#27604;&#20363;&#34920;&#31034;&#12290;&#24739;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#20020;&#24202;&#24773;&#20917;&#30340;&#27969;&#34892;&#31243;&#24230;&#21644;&#21307;&#30103;&#20013;&#24515;&#31867;&#22411;&#31561;&#22240;&#32032;&#23548;&#33268;&#36825;&#31181;&#19981;&#20805;&#20998;&#30340;&#20195;&#34920;&#24615;&#12290;&#22240;&#27492;&#65292;&#24403;&#22312;&#36825;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#27169;&#22411;&#24456;&#38590;&#36827;&#34892;&#33391;&#22909;&#30340;&#27010;&#25324;&#24182;&#22312;&#20195;&#34920;&#19981;&#36275;&#30340;SP&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22411;&#38598;&#25104;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;SP&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;GAN&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#24182;&#23558;&#21512;&#25104;&#26679;&#26412;&#32435;&#20837;&#27599;&#20010;SP&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#35757;&#32451;SP&#29305;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#20026;&#20102;&#27491;&#30830;&#35780;&#20272;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#27969;&#31243;&#65292;&#24182;&#20351;&#29992;&#20174;MIMIC&#25968;&#25454;&#24211;&#26597;&#35810;&#30340;&#20004;&#20010;&#30495;&#23454;&#29992;&#20363;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#22312;&#20195;&#34920;&#19981;&#36275;&#30340;SP&#19978;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#23558;&#20316;&#20026;&#34917;&#20805;&#26448;&#26009;&#25552;&#20379;&#65292;&#24182;&#23558;&#22312;&#20844;&#20849;&#23384;&#20648;&#24211;&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHR) often contain different rates of representation of certain subpopulations (SP). Factors like patient demographics, clinical condition prevalence, and medical center type contribute to this underrepresentation. Consequently, when training machine learning models on such datasets, the models struggle to generalize well and perform poorly on underrepresented SPs. To address this issue, we propose a novel ensemble framework that utilizes generative models. Specifically, we train a GAN-based synthetic data generator for each SP and incorporate synthetic samples into each SP training set. Ultimately, we train SP-specific prediction models. To properly evaluate this method, we design an evaluation pipeline with 2 real-world use case datasets, queried from the MIMIC database. Our approach shows increased model performance over underrepresented SPs. Our code and models are given as supplementary and will be made available on a public repository.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110; MMD &#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#65292;&#20811;&#26381;&#20102;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.15901</link><description>&lt;p&gt;
&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#30340;&#32463;&#39564;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Empirical Optimal Transport between Conditional Distributions. (arXiv:2305.15901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110; MMD &#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#65292;&#20811;&#26381;&#20102;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#20004;&#20010;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#20276;&#38543;&#26465;&#20214;&#20540;&#30340;&#36755;&#36816;&#25104;&#26412;&#65288;Wasserstein &#36317;&#31163;&#65289;&#65292;&#20197;&#21450;&#26465;&#20214;&#20998;&#24067;&#38388;&#30340;&#36755;&#36816;&#35745;&#21010;&#12290;&#30001;&#20110;&#21305;&#37197;&#26465;&#20214;&#20998;&#24067;&#26159;&#30417;&#30563;&#35757;&#32451;&#21028;&#21035;&#27169;&#22411;&#21644;&#65288;&#38544;&#24335;&#65289;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#24515;&#65292;&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#20855;&#26377;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#34987;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#38544;&#24335;&#29305;&#23450;&#20110;&#32852;&#21512;&#65288;&#26679;&#26412;&#65289;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#22240;&#27492;&#21046;&#23450;&#36825;&#20010;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#65288;i&#65289;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#65288;ii&#65289;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#29305;&#23450;&#30340;&#22522;&#20110; MMD&#65288;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65289;&#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given samples from two joint distributions, we consider the problem of Optimal Transportation (OT) between the corresponding distributions conditioned on a common variable. The objective of this work is to estimate the associated transport cost (Wasserstein distance) as well as the transport plan between the conditionals as a function of the conditioned value. Since matching conditional distributions is at the core of supervised training of discriminative models and (implicit) conditional-generative models, OT between conditionals has the potential to be employed in diverse machine learning applications. However, since the conditionals involved in OT are implicitly specified via the joint samples, it is challenging to formulate this problem, especially when (i) the variable conditioned on is continuous and (ii) the marginal of this variable in the two distributions is different. We overcome these challenges by employing a specific kernel MMD (Maximum Mean Discrepancy) based regularizer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31574;&#30053;&#30340;&#34892;&#20026;&#21644;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#32479;&#35745;&#25968;&#25454;&#34920;&#26126;&#65292;&#21333;&#20010;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#30340;&#26412;&#22320;&#35299;&#27604;&#20840;&#23616;&#26041;&#27861;&#24674;&#22797;&#30340;&#39044;&#26399;&#20540;&#26356;&#22909;&#12290;M&#252;ller&#31561;&#20154;&#25552;&#20986;&#30340;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#22312;&#26377;&#22122;&#38899;&#21644;&#26080;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#37117;&#26377;&#25512;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.15572</link><description>&lt;p&gt;
&#26412;&#22320;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#34892;&#20026;&#21644;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Behavior and Convergence of Local Bayesian Optimization. (arXiv:2305.15572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31574;&#30053;&#30340;&#34892;&#20026;&#21644;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#32479;&#35745;&#25968;&#25454;&#34920;&#26126;&#65292;&#21333;&#20010;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#30340;&#26412;&#22320;&#35299;&#27604;&#20840;&#23616;&#26041;&#27861;&#24674;&#22797;&#30340;&#39044;&#26399;&#20540;&#26356;&#22909;&#12290;M&#252;ller&#31561;&#20154;&#25552;&#20986;&#30340;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#22312;&#26377;&#22122;&#38899;&#21644;&#26080;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#37117;&#26377;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#19968;&#39033;&#26368;&#26032;&#30340;&#21457;&#23637;&#26159;&#20351;&#29992;&#26412;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#19982;&#20256;&#32479;&#30340;&#20840;&#23616;&#31574;&#30053;&#30456;&#27604;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#25552;&#20379;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#25991;&#29486;&#20013;&#30340;&#8220;&#20256;&#32479;&#26234;&#24935;&#8221;&#26159;&#65292;&#19987;&#27880;&#20110;&#26412;&#22320;&#20248;&#21270;&#35268;&#36991;&#20102;&#32500;&#24230;&#35781;&#21650;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#20363;&#31243;&#30340;&#39044;&#26399;&#34892;&#20026;&#25110;&#25910;&#25947;&#24615;&#20102;&#35299;&#29978;&#23569;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#26412;&#22320;&#26041;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#21333;&#20010;&#26412;&#22320;&#35299;&#30340;&#32479;&#35745;&#25968;&#25454;&#19982;&#20174;&#20840;&#23616;&#26041;&#27861;&#24674;&#22797;&#30340;&#39044;&#26399;&#20540;&#30456;&#27604;&#38750;&#24120;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#30001;M&#252;ller&#31561;&#20154;&#25552;&#20986;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31639;&#27861;&#30340;&#31532;&#19968;&#27425;&#20005;&#26684;&#20998;&#26512;&#65292;&#24182;&#22312;&#26377;&#22122;&#38899;&#21644;&#26080;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent development in Bayesian optimization is the use of local optimization strategies, which can deliver strong empirical performance on high-dimensional problems compared to traditional global strategies. The "folk wisdom" in the literature is that the focus on local optimization sidesteps the curse of dimensionality; however, little is known concretely about the expected behavior or convergence of Bayesian local optimization routines. We first study the behavior of the local approach, and find that the statistics of individual local solutions of Gaussian process sample paths are surprisingly good compared to what we would expect to recover from global methods. We then present the first rigorous analysis of such a Bayesian local optimization algorithm recently proposed by M\"uller et al. (2021), and derive convergence rates in both the noisy and noiseless settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#31639;&#23376;Newton&#26041;&#27861;&#65292;&#21487;&#20197;&#36845;&#20195;&#26500;&#36896;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#21407;&#27010;&#29575;&#27979;&#24230;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#28385;&#36275;&#30446;&#26631;&#20998;&#25968;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09792</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#31639;&#23376; Newton &#26041;&#27861;&#29992;&#20110;&#27979;&#37327;&#36816;&#36755;
&lt;/p&gt;
&lt;p&gt;
A score-based operator Newton method for measure transport. (arXiv:2305.09792v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#31639;&#23376;Newton&#26041;&#27861;&#65292;&#21487;&#20197;&#36845;&#20195;&#26500;&#36896;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#21407;&#27010;&#29575;&#27979;&#24230;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#28385;&#36275;&#30446;&#26631;&#20998;&#25968;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#27979;&#24230;&#30340;&#36816;&#36755;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#35768;&#22810;&#26680;&#24515;&#20219;&#21153;&#30340;&#22522;&#30784;&#65292;&#20174;&#21464;&#20998;&#25512;&#29702;&#21040;&#29983;&#25104;&#24314;&#27169;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#30446;&#26631;&#26159;&#23558;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#27010;&#29575;&#27979;&#24230;&#34920;&#31034;&#20026;&#36890;&#36807;&#23398;&#20064;&#30340;&#26144;&#23556;&#23558;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#21407;&#27010;&#29575;&#27979;&#24230;&#25512;&#21521;&#21069;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#36825;&#26679;&#19968;&#20010;&#36816;&#36755;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#32473;&#20986;&#20102;&#35780;&#20272;&#30446;&#26631;&#20998;&#24067;&#20998;&#25968;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35813;&#26144;&#23556;&#29305;&#24449;&#21270;&#20026;&#19968;&#20010;&#26080;&#31351;&#32500;&#30340;&#20998;&#25968;&#27531;&#24046;&#31639;&#23376;&#30340;&#38646;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31181;&#36845;&#20195;&#26500;&#36896;&#36825;&#26679;&#19968;&#20010;&#38646;&#30340;&#29275;&#39039;&#31867;&#22411;&#26041;&#27861;&#12290;&#36890;&#36807;&#35843;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32463;&#20856;&#26925;&#22278;&#27491;&#21017;&#24615;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#36845;&#20195;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#22312;&#30446;&#26631;&#20998;&#25968;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#36825;&#31181;&#26500;&#36896;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#20803;&#32032;&#26159;&#23558;&#22522;&#26412;&#30340;&#29275;&#39039;&#26041;&#27861;&#25512;&#24191;&#21040;&#26080;&#31351;&#32500;&#31639;&#23376;&#65292;&#20854;&#20182;&#24418;&#24335;&#30340;&#26080;&#31351;&#32500;&#31639;&#23376;&#24050;&#32463;&#20986;&#29616;&#22312;&#38750;&#32447;&#24615; PDE &#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transportation of probability measures underlies many core tasks in statistics and machine learning, from variational inference to generative modeling. A typical goal is to represent a target probability measure of interest as the push-forward of a tractable source measure through a learned map. We present a new construction of such a transport map, given the ability to evaluate the score of the target distribution. Specifically, we characterize the map as a zero of an infinite-dimensional score-residual operator and derive a Newton-type method for iteratively constructing such a zero. We prove convergence of these iterations by invoking classical elliptic regularity theory for partial differential equations (PDE) and show that this construction enjoys rapid convergence, under smoothness assumptions on the target score. A key element of our approach is a generalization of the elementary Newton method to infinite-dimensional operators, other forms of which have appeared in nonlinear PDE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#28082;&#20307;&#27689;&#25506;&#27979;&#22120;&#20302;&#33021;&#29289;&#29702;&#20013;&#20351;&#29992;Few-Hits&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#22312;&#21333;&#27604;&#29305;&#19982;&#21452;&#27604;&#29305;&#20107;&#20214;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer-Encoder&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;DUNE Phase II&#25506;&#27979;&#22120;&#20248;&#21270;&#20102;&#25506;&#27979;&#22120;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.09744</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#28082;&#20307;&#27689;&#25506;&#27979;&#22120;&#20302;&#33021;&#29289;&#29702;&#30340;Few-Hits&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Assessment of few-hits machine learning classification algorithms for low energy physics in liquid argon detectors. (arXiv:2305.09744v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#28082;&#20307;&#27689;&#25506;&#27979;&#22120;&#20302;&#33021;&#29289;&#29702;&#20013;&#20351;&#29992;Few-Hits&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#22312;&#21333;&#27604;&#29305;&#19982;&#21452;&#27604;&#29305;&#20107;&#20214;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer-Encoder&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;DUNE Phase II&#25506;&#27979;&#22120;&#20248;&#21270;&#20102;&#25506;&#27979;&#22120;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#33021;&#21306;&#22495;&#65292;&#22823;&#22411;&#28082;&#20307;&#27689;TPCs&#30340;&#29289;&#29702;&#28508;&#21147;&#20173;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#22240;&#20026;Few-Hits&#20107;&#20214;&#25152;&#32534;&#30721;&#30340;&#20449;&#24687;&#24456;&#38590;&#34987;&#20256;&#32479;&#20998;&#31867;&#31639;&#27861;&#21033;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#36825;&#20123;&#31867;&#22411;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20256;&#32479;&#65288;&#30830;&#23450;&#24615;&#65289;&#31639;&#27861;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer-Encoder&#26041;&#27861;&#22312;&#20302;&#33021;&#29289;&#29702;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#38382;&#39064;&#65288;&#21333;&#27604;&#29305;&#19982;&#21452;&#27604;&#29305;&#20107;&#20214;&#65289;&#20013;&#20248;&#20110;&#30830;&#23450;&#24615;&#31639;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;Transformer-Encoder&#26041;&#27861;&#30456;&#23545;&#20110;CNN&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#20248;&#21270;&#20102;&#25506;&#27979;&#22120;&#21442;&#25968;&#65292;&#37325;&#28857;&#20851;&#27880;DUNE Phase II&#25506;&#27979;&#22120;&#65288;"&#26426;&#20250;&#27169;&#22359;"&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The physics potential of massive liquid argon TPCs in the low-energy regime is still to be fully reaped because few-hits events encode information that can hardly be exploited by conventional classification algorithms. Machine learning (ML) techniques give their best in these types of classification problems. In this paper, we evaluate their performance against conventional (deterministic) algorithms. We demonstrate that both Convolutional Neural Networks (CNN) and Transformer-Encoder methods outperform deterministic algorithms in one of the most challenging classification problems of low-energy physics (single- versus double-beta events). We discuss the advantages and pitfalls of Transformer-Encoder methods versus CNN and employ these methods to optimize the detector parameters, with an emphasis on the DUNE Phase II detectors ("Module of Opportunity").
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;DeepTextMark&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#35813;&#25216;&#26415;&#23454;&#29616;&#20102;&#30450;&#30446;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#34109;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#27700;&#21360;&#26816;&#27979;&#31934;&#24230;&#21644;&#25269;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05773</link><description>&lt;p&gt;
DeepTextMark&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#29992;&#20110;&#26816;&#27979;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text. (arXiv:2305.05773v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;DeepTextMark&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#35813;&#25216;&#26415;&#23454;&#29616;&#20102;&#30450;&#30446;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#34109;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#27700;&#21360;&#26816;&#27979;&#31934;&#24230;&#21644;&#25269;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#25991;&#26412;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#20026;&#20102;&#38450;&#27490;&#28508;&#22312;&#30340;&#28389;&#29992;&#65292;&#26816;&#27979;&#25991;&#26412;&#26159;&#21542;&#30001;LLM&#29983;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#20123;&#30456;&#20851;&#30340;&#24037;&#20316;&#35797;&#22270;&#20351;&#29992;&#23558;&#36755;&#20837;&#25991;&#26412;&#20998;&#31867;&#20026;&#20154;&#31867;&#32534;&#20889;&#30340;&#25110;LLM&#29983;&#25104;&#30340;&#20108;&#20803;&#20998;&#31867;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#24050;&#34987;&#35777;&#26126;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#30001;&#20110;&#20998;&#31867;&#32467;&#26524;&#21487;&#33021;&#23545;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#20915;&#31574;&#20135;&#29983;&#24433;&#21709;&#65292;&#25991;&#26412;&#28304;&#30340;&#26816;&#27979;&#38656;&#35201;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DeepTextMark&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#27700;&#21360;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#28304;&#26816;&#27979;&#12290;DeepTextMark&#36890;&#36807;&#24212;&#29992;Word2Vec&#21644;&#21477;&#23376;&#32534;&#30721;&#36827;&#34892;&#27700;&#21360;&#25554;&#20837;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#27700;&#21360;&#26816;&#27979;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30450;&#30446;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#34109;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#27491;&#22914;&#26412;&#25991;&#25152;&#36827;&#19968;&#27493;&#35752;&#35770;&#30340;&#37027;&#26679;&#65292;&#36825;&#20123;&#29305;&#24615;&#23545;&#20110;&#36890;&#29992;&#25991;&#26412;&#28304;&#26816;&#27979;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#24182;&#19988;DeepTextMark&#22312;&#27700;&#21360;&#26816;&#27979;&#31934;&#24230;&#21644;&#25269;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of text generators have grown with the rapid development of Large Language Models (LLM). To prevent potential misuse, the ability to detect whether texts are produced by LLM has become increasingly important. Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously. As discussed further in the paper, these traits are indispensable for generic text source detec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#29983;&#25104;&#27169;&#22411;&#22312;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#32463;&#36807;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#35206;&#30422;&#29575;&#36739;&#20302;&#19988;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00418</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Effectiveness of Large Language Models in Generating Unit Tests. (arXiv:2305.00418v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#29983;&#25104;&#27169;&#22411;&#22312;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#32463;&#36807;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#35206;&#30422;&#29575;&#36739;&#20302;&#19988;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#27880;&#37322;&#12289;&#29616;&#26377;&#20195;&#30721;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#26469;&#29983;&#25104;&#20195;&#30721;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#19977;&#20010;&#29983;&#25104;&#27169;&#22411;&#65288;CodeGen&#12289;Codex&#21644;GPT-3.5&#65289;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#33021;&#22815;&#25104;&#21151;&#29992;&#20110;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#22522;&#20934;&#65288;HumanEval&#21644;Evosuite SF110&#65289;&#26469;&#35843;&#26597;&#29615;&#22659;&#29983;&#25104;&#23545;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#26681;&#25454;&#32534;&#35793;&#29575;&#12289;&#27979;&#35797;&#27491;&#30830;&#24615;&#12289;&#35206;&#30422;&#29575;&#21644;&#27979;&#35797;&#21619;&#36947;&#26469;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Codex&#27169;&#22411;&#22312;HumanEval&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;80%&#30340;&#35206;&#30422;&#29575;&#65292;&#20294;&#22312;EvoSuite SF110&#22522;&#20934;&#20013;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#36229;&#36807;2%&#30340;&#35206;&#30422;&#29575;&#12290;&#29983;&#25104;&#30340;&#27979;&#35797;&#36824;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#65292;&#27604;&#22914;&#37325;&#22797;&#30340;&#26029;&#35328;&#21644;&#31354;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning. To fill this gap, we investigated how well three generative models (CodeGen, Codex, and GPT-3.5) can generate test cases. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the context generation's effect in the unit test generation process. We evaluated the models based on compilation rates, test correctness, coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27714;&#35299;&#20809;&#28369;&#26377;&#30028;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#20869;&#28857;&#31639;&#27861;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#30340;&#25628;&#32034;&#26041;&#21521;&#21644;&#20869;&#37096;&#37051;&#22495;&#65292;&#33021;&#22815;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#35774;&#32622;&#19979;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14907</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#20869;&#28857;&#31639;&#27861;&#27714;&#35299;&#20809;&#28369;&#26377;&#30028;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth Bound-Constrained Optimization Problems. (arXiv:2304.14907v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27714;&#35299;&#20809;&#28369;&#26377;&#30028;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#20869;&#28857;&#31639;&#27861;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#30340;&#25628;&#32034;&#26041;&#21521;&#21644;&#20869;&#37096;&#37051;&#22495;&#65292;&#33021;&#22815;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#35774;&#32622;&#19979;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#30340;&#20869;&#28857;&#31639;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#23384;&#22312;&#32422;&#26463;&#30340;&#36830;&#32493;&#21487;&#24494;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#20809;&#28369;&#65288;&#38750;&#20984;&#65289;&#20248;&#21270;&#38382;&#39064;&#26102;&#19982;&#20854;&#20182;&#20869;&#28857;&#26041;&#27861;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#25628;&#32034;&#26041;&#21521;&#26159;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#35745;&#31639;&#24471;&#21040;&#30340;&#12290;&#23427;&#22312;&#20351;&#29992;&#21487;&#34892;&#22495;&#30340;&#20869;&#37096;&#37051;&#22495;&#65288;&#30001;&#27491;&#19988;&#28040;&#22833;&#30340;&#37051;&#22495;&#21442;&#25968;&#24207;&#21015;&#23450;&#20041;&#65289;&#30340;&#36807;&#31243;&#20013;&#20063;&#24456;&#29420;&#29305;&#65292;&#36890;&#36807;&#23558;&#36845;&#20195;&#24378;&#21046;&#20445;&#30041;&#22312;&#35813;&#37051;&#22495;&#20869;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#31934;&#24515;&#24179;&#34913;&#23631;&#38556;&#12289;&#27493;&#38271;&#21644;&#37051;&#22495;&#24207;&#21015;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#28385;&#36275;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#35774;&#32622;&#19979;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#22312;&#20004;&#31181;&#35774;&#32622;&#19979;&#65292;&#25968;&#20540;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20248;&#20110;&#25237;&#24433;-&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A stochastic-gradient-based interior-point algorithm for minimizing a continuously differentiable objective function (that may be nonconvex) subject to bound constraints is presented, analyzed, and demonstrated through experimental results. The algorithm is unique from other interior-point methods for solving smooth (nonconvex) optimization problems since the search directions are computed using stochastic gradient estimates. It is also unique in its use of inner neighborhoods of the feasible region -- defined by a positive and vanishing neighborhood-parameter sequence -- in which the iterates are forced to remain. It is shown that with a careful balance between the barrier, step-size, and neighborhood sequences, the proposed algorithm satisfies convergence guarantees in both deterministic and stochastic settings. The results of numerical experiments show that in both settings the algorithm can outperform a projected-(stochastic)-gradient method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#29992;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;(TNN)&#23545;&#30334;&#24917;&#22823;&#25481;&#26399;&#36827;&#34892;&#23450;&#20215;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;TNN&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23569;&#21442;&#25968;&#25935;&#24863;&#24230;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.09750</link><description>&lt;p&gt;
&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#22312;&#30334;&#24917;&#22823;&#25481;&#26399;&#23450;&#20215;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Tensor Neural Networks to Pricing Bermudan Swaptions. (arXiv:2304.09750v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#29992;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;(TNN)&#23545;&#30334;&#24917;&#22823;&#25481;&#26399;&#36827;&#34892;&#23450;&#20215;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;TNN&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23569;&#21442;&#25968;&#25935;&#24863;&#24230;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cheyette&#27169;&#22411;&#26159;&#19968;&#31181;&#20934;&#39640;&#26031;&#27874;&#21160;&#29575;&#21033;&#29575;&#27169;&#22411;&#65292;&#24191;&#27867;&#29992;&#20110;&#23450;&#20215;&#21033;&#29575;&#34893;&#29983;&#21697;&#65292;&#20363;&#22914;&#27431;&#24335;&#25481;&#26399;&#21644;&#30334;&#24917;&#22823;&#25481;&#26399;&#65292;&#32780;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#24050;&#25104;&#20026;&#34892;&#19994;&#26631;&#20934;&#12290;&#22312;&#20302;&#32500;&#24230;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#20026;&#27431;&#24335;&#25481;&#26399;&#25552;&#20379;&#20102;&#20934;&#30830;&#32780;&#31283;&#20581;&#30340;&#20215;&#26684;&#65292;&#20294;&#21363;&#20351;&#22312;&#36825;&#31181;&#35745;&#31639;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#20351;&#29992;&#29366;&#24577;&#21464;&#37327;&#20316;&#20026;&#22238;&#24402;&#22120;&#26102;&#65292;&#23427;&#20204;&#20063;&#20250;&#20302;&#20272;&#30334;&#24917;&#22823;&#25481;&#26399;&#30340;&#20215;&#20540;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#25152;&#29992;&#22238;&#24402;&#22120;&#20013;&#39044;&#20808;&#30830;&#23450;&#30340;&#22522;&#20989;&#25968;&#25968;&#37327;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#38754;&#20020;&#30528;&#32500;&#24230;&#28798;&#38590;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#21033;&#29992;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;(TNN)&#26469;&#36827;&#34892;&#30334;&#24917;&#22823;&#25481;&#26399;&#30340;&#23450;&#20215;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;TNN&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23545;&#20110;&#22238;&#24402;&#22120;&#20013;&#25152;&#29992;&#22522;&#20989;&#25968;&#30340;&#25968;&#37327;&#31561;&#21442;&#25968;&#65292;&#20943;&#23569;&#20102;&#25935;&#24863;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;TNN&#33021;&#22815;&#22312;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#23450;&#20215;&#27431;&#24335;&#25481;&#26399;&#21644;&#30334;&#24917;&#22823;&#25481;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Cheyette model is a quasi-Gaussian volatility interest rate model widely used to price interest rate derivatives such as European and Bermudan Swaptions for which Monte Carlo simulation has become the industry standard. In low dimensions, these approaches provide accurate and robust prices for European Swaptions but, even in this computationally simple setting, they are known to underestimate the value of Bermudan Swaptions when using the state variables as regressors. This is mainly due to the use of a finite number of predetermined basis functions in the regression. Moreover, in high-dimensional settings, these approaches succumb to the Curse of Dimensionality. To address these issues, Deep-learning techniques have been used to solve the backward Stochastic Differential Equation associated with the value process for European and Bermudan Swaptions; however, these methods are constrained by training time and memory. To overcome these limitations, we propose leveraging Tensor Neura
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#36716;&#31227;&#30697;&#38453;&#21644;&#22266;&#23450;&#20294;&#26410;&#30693;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;MDP&#23398;&#20064;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#32622;&#20449;&#21306;&#38388;&#26694;&#26550;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.00155</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19978;&#36827;&#34892;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Reinforcement Learning in Markov Decision Process Using Linear Programming. (arXiv:2304.00155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#36716;&#31227;&#30697;&#38453;&#21644;&#22266;&#23450;&#20294;&#26410;&#30693;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;MDP&#23398;&#20064;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#32622;&#20449;&#21306;&#38388;&#26694;&#26550;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#30693;&#36716;&#31227;&#30697;&#38453;&#21644;&#22266;&#23450;&#20294;&#26410;&#30693;&#20998;&#24067;&#30340;&#38543;&#26426;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#23398;&#20064;&#32773;&#26088;&#22312;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#24182;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#26368;&#23567;&#21270;&#20182;&#20204;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#27169;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#36807;&#28193;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#24182;&#20351;&#29992;&#21344;&#29992;&#24230;&#37327;&#23558;&#22312;&#32447;MDP&#19982;&#32447;&#24615;&#35268;&#21010;&#30456;&#36830;&#25509;&#65292;&#23454;&#29616;&#20102;$\tilde{O}(LX\sqrt{TA})$&#30340;&#39640;&#27010;&#29575;&#36951;&#25022;&#30028;&#12290;&#23427;&#27604;&#29616;&#26377;&#30340;&#20351;&#29992;&#31867;&#20284;&#32622;&#20449;&#21306;&#38388;&#26694;&#26550;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#26356;&#32039;&#30340;&#36951;&#25022;&#30028;&#24182;&#25913;&#21892;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online reinforcement learning in episodic Markov decision process (MDP) with an unknown transition matrix and stochastic rewards drawn from a fixed but unknown distribution. The learner aims to learn the optimal policy and minimize their regret over a finite time horizon through interacting with the environment. We devise a simple and efficient model-based algorithm that achieves $\tilde{O}(LX\sqrt{TA})$ regret with high probability, where $L$ is the episode length, $T$ is the number of episodes, and $X$ and $A$ are the cardinalities of the state space and the action space, respectively. The proposed algorithm, which is based on the concept of "optimism in the face of uncertainty", maintains confidence sets of transition and reward functions and uses occupancy measures to connect the online MDP with linear programming. It achieves a tighter regret bound compared to the existing works that use a similar confidence sets framework and improves the computational effort compared
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#21644;&#21338;&#24328;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#30528;&#37325;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#39640;&#32500;&#24230;&#21644;&#38750;&#24120;&#22797;&#26434;&#32467;&#26500;&#24773;&#20917;&#19979;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.10257</link><description>&lt;p&gt;
&#29992;&#20110;&#38543;&#26426;&#25511;&#21046;&#21644;&#21338;&#24328;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Developments in Machine Learning Methods for Stochastic Control and Games. (arXiv:2303.10257v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#21644;&#21338;&#24328;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#30528;&#37325;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#39640;&#32500;&#24230;&#21644;&#38750;&#24120;&#22797;&#26434;&#32467;&#26500;&#24773;&#20917;&#19979;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#21644;&#21338;&#24328;&#24050;&#32463;&#22312;&#37329;&#34701;&#12289;&#32463;&#27982;&#23398;&#12289;&#31038;&#20250;&#31185;&#23398;&#12289;&#26426;&#22120;&#20154;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#39046;&#22495;&#20013;&#25214;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#37117;&#28041;&#21450;&#21040;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#36825;&#25512;&#21160;&#20102;&#20808;&#36827;&#30340;&#25968;&#20540;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#26041;&#27861;&#24050;&#32463;&#21457;&#23637;&#29992;&#20110;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#21644;&#21338;&#24328;&#12290;&#25105;&#20204;&#22238;&#39038;&#36825;&#20123;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#24050;&#32463;&#35299;&#38145;&#20102;&#39640;&#32500;&#24230;&#21644;&#38750;&#24120;&#22797;&#26434;&#32467;&#26500;&#24773;&#20917;&#19979;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#26159;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#26080;&#27861;&#23436;&#25104;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20027;&#35201;&#32771;&#34385;&#36830;&#32493;&#26102;&#38388;&#21644;&#36830;&#32493;&#31354;&#38388;&#35774;&#32622;&#12290;&#35768;&#22810;&#26032;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#29992;&#20110;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#25110;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25110;&#32773;&#22522;&#20110;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#23548;&#33268;&#20102;&#31361;&#30772;&#24615;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimal control and games have found a wide range of applications, from finance and economics to social sciences, robotics and energy management. Many real-world applications involve complex models which have driven the development of sophisticated numerical methods. Recently, computational methods based on machine learning have been developed for stochastic control problems and games. We review such methods, with a focus on deep learning algorithms that have unlocked the possibility to solve such problems even when the dimension is high or when the structure is very complex, beyond what is feasible with traditional numerical methods. Here, we consider mostly the continuous time and continuous space setting. Many of the new approaches build on recent neural-network based methods for high-dimensional partial differential equations or backward stochastic differential equations, or on model-free reinforcement learning for Markov decision processes that have led to breakthrough 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24191;&#20041;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;CFM&#65289;&#30340;&#25216;&#26415;&#65292;&#22312;&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#65288;CNFs&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#24341;&#20837;&#20102;&#26368;&#20248;&#20256;&#36755;CFM&#65288;OT-CFM&#65289;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#20197;&#26080;&#27169;&#25311;&#26041;&#24335;&#35745;&#31639;&#21160;&#24577;OT&#65292;&#21152;&#36895;&#20102;&#25512;&#26029;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.00482</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#25209;&#37327;&#20248;&#21270;&#20256;&#36755;&#25913;&#36827;&#21644;&#27867;&#21270;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving and generalizing flow-based generative models with minibatch optimal transport. (arXiv:2302.00482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00482
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24191;&#20041;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;CFM&#65289;&#30340;&#25216;&#26415;&#65292;&#22312;&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#65288;CNFs&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#24341;&#20837;&#20102;&#26368;&#20248;&#20256;&#36755;CFM&#65288;OT-CFM&#65289;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#20197;&#26080;&#27169;&#25311;&#26041;&#24335;&#35745;&#31639;&#21160;&#24577;OT&#65292;&#21152;&#36895;&#20102;&#25512;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#65288;CNFs&#65289;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#20854;&#22522;&#20110;&#27169;&#25311;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#23384;&#22312;&#23616;&#38480;&#24615;&#32780;&#21463;&#21040;&#32422;&#26463;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#24191;&#20041;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;CFM&#65289;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;CNFs&#30340;&#26080;&#27169;&#25311;&#35757;&#32451;&#30446;&#26631;&#30340;&#38598;&#21512;&#12290;CFM&#20855;&#26377;&#31867;&#20284;&#20110;&#25193;&#25955;&#27169;&#22411;&#20013;&#29992;&#20110;&#35757;&#32451;&#38543;&#26426;&#27969;&#30340;&#31283;&#23450;&#22238;&#24402;&#30446;&#26631;&#65292;&#20294;&#21516;&#26102;&#20139;&#26377;&#30830;&#23450;&#24615;&#27969;&#27169;&#22411;&#30340;&#39640;&#25928;&#25512;&#26029;&#12290;&#19982;&#25193;&#25955;&#27169;&#22411;&#21644;&#20043;&#21069;&#30340;CNF&#35757;&#32451;&#31639;&#27861;&#30456;&#27604;&#65292;CFM&#19981;&#38656;&#35201;&#28304;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#20063;&#19981;&#38656;&#35201;&#23545;&#20854;&#23494;&#24230;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#30340;&#19968;&#31181;&#21464;&#20307;&#26159;&#26368;&#20248;&#20256;&#36755;CFM&#65288;OT-CFM&#65289;&#65292;&#23427;&#21019;&#24314;&#20102;&#26356;&#31616;&#21333;&#30340;&#27969;&#65292;&#26356;&#23481;&#26131;&#35757;&#32451;&#65292;&#24182;&#19988;&#23548;&#33268;&#26356;&#24555;&#30340;&#25512;&#26029;&#65292;&#22914;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#25152;&#31034;&#12290;&#27492;&#22806;&#65292;OT-CFM&#26159;&#31532;&#19968;&#31181;&#20197;&#26080;&#27169;&#25311;&#26041;&#24335;&#35745;&#31639;&#21160;&#24577;OT&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;CFM&#35757;&#32451;CNFs&#21487;&#20197;&#25913;&#36827;&#21508;&#31181;&#26465;&#20214;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, OT-CFM is the first method to compute dynamic OT in a simulation-free way. Training CNFs with CFM improves results on a variety of conditional and u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DiffSTG&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;STGNN&#30340;&#26102;&#31354;&#23398;&#20064;&#33021;&#21147;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;STG&#39044;&#27979;&#20013;&#30340;&#25490;&#21517;&#27010;&#29575;&#20998;&#25968;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2301.13629</link><description>&lt;p&gt;
DiffSTG: &#24102;&#26377;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#29575;&#26102;&#31354;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models. (arXiv:2301.13629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DiffSTG&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;STGNN&#30340;&#26102;&#31354;&#23398;&#20064;&#33021;&#21147;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;STG&#39044;&#27979;&#20013;&#30340;&#25490;&#21517;&#27010;&#29575;&#20998;&#25968;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNN&#65289;&#24050;&#25104;&#20026;&#26102;&#31354;&#22270;&#65288;STG&#65289;&#39044;&#27979;&#30340;&#20027;&#35201;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#23545;STG&#25968;&#25454;&#20013;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20851;&#27880;&#27010;&#29575;STG&#39044;&#27979;&#65292;&#30001;&#20110;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#21644;&#22797;&#26434;&#30340;ST&#20381;&#36182;&#20851;&#31995;&#30340;&#22256;&#38590;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#23558;&#27969;&#34892;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#25512;&#24191;&#21040;STG&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DiffSTG&#30340;&#26032;&#30340;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;STG&#21435;&#22122;&#32593;&#32476;UGnet&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;STGNN&#30340;&#26102;&#31354;&#23398;&#20064;&#33021;&#21147;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30456;&#32467;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;DiffSTG&#23558;&#25345;&#32493;&#25490;&#21517;&#27010;&#29575;&#20998;&#25968;&#65288;CRPS&#65289;&#38477;&#20302;&#20102;4%-14%&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#38477;&#20302;&#20102;2%-7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal graph neural networks (STGNN) have emerged as the dominant model for spatio-temporal graph (STG) forecasting. Despite their success, they fail to model intrinsic uncertainties within STG data, which cripples their practicality in downstream tasks for decision-making. To this end, this paper focuses on probabilistic STG forecasting, which is challenging due to the difficulty in modeling uncertainties and complex ST dependencies. In this study, we present the first attempt to generalize the popular denoising diffusion probabilistic models to STGs, leading to a novel non-autoregressive framework called DiffSTG, along with the first denoising network UGnet for STG in the framework. Our approach combines the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models. Extensive experiments validate that DiffSTG reduces the Continuous Ranked Probability Score (CRPS) by 4%-14%, and Root Mean Squared Error (RMSE) by 2%-7% over existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#65292;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#21516;&#26102;&#33021;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.13096</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#38170;&#28857;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Language-Driven Anchors for Zero-Shot Adversarial Robustness. (arXiv:2301.13096v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#65292;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#21516;&#26102;&#33021;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#12290;LAAT&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#22266;&#23450;&#30340;&#38170;&#28857;&#65288;&#24402;&#19968;&#21270;&#29305;&#24449;&#23884;&#20837;&#65289;&#65292;&#24182;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20351;&#29992;&#36825;&#20123;&#38170;&#28857;&#12290;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;LAAT&#21487;&#20197;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#22312;&#26032;&#31867;&#21035;&#19978;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26679;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#26368;&#36817;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#20960;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#23427;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAAT&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#65288;&#22914;ResNet-50&#21644;DenseNet-121&#65289;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are known to be susceptible to adversarial attacks. In this work, we focus on improving adversarial robustness in the challenging zero-shot image classification setting. To address this issue, we propose LAAT, a novel Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes a text encoder to generate fixed anchors (normalized feature embeddings) for each category and then uses these anchors for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT can enhance the adversarial robustness of the image model on novel categories without additional examples. We identify the large cosine similarity problem of recent text encoders and design several effective techniques to address it. The experimental results demonstrate that LAAT significantly improves zero-shot adversarial performance, outperforming previous state-of-the-art adversarially robust one-shot methods. Moreover, our method produces substantial zero-shot adver
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PhAST&#26041;&#27861;&#26469;&#24555;&#36895;&#21457;&#29616;&#26356;&#26377;&#25928;&#30340;&#20652;&#21270;&#21058;&#26469;&#39537;&#21160;&#30005;&#21270;&#23398;&#21453;&#24212;, &#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20307;&#31995;&#32467;&#26500;, &#21487;&#20197;&#22686;&#21152;&#35745;&#31639;&#25928;&#29575;&#21644;&#31934;&#24230;</title><link>http://arxiv.org/abs/2211.12020</link><description>&lt;p&gt;
PhAST&#65306;&#29289;&#29702;&#24863;&#30693;&#12289;&#21487;&#25193;&#23637;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;GNN&#22312;&#21152;&#36895;&#20652;&#21270;&#21058;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design. (arXiv:2211.12020v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PhAST&#26041;&#27861;&#26469;&#24555;&#36895;&#21457;&#29616;&#26356;&#26377;&#25928;&#30340;&#20652;&#21270;&#21058;&#26469;&#39537;&#21160;&#30005;&#21270;&#23398;&#21453;&#24212;, &#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20307;&#31995;&#32467;&#26500;, &#21487;&#20197;&#22686;&#21152;&#35745;&#31639;&#25928;&#29575;&#21644;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#27668;&#20505;&#21361;&#26426;&#38656;&#35201;&#24555;&#36895;&#21521;&#20302;&#30899;&#33021;&#28304;&#36716;&#21464;&#12290;&#20652;&#21270;&#21058;&#26448;&#26009;&#22312;&#35768;&#22810;&#24037;&#19994;&#36807;&#31243;&#20013;&#30340;&#30005;&#21270;&#23398;&#21453;&#24212;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22914;&#21487;&#20877;&#29983;&#33021;&#28304;&#20648;&#23384;&#21644;&#30005;&#33655;&#21512;&#25104;&#12290;&#20026;&#20102;&#20943;&#23569;&#22312;&#36825;&#20123;&#36807;&#31243;&#20013;&#28040;&#32791;&#30340;&#33021;&#37327;&#65292;&#25105;&#20204;&#24517;&#39035;&#24555;&#36895;&#21457;&#29616;&#26356;&#26377;&#25928;&#30340;&#20652;&#21270;&#21058;&#26469;&#39537;&#21160;&#30005;&#21270;&#23398;&#21453;&#24212;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26377;&#28508;&#21147;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#39640;&#25928;&#22320;&#27169;&#25311;&#26448;&#26009;&#30340;&#24615;&#36136;&#65292;&#20174;&#32780;&#21152;&#36895;&#30005;&#20652;&#21270;&#21058;&#30340;&#35774;&#35745;&#12290;&#20026;&#27492;&#65292;Open Catalyst Project OC20&#25968;&#25454;&#38598;&#24050;&#32463;&#34987;&#26500;&#24314;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24050;&#32463;&#22312;OC20&#19978;&#35757;&#32451;&#30340;&#29616;&#26377;ML&#27169;&#22411;&#20173;&#28982;&#26080;&#27861;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#35201;&#27714;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#21019;&#26032;&#65292;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#22686;&#21152;&#35745;&#31639;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;&#29305;&#21035;&#26159;&#25105;&#20204;&#22312;&#22270;&#32763;&#35793;&#23618;&#12289;&#22270;&#27880;&#24847;&#21147;&#23618;&#21644;&#27744;&#21270;&#23618;&#20013;&#25552;&#20986;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;Physical Attribute Scaling Transformer (PhAST)&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;PhAST&#22312;&#29983;&#25104;&#20934;&#30830;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#20960;&#20010;&#30456;&#20851;&#24212;&#29992;&#65292;&#21253;&#25324;&#30005;&#20652;&#21270;&#21058;&#30340;&#21457;&#29616;&#21644;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mitigating the climate crisis requires a rapid transition towards lower carbon energy. Catalyst materials play a crucial role in the electrochemical reactions involved in a great number of industrial processes key to this transition, such as renewable energy storage and electrofuel synthesis. To reduce the amount of energy spent on such processes, we must quickly discover more efficient catalysts to drive the electrochemical reactions. Machine learning (ML) holds the potential to efficiently model the properties of materials from large amounts of data, and thus to accelerate electrocatalyst design. The Open Catalyst Project OC20 data set was constructed to that end. However, most existing ML models trained on OC20 are still neither scalable nor accurate enough for practical applications. Here, we propose several task-specific innovations, applicable to most architectures, which increase both computational efficiency and accuracy. In particular, we propose improvements in (1) the graph 
&lt;/p&gt;</description></item><item><title>CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09935</link><description>&lt;p&gt;
CAPE: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
CAPE: Corrective Actions from Precondition Errors using Large Language Models. (arXiv:2211.09935v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09935
&lt;/p&gt;
&lt;p&gt;
CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24120;&#35782;&#30693;&#35782;&#20026;&#35774;&#35745;&#26234;&#33021;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#30340;&#26041;&#27861;&#22312;&#34892;&#21160;&#22833;&#36133;&#26102;&#26080;&#27861;&#24674;&#22797;&#65292;&#24182;&#19988;&#36890;&#24120;&#21482;&#33021;&#23581;&#35797;&#37325;&#26032;&#25191;&#34892;&#22833;&#36133;&#30340;&#34892;&#21160;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65288;CAPE&#65289;&#65292;&#35797;&#22270;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#25552;&#20986;&#32416;&#27491;&#21069;&#32622;&#26465;&#20214;&#38169;&#35823;&#30340;&#34892;&#21160;&#12290;CAPE&#36890;&#36807;&#21033;&#29992;&#23569;&#26679;&#26412;&#25512;&#29702;&#20174;&#34892;&#21160;&#21069;&#32622;&#26465;&#20214;&#20013;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#30830;&#20445;&#35821;&#20041;&#27491;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#37325;&#26032;&#25552;&#31034;&#12290;&#22312;VirtualHome&#20013;&#65292;CAPE&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#24182;&#19988;&#30456;&#27604;SayCan&#65292;&#23558;&#20154;&#24037;&#26631;&#27880;&#30340;&#35745;&#21010;&#27491;&#30830;&#24230;&#25351;&#26631;&#20174;28.89%&#25552;&#39640;&#21040;49.63%&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#20063;&#36866;&#29992;&#20110;&#19968;&#21488;&#37197;&#32622;&#20102;&#19968;&#32452;&#20197;&#35821;&#35328;&#20026;&#25351;&#23450;&#30340;&#25216;&#33021;&#21644;&#30456;&#20851;&#21069;&#32622;&#26465;&#20214;&#30340;&#27874;&#22763;&#39039;&#21160;&#21147;&#20844;&#21496;&#30340;Spot&#26426;&#22120;&#20154;&#65292;&#20854;&#20013;CAPE&#25552;&#39640;&#20102;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause.  We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32972;&#21518;&#25915;&#20987;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#32858;&#21512;&#22120;&#65292;&#38450;&#24481;&#32972;&#21518;&#25915;&#20987;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#25972;&#20307;&#25928;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#32972;&#21518;&#26679;&#26412;&#26469;&#35823;&#23548;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#19982;&#33391;&#24615;&#23458;&#25143;&#31471;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.01834</link><description>&lt;p&gt;
&#38450;&#24481;&#32852;&#37030;&#32972;&#21518;&#25915;&#20987;&#30340;&#19981;&#21464;&#32858;&#21512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Invariant Aggregator for Defending against Federated Backdoor Attacks. (arXiv:2210.01834v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32972;&#21518;&#25915;&#20987;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#32858;&#21512;&#22120;&#65292;&#38450;&#24481;&#32972;&#21518;&#25915;&#20987;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#25972;&#20307;&#25928;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#32972;&#21518;&#26679;&#26412;&#26469;&#35823;&#23548;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#19982;&#33391;&#24615;&#23458;&#25143;&#31471;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#22312;&#19981;&#30452;&#25509;&#20849;&#20139;&#31169;&#23494;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#25928;&#27169;&#22411;&#32780;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32852;&#37030;&#35774;&#32622;&#20351;&#24471;&#27169;&#22411;&#22312;&#23384;&#22312;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25932;&#23545;&#25915;&#20987;&#12290;&#23613;&#31649;&#23545;&#20110;&#26088;&#22312;&#38477;&#20302;&#27169;&#22411;&#25928;&#29992;&#30340;&#25915;&#20987;&#30340;&#38450;&#24481;&#24050;&#32463;&#21462;&#24471;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30340;&#25104;&#21151;&#65292;&#20294;&#38450;&#24481;&#20165;&#25552;&#39640;&#32972;&#21518;&#26679;&#26412;&#19978;&#27169;&#22411;&#20934;&#30830;&#24615;&#32780;&#19981;&#25439;&#23475;&#20854;&#20182;&#26679;&#26412;&#25928;&#29992;&#30340;&#32972;&#21518;&#25915;&#20987;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#19978;&#23545;&#32972;&#21518;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#31181;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#24120;&#35265;&#20110;&#35774;&#35745;&#33391;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;Resnet [He et al., 2015]&#65292;&#20294;&#24448;&#24448;&#34987;&#20808;&#21069;&#30340;&#24037;&#20316;&#25152;&#24573;&#35270;&#12290;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#19978;&#65292;&#35823;&#23548;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20197;&#20165;&#23545;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#32972;&#21518;&#26679;&#26412;&#26377;&#21033;&#65292;&#24182;&#19981;&#38656;&#35201;&#24694;&#24847;&#21644;&#33391;&#24615;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is gaining popularity as it enables training high-utility models across several clients without directly sharing their private data. As a downside, the federated setting makes the model vulnerable to various adversarial attacks in the presence of malicious clients. Despite the theoretical and empirical success in defending against attacks that aim to degrade models' utility, defense against backdoor attacks that increase model accuracy on backdoor samples exclusively without hurting the utility on other samples remains challenging. To this end, we first analyze the vulnerability of federated learning to backdoor attacks over a flat loss landscape which is common for well-designed neural networks such as Resnet [He et al., 2015] but is often overlooked by previous works. Over a flat loss landscape, misleading federated learning models to exclusively benefit malicious clients with backdoor samples do not require a significant difference between malicious and benign cli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#21253;&#21547;47&#31181;&#19981;&#21516;&#23646;&#24615;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;Deepfake&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#26088;&#22312;&#30740;&#31350;&#20844;&#20849;Deepfake&#25968;&#25454;&#38598;&#21487;&#33021;&#24102;&#26469;&#30340;AI&#20559;&#24046;&#38382;&#39064;</title><link>http://arxiv.org/abs/2208.05845</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#27880;&#37322;&#25968;&#25454;&#24211;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979; AI &#20559;&#24046;&#30340;&#20840;&#38754;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated Databases. (arXiv:2208.05845v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#21253;&#21547;47&#31181;&#19981;&#21516;&#23646;&#24615;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;Deepfake&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#26088;&#22312;&#30740;&#31350;&#20844;&#20849;Deepfake&#25968;&#25454;&#38598;&#21487;&#33021;&#24102;&#26469;&#30340;AI&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Deepfake &#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#31713;&#25913;&#24050;&#32463;&#25104;&#20026;&#23433;&#20840;&#21644;&#31038;&#20250;&#30340;&#20005;&#37325;&#20851;&#27880;&#28857;&#12290;&#35768;&#22810;&#26816;&#27979;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979; Deepfake &#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268; Deepfake &#26816;&#27979;&#22120;&#22833;&#25928;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#20116;&#20010;&#27969;&#34892;&#30340; Deepfake &#25968;&#25454;&#38598;&#20013; 47 &#31181;&#19981;&#21516;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#20154;&#21475;&#32479;&#35745;&#21644;&#38750;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#27880;&#37322;&#65292;&#24182;&#20840;&#38754;&#20998;&#26512;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340; Deepfake &#26816;&#27979;&#27169;&#22411;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340; AI &#20559;&#24046;&#38382;&#39064;&#65292;&#35843;&#26597;&#30740;&#31350;&#20102;&#36229;&#36807; 6500 &#19975;&#20010;&#26631;&#31614;&#30340;&#35768;&#22810;&#19981;&#21516;&#23646;&#24615;&#65288;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#65288;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#31181;&#26063;&#65289;&#21644;&#38750;&#20154;&#21475;&#32479;&#35745;&#23398;&#65288;&#22836;&#21457;&#12289;&#30382;&#32932;&#12289;&#37197;&#39280;&#31561;&#65289;&#20449;&#24687;&#23545;&#26816;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35843;&#26597;&#30340;&#25968;&#25454;&#24211;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268; AI &#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, image and video manipulations with Deepfake have become a severe concern for security and society. Many detection models and datasets have been proposed to detect Deepfake data reliably. However, there is an increased concern that these models and training databases might be biased and, thus, cause Deepfake detectors to fail. In this work, we investigate the bias issue caused by public Deepfake datasets by (a) providing large-scale demographic and non-demographic attribute annotations of 47 different attributes for five popular Deepfake datasets and (b) comprehensively analysing AI-bias of three state-of-the-art Deepfake detection backbone models on these datasets. The investigation analyses the influence of a large variety of distinctive attributes (from over 65M labels) on the detection performance, including demographic (age, gender, ethnicity) and non-demographic (hair, skin, accessories, etc.) information. The results indicate that investigated databases lack dive
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#32467;&#26500;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#20849;&#20139;&#19968;&#31181;&#24120;&#35265;&#32467;&#26500;&#32780;&#37096;&#20998;&#30830;&#23450;&#20102;&#30495;&#27491;&#30340;&#20301;&#32622;&#28151;&#21512;&#29289;&#30340;&#31751;&#20013;&#24515;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#30495;&#23454;&#28151;&#21512;&#32452;&#20998;&#28385;&#36275;&#26576;&#31181;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#65292;&#20063;&#36866;&#29992;&#20110;&#25104;&#20998;&#25968;&#37327;&#36807;&#22810;&#25110;&#36807;&#23569;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2009.13040</link><description>&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#23616;&#37096;&#26497;&#23567;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Local Minima Structures in Gaussian Mixture Models. (arXiv:2009.13040v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.13040
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#32467;&#26500;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#20849;&#20139;&#19968;&#31181;&#24120;&#35265;&#32467;&#26500;&#32780;&#37096;&#20998;&#30830;&#23450;&#20102;&#30495;&#27491;&#30340;&#20301;&#32622;&#28151;&#21512;&#29289;&#30340;&#31751;&#20013;&#24515;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#30495;&#23454;&#28151;&#21512;&#32452;&#20998;&#28385;&#36275;&#26576;&#31181;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#65292;&#20063;&#36866;&#29992;&#20110;&#25104;&#20998;&#25968;&#37327;&#36807;&#22810;&#25110;&#36807;&#23569;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20154;&#21475;&#26497;&#38480;&#30340;&#24773;&#20917;&#19979;&#35843;&#26597;&#20102;&#28151;&#21512;&#25104;&#20998;&#27169;&#22411;&#65288;GMM&#65289;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#20855;&#26377;&#19968;&#33324;&#25104;&#20998;&#25968;&#37327;&#30340;GMM&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#32467;&#26500;&#12290;&#30001;&#20110;&#30446;&#26631;&#20989;&#25968;&#26159;&#38750;&#20984;&#30340;&#65292;&#21363;&#20351;&#23545;&#20110;&#20998;&#31163;&#33391;&#22909;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#20063;&#21487;&#33021;&#23384;&#22312;&#19981;&#26159;&#20840;&#23616;&#26368;&#20248;&#30340;&#22810;&#20010;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25152;&#26377;&#23616;&#37096;&#26497;&#23567;&#20540;&#37117;&#20849;&#20139;&#19968;&#31181;&#24120;&#35265;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#37096;&#20998;&#30830;&#23450;&#20102;&#30495;&#27491;&#30340;&#20301;&#32622;&#28151;&#21512;&#29289;&#65288;&#21363;&#39640;&#26031;&#25104;&#20998;&#30340;&#22343;&#20540;&#65289;&#30340;&#31751;&#20013;&#24515;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#23616;&#37096;&#26497;&#23567;&#20540;&#21487;&#20197;&#34920;&#31034;&#20026;&#20004;&#31181;&#31867;&#22411;&#23376;&#37197;&#32622;&#30340;&#38750;&#37325;&#21472;&#32452;&#21512;&#65306;&#23558;&#21333;&#20010;&#22343;&#20540;&#20272;&#35745;&#19982;&#22810;&#20010;&#39640;&#26031;&#20998;&#37327;&#25311;&#21512;&#25110;&#23558;&#22810;&#20010;&#20272;&#35745;&#25311;&#21512;&#21040;&#21333;&#20010;&#30495;&#23454;&#20998;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#30495;&#23454;&#28151;&#21512;&#32452;&#20998;&#28385;&#36275;&#26576;&#31181;&#20998;&#31163;&#26465;&#20214;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#25104;&#20998;&#25968;&#37327;&#36807;&#22810;&#25110;&#36807;&#23569;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#19968;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#35774;&#32622;&#25552;&#20379;&#20102;&#26356;&#31934;&#32454;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#32467;&#26500;&#35745;&#25968;&#35770;&#35777;&#23548;&#20986;&#20102;&#36825;&#20123;&#38750;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#31934;&#30830;&#25968;&#37327;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the landscape of the negative log-likelihood function of Gaussian Mixture Models (GMMs) with a general number of components in the population limit. As the objective function is non-convex, there can be multiple local minima that are not globally optimal, even for well-separated mixture models. Our study reveals that all local minima share a common structure that partially identifies the cluster centers (i.e., means of the Gaussian components) of the true location mixture. Specifically, each local minimum can be represented as a non-overlapping combination of two types of sub-configurations: fitting a single mean estimate to multiple Gaussian components or fitting multiple estimates to a single true component. These results apply to settings where the true mixture components satisfy a certain separation condition, and are valid even when the number of components is overor under-specified. We also present a more fine-grained analysis for the setting of one-dimensional G
&lt;/p&gt;</description></item></channel></rss>