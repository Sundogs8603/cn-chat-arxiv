<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01293</link><description>&lt;p&gt;
MLLMs&#33021;&#21542;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can MLLMs Perform Text-to-Image In-Context Learning?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;CoBSAT&#12290;&#30740;&#31350;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#23637;&#21040;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#25512;&#21160;&#20102;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25193;&#23637;&#21040;&#22810;&#27169;&#24335;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;ICL&#19978;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;ICL&#65288;T2I-ICL&#65289;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24615;&#21644;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#20294;&#20173;&#28982;&#23569;&#26377;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;T2I-ICL&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;CoBSAT&#65292;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#20219;&#21153;&#30340;T2I-ICL&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;MLLMs&#65292;&#25105;&#20204;&#21457;&#29616;MLLMs&#22312;&#35299;&#20915;T2I-ICL&#38382;&#39064;&#26102;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22810;&#27169;&#24577;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#24494;&#35843;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#31574;&#30053;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2404.02138</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Topic-based Watermarks for LLM-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#36755;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#38590;&#20197;&#20998;&#36776;&#12290;&#27700;&#21360;&#31639;&#27861;&#26159;&#28508;&#22312;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#23884;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#21487;&#20197;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#22312;&#24050;&#30693;&#25915;&#20987;&#19979;&#32570;&#20047;&#20581;&#22766;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;LLM&#27599;&#22825;&#29983;&#25104;&#25968;&#19975;&#20010;&#25991;&#26412;&#36755;&#20986;&#65292;&#27700;&#21360;&#31639;&#27861;&#38656;&#35201;&#35760;&#24518;&#27599;&#20010;&#36755;&#20986;&#25165;&#33021;&#35753;&#26816;&#27979;&#27491;&#24120;&#24037;&#20316;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;&#8220;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#8221;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#24072;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;DE-HNN&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#30005;&#36335;&#32593;&#34920;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#20248;&#21270;&#24037;&#20855;&#36816;&#34892;&#26102;&#38388;&#38271;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00477</link><description>&lt;p&gt;
DE-HNN: &#19968;&#31181;&#29992;&#20110;&#30005;&#36335;&#32593;&#34920;&#34920;&#31034;&#30340;&#26377;&#25928;&#31070;&#32463;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DE-HNN: An effective neural model for Circuit Netlist representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00477
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#24072;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;DE-HNN&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#30005;&#36335;&#32593;&#34920;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#20248;&#21270;&#24037;&#20855;&#36816;&#34892;&#26102;&#38388;&#38271;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#24037;&#20855;&#30340;&#36816;&#34892;&#26102;&#38388;&#38543;&#30528;&#35774;&#35745;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#32780;&#22686;&#38271;&#65292;&#21040;&#20102;&#21487;&#20197;&#33457;&#36153;&#25968;&#22825;&#26469;&#23436;&#25104;&#19968;&#20010;&#35774;&#35745;&#21608;&#26399;&#30340;&#22320;&#27493;&#65292;&#36825;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#29942;&#39048;&#12290;&#35774;&#35745;&#24072;&#20204;&#24076;&#26395;&#33021;&#22815;&#24555;&#36895;&#33719;&#24471;&#35774;&#35745;&#21453;&#39304;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;&#20351;&#29992;&#36807;&#21435;&#35774;&#35745;&#30340;&#24037;&#20855;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#25968;&#25454;&#65292;&#21487;&#20197;&#23581;&#35797;&#26500;&#24314;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#26174;&#33879;&#36739;&#30701;&#30340;&#26102;&#38388;&#39044;&#27979;&#35774;&#35745;&#32467;&#26524;&#65292;&#36825;&#27604;&#36816;&#34892;&#24037;&#20855;&#35201;&#24555;&#24471;&#22810;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21463;&#21040;&#35774;&#35745;&#25968;&#25454;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#26159;&#25551;&#36848;&#25968;&#23383;&#30005;&#36335;&#20803;&#32032;&#21450;&#20854;&#36830;&#25509;&#26041;&#24335;&#30340;&#32593;&#34920;&#12290;&#32593;&#34920;&#30340;&#22270;&#34920;&#31034;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#20110;&#36825;&#31181;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33410;&#28857;&#25968;&#37327;&#20247;&#22810;&#21644;&#36828;&#31243;&#36830;&#25509;&#30340;&#37325;&#35201;&#24615;&#65292;&#32593;&#34920;&#30340;&#29305;&#24615;&#32473;&#29616;&#26377;&#22270;&#23398;&#20064;&#26694;&#26550;&#24102;&#26469;&#20102;&#20960;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00477v1 Announce Type: new  Abstract: The run-time for optimization tools used in chip design has grown with the complexity of designs to the point where it can take several days to go through one design cycle which has become a bottleneck. Designers want fast tools that can quickly give feedback on a design. Using the input and output data of the tools from past designs, one can attempt to build a machine learning model that predicts the outcome of a design in significantly shorter time than running the tool. The accuracy of such models is affected by the representation of the design data, which is usually a netlist that describes the elements of the digital circuit and how they are connected. Graph representations for the netlist together with graph neural networks have been investigated for such models. However, the characteristics of netlists pose several challenges for existing graph learning frameworks, due to the large number of nodes and the importance of long-range 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.00231</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24418;&#29366;&#21464;&#24418;&#32593;&#32476;&#29992;&#20110;&#26080;&#20266;&#24433;&#20960;&#20309;&#37325;&#26500;&#39592;&#30406;&#33136;&#26894;MR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00231
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33136;&#26894;&#26894;&#38388;&#30424;&#36864;&#21464;&#65292;&#26159;&#33136;&#26894;&#38388;&#30424;&#28176;&#36827;&#24615;&#32467;&#26500;&#24615;&#30952;&#25439;&#65292;&#34987;&#35748;&#20026;&#22312;&#33136;&#37096;&#30140;&#30171;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20840;&#29699;&#20581;&#24247;&#20851;&#27880;&#28966;&#28857;&#12290;&#20174;MR&#22270;&#20687;&#20013;&#33258;&#21160;&#37325;&#24314;&#33136;&#26894;&#20960;&#20309;&#24418;&#29366;&#65292;&#23558;&#20351;&#21307;&#23398;&#21442;&#25968;&#30340;&#24555;&#36895;&#27979;&#37327;&#25104;&#20026;&#21487;&#33021;&#65292;&#20197;&#35780;&#20272;&#33136;&#26894;&#29366;&#24577;&#65292;&#20174;&#32780;&#30830;&#23450;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#25216;&#26415;&#36890;&#24120;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#20998;&#21106;&#25110;&#19981;&#36866;&#21512;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#30340;&#26080;&#32467;&#26500;&#28857;&#20113;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransDeformer&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#39640;&#31354;&#38388;&#20934;&#30830;&#24230;&#21644;&#24739;&#32773;&#38388;&#32593;&#26684;&#23545;&#24212;&#30340;&#26041;&#24335;&#37325;&#24314;&#33136;&#26894;&#36718;&#24275;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;TransDeformer&#30340;&#21464;&#31181;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26032;&#30340;&#27880;&#24847;&#21147;&#20844;&#24335;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#21644;&#26631;&#35760;&#21270;&#30340;&#36718;&#24275;&#29305;&#24449;&#38598;&#25104;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00231v1 Announce Type: cross  Abstract: Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18742</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Learning Dynamics of Alignment with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#27169;&#22411;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#34429;&#28982;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29702;&#35770;&#19978;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#27491;&#24335;&#23637;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36824;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#29616;&#35937;&#65292;&#21363;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24403;&#20195;LLMs&#21644;&#23545;&#40784;&#20219;&#21153;&#19978;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24378;&#21270;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23545;&#40784;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#20813;&#36131;&#22768;&#26126;&#65306;&#26412;&#25991;&#21253;&#21547;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#65292;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#25552;&#21462;&#23494;&#38598;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;Triplet SSM&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.17701</link><description>&lt;p&gt;
&#26059;&#36716;&#25195;&#25551;&#65306;&#24102;&#26377;&#19977;&#20803;SSM&#27169;&#22359;&#30340;UNet-like Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#65292;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#25552;&#21462;&#23494;&#38598;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;Triplet SSM&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#21344;&#25454;&#37325;&#35201;&#20301;&#32622;&#12290;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#27169;&#22411;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#30001;&#20110;&#26377;&#38480;&#24863;&#21463;&#37326;&#25110;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#65292;&#29305;&#21035;&#26159;Mamba&#21450;&#20854;&#21464;&#20307;&#65292;&#22312;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21487;&#33021;&#19981;&#22815;&#26377;&#25928;&#65292;&#20445;&#30041;&#20102;&#19968;&#20123;&#20887;&#20313;&#32467;&#26500;&#65292;&#30041;&#19979;&#20102;&#21442;&#25968;&#20943;&#23569;&#30340;&#31354;&#38388;&#12290;&#21463;&#20808;&#21069;&#30340;&#31354;&#38388;&#21644;&#36890;&#36947;&#27880;&#24847;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#26469;&#25552;&#21462;&#23494;&#38598;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#21516;&#26102;&#21033;&#29992;Triplet SSM&#26469;&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;ISIC17&#12289;ISIC18&#12289;CVC-300&#12289;CVC-ClinicDB&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17701v1 Announce Type: cross  Abstract: Image segmentation holds a vital position in the realms of diagnosis and treatment within the medical domain. Traditional convolutional neural networks (CNNs) and Transformer models have made significant advancements in this realm, but they still encounter challenges because of limited receptive field or high computing complexity. Recently, State Space Models (SSMs), particularly Mamba and its variants, have demonstrated notable performance in the field of vision. However, their feature extraction methods may not be sufficiently effective and retain some redundant structures, leaving room for parameter reduction. Motivated by previous spatial and channel attention methods, we propose Triplet Mamba-UNet. The method leverages residual VSS Blocks to extract intensive contextual features, while Triplet SSM is employed to fuse features across spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18, CVC-300, CVC-ClinicDB, 
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26679;&#26412;&#30340;&#21516;&#26102;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#20445;&#35777;</title><link>https://arxiv.org/abs/2403.14421</link><description>&lt;p&gt;
&#23558;&#25193;&#25955;&#27169;&#22411;&#35843;&#25972;&#21040;&#31169;&#26377;&#39046;&#22495;&#32780;&#26080;&#38656;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14421
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26679;&#26412;&#30340;&#21516;&#26102;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14421v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25991;&#25688;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#23384;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#22797;&#21046;&#20986;&#19982;&#20854;&#35757;&#32451;&#22270;&#20687;&#20960;&#20046;&#23436;&#20840;&#30456;&#21516;&#30340;&#21103;&#26412;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#24076;&#26395;&#30475;&#21040;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26679;&#26412;&#24182;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#20445;&#35777;&#30340;&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#22312;&#23569;&#37327;&#20844;&#20849;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#19968;&#20010;DP&#26816;&#32034;&#26426;&#21046;&#65292;&#20197;&#20174;&#31169;&#26377;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#30340;&#26679;&#26412;&#26469;&#22686;&#24378;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;\emph{&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;}&#65288;DP-RDM&#65289;&#22312;&#36866;&#24212;&#21478;&#19968;&#20010;&#39046;&#22495;&#26102;&#26080;&#38656;&#23545;&#26816;&#32034;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#26679;&#26412;&#65292;&#21516;&#26102;&#28385;&#36275;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;&#20363;&#22914;&#65292;&#22312;&#35780;&#20272;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14421v1 Announce Type: new  Abstract: Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on M
&lt;/p&gt;</description></item><item><title>AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13269</link><description>&lt;p&gt;
AFLoRA: &#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#22312;&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13269
&lt;/p&gt;
&lt;p&gt;
AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#65288;AFLoRA&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#27599;&#20010;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#26435;&#37325;&#24352;&#37327;&#65292;&#25105;&#20204;&#28155;&#21152;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24182;&#34892;&#36335;&#24452;&#65292;&#21363;&#19979;&#25237;&#24433;&#21644;&#19978;&#25237;&#24433;&#30697;&#38453;&#65292;&#27599;&#20010;&#30697;&#38453;&#21518;&#38754;&#36319;&#30528;&#19968;&#20010;&#29305;&#24449;&#21464;&#25442;&#21521;&#37327;&#12290;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#20923;&#32467;&#20998;&#25968;&#65292;&#25105;&#20204;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36880;&#27493;&#20923;&#32467;&#36825;&#20123;&#25237;&#24433;&#30697;&#38453;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#24182;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25913;&#21892;&#39640;&#36798;0.85&#65285;&#65292;&#21516;&#26102;&#21487;&#20943;&#23569;&#39640;&#36798;9.5&#20493;&#30340;&#24179;&#22343;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#65292;&#19982;&#31867;&#20284;&#30340;PEFT&#22791;&#36873;&#26041;&#26696;&#30456;&#27604;&#65292;AFLoRA&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;1.86&#20493;&#30340;&#25913;&#36827;&#12290;&#38500;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13269v1 Announce Type: new  Abstract: We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the train
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ABCD&#35268;&#21017;&#65292;&#23558;&#21442;&#25968;&#20943;&#23569;&#20102;&#20174;$5W$&#21040;$5N$&#12290;</title><link>https://arxiv.org/abs/2403.12076</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuron-centric Hebbian Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ABCD&#35268;&#21017;&#65292;&#23558;&#21442;&#25968;&#20943;&#23569;&#20102;&#20174;$5W$&#21040;$5N$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#23398;&#20064;&#26426;&#21046;&#32972;&#21518;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#33021;&#21147;&#20043;&#19968;&#26159;&#36890;&#36807;&#32467;&#26500;&#21644;&#21151;&#33021;&#21487;&#22609;&#24615;&#35843;&#25972;&#20854;&#31361;&#35302;&#12290;&#23613;&#31649;&#31361;&#35302;&#22312;&#20256;&#36882;&#20449;&#24687;&#21040;&#25972;&#20010;&#22823;&#33041;&#20013;&#36215;&#30528;&#22522;&#26412;&#20316;&#29992;&#65292;&#20294;&#20960;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#26159;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20135;&#29983;&#20102;&#23545;&#31361;&#35302;&#30340;&#25913;&#21464;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20026;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#35774;&#35745;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#65292;&#22914;ABCD&#35268;&#21017;&#65292;&#20391;&#37325;&#20110;&#31361;&#35302;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#65292;&#22240;&#27492;&#20248;&#21270;&#31361;&#35302;&#29305;&#23450;&#30340;&#36203;&#24067;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22686;&#21152;&#20102;&#20248;&#21270;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#31361;&#35302;&#37117;&#19982;&#22810;&#20010;&#36203;&#24067;&#21442;&#25968;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#65292;&#31216;&#20026;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#65288;NcHL&#65289;&#65292;&#20854;&#20248;&#21270;&#20391;&#37325;&#20110;&#31070;&#32463;&#20803;&#32780;&#19981;&#26159;&#31361;&#35302;&#29305;&#23450;&#30340;&#36203;&#24067;&#21442;&#25968;&#12290;&#19982;ABCD&#35268;&#21017;&#30456;&#27604;&#65292;NcHL&#23558;&#21442;&#25968;&#20943;&#23569;&#20174;$5W$&#21040;$5N$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12076v1 Announce Type: cross  Abstract: One of the most striking capabilities behind the learning mechanisms of the brain is the adaptation, through structural and functional plasticity, of its synapses. While synapses have the fundamental role of transmitting information across the brain, several studies show that it is the neuron activations that produce changes on synapses. Yet, most plasticity models devised for artificial Neural Networks (NNs), e.g., the ABCD rule, focus on synapses, rather than neurons, therefore optimizing synaptic-specific Hebbian parameters. This approach, however, increases the complexity of the optimization process since each synapse is associated to multiple Hebbian parameters. To overcome this limitation, we propose a novel plasticity model, called Neuron-centric Hebbian Learning (NcHL), where optimization focuses on neuron- rather than synaptic-specific Hebbian parameters. Compared to the ABCD rule, NcHL reduces the parameters from $5W$ to $5N$
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22270;&#21516;&#24577;&#30340;&#29109;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#35270;&#35282;&#65292;&#25512;&#23548;&#20986;&#20102;&#36866;&#29992;&#20110;&#22270;&#21644;&#33410;&#28857;&#20998;&#31867;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#22270;&#32467;&#26500;&#30340;&#32454;&#24494;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#26694;&#26550;&#21051;&#30011;&#20102;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.06079</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#24577;&#30340;&#35282;&#24230;&#25512;&#24191;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalization of Graph Neural Networks through the Lens of Homomorphism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22270;&#21516;&#24577;&#30340;&#29109;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#35270;&#35282;&#65292;&#25512;&#23548;&#20986;&#20102;&#36866;&#29992;&#20110;&#22270;&#21644;&#33410;&#28857;&#20998;&#31867;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#22270;&#32467;&#26500;&#30340;&#32454;&#24494;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#26694;&#26550;&#21051;&#30011;&#20102;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24191;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#27867;&#21270;&#33021;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282; - &#20998;&#26512;&#22270;&#21516;&#24577;&#30340;&#29109;&#26469;&#30740;&#31350;GNNs&#30340;&#27867;&#21270;&#12290;&#36890;&#36807;&#23558;&#22270;&#21516;&#24577;&#19982;&#20449;&#24687;&#35770;&#24230;&#37327;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#38024;&#23545;&#22270;&#21644;&#33410;&#28857;&#20998;&#31867;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36825;&#20123;&#30028;&#38480;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#22270;&#32467;&#26500;&#22266;&#26377;&#30340;&#32454;&#24494;&#24046;&#24322;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#36335;&#24452;&#12289;&#29615;&#21644;&#22242;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#20855;&#26377;&#31283;&#20581;&#29702;&#35770;&#20445;&#35777;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;&#27867;&#21270;&#20998;&#26512;&#12290;&#20026;&#20102;&#38416;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#30028;&#38480;&#30340;&#26222;&#36866;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#21516;&#24577;&#30340;&#35270;&#35282;&#21051;&#30011;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#26174;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#19982;&#23454;&#38469;&#30340;&#19968;&#33268;&#24615;&#26469;&#39564;&#35777;&#25105;&#20204;&#29702;&#35770;&#21457;&#29616;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06079v1 Announce Type: new  Abstract: Despite the celebrated popularity of Graph Neural Networks (GNNs) across numerous applications, the ability of GNNs to generalize remains less explored. In this work, we propose to study the generalization of GNNs through a novel perspective - analyzing the entropy of graph homomorphism. By linking graph homomorphism with information-theoretic measures, we derive generalization bounds for both graph and node classifications. These bounds are capable of capturing subtleties inherent in various graph structures, including but not limited to paths, cycles and cliques. This enables a data-dependent generalization analysis with robust theoretical guarantees. To shed light on the generality of of our proposed bounds, we present a unifying framework that can characterize a broad spectrum of GNN models through the lens of graph homomorphism. We validate the practical applicability of our theoretical findings by showing the alignment between the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#25233;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#25913;&#36827;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#22522;&#20934;&#32447;&#30456;&#24403;&#30340;&#20219;&#21153;&#22870;&#21169;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.15650</link><description>&lt;p&gt;
&#20855;&#26377;&#30446;&#26631;&#25233;&#21046;&#30340;&#22810;&#32422;&#26463;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Constraint Safe RL with Objective Suppression for Safety-Critical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#25233;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#25913;&#36827;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#22522;&#20934;&#32447;&#30456;&#24403;&#30340;&#20219;&#21153;&#22870;&#21169;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38750;&#24120;&#24120;&#35265;&#65292;&#20294;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#30446;&#26631;&#25233;&#21046;&#65292;&#26681;&#25454;&#23433;&#20840;&#35780;&#21028;&#22120;&#33258;&#36866;&#24212;&#22320;&#25233;&#21046;&#20219;&#21153;&#22870;&#21169;&#26368;&#22823;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#23545;&#30446;&#26631;&#25233;&#21046;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19968;&#20010;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20219;&#20309;&#38169;&#35823;&#30340;&#34892;&#20026;&#37117;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#26174;&#33879;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#21305;&#37197;&#25105;&#20204;&#30340;&#22522;&#20934;&#32447;&#25152;&#36798;&#21040;&#30340;&#20219;&#21153;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15650v1 Announce Type: cross  Abstract: Safe reinforcement learning tasks with multiple constraints are a challenging domain despite being very common in the real world. To address this challenge, we propose Objective Suppression, a novel method that adaptively suppresses the task reward maximizing objectives according to a safety critic. We benchmark Objective Suppression in two multi-constraint safety domains, including an autonomous driving domain where any incorrect behavior can lead to disastrous consequences. Empirically, we demonstrate that our proposed method, when combined with existing safe RL algorithms, can match the task reward achieved by our baselines with significantly fewer constraint violations.
&lt;/p&gt;</description></item><item><title>APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;</title><link>https://arxiv.org/abs/2402.14866</link><description>&lt;p&gt;
APTQ: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14866
&lt;/p&gt;
&lt;p&gt;
APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#39640;&#35745;&#31639;&#36127;&#36733;&#21644;&#24040;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#23545;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;APTQ&#65288;Attention-aware Post-Training Mixed-Precision Quantization&#65289;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#20102;&#27599;&#23618;&#26435;&#37325;&#30340;&#20108;&#38454;&#20449;&#24687;&#65292;&#32780;&#19988;&#39318;&#27425;&#32771;&#34385;&#20102;&#27880;&#24847;&#21147;&#36755;&#20986;&#23545;&#25972;&#20010;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#21033;&#29992;Hessian&#36857;&#20316;&#20026;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25935;&#24863;&#24230;&#24230;&#37327;&#65292;&#30830;&#20445;&#32463;&#36807;&#29702;&#24615;&#30340;&#31934;&#24230;&#38477;&#20302;&#33021;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;APTQ&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;C4&#25968;&#25454;&#38598;&#20013;&#20197;&#24179;&#22343;4&#20301;&#23485;&#24230;&#33719;&#24471;5.22&#22256;&#24785;&#24230;&#65292;&#20960;&#20046;&#31561;&#25928;&#20110;&#20840;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;APTQ&#22312;LLaMa-7B&#21644;LLaMa-1&#20013;&#20197;&#24179;&#22343;3.8&#20301;&#23485;&#24230;&#36798;&#21040;&#20102;68.24&#65285;&#21644;70.48&#65285;&#30340;&#26368;&#20808;&#36827;&#38646;-shot&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21644;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#20316;&#21697;&#65292;&#24182;&#25351;&#20986;&#33258;2015&#24180;&#20197;&#26469;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#24320;&#22987;&#20559;&#21521;&#20110;&#20351;&#29992;LLRMA&#65292;&#26174;&#31034;&#20854;&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14045</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#25512;&#36827;&#20302;&#31209;&#21644;&#23616;&#37096;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical Imaging: A Systematic Literature Review and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21644;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#20316;&#21697;&#65292;&#24182;&#25351;&#20986;&#33258;2015&#24180;&#20197;&#26469;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#24320;&#22987;&#20559;&#21521;&#20110;&#20351;&#29992;LLRMA&#65292;&#26174;&#31034;&#20854;&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#30340;&#22823;&#23481;&#37327;&#21644;&#22797;&#26434;&#24615;&#26159;&#23384;&#20648;&#12289;&#20256;&#36755;&#21644;&#22788;&#29702;&#30340;&#29942;&#39048;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21450;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#24212;&#29992;&#24050;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#23637;&#31034;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;LRMA&#21644;LLRMA&#30340;&#20316;&#21697;&#12290;&#25991;&#29486;&#30340;&#35814;&#32454;&#20998;&#26512;&#30830;&#35748;&#20102;&#24212;&#29992;&#20110;&#21508;&#31181;&#25104;&#20687;&#27169;&#24577;&#30340;LRMA&#21644;LLRMA&#26041;&#27861;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#29616;&#26377;LRMA&#21644;LLRMA&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#33258;2015&#24180;&#20197;&#26469;&#65292;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26126;&#26174;&#20559;&#21521;&#20110;LLRMA&#65292;&#26174;&#31034;&#20102;&#30456;&#23545;&#20110;LRMA&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;LLRMA&#25152;&#20351;&#29992;&#30340;&#27973;&#23618;&#30456;&#20284;&#24615;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20808;&#36827;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#26469;&#22788;&#29702;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14045v1 Announce Type: cross  Abstract: The large volume and complexity of medical imaging datasets are bottlenecks for storage, transmission, and processing. To tackle these challenges, the application of low-rank matrix approximation (LRMA) and its derivative, local LRMA (LLRMA) has demonstrated potential.   This paper conducts a systematic literature review to showcase works applying LRMA and LLRMA in medical imaging. A detailed analysis of the literature identifies LRMA and LLRMA methods applied to various imaging modalities. This paper addresses the challenges and limitations associated with existing LRMA and LLRMA methods.   We note a significant shift towards a preference for LLRMA in the medical imaging field since 2015, demonstrating its potential and effectiveness in capturing complex structures in medical data compared to LRMA. Acknowledging the limitations of shallow similarity methods used with LLRMA, we suggest advanced semantic image segmentation for similarit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FMTL-Bench&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;FMTL&#65289;&#33539;&#24335;&#65292;&#22635;&#34917;&#20102;FL&#21644;MTL&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.12876</link><description>&lt;p&gt;
&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23396;&#23707;&#19978;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65306;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12876
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FMTL-Bench&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;FMTL&#65289;&#33539;&#24335;&#65292;&#22635;&#34917;&#20102;FL&#21644;MTL&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;FMTL&#65289;&#26041;&#27861;&#25972;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#20248;&#28857;&#65292;&#33021;&#22815;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#35813;&#39046;&#22495;&#32570;&#20047;&#25972;&#21512;FL&#21644;MTL&#29420;&#29305;&#29305;&#24615;&#30340;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;FMTL-Bench&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;FMTL&#33539;&#24335;&#12290;&#36825;&#20010;&#22522;&#20934;&#28085;&#30422;&#20102;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#20248;&#21270;&#31639;&#27861;&#32423;&#21035;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#24182;&#21253;&#25324;&#19971;&#32452;&#27604;&#36739;&#23454;&#39564;&#65292;&#23553;&#35013;&#20102;&#24191;&#27867;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;Non-IID&#65289;&#25968;&#25454;&#20998;&#21306;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21508;&#31181;&#25351;&#26631;&#30340;&#22522;&#32447;&#30340;&#31995;&#32479;&#36807;&#31243;&#65292;&#24182;&#23545;&#36890;&#20449;&#24320;&#25903;&#12289;&#26102;&#38388;&#21644;&#33021;&#28304;&#28040;&#32791;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12876v1 Announce Type: new  Abstract: The innovative Federated Multi-Task Learning (FMTL) approach consolidates the benefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling collaborative model training on multi-task learning datasets. However, a comprehensive evaluation method, integrating the unique features of both FL and MTL, is currently absent in the field. This paper fills this void by introducing a novel framework, FMTL-Bench, for systematic evaluation of the FMTL paradigm. This benchmark covers various aspects at the data, model, and optimization algorithm levels, and comprises seven sets of comparative experiments, encapsulating a wide array of non-independent and identically distributed (Non-IID) data partitioning scenarios. We propose a systematic process for comparing baselines of diverse indicators and conduct a case study on communication expenditure, time, and energy consumption. Through our exhaustive experiments, we aim to provide valuable
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11809</link><description>&lt;p&gt;
&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#65306;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#21152;&#36895;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#8221;&#65288;SPACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;LLMs&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#33021;&#21147;&#65292;SPACE&#29420;&#29305;&#22320;&#20351;&#33258;&#22238;&#24402;LLMs&#33021;&#22815;&#24182;&#34892;&#29983;&#25104;&#21644;&#39564;&#35777;&#20196;&#29260;&#12290;&#36825;&#26159;&#36890;&#36807;&#19987;&#38376;&#30340;&#21322;&#33258;&#22238;&#24402;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29616;&#26377;LLMs&#20855;&#26377;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#31639;&#27861;&#20419;&#36827;&#20102;&#21333;&#20010;&#27169;&#22411;&#35843;&#29992;&#20869;&#20196;&#29260;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;LLMs&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;SPACE&#22312;HumanEval-X&#19978;&#34920;&#29616;&#20986;2.7&#20493;&#33267;4.0&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.08193</link><description>&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08193
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#65288;GEnBP&#65289;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#65288;GaBP&#65289;&#26041;&#27861;&#30340;&#32467;&#21512;&#12290;GEnBP&#36890;&#36807;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20256;&#36882;&#20302;&#31209;&#26412;&#22320;&#20449;&#24687;&#26469;&#26356;&#26032;&#38598;&#25104;&#27169;&#22411;&#12290;&#36825;&#31181;&#32452;&#21512;&#32487;&#25215;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26377;&#21033;&#29305;&#24615;&#12290;&#38598;&#25104;&#25216;&#26415;&#20351;&#24471;GEnBP&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#12289;&#22024;&#26434;&#30340;&#40657;&#31665;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20351;&#29992;&#26412;&#22320;&#20449;&#24687;&#30830;&#20445;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#24182;&#33021;&#39640;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#24403;&#38598;&#25104;&#22823;&#23567;&#36828;&#23567;&#20110;&#25512;&#26029;&#32500;&#24230;&#26102;&#65292;GEnBP&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#31354;&#26102;&#24314;&#27169;&#12289;&#22270;&#20687;&#22788;&#29702;&#21644;&#29289;&#29702;&#27169;&#22411;&#21453;&#28436;&#31561;&#39046;&#22495;&#32463;&#24120;&#20986;&#29616;&#12290;GEnBP&#21487;&#20197;&#24212;&#29992;&#20110;&#19968;&#33324;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.07938</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#29992;&#25143;&#30028;&#38754;&#65306;&#30001;LLMs&#39537;&#21160;&#30340;&#35821;&#38899;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#36825;&#20123;&#26032;&#21457;&#29616;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#26032;&#19968;&#20195;&#36719;&#20214;&#30340;&#35806;&#29983;&#65292;&#27491;&#22914;&#23427;&#20204;&#22312;&#24037;&#19994;&#30028;&#26080;&#25968;&#24212;&#29992;&#20013;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#12290;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;LLM&#24341;&#25806;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#20998;&#31867;&#26368;&#26377;&#21487;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#35782;&#21035;&#25152;&#38656;&#30340;UI&#32452;&#20214;&#65292;&#24182;&#38543;&#21518;&#25191;&#34892;&#29992;&#25143;&#26399;&#26395;&#30340;&#25805;&#20316;&#12290;&#36825;&#31181;&#38598;&#25104;&#21487;&#20197;&#23558;&#38745;&#24577;UI&#31995;&#32479;&#21457;&#23637;&#25104;&#39640;&#24230;&#21160;&#24577;&#21644;&#21487;&#36866;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24341;&#20837;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#39046;&#22495;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#29992;&#25143;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#26497;&#22823;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent meteoric advancements in large language models have showcased a remarkable capacity for logical reasoning and comprehension. These newfound capabilities have opened the door to a new generation of software, as has been made obvious through the innumerable ways they are being applied in the industry. This research focuses on harnessing and guiding the upgraded power of LLMs to construct a framework that can serve as an intermediary between a user and their user interface. By comprehending a user's needs through a thorough analysis of natural textual inputs, an effectively crafted LLM engine can classify the most likely available application, identify the desired UI component and subsequently execute the user's expected actions. This integration can evolve static UI systems into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences. Such a framework can fundamentally shift how users accomplish daily tasks, skyrocket e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20351;&#29992;&#19981;&#21516;&#36817;&#20284;&#26041;&#27861;&#30340;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#25216;&#26415;&#65292;&#20197;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#65292;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20851;&#38190;&#29305;&#24449;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2401.17541</link><description>&lt;p&gt;
&#36879;&#36807;&#26657;&#20934;&#30340;&#35270;&#35282;&#29702;&#35299;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20351;&#29992;&#19981;&#21516;&#36817;&#20284;&#26041;&#27861;&#30340;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#25216;&#26415;&#65292;&#20197;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#65292;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20851;&#38190;&#29305;&#24449;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26159;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#27979;&#35797;&#20998;&#24067;&#24448;&#24448;&#19982;&#35757;&#32451;&#19981;&#21516;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#36234;&#22495;&#27867;&#21270;&#65292;&#22312;&#24120;&#35268;&#27169;&#22411;&#38754;&#20020;&#25361;&#25112;&#12290;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#20316;&#20026;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#26088;&#22312;&#35782;&#21035;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#20445;&#25345;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#36234;&#22495;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;IRM&#30340;&#22797;&#26434;&#24615;&#65292;&#29305;&#21035;&#26159;&#20854;&#21452;&#23618;&#20248;&#21270;&#65292;&#23548;&#33268;&#20102;&#21508;&#31181;&#36817;&#20284;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#36825;&#20123;&#36817;&#20284;IRM&#25216;&#26415;&#65292;&#20351;&#29992;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#12290;ECE&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#23427;&#26159;&#34913;&#37327;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#25429;&#25417;&#21040;&#29615;&#22659;&#19981;&#21464;&#29305;&#24449;&#30340;&#25351;&#26631;&#12290;&#36890;&#36807;&#23545;&#20855;&#26377;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20102;...&#65288;&#25509;&#19979;&#37096;&#20998;&#25688;&#35201;&#36229;&#36807;200&#23383;&#65292;&#25552;&#21462;&#21069;200&#23383;&#65289;
&lt;/p&gt;
&lt;p&gt;
Machine learning models traditionally assume that training and test data are independently and identically distributed. However, in real-world applications, the test distribution often differs from training. This problem, known as out-of-distribution generalization, challenges conventional models. Invariant Risk Minimization (IRM) emerges as a solution, aiming to identify features invariant across different environments to enhance out-of-distribution robustness. However, IRM's complexity, particularly its bi-level optimization, has led to the development of various approximate methods. Our study investigates these approximate IRM techniques, employing the Expected Calibration Error (ECE) as a key metric. ECE, which measures the reliability of model prediction, serves as an indicator of whether models effectively capture environment-invariant features. Through a comparative analysis of datasets with distributional shifts, we observe that Information Bottleneck-based IRM, which condenses
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#23454;&#26102;&#39044;&#27979;&#38271;&#36317;&#31163;&#22320;&#24418;&#39640;&#31243;&#22320;&#22270;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;transformer-based&#32534;&#30721;&#22120;&#12289;&#26041;&#21521;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#21382;&#21490;&#22686;&#24378;&#30340;&#21487;&#23398;&#20064;&#22320;&#22270;&#23884;&#20837;&#12290;&#36890;&#36807;&#23398;&#20064;&#35270;&#35282;&#22270;&#20687;&#19982;&#40479;&#30640;&#22270;&#39640;&#31243;&#22320;&#22270;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#32467;&#21512;&#36710;&#36742;&#23039;&#24577;&#20449;&#24687;&#21644;&#35270;&#35273;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#22320;&#22270;&#39044;&#27979;&#26102;&#24207;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17484</link><description>&lt;p&gt;
&#20687;&#32032;&#21040;&#39640;&#31243;&#65306;&#20351;&#29992;&#22270;&#20687;&#23398;&#20064;&#39044;&#27979;&#33258;&#20027;&#36234;&#37326;&#23548;&#33322;&#20013;&#30340;&#38271;&#36317;&#31163;&#39640;&#31243;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Pixel to Elevation: Learning to Predict Elevation Maps at Long Range using Images for Autonomous Offroad Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#23454;&#26102;&#39044;&#27979;&#38271;&#36317;&#31163;&#22320;&#24418;&#39640;&#31243;&#22320;&#22270;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;transformer-based&#32534;&#30721;&#22120;&#12289;&#26041;&#21521;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#21382;&#21490;&#22686;&#24378;&#30340;&#21487;&#23398;&#20064;&#22320;&#22270;&#23884;&#20837;&#12290;&#36890;&#36807;&#23398;&#20064;&#35270;&#35282;&#22270;&#20687;&#19982;&#40479;&#30640;&#22270;&#39640;&#31243;&#22320;&#22270;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#32467;&#21512;&#36710;&#36742;&#23039;&#24577;&#20449;&#24687;&#21644;&#35270;&#35273;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#22320;&#22270;&#39044;&#27979;&#26102;&#24207;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#38271;&#36317;&#31163;&#29702;&#35299;&#22320;&#24418;&#25299;&#25169;&#23545;&#20110;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#36895;&#23548;&#33322;&#26102;&#12290;&#30446;&#21069;&#65292;&#20960;&#20309;&#26144;&#23556;&#20027;&#35201;&#20381;&#36182;&#20110;LiDAR&#20256;&#24863;&#22120;&#65292;&#20294;&#22312;&#26356;&#36828;&#36317;&#31163;&#30340;&#26144;&#23556;&#26102;&#25552;&#20379;&#30340;&#27979;&#37327;&#25968;&#36739;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20165;&#20351;&#29992;&#23454;&#26102;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#39044;&#27979;&#38271;&#36317;&#31163;&#22320;&#24418;&#39640;&#31243;&#22320;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#19977;&#20010;&#20027;&#35201;&#20803;&#32032;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#23398;&#20064;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#19982;&#20808;&#21069;&#30340;&#40479;&#30640;&#22270;&#39640;&#31243;&#22320;&#22270;&#39044;&#27979;&#20043;&#38388;&#30340;&#36328;&#35270;&#22270;&#20851;&#32852;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#21521;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#65292;&#23558;3D&#36710;&#36742;&#23039;&#24577;&#20449;&#24687;&#19982;&#22810;&#35270;&#35282;&#35270;&#35273;&#22270;&#20687;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#22320;&#24418;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#22686;&#24378;&#30340;&#21487;&#23398;&#20064;&#22320;&#22270;&#23884;&#20837;&#65292;&#20197;&#23454;&#29616;&#39640;&#31243;&#22320;&#22270;&#39044;&#27979;&#20043;&#38388;&#30340;&#26356;&#22909;&#26102;&#24207;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding terrain topology at long-range is crucial for the success of off-road robotic missions, especially when navigating at high-speeds. LiDAR sensors, which are currently heavily relied upon for geometric mapping, provide sparse measurements when mapping at greater distances. To address this challenge, we present a novel learning-based approach capable of predicting terrain elevation maps at long-range using only onboard egocentric images in real-time. Our proposed method is comprised of three main elements. First, a transformer-based encoder is introduced that learns cross-view associations between the egocentric views and prior bird-eye-view elevation map predictions. Second, an orientation-aware positional encoding is proposed to incorporate the 3D vehicle pose information over complex unstructured terrain with multi-view visual image features. Lastly, a history-augmented learn-able map embedding is proposed to achieve better temporal consistency between elevation map predi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#22270;&#19978;&#23454;&#29616;&#36328;&#39046;&#22495;&#27867;&#21270;&#65292;&#35299;&#20915;&#20102;&#28304;HG&#19982;&#30446;&#26631;HG&#20998;&#24067;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#30693;&#35782;&#20256;&#36882;&#26080;&#25928;&#21644;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.03597</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#24322;&#26500;&#22270;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Causal Representation Learning for Out-of-Distribution Generalization on Heterogeneous Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03597
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#22270;&#19978;&#23454;&#29616;&#36328;&#39046;&#22495;&#27867;&#21270;&#65292;&#35299;&#20915;&#20102;&#28304;HG&#19982;&#30446;&#26631;HG&#20998;&#24067;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#30693;&#35782;&#20256;&#36882;&#26080;&#25928;&#21644;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;HGFL&#65289;&#24050;&#32463;&#34987;&#30740;&#21457;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#65288;HGs&#65289;&#20013;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#21508;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;HGFL&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#20174;&#28304;HG&#20013;&#23500;&#26631;&#35760;&#31867;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#23558;&#36825;&#20123;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;HG&#20197;&#20419;&#36827;&#23398;&#20064;&#26032;&#31867;&#21035;&#65292;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#26368;&#32456;&#22312;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#28304;HG&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#37117;&#20849;&#20139;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#19977;&#31181;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#21464;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#21407;&#22240;&#26377;&#20004;&#20010;&#65306;&#65288;1&#65289;&#28304;HG&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#19982;&#30446;&#26631;HG&#20998;&#24067;&#21305;&#37197;&#65292;&#20197;&#21450;&#65288;2&#65289;&#30446;&#26631;HG&#30340;&#19981;&#21487;&#39044;&#27979;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#12290;&#36825;&#31181;&#20998;&#24067;&#36716;&#21464;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#20013;&#30693;&#35782;&#20256;&#36882;&#26080;&#25928;&#21644;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03597v2 Announce Type: replace  Abstract: Heterogeneous graph few-shot learning (HGFL) has been developed to address the label sparsity issue in heterogeneous graphs (HGs), which consist of various types of nodes and edges. The core concept of HGFL is to extract knowledge from rich-labeled classes in a source HG, transfer this knowledge to a target HG to facilitate learning new classes with few-labeled training data, and finally make predictions on unlabeled testing data. Existing methods typically assume that the source HG, training data, and testing data all share the same distribution. However, in practice, distribution shifts among these three types of data are inevitable due to two reasons: (1) the limited availability of the source HG that matches the target HG distribution, and (2) the unpredictable data generation mechanism of the target HG. Such distribution shifts result in ineffective knowledge transfer and poor learning performance in existing methods, thereby le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2401.16412</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#36827;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning to Manipulate under Limited Information. (arXiv:2401.16412v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#32463;&#20856;&#32467;&#26524;&#65292;&#20219;&#20309;&#21512;&#29702;&#30340;&#20559;&#22909;&#25237;&#31080;&#26041;&#27861;&#26377;&#26102;&#20250;&#32473;&#20010;&#20307;&#25552;&#20379;&#25253;&#21578;&#19981;&#30495;&#23454;&#20559;&#22909;&#30340;&#28608;&#21169;&#12290;&#23545;&#20110;&#27604;&#36739;&#25237;&#31080;&#26041;&#27861;&#26469;&#35828;&#65292;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26356;&#25110;&#32773;&#26356;&#23569;&#25269;&#25239;&#36825;&#31181;&#31574;&#30053;&#24615;&#25805;&#32437;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#35268;&#27169;&#19979;&#23545;&#38480;&#21046;&#20449;&#24687;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#32473;&#23450;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#30340;&#25104;&#21151;&#31243;&#24230;&#26469;&#34913;&#37327;&#25805;&#32437;&#30340;&#25269;&#25239;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#23558;&#36817;40,000&#20010;&#19981;&#21516;&#35268;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23545;&#25239;8&#31181;&#19981;&#21516;&#30340;&#25237;&#31080;&#26041;&#27861;&#65292;&#22312;6&#31181;&#38480;&#21046;&#20449;&#24687;&#24773;&#20917;&#19979;&#65292;&#36827;&#34892;&#21253;&#21547;5-21&#21517;&#36873;&#27665;&#21644;3-6&#21517;&#20505;&#36873;&#20154;&#30340;&#22996;&#21592;&#20250;&#35268;&#27169;&#36873;&#20030;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#25237;&#31080;&#26041;&#27861;&#65292;&#22914;Borda&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#21487;&#20197;&#34987;&#31070;&#32463;&#32593;&#32476;&#39640;&#24230;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;Instant Runoff&#26041;&#27861;&#65292;&#34429;&#28982;&#34987;&#19968;&#20010;&#29702;&#24819;&#30340;&#25805;&#32437;&#32773;&#21033;&#28070;&#21270;&#25805;&#32437;&#65292;&#20294;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#19981;&#20250;&#21463;&#21040;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31639;&#23376;&#26469;&#35299;&#20915;&#39640;&#36895;&#27969;&#21160;&#20013;&#30340;Riemann&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;DeepONet&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#65292;&#22312;&#26497;&#31471;&#21387;&#21147;&#36339;&#36291;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.08886</link><description>&lt;p&gt;
RiemannONets: &#21487;&#35299;&#37322;&#30340;&#29992;&#20110;Riemann&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
RiemannONets: Interpretable Neural Operators for Riemann Problems. (arXiv:2401.08886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31639;&#23376;&#26469;&#35299;&#20915;&#39640;&#36895;&#27969;&#21160;&#20013;&#30340;Riemann&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;DeepONet&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#65292;&#22312;&#26497;&#31471;&#21387;&#21147;&#36339;&#36291;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#20540;&#20998;&#26512;&#20013;&#65292;&#22914;&#20309;&#20026;&#27169;&#25311;&#20855;&#26377;&#24378;&#28608;&#27874;&#12289;&#31232;&#30095;&#21270;&#21644;&#25509;&#35302;&#38388;&#26029;&#30340;&#39640;&#36895;&#27969;&#21160;&#24320;&#21457;&#21512;&#36866;&#30340;&#34920;&#31034;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#31070;&#32463;&#31639;&#23376;&#26469;&#35299;&#20915;&#22312;&#21487;&#21387;&#32553;&#27969;&#20013;&#36935;&#21040;&#30340;Riemann&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#26497;&#31471;&#21387;&#21147;&#36339;&#36291;&#65288;&#39640;&#36798;$10^{10}$&#30340;&#21387;&#21147;&#27604;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;DeepONet&#65292;&#23427;&#26159;&#22312;Lee&#21644;Shin&#30340;&#26368;&#26032;&#24037;&#20316;&#22522;&#30784;&#19978;&#36827;&#34892;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20174;&#20027;&#24178;&#32593;&#32476;&#20013;&#25552;&#21462;&#20102;&#19968;&#20010;&#22522;&#30784;&#24182;&#20351;&#20854;&#27491;&#20132;&#21270;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#20998;&#25903;&#32593;&#32476;&#12290;&#36825;&#20010;&#23545;DeepONet&#30340;&#31616;&#21333;&#20462;&#25913;&#23545;&#20854;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#26377;&#30528;&#28145;&#36828;&#24433;&#21709;&#65292;&#30456;&#27604;&#21407;&#22987;&#29256;&#26412;&#65292;&#23427;&#25552;&#20379;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;Riemann&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29289;&#29702;&#19978;&#23545;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#65292;&#22240;&#20026;&#23618;&#27425;&#21270;&#30340;&#25968;&#25454;&#39537;&#21160;&#29983;&#25104;&#30340;&#22522;&#30784;&#21453;&#26144;&#20102;&#25152;&#26377;&#27969;&#21160;&#29305;&#24449;&#65292;&#21542;&#21017;&#23558;&#34987;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing the proper representations for simulating high-speed flows with strong shock waves, rarefactions, and contact discontinuities has been a long-standing question in numerical analysis. Herein, we employ neural operators to solve Riemann problems encountered in compressible flows for extreme pressure jumps (up to $10^{10}$ pressure ratio). In particular, we first consider the DeepONet that we train in a two-stage process, following the recent work of Lee and Shin, wherein the first stage, a basis is extracted from the trunk net, which is orthonormalized and subsequently is used in the second stage in training the branch net. This simple modification of DeepONet has a profound effect on its accuracy, efficiency, and robustness and leads to very accurate solutions to Riemann problems compared to the vanilla version. It also enables us to interpret the results physically as the hierarchical data-driven produced basis reflects all the flow features that would otherwise be introduce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.17722</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Generalizable Policies for Embodied Tasks. (arXiv:2310.17722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#35270;&#35273;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;(LLaRP)&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#30340;LLM&#35843;&#25972;&#20026;&#25509;&#25910;&#25991;&#26412;&#25351;&#20196;&#21644;&#35270;&#35273;&#33258;&#25105;&#20013;&#24515;&#35266;&#27979;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#30452;&#25509;&#22312;&#29615;&#22659;&#20013;&#36755;&#20986;&#21160;&#20316;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#35757;&#32451;LLaRP&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#30475;&#21644;&#34892;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLaRP&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;1,000&#20010;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#25104;&#21151;&#29575;&#36798;&#21040;&#20102;42%&#65292;&#26159;&#20854;&#20182;&#24120;&#35265;&#23398;&#20064;&#22522;&#32447;&#25110;&#38646;&#26679;&#26412;&#24212;&#29992;&#30340;1.7&#20493;&#25104;&#21151;&#29575;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#24110;&#21161;&#31038;&#21306;&#30740;&#31350;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#12289;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;AI&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#65292;&#21253;&#25324;150,000&#20010;&#35757;&#32451;&#20219;&#21153;&#21644;1,000&#20010;&#27979;&#35797;&#20219;&#21153;&#65292;&#29992;&#20110;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#37325;&#26032;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangem
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22312;&#31232;&#26377;&#31867;&#27010;&#29575;&#36235;&#36817;&#20110;&#38646;&#30340;&#24773;&#20917;&#19979;&#30340;&#26032;&#36129;&#29486;&#65292;&#20998;&#21035;&#26159;&#19968;&#31181;&#38750;&#28176;&#36817;&#24555;&#36895;&#29575;&#27010;&#29575;&#30028;&#38480;&#21644;&#19968;&#31181;&#19968;&#33268;&#19978;&#30028;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#20123;&#21457;&#29616;&#20026;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#31867;&#21035;&#21152;&#26435;&#25552;&#20379;&#20102;&#26356;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.14826</link><description>&lt;p&gt;
&#38024;&#23545;&#19981;&#24179;&#34913;&#20998;&#31867;&#30340;&#23574;&#38160;&#35823;&#24046;&#30028;&#65306;&#23569;&#25968;&#31867;&#20013;&#26377;&#22810;&#23569;&#26679;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
Sharp error bounds for imbalanced classification: how many examples in the minority class?. (arXiv:2310.14826v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22312;&#31232;&#26377;&#31867;&#27010;&#29575;&#36235;&#36817;&#20110;&#38646;&#30340;&#24773;&#20917;&#19979;&#30340;&#26032;&#36129;&#29486;&#65292;&#20998;&#21035;&#26159;&#19968;&#31181;&#38750;&#28176;&#36817;&#24555;&#36895;&#29575;&#27010;&#29575;&#30028;&#38480;&#21644;&#19968;&#31181;&#19968;&#33268;&#19978;&#30028;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#20123;&#21457;&#29616;&#20026;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#31867;&#21035;&#21152;&#26435;&#25552;&#20379;&#20102;&#26356;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#19981;&#24179;&#34913;&#20998;&#31867;&#25968;&#25454;&#26102;&#65292;&#37325;&#26032;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#26159;&#19968;&#31181;&#26631;&#20934;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#39118;&#38505;&#24230;&#37327;&#20013;&#24179;&#34913;&#30495;&#27491;&#30340;&#27491;&#20363;&#29575;&#21644;&#30495;&#27491;&#30340;&#36127;&#20363;&#29575;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#26377;&#37325;&#35201;&#30340;&#29702;&#35770;&#24037;&#20316;&#65292;&#20294;&#29616;&#26377;&#30340;&#32467;&#26524;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#19981;&#24179;&#34913;&#20998;&#31867;&#26694;&#26550;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#30456;&#23545;&#20110;&#25972;&#20010;&#26679;&#26412;&#22823;&#23567;&#26469;&#35828;&#19968;&#20010;&#31867;&#30340;&#21487;&#24573;&#30053;&#30340;&#22823;&#23567;&#20197;&#21450;&#38656;&#35201;&#36890;&#36807;&#36235;&#36817;&#20110;&#38646;&#30340;&#27010;&#29575;&#26469;&#37325;&#26032;&#35843;&#25972;&#39118;&#38505;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#31232;&#26377;&#31867;&#27010;&#29575;&#36235;&#36817;&#20110;&#38646;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36129;&#29486;&#65306;&#65288;1&#65289;&#29992;&#20110;&#32422;&#26463;&#24179;&#34913;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38750;&#28176;&#36817;&#24555;&#36895;&#29575;&#27010;&#29575;&#30028;&#38480;&#65292;&#20197;&#21450;&#65288;2&#65289;&#29992;&#20110;&#24179;&#34913;&#26368;&#36817;&#37051;&#20272;&#35745;&#30340;&#19968;&#33268;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26356;&#28165;&#26970;&#22320;&#35828;&#26126;&#20102;&#31867;&#21035;&#21152;&#26435;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#30410;&#22788;&#65292;&#20026;&#36825;&#20010;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
When dealing with imbalanced classification data, reweighting the loss function is a standard procedure allowing to equilibrate between the true positive and true negative rates within the risk measure. Despite significant theoretical work in this area, existing results do not adequately address a main challenge within the imbalanced classification framework, which is the negligible size of one class in relation to the full sample size and the need to rescale the risk function by a probability tending to zero. To address this gap, we present two novel contributions in the setting where the rare class probability approaches zero: (1) a non asymptotic fast rate probability bound for constrained balanced empirical risk minimization, and (2) a consistent upper bound for balanced nearest neighbors estimates. Our findings provide a clearer understanding of the benefits of class-weighting in realistic settings, opening new avenues for further research in this field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#21644;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#30340;&#38598;&#25104;&#23454;&#29616;&#23545;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#30340;&#36924;&#36817;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10745</link><description>&lt;p&gt;
Mori-Zwanzig&#28508;&#21464;&#31354;&#38388;Koopman&#38381;&#21253;&#29992;&#20110;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder. (arXiv:2310.10745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#21644;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#30340;&#38598;&#25104;&#23454;&#29616;&#23545;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#30340;&#36924;&#36817;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#65292;&#20351;&#20854;&#25104;&#20026;&#31616;&#21270;&#22797;&#26434;&#21160;&#21147;&#23398;&#29702;&#35299;&#30340;&#23453;&#36149;&#26041;&#27861;&#12290;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#36924;&#36817;&#26377;&#38480;Koopman&#31639;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#20363;&#22914;&#36873;&#25321;&#21512;&#36866;&#30340;&#21487;&#35266;&#23519;&#37327;&#12289;&#38477;&#32500;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#25552;&#21462;&#20851;&#38190;&#21487;&#35266;&#23519;&#37327;&#26469;&#36924;&#36817;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#38598;&#25104;&#38750;&#39532;&#23572;&#21487;&#22827;&#26657;&#27491;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#21464;&#27969;&#24418;&#20013;&#20135;&#29983;&#20102;&#21160;&#21147;&#23398;&#30340;&#23553;&#38381;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31934;&#30830;&#24615;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields a closed representation of dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the precision and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#36890;&#36807;&#22810;&#39033;&#30740;&#31350;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07023</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25366;&#25496;&#23439;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Automatic Macro Mining from Interaction Traces at Scale. (arXiv:2310.07023v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#36890;&#36807;&#22810;&#39033;&#30740;&#31350;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23439;&#20219;&#21153;&#26159;&#25105;&#20204;&#26085;&#24120;&#25163;&#26426;&#27963;&#21160;&#30340;&#26500;&#24314;&#22359;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#8220;&#30331;&#24405;&#8221;&#25110;&#8220;&#39044;&#23450;&#33322;&#29677;&#8221;&#65289;&#12290;&#26377;&#25928;&#22320;&#25552;&#21462;&#23439;&#20219;&#21153;&#23545;&#20110;&#29702;&#35299;&#31227;&#21160;&#20132;&#20114;&#21644;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23439;&#20219;&#21153;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#24456;&#38590;&#25552;&#21462;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#30001;&#22810;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#21516;&#26102;&#21448;&#38544;&#34255;&#22312;&#24212;&#29992;&#30340;&#32534;&#31243;&#32452;&#20214;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#20174;&#38543;&#26426;&#21644;&#29992;&#25143;&#31574;&#21010;&#30340;&#31227;&#21160;&#20132;&#20114;&#36712;&#36857;&#20013;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23439;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23439;&#20219;&#21153;&#33258;&#21160;&#26631;&#35760;&#20102;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#19988;&#21487;&#20197;&#23436;&#20840;&#25191;&#34892;&#12290;&#20026;&#20102;&#26816;&#39564;&#25552;&#21462;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#39033;&#30740;&#31350;&#65292;&#21253;&#25324;&#29992;&#25143;&#35780;&#20272;&#12289;&#19982;&#20154;&#24037;&#31574;&#21010;&#20219;&#21153;&#30340;&#27604;&#36739;&#20998;&#26512;&#20197;&#21450;&#23545;&#36825;&#20123;&#23439;&#20219;&#21153;&#30340;&#33258;&#21160;&#25191;&#34892;&#12290;&#36825;&#20123;&#23454;&#39564;&#21644;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#25552;&#21462;&#30340;&#23439;&#20219;&#21153;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Macros are building block tasks of our everyday smartphone activity (e.g., "login", or "booking a flight"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of the app. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. To examine the quality of extraction, we conduct multiple studies, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses show the effectiveness of our approach and the usefulness of extracted macros in various dow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26126;&#30830;&#20102;&#35299;&#37322;&#24615;&#30340;&#30446;&#26631;&#21644;&#35201;&#32032;&#65292;&#20197;&#25351;&#23548;&#26041;&#27861;&#35774;&#35745;&#24182;&#25913;&#36827;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.01685</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Interpretability in Machine Learning for Medical Imaging. (arXiv:2310.01685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26126;&#30830;&#20102;&#35299;&#37322;&#24615;&#30340;&#30446;&#26631;&#21644;&#35201;&#32032;&#65292;&#20197;&#25351;&#23548;&#26041;&#27861;&#35774;&#35745;&#24182;&#25913;&#36827;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#23450;&#20041;&#23384;&#22312;&#19968;&#31181;&#26222;&#36941;&#30340;&#27169;&#31946;&#24863;&#12290;&#20026;&#20160;&#20040;&#38656;&#35201;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#37322;&#24615;&#65311;&#24403;&#38656;&#35201;&#35299;&#37322;&#24615;&#26102;&#65292;&#23454;&#38469;&#19978;&#36861;&#27714;&#30340;&#30446;&#26631;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#30446;&#26631;&#21644;&#35201;&#32032;&#38656;&#35201;&#24418;&#24335;&#21270;&#12290;&#36890;&#36807;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#28857;&#20013;&#24120;&#35265;&#30340;&#23454;&#38469;&#20219;&#21153;&#21644;&#30446;&#26631;&#36827;&#34892;&#25512;&#29702;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#26680;&#24515;&#35201;&#32032;&#65306;&#23450;&#20301;&#12289;&#35270;&#35273;&#21487;&#35782;&#21035;&#24615;&#12289;&#29289;&#29702;&#24402;&#22240;&#21644;&#36879;&#26126;&#24230;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#22312;&#21307;&#23398;&#24433;&#20687;&#30340;&#32972;&#26223;&#19979;&#31995;&#32479;&#21270;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#30340;&#23454;&#36341;&#35266;&#28857;&#28548;&#28165;&#20102;&#20855;&#20307;&#30340;&#21307;&#23398;&#24433;&#20687;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30446;&#26631;&#21644;&#32771;&#34385;&#22240;&#32032;&#65292;&#20197;&#25351;&#23548;&#26041;&#27861;&#35774;&#35745;&#24182;&#25913;&#36827;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#23454;&#29992;&#21644;&#25945;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability for machine learning models in medical imaging (MLMI) is an important direction of research. However, there is a general sense of murkiness in what interpretability means. Why does the need for interpretability in MLMI arise? What goals does one actually seek to address when interpretability is needed? To answer these questions, we identify a need to formalize the goals and elements of interpretability in MLMI. By reasoning about real-world tasks and goals common in both medical image analysis and its intersection with machine learning, we identify four core elements of interpretability: localization, visual recognizability, physical attribution, and transparency. Overall, this paper formalizes interpretability needs in the context of medical imaging, and our applied perspective clarifies concrete MLMI-specific goals and considerations in order to guide method design and improve real-world usage. Our goal is to provide practical and didactic information for model desig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.10003</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19987;&#21033;&#26435;&#35201;&#27714;&#30340;&#33539;&#22260;&#27979;&#37327;&#20026;&#35813;&#35201;&#27714;&#25152;&#21253;&#21547;&#30340;&#33258;&#20449;&#24687;&#30340;&#20498;&#25968;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#65292;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#32597;&#35265;&#30340;&#27010;&#24565;&#27604;&#24179;&#24120;&#30340;&#27010;&#24565;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#22240;&#20026;&#23427;&#26356;&#20196;&#20154;&#24778;&#35766;&#12290;&#33258;&#20449;&#24687;&#26159;&#20174;&#35813;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#35745;&#31639;&#24471;&#20986;&#30340;&#65292;&#20854;&#20013;&#27010;&#29575;&#26159;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20116;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#65288;&#27599;&#20010;&#21333;&#35789;&#25110;&#23383;&#31526;&#22343;&#20174;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#21462;&#65289;&#21040;&#20013;&#31561;&#27169;&#22411;&#65288;&#20351;&#29992;&#24179;&#22343;&#35789;&#25110;&#23383;&#31526;&#39057;&#29575;&#65289;&#65292;&#20877;&#21040;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT2&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33539;&#22260;&#24230;&#37327;&#20943;&#23569;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#65292;&#36825;&#26159;&#20808;&#21069;&#20316;&#21697;&#20013;&#24050;&#32463;&#20351;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20061;&#20010;&#31995;&#21015;&#30340;&#38024;&#23545;&#19981;&#21516;&#21457;&#26126;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#65292;&#20854;&#20013;&#27599;&#20010;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PCN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21033;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21943;&#27880;&#26631;&#35760;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.08630</link><description>&lt;p&gt;
PCN&#65306;&#19968;&#31181;&#21033;&#29992;&#26032;&#39062;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#21943;&#27880;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions. (arXiv:2309.08630v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PCN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21033;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21943;&#27880;&#26631;&#35760;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21943;&#27880;&#26631;&#35760;&#26159;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#31890;&#23376;&#30896;&#25758;&#20135;&#29983;&#30340;&#38181;&#29366;&#21943;&#27880;&#65292;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#21457;&#23556;&#31890;&#23376;&#12290;&#21943;&#27880;&#26631;&#35760;&#30340;&#36827;&#23637;&#20026;&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#30340;&#26032;&#29289;&#29702;&#25628;&#32034;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#22797;&#26434;&#30896;&#25758;&#25968;&#25454;&#20013;&#23547;&#25214;&#38544;&#34255;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#23558;&#21943;&#27880;&#34920;&#31034;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#30340;&#26041;&#27861;&#22810;&#31181;&#22810;&#26679;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#21521;&#27169;&#22411;&#38544;&#34255;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#23613;&#21487;&#33021;&#22320;&#32534;&#30721;&#26368;&#22810;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20174;&#36825;&#31181;&#34920;&#31034;&#20013;&#26368;&#22909;&#22320;&#23398;&#20064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Particle Chebyshev Network&#65288;PCN&#65289;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#12290;ChebConv&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;GNN&#20013;&#30340;&#19968;&#31181;&#26377;&#25928;&#26367;&#20195;&#20256;&#32479;&#22270;&#21367;&#31215;&#30340;&#26041;&#27861;&#65292;&#32780;&#22312;&#21943;&#27880;&#26631;&#35760;&#20013;&#36824;&#27809;&#26377;&#34987;&#25506;&#32034;&#36807;&#12290;PCN&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jet tagging is a classification problem in high-energy physics experiments that aims to identify the collimated sprays of subatomic particles, jets, from particle collisions and tag them to their emitter particle. Advances in jet tagging present opportunities for searches of new physics beyond the Standard Model. Current approaches use deep learning to uncover hidden patterns in complex collision data. However, the representation of jets as inputs to a deep learning model have been varied, and often, informative features are withheld from models. In this study, we propose a graph-based representation of a jet that encodes the most information possible. To learn best from this representation, we design Particle Chebyshev Network (PCN), a graph neural network (GNN) using Chebyshev graph convolutions (ChebConv). ChebConv has been demonstrated as an effective alternative to classical graph convolutions in GNNs and has yet to be explored in jet tagging. PCN achieves a substantial improvemen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#21457;&#29616;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.05855</link><description>&lt;p&gt;
&#38543;&#26426;&#28388;&#27874;&#22120;&#32452;&#30340;&#33021;&#37327;&#20445;&#25345;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Energy Preservation and Stability of Random Filterbanks. (arXiv:2309.05855v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#21457;&#29616;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#24418;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#20026;&#20160;&#20040;&#22914;&#27492;&#22256;&#38590;&#65311;&#23613;&#31649;&#26377;&#22810;&#27425;&#23581;&#35797;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(convnets)&#36827;&#34892;&#28388;&#27874;&#22120;&#35774;&#35745;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#36229;&#36234;&#25163;&#24037;&#21019;&#24314;&#30340;&#22522;&#32447;&#12290;&#36825;&#26356;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#36825;&#20123;&#22522;&#32447;&#26159;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65306;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#20256;&#36882;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#22823;&#24863;&#21463;&#37326;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#38543;&#26426;&#21367;&#31215;&#31639;&#23376;&#30340;&#25968;&#23398;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#20102;&#31616;&#21333;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#30340;FIR&#28388;&#27874;&#22120;&#32452;&#22312;&#22823;&#28388;&#27874;&#22120;&#21644;&#23616;&#37096;&#21608;&#26399;&#36755;&#20837;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26159;&#30149;&#24577;&#30340;&#65292;&#36825;&#22312;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#20013;&#26159;&#20856;&#22411;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38543;&#26426;&#28388;&#27874;&#22120;&#32452;&#30340;&#26399;&#26395;&#33021;&#37327;&#20445;&#25345;&#23545;&#20110;&#25968;&#20540;&#31283;&#23450;&#24615;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26399;&#26395;&#24103;&#36793;&#30028;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
What makes waveform-based deep learning so hard? Despite numerous attempts at training convolutional neural networks (convnets) for filterbank design, they often fail to outperform hand-crafted baselines. This is all the more surprising because these baselines are linear time-invariant systems: as such, their transfer functions could be accurately represented by a convnet with a large receptive field. In this article, we elaborate on the statistical properties of simple convnets from the mathematical perspective of random convolutional operators. We find that FIR filterbanks with random Gaussian weights are ill-conditioned for large filters and locally periodic input signals, which both are typical in audio signal processing applications. Furthermore, we observe that expected energy preservation of a random filterbank is not sufficient for numerical stability and derive theoretical bounds for its expected frame bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#34920;&#24773;&#31526;&#21495;&#22312;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#30340;&#20351;&#29992;&#23545;&#24320;&#21457;&#32773;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34920;&#24773;&#31526;&#21495;&#21487;&#20197;&#26174;&#33879;&#32553;&#30701;&#38382;&#39064;&#30340;&#35299;&#20915;&#26102;&#38388;&#65292;&#24182;&#21560;&#24341;&#26356;&#22810;&#29992;&#25143;&#21442;&#19982;&#12290;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#25928;&#26524;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.16360</link><description>&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#20419;&#36827;&#20102;GitHub&#19978;&#24320;&#21457;&#32773;&#30340;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Emoji Promotes Developer Participation and Issue Resolution on GitHub. (arXiv:2308.16360v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#34920;&#24773;&#31526;&#21495;&#22312;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#30340;&#20351;&#29992;&#23545;&#24320;&#21457;&#32773;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34920;&#24773;&#31526;&#21495;&#21487;&#20197;&#26174;&#33879;&#32553;&#30701;&#38382;&#39064;&#30340;&#35299;&#20915;&#26102;&#38388;&#65292;&#24182;&#21560;&#24341;&#26356;&#22810;&#29992;&#25143;&#21442;&#19982;&#12290;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#25928;&#26524;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#30123;&#24773;&#26399;&#38388;&#36828;&#31243;&#24037;&#20316;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#35768;&#22810;&#20154;&#23545;&#36828;&#31243;&#24037;&#20316;&#30340;&#20302;&#25928;&#29575;&#34920;&#31034;&#25285;&#24551;&#12290;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#27807;&#36890;&#20013;&#32570;&#20047;&#38754;&#37096;&#34920;&#24773;&#21644;&#32930;&#20307;&#35821;&#35328;&#31561;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#36825;&#22952;&#30861;&#20102;&#26377;&#25928;&#30340;&#27807;&#36890;&#65292;&#24182;&#23545;&#24037;&#20316;&#32467;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20316;&#20026;&#26367;&#20195;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#20351;&#29992;&#30340;&#34920;&#24773;&#31526;&#21495;&#22312;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#20063;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#34920;&#24773;&#31526;&#21495;&#30340;&#20351;&#29992;&#22914;&#20309;&#24433;&#21709;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#24320;&#21457;&#32773;&#30340;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;GitHub&#30340;&#19968;&#20010;&#19968;&#24180;&#21608;&#26399;&#20869;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#34913;&#37327;&#34920;&#24773;&#31526;&#21495;&#23545;&#38382;&#39064;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#25511;&#21046;&#38382;&#39064;&#20869;&#23481;&#12289;&#20179;&#24211;&#21644;&#20316;&#32773;&#20449;&#24687;&#31561;&#28151;&#28102;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#34920;&#24773;&#31526;&#21495;&#21487;&#20197;&#26174;&#33879;&#32553;&#30701;&#38382;&#39064;&#30340;&#35299;&#20915;&#26102;&#38388;&#65292;&#24182;&#21560;&#24341;&#26356;&#22810;&#29992;&#25143;&#21442;&#19982;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#30340;&#24322;&#36136;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although remote working is increasingly adopted during the pandemic, many are concerned by the low-efficiency in the remote working. Missing in text-based communication are non-verbal cues such as facial expressions and body language, which hinders the effective communication and negatively impacts the work outcomes. Prevalent on social media platforms, emojis, as alternative non-verbal cues, are gaining popularity in the virtual workspaces well. In this paper, we study how emoji usage influences developer participation and issue resolution in virtual workspaces. To this end, we collect GitHub issues for a one-year period and apply causal inference techniques to measure the causal effect of emojis on the outcome of issues, controlling for confounders such as issue content, repository, and author information. We find that emojis can significantly reduce the resolution time of issues and attract more user participation. We also compare the heterogeneous effect on different types of issue
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23567;&#23398;&#20064;&#29575;&#21644;&#26799;&#24230;&#22122;&#22768;&#26159;&#20027;&#35201;&#19981;&#31283;&#23450;&#28304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#21160;&#37327;&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#30340;&#36793;&#38469;&#20215;&#20540;&#26159;&#26377;&#38480;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.15196</link><description>&lt;p&gt;
&#23567;&#23398;&#20064;&#29575;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#21160;&#37327;&#30340;&#36793;&#38469;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
The Marginal Value of Momentum for Small Learning Rate SGD. (arXiv:2307.15196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23567;&#23398;&#20064;&#29575;&#21644;&#26799;&#24230;&#22122;&#22768;&#26159;&#20027;&#35201;&#19981;&#31283;&#23450;&#28304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#21160;&#37327;&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#30340;&#36793;&#38469;&#20215;&#20540;&#26159;&#26377;&#38480;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#30340;&#24378;&#20984;&#29615;&#22659;&#20013;&#65292;&#21160;&#37327;&#24050;&#34987;&#35777;&#26126;&#33021;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#12290;&#22312;&#38543;&#26426;&#20248;&#21270;&#20013;&#65292;&#22914;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#20256;&#35328;&#35748;&#20026;&#21160;&#37327;&#21487;&#20197;&#36890;&#36807;&#20943;&#23567;&#38543;&#26426;&#26799;&#24230;&#26356;&#26032;&#30340;&#26041;&#24046;&#26469;&#24110;&#21161;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#65292;&#20294;&#20043;&#21069;&#30340;&#29702;&#35770;&#20998;&#26512;&#24182;&#27809;&#26377;&#21457;&#29616;&#21160;&#37327;&#21487;&#20197;&#25552;&#20379;&#20219;&#20309;&#21487;&#35777;&#23454;&#30340;&#21152;&#36895;&#12290;&#26412;&#25991;&#30340;&#29702;&#35770;&#32467;&#26524;&#38416;&#26126;&#20102;&#22312;&#23398;&#20064;&#29575;&#36739;&#23567;&#19988;&#26799;&#24230;&#22122;&#22768;&#26159;&#20027;&#35201;&#19981;&#31283;&#23450;&#28304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#21160;&#37327;&#30340;&#20316;&#29992;&#65292;&#34920;&#26126;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#27573;&#20869;&#34920;&#29616;&#30456;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23454;&#38469;&#35757;&#32451;&#20013;&#65292;&#21160;&#37327;&#22312;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#38754;&#30830;&#23454;&#26377;&#23616;&#38480;&#30340;&#30410;&#22788;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#29575;&#19981;&#26159;&#24456;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#22312;ImageNet&#19978;&#20174;&#22836;&#35757;&#32451;&#23567;&#33267;&#20013;&#31561;&#25209;&#27425;&#22823;&#23567;&#30340;&#27169;&#22411;&#21644;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks.
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#32423;&#21035;&#33258;&#36866;&#24212;&#24615;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#22810;&#31181;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#20998;&#26512;&#20013;&#30452;&#25509;&#24212;&#29992;&#20110;&#23567;&#25439;&#22833;&#30028;&#12290;&#21516;&#26102;&#65292;&#23427;&#19982;&#23545;&#25239;&#24615;/&#38543;&#26426;&#20984;&#20248;&#21270;&#21644;&#21338;&#24328;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.08360</link><description>&lt;p&gt;
&#20855;&#26377;&#36880;&#28176;&#21464;&#21270;&#30340;&#36890;&#29992;&#22312;&#32447;&#23398;&#20064;&#65306;&#19968;&#31181;&#22810;&#23618;&#22312;&#32447;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Online Learning with Gradual Variations: A Multi-layer Online Ensemble Approach. (arXiv:2307.08360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08360
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#32423;&#21035;&#33258;&#36866;&#24212;&#24615;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#22810;&#31181;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#20998;&#26512;&#20013;&#30452;&#25509;&#24212;&#29992;&#20110;&#23567;&#25439;&#22833;&#30028;&#12290;&#21516;&#26102;&#65292;&#23427;&#19982;&#23545;&#25239;&#24615;/&#38543;&#26426;&#20984;&#20248;&#21270;&#21644;&#21338;&#24328;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#32423;&#21035;&#33258;&#36866;&#24212;&#24615;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;&#12290;&#22312;&#26356;&#39640;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20855;&#20307;&#31867;&#22411;&#21644;&#26354;&#29575;&#19981;&#30693;&#24773;&#65292;&#32780;&#22312;&#26356;&#20302;&#32423;&#21035;&#19978;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#29615;&#22659;&#30340;&#33391;&#22909;&#24615;&#36136;&#24182;&#33719;&#24471;&#38382;&#39064;&#30456;&#20851;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#24378;&#20984;&#12289;&#25351;&#25968;&#20985;&#21644;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#20998;&#21035;&#33719;&#24471;&#20102;$O(\ln V_T)$&#12289;$O(d \ln V_T)$&#21644;$\hat{O}(\sqrt{V_T})$&#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#32500;&#24230;&#65292;$V_T$&#34920;&#31034;&#38382;&#39064;&#30456;&#20851;&#30340;&#26799;&#24230;&#21464;&#21270;&#65292;$\hat{O}(\cdot)$&#34920;&#31034;&#22312;$V_T$&#19978;&#30465;&#30053;&#23545;&#25968;&#22240;&#23376;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20855;&#26377;&#24191;&#27867;&#30340;&#24433;&#21709;&#21644;&#24212;&#29992;&#12290;&#23427;&#19981;&#20165;&#20445;&#35777;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#36824;&#30452;&#25509;&#23548;&#20986;&#20102;&#20998;&#26512;&#20013;&#30340;&#23567;&#25439;&#22833;&#30028;&#12290;&#27492;&#22806;&#65292;&#23427;&#19982;&#23545;&#25239;&#24615;/&#38543;&#26426;&#20984;&#20248;&#21270;&#21644;&#21338;&#24328;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#20854;&#23454;&#38469;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an online convex optimization method with two different levels of adaptivity. On a higher level, our method is agnostic to the specific type and curvature of the loss functions, while at a lower level, it can exploit the niceness of the environments and attain problem-dependent guarantees. To be specific, we obtain $\mathcal{O}(\ln V_T)$, $\mathcal{O}(d \ln V_T)$ and $\hat{\mathcal{O}}(\sqrt{V_T})$ regret bounds for strongly convex, exp-concave and convex loss functions, respectively, where $d$ is the dimension, $V_T$ denotes problem-dependent gradient variations and $\hat{\mathcal{O}}(\cdot)$-notation omits logarithmic factors on $V_T$. Our result finds broad implications and applications. It not only safeguards the worst-case guarantees, but also implies the small-loss bounds in analysis directly. Besides, it draws deep connections with adversarial/stochastic convex optimization and game theory, further validating its practical potential. Our method is based
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#30382;&#32932;&#30149;&#21464;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22495;&#36866;&#24212;&#22312;&#20943;&#23569;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#23545;&#20110;&#20108;&#20998;&#31867;&#20219;&#21153;&#26377;&#25928;&#65292;&#20294;&#22312;&#22810;&#20998;&#31867;&#20219;&#21153;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#38656;&#35201;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03157</link><description>&lt;p&gt;
&#22495;&#36866;&#24212;&#33021;&#25552;&#39640;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion Classification?. (arXiv:2307.03157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#30382;&#32932;&#30149;&#21464;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22495;&#36866;&#24212;&#22312;&#20943;&#23569;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#23545;&#20110;&#20108;&#20998;&#31867;&#20219;&#21153;&#26377;&#25928;&#65292;&#20294;&#22312;&#22810;&#20998;&#31867;&#20219;&#21153;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#38656;&#35201;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#35786;&#26029;&#31995;&#32479;&#22312;&#20998;&#31867;&#30382;&#32932;&#30284;&#30151;&#30149;&#21464;&#26102;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#20250;&#24433;&#21709;&#20934;&#30830;&#21487;&#38752;&#30340;&#35786;&#26029;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#20010;&#30382;&#32932;&#30149;&#21464;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;&#23588;&#20854;&#26159;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#22495;&#36866;&#24212;&#35757;&#32451;&#26041;&#26696;&#65306;&#21333;&#28304;&#12289;&#32508;&#21512;&#21644;&#22810;&#28304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22495;&#36866;&#24212;&#22312;&#20108;&#20998;&#31867;&#20013;&#26159;&#26377;&#25928;&#30340;&#65292;&#22312;&#20943;&#23569;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#36824;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#22312;&#22810;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20854;&#24615;&#33021;&#19981;&#22826;&#26126;&#26174;&#65292;&#38656;&#35201;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#25165;&#33021;&#36798;&#21040;&#22522;&#20934;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#20998;&#31867;&#20219;&#21153;&#30340;&#27979;&#35797;&#38169;&#35823;&#19982;&#26631;&#31614;&#20559;&#31227;&#24378;&#28872;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based diagnostic system has demonstrated potential in classifying skin cancer conditions when labeled training example are abundant. However, skin lesion analysis often suffers from a scarcity of labeled data, hindering the development of an accurate and reliable diagnostic system. In this work, we leverage multiple skin lesion datasets and investigate the feasibility of various unsupervised domain adaptation (UDA) methods in binary and multi-class skin lesion classification. In particular, we assess three UDA training schemes: single-, combined-, and multi-source. Our experiment results show that UDA is effective in binary classification, with further improvement being observed when imbalance is mitigated. In multi-class task, its performance is less prominent, and imbalance problem again needs to be addressed to achieve above-baseline accuracy. Through our quantitative analysis, we find that the test error of multi-class tasks is strongly correlated with label shift, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.14502</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#39034;&#24207;&#26816;&#32034;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;RetICL
&lt;/p&gt;
&lt;p&gt;
RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#21457;&#23637;&#37117;&#38598;&#20013;&#22312;&#20419;&#20351;&#23427;&#20204;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#20010;&#65288;&#25110;&#22810;&#20010;&#65289;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#65288;&#21487;&#33021;&#26159;&#26032;&#30340;&#65289;&#29983;&#25104;/&#39044;&#27979;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#31034;&#20363;&#30340;&#36873;&#25321;&#21487;&#33021;&#23545;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#22909;&#30340;&#31034;&#20363;&#24182;&#19981;&#26159;&#31616;&#21333;&#30340;&#65292;&#22240;&#20026;&#20195;&#34920;&#24615;&#31034;&#20363;&#32452;&#30340;&#23450;&#20041;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#19981;&#21516;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#29420;&#31435;&#22320;&#23545;&#31034;&#20363;&#36827;&#34892;&#35780;&#20998;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31034;&#20363;&#30340;&#39034;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#26041;&#27861;&#8212;&#8212;In-Context Learning&#30340;&#26816;&#32034;RetICL&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#36880;&#27493;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#25226;&#39034;&#24207;&#31034;&#20363;&#36873;&#25321;&#30340;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent developments in large language models focus on prompting them to perform specific tasks. One effective prompting method is in-context learning, where the model performs a (possibly new) generation/prediction task given one (or more) examples. Past work has shown that the choice of examples can make a large impact on task performance. However, finding good examples is not straightforward since the definition of a representative group of examples can vary greatly depending on the task. While there are many existing methods for selecting in-context examples, they generally score examples independently, ignoring the dependency between them and the order in which they are provided to the large language model. In this work, we propose Retrieval for In-Context Learning (RetICL), a learnable method for modeling and optimally selecting examples sequentially for in-context learning. We frame the problem of sequential example selection as a Markov decision process, design an example r
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;</title><link>http://arxiv.org/abs/2305.02942</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#34913;&#37327;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#22312;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging gradient-derived metrics for data selection and valuation in differentially private training. (arXiv:2305.02942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02942
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30417;&#31649;&#25285;&#24551;&#21644;&#21442;&#19982;&#24230;&#30340;&#19981;&#36275;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#26159;&#35299;&#20915;&#30417;&#31649;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#26159;&#20854;&#20013;&#26368;&#24120;&#29992;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#26799;&#24230;&#20449;&#24687;&#26469;&#35782;&#21035;&#38544;&#31169;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26368;&#20005;&#26684;&#30340;&#38544;&#31169;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#30528;&#33021;&#22815;&#20026;&#23458;&#25143;&#25552;&#20379;&#26377;&#21407;&#21017;&#30340;&#25968;&#25454;&#36873;&#25321;&#24037;&#20855;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining high-quality data for collaborative training of machine learning models can be a challenging task due to A) the regulatory concerns and B) lack of incentive to participate. The first issue can be addressed through the use of privacy enhancing technologies (PET), one of the most frequently used one being differentially private (DP) training. The second challenge can be addressed by identifying which data points can be beneficial for model training and rewarding data owners for sharing this data. However, DP in deep learning typically adversely affects atypical (often informative) data samples, making it difficult to assess the usefulness of individual contributions. In this work we investigate how to leverage gradient information to identify training samples of interest in private training settings. We show that there exist techniques which are able to provide the clients with the tools for principled data selection even in strictest privacy settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17963</link><description>&lt;p&gt;
&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States. (arXiv:2303.17963v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25511;&#21046;&#24037;&#31243;&#26041;&#27861;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#25104;&#20026;&#29289;&#29702;&#24314;&#27169;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#29366;&#24577;&#27979;&#37327;&#30340;&#21487;&#29992;&#24615;&#65292;&#32780;&#22797;&#26434;&#31995;&#32479;&#30340;&#29366;&#24577;&#36890;&#24120;&#19981;&#26159;&#30452;&#25509;&#21487;&#27979;&#37327;&#30340;&#12290;&#22240;&#27492;&#65292;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#20272;&#35745;&#21160;&#21147;&#23398;&#21644;&#28508;&#22312;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#22320;&#35774;&#35745;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#25511;&#21046;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#30340;&#26410;&#30693;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#36755;&#20837;&#36712;&#36857;&#12290;&#23545;&#32467;&#26524;&#36755;&#20837;&#36712;&#36857;&#36827;&#34892;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As control engineering methods are applied to increasingly complex systems, data-driven approaches for system identification appear as a promising alternative to physics-based modeling. While many of these approaches rely on the availability of state measurements, the states of a complex system are often not directly measurable. It may then be necessary to jointly estimate the dynamics and a latent state, making it considerably more challenging to design controllers with performance guarantees. This paper proposes a novel method for the computation of an optimal input trajectory for unknown nonlinear systems with latent states. Probabilistic performance guarantees are derived for the resulting input trajectory, and an approach to validate the performance of arbitrary control laws is presented. The effectiveness of the proposed method is demonstrated in a numerical simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.09446</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#36755;&#20837;&#25511;&#21046;&#39640;&#32500;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlling High-Dimensional Data With Sparse Input. (arXiv:2303.09446v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#29983;&#25104;&#39640;&#24230;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#30340;&#25509;&#21475;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#36755;&#20986;&#65292;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29992;&#25143;&#25110;&#25163;&#21160;&#25506;&#32034;&#19981;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25110;&#32773;&#36153;&#21147;&#22320;&#27880;&#37322;&#25968;&#25454;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#20110;&#25511;&#21046;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#38901;&#24459;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#23454;&#20363;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(MICVAE)&#65292;&#23427;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#32534;&#30721;&#31232;&#30095;&#30340;&#38901;&#24459;&#29305;&#24449;&#24182;&#36755;&#20986;&#23436;&#25972;&#30340;&#27874;&#24418;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MICVAE&#34920;&#29616;&#20986;&#20102;&#31232;&#30095;&#30340;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#26426;&#21046;&#25152;&#38656;&#30340;&#33391;&#22909;&#21697;&#36136;&#65306;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#12290;&#21363;&#20351;&#21482;&#26377;&#38750;&#24120;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;(~4)&#65292;MICVAE&#20063;&#33021;&#35753;&#29992;&#25143;&#23454;&#29616;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an efficient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: efficiency, robustness, and faithfulness. With even a very small number of input values (~4), MICVAE enables users to im
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20598;&#28982;&#24615;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#23558;&#20854;&#20998;&#31867;&#20026;&#25968;&#25454;&#20998;&#24067;&#20013;&#22266;&#26377;&#30340;&#27495;&#35270;&#21644;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#23548;&#33268;&#30340;&#27495;&#35270;&#12290;&#36890;&#36807;&#37327;&#21270;&#20598;&#28982;&#24615;&#27495;&#35270;&#30340;&#24615;&#33021;&#38480;&#21046;&#21644;&#21051;&#30011;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20844;&#24179;&#24178;&#39044;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#30740;&#31350;&#36824;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#65292;&#24182;&#25506;&#31350;&#20102;&#22312;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#30340;&#20844;&#24179;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2301.11781</link><description>&lt;p&gt;
&#20598;&#28982;&#24615;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65306;&#20844;&#24179;&#24178;&#39044;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions. (arXiv:2301.11781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11781
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20598;&#28982;&#24615;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#23558;&#20854;&#20998;&#31867;&#20026;&#25968;&#25454;&#20998;&#24067;&#20013;&#22266;&#26377;&#30340;&#27495;&#35270;&#21644;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#23548;&#33268;&#30340;&#27495;&#35270;&#12290;&#36890;&#36807;&#37327;&#21270;&#20598;&#28982;&#24615;&#27495;&#35270;&#30340;&#24615;&#33021;&#38480;&#21046;&#21644;&#21051;&#30011;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#25581;&#31034;&#20102;&#20844;&#24179;&#24178;&#39044;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#30740;&#31350;&#36824;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#65292;&#24182;&#25506;&#31350;&#20102;&#22312;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#30340;&#20844;&#24179;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26576;&#20123;&#20154;&#32676;&#20013;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22312;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#20570;&#20986;&#30340;&#36873;&#25321;&#21644;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#23558;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#27495;&#35270;&#26469;&#28304;&#20998;&#20026;&#20004;&#31867;&#65306;&#20598;&#28982;&#24615;&#27495;&#35270;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#20013;&#22266;&#26377;&#30340;&#27495;&#35270;&#65292;&#21644;&#35748;&#30693;&#24615;&#27495;&#35270;&#65292;&#21363;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#20570;&#20986;&#30340;&#20915;&#31574;&#23548;&#33268;&#30340;&#27495;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#23450;&#22312;&#23436;&#20840;&#20102;&#35299;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20844;&#24179;&#32422;&#26463;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#38480;&#21046;&#26469;&#37327;&#21270;&#20598;&#28982;&#24615;&#27495;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#24067;&#33713;&#20811;&#38886;&#23572;&#23545;&#27604;&#32479;&#35745;&#23454;&#39564;&#30340;&#32467;&#26524;&#26469;&#21051;&#30011;&#20598;&#28982;&#24615;&#27495;&#35270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35748;&#30693;&#24615;&#27495;&#35270;&#23450;&#20041;&#20026;&#22312;&#24212;&#29992;&#20844;&#24179;&#32422;&#26463;&#26102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19982;&#20598;&#28982;&#24615;&#27495;&#35270;&#25152;&#38480;&#23450;&#30340;&#30028;&#38480;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#30340;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#65292;&#24182;&#35843;&#26597;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#20013;&#30340;&#20844;&#24179;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and epistemic discrimination, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#25509;&#35302;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#31867;&#22411;&#30340;&#21628;&#21560;&#27169;&#24335;&#26469;&#26816;&#27979;&#21628;&#21560;&#24322;&#24120;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#25968;&#25454;&#30340;&#21628;&#21560;&#27874;&#24418;&#20002;&#24323;&#24178;&#25200;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#26080;&#21019;&#30340;&#20154;&#20307;&#21628;&#21560;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.03713</link><description>&lt;p&gt;
&#26080;&#25509;&#35302;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#30340;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Non-contact Respiratory Anomaly Detection using Infrared Light-wave Sensing. (arXiv:2301.03713v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#25509;&#35302;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#31867;&#22411;&#30340;&#21628;&#21560;&#27169;&#24335;&#26469;&#26816;&#27979;&#21628;&#21560;&#24322;&#24120;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#25968;&#25454;&#30340;&#21628;&#21560;&#27874;&#24418;&#20002;&#24323;&#24178;&#25200;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#26080;&#21019;&#30340;&#20154;&#20307;&#21628;&#21560;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#30340;&#21628;&#21560;&#39057;&#29575;&#21644;&#21628;&#21560;&#27169;&#24335;&#20256;&#36798;&#20102;&#20851;&#20110;&#20027;&#20307;&#30340;&#36523;&#20307;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#24322;&#24120;&#21628;&#21560;&#21487;&#33021;&#34920;&#26126;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#20351;&#29992;&#38750;&#30456;&#24178;&#32418;&#22806;&#20809;&#30340;&#26080;&#32447;&#20809;&#27874;&#20256;&#24863;&#65288;LWS&#65289;&#22312;&#19981;&#24341;&#36215;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#31034;&#20102;&#23433;&#20840;&#12289;&#38544;&#34109;&#12289;&#39640;&#25928;&#21644;&#26080;&#21019;&#30340;&#20154;&#20307;&#21628;&#21560;&#30417;&#27979;&#30340;&#28508;&#21147;&#12290;&#21628;&#21560;&#30417;&#27979;&#31995;&#32479;&#38656;&#35201;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#21628;&#21560;&#27169;&#24335;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#35782;&#21035;&#21628;&#21560;&#24322;&#24120;&#12290;&#35813;&#31995;&#32479;&#36824;&#24517;&#39035;&#39564;&#35777;&#25152;&#25910;&#38598;&#30340;&#25968;&#25454;&#26159;&#21542;&#20026;&#21628;&#21560;&#27874;&#24418;&#65292;&#20002;&#24323;&#30001;&#22806;&#37096;&#24178;&#25200;&#12289;&#29992;&#25143;&#31227;&#21160;&#25110;&#31995;&#32479;&#25925;&#38556;&#24341;&#36215;&#30340;&#20219;&#20309;&#38169;&#35823;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#27169;&#25311;&#20154;&#31867;&#21628;&#21560;&#27169;&#24335;&#30340;&#26426;&#22120;&#20154;&#65292;&#27169;&#25311;&#20102;&#27491;&#24120;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#24322;&#24120;&#21628;&#21560;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#25216;&#26415;&#25910;&#38598;&#20102;&#26102;&#38388;&#24207;&#21015;&#21628;&#21560;&#25968;&#25454;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#29992;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human respiratory rate and its pattern convey essential information about the physical and psychological states of the subject. Abnormal breathing can indicate fatal health issues leading to further diagnosis and treatment. Wireless light-wave sensing (LWS) using incoherent infrared light shows promise in safe, discreet, efficient, and non-invasive human breathing monitoring without raising privacy concerns. The respiration monitoring system needs to be trained on different types of breathing patterns to identify breathing anomalies.The system must also validate the collected data as a breathing waveform, discarding any faulty data caused by external interruption, user movement, or system malfunction. To address these needs, this study simulated normal and different types of abnormal respiration using a robot that mimics human breathing patterns. Then, time-series respiration data were collected using infrared light-wave sensing technology. Three machine learning algorithms, decision t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#21644;&#40654;&#26364;&#27969;&#24418;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.14598</link><description>&lt;p&gt;
&#30830;&#20999;&#30340;&#27969;&#24418;&#39640;&#26031;&#21464;&#20998;&#36125;&#21494;&#26031;
&lt;/p&gt;
&lt;p&gt;
Exact Manifold Gaussian Variational Bayes. (arXiv:2210.14598v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14598
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22797;&#26434;&#27169;&#22411;&#20013;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#21644;&#40654;&#26364;&#27969;&#24418;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#27169;&#22411;&#20013;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#65292;&#20854;&#20013;&#21464;&#20998;&#31354;&#38388;&#26159;&#19968;&#20010;&#40654;&#26364;&#27969;&#24418;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#20197;&#38544;&#24335;&#28385;&#36275;&#21464;&#20998;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#27491;&#23450;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30830;&#20999;&#27969;&#24418;&#39640;&#26031;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;EMGVB&#65289;&#25552;&#20379;&#20102;&#31934;&#30830;&#20294;&#31616;&#21333;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#24182;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#30001;&#20110;&#20854;&#40657;&#30418;&#24615;&#36136;&#65292;EMGVB&#25104;&#20026;&#22797;&#26434;&#27169;&#22411;&#20013;&#21363;&#25554;&#21363;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#32479;&#35745;&#12289;&#35745;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#20351;&#29992;&#20116;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#21487;&#34892;&#24615;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#24182;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an optimization algorithm for Variational Inference (VI) in complex models. Our approach relies on natural gradient updates where the variational space is a Riemann manifold. We develop an efficient algorithm for Gaussian Variational Inference that implicitly satisfies the positive definite constraint on the variational covariance matrix. Our Exact manifold Gaussian Variational Bayes (EMGVB) provides exact but simple update rules and is straightforward to implement. Due to its black-box nature, EMGVB stands as a ready-to-use solution for VI in complex models. Over five datasets, we empirically validate our feasible approach on different statistical, econometric, and deep learning models, discussing its performance with respect to baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21270;&#65292;&#21457;&#29616;&#30446;&#21069;&#24120;&#29992;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#35843;&#25972;&#21516;&#36136;&#24615;&#65292;&#35813;&#25351;&#26631;&#28385;&#36275;&#26356;&#22810;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#20855;&#26377;&#36739;&#23569;&#22312;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2209.06177</link><description>&lt;p&gt;
&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21270;&#65306;&#21516;&#36136;&#24615;-&#24322;&#36136;&#24615;&#20108;&#20998;&#27861;&#21450;&#20854;&#24310;&#20280;
&lt;/p&gt;
&lt;p&gt;
Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond. (arXiv:2209.06177v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21270;&#65292;&#21457;&#29616;&#30446;&#21069;&#24120;&#29992;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#35843;&#25972;&#21516;&#36136;&#24615;&#65292;&#35813;&#25351;&#26631;&#28385;&#36275;&#26356;&#22810;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#20855;&#26377;&#36739;&#23569;&#22312;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#36136;&#24615;&#26159;&#25551;&#36848;&#36793;&#36830;&#25509;&#30456;&#20284;&#33410;&#28857;&#20542;&#21521;&#30340;&#22270;&#24418;&#23646;&#24615;&#65307;&#30456;&#21453;&#30340;&#27010;&#24565;&#20026;&#24322;&#36136;&#24615;&#12290;&#20154;&#20204;&#36890;&#24120;&#35748;&#20026;&#24322;&#36136;&#24615;&#22270;&#23545;&#20110;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#24050;&#32463;&#20184;&#20986;&#20102;&#35768;&#22810;&#21162;&#21147;&#26469;&#24320;&#21457;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#25991;&#29486;&#20013;&#27809;&#26377;&#26222;&#36941;&#34987;&#25509;&#21463;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20026;&#27491;&#30830;&#30340;&#21516;&#36136;&#24615;&#27979;&#37327;&#25351;&#26631;&#24418;&#24335;&#21270;&#20102;&#26399;&#26395;&#30340;&#29305;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#21738;&#20123;&#25351;&#26631;&#28385;&#36275;&#21738;&#20123;&#29305;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#35843;&#25972;&#21516;&#36136;&#24615;&#30340;&#25351;&#26631;&#28385;&#36275;&#27604;&#20854;&#20182;&#27969;&#34892;&#21516;&#36136;&#24615;&#27979;&#37327;&#26041;&#27861;&#26356;&#22810;&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#32780;&#22312;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24456;&#23569;&#34987;&#20351;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#21516;&#36136;&#24615;-&#24322;&#36136;&#24615;&#20108;&#20998;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#65292;&#20351;&#24471;...
&lt;/p&gt;
&lt;p&gt;
Homophily is a graph property describing the tendency of edges to connect similar nodes; the opposite is called heterophily. It is often believed that heterophilous graphs are challenging for standard message-passing graph neural networks (GNNs), and much effort has been put into developing efficient methods for this setting. However, there is no universally agreed-upon measure of homophily in the literature. In this work, we show that commonly used homophily measures have critical drawbacks preventing the comparison of homophily levels across different datasets. For this, we formalize desirable properties for a proper homophily measure and verify which measures satisfy which properties. In particular, we show that a measure that we call adjusted homophily satisfies more desirable properties than other popular homophily measures while being rarely used in graph machine learning literature. Then, we go beyond the homophily-heterophily dichotomy and propose a new characteristic that allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#20989;&#25968;&#30340;Lipschitz&#29575;&#26469;&#20998;&#26512;&#35299;&#37322;&#22120;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#30340;&#27010;&#24565;&#24182;&#19982;&#39044;&#27979;&#22120;&#30340;&#27010;&#29575;Lipschitz&#29575;&#30456;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#30340;&#19979;&#30028;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.12481</link><description>&lt;p&gt;
&#20998;&#26512;&#36890;&#36807;&#39044;&#27979;&#20989;&#25968;&#30340;Lipschitz&#29575;&#26469;&#35780;&#20272;&#35299;&#37322;&#22120;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analyzing Explainer Robustness via Lipschitzness of Prediction Functions. (arXiv:2206.12481v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#20989;&#25968;&#30340;Lipschitz&#29575;&#26469;&#20998;&#26512;&#35299;&#37322;&#22120;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#30340;&#27010;&#24565;&#24182;&#19982;&#39044;&#27979;&#22120;&#30340;&#27010;&#29575;Lipschitz&#29575;&#30456;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#30340;&#19979;&#30028;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#33021;&#21147;&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#22120;&#24635;&#26159;&#34987;&#20381;&#36182;&#20110;&#20026;&#36825;&#20123;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#20026;&#20851;&#38190;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#37325;&#35201;&#30340;&#26159;&#36825;&#20123;&#35299;&#37322;&#22120;&#26412;&#36523;&#26159;&#31283;&#20581;&#30340;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#31283;&#20581;&#24615;&#30340;&#19968;&#20010;&#29305;&#23450;&#26041;&#38754;&#65292;&#21363;&#35299;&#37322;&#22120;&#22312;&#30456;&#20284;&#30340;&#25968;&#25454;&#36755;&#20837;&#19978;&#24212;&#35813;&#32473;&#20986;&#31867;&#20284;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21644;&#23450;&#20041;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#30340;&#27010;&#24565;&#26469;&#24418;&#24335;&#21270;&#36825;&#20010;&#35266;&#24565;&#65292;&#31867;&#20284;&#20110;&#39044;&#27979;&#20989;&#25968;&#30340;&#25935;&#38160;&#24615;&#12290;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#35299;&#37322;&#22120;&#30340;&#31283;&#20581;&#24615;&#19982;&#39044;&#27979;&#22120;&#30340;&#27010;&#29575;Lipschitz&#29575;&#30456;&#32852;&#31995;&#65292;&#35813;&#29575;&#25429;&#25417;&#20102;&#20989;&#25968;&#23616;&#37096;&#24179;&#28369;&#24615;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#26681;&#25454;&#39044;&#27979;&#20989;&#25968;&#30340;Lipschitz&#29575;&#25552;&#20379;&#23545;&#21508;&#31181;&#35299;&#37322;&#22120;&#65288;&#22914;SHAP&#65292;RISE&#65292;CXPlain&#65289;&#30340;&#25935;&#38160;&#24615;&#30340;&#19979;&#30028;&#20445;&#35777;&#12290;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#26263;&#31034;&#20102;&#23545;&#20110;&#20855;&#26377;&#23616;&#37096;&#20809;&#28369;&#24615;&#39044;&#27979;&#20989;&#25968;&#30340;&#35299;&#37322;&#22120;&#25935;&#38160;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods have significantly improved in their predictive capabilities, but at the same time they are becoming more complex and less transparent. As a result, explainers are often relied on to provide interpretability to these black-box prediction models. As crucial diagnostics tools, it is important that these explainers themselves are robust. In this paper we focus on one particular aspect of robustness, namely that an explainer should give similar explanations for similar data inputs. We formalize this notion by introducing and defining explainer astuteness, analogous to astuteness of prediction functions. Our formalism allows us to connect explainer robustness to the predictor's probabilistic Lipschitzness, which captures the probability of local smoothness of a function. We provide lower bound guarantees on the astuteness of a variety of explainers (e.g., SHAP, RISE, CXPlain) given the Lipschitzness of the prediction function. These theoretical results imply that lo
&lt;/p&gt;</description></item></channel></rss>