<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;Adam&#20248;&#21270;&#22120;&#30340;&#26500;&#25104;&#21644;&#31639;&#27861;&#32452;&#25104;&#30340;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;Adam&#23454;&#38469;&#19978;&#26159;&#20266;&#35013;&#25104;FTRL&#30340;&#65292;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;&#20854;&#31639;&#27861;&#32452;&#25104;&#30340;&#22909;&#22788;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01567</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#26356;&#26032;&#29702;&#35299;Adam&#20248;&#21270;&#22120;&#65306;Adam&#26159;&#20266;&#35013;&#25104;FTRL&#30340;
&lt;/p&gt;
&lt;p&gt;
Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;Adam&#20248;&#21270;&#22120;&#30340;&#26500;&#25104;&#21644;&#31639;&#27861;&#32452;&#25104;&#30340;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;Adam&#23454;&#38469;&#19978;&#26159;&#20266;&#35013;&#25104;FTRL&#30340;&#65292;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;&#20854;&#31639;&#27861;&#32452;&#25104;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Adam&#20248;&#21270;&#22120;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#31639;&#27861;&#32452;&#25104;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23545;Adam&#30340;&#20998;&#26512;&#20165;&#26174;&#31034;&#20102;&#21487;&#20197;&#31616;&#21333;&#22320;&#36890;&#36807;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;&#22914;SGD&#23454;&#29616;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#19981;&#21516;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102;Adam&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#21463;Cutkosky&#31561;&#20154;&#65288;2023&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#31216;&#20026;&#22312;&#32447;&#23398;&#20064;&#26356;&#26032;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#25105;&#20204;&#26681;&#25454;&#22312;&#32447;&#23398;&#20064;&#32773;&#36873;&#25321;&#20248;&#21270;&#22120;&#30340;&#26356;&#26032;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#35774;&#35745;&#19968;&#20010;&#22909;&#30340;&#20248;&#21270;&#22120;&#23601;&#31561;&#21516;&#20110;&#35774;&#35745;&#19968;&#20010;&#22909;&#30340;&#22312;&#32447;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#26159;&#65292;Adam&#23545;&#24212;&#20110;&#19968;&#31181;&#34987;&#31216;&#20026;Follow-the-Regularized-Leader (FTRL)&#30340;&#21407;&#21017;&#24615;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20854;&#31639;&#27861;&#32452;&#25104;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of the Adam optimizer in practice, the theoretical understanding of its algorithmic components still remains limited. In particular, most existing analyses of Adam show the convergence rate that can be simply achieved by non-adative algorithms like SGD. In this work, we provide a different perspective based on online learning that underscores the importance of Adam's algorithmic components. Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates, where we choose the updates of an optimizer based on an online learner. With this framework, the design of a good optimizer is reduced to the design of a good online learner. Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL). Building on this observation, we study the benefits of its algorithmic components from the online learning perspective.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#25968;&#30028;&#38480;&#65292;&#22686;&#24378;&#20102;&#26680;&#23725;&#22238;&#24402;&#30340;&#27979;&#35797;&#35823;&#24046;&#30028;&#38480;&#12290;&#23545;&#20110;&#22810;&#39033;&#24335;&#35889;&#34928;&#20943;&#30340;&#26680;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#20808;&#21069;&#30340;&#32467;&#26524;&#65307;&#23545;&#20110;&#25351;&#25968;&#35889;&#34928;&#20943;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#38750;&#24179;&#20961;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29305;&#24449;&#35889;&#34928;&#20943;&#22810;&#39033;&#24335;&#30340;&#26680;&#22238;&#24402;&#22120;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#29305;&#24449;&#35889;&#25351;&#25968;&#34928;&#20943;&#30340;&#26680;&#22238;&#24402;&#22120;&#21017;&#20855;&#26377;&#28798;&#38590;&#24615;&#30340;&#36807;&#25311;&#21512;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01297</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#35889;&#34920;&#24449;&#26680;&#23725;&#22238;&#24402;&#30340;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01297
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#25968;&#30028;&#38480;&#65292;&#22686;&#24378;&#20102;&#26680;&#23725;&#22238;&#24402;&#30340;&#27979;&#35797;&#35823;&#24046;&#30028;&#38480;&#12290;&#23545;&#20110;&#22810;&#39033;&#24335;&#35889;&#34928;&#20943;&#30340;&#26680;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#20808;&#21069;&#30340;&#32467;&#26524;&#65307;&#23545;&#20110;&#25351;&#25968;&#35889;&#34928;&#20943;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#38750;&#24179;&#20961;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29305;&#24449;&#35889;&#34928;&#20943;&#22810;&#39033;&#24335;&#30340;&#26680;&#22238;&#24402;&#22120;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#29305;&#24449;&#35889;&#25351;&#25968;&#34928;&#20943;&#30340;&#26680;&#22238;&#24402;&#22120;&#21017;&#20855;&#26377;&#28798;&#38590;&#24615;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#20102;&#26680;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#30340;&#26032;&#30028;&#38480;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#22686;&#24378;&#20102;&#22312;&#22266;&#23450;&#36755;&#20837;&#32500;&#24230;&#30340;&#36807;&#21442;&#25968;&#21270;&#21306;&#22495;&#20013;&#26680;&#23725;&#22238;&#24402;&#30340;&#29616;&#26377;&#38750;&#28176;&#36817;&#27979;&#35797;&#35823;&#24046;&#30028;&#38480;&#12290;&#23545;&#20110;&#20855;&#26377;&#22810;&#39033;&#24335;&#35889;&#34928;&#20943;&#30340;&#26680;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#30028;&#38480;&#65307;&#23545;&#20110;&#25351;&#25968;&#34928;&#20943;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#38750;&#24179;&#20961;&#21644;&#26032;&#39062;&#30340;&#12290;&#25105;&#20204;&#23545;&#36807;&#25311;&#21512;&#30340;&#32467;&#35770;&#26159;&#21452;&#37325;&#30340;&#65306;(i) &#35889;&#34928;&#20943;&#22810;&#39033;&#24335;&#30340;&#26680;&#22238;&#24402;&#22120;&#24517;&#39035;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#24456;&#22909;&#30340;&#27867;&#21270;&#65307;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#25152;&#35859;&#30340;&#28201;&#21644;&#36807;&#25311;&#21512;&#65307;(ii) &#22914;&#26524;&#20219;&#20309;&#26680;&#23725;&#22238;&#24402;&#22120;&#30340;&#29305;&#24449;&#35889;&#25351;&#25968;&#34928;&#20943;&#65292;&#21017;&#20854;&#27867;&#21270;&#24046;&#65292;&#21363;&#34920;&#29616;&#20986;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;&#36825;&#22686;&#21152;&#20102;&#26680;&#23725;&#22238;&#24402;&#22120;&#34920;&#29616;&#20986;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#21487;&#29992;&#29305;&#24449;&#35889;&#34928;&#20943;&#27425;&#22810;&#39033;&#24335;&#30340;&#26497;&#31471;&#24773;&#20917;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#21512;&#20102;&#26032;&#30340;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;(RMT)&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive new bounds for the condition number of kernel matrices, which we then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression in the over-parameterized regime for a fixed input dimension. For kernels with polynomial spectral decay, we recover the bound from previous work; for exponential decay, our bound is non-trivial and novel.   Our conclusion on overfitting is two-fold: (i) kernel regressors whose eigenspectrum decays polynomially must generalize well, even in the presence of noisy labeled training data; these models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting. This adds to the available characterization of kernel ridge regressors exhibiting benign overfitting as the extremal case where the eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new random matrix theory (RMT) te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#35813;&#32467;&#26500;&#21487;&#20197;&#27169;&#25311;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01107</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#27169;&#25311;&#22270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simulation of Graph Algorithms with Looped Transformers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#35813;&#32467;&#26500;&#21487;&#20197;&#27169;&#25311;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#22270;&#31639;&#27861;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#30001;&#20110;&#26377;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#35777;&#36827;&#23637;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#36827;&#19968;&#27493;&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#33021;&#22815;&#20351;&#29992;&#20851;&#31995;&#25968;&#25454;&#22797;&#21046;&#25512;&#29702;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#26550;&#26500;&#26159;&#19968;&#20010;&#24102;&#39069;&#22806;&#27880;&#24847;&#21147;&#22836;&#21644;&#19982;&#22270;&#24418;&#20132;&#20114;&#30340;&#24490;&#29615;&#21464;&#21387;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#20102;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#27169;&#25311;&#35832;&#22914;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#31561;&#31639;&#27861;&#12290;&#32593;&#32476;&#30340;&#23485;&#24230;&#19981;&#38543;&#36755;&#20837;&#22270;&#30340;&#22823;&#23567;&#22686;&#21152;&#65292;&#36825;&#24847;&#21619;&#30528;&#32593;&#32476;&#21487;&#20197;&#27169;&#25311;&#20219;&#20309;&#22270;&#19978;&#30340;&#19978;&#36848;&#31639;&#27861;&#12290;&#23613;&#31649;&#26377;&#36825;&#20010;&#29305;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#26377;&#19968;&#20010;&#30001;&#20110;&#26377;&#38480;&#31934;&#24230;&#32780;&#21463;&#21040;&#38480;&#21046;&#30340;&#27169;&#25311;&#26497;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The execution of graph algorithms using neural networks has recently attracted significant interest due to promising empirical progress. This motivates further understanding of how neural networks can replicate reasoning steps with relational data. In this work, we study the ability of transformer networks to simulate algorithms on graphs from a theoretical perspective. The architecture that we utilize is a looped transformer with extra attention heads that interact with the graph. We prove by construction that this architecture can simulate algorithms such as Dijkstra's shortest path algorithm, Breadth- and Depth-First Search, and Kosaraju's strongly connected components algorithm. The width of the network does not increase with the size of the input graph, which implies that the network can simulate the above algorithms for any graph. Despite this property, we show that there is a limit to simulation in our solution due to finite precision. Finally, we show a Turing Completeness resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;TDIL&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36716;&#25442;&#37492;&#21035;&#22120;&#30340;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#65292;&#40723;&#21169;&#20195;&#29702;&#21521;&#38752;&#36817;&#19987;&#23478;&#29366;&#24577;&#30340;&#29366;&#24577;&#23548;&#33322;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22870;&#21169;&#20449;&#21495;&#31232;&#30095;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01057</link><description>&lt;p&gt;
&#19987;&#23478;&#25509;&#36817;&#24615;&#20316;&#20026;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#30340;&#26367;&#20195;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;TDIL&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36716;&#25442;&#37492;&#21035;&#22120;&#30340;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#65292;&#40723;&#21169;&#20195;&#29702;&#21521;&#38752;&#36817;&#19987;&#23478;&#29366;&#24577;&#30340;&#29366;&#24577;&#23548;&#33322;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22870;&#21169;&#20449;&#21495;&#31232;&#30095;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33719;&#21462;&#22823;&#37327;&#19987;&#23478;&#28436;&#31034;&#22256;&#38590;&#25110;&#19981;&#21487;&#34892;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#19982; typicIL &#35774;&#32622;&#20013;&#20855;&#26377;&#22810;&#20010;&#31034;&#33539;&#19981;&#21516;&#65292;&#21333;&#28436;&#31034;IL&#28041;&#21450;&#20195;&#29702;&#21482;&#26377;&#19968;&#26465;&#19987;&#23478;&#36712;&#36857;&#30340;&#35775;&#38382;&#12290;&#25105;&#20204;&#24378;&#35843;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22870;&#21169;&#20449;&#21495;&#31232;&#30095;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#36716;&#25442;&#37492;&#21035;&#22120;&#30340;IL&#65288;TDIL&#65289;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;TDIL&#26159;&#19968;&#31181;&#22522;&#20110;IRL&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#32771;&#34385;&#29615;&#22659;&#21160;&#24577;&#30340;&#26356;&#23494;&#38598;&#30340;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#26469;&#35299;&#20915;&#22870;&#21169;&#31232;&#30095;&#24615;&#12290;&#36825;&#20010;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#40723;&#21169;&#20195;&#29702;&#21521;&#38752;&#36817;&#19987;&#23478;&#29366;&#24577;&#30340;&#29366;&#24577;&#23548;&#33322;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;TDIL&#35757;&#32451;&#19968;&#20010;&#36807;&#28193;&#37492;&#21035;&#22120;&#26469;&#21306;&#20998;&#32473;&#23450;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#21644;&#38750;&#26377;&#25928;&#36807;&#28193;&#20197;&#35745;&#31639;&#26367;&#20195;&#22870;&#21169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;TDIL&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on single-demonstration imitation learning (IL), a practical approach for real-world applications where obtaining numerous expert demonstrations is costly or infeasible. In contrast to typical IL settings with multiple demonstrations, single-demonstration IL involves an agent having access to only one expert trajectory. We highlight the issue of sparse reward signals in this setting and propose to mitigate this issue through our proposed Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed to address reward sparsity by introducing a denser surrogate reward function that considers environmental dynamics. This surrogate reward function encourages the agent to navigate towards states that are proximal to expert states. In practice, TDIL trains a transition discriminator to differentiate between valid and non-valid transitions in a given environment to compute the surrogate rewards. The experiments demonstrate that TDIL outperforms existin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#24452;&#21521;&#22522;&#32593;&#32476;&#26041;&#27861;&#26469;&#25104;&#21151;&#39044;&#27979;&#22797;&#26434;&#28151;&#27788;&#34892;&#20026;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24182;&#24341;&#20837;&#20102;&#27880;&#24847;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#38416;&#26126;&#20013;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2404.00618</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#20998;&#25903;&#24452;&#21521;&#22522;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#28151;&#27788;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
A Multi-Branched Radial Basis Network Approach to Predicting Complex Chaotic Behaviours
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#24452;&#21521;&#22522;&#32593;&#32476;&#26041;&#27861;&#26469;&#25104;&#21151;&#39044;&#27979;&#22797;&#26434;&#28151;&#27788;&#34892;&#20026;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24182;&#24341;&#20837;&#20102;&#27880;&#24847;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#38416;&#26126;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#30001;&#38169;&#32508;&#22797;&#26434;&#21644;&#28151;&#27788;&#34892;&#20026;&#29305;&#24449;&#30340;&#29289;&#29702;&#21560;&#24341;&#23376;&#30340;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#30001;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#23618;&#21644;&#26088;&#22312;&#26377;&#25928;&#25429;&#25417;&#21560;&#24341;&#23376;&#26102;&#38388;&#28436;&#21464;&#20013;&#38750;&#32447;&#24615;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#30340;&#27880;&#24847;&#26426;&#21046;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#22823;&#32422;28&#20998;&#38047;&#27963;&#21160;&#30340;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;36,700&#20010;&#26102;&#38388;&#24207;&#21015;&#35266;&#27979;&#65292;&#25105;&#20204;&#25104;&#21151;&#39044;&#27979;&#20102;&#21560;&#24341;&#23376;&#30340;&#36712;&#36857;&#30340;100&#27425;&#39044;&#27979;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35828;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#21487;&#35270;&#21270;&#65292;&#23637;&#31034;&#20102;&#21560;&#24341;&#23376;&#30340;&#21407;&#22987;&#21644;&#39044;&#27979;&#34892;&#20026;&#65292;&#24182;&#23558;&#35266;&#23519;&#21040;&#30340;&#19982;&#20272;&#35745;&#32467;&#26524;&#36827;&#34892;&#20102;&#23450;&#37327;&#27604;&#36739;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#38416;&#26126;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00618v1 Announce Type: new  Abstract: In this study, we propose a multi branched network approach to predict the dynamics of a physics attractor characterized by intricate and chaotic behavior. We introduce a unique neural network architecture comprised of Radial Basis Function (RBF) layers combined with an attention mechanism designed to effectively capture nonlinear inter-dependencies inherent in the attractor's temporal evolution. Our results demonstrate successful prediction of the attractor's trajectory across 100 predictions made using a real-world dataset of 36,700 time-series observations encompassing approximately 28 minutes of activity. To further illustrate the performance of our proposed technique, we provide comprehensive visualizations depicting the attractor's original and predicted behaviors alongside quantitative measures comparing observed versus estimated outcomes. Overall, this work showcases the potential of advanced machine learning algorithms in elucid
&lt;/p&gt;</description></item><item><title>Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19546</link><description>&lt;p&gt;
Croissant&#65306;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Croissant: A Metadata Format for ML-Ready Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19546
&lt;/p&gt;
&lt;p&gt;
Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20851;&#38190;&#36164;&#28304;&#65292;&#20294;&#22788;&#29702;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25705;&#25830;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Croissant&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#34987;ML&#24037;&#20855;&#21644;&#26694;&#26550;&#20351;&#29992;&#30340;&#26041;&#24335;&#12290;Croissant&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;Croissant&#24050;&#24471;&#21040;&#20960;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#24211;&#30340;&#25903;&#25345;&#65292;&#28085;&#30422;&#25968;&#21313;&#19975;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#21152;&#36733;&#21040;&#26368;&#27969;&#34892;&#30340;ML&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
&lt;/p&gt;</description></item><item><title>Serpent&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#24674;&#22797;&#26550;&#26500;&#65292;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#20840;&#23616;&#24863;&#21463;&#37326;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#20294;&#35745;&#31639;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2403.17902</link><description>&lt;p&gt;
Serpent&#65306;&#36890;&#36807;&#22810;&#23610;&#24230;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#23454;&#29616;&#21487;&#25193;&#23637;&#39640;&#25928;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17902
&lt;/p&gt;
&lt;p&gt;
Serpent&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#24674;&#22797;&#26550;&#26500;&#65292;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#20840;&#23616;&#24863;&#21463;&#37326;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#20294;&#35745;&#31639;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22270;&#20687;&#24674;&#22797;&#26550;&#26500;&#30340;&#35745;&#31639;&#24314;&#31569;&#22359;&#39046;&#22495;&#65292;&#20027;&#35201;&#30001;&#21367;&#31215;&#22788;&#29702;&#21644;&#21508;&#31181;&#27880;&#24847;&#26426;&#21046;&#30340;&#32452;&#21512;&#25152;&#20027;&#23548;&#12290;&#28982;&#32780;&#65292;&#21367;&#31215;&#28388;&#27874;&#22120;&#26412;&#36136;&#19978;&#26159;&#23616;&#37096;&#30340;&#65292;&#22240;&#27492;&#22312;&#24314;&#27169;&#22270;&#20687;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#27880;&#24847;&#26426;&#21046;&#25797;&#38271;&#25429;&#33719;&#20219;&#24847;&#22270;&#20687;&#21306;&#22495;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20114;&#20316;&#29992;&#65292;&#20294;&#23545;&#22270;&#20687;&#23610;&#23544;&#30340;&#20108;&#27425;&#25104;&#26412;&#36739;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Serpent&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#22312;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#20316;&#20026;&#20854;&#26680;&#24515;&#35745;&#31639;&#27169;&#22359;&#30340;&#26550;&#26500;&#12290;SSMs&#26368;&#21021;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#65292;&#21487;&#20197;&#36890;&#36807;&#26377;&#21033;&#30340;&#36755;&#20837;&#23610;&#23544;&#30340;&#32447;&#24615;&#32553;&#25918;&#26469;&#32500;&#25345;&#20840;&#23616;&#24863;&#21463;&#37326;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;Serpent&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#21516;&#26102;&#38656;&#35201;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#37327;&#36739;&#23569;&#65288;&#22312;FLOPS&#19978;&#39640;&#36798;150&#20493;&#30340;&#20943;&#23569;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17902v1 Announce Type: cross  Abstract: The landscape of computational building blocks of efficient image restoration architectures is dominated by a combination of convolutional processing and various attention mechanisms. However, convolutional filters are inherently local and therefore struggle at modeling long-range dependencies in images. On the other hand, attention excels at capturing global interactions between arbitrary image regions, however at a quadratic cost in image dimension. In this work, we propose Serpent, an architecture that leverages recent advances in state space models (SSMs) in its core computational block. SSMs, originally introduced for sequence modeling, can maintain a global receptive field with a favorable linear scaling in input size. Our preliminary results demonstrate that Serpent can achieve reconstruction quality on par with state-of-the-art techniques, while requiring orders of magnitude less compute (up to $150$ fold reduction in FLOPS) an
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.11904</link><description>&lt;p&gt;
CICLe: &#36866;&#24212;&#19978;&#19979;&#25991;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#27745;&#26579;&#25110;&#25530;&#20551;&#39135;&#21697;&#23545;&#20154;&#31867;&#20581;&#24247;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#22312;&#32473;&#23450;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#32593;&#32476;&#25991;&#26412;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#33258;&#21160;&#26816;&#27979;&#36825;&#31181;&#39118;&#38505;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;7,546&#20010;&#25551;&#36848;&#20844;&#20849;&#39135;&#21697;&#21484;&#22238;&#20844;&#21578;&#30340;&#30701;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25991;&#26412;&#37117;&#32463;&#36807;&#25163;&#21160;&#26631;&#35760;&#65292;&#20998;&#20026;&#20004;&#20010;&#31890;&#24230;&#32423;&#21035;&#65288;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#21484;&#22238;&#23545;&#24212;&#30340;&#39135;&#21697;&#20135;&#21697;&#21644;&#21361;&#23475;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#24182;&#23545;&#26420;&#32032;&#12289;&#20256;&#32479;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;tf-idf&#34920;&#31034;&#30340;&#36923;&#36753;&#22238;&#24402;&#22312;&#25903;&#25345;&#36739;&#20302;&#30340;&#31867;&#21035;&#19978;&#20248;&#20110;RoBERTa&#21644;XLM-R&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#19982;&#26222;&#36890;&#25552;&#31034;&#30456;&#27604;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11904v1 Announce Type: new  Abstract: Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2403.09919</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#24555;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recurrent Drafter for Fast Speculative Decoding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#25913;&#36827;&#30340;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#25104;&#29087;&#25216;&#26415;&#30340;&#20248;&#21183;&#65306;&#32463;&#20856;&#30340;&#21452;&#27169;&#22411;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#21333;&#27169;&#22411;&#26041;&#27861;Medusa&#12290;&#20174;Medusa&#24471;&#21040;&#28789;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#21333;&#27169;&#22411;&#31574;&#30053;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#30340;&#21333;&#20010;&#36731;&#37327;&#32423;&#33609;&#31295;&#22836;&#26469;&#21306;&#20998;&#33258;&#24049;&#65292;&#26412;&#36136;&#19978;&#31867;&#20284;&#20110;&#32463;&#20856;&#25512;&#27979;&#35299;&#30721;&#20013;&#20351;&#29992;&#30340;&#23567;&#22411;&#33609;&#31295;&#27169;&#22411;&#65292;&#20294;&#36991;&#20813;&#20102;&#23436;&#25972;transformer&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#24490;&#29615;&#20381;&#36182;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#24555;&#36895;&#36807;&#28388;&#20986;&#33609;&#31295;&#22836;&#20013;&#19981;&#38656;&#35201;&#30340;&#20505;&#36873;&#39033;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21333;&#27169;&#22411;&#35774;&#35745;&#31616;&#26131;&#24615;&#24182;&#36991;&#20813;&#20102;&#21019;&#24314;&#25968;&#25454;&#30456;&#20851;&#26641;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22270;&#25991;&#27861;&#25551;&#36848;&#20998;&#23376;&#30340;&#23618;&#27425;&#21270;&#35774;&#35745;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#22312;&#35774;&#35745;&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#23376;&#29983;&#25104;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#21487;&#21512;&#25104;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08147</link><description>&lt;p&gt;
&#23558;&#20998;&#23376;&#34920;&#31034;&#20026;&#21487;&#35299;&#37322;&#30340;&#25991;&#27861;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;
&lt;/p&gt;
&lt;p&gt;
Representing Molecules as Random Walks Over Interpretable Grammars
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08147
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22270;&#25991;&#27861;&#25551;&#36848;&#20998;&#23376;&#30340;&#23618;&#27425;&#21270;&#35774;&#35745;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#22312;&#35774;&#35745;&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#23376;&#29983;&#25104;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#21487;&#21512;&#25104;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20998;&#23376;&#25506;&#32034;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23567;&#22411;&#12289;&#31867;&#20284;&#33647;&#29289;&#30340;&#20998;&#23376;&#19978;&#65292;&#23548;&#33268;&#35768;&#22810;&#22312;&#26448;&#26009;&#35774;&#35745;&#20013;&#21516;&#26679;&#37325;&#35201;&#30340;&#24212;&#29992;&#32570;&#20047;&#36275;&#22815;&#30340;&#25216;&#26415;&#25903;&#25345;&#12290;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#20381;&#36182;&#20110;&#26356;&#22797;&#26434;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#26377;&#26356;&#23569;&#30340;&#20363;&#23376;&#65292;&#26159;&#20351;&#29992;&#24050;&#30693;&#30340;&#20122;&#32467;&#26500;&#31934;&#24515;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20197;&#22270;&#25991;&#27861;&#30340;&#24418;&#24335;&#34920;&#31034;&#21644;&#25512;&#29702;&#36825;&#20123;&#20998;&#23376;&#65292;&#26126;&#30830;&#25551;&#36848;&#20102;&#29305;&#24449;&#20026;&#35774;&#35745;&#22522;&#30784;&#30340;&#23618;&#27425;&#21270;&#35774;&#35745;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#21363;&#22312;&#35774;&#35745;&#31354;&#38388;&#19978;&#36827;&#34892;&#38543;&#26426;&#28216;&#36208;&#65292;&#26082;&#26377;&#21161;&#20110;&#20998;&#23376;&#29983;&#25104;&#65292;&#21448;&#26377;&#21161;&#20110;&#23646;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#39044;&#27979;&#20998;&#23376;&#21487;&#21512;&#25104;&#24615;&#26041;&#38754;&#30340;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#35813;&#26041;&#27861;&#30340;&#21270;&#23398;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08147v1 Announce Type: new  Abstract: Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#38024;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24357;&#21512;&#20102;&#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#26368;&#21518;&#36845;&#20195;&#30340;&#33391;&#22909;&#24615;&#33021;&#19982;&#29616;&#26377;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07723</link><description>&lt;p&gt;
&#20851;&#20110;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Last-Iterate Convergence of Shuffling Gradient Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#38024;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24357;&#21512;&#20102;&#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#26368;&#21518;&#36845;&#20195;&#30340;&#33391;&#22909;&#24615;&#33021;&#19982;&#29616;&#26377;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#65292;&#20063;&#34987;&#31216;&#20026;&#26080;&#26367;&#25442;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65292;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#21253;&#25324;&#19977;&#31181;&#27969;&#34892;&#31639;&#27861;&#65306;Random Reshuffle&#65288;RR&#65289;&#12289;Shuffle Once&#65288;SO&#65289;&#21644;Incremental Gradient&#65288;IG&#65289;&#12290;&#19982;&#32463;&#39564;&#25104;&#21151;&#30456;&#27604;&#65292;&#38271;&#26399;&#20197;&#26469;&#23545;&#20110;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#24182;&#19981;&#20805;&#20998;&#20102;&#35299;&#12290;&#26368;&#36817;&#65292;&#21482;&#20026;&#20984;&#20989;&#25968;&#30340;&#24179;&#22343;&#36845;&#20195;&#21644;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#21518;&#36845;&#20195;&#65288;&#20197;&#24179;&#26041;&#36317;&#31163;&#20026;&#24230;&#37327;&#65289;&#24314;&#31435;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#20989;&#25968;&#20540;&#24046;&#20316;&#20026;&#25910;&#25947;&#20934;&#21017;&#26102;&#65292;&#29616;&#26377;&#29702;&#35770;&#26080;&#27861;&#35299;&#37322;&#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#65288;&#20363;&#22914;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#65289;&#26368;&#21518;&#36845;&#20195;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#23454;&#36341;&#19982;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#38024;&#23545;&#30446;&#26631;&#20989;&#25968;&#35777;&#26126;&#20102;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07723v1 Announce Type: new  Abstract: Shuffling gradient methods, which are also known as stochastic gradient descent (SGD) without replacement, are widely implemented in practice, particularly including three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understanding for a long time. Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric). However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization). To bridge this gap between practice and theory, we prove last-iterate convergence rates for shuffling gradient methods with respect to the objectiv
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07262</link><description>&lt;p&gt;
&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Advantage-Aware Policy Optimization for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07262
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33268;&#21147;&#20110;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#21046;&#23450;&#26377;&#25928;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#22312;&#32447;&#20132;&#20114;&#65292;&#36890;&#36807;&#22312;&#34892;&#20026;&#31574;&#30053;&#30340;&#25903;&#25345;&#19979;&#26045;&#21152;&#36866;&#24403;&#30340;&#20445;&#23432;&#32422;&#26463;&#26469;&#35299;&#20915;&#20998;&#24067;&#22806;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#26500;&#24314;&#38024;&#23545;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#23398;&#20064;&#20248;&#21183;&#24863;&#30693;&#31574;&#30053;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05300</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#32479;&#19968;&#22810;&#26679;&#24615;&#65306;&#25913;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unity by Diversity: Improved Representation Learning in Multimodal VAEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#22914;&#34920;&#31034;&#23398;&#20064;&#12289;&#26377;&#26465;&#20214;&#29983;&#25104;&#21644;&#22635;&#34917;&#12290;&#30446;&#21069;&#30340;&#26550;&#26500;&#35201;&#20040;&#36328;&#27169;&#24577;&#20849;&#20139;&#32534;&#30721;&#22120;&#36755;&#20986;&#12289;&#35299;&#30721;&#22120;&#36755;&#20837;&#65292;&#35201;&#20040;&#20004;&#32773;&#37117;&#35201;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#26550;&#26500;&#23545;&#27169;&#22411;&#26045;&#21152;&#20102;&#20005;&#26684;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#36719;&#32422;&#26463;&#21462;&#20195;&#36825;&#20123;&#30828;&#32422;&#26463;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#36719;&#24615;&#22320;&#24341;&#23548;&#27599;&#20010;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26397;&#30528;&#20849;&#20139;&#30340;&#21518;&#39564;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20248;&#31168;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#27599;&#20010;&#32534;&#30721;&#20445;&#30041;&#26469;&#33258;&#20854;&#26410;&#21387;&#32553;&#21407;&#22987;&#29305;&#24449;&#26356;&#22909;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#21644;&#22635;&#34917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05300v1 Announce Type: cross  Abstract: Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of m
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDP&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20013;&#20026;&#32852;&#21512;&#30142;&#30149;&#39044;&#27979;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.04086</link><description>&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#33258;&#21160;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDP&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20013;&#20026;&#32852;&#21512;&#30142;&#30149;&#39044;&#27979;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#21644;&#25968;&#23383;&#21270;&#21307;&#30103;&#39046;&#22495;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#24050;&#25104;&#20026;&#19968;&#20010;&#20016;&#23500;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#26377;&#28508;&#21147;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#21644;&#21307;&#23398;&#30740;&#31350;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#26512;EHR&#25968;&#25454;&#20197;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#20581;&#24247;&#29366;&#20917;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#20854;&#20013;&#65292;&#19968;&#20123;&#30740;&#31350;&#25552;&#20513;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26469;&#32852;&#21512;&#39044;&#27979;&#22810;&#20010;&#30446;&#26631;&#30142;&#30149;&#65292;&#20197;&#25552;&#39640;&#21333;&#20219;&#21153;&#23398;&#20064;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;EHR&#25968;&#25454;&#30340;MTL&#26694;&#26550;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#19987;&#23478;&#26469;&#35782;&#21035;&#29992;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#20219;&#21153;&#32452;&#21644;&#35774;&#35745;&#27169;&#22411;&#26550;&#26500;&#65292;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#12290;&#20026;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#24182;&#25913;&#36827;&#26694;&#26550;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDP&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25628;&#32034;&#20219;&#21153;&#20998;&#32452;&#21644;&#26550;&#26500;&#30340;&#26368;&#20339;&#37197;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#28085;&#30422;&#20219;&#21153;&#32452;&#21512;&#21644;&#26550;&#26500;&#30340;&#24191;&#27867;&#32852;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04086v1 Announce Type: new  Abstract: In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research. In recent years, machine learning models have proliferated for analyzing EHR data to predict patients future health conditions. Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning. Nevertheless, current MTL frameworks for EHR data have significant limitations due to their heavy reliance on human experts to identify task groups for joint training and design model architectures. To reduce human intervention and improve the framework design, we propose an automated approach named AutoDP, which can search for the optimal configuration of task grouping and architectures simultaneously. To tackle the vast joint search space encompassing ta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01643</link><description>&lt;p&gt;
&#24744;&#38656;&#35201;&#26356;&#22909;&#22320;&#20851;&#27880;&#20184;&#36153;
&lt;/p&gt;
&lt;p&gt;
You Need to Pay Better Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#32988;&#36807;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#20248;&#21270;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#20284;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#23569;&#20102;&#22235;&#20998;&#20043;&#19977;&#65292;&#27599;&#20010;&#22836;&#37096;&#23569;&#20102;&#19968;&#20010;&#30697;&#38453;&#20056;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#27599;&#20010;&#22836;&#37096;&#20943;&#23569;&#20102;&#20004;&#20010;&#30697;&#38453;&#20056;&#27861;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#27880;&#24847;&#21147;&#24555;&#20004;&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36229;&#32423;&#27880;&#24847;&#21147;&#65292;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26126;&#26174;&#36229;&#36234;&#20102;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#30697;&#38453;&#20056;&#27861;&#12290;&#38500;&#20102;&#25552;&#20379;&#20005;&#26684;&#30340;&#25968;&#23398;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;MN&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#30561;&#30496;&#33218;&#20915;&#31574;&#38382;&#39064;&#30340;&#25509;&#36817;&#26368;&#20248;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#30028;&#65292;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#65292;&#20351;&#29992;Generalized EXP3&#12289;EXP3-IX&#21644;Tsallis entropy&#19979;&#30340;FTRL&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#36739;&#20043;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.01315</link><description>&lt;p&gt;
&#30561;&#30496;&#33218;&#20915;&#31574;&#38382;&#39064;&#20013;&#25509;&#36817;&#26368;&#20248;&#30340;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#30028;
&lt;/p&gt;
&lt;p&gt;
Near-optimal Per-Action Regret Bounds for Sleeping Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01315
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#30561;&#30496;&#33218;&#20915;&#31574;&#38382;&#39064;&#30340;&#25509;&#36817;&#26368;&#20248;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#30028;&#65292;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#65292;&#20351;&#29992;Generalized EXP3&#12289;EXP3-IX&#21644;Tsallis entropy&#19979;&#30340;FTRL&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#36739;&#20043;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#20102;&#38024;&#23545;&#30561;&#30496;&#33218;&#20915;&#31574;&#38382;&#39064;&#30340;&#25509;&#36817;&#26368;&#20248;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;&#25932;&#25163;&#36873;&#25321;&#27599;&#36718;&#21487;&#29992;&#33218;&#30340;&#38598;&#21512;&#21644;&#23427;&#20204;&#30340;&#25439;&#22833;&#12290;&#22312;&#27599;&#36718;&#33267;&#22810;&#26377; $A$ &#20010;&#21487;&#29992;&#33218;&#30340; $K$ &#20010;&#24635;&#33218;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#30693;&#30340;&#26368;&#22909;&#19978;&#30028;&#20026; $O(K\sqrt{TA\ln{K}})$&#65292;&#36890;&#36807;&#38388;&#25509;&#26368;&#23567;&#21270;&#20869;&#37096;&#30561;&#30496;&#36951;&#25022;&#33719;&#24471;&#12290;&#19982;&#26497;&#23567;&#20540; $\Omega(\sqrt{TA})$ &#19979;&#30028;&#30456;&#27604;&#65292;&#36825;&#20010;&#19978;&#30028;&#21253;&#21547;&#39069;&#22806;&#30340;&#20056;&#25968;&#22240;&#23376; $K\ln{K}$&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#27599;&#27425;&#34892;&#21160;&#36951;&#25022;&#65292;&#20351;&#29992;EXP3&#12289;EXP3-IX&#21644;&#24102;&#26377;Tsallis&#29109;&#30340;FTRL&#30340;&#25512;&#24191;&#29256;&#26412;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#39034;&#24207;&#20026; $O(\sqrt{TA\ln{K}})$ &#21644; $O(\sqrt{T\sqrt{AK}})$ &#30340;&#25509;&#36817;&#26368;&#20248;&#30028;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#20174;&#30561;&#30496;&#19987;&#23478;&#33719;&#24471;&#24314;&#35758;&#30340;&#33218;&#20915;&#31574;&#38382;&#39064;&#35774;&#32622;&#65292;&#21516;&#26102;&#25512;&#24191;&#20102;EXP4&#12290;&#36825;&#20026;&#29616;&#26377;&#30340;&#22810;&#20010;&#33258;&#36866;&#24212;&#21644;&#36319;&#36394;&#36951;&#25022;&#30028;&#30340;&#26032;&#35777;&#26126;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01315v1 Announce Type: new  Abstract: We derive near-optimal per-action regret bounds for sleeping bandits, in which both the sets of available arms and their losses in every round are chosen by an adversary. In a setting with $K$ total arms and at most $A$ available arms in each round over $T$ rounds, the best known upper bound is $O(K\sqrt{TA\ln{K}})$, obtained indirectly via minimizing internal sleeping regrets. Compared to the minimax $\Omega(\sqrt{TA})$ lower bound, this upper bound contains an extra multiplicative factor of $K\ln{K}$. We address this gap by directly minimizing the per-action regret using generalized versions of EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal bounds of order $O(\sqrt{TA\ln{K}})$ and $O(\sqrt{T\sqrt{AK}})$. We extend our results to the setting of bandits with advice from sleeping experts, generalizing EXP4 along the way. This leads to new proofs for a number of existing adaptive and tracking regret bounds for 
&lt;/p&gt;</description></item><item><title>UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.00131</link><description>&lt;p&gt;
UniTS: &#26500;&#24314;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniTS: Building a Unified Time Series Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00131
&lt;/p&gt;
&lt;p&gt;
UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LLMs&#65292;&#27491;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#25110;&#24494;&#35843;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#35768;&#22810;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#36866;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22266;&#26377;&#22810;&#26679;&#24615;&#21644;&#22810;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#20854;&#20182;&#31867;&#22411;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#35268;&#33539;&#20998;&#27495;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#19987;&#29992;&#27169;&#22411;&#30340;&#26126;&#26174;&#38656;&#27714;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;UNITS&#65292;&#19968;&#31181;&#25903;&#25345;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#30340;&#32479;&#19968;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#23481;&#32435;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#30340;&#65292;&#35813;&#39592;&#24178;&#32467;&#21512;&#20102;&#24207;&#21015;&#21644;&#21464;&#37327;&#27880;&#24847;&#21147;&#20197;&#21450;&#21160;&#24577;&#32447;&#24615;&#31639;&#23376;&#65292;&#24182;&#20316;&#20026;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;38&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#65292;UNITS&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00131v1 Announce Type: cross  Abstract: Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrate
&lt;/p&gt;</description></item><item><title>RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17257</link><description>&lt;p&gt;
RIME: &#20855;&#26377;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17257
&lt;/p&gt;
&lt;p&gt;
RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#23545;&#22870;&#21169;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;PbRL&#31639;&#27861;&#36807;&#24230;&#20381;&#36182;&#26469;&#33258;&#39046;&#22495;&#19987;&#23478;&#30340;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#23548;&#33268;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RIME&#65292;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#22024;&#26434;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#37492;&#21035;&#22120;&#65292;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#20197;&#36827;&#34892;&#20581;&#22766;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#19981;&#27491;&#30830;&#36896;&#25104;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#27492;&#22806;&#36824;&#33021;&#22635;&#34917;PbRL&#20013;&#20174;&#39044;&#35757;&#32451;&#21040;&#22312;&#32447;&#35757;&#32451;&#36807;&#28193;&#26102;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#21644;&#36816;&#21160;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RIME&#26174;&#33879;&#25552;&#21319;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#28909;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29305;&#24615;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#24182;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;</title><link>https://arxiv.org/abs/2402.17106</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20844;&#24179;&#24615;&#65306;&#22312;&#24744;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29305;&#24615;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#24182;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#20013;&#65292;&#35757;&#32451;&#33021;&#22815;&#26368;&#23567;&#21270;&#19981;&#21516;&#25935;&#24863;&#32676;&#20307;&#20043;&#38388;&#24046;&#24322;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#30340;&#20005;&#37325;&#31243;&#24230;&#22522;&#26412;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65292;&#22914;&#25968;&#25454;&#38598;&#30340;&#19981;&#22343;&#34913;&#25110;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#20351;&#29992;&#32479;&#19968;&#30340;&#20844;&#24179;&#24615;&#35201;&#27714;&#20173;&#28982;&#20540;&#24471;&#24576;&#30097;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#23548;&#33268;&#25928;&#29992;&#26497;&#20302;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#20010;&#25968;&#25454;&#38598;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25903;&#25345;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;You-Only-Train-Once&#65288;YOTO&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#22312;&#36924;&#36817;&#26435;&#34913;&#26354;&#32447;&#26102;&#38656;&#35201;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#35813;&#26354;&#32447;&#21608;&#22260;&#24341;&#20837;&#32622;&#20449;&#21306;&#38388;&#26469;&#37327;&#21270;&#25105;&#20204;&#36817;&#20284;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17106v1 Announce Type: cross  Abstract: In machine learning fairness, training models which minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off fundamentally depends on dataset characteristics such as dataset imbalances or biases. Therefore using a uniform fairness requirement across datasets remains questionable and can often lead to models with substantially low utility. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Moreover, we quantify the uncertainty in our approximation by introducing confidence intervals around this curve, offe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.15159</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning of Pre-trained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32972;&#26223;&#19979;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20197;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#35757;&#32451;&#27169;&#22411;&#8212;&#8212;&#19968;&#20010;&#26126;&#26174;&#32570;&#20047;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;LLMs&#20013;&#21246;&#21202;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#19971;&#31181;&#19981;&#21516;&#36951;&#24536;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;arXiv&#12289;&#20070;&#31821;&#21644;GitHub&#30340;&#31574;&#21010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#21147;&#30340;&#26426;&#22120;&#36951;&#24536;&#24615;&#33021;&#22522;&#20934;&#65292;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#27604;&#37325;&#26032;&#35757;&#32451;&#39640;&#20986; $10^5$ &#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#23558;&#26799;&#24230;&#19978;&#21319;&#19982;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#21487;&#20197;&#25913;&#21892;&#36229;&#21442;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36951;&#24536;&#36807;&#31243;&#20013;&#36827;&#34892;&#39640;&#25928;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#35814;&#32454;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#26377;&#20851;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30340;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14800</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#19987;&#23478;&#37117;&#30456;&#31561;: &#28151;&#21512;&#19987;&#23478;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;
&lt;/p&gt;
&lt;p&gt;
Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14800
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#23637;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#26159;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;LLMs&#30340;&#20986;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;LLMs&#30456;&#27604;&#65292;MoE LLMs&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#21442;&#25968;&#22823;&#23567;&#65292;&#20173;&#28982;&#24456;&#38590;&#37096;&#32626;&#23427;&#20204;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#19987;&#38376;&#35774;&#35745;&#30340;&#30828;&#20214;&#30340;&#26435;&#37325;&#21098;&#26525;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#20027;&#35201;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#21363;&#25554;&#21363;&#29992;&#30340;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#26469;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;MoE LLMs&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#37096;&#32626;&#25928;&#29575;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#22686;&#21152;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#39281;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14800v1 Announce Type: cross  Abstract: A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining sat
&lt;/p&gt;</description></item><item><title>&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20316;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#34917;&#20805;&#65292;&#32473;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#25552;&#20379;&#20102;&#33258;&#28982;&#36873;&#25321;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.08871</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Challenges and Opportunities in Topological Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08871
&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20316;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#34917;&#20805;&#65292;&#32473;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#25552;&#20379;&#20102;&#33258;&#28982;&#36873;&#25321;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#23427;&#21033;&#29992;&#25299;&#25169;&#29305;&#24449;&#26469;&#29702;&#35299;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#36890;&#36807;&#34701;&#20837;&#25299;&#25169;&#27010;&#24565;&#65292;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#34917;&#20805;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#25104;&#20026;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#33258;&#28982;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#23454;&#29992;&#30410;&#22788;&#21040;&#29702;&#35770;&#22522;&#30784;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#38024;&#23545;&#27599;&#20010;&#38382;&#39064;&#65292;&#23427;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20063;&#26159;&#23545;&#31185;&#23398;&#30028;&#30340;&#36992;&#35831;&#65292;&#24076;&#26395;&#31215;&#26497;&#21442;&#19982;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#65292;&#24320;&#21457;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08871v1 Announce Type: new Abstract: Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations. For each problem, it outlines potential solutions and future research opportunities. At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#32447;&#24615;&#21270;&#25910;&#32553;&#21160;&#21147;&#23398;&#65288;ELCD&#65289;&#65292;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20840;&#23616;&#25910;&#32553;&#24615;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#21521;&#37327;&#22330;&#30340;&#25193;&#23637;&#32447;&#24615;&#21270;&#23454;&#29616;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#35757;&#32451;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#25910;&#32553;&#24615;&#65292;ELCD&#33021;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20445;&#25345;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08090</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#25910;&#32553;&#21160;&#21147;&#23398;&#65306;&#25193;&#23637;&#32447;&#24615;&#21270;&#21644;&#20840;&#23616;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#32447;&#24615;&#21270;&#25910;&#32553;&#21160;&#21147;&#23398;&#65288;ELCD&#65289;&#65292;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20840;&#23616;&#25910;&#32553;&#24615;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#21521;&#37327;&#22330;&#30340;&#25193;&#23637;&#32447;&#24615;&#21270;&#23454;&#29616;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#35757;&#32451;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#25910;&#32553;&#24615;&#65292;ELCD&#33021;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20445;&#25345;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#20445;&#35777;&#23545;&#20110;&#30830;&#20445;&#31995;&#32479;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#33391;&#22909;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#23637;&#32447;&#24615;&#21270;&#25910;&#32553;&#21160;&#21147;&#23398;&#65288;ELCD&#65289;&#65292;&#35813;&#31995;&#32479;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20219;&#24847;&#24230;&#37327;&#19979;&#20840;&#23616;&#25910;&#32553;&#24615;&#20445;&#35777;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;ELCD&#30340;&#20851;&#38190;&#29305;&#24615;&#26159;&#38750;&#32447;&#24615;&#21521;&#37327;&#22330;&#25193;&#23637;&#32447;&#24615;&#21270;&#30340;&#21442;&#25968;&#21270;&#12290;&#22312;&#20854;&#26368;&#22522;&#26412;&#24418;&#24335;&#19979;&#65292;ELCD&#20445;&#35777;&#20840;&#23616;&#25351;&#25968;&#31283;&#23450;&#12289;&#24179;&#34913;&#25910;&#32553;&#20197;&#21450;&#22312;&#26576;&#20123;&#24230;&#37327;&#19979;&#20840;&#23616;&#25910;&#32553;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#30456;&#23545;&#20110;&#26356;&#19968;&#33324;&#24230;&#37327;&#30340;&#25910;&#32553;&#65292;&#25105;&#20204;&#35757;&#32451;&#25968;&#25454;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#25910;&#32553;&#24615;&#65292;&#20174;&#32780;&#30830;&#20445;&#25968;&#25454;&#31354;&#38388;&#30340;&#20840;&#23616;&#25910;&#32553;&#24615;&#12290;&#25105;&#20204;&#22312;2D&#12289;4D&#21644;8D&#30340;LASA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;ELCD&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty. We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field. In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric. To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D LASA datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07865</link><description>&lt;p&gt;
&#36879;&#35270;VLMs&#65306;&#25506;&#32034;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#23545;&#35805;&#12289;&#22330;&#26223;&#29702;&#35299;&#21644;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#65292;&#36825;&#31181;&#24212;&#29992;&#20419;&#20351;&#20102;&#20687;LLaVa&#12289;InstructBLIP&#21644;PaLI-3&#31561;&#35768;&#22810;&#26032;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#26377;&#36825;&#20040;&#22810;&#26032;&#30340;&#21457;&#24067;&#65292;&#20294;&#20851;&#20110;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#26550;&#26500;&#21644;&#20248;&#21270;&#30340;&#20851;&#38190;&#35774;&#35745;&#20915;&#31574;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#24456;&#38590;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#36825;&#19968;&#25361;&#25112;&#21448;&#22240;&#32570;&#20047;&#23458;&#35266;&#12289;&#19968;&#33268;&#30340;&#35780;&#20272;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#35270;&#35273;&#38382;&#31572;&#12289;&#20174;&#35821;&#35328;&#20013;&#23450;&#20301;&#29289;&#20307;&#20197;&#21450;&#25506;&#32034;&#24187;&#35273;&#31561;&#23646;&#24615;&#30340;&#30446;&#26631;&#25361;&#25112;&#38598;&#65292;&#36825;&#20123;&#35780;&#20272;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;VLM&#33021;&#21147;&#30340;&#31934;&#32454;&#12289;&#20934;&#30830;&#30340;&#35265;&#35299;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20851;&#38190;&#30340;&#35774;&#35745;&#36724;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#20351;&#29992;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using 
&lt;/p&gt;</description></item><item><title>&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.07240</link><description>&lt;p&gt;
&#38408;&#20540;Oja&#26159;&#21542;&#36866;&#29992;&#20110;&#31232;&#30095;PCA&#65311;
&lt;/p&gt;
&lt;p&gt;
Thresholded Oja does Sparse PCA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07240
&lt;/p&gt;
&lt;p&gt;
&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#24403;&#27604;&#20540;$d/n \rightarrow c &gt; 0$&#26102;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#38382;&#39064;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#65292;&#20851;&#20110;&#31232;&#30095;PCA&#30340;&#26368;&#20248;&#29575;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#20854;&#20013;&#25152;&#26377;&#25968;&#25454;&#37117;&#21487;&#20197;&#29992;&#20110;&#22810;&#27425;&#20256;&#36882;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#21475;&#29305;&#24449;&#21521;&#37327;&#26159;$s$-&#31232;&#30095;&#26102;&#65292;&#20855;&#26377;$O(d)$&#23384;&#20648;&#21644;$O(nd)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#27969;&#31639;&#27861;&#36890;&#24120;&#35201;&#27714;&#24378;&#21021;&#22987;&#21270;&#26465;&#20214;&#65292;&#21542;&#21017;&#20250;&#26377;&#27425;&#20248;&#38169;&#35823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#23545;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#65288;Oja&#21521;&#37327;&#65289;&#36827;&#34892;&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#12290;&#36825;&#38750;&#24120;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#27809;&#26377;&#38408;&#20540;&#65292;Oja&#21521;&#37327;&#30340;&#35823;&#24046;&#24456;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#38480;&#21046;&#26410;&#24402;&#19968;&#21270;&#30340;Oja&#21521;&#37327;&#30340;&#39033;&#19978;&#65292;&#36825;&#28041;&#21450;&#23558;&#19968;&#32452;&#29420;&#31435;&#38543;&#26426;&#30697;&#38453;&#30340;&#20056;&#31215;&#22312;&#38543;&#26426;&#21021;&#22987;&#21521;&#37327;&#19978;&#30340;&#25237;&#24433;&#12290; &#36825;&#26159;&#38750;&#24179;&#20961;&#19988;&#26032;&#39062;&#30340;&#65292;&#22240;&#20026;&#20197;&#21069;&#30340;Oja&#31639;&#27861;&#20998;&#26512;&#27809;&#26377;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.07240v2 Announce Type: cross  Abstract: We consider the problem of Sparse Principal Component Analysis (PCA) when the ratio $d/n \rightarrow c &gt; 0$. There has been a lot of work on optimal rates on sparse PCA in the offline setting, where all the data is available for multiple passes. In contrast, when the population eigenvector is $s$-sparse, streaming algorithms that have $O(d)$ storage and $O(nd)$ time complexity either typically require strong initialization conditions or have a suboptimal error. We show that a simple algorithm that thresholds and renormalizes the output of Oja's algorithm (the Oja vector) obtains a near-optimal error rate. This is very surprising because, without thresholding, the Oja vector has a large error. Our analysis centers around bounding the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector. This is nontrivial and novel since previous analyses of Oja's al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20381;&#36182;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#23574;&#38160;&#29575;&#65292;&#20027;&#35201;&#26159;&#20026;&#20102;&#36991;&#20813;&#26679;&#26412;&#22823;&#23567;&#32553;&#20943;&#23545;&#26041;&#24046;&#20135;&#29983;&#24433;&#21709;&#12290;&#24403;&#20551;&#35774;&#31867;&#21035;&#30340;&#25299;&#25169;&#32467;&#26500;&#31526;&#21512;&#26576;&#20123;&#26465;&#20214;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#32773;&#30340;&#24615;&#33021;&#19982;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#21644;&#20108;&#38454;&#32479;&#35745;&#37327;&#26377;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.05928</link><description>&lt;p&gt;
&#20381;&#36182;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#23574;&#38160;&#29575;&#65306;&#36991;&#20813;&#26679;&#26412;&#22823;&#23567;&#32553;&#20943;&#30340;&#24179;&#26041;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20381;&#36182;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#23574;&#38160;&#29575;&#65292;&#20027;&#35201;&#26159;&#20026;&#20102;&#36991;&#20813;&#26679;&#26412;&#22823;&#23567;&#32553;&#20943;&#23545;&#26041;&#24046;&#20135;&#29983;&#24433;&#21709;&#12290;&#24403;&#20551;&#35774;&#31867;&#21035;&#30340;&#25299;&#25169;&#32467;&#26500;&#31526;&#21512;&#26576;&#20123;&#26465;&#20214;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#32773;&#30340;&#24615;&#33021;&#19982;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#21644;&#20108;&#38454;&#32479;&#35745;&#37327;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20381;&#36182;&#24615;&#65288;&#946;-&#28151;&#21512;&#65289;&#25968;&#25454;&#21644;&#24179;&#26041;&#25439;&#22833;&#30340;&#32479;&#35745;&#23398;&#20064;&#65292;&#22312;&#19968;&#20010;&#20551;&#35774;&#31867;&#21035;&#934;_p&#30340;&#23376;&#38598;F&#20013;&#65292;&#20854;&#20013;&#934;_p&#26159;&#33539;&#25968;&#8741;f&#8741;_&#934;_p&#8801;sup_m&#8805;1 m^{-1/p}&#8741;f&#8741;_L^m&#65292;&#20854;&#20013;p&#8712;[2&#65292;&#8734;]&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#22312;&#20855;&#26377;&#20381;&#36182;&#24615;&#25968;&#25454;&#30340;&#23398;&#20064;&#20013;&#23547;&#25214;&#23574;&#38160;&#30340;&#22122;&#22768;&#20132;&#20114;&#39033;&#25110;&#26041;&#24046;&#20195;&#29702;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20856;&#22411;&#30340;&#38750;&#28176;&#36817;&#32467;&#26524;&#26174;&#31034;&#20986;&#26041;&#24046;&#20195;&#29702;&#36890;&#36807;&#24213;&#23618;&#21327;&#21464;&#37327;&#36807;&#31243;&#30340;&#28151;&#21512;&#26102;&#38388;&#36827;&#34892;&#20102;&#20056;&#31215;&#32553;&#20943;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#22312;&#25105;&#20204;&#30340;&#20551;&#35774;&#31867;&#21035;F&#19978;&#65292;L^2&#21644;&#934;_p&#30340;&#25299;&#25169;&#26159;&#21487;&#27604;&#36739;&#30340;&#65292;&#21363;&#934;_p&#26159;&#19968;&#20010;&#24369;&#20122;&#39640;&#26031;&#31867;&#21035;&#65306;&#8741;f&#8741;_&#934;_p&#8818;&#8741;f&#8741;_L^2^&#951;&#65292;&#20854;&#20013;&#951;&#8712;(0&#65292;1]&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#32773;&#22312;&#20854;&#20027;&#23548;&#39033;&#20013;&#21482;&#23454;&#29616;&#20102;&#19968;&#31181;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#22797;&#26434;&#24615;&#21644;&#20108;&#38454;&#32479;&#35745;&#37327;&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#35768;&#22810;&#20381;&#36182;&#24615;&#25968;&#25454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study statistical learning with dependent ($\beta$-mixing) data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$ where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p} \|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent data. Absent any realizability assumption, typical non-asymptotic results exhibit variance proxies that are deflated \emph{multiplicatively} by the mixing time of the underlying covariates process. We show that whenever the topologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class $\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class: $\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term. Our result holds whether t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05680</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpretable classifiers for tabular data via discretization and feature selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#20998;&#31867;&#22120;&#26159;&#31616;&#30701;&#30340;DNF&#20844;&#24335;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25968;&#25454;&#31163;&#25955;&#21270;&#20026;&#24067;&#23572;&#24418;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#32467;&#21512;&#38750;&#24120;&#24555;&#36895;&#30340;&#31639;&#27861;&#26469;&#20135;&#29983;&#26368;&#20339;&#30340;&#24067;&#23572;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;14&#20010;&#23454;&#39564;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#65292;&#24471;&#21040;&#30340;&#32467;&#26524;&#30340;&#20934;&#30830;&#24230;&#20027;&#35201;&#19982;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#20197;&#21450;&#25991;&#29486;&#20013;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#29616;&#26377;&#32467;&#26524;&#30456;&#20284;&#12290;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#38469;&#19978;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#21442;&#32771;&#32467;&#26524;&#65292;&#23613;&#31649;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#30340;&#21363;&#26102;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#20851;&#20110;&#20174;&#29616;&#23454;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#20998;&#31867;&#22120;&#19982;&#26469;&#33258;&#25968;&#25454;&#32972;&#26223;&#20998;&#24067;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#30456;&#23545;&#24212;&#30340;&#27010;&#29575;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method for computing immediately human interpretable yet accurate classifiers from tabular data. The classifiers obtained are short DNF-formulas, computed via first discretizing the original data to Boolean form and then using feature selection coupled with a very fast algorithm for producing the best possible Boolean classifier for the setting. We demonstrate the approach via 14 experiments, obtaining results with accuracies mainly similar to ones obtained via random forests, XGBoost, and existing results for the same datasets in the literature. In several cases, our approach in fact outperforms the reference results in relation to accuracy, even though the main objective of our study is the immediate interpretability of our classifiers. We also prove a new result on the probability that the classifier we obtain from real-life data corresponds to the ideally best classifier with respect to the background distribution the data comes from.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05140</link><description>&lt;p&gt;
Tag-LLM: &#23558;&#36890;&#29992;&#30340;LLM&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#20877;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#19987;&#38376;&#39046;&#22495;&#20013;&#65292;&#22914;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26410;&#20805;&#20998;&#28085;&#30422;&#30340;&#39046;&#22495;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36890;&#29992;LLMs&#37325;&#26032;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26377;&#25928;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#34987;&#21442;&#25968;&#21270;&#20026;&#36830;&#32493;&#21521;&#37327;&#24182;&#38468;&#21152;&#21040;LLMs&#30340;&#23884;&#20837;&#23618;&#65292;&#20197;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26631;&#31614;&#65306;&#39046;&#22495;&#26631;&#31614;&#29992;&#20110;&#38480;&#23450;&#19987;&#19994;&#34920;&#31034;&#65288;&#20363;&#22914;&#21270;&#23398;&#24335;&#65289;&#24182;&#25552;&#20379;&#39046;&#22495;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65307;&#21151;&#33021;&#26631;&#31614;&#29992;&#20110;&#34920;&#31034;&#29305;&#23450;&#30340;&#21151;&#33021;&#65288;&#20363;&#22914;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65289;&#24182;&#21387;&#32553;&#21151;&#33021;&#35299;&#20915;&#25351;&#20196;&#12290;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#39046;&#22495;&#30693;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#36825;&#20123;&#26631;&#31614;&#30340;&#21327;&#35758;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#22330;&#26223;&#30340;&#26080;&#31034;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#33539;&#29305;&#24449;&#28418;&#31227;&#24182;&#21033;&#29992;&#21407;&#22411;&#26469;&#20943;&#23569;&#20219;&#21153;&#26032;&#40092;&#24230;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.03917</link><description>&lt;p&gt;
&#20919;&#21551;&#21160;&#26080;&#31034;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03917
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#22330;&#26223;&#30340;&#26080;&#31034;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#33539;&#29305;&#24449;&#28418;&#31227;&#24182;&#21033;&#29992;&#21407;&#22411;&#26469;&#20943;&#23569;&#20219;&#21153;&#26032;&#40092;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#31034;&#20363;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;EFCIL&#65289;&#26088;&#22312;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20808;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#20013;&#27809;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#39592;&#24178;&#32593;&#32476;&#12290;&#23545;&#20110;EFCIL&#26469;&#35828;&#65292;&#36825;&#26159;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39640;&#24230;&#30340;&#21487;&#22609;&#24615;&#65292;&#36825;&#20250;&#23548;&#33268;&#29305;&#24449;&#28418;&#31227;&#65292;&#22312;&#26080;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#36827;&#34892;&#34917;&#20607;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#33539;&#22312;&#19982;&#20808;&#21069;&#20219;&#21153;&#39640;&#24230;&#30456;&#20851;&#30340;&#26041;&#21521;&#19978;&#30340;&#28418;&#31227;&#65292;&#24182;&#21033;&#29992;&#21407;&#22411;&#26469;&#20943;&#23569;&#20219;&#21153;&#26032;&#40092;&#24230;&#20559;&#24046;&#65292;&#20197;&#25972;&#21512;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;&#65288;EFC&#65289;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#32463;&#39564;&#29305;&#24449;&#30697;&#38453;&#65288;EFM&#65289;&#30340;&#21487;&#35299;&#20108;&#38454;&#36817;&#20284;&#26469;&#22788;&#29702;&#29305;&#24449;&#28418;&#31227;&#12290;EFM&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#20266;&#24230;&#37327;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#35268;&#33539;&#37325;&#35201;&#26041;&#21521;&#19978;&#30340;&#29305;&#24449;&#28418;&#31227;&#65292;&#24182;&#26356;&#26032;&#39640;&#26031;&#21407;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03286</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#19968;&#33268;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Training-Free Consistent Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#36896;&#24615;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#19979;&#19968;&#33268;&#22320;&#25551;&#32472;&#30456;&#21516;&#30340;&#20027;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26469;&#25945;&#25480;&#23427;&#25551;&#36848;&#29305;&#23450;&#29992;&#25143;&#25552;&#20379;&#20027;&#39064;&#30340;&#26032;&#35789;&#27719;&#25110;&#32773;&#20026;&#27169;&#22411;&#28155;&#21152;&#22270;&#20687;&#26465;&#20214;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#38024;&#23545;&#27599;&#20010;&#20027;&#39064;&#36827;&#34892;&#28459;&#38271;&#30340;&#20248;&#21270;&#25110;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#21644;&#25551;&#32472;&#22810;&#20010;&#20027;&#39064;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#35757;&#32451;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#26469;&#23454;&#29616;&#19968;&#33268;&#30340;&#20027;&#39064;&#29983;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20027;&#39064;&#39537;&#21160;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20197;&#20419;&#36827;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31574;&#30053;&#20197;&#40723;&#21169;&#24067;&#23616;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining su
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.03271</link><description>&lt;p&gt;
&#24819;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#25628;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03271
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35268;&#21010;&#65288;UoT&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#27169;&#25311;&#26410;&#26469;&#22330;&#26223;&#12289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#21644;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20248;&#21270;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23547;&#27714;&#20449;&#24687;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27604;&#22914;&#21307;&#23398;&#35786;&#26029;&#21644;&#25925;&#38556;&#25490;&#38500;&#65292;&#35299;&#20915;&#20219;&#21153;&#25152;&#38656;&#30340;&#20449;&#24687;&#19981;&#26159;&#21021;&#22987;&#32473;&#23450;&#30340;&#65292;&#32780;&#38656;&#35201;&#36890;&#36807;&#35810;&#38382;&#21518;&#32493;&#38382;&#39064;&#26469;&#20027;&#21160;&#23547;&#27714;&#65288;&#20363;&#22914;&#65292;&#21307;&#29983;&#21521;&#24739;&#32773;&#35810;&#38382;&#30151;&#29366;&#30340;&#26356;&#22810;&#32454;&#33410;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24605;&#24819;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;UoT&#65289;&#65292;&#19968;&#31181;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#20027;&#21160;&#25552;&#38382;&#20449;&#24687;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;UoT&#32467;&#21512;&#20102;1&#65289;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20223;&#30495;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#21487;&#33021;&#30340;&#26410;&#26469;&#22330;&#26223;&#65292;&#24182;&#20272;&#35745;&#20854;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;2&#65289;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#26426;&#21046;&#65292;&#28608;&#21169;&#27169;&#22411;&#23547;&#27714;&#20449;&#24687;&#65307;3&#65289;&#22870;&#21169;&#20256;&#25773;&#26041;&#26696;&#65292;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#22870;&#21169;&#30340;&#26041;&#24335;&#36873;&#25321;&#26368;&#20339;&#30340;&#38382;&#39064;&#25552;&#38382;&#26041;&#24335;&#12290;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#25925;&#38556;&#25490;&#38500;&#21644;'20&#30340;&#23454;&#39564;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;&#21644;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#30340;&#26041;&#21521;&#24615;&#36890;&#35759;&#12290;&#36890;&#36807;&#24314;&#31435;LDS&#19982;&#22810;&#36755;&#20986;GP&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#32447;&#24615;&#25512;&#26029;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.02686</link><description>&lt;p&gt;
&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#65306;&#19968;&#31181;&#21457;&#29616;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#26041;&#21521;&#24615;&#36890;&#35759;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;&#21644;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#30340;&#26041;&#21521;&#24615;&#36890;&#35759;&#12290;&#36890;&#36807;&#24314;&#31435;LDS&#19982;&#22810;&#36755;&#20986;GP&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#32447;&#24615;&#25512;&#26029;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#19981;&#21516;&#33041;&#21306;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#31070;&#32463;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#21508;&#31181;&#32479;&#35745;&#26041;&#27861;&#24050;&#32463;&#25506;&#32034;&#20102;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#28508;&#22312;&#30340;&#36890;&#35759;&#12290;&#20004;&#20010;&#20027;&#35201;&#30340;&#31867;&#21035;&#26159;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21644;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#65292;&#27599;&#20010;&#26041;&#27861;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#12290;&#22522;&#20110;GP&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#28508;&#22312;&#21464;&#37327;&#65292;&#22914;&#39057;&#24102;&#21644;&#36890;&#35759;&#26041;&#21521;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;LDS&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#39640;&#65292;&#20294;&#22312;&#28508;&#22312;&#34920;&#31034;&#26041;&#38754;&#32570;&#20047;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#19982;&#22810;&#36755;&#20986;GP&#30456;&#23545;&#24212;&#30340;LDS&#65292;&#21363;&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#65288;MRM-GP&#65289;&#65292;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#21512;&#20108;&#20026;&#19968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#24314;&#31435;&#20102;LDS&#21644;&#22810;&#36755;&#20986;GP&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#31070;&#32463;&#35760;&#24405;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26126;&#30830;&#24314;&#27169;&#20102;&#39057;&#29575;&#21644;&#30456;&#20301;&#24310;&#36831;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#22312;&#26102;&#38388;&#28857;&#19978;&#23454;&#29616;&#20102;&#32447;&#24615;&#25512;&#26029;&#25104;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying the complex interactions between different brain regions is crucial in neuroscience. Various statistical methods have explored the latent communication across multiple brain regions. Two main categories are the Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique strengths. The GP-based approach effectively discovers latent variables such as frequency bands and communication directions. Conversely, the LDS-based approach is computationally efficient but lacks powerful expressiveness in latent representation. In this study, we merge both methodologies by creating an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian Process (MRM-GP). Our work is the first to establish a connection between an LDS and a multi-output GP that explicitly models frequencies and phase delays within the latent space of neural recordings. Consequently, the model achieves a linear inference cost over time points and provides an interpretable low-dimensional repre
&lt;/p&gt;</description></item><item><title>LQER&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#25110;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.02446</link><description>&lt;p&gt;
LQER: &#20302;&#31209;&#37327;&#21270;&#35823;&#24046;&#37325;&#24314;&#29992;&#20110;LLMs
&lt;/p&gt;
&lt;p&gt;
LQER: Low-Rank Quantization Error Reconstruction for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02446
&lt;/p&gt;
&lt;p&gt;
LQER&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#36817;&#20046;&#26080;&#25439;&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#25110;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20302;&#31209;&#37327;&#21270;&#35823;&#24046;&#20943;&#23569;&#65288;LQER&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#37327;&#21270;&#21644;&#20302;&#31209;&#36924;&#36817;&#26469;&#24674;&#22797;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;LQER&#21033;&#29992;&#28608;&#27963;&#24341;&#36215;&#30340;&#23610;&#24230;&#30697;&#38453;&#23558;&#37327;&#21270;&#35823;&#24046;&#30340;&#22855;&#24322;&#20540;&#20998;&#24067;&#25512;&#21521;&#26399;&#26395;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;LLMs&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#36817;&#20046;&#26080;&#25439;&#30340;W4A8&#37327;&#21270;&#65292;&#26080;&#38656;&#30693;&#35782;&#33976;&#39311;&#12289;&#32593;&#26684;&#25628;&#32034;&#25110;&#22522;&#20110;&#26799;&#24230;&#30340;&#36845;&#20195;&#20248;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;LQER&#30340;&#35745;&#31639;&#27169;&#24335;&#28040;&#38500;&#20102;&#20174;&#19981;&#35268;&#21017;&#20869;&#23384;&#20301;&#32622;&#25910;&#38598;&#39640;&#31934;&#24230;&#26435;&#37325;&#25152;&#38656;&#30340;&#19987;&#29992;Scatter&#21644;Gather&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;W4A8 LLMs&#22312;&#20845;&#20010;&#28909;&#38376;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#30828;&#20214;&#36164;&#28304;&#27604;&#39046;&#20808;&#30340;&#26368;&#26032;&#26041;&#27861;&#23569;1.36&#20493;&#12290;&#19968;&#26086;&#35770;&#25991;&#34987;&#25509;&#21463;&#65292;&#25105;&#20204;&#23558;&#24320;&#28304;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.
&lt;/p&gt;</description></item><item><title>EuLagNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#36319;&#36394;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#30001;&#20110;&#27431;&#25289;&#35266;&#23519;&#32780;&#23548;&#33268;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#22256;&#38590;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02425</link><description>&lt;p&gt;
EuLagNet: &#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#30340;&#27431;&#25289;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02425
&lt;/p&gt;
&lt;p&gt;
EuLagNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#36319;&#36394;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#30001;&#20110;&#27431;&#25289;&#35266;&#23519;&#32780;&#23548;&#33268;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#22256;&#38590;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#23545;&#27668;&#35937;&#23398;&#12289;&#28023;&#27915;&#23398;&#21644;&#31354;&#27668;&#21160;&#21147;&#23398;&#31561;&#24191;&#27867;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27969;&#20307;&#36890;&#24120;&#20174;&#27431;&#25289;&#35282;&#24230;&#35266;&#23519;&#65292;&#20854;&#27963;&#36291;&#21644;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#22312;&#38745;&#27490;&#30340;&#32593;&#26684;&#20013;&#20005;&#37325;&#34987;&#25513;&#30422;&#21644;&#28151;&#28102;&#65292;&#32473;&#39044;&#27979;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#20026;&#23548;&#21521;&#30340;&#27431;&#25289;-&#25289;&#26684;&#26391;&#26085;&#21452;&#37325;&#36882;&#24402;&#32593;&#32476;&#65288;EuLagNet&#65289;&#65292;&#36890;&#36807;&#36319;&#36394;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#24182;&#38543;&#26102;&#38388;&#31215;&#32047;&#21160;&#21147;&#23398;&#20449;&#24687;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;EuLag&#22359;&#65292;&#29992;&#20110;&#22312;&#27599;&#20010;&#26102;&#21051;&#21644;&#23610;&#24230;&#19978;&#20256;&#36882;&#23398;&#20064;&#21040;&#30340;&#27431;&#25289;&#21644;&#25289;&#26684;&#26391;&#26085;&#29305;&#24449;&#65292;&#20854;&#20013;&#36319;&#36394;&#31890;&#23376;&#30340;&#36816;&#21160;&#26159;&#20174;&#27431;&#25289;&#35266;&#23519;&#20013;&#25512;&#26029;&#20986;&#26469;&#30340;&#65292;&#23427;&#20204;&#31215;&#32047;&#30340;&#21160;&#21147;&#23398;&#20449;&#24687;&#34987;&#32435;&#20837;&#21040;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics. However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction. This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time. Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#30028;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#65292;&#19982;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#22797;&#26434;&#24615;&#30456;&#20851;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#25512;&#26029;&#25968;&#25454;&#38598;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.02407</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#38598;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Defining Neural Network Architecture through Polytope Structures of Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#30028;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#65292;&#19982;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#22797;&#26434;&#24615;&#30456;&#20851;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#25512;&#26029;&#25968;&#25454;&#38598;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#22823;&#22411;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#24443;&#24213;&#20998;&#31867;&#65292;&#28982;&#32780;&#36825;&#31181;&#20851;&#31995;&#30340;&#20855;&#20307;&#24615;&#36136;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#30340;&#19978;&#19979;&#30028;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#30028;&#38480;&#26159;&#30001;&#25152;&#35752;&#35770;&#25968;&#25454;&#38598;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#25152;&#30830;&#23450;&#30340;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#21407;&#21017;&#22312;&#21333;&#32431;&#22797;&#21512;&#20307;&#21644;&#29305;&#23450;&#22810;&#26679;&#26354;&#38754;&#24418;&#29366;&#19978;&#30340;&#24212;&#29992;&#65292;&#35299;&#37322;&#20102;&#32593;&#32476;&#23485;&#24230;&#38656;&#27714;&#22914;&#20309;&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#22797;&#26434;&#24615;&#32780;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#30740;&#31350;&#19968;&#31181;&#30456;&#21453;&#24773;&#20917;&#65292;&#21363;&#21487;&#20197;&#20174;&#30456;&#24212;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20986;&#25968;&#25454;&#38598;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;MNIST&#12289;Fashion-MNIST&#21644;CIFAR10&#65289;&#21487;&#20197;&#29992;&#21482;&#26377;&#23569;&#25968;&#38754;&#30340;&#20004;&#20010;&#22810;&#38754;&#20307;&#26377;&#25928;&#22320;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current theoretical and empirical research in neural networks suggests that complex datasets require large network architectures for thorough classification, yet the precise nature of this relationship remains unclear. This paper tackles this issue by defining upper and lower bounds for neural network widths, which are informed by the polytope structure of the dataset in question. We also delve into the application of these principles to simplicial complexes and specific manifold shapes, explaining how the requirement for network width varies in accordance with the geometric complexity of the dataset. Moreover, we develop an algorithm to investigate a converse situation where the polytope structure of a dataset can be inferred from its corresponding trained neural networks. Through our algorithm, it is established that popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be efficiently encapsulated using no more than two polytopes with a small number of faces.
&lt;/p&gt;</description></item><item><title>APIServe&#26159;&#38024;&#23545;API&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#20010;&#39640;&#25928;&#24037;&#20855;&#65292;&#23427;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#30001; API &#35843;&#29992;&#24341;&#36215;&#30340; GPU &#36164;&#28304;&#28010;&#36153;&#65292;&#24182;&#25552;&#39640;&#20102;&#25972;&#20307;&#26381;&#21153;&#21534;&#21520;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.01869</link><description>&lt;p&gt;
APIServe: &#39640;&#25928;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;API&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
APIServe: Efficient API Support for Large-Language Model Inferencing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01869
&lt;/p&gt;
&lt;p&gt;
APIServe&#26159;&#38024;&#23545;API&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#20010;&#39640;&#25928;&#24037;&#20855;&#65292;&#23427;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#30001; API &#35843;&#29992;&#24341;&#36215;&#30340; GPU &#36164;&#28304;&#28010;&#36153;&#65292;&#24182;&#25552;&#39640;&#20102;&#25972;&#20307;&#26381;&#21153;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#22806;&#37096;&#24037;&#20855;&#21644;API&#38598;&#25104;&#65292;&#22914;ChatGPT&#25554;&#20214;&#65292;&#20197;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#24515;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LLM&#25512;&#29702;&#31995;&#32479;&#26159;&#20026;&#29420;&#31435;&#30340;LLM&#35774;&#35745;&#30340;&#12290;&#23427;&#20204;&#23558;API&#35843;&#29992;&#35270;&#20026;&#26032;&#35831;&#27714;&#65292;&#23548;&#33268;&#19981;&#24517;&#35201;&#30340;&#37325;&#26032;&#35745;&#31639;&#24050;&#32463;&#35745;&#31639;&#36807;&#30340;&#19978;&#19979;&#25991;&#65292;&#36825;&#21344;&#20102;&#24635;&#27169;&#22411;&#21069;&#21521;&#26102;&#38388;&#30340;37-40%&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;APIServe&#65292;&#36825;&#26159;&#38024;&#23545;API&#22686;&#24378;&#30340;LLM&#25512;&#29702;&#26694;&#26550;&#12290;APIServe&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#30001;API&#35843;&#29992;&#24341;&#36215;&#30340;GPU&#36164;&#28304;&#28010;&#36153;&#65292;&#24182;&#23558;&#33410;&#30465;&#30340;&#20869;&#23384;&#29992;&#20110;&#26381;&#21153;&#26356;&#22810;&#30340;&#35831;&#27714;&#12290;&#19982;&#29616;&#26377;&#30340;LLM&#25512;&#29702;&#31995;&#32479;&#30456;&#27604;&#65292;APIServe&#23558;&#25972;&#20307;&#26381;&#21153;&#21534;&#21520;&#37327;&#25552;&#21319;&#20102;1.6&#20493;&#65292;&#27599;&#31186;&#23436;&#25104;&#30340;&#35831;&#27714;&#22686;&#21152;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are increasingly integrated with external tools and APIs like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat API calls as new requests, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents APIServe, the first LLM inference framework targeting API-augmented LLMs. APISERVE minimizes the GPU resource waste caused by API calls and dedicates saved memory for serving more requests. APISERVE improves the overall serving throughput by 1.6x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEAM&#30340;&#20998;&#23376;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26500;&#24314;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#35789;&#27719;&#65292;&#35782;&#21035;&#36129;&#29486;&#20110;&#25152;&#38656;&#30446;&#26631;&#29305;&#24615;&#30340;&#37325;&#35201;&#29255;&#27573;&#65292;&#24182;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#26032;&#29255;&#27573;&#35789;&#27719;&#12290;</title><link>https://arxiv.org/abs/2310.00841</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#36827;&#34892;&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Drug Discovery with Dynamic Goal-aware Fragments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00841
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEAM&#30340;&#20998;&#23376;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26500;&#24314;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#35789;&#27719;&#65292;&#35782;&#21035;&#36129;&#29486;&#20110;&#25152;&#38656;&#30446;&#26631;&#29305;&#24615;&#30340;&#37325;&#35201;&#29255;&#27573;&#65292;&#24182;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#26032;&#29255;&#27573;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29255;&#27573;&#30340;&#33647;&#29289;&#21457;&#29616;&#26159;&#22312;&#24222;&#22823;&#30340;&#21270;&#23398;&#31354;&#38388;&#20013;&#21457;&#29616;&#33647;&#29289;&#20505;&#36873;&#29289;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#24182;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#29255;&#27573;&#25552;&#21462;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#30446;&#26631;&#21270;&#23398;&#24615;&#36136;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#29255;&#27573;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#33021;&#20351;&#29992;&#29983;&#25104;&#36807;&#31243;&#20013;&#26032;&#21457;&#29616;&#30340;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#26469;&#26356;&#26032;&#29255;&#27573;&#35789;&#27719;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#20998;&#23376;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#25552;&#21462;&#12289;&#32452;&#35013;&#21644;&#20462;&#25913;&#65288;GEAM&#65289;&#12290;GEAM&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#65292;&#20998;&#21035;&#36127;&#36131;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#25552;&#21462;&#12289;&#29255;&#27573;&#32452;&#35013;&#21644;&#29255;&#27573;&#20462;&#25913;&#12290;&#29255;&#27573;&#25552;&#21462;&#27169;&#22359;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#35782;&#21035;&#23545;&#25152;&#38656;&#30446;&#26631;&#29305;&#24615;&#26377;&#36129;&#29486;&#30340;&#37325;&#35201;&#29255;&#27573;&#65292;&#20174;&#32780;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fragment-based drug discovery is an effective strategy for discovering drug candidates in the vast chemical space, and has been widely employed in molecular generative models. However, many existing fragment extraction methods in such models do not take the target chemical properties into account or rely on heuristic rules. Additionally, the existing fragment-based generative models cannot update the fragment vocabulary with goal-aware fragments newly discovered during the generation. To this end, we propose a molecular generative framework for drug discovery, named Goal-aware fragment Extraction, Assembly, and Modification (GEAM). GEAM consists of three modules, each responsible for goal-aware fragment extraction, fragment assembly, and fragment modification. The fragment extraction module identifies important fragments contributing to the desired target properties with the information bottleneck principle, thereby constructing an effective goal-aware fragment vocabulary. Moreover, GE
&lt;/p&gt;</description></item><item><title>CaRiNG&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36776;&#35782;&#24615;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#38750;&#21487;&#36870;&#29983;&#25104;&#36807;&#31243;&#30340;&#26102;&#38388;&#22240;&#26524;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#32452;&#20998;&#65292;&#21363;&#20351;&#23427;&#20204;&#26469;&#33258;&#20110;&#38750;&#32447;&#24615;&#19988;&#38750;&#21487;&#36870;&#30340;&#28151;&#21512;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.14535</link><description>&lt;p&gt;
CaRiNG: &#22312;&#38750;&#21487;&#36870;&#29983;&#25104;&#36807;&#31243;&#19979;&#23398;&#20064;&#26102;&#38388;&#22240;&#26524;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process. (arXiv:2401.14535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14535
&lt;/p&gt;
&lt;p&gt;
CaRiNG&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#36776;&#35782;&#24615;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#38750;&#21487;&#36870;&#29983;&#25104;&#36807;&#31243;&#30340;&#26102;&#38388;&#22240;&#26524;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#32452;&#20998;&#65292;&#21363;&#20351;&#23427;&#20204;&#26469;&#33258;&#20110;&#38750;&#32447;&#24615;&#19988;&#38750;&#21487;&#36870;&#30340;&#28151;&#21512;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#21035;&#39034;&#24207;&#25968;&#25454;&#20013;&#28508;&#22312;&#30340;&#24310;&#36831;&#26102;&#38388;&#22240;&#26524;&#36807;&#31243;&#23545;&#20110;&#25226;&#25569;&#26102;&#38388;&#21160;&#21147;&#23398;&#21644;&#36827;&#34892;&#19979;&#28216;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#31283;&#20581;&#22320;&#35782;&#21035;&#36825;&#20123;&#28508;&#22312;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#20174;&#28508;&#22312;&#21464;&#37327;&#21040;&#35266;&#27979;&#25968;&#25454;&#30340;&#21487;&#36870;&#29983;&#25104;&#36807;&#31243;&#30340;&#20005;&#26684;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20551;&#35774;&#36890;&#24120;&#22312;&#21253;&#21547;&#20449;&#24687;&#25439;&#22833;&#30340;&#29616;&#23454;&#24212;&#29992;&#20013;&#38590;&#20197;&#28385;&#36275;&#12290;&#20363;&#22914;&#65292;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#23558;3D&#31354;&#38388;&#36716;&#21270;&#20026;2D&#22270;&#20687;&#65292;&#25110;&#32773;&#35270;&#35273;&#22362;&#25345;&#29616;&#35937;&#22312;&#24403;&#21069;&#24863;&#30693;&#20013;&#34701;&#20837;&#21382;&#21490;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#36776;&#35782;&#24615;&#29702;&#35770;&#65292;&#20801;&#35768;&#22312;&#38750;&#32447;&#24615;&#21644;&#38750;&#21487;&#36870;&#28151;&#21512;&#24773;&#20917;&#19979;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#32452;&#20998;&#12290;&#22312;&#27492;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;CaRiNG&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#21487;&#36776;&#35782;&#24615;&#30340;&#38750;&#21487;&#36870;&#29983;&#25104;&#26102;&#38388;&#25968;&#25454;&#30340;&#22240;&#26524;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the underlying time-delayed latent causal processes in sequential data is vital for grasping temporal dynamics and making downstream reasoning. While some recent methods can robustly identify these latent causal variables, they rely on strict assumptions about the invertible generation process from latent variables to observed data. However, these assumptions are often hard to satisfy in real-world applications containing information loss. For instance, the visual perception process translates a 3D space into 2D images, or the phenomenon of persistence of vision incorporates historical data into current perceptions. To address this challenge, we establish an identifiability theory that allows for the recovery of independent latent components even when they come from a nonlinear and non-invertible mix. Using this theory as a foundation, we propose a principled approach, CaRiNG, to learn the CAusal RepresentatIon of Non-invertible Generative temporal data with identifiability
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>GA-SmaAt-GNet&#26159;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#26550;&#26500;&#65292;&#29992;&#20110;&#25913;&#36827;&#26497;&#31471;&#38477;&#27700;&#26242;&#26102;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#22120;&#21644;&#20351;&#29992;&#27880;&#24847;&#21147;&#22686;&#24378;&#37492;&#21035;&#22120;&#26469;&#21033;&#29992;&#38477;&#27700;&#25513;&#30721;&#25552;&#20379;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.09881</link><description>&lt;p&gt;
GA-SmaAt-GNet&#65306;&#29992;&#20110;&#26497;&#31471;&#38477;&#27700;&#26242;&#26102;&#39044;&#27979;&#30340;&#29983;&#25104;&#23545;&#25239;&#23567;&#22411;&#27880;&#24847;&#21147;GNet
&lt;/p&gt;
&lt;p&gt;
GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme Precipitation Nowcasting. (arXiv:2401.09881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09881
&lt;/p&gt;
&lt;p&gt;
GA-SmaAt-GNet&#26159;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#26550;&#26500;&#65292;&#29992;&#20110;&#25913;&#36827;&#26497;&#31471;&#38477;&#27700;&#26242;&#26102;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#22120;&#21644;&#20351;&#29992;&#27880;&#24847;&#21147;&#22686;&#24378;&#37492;&#21035;&#22120;&#26469;&#21033;&#29992;&#38477;&#27700;&#25513;&#30721;&#25552;&#20379;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#26041;&#27861;&#22312;&#21508;&#31181;&#27668;&#35937;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#30456;&#24403;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#26497;&#31471;&#22825;&#27668;&#26465;&#20214;&#26102;&#24120;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GA-SmaAt-GNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#23545;&#25239;&#32467;&#26500;&#65292;&#21033;&#29992;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26497;&#31471;&#38477;&#27700;&#26242;&#26102;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#25104;&#21151;&#30340;SmaAt-UNet&#32467;&#26500;&#26500;&#24314;&#30340;&#26032;&#22411;SmaAt-GNet&#20316;&#20026;&#29983;&#25104;&#22120;&#12290;&#35813;&#32593;&#32476;&#23558;&#38477;&#27700;&#25513;&#30721;&#65288;&#20108;&#20540;&#21270;&#38477;&#27700;&#22270;&#65289;&#20316;&#20026;&#38468;&#21152;&#25968;&#25454;&#28304;&#65292;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#36827;&#34892;&#25913;&#36827;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;GA-SmaAt-GNet&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#28789;&#24863;&#26469;&#33258;&#20110;&#33879;&#21517;&#30340;Pix2Pix&#32467;&#26500;&#30340;&#27880;&#24847;&#21147;&#22686;&#24378;&#37492;&#21035;&#22120;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#33655;&#20848;&#30340;&#23454;&#38469;&#38477;&#27700;&#25968;&#25454;&#38598;&#23545;GA-SmaAt-GNet&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, data-driven modeling approaches have gained considerable traction in various meteorological applications, particularly in the realm of weather forecasting. However, these approaches often encounter challenges when dealing with extreme weather conditions. In light of this, we propose GA-SmaAt-GNet, a novel generative adversarial architecture that makes use of two methodologies aimed at enhancing the performance of deep learning models for extreme precipitation nowcasting. Firstly, it uses a novel SmaAt-GNet built upon the successful SmaAt-UNet architecture as generator. This network incorporates precipitation masks (binarized precipitation maps) as an additional data source, leveraging valuable information for improved predictions. Additionally, GA-SmaAt-GNet utilizes an attention-augmented discriminator inspired by the well-established Pix2Pix architecture. Furthermore, we assess the performance of GA-SmaAt-GNet using real-life precipitation dataset from the Netherland
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06102</link><description>&lt;p&gt;
Patchscope: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#39564;&#35777;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;LLM&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#27169;&#22411;&#26412;&#36523;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;Patchscopes&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#22238;&#31572;&#20851;&#20110;LLM&#35745;&#31639;&#30340;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20808;&#21069;&#22522;&#20110;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#21644;&#24178;&#39044;LLM&#35745;&#31639;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35813;&#26694;&#26550;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;Patchscope&#21487;&#20197;&#24357;&#34917;&#20248;&#21183;&#65292;&#22914;&#26816;&#26597;&#26089;&#26399;&#23618;&#22833;&#36133;&#25110;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#12290;&#38500;&#20102;&#32479;&#19968;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;Patchscopes&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#35299;&#37322;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.12728</link><description>&lt;p&gt;
Lookahead:&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#26080;&#25439;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy. (arXiv:2312.12728v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#38382;&#31572;&#12289;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20687;&#25903;&#20184;&#23453;&#36825;&#26679;&#20026;&#25968;&#21313;&#20159;&#29992;&#25143;&#25552;&#20379;&#37325;&#35201;&#37329;&#34701;&#20135;&#21697;&#30340;&#38656;&#35201;&#20934;&#30830;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25903;&#20184;&#23453;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#23558;LLMs&#19982;&#26368;&#20934;&#30830;&#21644;&#26368;&#26032;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20026;&#25968;&#30334;&#19975;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#30340;&#30495;&#23454;&#20135;&#21697;&#26469;&#35828;&#65292;LLMs&#30340;&#25512;&#29702;&#36895;&#24230;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#23454;&#39564;&#24615;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;RAG&#31995;&#32479;&#30340;&#36895;&#24230;&#22823;&#24133;&#25552;&#21319;&#21644;&#25104;&#26412;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25345;&#30528;&#26080;&#25439;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#12290;&#22312;&#20256;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#20196;&#29260;&#37117;&#30001;LLMs&#25353;&#39034;&#24207;&#29983;&#25104;&#65292;&#23548;&#33268;&#30340;&#26102;&#38388;&#28040;&#32791;&#19982;&#29983;&#25104;&#30340;&#20196;&#29260;&#25968;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation (RAG) system that grounds LLMs on the most accurate and up-to-date information. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.  Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our RAG system, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13230</link><description>&lt;p&gt;
&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20449;&#20219;&#22495;&#30340;&#22312;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#21644;&#28216;&#25103;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31867;&#21035;&#20013;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#20027;&#35201;&#24378;&#35843;&#23545;&#39044;&#26399;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#32570;&#20047;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65307;&#36890;&#36807;&#20248;&#21270;&#35813;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;&#36817;&#20046;&#24635;&#20307;&#24615;&#33021;&#26679;&#26412;&#30340;&#19979;&#30028;&#65288;&#32477;&#23545;&#24615;&#33021;&#65289;&#21576;&#29616;&#21333;&#35843;&#25913;&#36827;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36817;&#20284;&#23545;&#36825;&#20010;&#29702;&#35770;&#22522;&#30784;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#31216;&#20026;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#25484;&#25569;Atari&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;APO&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20063;&#26174;&#33879;&#25913;&#21892;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.12942</link><description>&lt;p&gt;
&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#12290;Siegelmann&#21644;Sontag(1992)&#26366;&#32463;&#23637;&#31034;&#20102;&#20855;&#26377;&#26377;&#29702;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#20197;&#21450;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#30340;RNNs&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#12290;&#28982;&#32780;&#65292;LMs&#19981;&#20165;&#23450;&#20041;&#20102;&#23383;&#31526;&#20018;&#19978;&#30340;&#21152;&#26435;&#65292;&#36824;&#23450;&#20041;&#20102;(&#38750;&#21152;&#26435;)&#35821;&#35328;&#25104;&#21592;&#20851;&#31995;&#65292;&#23545;RNN LMs&#65288;RLMs&#65289;&#30340;&#35745;&#31639;&#33021;&#21147;&#20998;&#26512;&#24212;&#35813;&#21453;&#26144;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#23558;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#25193;&#23637;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26377;&#29702;&#26435;&#37325;&#30340;RLM&#21644;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#26469;&#27169;&#25311;&#20219;&#20309;&#27010;&#29575;&#22270;&#28789;&#26426;(PTM)&#12290;&#30001;&#20110;&#22312;&#23454;&#36341;&#20013;&#65292;RLMs&#23454;&#26102;&#24037;&#20316;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#22788;&#29702;&#19968;&#20010;&#31526;&#21495;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#19978;&#36848;&#32467;&#26524;&#20316;&#20026;RLMs&#34920;&#36798;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23637;&#31034;&#22312;&#23454;&#26102;&#35745;&#31639;&#38480;&#21046;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#30830;&#23450;&#24615;&#23454;&#26102;&#26377;&#29702;PTMs&#26469;&#25552;&#20379;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26694;&#26550;&#65292;&#20026;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08732</link><description>&lt;p&gt;
&#21487;&#35777;&#20445;&#20581;&#24247;&#21830;&#21153;&#23383;&#20307;&#23609;&#21253;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;(&#35793;&#27880;)&#27700;&#12290;
&lt;/p&gt;
&lt;p&gt;
Provably Robust Cost-Sensitive Learning via Randomized Smoothing. (arXiv:2310.08732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26694;&#26550;&#65292;&#20026;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#20110;&#22312;&#25104;&#26412;&#25935;&#24863;&#30340;&#24773;&#26223;&#19979;&#23398;&#20064;&#23545;&#25239;&#24615;&#31283;&#20581;&#20998;&#31867;&#22120;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#21464;&#25442;&#30340;&#28508;&#22312;&#21361;&#23475;&#34987;&#32534;&#30721;&#22312;&#19968;&#20010;&#20108;&#36827;&#21046;&#25104;&#26412;&#30697;&#38453;&#20013;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#26080;&#27861;&#35777;&#26126;&#31283;&#20581;&#24615;&#65292;&#35201;&#20040;&#23384;&#22312;&#22266;&#26377;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#24179;&#28369;&#65292;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#31283;&#20581;&#24615;&#35748;&#35777;&#26694;&#26550;&#65292;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35777;&#26126;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#24615;&#12290;&#24314;&#31435;&#22312;&#19968;&#31181;&#25104;&#26412;&#25935;&#24863;&#35748;&#35777;&#21322;&#24452;&#30340;&#27010;&#24565;&#20043;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35843;&#25972;&#26631;&#20934;&#30340;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#27969;&#31243;&#65292;&#20026;&#20219;&#20309;&#25104;&#26412;&#30697;&#38453;&#20135;&#29983;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#30340;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#20248;&#21270;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#38024;&#23545;&#25104;&#26412;&#25935;&#24863;&#31283;&#20581;&#24615;&#20248;&#21270;&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#22312;&#22270;&#20687;&#22522;&#20934;&#27979;&#35797;&#21644;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on learning adversarially robust classifiers under a cost-sensitive scenario, where the potential harm of different classwise adversarial transformations is encoded in a binary cost matrix. Existing methods are either empirical that cannot certify robustness or suffer from inherent scalability issues. In this work, we study whether randomized smoothing, a more scalable robustness certification framework, can be leveraged to certify cost-sensitive robustness. Built upon a notion of cost-sensitive certified radius, we show how to adapt the standard randomized smoothing certification pipeline to produce tight robustness guarantees for any cost matrix. In addition, with fine-grained certified radius optimization schemes specifically designed for different data subgroups, we propose an algorithm to train smoothed classifiers that are optimized for cost-sensitive robustness. Extensive experiments on image benchmarks and a real-world medical dataset demonstrate the superiority of our
&lt;/p&gt;</description></item><item><title>Decision ConvFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#20316;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#21367;&#31215;&#36807;&#28388;&#26469;&#25429;&#25417;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#20851;&#32852;&#65292;&#21516;&#26102;&#22312;&#21508;&#20010;&#26631;&#20934;RL&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03022</link><description>&lt;p&gt;
Decision ConvFormer: MetaFormer&#20013;&#30340;&#26412;&#22320;&#36807;&#28388;&#23545;&#20110;&#20915;&#31574;&#21046;&#23450;&#24050;&#32463;&#36275;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making. (arXiv:2310.03022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03022
&lt;/p&gt;
&lt;p&gt;
Decision ConvFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#20316;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#21367;&#31215;&#36807;&#28388;&#26469;&#25429;&#25417;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#20851;&#32852;&#65292;&#21516;&#26102;&#22312;&#21508;&#20010;&#26631;&#20934;RL&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;Decision Transformer&#65288;DT&#65289;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#36880;&#28176;&#23853;&#38706;&#22836;&#35282;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;DT&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#19981;&#36866;&#21512;&#25429;&#25417;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#24314;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#36712;&#36857;&#20013;&#22266;&#26377;&#30340;&#23616;&#37096;&#20381;&#36182;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;DT&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MetaFormer&#26550;&#26500;&#30340;&#26032;&#22411;&#21160;&#20316;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#31216;&#20026;Decision ConvFormer&#65288;DC&#65289;&#12290;DC&#37319;&#29992;&#26412;&#22320;&#21367;&#31215;&#36807;&#28388;&#20316;&#20026;&#20196;&#29260;&#28151;&#21512;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;RL&#25968;&#25454;&#38598;&#20013;&#22266;&#26377;&#30340;&#23616;&#37096;&#20851;&#32852;&#12290;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;DC&#22312;&#21508;&#31181;&#26631;&#20934;RL&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of Transformer in natural language processing has sparked its use in various domains. In offline reinforcement learning (RL), Decision Transformer (DT) is emerging as a promising model based on Transformer. However, we discovered that the attention module of DT is not appropriate to capture the inherent local dependence pattern in trajectories of RL modeled as a Markov decision process. To overcome the limitations of DT, we propose a novel action sequence predictor, named Decision ConvFormer (DC), based on the architecture of MetaFormer, which is a general structure to process multiple entities in parallel and understand the interrelationship among the multiple entities. DC employs local convolution filtering as the token mixer and can effectively capture the inherent local associations of the RL dataset. In extensive experiments, DC achieved state-of-the-art performance across various standard RL benchmarks while requiring fewer resources. Furthermore, we show that 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#23545;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.16733</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#38887;&#24615;&#65306;&#20998;&#26512;&#21644;&#21152;&#22266;&#25216;&#26415;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques. (arXiv:2309.16733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16733
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#23545;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30446;&#21069;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#26159;&#26368;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#20043;&#19968;&#65292;&#22914;&#35270;&#35273;&#12289;&#33258;&#20027;&#31995;&#32479;&#31561;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#20154;&#20204;&#23545;ML&#24212;&#29992;&#22312;&#24213;&#23618;&#30828;&#20214;&#25925;&#38556;&#24433;&#21709;&#19979;&#30340;&#20998;&#26512;&#21644;&#35774;&#35745;&#20570;&#20986;&#20102;&#22823;&#37327;&#36129;&#29486;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#27425;&#28145;&#20837;&#30340;&#22238;&#39038;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;ML&#25216;&#26415;&#20043;&#19968;&#65289;&#23545;&#25239;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#30340;&#24050;&#26377;&#30693;&#35782;&#65292;&#28165;&#26224;&#22320;&#21576;&#29616;&#20102;&#36825;&#19968;&#25991;&#29486;&#27969;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25991;&#31456;&#22522;&#20110;2019&#24180;1&#26376;&#33267;2023&#24180;3&#26376;&#38388;&#21457;&#34920;&#30340;163&#31687;&#31185;&#23398;&#35770;&#25991;&#65292;&#37319;&#29992;&#20998;&#31867;&#26694;&#26550;&#26469;&#35299;&#35835;&#21644;&#31361;&#20986;&#30740;&#31350;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#29305;&#28857;&#65292;&#20174;&#24037;&#20316;&#30340;&#20027;&#35201;&#33539;&#22260;&#12289;&#37319;&#29992;&#30340;&#25925;&#38556;&#21644;&#38169;&#35823;&#27169;&#22411;&#31561;&#22810;&#20010;&#21442;&#25968;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is currently being exploited in numerous applications being one of the most effective Artificial Intelligence (AI) technologies, used in diverse fields, such as vision, autonomous systems, and alike. The trend motivated a significant amount of contributions to the analysis and design of ML applications against faults affecting the underlying hardware. The authors investigate the existing body of knowledge on Deep Learning (among ML techniques) resilience against hardware faults systematically through a thoughtful review in which the strengths and weaknesses of this literature stream are presented clearly and then future avenues of research are set out. The review is based on 163 scientific articles published between January 2019 and March 2023. The authors adopt a classifying framework to interpret and highlight research similarities and peculiarities, based on several parameters, starting from the main scope of the work, the adopted fault and error models, to the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#25554;&#20540;&#22120;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#20195;&#25968;&#21644;&#32479;&#35745;&#23646;&#24615;&#65292;&#24182;&#20026;&#26368;&#23567;l2&#33539;&#25968;OLS&#25554;&#20540;&#22120;&#25552;&#20379;&#20102;&#22522;&#26412;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#29702;&#35299;OLS&#25554;&#20540;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.15769</link><description>&lt;p&gt;
&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#25554;&#20540;&#22120;&#30340;&#20195;&#25968;&#21644;&#32479;&#35745;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Algebraic and Statistical Properties of the Ordinary Least Squares Interpolator. (arXiv:2309.15769v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#25554;&#20540;&#22120;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#20195;&#25968;&#21644;&#32479;&#35745;&#23646;&#24615;&#65292;&#24182;&#20026;&#26368;&#23567;l2&#33539;&#25968;OLS&#25554;&#20540;&#22120;&#25552;&#20379;&#20102;&#22522;&#26412;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#29702;&#35299;OLS&#25554;&#20540;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#36229;&#21442;&#25968;&#21270;&#32479;&#35745;&#27169;&#22411;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#37325;&#22823;&#30340;&#29702;&#35770;&#20852;&#36259;&#12290;&#37492;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#25554;&#20540;&#22120;&#24050;&#25104;&#20026;&#33719;&#24471;&#23545;&#36825;&#31181;&#29616;&#35937;&#22522;&#30784;&#27934;&#23519;&#21147;&#30340;&#20851;&#38190;&#25152;&#22312;&#12290;&#23613;&#31649;OLS&#22312;&#32463;&#20856;&#29615;&#22659;&#20013;&#30340;&#24615;&#36136;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#24314;&#31435;&#65292;&#20294;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#36824;&#27809;&#26377;&#20687;&#23725;&#22238;&#24402;&#25110;&#22871;&#32034;&#22238;&#24402;&#37027;&#26679;&#34987;&#25506;&#32034;&#24471;&#37027;&#20040;&#36879;&#24443;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#26368;&#23567;l2&#33539;&#25968;OLS&#25554;&#20540;&#22120;&#25552;&#20379;&#22522;&#26412;&#30340;&#20195;&#25968;&#21644;&#32479;&#35745;&#32467;&#26524;&#26469;&#36129;&#29486;&#20110;&#36825;&#19968;&#26085;&#30410;&#22686;&#38271;&#30340;&#25991;&#29486;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#65288;i&#65289;&#30041;-k-out&#27531;&#24046;&#20844;&#24335;&#30340;&#39640;&#32500;&#20195;&#25968;&#31561;&#20215;&#29289;&#65292;&#65288;ii&#65289; Cochran&#20844;&#24335;&#65292;&#20197;&#21450;&#65288;iii&#65289;Frisch-Waugh-Lovell&#23450;&#29702;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#29702;&#35299;OLS&#25554;&#20540;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20855;&#26377;&#23454;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning research has uncovered the phenomenon of benign overfitting for over-parameterized statistical models, which has drawn significant theoretical interest in recent years. Given its simplicity and practicality, the ordinary least squares (OLS) interpolator has become essential to gain foundational insights into this phenomenon. While properties of OLS are well established in classical settings, its behavior in high-dimensional settings is less explored (unlike for ridge or lasso regression) though significant progress has been made of late. We contribute to this growing literature by providing fundamental algebraic and statistical results for the minimum $\ell_2$-norm OLS interpolator. In particular, we provide high-dimensional algebraic equivalents of (i) the leave-$k$-out residual formula, (ii) Cochran's formula, and (iii) the Frisch-Waugh-Lovell theorem. These results aid in understanding the OLS interpolator's ability to generalize and have substantive implications for c
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20154;&#29983;&#25104;&#30340;&#22240;&#26524;&#30693;&#35782;&#26469;&#25913;&#36827;&#25968;&#25454;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#25351;&#31034;&#20102;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10211</link><description>&lt;p&gt;
&#22240;&#26524;&#29702;&#35770;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#34920;&#31034;&#30340;&#25913;&#36827;&#25928;&#26524;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causal Theories and Structural Data Representations for Improving Out-of-Distribution Classification. (arXiv:2309.10211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10211
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#29983;&#25104;&#30340;&#22240;&#26524;&#30693;&#35782;&#26469;&#25913;&#36827;&#25968;&#25454;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#25351;&#31034;&#20102;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22914;&#20309;&#20351;&#29992;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22240;&#26524;&#29702;&#35770;&#21644;&#26469;&#33258;&#21160;&#21147;&#31995;&#32479;&#25991;&#29486;&#30340;&#24037;&#20855;&#65292;&#26469;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#20998;&#31867;&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#26469;&#23637;&#31034;&#65292;&#20351;&#29992;&#23558;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#19981;&#21464;&#32467;&#26500;&#22240;&#26524;&#29305;&#24449;&#26126;&#30830;&#26174;&#31034;&#22312;&#25968;&#25454;&#34920;&#31034;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;&#26356;&#20026;&#22825;&#30495;&#30340;&#25968;&#25454;&#34920;&#31034;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#20154;&#29983;&#25104;&#30340;&#22240;&#26524;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#32773;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#23548;&#33268;&#26356;&#21152;&#26126;&#30830;&#35268;&#33539;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#12290;&#36825;&#36827;&#32780;&#25351;&#31034;&#20102;&#36890;&#36807;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#23454;&#36341;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#26356;&#24191;&#27867;&#21162;&#21147;&#20013;&#30340;&#21160;&#21147;&#31995;&#32479;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider how human-centered causal theories and tools from the dynamical systems literature can be deployed to guide the representation of data when training neural networks for complex classification tasks. Specifically, we use simulated data to show that training a neural network with a data representation that makes explicit the invariant structural causal features of the data generating process of an epidemic system improves out-of-distribution (OOD) generalization performance on a classification task as compared to a more naive approach to data representation. We take these results to demonstrate that using human-generated causal knowledge to reduce the epistemic uncertainty of ML developers can lead to more well-specified ML pipelines. This, in turn, points to the utility of a dynamical systems approach to the broader effort aimed at improving the robustness and safety of machine learning systems via improved ML system development practices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; ROBAI &#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#24555;&#36895;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#36830;&#32493;&#22238;&#21512;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#35813;&#31639;&#27861;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#21644;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#35201;&#27714;&#19979;&#22343;&#23454;&#29616;&#20102;&#28176;&#36827;&#26368;&#20248;&#36951;&#25022;&#65292;&#24182;&#19988;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#22312;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log^2 T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#12290;</title><link>http://arxiv.org/abs/2309.00591</link><description>&lt;p&gt;
&#24555;&#36895;&#21644;&#36951;&#25022;&#26368;&#23567;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65306;&#22522;&#26412;&#38480;&#21046;&#21644;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms. (arXiv:2309.00591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; ROBAI &#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#24555;&#36895;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#36830;&#32493;&#22238;&#21512;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#35813;&#31639;&#27861;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#21644;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#35201;&#27714;&#19979;&#22343;&#23454;&#29616;&#20102;&#28176;&#36827;&#26368;&#20248;&#36951;&#25022;&#65292;&#24182;&#19988;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#22312;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log^2 T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#21452;&#37325;&#30446;&#26631;&#30340;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;(MAB)&#38382;&#39064;&#65306;(i) &#24555;&#36895;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#20197;&#21450;(ii) &#22312;&#19968;&#31995;&#21015;T&#20010;&#36830;&#32493;&#22238;&#21512;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#23613;&#31649;&#27599;&#20010;&#30446;&#26631;&#37117;&#24050;&#32463;&#24471;&#21040;&#20102;&#29420;&#31435;&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#21363;(i)&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;(ii)&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#20294;&#26159;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#26631;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#23613;&#31649;&#23427;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#8221;(ROBAI)&#65292;&#26088;&#22312;&#23454;&#29616;&#36825;&#20004;&#20010;&#21452;&#37325;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#20855;&#26377;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#21644;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#35201;&#27714;&#30340;ROBAI&#65292;&#25105;&#20204;&#20998;&#21035;&#25552;&#20986;&#20102;$\mathsf{EOCP}$&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;&#65292;&#19981;&#20165;&#22312;&#39640;&#26031;&#32769;&#34382;&#26426;&#21644;&#19968;&#33324;&#32769;&#34382;&#26426;&#20013;&#36798;&#21040;&#20102;&#28176;&#36827;&#26368;&#20248;&#36951;&#25022;&#65292;&#32780;&#19988;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#19979;&#65292;&#22312;$\mathcal{O}(\log T)$&#22238;&#21512;&#20869;&#36873;&#25321;&#20102;&#26368;&#20339;&#33218;&#65292;&#22312;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#19979;&#65292;&#36873;&#25321;&#20102;&#26368;&#20339;&#33218;&#22312;$\mathcal{O}(\log^2 T)$&#22238;&#21512;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a stochastic multi-armed bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of $T$ consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces \emph{Regret Optimal Best Arm Identification} (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present the $\mathsf{EOCP}$ algorithm and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in $\mathcal{O}(\log T)$ rounds with pre-determined stopping time and $\mathcal{O}(\log^2 T)$ rounds with adaptive stopping ti
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24182;&#35299;&#20915;&#22240;&#20026;&#32570;&#22833;&#20540;&#24341;&#20837;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13047</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#35266;&#27979;&#25968;&#25454;&#30340;&#32852;&#37030;&#22240;&#26524;&#25928;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Causal Effects from Incomplete Observational Data. (arXiv:2308.13047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13047
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24182;&#35299;&#20915;&#22240;&#20026;&#32570;&#22833;&#20540;&#24341;&#20837;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#23545;&#22240;&#26524;&#25512;&#26029;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#26080;&#27861;&#21512;&#24182;&#20026;&#19968;&#20010;&#23454;&#20307;&#65292;&#32780;&#20854;&#20013;&#30340;&#32570;&#22833;&#20540;&#21487;&#33021;&#20250;&#24341;&#20837;&#20559;&#24046;&#21040;&#22240;&#26524;&#20272;&#35745;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#32852;&#37030;&#22240;&#26524;&#25512;&#26029;&#65292;&#20174;&#32780;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25439;&#22833;&#20989;&#25968;&#25286;&#20998;&#20026;&#22810;&#20010;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#29305;&#23450;&#25968;&#25454;&#28304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32570;&#22833;&#38543;&#26426;&#20551;&#35774;&#19979;&#32771;&#34385;&#20102;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#20272;&#35745;&#20102;&#22240;&#26524;&#20272;&#35745;&#30340;&#39640;&#38454;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#20998;&#25955;&#30340;&#25968;&#25454;&#28304;&#20013;&#24674;&#22797;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20272;&#35745;&#20102;&#24322;&#36136;&#30340;&#26465;&#20214;&#20998;&#24067;&#20197;&#24212;&#23545;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized and incomplete data sources are prevalent in real-world applications, posing a formidable challenge for causal inference. These sources cannot be consolidated into a single entity owing to privacy constraints, and the presence of missing values within them can potentially introduce bias to the causal estimands. We introduce a new approach for federated causal inference from incomplete data, enabling the estimation of causal effects from multiple decentralized and incomplete data sources. Our approach disentangles the loss function into multiple components, each corresponding to a specific data source with missing values. Our approach accounts for the missing data under the missing at random assumption, while also estimating higher-order statistics of the causal estimands. Our method recovers the conditional distribution of missing confounders given the observed confounders from the decentralized data sources to identify causal effects. Our framework estimates heterogeneou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#65292;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#20272;&#35745;&#22120;&#12290;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13885</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#39640;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Estimation of the Local Robustness of Machine Learning Models. (arXiv:2307.13885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#65292;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#20272;&#35745;&#22120;&#12290;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#23545;&#22122;&#22768;&#36755;&#20837;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22122;&#22768;&#65288;&#36890;&#24120;&#26159;&#38543;&#26426;&#30340;&#65289;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#26469;&#25429;&#25417;&#65292;&#21363;&#22312;&#36755;&#20837;&#21608;&#22260;&#30340;&#23616;&#37096;&#21306;&#22495;&#20869;&#27169;&#22411;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#30340;&#35745;&#31639;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#26420;&#32032;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#32780;&#35328;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#24320;&#21457;&#20102;&#39318;&#20010;&#20998;&#26512;&#20272;&#35745;&#22120;&#65292;&#20197;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#36825;&#20123;&#20272;&#35745;&#22120;&#30340;&#25512;&#23548;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;&#40065;&#26834;&#24615;&#19982;&#38543;&#26426;&#24179;&#28369;&#21644;softmax&#27010;&#29575;&#31561;&#27010;&#24565;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estima
&lt;/p&gt;</description></item><item><title>FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.11650</link><description>&lt;p&gt;
FedNoisy: &#20998;&#24067;&#24335;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedNoisy: Federated Noisy Label Learning Benchmark. (arXiv:2306.11650v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11650
&lt;/p&gt;
&lt;p&gt;
FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#24050;&#32463;&#22240;&#20026;&#26080;&#38656;&#23545;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#25935;&#24863;&#25968;&#25454;&#36827;&#34892;&#32858;&#21512;&#32780;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#20294;&#26159;&#65292;&#25968;&#25454;&#38548;&#31163;&#30340;&#20998;&#24067;&#24335;&#21644;&#23396;&#31435;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#36136;&#37327;&#30340;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#26631;&#31614;&#30340;&#24178;&#25200;&#12290;&#35768;&#22810;&#21162;&#21147;&#37117;&#33268;&#21147;&#20110;&#22312;&#38598;&#20013;&#24335;&#25110;&#32852;&#21512;&#24335;&#29615;&#22659;&#20013;&#38450;&#24481;&#22122;&#22768;&#26631;&#31614;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#32771;&#34385;&#21508;&#31181;&#20856;&#22411;&#32852;&#21512;&#23398;&#20064;&#22330;&#26223;&#20013;&#22122;&#22768;&#26631;&#31614;&#24433;&#21709;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20805;&#20998;&#25506;&#32034;&#28508;&#22312;&#30340;&#32852;&#21512;&#22122;&#22768;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25506;&#32034;&#36825;&#20123;&#25968;&#25454;&#35774;&#32622;&#30340;&#29305;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#65292;&#36825;&#21487;&#33021;&#25351;&#23548;&#26410;&#26469;&#30340;&#26041;&#27861;&#24320;&#21457;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20986;&#30340;20&#20010;&#22522;&#26412;&#35774;&#32622;&#65292;&#36866;&#29992;&#20110;5&#20010;&#20197;&#19978;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels. Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings. However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings. In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings. Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future. We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#20041;ID&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29289;&#21697;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36825;&#20123;ID&#26159;&#20174;&#20869;&#23481;&#23884;&#20837;&#20013;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#25429;&#25417;&#27010;&#24565;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#28040;&#38500;ID&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35821;&#20041;ID&#33021;&#26356;&#22909;&#22320;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.08121</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;ID&#36827;&#34892;&#26356;&#22909;&#30340;&#27867;&#21270;&#65306;&#25512;&#33616;&#25490;&#21517;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Better Generalization with Semantic IDs: A case study in Ranking for Recommendations. (arXiv:2306.08121v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#20041;ID&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29289;&#21697;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36825;&#20123;ID&#26159;&#20174;&#20869;&#23481;&#23884;&#20837;&#20013;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#25429;&#25417;&#27010;&#24565;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#28040;&#38500;ID&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35821;&#20041;ID&#33021;&#26356;&#22909;&#22320;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#27169;&#22411;&#20013;&#65292;&#35757;&#32451;&#22909;&#30340;&#29289;&#21697;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36890;&#24120;&#65292;&#19968;&#39033;&#21830;&#21697;&#20250;&#34987;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#38543;&#26426;&#29983;&#25104;&#30340;ID&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#36890;&#36807;&#23398;&#20064;&#19982;&#38543;&#26426;ID&#20540;&#30456;&#23545;&#24212;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;&#29289;&#21697;&#25968;&#37327;&#22823;&#19988;&#29289;&#21697;&#26381;&#20174;&#24130;&#24459;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#8212;&#8212;&#36825;&#26159;&#30495;&#23454;&#19990;&#30028;&#25512;&#33616;&#31995;&#32479;&#30340;&#20856;&#22411;&#29305;&#24449;&#8212;&#8212;&#20250;&#26377;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;&#36825;&#20250;&#23548;&#33268;&#29289;&#21697;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#27169;&#22411;&#26080;&#27861;&#23545;&#23614;&#37096;&#21644;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#29289;&#21697;&#36827;&#34892;&#21487;&#38752;&#30340;&#25512;&#33616;&#12290;&#23436;&#20840;&#28040;&#38500;&#36825;&#20123;ID&#29305;&#24449;&#21450;&#20854;&#23398;&#20064;&#30340;&#23884;&#20837;&#20197;&#35299;&#20915;&#20919;&#21551;&#21160;&#38382;&#39064;&#20250;&#20005;&#37325;&#38477;&#20302;&#25512;&#33616;&#36136;&#37327;&#12290;&#22522;&#20110;&#20869;&#23481;&#30340;&#29289;&#21697;&#23884;&#20837;&#26356;&#20026;&#21487;&#38752;&#65292;&#20294;&#23545;&#20110;&#29992;&#25143;&#36807;&#21435;&#30340;&#29289;&#21697;&#20132;&#20114;&#24207;&#21015;&#26469;&#35828;&#65292;&#23427;&#20204;&#25104;&#26412;&#39640;&#19988;&#20351;&#29992;&#22256;&#38590;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#20041;ID&#26469;&#34920;&#31034;&#31163;&#25955;&#30340;&#29289;&#21697;&#65292;&#36825;&#20123;ID&#26159;&#36890;&#36807;&#20351;&#29992;RQ-VAE&#20174;&#20869;&#23481;&#23884;&#20837;&#20013;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#25429;&#25417;&#27010;&#24565;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training good representations for items is critical in recommender models. Typically, an item is assigned a unique randomly generated ID, and is commonly represented by learning an embedding corresponding to the value of the random ID. Although widely used, this approach have limitations when the number of items are large and items are power-law distributed -- typical characteristics of real-world recommendation systems. This leads to the item cold-start problem, where the model is unable to make reliable inferences for tail and previously unseen items. Removing these ID features and their learned embeddings altogether to combat cold-start issue severely degrades the recommendation quality. Content-based item embeddings are more reliable, but they are expensive to store and use, particularly for users' past item interaction sequence. In this paper, we use Semantic IDs, a compact discrete item representations learned from content embeddings using RQ-VAE that captures hierarchy of concep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14521</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#21033;&#29992;&#20102;&#26576;&#20123;&#23646;&#24615;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;&#22823;&#22810;&#25968;&#31034;&#20363;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#24182;&#19981;&#36275;&#20197;&#39044;&#27979;&#36825;&#20123;&#31867;&#21035;&#12290;&#23398;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#22312;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21518;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20250;&#38477;&#20302;&#27169;&#22411;&#23545;&#19981;&#23637;&#29616;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#19968;&#23567;&#32452;&#24102;&#26377;&#34394;&#20551;&#23646;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#24179;&#34913;&#25152;&#26377;&#31867;&#21035;&#20013;&#30340;&#34394;&#20551;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#65292;&#21253;&#25324;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation
&lt;/p&gt;</description></item><item><title>HyFL&#26159;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#25216;&#26415;&#21644;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20445;&#35777;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#38544;&#31169;&#23433;&#20840;&#65292;&#26377;&#21161;&#20110;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2302.09904</link><description>&lt;p&gt;
HyFL:&#19968;&#31181;&#29992;&#20110;&#31169;&#26377;&#32852;&#21512;&#23398;&#20064;&#30340;&#28151;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyFL: A Hybrid Framework For Private Federated Learning. (arXiv:2302.09904v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09904
&lt;/p&gt;
&lt;p&gt;
HyFL&#26159;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#25216;&#26415;&#21644;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20445;&#35777;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#38544;&#31169;&#23433;&#20840;&#65292;&#26377;&#21161;&#20110;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#35774;&#22791;&#19978;&#20445;&#30041;&#35757;&#32451;&#25968;&#25454;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;FL&#20013;&#30340;&#28431;&#27934;&#65292;&#21253;&#25324;&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#26356;&#26032;&#29978;&#33267;&#25972;&#20010;&#20840;&#23616;&#27169;&#22411;&#27844;&#28431;&#25935;&#24863;&#20449;&#24687;&#12290;&#34429;&#28982;&#20851;&#27880;&#28857;&#24050;&#25918;&#22312;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#19978;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#20840;&#23616;&#27169;&#22411;&#38544;&#31169;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23458;&#25143;&#31471;&#26412;&#22320;&#35757;&#32451;&#20026;&#24694;&#24847;&#23458;&#25143;&#31471;&#21551;&#21160;&#24378;&#22823;&#30340;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#24320;&#36767;&#20102;&#36884;&#24452;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#24037;&#20316;&#25552;&#20379;&#20840;&#38754;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HyFL&#65292;&#36825;&#26159;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#21487;&#23454;&#29616;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#38544;&#31169;&#65292;&#24182;&#20419;&#36827;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;HyFL&#30340;&#22522;&#30784;&#26159;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#25216;&#26415;&#21644;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#30340;&#29420;&#29305;&#32452;&#21512;&#12290;&#22312;HyFL&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#22312;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#19978;&#36827;&#34892;&#23433;&#20840;&#32858;&#21512;&#65292;&#20197;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HyFL&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#65292;&#21516;&#26102;&#30830;&#20445;&#23458;&#25143;&#31471;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24378;&#22823;&#38544;&#31169;&#21644;&#23433;&#20840;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as an efficient approach for large-scale distributed machine learning, ensuring data privacy by keeping training data on client devices. However, recent research has highlighted vulnerabilities in FL, including the potential disclosure of sensitive information through individual model updates and even the aggregated global model. While much attention has been given to clients' data privacy, limited research has addressed the issue of global model privacy. Furthermore, local training at the client's side has opened avenues for malicious clients to launch powerful model poisoning attacks. Unfortunately, no existing work has provided a comprehensive solution that tackles all these issues. Therefore, we introduce HyFL, a hybrid framework that enables data and global model privacy while facilitating large-scale deployments. The foundation of HyFL is a unique combination of secure multi-party computation (MPC) techniques with hierarchical federated learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#12289;&#23454;&#26102;&#35299;&#20915;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#22810;&#20010;&#35299;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25910;&#25947;&#21040;&#36817;&#20284;&#31561;&#20215;&#35299;&#30340;&#27491;&#21017;&#21270;&#21382;&#21490;&#22534;&#26632;&#35266;&#23519;&#22120;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#35813;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16299</link><description>&lt;p&gt;
&#22522;&#20110;&#35266;&#27979;&#22120;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#31561;&#20215;&#35299;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning. (arXiv:2210.16299v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#12289;&#23454;&#26102;&#35299;&#20915;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#22810;&#20010;&#35299;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25910;&#25947;&#21040;&#36817;&#20284;&#31561;&#20215;&#35299;&#30340;&#27491;&#21017;&#21270;&#21382;&#21490;&#22534;&#26632;&#35266;&#23519;&#22120;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#35813;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#21644;&#23454;&#26102;&#35299;&#20915;&#30830;&#23450;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#22810;&#20010;&#35299;&#30340;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#38750;&#21807;&#19968;&#24615;&#38656;&#35201;&#30740;&#31350;&#31561;&#20215;&#35299;&#30340;&#27010;&#24565;&#65292;&#21363;&#32467;&#26524;&#22312;&#19981;&#21516;&#30340;&#20195;&#20215;&#20989;&#25968;&#20294;&#30456;&#21516;&#30340;&#21453;&#39304;&#30697;&#38453;&#65292;&#20197;&#21450;&#25910;&#25947;&#21040;&#36825;&#20123;&#35299;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#24320;&#21457;&#20102;&#31163;&#32447;&#31639;&#27861;&#20197;&#25910;&#25947;&#21040;&#31561;&#20215;&#35299;&#65292;&#20294;&#23578;&#26410;&#25552;&#20379;&#35299;&#20915;&#38750;&#21807;&#19968;&#24615;&#30340;&#22312;&#32447;&#12289;&#23454;&#26102;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25910;&#25947;&#21040;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#36817;&#20284;&#31561;&#20215;&#35299;&#30340;&#27491;&#21017;&#21270;&#21382;&#21490;&#22534;&#26632;&#35266;&#23519;&#22120;&#12290;&#21457;&#23637;&#20102;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#26465;&#20214;&#20197;&#20419;&#36827;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#24320;&#21457;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in solving the deterministic inverse reinforcement learning (IRL) problem online and in real-time is the existence of multiple solutions. Nonuniqueness necessitates the study of the notion of equivalent solutions, i.e., solutions that result in a different cost functional but same feedback matrix, and convergence to such solutions. While offline algorithms that result in convergence to equivalent solutions have been developed in the literature, online, real-time techniques that address nonuniqueness are not available. In this paper, a regularized history stack observer that converges to approximately equivalent solutions of the IRL problem is developed. Novel data-richness conditions are developed to facilitate the analysis and simulation results are provided to demonstrate the effectiveness of the developed technique.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#12290;&#30456;&#27604;&#20110;&#24120;&#35265;&#30340;&#27431;&#20960;&#37324;&#24471;&#25104;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#28789;&#27963;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#36741;&#21161;&#20449;&#24687;&#26500;&#24314;&#20256;&#36755;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#31354;&#38388;&#19979;&#22788;&#29702;&#26032;&#25968;&#25454;&#28857;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#35823;&#24046;&#20998;&#26512;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#35813;&#35770;&#25991;&#26500;&#36896;&#20102;&#19968;&#20010;&#33021;&#22815;&#22312;&#20445;&#30041;&#31867;&#21035;&#32467;&#26500;&#30340;&#21516;&#26102;&#26144;&#23556;&#25968;&#25454;&#20998;&#24067;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2205.15403</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#30340;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Neural Optimal Transport with General Cost Functionals. (arXiv:2205.15403v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15403
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#12290;&#30456;&#27604;&#20110;&#24120;&#35265;&#30340;&#27431;&#20960;&#37324;&#24471;&#25104;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#28789;&#27963;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#36741;&#21161;&#20449;&#24687;&#26500;&#24314;&#20256;&#36755;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#31354;&#38388;&#19979;&#22788;&#29702;&#26032;&#25968;&#25454;&#28857;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#35823;&#24046;&#20998;&#26512;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#35813;&#35770;&#25991;&#26500;&#36896;&#20102;&#19968;&#20010;&#33021;&#22815;&#22312;&#20445;&#30041;&#31867;&#21035;&#32467;&#26500;&#30340;&#21516;&#26102;&#26144;&#23556;&#25968;&#25454;&#20998;&#24067;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#12290;&#19982;&#24120;&#35265;&#30340;&#27431;&#20960;&#37324;&#24471;&#25104;&#26412;&#65288;&#22914;$\ell^1$&#25110;$\ell^2$&#65289;&#19981;&#21516;&#65292;&#36825;&#31181;&#20989;&#25968;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#36741;&#21161;&#20449;&#24687;&#65288;&#22914;&#31867;&#21035;&#26631;&#31614;&#65289;&#26469;&#26500;&#24314;&#25152;&#38656;&#30340;&#20256;&#36755;&#26144;&#23556;&#12290;&#29616;&#26377;&#30340;&#19968;&#33324;&#25104;&#26412;&#26041;&#27861;&#26159;&#31163;&#25955;&#30340;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#38480;&#21046;&#65292;&#21363;&#23427;&#20204;&#19981;&#33021;&#25552;&#20379;&#26679;&#26412;&#22806;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#38024;&#23545;&#19968;&#33324;&#25104;&#26412;&#35774;&#35745;&#36830;&#32493;&#30340;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25512;&#24191;&#21040;&#39640;&#32500;&#31354;&#38388;&#65288;&#22914;&#22270;&#20687;&#65289;&#20013;&#30340;&#26032;&#25968;&#25454;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#24674;&#22797;&#30340;&#20256;&#36755;&#26041;&#26696;&#36827;&#34892;&#20102;&#29702;&#35770;&#35823;&#24046;&#20998;&#26512;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;&#65292;&#29992;&#20110;&#22312;&#20445;&#30041;&#31867;&#21035;&#32467;&#26500;&#30340;&#21516;&#26102;&#26144;&#23556;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel neural network-based algorithm to compute optimal transport (OT) plans for general cost functionals. In contrast to common Euclidean costs, i.e., $\ell^1$ or $\ell^2$, such functionals provide more flexibility and allow using auxiliary information, such as class labels, to construct the required transport map. Existing methods for general costs are discrete and have limitations in practice, i.e. they do not provide an out-of-sample estimation. We address the challenge of designing a continuous OT approach for general costs that generalizes to new data points in high-dimensional spaces, such as images. Additionally, we provide the theoretical error analysis for our recovered transport plans. As an application, we construct a cost functional to map data distributions while preserving the class-wise structure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38544;&#31169;&#22312;&#32447;&#38543;&#26426;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#31639;&#27861;&#21644;&#31169;&#26377;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#30340;&#23436;&#20840;&#20449;&#24687;&#29256;&#26412;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#38543;&#26102;&#21487;&#29992;&#30340;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2102.07929</link><description>&lt;p&gt;
&#38544;&#31169;&#22312;&#32447;&#38543;&#26426;&#23398;&#20064;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Algorithms for Private Online Learning in a Stochastic Environment. (arXiv:2102.07929v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38544;&#31169;&#22312;&#32447;&#38543;&#26426;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#31639;&#27861;&#21644;&#31169;&#26377;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#30340;&#23436;&#20840;&#20449;&#24687;&#29256;&#26412;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#38543;&#26102;&#21487;&#29992;&#30340;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#31169;&#26377;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#30340;&#21464;&#20307;&#12290;&#31532;&#19968;&#31181;&#21464;&#20307;&#26159;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26102;&#21487;&#29992;&#30340;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#24615;&#33021;&#12290;&#31532;&#20108;&#31181;&#21464;&#20307;&#26159;&#31169;&#26377;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#30340;&#23436;&#20840;&#20449;&#24687;&#29256;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21487;&#21516;&#26102;&#33719;&#24471;&#38544;&#31169;&#21644;&#24615;&#33021;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider two variants of private stochastic online learning. The first variant is differentially private stochastic bandits. Previously, Sajed and Sheffet (2019) devised the DP Successive Elimination (DP-SE) algorithm that achieves the optimal $ O \biggl(\sum\limits_{1\le j \le K: \Delta_j &gt;0} \frac{ \log T}{ \Delta_j} + \frac{ K\log T}{\epsilon} \biggr)$ problem-dependent regret bound, where $K$ is the number of arms, $\Delta_j$ is the mean reward gap of arm $j$, $T$ is the time horizon, and $\epsilon$ is the required privacy parameter. However, like other elimination style algorithms, it is not an anytime algorithm. Until now, it was not known whether UCB-based algorithms could achieve this optimal regret bound. We present an anytime, UCB-based algorithm that achieves optimality. Our experiments show that the UCB-based algorithm is competitive with DP-SE. The second variant is the full information version of private stochastic online learning. Specifically, for the problem of deci
&lt;/p&gt;</description></item></channel></rss>