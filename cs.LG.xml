<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#37027;&#37324;&#30452;&#25509;&#33719;&#21462;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#33719;&#24471;&#36866;&#24212;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.06333</link><description>&lt;p&gt;
&#35786;&#26029;&#12289;&#21453;&#39304;&#12289;&#36866;&#24212;&#24615;: &#29992;&#20110;&#27979;&#35797;&#26102;&#25919;&#31574;&#35843;&#25972;&#30340;&#20154;-&#26426;&#29615;&#36335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation. (arXiv:2307.06333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#37027;&#37324;&#30452;&#25509;&#33719;&#21462;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#33719;&#24471;&#36866;&#24212;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#24120;&#24120;&#30001;&#20110;&#20998;&#24067;&#20559;&#31227;&#32780;&#22833;&#25928;&#8212;&#8212;&#21363;&#24403;&#25919;&#31574;&#22312;&#26032;&#29615;&#22659;&#20013;&#37096;&#32626;&#26102;&#65292;&#29366;&#24577;&#21644;&#22870;&#21169;&#21457;&#29983;&#21464;&#21270;&#12290;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#36890;&#36807;&#20351;&#27169;&#22411;&#23545;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#26469;&#22686;&#21152;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#32773;&#22312;&#20107;&#20808;&#24448;&#24448;&#19981;&#30693;&#36947;&#21738;&#20123;&#27010;&#24565;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#19981;&#21516;&#30340;&#26368;&#32456;&#29992;&#25143;&#23545;&#20219;&#21153;&#25191;&#34892;&#26041;&#24335;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#29983;&#25104;&#21453;&#20107;&#23454;&#28436;&#31034;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#30830;&#23450;&#21487;&#33021;&#19982;&#20219;&#21153;&#30456;&#20851;&#21644;&#26080;&#20851;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#21033;&#29992;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#30340;&#30693;&#35782;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#32780;&#33719;&#24471;&#36866;&#24212;&#20110;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;&#25105;&#20204;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;(1)&#20351;&#29992;&#25143;&#33021;&#22815;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Policies often fail due to distribution shift -- changes in the state and reward that occur when a policy is deployed in new environments. Data augmentation can increase robustness by making the model invariant to task-irrelevant changes in the agent's observation. However, designers don't know which concepts are irrelevant a priori, especially when different end users have different preferences about how the task is performed. We propose an interactive framework to leverage feedback directly from the user to identify personalized task-irrelevant concepts. Our key idea is to generate counterfactual demonstrations that allow users to quickly identify possible task-relevant and irrelevant concepts. The knowledge of task-irrelevant concepts is then used to perform data augmentation and thus obtain a policy adapted to personalized user objectives. We present experiments validating our framework on discrete and continuous control tasks with real human users. Our method (1) enables users to 
&lt;/p&gt;</description></item><item><title>&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#38480;&#21046;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.06328</link><description>&lt;p&gt;
Offline RL&#30340;&#39044;&#31639;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Budgeting Counterfactual for Offline RL. (arXiv:2307.06328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06328
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#38480;&#21046;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#28508;&#22312;&#21160;&#20316;&#39046;&#22495;&#20869;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#22256;&#22659;&#25152;&#24341;&#36215;&#65306;&#22914;&#26524;&#25105;&#20204;&#36873;&#25321;&#20102;&#19981;&#21516;&#30340;&#34892;&#21160;&#20250;&#24590;&#20040;&#26679;&#65311;&#36825;&#20123;&#24773;&#20917;&#36890;&#24120;&#20250;&#23548;&#33268;&#25351;&#25968;&#32423;&#32047;&#31215;&#30340;&#22806;&#25512;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#35748;&#35782;&#21040;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#20915;&#31574;&#27493;&#39588;&#23545;&#26368;&#32456;&#32467;&#26524;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#24182;&#22312;&#25919;&#31574;&#21046;&#23450;&#20013;&#39044;&#31639;&#21453;&#20107;&#23454;&#20915;&#31574;&#30340;&#25968;&#37327;&#20197;&#25511;&#21046;&#22806;&#25512;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#22312;&#25919;&#31574;&#25110;&#20540;&#20989;&#25968;&#19978;&#20351;&#29992;&#35268;&#21017;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#26126;&#30830;&#38480;&#21046;&#35757;&#32451;&#26399;&#38388;&#30340;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#26469;&#20915;&#23450;&#22312;&#21738;&#37324;&#36827;&#34892;&#22806;&#25512;&#21644;&#22312;&#21738;&#37324;&#19981;&#36827;&#34892;&#22806;&#25512;&#65292;&#24182;&#19988;&#23545;&#20915;&#31574;&#30340;&#19978;&#38480;&#19981;&#21516;&#20110;&#34892;&#20026;&#31574;&#30053;&#12290;&#23427;&#22312;&#28508;&#22312;&#25913;&#36827;&#30340;&#28508;&#21147;&#21644;&#22806;&#25512;&#25511;&#21046;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main challenge of offline reinforcement learning, where data is limited, arises from a sequence of counterfactual reasoning dilemmas within the realm of potential actions: What if we were to choose a different course of action? These circumstances frequently give rise to extrapolation errors, which tend to accumulate exponentially with the problem horizon. Hence, it becomes crucial to acknowledge that not all decision steps are equally important to the final outcome, and to budget the number of counterfactual decisions a policy make in order to control the extrapolation. Contrary to existing approaches that use regularization on either the policy or value function, we propose an approach to explicitly bound the amount of out-of-distribution actions during training. Specifically, our method utilizes dynamic programming to decide where to extrapolate and where not to, with an upper bound on the decisions different from behavior policy. It balances between the potential for improvemen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.06324</link><description>&lt;p&gt;
&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20801;&#35768;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#27425;&#36845;&#20195;&#30340;&#25972;&#20307;&#25928;&#26524;&#32780;&#19981;&#26159;&#20856;&#22411;&#30340;&#19968;&#27425;&#36845;&#20195;&#24402;&#32435;&#20351;&#29992;&#30340;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#30772;&#22351;&#19979;&#38477;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#38271;&#36317;&#31163;&#27493;&#39588;&#65292;&#21487;&#33021;&#22312;&#30701;&#26399;&#20869;&#22686;&#21152;&#30446;&#26631;&#20540;&#65292;&#20294;&#22312;&#38271;&#26399;&#20869;&#24102;&#26469;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26799;&#24230;&#19979;&#38477;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#30340;&#29468;&#24819;&#65292;&#24182;&#36827;&#34892;&#20102;&#31616;&#21333;&#30340;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work establishes provably faster convergence rates for gradient descent via a computer-assisted analysis technique. Our theory allows nonconstant stepsize policies with frequent long steps potentially violating descent by analyzing the overall effect of many iterations at once rather than the typical one-iteration inductions used in most first-order method analyses. We show that long steps, which may increase the objective value in the short term, lead to provably faster convergence in the long term. A conjecture towards proving a faster $O(1/T\log T)$ rate for gradient descent is also motivated along with simple numerical validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20010;&#24615;&#21270;&#29983;&#25104;&#22120;&#36827;&#34892;&#38754;&#37096;&#22797;&#21407;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#22120;&#24182;&#26681;&#25454;&#20010;&#20307;&#30340;&#30524;&#30555;&#12289;&#40763;&#23376;&#21644;&#22068;&#24052;&#20301;&#32622;&#26469;&#29983;&#25104;&#20445;&#30041;&#36523;&#20221;&#20449;&#24687;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2307.06307</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#29983;&#25104;&#22120;&#22312;&#38754;&#37096;&#22797;&#21407;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Facial Reenactment Through a Personalized Generator. (arXiv:2307.06307v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20010;&#24615;&#21270;&#29983;&#25104;&#22120;&#36827;&#34892;&#38754;&#37096;&#22797;&#21407;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#22120;&#24182;&#26681;&#25454;&#20010;&#20307;&#30340;&#30524;&#30555;&#12289;&#40763;&#23376;&#21644;&#22068;&#24052;&#20301;&#32622;&#26469;&#29983;&#25104;&#20445;&#30041;&#36523;&#20221;&#20449;&#24687;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#38754;&#37096;&#22797;&#21407;&#20013;&#30340;&#20316;&#29992;&#36880;&#28176;&#22686;&#21152;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#20010;&#20307;&#65292;&#24182;&#19988;&#26159;&#22312;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#20010;&#20307;&#25972;&#20307;&#22806;&#35980;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#22270;&#20687;&#19981;&#22815;&#20934;&#30830;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#35757;&#32451;&#20010;&#24615;&#21270;&#29983;&#25104;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20010;&#24615;&#21270;&#29983;&#25104;&#22120;&#36827;&#34892;&#38754;&#37096;&#22797;&#21407;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#21488;&#26222;&#36890;&#30456;&#26426;&#25293;&#25668;&#30340;&#19968;&#27573;&#30701;&#32780;&#22810;&#26679;&#21270;&#30340;&#33258;&#25293;&#35270;&#39057;&#20013;&#30340;&#24103;&#26469;&#35757;&#32451;&#29983;&#25104;&#22120;&#12290;&#30001;&#20010;&#24615;&#21270;&#29983;&#25104;&#22120;&#21512;&#25104;&#30340;&#22270;&#20687;&#23558;&#20445;&#30041;&#20010;&#20307;&#30340;&#36523;&#20221;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#21069;&#25552;&#65292;&#21363;&#22797;&#21407;&#20219;&#21153;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#20223;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20301;&#21040;&#30340;&#26159;&#20010;&#20307;&#30340;&#30524;&#30555;&#12289;&#40763;&#23376;&#21644;&#22068;&#24052;&#30340;&#20301;&#32622;&#26469;&#36827;&#34892;&#38754;&#37096;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the role of image generative models in facial reenactment has been steadily increasing. Such models are usually subject-agnostic and trained on domain-wide datasets. The appearance of the reenacted individual is learned from a single image, and hence, the entire breadth of the individual's appearance is not entirely captured, leading these methods to resort to unfaithful hallucination. Thanks to recent advancements, it is now possible to train a personalized generative model tailored specifically to a given individual. In this paper, we propose a novel method for facial reenactment using a personalized generator. We train the generator using frames from a short, yet varied, self-scan video captured using a simple commodity camera. Images synthesized by the personalized generator are guaranteed to preserve identity. The premise of our work is that the task of reenactment is thus reduced to accurately mimicking head poses and expressions. To this end, we locate the desir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#38543;&#26426;Polyak&#27493;&#38271;&#26041;&#27861;&#25193;&#23637;&#21040;&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#21644;&#20960;&#20046;&#26080;&#38656;&#35843;&#21442;&#30340;FedSPS&#21644;FedDecSPS&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#25554;&#20540;&#26465;&#20214;&#28385;&#36275;&#26102;&#65292;FedSPS&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#35299;&#30340;&#37051;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.06306</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;Polyak&#27493;&#38271;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes. (arXiv:2307.06306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#38543;&#26426;Polyak&#27493;&#38271;&#26041;&#27861;&#25193;&#23637;&#21040;&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#21644;&#20960;&#20046;&#26080;&#38656;&#35843;&#21442;&#30340;FedSPS&#21644;FedDecSPS&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#25554;&#20540;&#26465;&#20214;&#28385;&#36275;&#26102;&#65292;FedSPS&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#35299;&#30340;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;FedAvg&#65292;&#38656;&#35201;&#31934;&#24515;&#35843;&#25972;&#30340;&#27493;&#38271;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#29616;&#26377;&#33258;&#36866;&#24212;&#32852;&#37030;&#26041;&#27861;&#25552;&#20986;&#30340;&#25913;&#36827;&#20165;&#28041;&#21450;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22914;&#21160;&#37327;&#21442;&#25968;&#65292;&#24182;&#19988;&#20165;&#32771;&#34385;&#22312;&#26381;&#21153;&#22120;&#32858;&#21512;&#36718;&#27425;&#20013;&#30340;&#36866;&#24212;&#24615;&#65292;&#32780;&#19981;&#26159;&#23616;&#37096;&#30340;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#19979;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#19981;&#33021;&#25429;&#25417;&#23616;&#37096;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340;&#38543;&#26426;Polyak&#27493;&#38271;&#26041;&#27861;&#25193;&#23637;&#21040;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#21644;&#20960;&#20046;&#26080;&#38656;&#35843;&#21442;&#30340;&#20998;&#24067;&#24335;SPS&#21464;&#20307;&#65288;FedSPS&#21644;FedDecSPS&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#24403;&#25554;&#20540;&#26465;&#20214;&#65288;&#36807;&#21442;&#25968;&#21270;&#65289;&#28385;&#36275;&#26102;&#65292;FedSPS&#22312;&#24378;&#20984;&#21644;&#20984;&#35774;&#32622;&#20013;&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#35299;&#30340;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art federated learning algorithms such as FedAvg require carefully tuned stepsizes to achieve their best performance. The improvements proposed by existing adaptive federated methods involve tuning of additional hyperparameters such as momentum parameters, and consider adaptivity only in the server aggregation round, but not locally. These methods can be inefficient in many practical scenarios because they require excessive tuning of hyperparameters and do not capture local geometric information. In this work, we extend the recently proposed stochastic Polyak stepsize (SPS) to the federated learning setting, and propose new locally adaptive and nearly parameter-free distributed SPS variants (FedSPS and FedDecSPS). We prove that FedSPS converges linearly in strongly convex and sublinearly in convex settings when the interpolation condition (overparametrization) is satisfied, and converges to a neighborhood of the solution in the general case. We extend our proposed method t
&lt;/p&gt;</description></item><item><title>NaViT&#26159;&#19968;&#20010;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#21015;&#25171;&#21253;&#30340;&#26041;&#24335;&#22788;&#29702;&#20219;&#24847;&#20998;&#36776;&#29575;&#21644;&#32437;&#27178;&#27604;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#22312;&#26631;&#20934;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.06304</link><description>&lt;p&gt;
Patch n' Pack: NaViT,&#19968;&#20010;&#36866;&#29992;&#20110;&#20219;&#24847;&#32437;&#27178;&#27604;&#21644;&#20998;&#36776;&#29575;&#30340;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution. (arXiv:2307.06304v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06304
&lt;/p&gt;
&lt;p&gt;
NaViT&#26159;&#19968;&#20010;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#21015;&#25171;&#21253;&#30340;&#26041;&#24335;&#22788;&#29702;&#20219;&#24847;&#20998;&#36776;&#29575;&#21644;&#32437;&#27178;&#27604;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#22312;&#26631;&#20934;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20013;&#65292;&#23558;&#22270;&#20687;&#35843;&#25972;&#20026;&#22266;&#23450;&#20998;&#36776;&#29575;&#21518;&#36827;&#34892;&#22788;&#29702;&#26159;&#26222;&#36941;&#19988;&#26126;&#26174;&#27425;&#20248;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20687;Vision Transformer&#65288;ViT&#65289;&#36825;&#26679;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#22522;&#20110;&#24207;&#21015;&#30340;&#24314;&#27169;&#65292;&#22240;&#27492;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#24207;&#21015;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#20351;&#29992;&#31216;&#20026;NaViT&#65288;Native Resolution ViT&#65289;&#30340;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#24207;&#21015;&#25171;&#21253;&#65292;&#20197;&#22788;&#29702;&#20219;&#24847;&#20998;&#36776;&#29575;&#21644;&#32437;&#27178;&#27604;&#30340;&#36755;&#20837;&#12290;&#38500;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20351;&#29992;&#26041;&#24335;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#30417;&#30563;&#21644;&#23545;&#27604;&#24230;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#30340;&#35757;&#32451;&#25928;&#29575;&#25552;&#21319;&#12290;NaViT&#21487;&#20197;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#31561;&#26631;&#20934;&#20219;&#21153;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#36755;&#20837;&#20998;&#36776;&#29575;&#30340;&#28789;&#27963;&#24615;&#21487;&#20197;&#29992;&#20110;&#24179;&#31283;&#22320;&#22312;&#27979;&#35797;&#26102;&#38388;&#30340;&#25104;&#26412;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#30456;&#20449;NaViT&#26631;&#24535;&#30528;&#19968;&#20010;&#31163;&#24320;&#20102;&#20197;&#24448;&#24605;&#32500;&#30340;&#26032;&#31687;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a depart
&lt;/p&gt;</description></item><item><title>&#38754;&#21521;DNN&#39564;&#35777;&#30340;&#19968;&#31181;&#26032;&#22411;&#35777;&#26126;&#26816;&#26597;&#22120;&#23454;&#29616;&#65292;&#36890;&#36807;&#25552;&#20379;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#26356;&#22823;&#30340;&#21487;&#39564;&#35777;&#24615;&#25913;&#36827;&#29616;&#26377;&#25628;&#32034;&#24037;&#20855;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#23454;&#29616;&#21033;&#29992;&#20102;Imandra&#30340;&#20004;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#26080;&#38480;&#31934;&#24230;&#23454;&#25968;&#31639;&#26415;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#22522;&#30784;&#35774;&#26045;&#12290;</title><link>http://arxiv.org/abs/2307.06299</link><description>&lt;p&gt;
&#38754;&#21521;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#21487;&#35777;&#26126;&#23457;&#26597;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards a Certified Proof Checker for Deep Neural Network Verification. (arXiv:2307.06299v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06299
&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;DNN&#39564;&#35777;&#30340;&#19968;&#31181;&#26032;&#22411;&#35777;&#26126;&#26816;&#26597;&#22120;&#23454;&#29616;&#65292;&#36890;&#36807;&#25552;&#20379;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#26356;&#22823;&#30340;&#21487;&#39564;&#35777;&#24615;&#25913;&#36827;&#29616;&#26377;&#25628;&#32034;&#24037;&#20855;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#23454;&#29616;&#21033;&#29992;&#20102;Imandra&#30340;&#20004;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#26080;&#38480;&#31934;&#24230;&#23454;&#25968;&#31639;&#26415;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#20445;&#35777;&#30340;&#38656;&#27714;&#12290;&#21487;&#20197;&#20351;&#29992;&#30001;&#39564;&#35777;&#31038;&#21306;&#24320;&#21457;&#30340;&#24037;&#20855;&#26469;&#35777;&#26126;DNN&#30340;&#36825;&#20123;&#23433;&#20840;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#26412;&#36523;&#23481;&#26131;&#20986;&#29616;&#23454;&#29616;&#38169;&#35823;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#20540;&#24471;&#24576;&#30097;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#39564;&#35777;&#22120;&#20250;&#20135;&#29983;&#21487;&#20197;&#30001;&#21487;&#20449;&#26816;&#26597;&#22120;&#26816;&#26597;&#30340;&#32467;&#26524;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;DNN&#39564;&#35777;&#30340;&#26032;&#22411;&#35777;&#26126;&#26816;&#26597;&#22120;&#30340;&#23454;&#29616;&#12290;&#23427;&#36890;&#36807;&#25552;&#20379;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#26356;&#22823;&#30340;&#21487;&#39564;&#35777;&#24615;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;&#23454;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Imandra&#65288;&#19968;&#31181;&#24037;&#19994;&#32423;&#30340;&#23450;&#29702;&#35777;&#26126;&#22120;&#65289;&#30340;&#20004;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#23545;&#26080;&#38480;&#31934;&#24230;&#23454;&#25968;&#31639;&#26415;&#30340;&#25903;&#25345;&#21644;&#20854;&#24418;&#24335;&#21270;&#39564;&#35777;&#22522;&#30784;&#35774;&#26045;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#22312;Imandra&#20013;&#23454;&#29616;&#20102;&#19968;&#20010;&#35777;&#26126;&#26816;&#26597;&#22120;&#65292;&#35268;&#23450;&#20102;&#20854;&#27491;&#30830;&#24615;&#23646;&#24615;&#21644;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
Recent developments in deep neural networks (DNNs) have led to their adoption in safety-critical systems, which in turn has heightened the need for guaranteeing their safety. These safety properties of DNNs can be proven using tools developed by the verification community. However, these tools are themselves prone to implementation bugs and numerical stability problems, which make their reliability questionable. To overcome this, some verifiers produce proofs of their results which can be checked by a trusted checker. In this work, we present a novel implementation of a proof checker for DNN verification. It improves on existing implementations by offering numerical stability and greater verifiability. To achieve this, we leverage two key capabilities of Imandra, an industrial theorem prover: its support of infinite precision real arithmetic and its formal verification infrastructure. So far, we have implemented a proof checker in Imandra, specified its correctness properties and start
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2307.06290</link><description>&lt;p&gt;
&#25351;&#20196;&#25366;&#25496;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#32463;&#21382;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#36171;&#20104;&#27169;&#22411;&#24378;&#22823;&#30340;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#22238;&#24212;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#26102;&#20173;&#28982;&#26080;&#27861;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#12290;&#20026;&#20102;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#36136;&#37327;&#30340;&#32447;&#24615;&#35268;&#21017;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#26469;&#36827;&#34892;InstructMining&#30340;&#24314;&#27169;&#12290;&#20026;&#20102;&#30740;&#31350;&#25968;&#25454;&#36136;&#37327;&#19982;&#36825;&#20123;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32454;&#33268;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19981;&#36866;&#21512;&#20316;&#20026;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#30528;&#37325;&#20998;&#26512;&#21644;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06287</link><description>&lt;p&gt;
&#29702;&#24615;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Rational Neural Network Controllers. (arXiv:2307.06287v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06287
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19981;&#36866;&#21512;&#20316;&#20026;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#30528;&#37325;&#20998;&#26512;&#21644;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#33021;&#21147;&#65292;&#24050;&#22312;&#35768;&#22810;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#65288;&#31216;&#20026;&#31070;&#32463;&#21453;&#39304;&#29615;&#65289;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#31070;&#32463;&#32593;&#32476;&#23545;&#25932;&#23545;&#25915;&#20987;&#38750;&#24120;&#25935;&#24863;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#38500;&#38750;&#35774;&#35745;&#24471;&#24403;&#65292;&#21542;&#21017;&#23427;&#20204;&#19981;&#26159;&#25511;&#21046;&#22120;&#30340;&#29702;&#24819;&#36873;&#25321;&#65292;&#22240;&#20026;&#25511;&#21046;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#12290;&#24050;&#32463;&#21021;&#27493;&#30740;&#31350;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#36827;&#34892;&#40065;&#26834;&#24615;&#20998;&#26512;&#21644;&#35774;&#35745;&#30340;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#31361;&#20986;&#38382;&#39064;&#26159;&#23427;&#20204;&#20351;&#29992;&#29616;&#26377;&#38024;&#23545;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#36825;&#20123;&#32467;&#26500;&#21487;&#33021;&#19981;&#36866;&#21512;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properly, they are not an ideal candidate for controllers due to issues with robustness and uncertainty, which are pivotal aspects of control systems. There has been initial work on robustness to both analyse and design dynamical systems with neural network controllers. However, one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning tasks. These structures may not be appropriate for neural network controllers and it is imp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#24322;&#36136;&#24615;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#21040;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#21644;&#26412;&#22320;&#26356;&#26032;&#65292;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;&#36827;&#34892;&#20102;&#24191;&#27867;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.06283</link><description>&lt;p&gt;
&#22788;&#29702;FL&#20013;&#30340;&#35745;&#31639;&#24322;&#36136;&#24615;&#65306;&#19968;&#20123;&#29702;&#35770;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Tackling Computational Heterogeneity in FL: A Few Theoretical Insights. (arXiv:2307.06283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#24322;&#36136;&#24615;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#21040;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#21644;&#26412;&#22320;&#26356;&#26032;&#65292;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;&#36827;&#34892;&#20102;&#24191;&#27867;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26410;&#26469;&#22312;&#20110;&#23558;&#25968;&#25454;&#25910;&#38598;&#19982;&#35757;&#32451;&#31227;&#33267;&#36793;&#32536;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#30340;&#21407;&#21017;&#26159;&#32858;&#21512;&#22312;&#22823;&#37327;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#19978;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21363;&#20174;&#20854;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#25454;&#30340;&#36164;&#28304;&#21463;&#38480;&#31227;&#21160;&#35774;&#22791;&#65292;&#20197;&#33719;&#24471;&#26032;&#30340;&#26356;&#36890;&#29992;&#30340;&#27169;&#22411;&#12290;&#21518;&#32773;&#38543;&#21518;&#37325;&#26032;&#20998;&#21457;&#32473;&#23458;&#25143;&#31471;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#12290;&#32852;&#37030;&#23398;&#20064;&#19982;&#25968;&#25454;&#20013;&#24515;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#20043;&#38388;&#30340;&#20851;&#38190;&#29305;&#24449;&#26159;&#20869;&#22312;&#30340;&#24322;&#36136;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#26694;&#26550;&#65292;&#20197;&#20415;&#24418;&#24335;&#21270;&#21644;&#35299;&#20915;&#32852;&#37030;&#20248;&#21270;&#20013;&#30340;&#35745;&#31639;&#24322;&#36136;&#24615;&#65292;&#21253;&#25324;&#24322;&#26500;&#25968;&#25454;&#21644;&#26412;&#22320;&#26356;&#26032;&#12290;&#25152;&#25552;&#20986;&#30340;&#32858;&#21512;&#31639;&#27861;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#35282;&#24230;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The future of machine learning lies in moving data collection along with training to the edge. Federated Learning, for short FL, has been recently proposed to achieve this goal. The principle of this approach is to aggregate models learned over a large number of distributed clients, i.e., resource-constrained mobile devices that collect data from their environment, to obtain a new more general model. The latter is subsequently redistributed to clients for further training. A key feature that distinguishes federated learning from data-center-based distributed training is the inherent heterogeneity. In this work, we introduce and analyse a novel aggregation framework that allows for formalizing and tackling computational heterogeneity in federated optimization, in terms of both heterogeneous data and local updates. Proposed aggregation algorithms are extensively analyzed from a theoretical, and an experimental prospective.
&lt;/p&gt;</description></item><item><title>SpreadNUTS&#26159;&#19968;&#20010;&#36870;&#36716;&#37319;&#26679;&#21644;&#21010;&#20998;&#35775;&#38382;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;MCMC&#31639;&#27861;&#30340;&#25928;&#29575;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.06279</link><description>&lt;p&gt;
SpreadNUTS -- &#36866;&#24230;&#21160;&#24577;&#36335;&#24452;&#25193;&#23637;&#30340;&#36870;&#36716;&#37319;&#26679;&#21644;&#21010;&#20998;&#35775;&#38382;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
SpreadNUTS -- Moderate Dynamic Extension of Paths for No-U-Turn Sampling &amp; Partitioning Visited Regions. (arXiv:2307.06279v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06279
&lt;/p&gt;
&lt;p&gt;
SpreadNUTS&#26159;&#19968;&#20010;&#36870;&#36716;&#37319;&#26679;&#21644;&#21010;&#20998;&#35775;&#38382;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;MCMC&#31639;&#27861;&#30340;&#25928;&#29575;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#26041;&#27861;&#23384;&#22312;&#24050;&#20037;&#65292;&#35813;&#39046;&#22495;&#24050;&#32463;&#28145;&#20837;&#30740;&#31350;&#12290;MCMC&#26041;&#27861;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#37325;&#22797;&#37319;&#26679;&#26469;&#36817;&#20284;&#19968;&#20010;&#20998;&#24067;&#65307;&#22823;&#22810;&#25968;MCMC&#31639;&#27861;&#22312;&#28176;&#36827;&#26102;&#37117;&#34920;&#29616;&#20986;&#26368;&#20248;&#34892;&#20026;&#65292;&#21363;&#22312;&#26497;&#38480;&#19979;&#25910;&#25947;&#20110;&#30495;&#23454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#21306;&#21035;&#22312;&#20110;&#23427;&#20204;&#30340;&#23454;&#38469;&#25910;&#25947;&#20445;&#35777;&#21644;&#25928;&#29575;&#12290;&#34429;&#28982;&#37319;&#26679;&#22120;&#21487;&#33021;&#26368;&#32456;&#24456;&#22909;&#22320;&#36817;&#20284;&#20102;&#19968;&#20010;&#20998;&#24067;&#65292;&#20294;&#22240;&#20026;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#65292;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#36798;&#21040;&#37319;&#26679;&#22120;&#20135;&#29983;&#33391;&#22909;&#20998;&#24067;&#20272;&#35745;&#30340;&#28857;&#26159;&#24517;&#35201;&#30340;&#12290;&#21516;&#26679;&#65292;&#22914;&#26524;&#20174;&#19968;&#20010;&#20998;&#24067;&#20013;&#20135;&#29983;&#33391;&#22909;&#30340;&#26679;&#26412;&#29992;&#20110;&#20272;&#35745;&#22312;&#35745;&#31639;&#19978;&#24456;&#22256;&#38590;&#25110;&#26080;&#27861;&#25805;&#20316;&#65292;&#21017;&#37319;&#26679;&#22120;&#27809;&#26377;&#25552;&#20379;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#29616;&#22312;&#30340;&#22823;&#22810;&#25968;MCMC&#26041;&#27861;&#37117;&#38598;&#20013;&#20110;&#25552;&#39640;&#25928;&#29575;&#21644;&#21152;&#36895;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;MCMC&#31639;&#27861;&#37117;&#21463;&#21040;&#38543;&#26426;&#28216;&#36208;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov chain Monte Carlo (MCMC) methods have existed for a long time and the field is well-explored. The purpose of MCMC methods is to approximate a distribution through repeated sampling; most MCMC algorithms exhibit asymptotically optimal behavior in that they converge to the true distribution at the limit. However, what differentiates these algorithms are their practical convergence guarantees and efficiency. While a sampler may eventually approximate a distribution well, because it is used in the real world it is necessary that the point at which the sampler yields a good estimate of the distribution is reachable in a reasonable amount of time. Similarly, if it is computationally difficult or intractable to produce good samples from a distribution for use in estimation, then there is no real-world utility afforded by the sampler. Thus, most MCMC methods these days focus on improving efficiency and speeding up convergence. However, many MCMC algorithms suffer from random walk behavi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeDID&#30340;&#26032;&#30340;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#32479;&#35745;&#21644;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#24182;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#29420;&#29305;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#30340;&#26377;&#25928;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SeDID&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#21306;&#20998;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2307.06272</link><description>&lt;p&gt;
&#25581;&#38706;&#20551;&#20687;&#65306;&#26377;&#25928;&#30340;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exposing the Fake: Effective Diffusion-Generated Images Detection. (arXiv:2307.06272v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeDID&#30340;&#26032;&#30340;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#32479;&#35745;&#21644;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#24182;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#29420;&#29305;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#30340;&#26377;&#25928;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SeDID&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#21306;&#20998;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25193;&#25955;&#24335;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65289;&#30340;&#20986;&#29616;&#65292;&#22270;&#20687;&#21512;&#25104;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#38750;&#24120;&#39640;&#25928;&#65292;&#20294;&#22312;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#65292;&#32780;&#36825;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Stepwise Error for Diffusion-generated Image Detection&#65288;SeDID&#65289;&#30340;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;SeDID&#30001;&#22522;&#20110;&#32479;&#35745;&#30340;$\text{SeDID}_{\text{Stat}}$&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;$\text{SeDID}_{\text{NNs}}$&#32452;&#25104;&#65292;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#29420;&#29305;&#23646;&#24615;&#65292;&#21363;&#30830;&#23450;&#24615;&#21453;&#36716;&#21644;&#30830;&#23450;&#24615;&#21435;&#22122;&#35745;&#31639;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24212;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#26102;&#65292;SeDID&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#21306;&#20998;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image synthesis has seen significant advancements with the advent of diffusion-based generative models like Denoising Diffusion Probabilistic Models (DDPM) and text-to-image diffusion models. Despite their efficacy, there is a dearth of research dedicated to detecting diffusion-generated images, which could pose potential security and privacy risks. This paper addresses this gap by proposing a novel detection method called Stepwise Error for Diffusion-generated Image Detection (SeDID). Comprising statistical-based $\text{SeDID}_{\text{Stat}}$ and neural network-based $\text{SeDID}_{\text{NNs}}$, SeDID exploits the unique attributes of diffusion models, namely deterministic reverse and deterministic denoising computation errors. Our evaluations demonstrate SeDID's superior performance over existing methods when applied to diffusion models. Thus, our work makes a pivotal contribution to distinguishing diffusion model-generated images, marking a significant step in the domain of artificia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#26657;&#20934;&#23439;&#35266;&#20132;&#36890;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#19982;&#20132;&#36890;&#27969;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#26657;&#20934;&#25928;&#26524;&#12290;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06267</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#26657;&#20934;&#23439;&#35266;&#20132;&#36890;&#27969;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Machine Learning for Calibrating Macroscopic Traffic Flow Models. (arXiv:2307.06267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#26657;&#20934;&#23439;&#35266;&#20132;&#36890;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#19982;&#20132;&#36890;&#27969;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#26657;&#20934;&#25928;&#26524;&#12290;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#22909;&#26657;&#20934;&#30340;&#20132;&#36890;&#27969;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#20132;&#36890;&#29616;&#35937;&#21644;&#35774;&#35745;&#25511;&#21046;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#26657;&#20934;&#26041;&#27861;&#26159;&#22522;&#20110;&#20248;&#21270;&#31639;&#27861;&#24320;&#21457;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#20248;&#21270;&#31639;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#32463;&#20856;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#19982;&#20132;&#36890;&#27969;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29289;&#29702;&#20132;&#36890;&#27969;&#27169;&#22411;&#20449;&#24687;&#25552;&#20379;&#32473;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#20351;&#24471;&#32534;&#30721;&#22120;&#33021;&#22815;&#26681;&#25454;&#27969;&#37327;&#21644;&#36895;&#24230;&#27979;&#37327;&#32467;&#26524;&#24471;&#20986;&#21512;&#29702;&#30340;&#20132;&#36890;&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;&#20854;&#19981;&#20165;&#33021;&#22788;&#29702;&#27491;&#24120;&#25968;&#25454;&#65292;&#36824;&#33021;&#22788;&#29702;&#26377;&#32570;&#22833;&#20540;&#30340;&#38169;&#35823;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;I-210 E&#30340;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well-calibrated traffic flow models are fundamental to understanding traffic phenomena and designing control strategies. Traditional calibration has been developed base on optimization methods. In this paper, we propose a novel physics-informed, learning-based calibration approach that achieves performances comparable to and even better than those of optimization-based methods. To this end, we combine the classical deep autoencoder, an unsupervised machine learning model consisting of one encoder and one decoder, with traffic flow models. Our approach informs the decoder of the physical traffic flow models and thus induces the encoder to yield reasonable traffic parameters given flow and speed measurements. We also introduce the denoising autoencoder into our method so that it can handles not only with normal data but also with corrupted data with missing values. We verified our approach with a case study of I-210 E in California.
&lt;/p&gt;</description></item><item><title>&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;(PBSHM)&#26088;&#22312;&#22312;&#20154;&#32676;&#25104;&#21592;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#20197;&#25913;&#21892;&#20581;&#24247;&#29366;&#24577;&#30340;&#25512;&#26029;&#12290;&#30001;&#20110;&#24046;&#24322;&#21644;&#25968;&#25454;&#20002;&#22833;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2307.06263</link><description>&lt;p&gt;
&#20851;&#20110;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
On the hierarchical Bayesian modelling of frequency response functions. (arXiv:2307.06263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06263
&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;(PBSHM)&#26088;&#22312;&#22312;&#20154;&#32676;&#25104;&#21592;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#20197;&#25913;&#21892;&#20581;&#24247;&#29366;&#24577;&#30340;&#25512;&#26029;&#12290;&#30001;&#20110;&#24046;&#24322;&#21644;&#25968;&#25454;&#20002;&#22833;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#32676;&#30340;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;(PBSHM)&#26088;&#22312;&#22312;&#20154;&#32676;&#25104;&#21592;&#20043;&#38388;&#20849;&#20139;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#27491;&#24120;&#21644;&#25439;&#20260;&#29366;&#24577;&#30340;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#23545;&#25104;&#21592;&#20581;&#24247;&#29366;&#24577;&#30340;&#25512;&#26029;&#12290;&#21363;&#20351;&#20154;&#32676;&#30001;&#21517;&#20041;&#19978;&#30456;&#21516;&#30340;&#32467;&#26500;&#32452;&#25104;&#65292;&#30001;&#20110;&#26448;&#26009;&#24615;&#36136;&#12289;&#20960;&#20309;&#24418;&#29366;&#12289;&#36793;&#30028;&#26465;&#20214;&#25110;&#29615;&#22659;&#24433;&#21709;(&#20363;&#22914;&#28201;&#24230;&#21464;&#21270;)&#30340;&#32454;&#24494;&#24046;&#24322;&#20135;&#29983;&#20102; benign variations&#12290;&#36825;&#20123;&#24046;&#24322;&#21487;&#20197;&#24433;&#21709;&#27169;&#24577;&#24615;&#36136;&#65292;&#24182;&#34920;&#29616;&#20026;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;(FRF)&#30340;&#20849;&#25391;&#23792;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#35768;&#22810;SHM&#31574;&#30053;&#20381;&#36182;&#20110;&#23545;&#32467;&#26500;&#30340;&#21160;&#24577;&#29305;&#24615;&#36827;&#34892;&#30417;&#27979;&#65292;&#22240;&#27492; benign variations &#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#23454;&#26045;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25391;&#21160;&#24335;SHM&#30340;&#21478;&#19968;&#20010;&#24120;&#35265;&#25361;&#25112;&#26159;&#25968;&#25454;&#20002;&#22833;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20256;&#36755;&#38382;&#39064;&#12289;&#20256;&#24863;&#22120;&#25925;&#38556;&#12289;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#37319;&#26679;&#29575;&#19981;&#21305;&#37197;&#31561;&#21407;&#22240;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Population-based structural health monitoring (PBSHM) aims to share valuable information among members of a population, such as normal- and damage-condition data, to improve inferences regarding the health states of the members. Even when the population is comprised of nominally-identical structures, benign variations among the members will exist as a result of slight differences in material properties, geometry, boundary conditions, or environmental effects (e.g., temperature changes). These discrepancies can affect modal properties and present as changes in the characteristics of the resonance peaks of the frequency response function (FRF). Many SHM strategies depend on monitoring the dynamic properties of structures, so benign variations can be challenging for the practical implementation of these systems. Another common challenge with vibration-based SHM is data loss, which may result from transmission issues, sensor failure, a sample-rate mismatch between sensors, and other causes
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25581;&#31034;&#20102;&#20154;&#31867;&#20083;&#31361;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#29305;&#24449;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20083;&#31361;&#24418;&#29366;&#30340;&#25345;&#32493;&#21516;&#35843;&#29305;&#24449;&#22312;&#39044;&#27979;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#26368;&#39640;&#25928;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.06255</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#22312;3D&#25195;&#25551;&#20013;&#35782;&#21035;&#20986;&#20154;&#31867;&#20083;&#31361;&#30340;&#29305;&#24449;&#12290;&lt;/br&gt;
&lt;/p&gt;
&lt;p&gt;
Machine learning and Topological data analysis identify unique features of human papillae in 3D scans. (arXiv:2307.06255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25581;&#31034;&#20102;&#20154;&#31867;&#20083;&#31361;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#29305;&#24449;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20083;&#31361;&#24418;&#29366;&#30340;&#25345;&#32493;&#21516;&#35843;&#29305;&#24449;&#22312;&#39044;&#27979;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#26368;&#39640;&#25928;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33292;&#34920;&#38754;&#26377;&#35768;&#22810;&#20083;&#31361;&#65292;&#23545;&#20110;&#21619;&#35273;&#21644;&#21475;&#24863;&#30340;&#26426;&#26800;&#21644;&#21270;&#23398;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20083;&#31361;&#30340;&#21619;&#35273;&#21151;&#33021;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#20083;&#31361;&#22312;&#20010;&#20307;&#20869;&#22806;&#30340;&#29420;&#29305;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;3D&#26174;&#24494;&#25195;&#25551;&#30340;&#20154;&#31867;&#20083;&#31361;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65288;n = 2092&#65289;&#65292;&#25581;&#31034;&#20102;&#20083;&#31361;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#29305;&#24449;&#30340;&#29420;&#29305;&#24615;&#12290;&#22522;&#20110;&#31163;&#25955;&#24494;&#20998;&#20960;&#20309;&#21644;&#35745;&#31639;&#25299;&#25169;&#23398;&#30340;&#29305;&#24449;&#65292;&#35745;&#31639;&#26426;&#27169;&#25311;&#20102;&#20083;&#31361;&#24418;&#29366;&#20013;&#24494;&#23567;&#30340;&#24046;&#24322;&#12290;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#26126;&#65292;&#20083;&#31361;&#24418;&#29366;&#30340;&#25345;&#32493;&#21516;&#35843;&#29305;&#24449;&#26159;&#39044;&#27979;&#29983;&#29289;&#21464;&#37327;&#26368;&#26377;&#25928;&#30340;&#12290;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;85%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#20083;&#31361;&#31867;&#22411;&#12290;&#20083;&#31361;&#31867;&#22411;&#20998;&#31867;&#27169;&#22411;&#21487;&#20197;&#26144;&#23556;&#20083;&#19997;&#30340;&#31354;&#38388;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tongue surface houses a range of papillae that are integral to the mechanics and chemistry of taste and textural sensation. Although gustatory function of papillae is well investigated, the uniqueness of papillae within and across individuals remains elusive. Here, we present the first machine learning framework on 3D microscopic scans of human papillae (n = 2092), uncovering the uniqueness of geometric and topological features of papillae. The finer differences in shapes of papillae are investigated computationally based on a number of features derived from discrete differential geometry and computational topology. Interpretable machine learning techniques show that persistent homology features of the papillae shape are the most effective in predicting the biological variables. Models trained on these features with small volumes of data samples predict the type of papillae with an accuracy of 85%. The papillae type classification models can map the spatial arrangement of filiform 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#20173;&#28982;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06250</link><description>&lt;p&gt;
&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#20173;&#28982;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20998;&#35299;&#26088;&#22312;&#36890;&#36807;&#28508;&#22312;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#25581;&#31034;&#25968;&#25454;&#30340;&#34920;&#24449;&#65292;&#20854;&#36890;&#36807;&#22240;&#26524;&#27169;&#22411;&#30456;&#20114;&#20851;&#32852;&#12290;&#22914;&#26524;&#35299;&#37322;&#25968;&#25454;&#30340;&#28508;&#22312;&#27169;&#22411;&#26159;&#21807;&#19968;&#30340;&#65292;&#37027;&#20040;&#36825;&#31181;&#34920;&#31034;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#24403;&#23384;&#22312;&#19981;&#37197;&#23545;&#30340;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#26102;&#30340;&#24773;&#20917;&#65292;&#27599;&#20010;&#24178;&#39044;&#37117;&#20250;&#25913;&#21464;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26426;&#21046;&#12290;&#24403;&#22240;&#26524;&#21464;&#37327;&#23436;&#20840;&#35266;&#27979;&#21040;&#26102;&#65292;&#22312;&#35802;&#23454;&#24615;&#20551;&#35774;&#19979;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#32479;&#35745;&#19968;&#33268;&#30340;&#31639;&#27861;&#26469;&#35782;&#21035;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20445;&#35777;&#20102;&#25105;&#20204;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#65292;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#21644;ap&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#22240;&#26524;&#20998;&#35299;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal disentanglement aims to uncover a representation of data using latent variables that are interrelated through a causal model. Such a representation is identifiable if the latent model that explains the data is unique. In this paper, we focus on the scenario where unpaired observational and interventional data are available, with each intervention changing the mechanism of a latent variable. When the causal variables are fully observed, statistically consistent algorithms have been developed to identify the causal model under faithfulness assumptions. We here show that identifiability can still be achieved with unobserved causal variables, given a generalized notion of faithfulness. Our results guarantee that we can recover the latent causal model up to an equivalence class and predict the effect of unseen combinations of interventions, in the limit of infinite data. We implement our causal disentanglement framework by developing an autoencoding variational Bayes algorithm and ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CADENCE&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#36861;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#31232;&#30095;&#29366;&#24577;&#20449;&#24687;&#29983;&#25104;&#20840;&#38754;&#30340;&#23545;&#25163;&#20301;&#32622;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06244</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Diffusion Based Multi-Agent Adversarial Tracking. (arXiv:2307.06244v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CADENCE&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#36861;&#36394;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#31232;&#30095;&#29366;&#24577;&#20449;&#24687;&#29983;&#25104;&#20840;&#38754;&#30340;&#23545;&#25163;&#20301;&#32622;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#36861;&#36394;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#25171;&#20987;&#27602;&#21697;&#36208;&#31169;&#34892;&#21160;&#20013;&#65292;&#23545;&#25239;&#24615;&#30446;&#26631;&#30340;&#20301;&#32622;&#20449;&#24687;&#24448;&#24448;&#26159;&#26377;&#38480;&#30340;&#12290;&#25913;&#36827;&#33258;&#20027;&#36861;&#36394;&#31995;&#32479;&#23558;&#20351;&#26080;&#20154;&#26426;&#12289;&#27700;&#38754;&#33328;&#33351;&#21644;&#27700;&#19979;&#22120;&#26800;&#33021;&#22815;&#26356;&#22909;&#22320;&#21327;&#21161;&#25171;&#20987;&#20351;&#29992;&#20154;&#24037;&#27700;&#38754;&#33337;&#21482;&#12289;&#21322;&#28508;&#33351;&#21644;&#33322;&#31354;&#22120;&#30340;&#36208;&#31169;&#29359;&#12290;&#38543;&#30528;&#26080;&#20154;&#26426;&#30340;&#26222;&#21450;&#65292;&#20934;&#30830;&#30340;&#33258;&#20027;&#30446;&#26631;&#20272;&#35745;&#23545;&#23433;&#20840;&#21644;&#20445;&#38556;&#26356;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CADENCE&#30340;&#32422;&#26463;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#25193;&#25955;&#22686;&#24378;&#22810;&#26234;&#33021;&#20307;&#36861;&#36394;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#31232;&#30095;&#29366;&#24577;&#20449;&#24687;&#29983;&#25104;&#23545;&#25163;&#20301;&#32622;&#30340;&#20840;&#38754;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36861;&#36394;&#29615;&#22659;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#20272;&#35745;&#27599;&#20010;&#29983;&#25104;&#36712;&#36857;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Target tracking plays a crucial role in real-world scenarios, particularly in drug-trafficking interdiction, where the knowledge of an adversarial target's location is often limited. Improving autonomous tracking systems will enable unmanned aerial, surface, and underwater vehicles to better assist in interdicting smugglers that use manned surface, semi-submersible, and aerial vessels. As unmanned drones proliferate, accurate autonomous target estimation is even more crucial for security and safety. This paper presents Constrained Agent-based Diffusion for Enhanced Multi-Agent Tracking (CADENCE), an approach aimed at generating comprehensive predictions of adversary locations by leveraging past sparse state information. To assess the effectiveness of this approach, we evaluate predictions on single-target and multi-target pursuit environments, employing Monte-Carlo sampling of the diffusion model to estimate the probability associated with each generated trajectory. We propose a novel 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;C-VAE&#27169;&#22411;&#26469;&#29983;&#25104;&#24179;&#28369;&#19988;&#36924;&#30495;&#30340;&#26102;&#31354;&#28436;&#21464;&#34920;&#31034;&#65292;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#37325;&#24314;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.06243</link><description>&lt;p&gt;
&#29992;C-VAEs&#37325;&#24314;&#26102;&#31354;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Spatiotemporal Data with C-VAEs. (arXiv:2307.06243v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;C-VAE&#27169;&#22411;&#26469;&#29983;&#25104;&#24179;&#28369;&#19988;&#36924;&#30495;&#30340;&#26102;&#31354;&#28436;&#21464;&#34920;&#31034;&#65292;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#37325;&#24314;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#25968;&#25454;&#30340;&#36830;&#32493;&#34920;&#31034;&#36890;&#24120;&#20381;&#36182;&#20110;&#20351;&#29992;&#25277;&#35937;&#25968;&#25454;&#31867;&#22411;&#65292;&#20363;&#22914;&#31227;&#21160;&#21306;&#22495;&#65292;&#26469;&#34920;&#31034;&#24418;&#29366;&#21644;&#20301;&#32622;&#22312;&#26102;&#38388;&#19978;&#36830;&#32493;&#21464;&#21270;&#30340;&#23454;&#20307;&#12290;&#20174;&#31163;&#25955;&#30340;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#24555;&#29031;&#21019;&#24314;&#36825;&#31181;&#34920;&#31034;&#38656;&#35201;&#20351;&#29992;&#25554;&#20540;&#26041;&#27861;&#26469;&#35745;&#31639;&#20013;&#38388;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#20272;&#35745;&#24863;&#20852;&#36259;&#23545;&#35937;&#22312;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#20301;&#32622;&#21644;&#24418;&#29366;&#12290;&#29616;&#26377;&#30340;&#21306;&#22495;&#25554;&#20540;&#26041;&#27861;&#24120;&#24120;&#26080;&#27861;&#29983;&#25104;&#24179;&#28369;&#21644;&#36924;&#30495;&#30340;&#21306;&#22495;&#28436;&#21464;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#25581;&#31034;&#20102;&#22522;&#20110;&#31163;&#25955;&#35266;&#27979;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#36890;&#36807;&#38544;&#24335;&#29305;&#24449;&#23398;&#20064;&#21487;&#20197;&#25429;&#25417;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;C-VAE&#65289;&#27169;&#22411;&#29983;&#25104;&#31227;&#21160;&#21306;&#22495;&#30340;&#26102;&#31354;&#28436;&#21464;&#24179;&#28369;&#21644;&#36924;&#30495;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous representation of spatiotemporal data commonly relies on using abstract data types, such as \textit{moving regions}, to represent entities whose shape and position continuously change over time. Creating this representation from discrete snapshots of real-world entities requires using interpolation methods to compute in-between data representations and estimate the position and shape of the object of interest at arbitrary temporal points. Existing region interpolation methods often fail to generate smooth and realistic representations of a region's evolution. However, recent advancements in deep learning techniques have revealed the potential of deep models trained on discrete observations to capture spatiotemporal dependencies through implicit feature learning.  In this work, we explore the capabilities of Conditional Variational Autoencoder (C-VAE) models to generate smooth and realistic representations of the spatiotemporal evolution of moving regions. We evaluate our
&lt;/p&gt;</description></item><item><title>DSSE&#26159;&#19968;&#20010;&#26080;&#20154;&#26426;&#32676;&#38598;&#25628;&#32034;&#29615;&#22659;&#65292;&#29992;&#20110;&#30740;&#31350;&#38656;&#35201;&#21160;&#24577;&#27010;&#29575;&#20316;&#20026;&#36755;&#20837;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06240</link><description>&lt;p&gt;
DSSE: &#26080;&#20154;&#26426;&#32676;&#38598;&#25628;&#32034;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
DSSE: a drone swarm search environment. (arXiv:2307.06240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06240
&lt;/p&gt;
&lt;p&gt;
DSSE&#26159;&#19968;&#20010;&#26080;&#20154;&#26426;&#32676;&#38598;&#25628;&#32034;&#29615;&#22659;&#65292;&#29992;&#20110;&#30740;&#31350;&#38656;&#35201;&#21160;&#24577;&#27010;&#29575;&#20316;&#20026;&#36755;&#20837;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#32676;&#38598;&#25628;&#32034;&#39033;&#30446;&#26159;&#19968;&#20010;&#22522;&#20110;PettingZoo&#30340;&#29615;&#22659;&#65292;&#19982;&#22810;&#26234;&#33021;&#20307;&#65288;&#25110;&#21333;&#26234;&#33021;&#20307;&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#37197;&#21512;&#20351;&#29992;&#12290;&#35813;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#65288;&#26080;&#20154;&#26426;&#65289;&#24517;&#39035;&#25214;&#21040;&#30446;&#26631;&#65288;&#36935;&#38505;&#20154;&#21592;&#65289;&#65292;&#20294;&#19981;&#30693;&#36947;&#30446;&#26631;&#30340;&#20301;&#32622;&#65292;&#24182;&#19988;&#19981;&#20250;&#26681;&#25454;&#33258;&#36523;&#19982;&#30446;&#26631;&#30340;&#36317;&#31163;&#24471;&#21040;&#22870;&#21169;&#12290;&#20294;&#26159;&#65292;&#26234;&#33021;&#20307;&#20250;&#25509;&#25910;&#21040;&#30446;&#26631;&#20986;&#29616;&#22312;&#22320;&#22270;&#26576;&#20010;&#21333;&#20803;&#26684;&#30340;&#27010;&#29575;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#24110;&#21161;&#30740;&#31350;&#38656;&#35201;&#21160;&#24577;&#27010;&#29575;&#20316;&#20026;&#36755;&#20837;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Drone Swarm Search project is an environment, based on PettingZoo, that is to be used in conjunction with multi-agent (or single-agent) reinforcement learning algorithms. It is an environment in which the agents (drones), have to find the targets (shipwrecked people). The agents do not know the position of the target and do not receive rewards related to their own distance to the target(s). However, the agents receive the probabilities of the target(s) being in a certain cell of the map. The aim of this project is to aid in the study of reinforcement learning algorithms that require dynamic probabilities as inputs.
&lt;/p&gt;</description></item><item><title>MoleBLEND&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;2D&#21644;3D&#20998;&#23376;&#32467;&#26500;&#36827;&#34892;&#32479;&#19968;&#32534;&#30721;&#21644;&#34701;&#21512;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#26032;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.06235</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#24577;&#34701;&#21512;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Unified Molecular Modeling via Modality Blending. (arXiv:2307.06235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06235
&lt;/p&gt;
&lt;p&gt;
MoleBLEND&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;2D&#21644;3D&#20998;&#23376;&#32467;&#26500;&#36827;&#34892;&#32479;&#19968;&#32534;&#30721;&#21644;&#34701;&#21512;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#26032;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#23545;&#20110;&#22522;&#20110;&#20998;&#23376;&#30340;&#20219;&#21153;&#22914;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#33647;&#29289;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;2D&#21644;3D&#20449;&#24687;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#37319;&#29992;&#23558;&#27599;&#31181;&#27169;&#24577;&#20998;&#24320;&#22788;&#29702;&#30340;&#30452;&#25509;&#23545;&#40784;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;"&#28151;&#21512;-&#39044;&#27979;"&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;MoleBLEND&#65289;&#65292;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#21407;&#23376;&#38388;&#20851;&#31995;&#34701;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#20851;&#31995;&#30697;&#38453;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#24674;&#22797;2D&#21644;3D&#32467;&#26500;&#30340;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#12290;&#36890;&#36807;&#23558;&#21407;&#23376;&#20851;&#31995;&#35270;&#20026;&#38170;&#28857;&#65292;&#30475;&#20284;&#19981;&#30456;&#20284;&#30340;2D&#21644;3D&#27969;&#24418;&#22312;&#32454;&#31890;&#24230;&#30340;&#20851;&#31995;&#32423;&#21035;&#19978;&#26377;&#26426;&#22320;&#23545;&#40784;&#21644;&#25972;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;MoleBLEND&#22312;&#20027;&#35201;&#30340;2D/3D&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#30456;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#23519;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#23545;&#27604;&#12289;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-supervised molecular representation learning is critical for molecule-based tasks such as AI-assisted drug discovery. Recent studies consider leveraging both 2D and 3D information for representation learning, with straightforward alignment strategies that treat each modality separately. In this work, we introduce a novel "blend-then-predict" self-supervised learning method (MoleBLEND), which blends atom relations from different modalities into one unified relation matrix for encoding, then recovers modality-specific information for both 2D and 3D structures. By treating atom relationships as anchors, seemingly dissimilar 2D and 3D manifolds are aligned and integrated at fine-grained relation-level organically. Extensive experiments show that MoleBLEND achieves state-of-the-art performance across major 2D/3D benchmarks. We further provide theoretical insights from the perspective of mutual-information maximization, demonstrating that our method unifies contrastive, generative (inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22330;&#22343;&#25511;&#21046;&#27169;&#22411;&#65288;Dec-POMFC&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#25955;&#21270;&#12289;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#25361;&#25112;&#12290;&#35813;&#27169;&#22411;&#21487;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#21487;&#35299;&#20915;&#30340;&#21333;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#38598;&#20307;&#34892;&#20026;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.06175</link><description>&lt;p&gt;
&#23398;&#20064;&#20998;&#25955;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#22330;&#22343;&#25511;&#21046;&#26469;&#23454;&#29616;&#20154;&#24037;&#38598;&#20307;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior. (arXiv:2307.06175v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22330;&#22343;&#25511;&#21046;&#27169;&#22411;&#65288;Dec-POMFC&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#25955;&#21270;&#12289;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#21487;&#25193;&#23637;&#24615;&#31561;&#25361;&#25112;&#12290;&#35813;&#27169;&#22411;&#21487;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#21487;&#35299;&#20915;&#30340;&#21333;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#38598;&#20307;&#34892;&#20026;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#20998;&#25955;&#21270;&#12289;&#37096;&#20998;&#21487;&#35266;&#23519;&#20197;&#21450;&#38754;&#23545;&#20247;&#22810;&#26234;&#33021;&#20307;&#26102;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#38598;&#20307;&#34892;&#20026;&#35201;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#19988;&#23545;&#20110;&#35768;&#22810;&#26368;&#21069;&#27839;&#30340;&#24212;&#29992;&#65292;&#22914;&#27963;&#21160;&#29289;&#36136;&#29289;&#29702;&#12289;&#33258;&#32452;&#32455;&#31995;&#32479;&#12289;&#33286;&#35770;&#21160;&#24577;&#20197;&#21450;&#29983;&#29289;&#25110;&#26426;&#22120;&#20154;&#32676;&#20307;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#20998;&#25955;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#22330;&#22343;&#25511;&#21046;&#65288;Dec-POMFC&#65289;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#22312;&#37096;&#20998;&#20449;&#24687;&#19979;&#30340;&#20998;&#25955;&#34892;&#20026;&#65292;&#36825;&#26159;&#19968;&#31867;&#20801;&#35768;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#21487;&#35299;&#20915;&#30340;&#21333;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#25490;&#21015;&#19981;&#21464;&#20195;&#29702;&#30340;&#24191;&#27867;&#38382;&#39064;&#65292;&#20197;&#21450;&#21333;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms. Here, MARL via mean field control (MFC) offers a potential solution to scalability, but fails to consider decentralized and partially observable systems. In this paper, we enable decentralized behavior of agents under partial information by proposing novel models for decentralized partially observable MFC (Dec-POMFC), a broad class of problems with permutation-invariant agents allowing for reduction to tractable single-agent Markov decision processes (MDP) with single-agent RL solution. We provide rigorous theoretical
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;ATL-PINNs&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#21407;&#22987;PINNs&#22312;&#22797;&#26434;&#29289;&#29702;&#24773;&#22659;&#20013;&#30340;&#20302;&#20934;&#30830;&#24615;&#21644;&#19981;&#25910;&#25947;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#31181;&#19981;&#21516;&#30340;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#27169;&#24335;&#21644;&#26799;&#24230;&#20313;&#24358;&#30456;&#20284;&#24230;&#31639;&#27861;&#65292;&#25552;&#21319;&#20102;ATL-PINNs&#30340;&#25928;&#26524;&#12290;&#36825;&#26159;&#39318;&#27425;&#23558;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#27169;&#24335;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#30740;&#31350;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.06167</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Auxiliary-Tasks Learning for Physics-Informed Neural Network-Based Partial Differential Equations Solving. (arXiv:2307.06167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06167
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;ATL-PINNs&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#21407;&#22987;PINNs&#22312;&#22797;&#26434;&#29289;&#29702;&#24773;&#22659;&#20013;&#30340;&#20302;&#20934;&#30830;&#24615;&#21644;&#19981;&#25910;&#25947;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#31181;&#19981;&#21516;&#30340;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#27169;&#24335;&#21644;&#26799;&#24230;&#20313;&#24358;&#30456;&#20284;&#24230;&#31639;&#27861;&#65292;&#25552;&#21319;&#20102;ATL-PINNs&#30340;&#25928;&#26524;&#12290;&#36825;&#26159;&#39318;&#27425;&#23558;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#27169;&#24335;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#27169;&#24335;&#12290;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#22312;&#20110;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#19982;&#35299;&#20915;&#26041;&#26696;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;PINNs&#32463;&#24120;&#38754;&#20020;&#20302;&#20934;&#30830;&#24615;&#21644;&#19981;&#25910;&#25947;&#31561;&#29942;&#39048;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22797;&#26434;&#29289;&#29702;&#24773;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;ATL-PINNs&#65289;&#65292;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#27169;&#24335;&#65292;&#24182;&#19982;&#21407;&#22987;PINNs&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#26799;&#24230;&#20313;&#24358;&#30456;&#20284;&#24230;&#31639;&#27861;&#23558;&#36741;&#21161;&#38382;&#39064;&#25439;&#22833;&#19982;&#20027;&#35201;&#38382;&#39064;&#25439;&#22833;&#30456;&#32467;&#21512;&#22312;ATL-PINNs&#20013;&#65292;&#26088;&#22312;&#22686;&#24378;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#23398;&#20064;&#27169;&#24335;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;...
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have emerged as promising surrogate modes for solving partial differential equations (PDEs). Their effectiveness lies in the ability to capture solution-related features through neural networks. However, original PINNs often suffer from bottlenecks, such as low accuracy and non-convergence, limiting their applicability in complex physical contexts. To alleviate these issues, we proposed auxiliary-task learning-based physics-informed neural networks (ATL-PINNs), which provide four different auxiliary-task learning modes and investigate their performance compared with original PINNs. We also employ the gradient cosine similarity algorithm to integrate auxiliary problem loss with the primary problem loss in ATL-PINNs, which aims to enhance the effectiveness of the auxiliary-task learning modes. To the best of our knowledge, this is the first study to introduce auxiliary-task learning modes in the context of physics-informed learning. We conduct exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#29702;&#20449;&#21495;&#30740;&#31350;&#39046;&#22495;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#26368;&#20808;&#36827;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#26377;&#21161;&#20110;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#29702;&#20449;&#21495;&#20013;&#30340;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2307.06162</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23545;&#29983;&#29702;&#20449;&#21495;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Physiological Signals: A Systematic Literature Review. (arXiv:2307.06162v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#29702;&#20449;&#21495;&#30740;&#31350;&#39046;&#22495;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#26368;&#20808;&#36827;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#26377;&#21161;&#20110;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#29702;&#20449;&#21495;&#20013;&#30340;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#29702;&#20449;&#21495;&#65292;&#29305;&#21035;&#26159;&#24515;&#30005;&#22270;&#12289;&#33041;&#30005;&#22270;&#12289;&#20809;&#30005;&#23481;&#25239;&#22270;&#21644;&#32908;&#30005;&#22270;&#39046;&#22495;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#19982;&#24050;&#26377;&#30340;&#32508;&#36848;&#25991;&#31456;&#30456;&#27604;&#65292;&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#24635;&#32467;&#26368;&#26032;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#32508;&#36848;&#12290;&#36890;&#36807;&#20998;&#26512;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30456;&#20851;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#20197;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#20027;&#35201;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#26412;&#32508;&#36848;&#20026;&#23545;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#29983;&#29702;&#20449;&#21495;&#30340;&#25972;&#20307;&#29702;&#35299;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24378;&#35843;&#37319;&#29992;&#30340;&#35780;&#20272;&#21327;&#35758;&#21644;&#26368;&#24120;&#29992;&#30340;&#29983;&#29702;&#25968;&#25454;&#24211;&#65292;&#26412;&#32508;&#36848;&#26377;&#21161;&#20110;&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a systematic literature review on deep generative models for physiological signals, particularly electrocardiogram, electroencephalogram, photoplethysmogram and electromyogram. Compared to the existing review papers, we present the first review that summarizes the recent state-of-the-art deep generative models. By analysing the state-of-the-art research related to deep generative models along with their main applications and challenges, this review contributes to the overall understanding of these models applied to physiological signals. Additionally, by highlighting the employed evaluation protocol and the most used physiological databases, this review facilitates the assessment and benchmarking of deep generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31354;&#25112;&#20013;&#30340;&#26377;&#25928;&#20915;&#31574;&#12290;&#36890;&#36807;&#36880;&#28176;&#23398;&#20064;&#19968;&#31995;&#21015;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#23376;&#20219;&#21153;&#65292;&#20195;&#29702;&#33021;&#22815;&#22312;&#21508;&#31181;&#29366;&#24577;&#19979;&#20570;&#20986;&#26377;&#25928;&#30340;&#26426;&#21160;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.06152</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#26426;&#21160;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning Without Handcrafted Reward functions. (arXiv:2307.06152v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31354;&#25112;&#20013;&#30340;&#26377;&#25928;&#20915;&#31574;&#12290;&#36890;&#36807;&#36880;&#28176;&#23398;&#20064;&#19968;&#31995;&#21015;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#23376;&#20219;&#21153;&#65292;&#20195;&#29702;&#33021;&#22815;&#22312;&#21508;&#31181;&#29366;&#24577;&#19979;&#20570;&#20986;&#26377;&#25928;&#30340;&#26426;&#21160;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#21160;&#20915;&#31574;&#26159;&#26080;&#20154;&#20316;&#25112;&#39134;&#34892;&#22120;&#33258;&#20027;&#31354;&#25112;&#30340;&#26680;&#24515;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31354;&#25112;&#20013;&#30340;&#26377;&#25928;&#20915;&#31574;&#12290;&#36890;&#36807;&#21021;&#22987;&#29366;&#24577;&#33539;&#22260;&#26469;&#21306;&#20998;&#19981;&#21516;&#38590;&#24230;&#27700;&#24179;&#30340;&#35838;&#31243;&#65292;&#23558;&#26426;&#21160;&#20915;&#31574;&#20998;&#20026;&#19968;&#31995;&#21015;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#27979;&#35797;&#32467;&#26524;&#26469;&#25913;&#21464;&#23376;&#20219;&#21153;&#12290;&#38543;&#30528;&#23376;&#20219;&#21153;&#30340;&#21464;&#21270;&#65292;&#20195;&#29702;&#36880;&#28176;&#23398;&#20250;&#23436;&#25104;&#19968;&#31995;&#21015;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#23376;&#20219;&#21153;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#26080;&#38656;&#36827;&#34892;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#20570;&#20986;&#26377;&#25928;&#30340;&#26426;&#21160;&#20915;&#31574;&#20197;&#24212;&#23545;&#21508;&#31181;&#29366;&#24577;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#33258;&#21160;&#35838;&#31243;&#23398;&#20064;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#20195;&#29702;&#22312;&#27809;&#26377;&#35838;&#31243;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#23436;&#25104;&#26377;&#25928;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maneuver decision-making is the core of unmanned combat aerial vehicle for autonomous air combat. To solve this problem, we propose an automatic curriculum reinforcement learning method, which enables agents to learn effective decisions in air combat from scratch. The range of initial states are used for distinguishing curricula of different difficulty levels, thereby maneuver decision is divided into a series of sub-tasks from easy to difficult, and test results are used to change sub-tasks. As sub-tasks change, agents gradually learn to complete a series of sub-tasks from easy to difficult, enabling them to make effective maneuvering decisions to cope with various states without the need to spend effort designing reward functions. The ablation studied show that the automatic curriculum learning proposed in this article is an essential component for training through reinforcement learning, namely, agents cannot complete effective decisions without curriculum learning. Simulation exper
&lt;/p&gt;</description></item><item><title>NetGPT&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#26469;&#20248;&#21270;&#36164;&#28304;&#21327;&#35843;&#21644;&#20114;&#21160;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06148</link><description>&lt;p&gt;
NetGPT: &#36229;&#36234;&#25552;&#20379;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services. (arXiv:2307.06148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06148
&lt;/p&gt;
&lt;p&gt;
NetGPT&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#22320;AI&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#26469;&#20248;&#21270;&#36164;&#28304;&#21327;&#35843;&#21644;&#20114;&#21160;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#29983;&#25104;&#20449;&#24687;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;LLMs&#30340;&#20010;&#24615;&#21270;&#21487;&#33021;&#36827;&#19968;&#27493;&#20419;&#36827;&#23427;&#20204;&#22312;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#12290;&#38024;&#23545;&#20010;&#24615;&#21270;&#29983;&#25104;&#26381;&#21153;&#65292;&#21327;&#20316;&#20113;&#36793;&#26041;&#27861;&#35770;&#21548;&#36215;&#26469;&#24456;&#26377;&#21069;&#26223;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#26377;&#25928;&#21327;&#35843;&#24322;&#26500;&#20998;&#24067;&#24335;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#31181;&#20505;&#36873;&#30340;&#20113;&#36793;&#21327;&#20316;&#25216;&#26415;&#30340;&#21033;&#24330;&#65292;&#25552;&#20986;&#20102;NetGPT&#65292;&#26681;&#25454;&#20854;&#35745;&#31639;&#33021;&#21147;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#36866;&#24403;&#30340;LLMs&#12290;&#27492;&#22806;&#65292;&#36793;&#32536;LLMs&#21487;&#20197;&#39640;&#25928;&#21033;&#29992;&#22522;&#20110;&#20301;&#32622;&#30340;&#20449;&#24687;&#36827;&#34892;&#20010;&#24615;&#21270;&#25552;&#31034;&#23436;&#25104;&#65292;&#20174;&#32780;&#26377;&#30410;&#20110;&#19982;&#20113;&#31471;LLMs&#30340;&#20114;&#21160;&#12290;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#37096;&#32626;&#20195;&#34920;&#24615;&#30340;&#24320;&#28304;LLMs&#65288;&#20363;&#22914;GPT-2-base&#21644;LLaMA&#27169;&#22411;&#65289;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NetGPT&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have triggered tremendous success to empower daily life by generative information, and the personalization of LLMs could further contribute to their applications due to better alignment with human intents. Towards personalized generative services, a collaborative cloud-edge methodology sounds promising, as it facilitates the effective orchestration of heterogeneous distributed communication and computing resources. In this article, after discussing the pros and cons of several candidate cloud-edge collaboration techniques, we put forward NetGPT to capably deploy appropriate LLMs at the edge and the cloud in accordance with their computing capacity. In addition, edge LLMs could efficiently leverage location-based information for personalized prompt completion, thus benefiting the interaction with cloud LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06125</link><description>&lt;p&gt;
&#23398;&#20064;&#23618;&#27425;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20197;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06125
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#33258;&#30001;&#36335;&#24452;&#19978;&#36827;&#34892;&#25628;&#32034;&#65292;&#28982;&#32780;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#32463;&#24120;&#38656;&#35201;&#25805;&#25511;&#29615;&#22659;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#25171;&#24320;&#38376;&#20197;&#27983;&#35272;&#25151;&#38388;&#65292;&#24182;&#22312;&#27249;&#26588;&#21644;&#25277;&#23625;&#20869;&#25628;&#32034;&#30446;&#26631;&#29289;&#21697;&#12290;&#36825;&#20123;&#26032;&#25361;&#25112;&#38656;&#35201;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#32467;&#21512;&#25805;&#25511;&#21644;&#23548;&#33322;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HIMOS&#65292;&#19968;&#31181;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#32452;&#21512;&#25506;&#32034;&#12289;&#23548;&#33322;&#21644;&#25805;&#25511;&#25216;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22260;&#32469;&#35821;&#20041;&#22320;&#22270;&#35760;&#24518;&#30340;&#25277;&#35937;&#39640;&#32423;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#25506;&#32034;&#36807;&#30340;&#29615;&#22659;&#20316;&#20026;&#23454;&#20363;&#23548;&#33322;&#28857;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;HIMOS&#21487;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#26377;&#25928;&#22320;&#36801;&#31227;&#21040;&#26032;&#30340;&#29615;&#22659;&#20013;&#65292;&#24182;&#19988;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#21363;MIBench&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#36825;&#20010;&#22522;&#20934;&#19981;&#20165;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#65292;&#36824;&#21253;&#25324;&#35780;&#20272;&#22330;&#26223;&#65292;&#20174;&#19981;&#21516;&#35282;&#24230;&#32771;&#34385;&#20102;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#20998;&#24067;&#21644;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.06123</link><description>&lt;p&gt;
SoK: &#20351;&#29992;&#32508;&#21512;&#22522;&#20934;&#27604;&#36739;&#19981;&#21516;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark. (arXiv:2307.06123v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#21363;MIBench&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#36825;&#20010;&#22522;&#20934;&#19981;&#20165;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#65292;&#36824;&#21253;&#25324;&#35780;&#20272;&#22330;&#26223;&#65292;&#20174;&#19981;&#21516;&#35282;&#24230;&#32771;&#34385;&#20102;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#20998;&#24067;&#21644;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#23041;&#32961;&#29992;&#25143;&#38544;&#31169;&#65292;&#36890;&#36807;&#30830;&#23450;&#32473;&#23450;&#30340;&#25968;&#25454;&#31034;&#20363;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#20154;&#35748;&#35782;&#21040;&#29616;&#26377;&#20316;&#21697;&#20013;&#20351;&#29992;&#30340;"&#27604;&#36739;&#19981;&#21516;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;"&#26041;&#27861;&#26377;&#20005;&#37325;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#26412;&#30740;&#31350;&#20013;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#25991;&#29486;&#20013;&#25253;&#21578;&#30340;&#27604;&#36739;&#32467;&#26524;&#26159;&#30456;&#24403;&#35823;&#23548;&#20154;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#31216;&#20026;MIBench&#65292;&#23427;&#19981;&#20165;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#65292;&#36824;&#21253;&#25324;&#35780;&#20272;&#22330;&#26223;&#12290;&#25105;&#20204;&#20174;&#22235;&#20010;&#35282;&#24230;&#35774;&#35745;&#35780;&#20272;&#22330;&#26223;&#65306;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#25968;&#25454;&#26679;&#26412;&#30340;&#36317;&#31163;&#20998;&#24067;&#65292;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20004;&#20010;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#36317;&#31163;&#65288;&#21363;&#30446;&#26631;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20165;&#21253;&#21547;&#38750;&#25104;&#21592;&#30340;&#29983;&#25104;&#25968;&#25454;&#38598;&#65289;&#65292;&#20197;&#21450;&#27809;&#26377;&#25512;&#26029;&#30340;&#26679;&#26412;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference (MI) attacks threaten user privacy through determining if a given data example has been used to train a target model. However, it has been increasingly recognized that the "comparing different MI attacks" methodology used in the existing works has serious limitations. Due to these limitations, we found (through the experiments in this work) that some comparison results reported in the literature are quite misleading. In this paper, we seek to develop a comprehensive benchmark for comparing different MI attacks, called MIBench, which consists not only the evaluation metrics, but also the evaluation scenarios. And we design the evaluation scenarios from four perspectives: the distance distribution of data samples in the target dataset, the distance between data samples of the target dataset, the differential distance between two datasets (i.e., the target dataset and a generated dataset with only nonmembers), and the ratio of the samples that are made no inferences b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#39046;&#22495;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06104</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#65306;&#27169;&#22411;&#19982;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Deep learning for dynamic graphs: models and benchmarks. (arXiv:2307.06104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#21160;&#24577;&#22270;&#39046;&#22495;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#28145;&#24230;&#22270;&#32593;&#32476;&#65288;DGNs&#65289;&#30340;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#25512;&#21160;&#20102;&#22270;&#19978;&#23398;&#20064;&#30340;&#39046;&#22495;&#25104;&#29087;&#21457;&#23637;&#12290;&#23613;&#31649;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#27491;&#22312;&#24555;&#36895;&#22686;&#38271;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#23578;&#26410;&#35299;&#20915;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#29305;&#21035;&#22320;&#65292;&#24613;&#38656;&#20351;DGNs&#36866;&#29992;&#20110;&#23454;&#26102;&#31995;&#32479;&#20013;&#38543;&#26102;&#38388;&#25512;&#31227;&#19981;&#26029;&#28436;&#21270;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#20026;&#20419;&#36827;&#21160;&#24577;&#22270;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23398;&#20064;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#30340;&#26368;&#26032;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#24403;&#21069;&#26368;&#26032;&#27010;&#35272;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#35780;&#20272;&#65292;&#20026;&#35780;&#20272;&#26032;&#26550;&#26500;&#21644;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in research on Deep Graph Networks (DGNs) has led to a maturation of the domain of learning on graphs. Despite the growth of this research field, there are still important challenges that are yet unsolved. Specifically, there is an urge of making DGNs suitable for predictive tasks on realworld systems of interconnected entities, which evolve over time. With the aim of fostering research in the domain of dynamic graphs, at first, we survey recent advantages in learning both temporal and spatial information, providing a comprehensive overview of the current state-of-the-art in the domain of representation learning for dynamic graphs. Secondly, we conduct a fair performance comparison among the most popular proposed approaches, leveraging rigorous model selection and assessment for all the methods, thus establishing a sound baseline for evaluating new architectures and approaches
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28418;&#31227;&#39033;&#21644;&#25193;&#25955;&#39033;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#35266;&#27979;&#21040;&#30340;&#38543;&#26426;&#24615;&#21644;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06097</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Stochastic Dynamical Systems as an Implicit Regularization with Graph Neural Networks. (arXiv:2307.06097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28418;&#31227;&#39033;&#21644;&#25193;&#25955;&#39033;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#35266;&#27979;&#21040;&#30340;&#38543;&#26426;&#24615;&#21644;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38543;&#26426;Gumbel&#22270;&#32593;&#32476;&#26469;&#23398;&#20064;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#20013;&#35266;&#27979;&#21040;&#30340;&#32500;&#24230;&#36890;&#24120;&#20855;&#26377;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#27492;&#65292;&#36890;&#36807;&#23398;&#20064;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28418;&#31227;&#39033;&#21644;&#25193;&#25955;&#39033;&#65292;&#20998;&#21035;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#38543;&#26426;&#24615;&#21644;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#37319;&#29992;Gumbel&#30697;&#38453;&#23884;&#20837;&#26469;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;S-GGNs&#20013;&#22122;&#22768;&#39033;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#26524;&#12290;&#36890;&#36807;&#25512;&#23548;&#26435;&#37325;&#30340;&#23567;&#37051;&#22495;&#20013;&#20004;&#20010;&#30456;&#24212;&#25439;&#22833;&#20989;&#25968;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#20026;&#25552;&#20986;&#30340;S-GGNs&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Kuramoto&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#65292;&#27604;&#36739;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#35889;&#23494;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;S-GGNs&#20855;&#26377;&#36739;&#22909;&#30340;&#25910;&#25947;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gumbel graph networks are proposed to learn high-dimensional time series, where the observed dimensions are often spatially correlated. To that end, the observed randomness and spatial-correlations are captured by learning the drift and diffusion terms of the stochastic differential equation with a Gumble matrix embedding, respectively. In particular, this novel framework enables us to investigate the implicit regularization effect of the noise terms in S-GGNs. We provide a theoretical guarantee for the proposed S-GGNs by deriving the difference between the two corresponding loss functions in a small neighborhood of weight. Then, we employ Kuramoto's model to generate data for comparing the spectral density from the Hessian Matrix of the two loss functions. Experimental results on real-world data, demonstrate that S-GGNs exhibit superior convergence, robustness, and generalization, compared with state-of-the-arts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#25512;&#23548;&#20102;&#22312;&#32447; Laplace &#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#30446;&#26631;&#23450;&#20301;&#20026;&#27169;&#24577;&#20462;&#27491;&#30340;&#21464;&#20998;&#19978;&#30028;&#65292;&#36991;&#20813;&#20102;&#23545;&#24179;&#31283;&#24615;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#20351;&#29992;&#20840;&#25209;&#37327;&#26799;&#24230;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#20102;&#36825;&#20123;&#26368;&#20248;&#28857;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06093</link><description>&lt;p&gt;
&#22312;&#32447; Laplace &#27169;&#22411;&#36873;&#25321;&#30340;&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Online Laplace Model Selection Revisited. (arXiv:2307.06093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#25512;&#23548;&#20102;&#22312;&#32447; Laplace &#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#30446;&#26631;&#23450;&#20301;&#20026;&#27169;&#24577;&#20462;&#27491;&#30340;&#21464;&#20998;&#19978;&#30028;&#65292;&#36991;&#20813;&#20102;&#23545;&#24179;&#31283;&#24615;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#20351;&#29992;&#20840;&#25209;&#37327;&#26799;&#24230;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#20102;&#36825;&#20123;&#26368;&#20248;&#28857;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Laplace &#36817;&#20284;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#30340;&#27169;&#22411;&#36873;&#25321;&#30446;&#26631;&#12290;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#19982;&#36229;&#21442;&#25968;&#65288;&#22914;&#26435;&#37325;&#34928;&#20943;&#24378;&#24230;&#65289;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#30340;&#22312;&#32447;&#21464;&#20307;&#26041;&#27861;&#20877;&#27425;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36829;&#21453;&#20102; Laplace &#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#65292;&#21363;&#36817;&#20284;&#26159;&#22260;&#32469;&#25439;&#22833;&#30340;&#27169;&#24577;&#36827;&#34892;&#30340;&#65292;&#36825;&#23601;&#23545;&#23427;&#20204;&#30340;&#21512;&#29702;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#25512;&#23548;&#20102;&#22312;&#32447; Laplace &#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#38024;&#23545; Laplace &#35777;&#25454;&#30340;&#19968;&#20010;&#20462;&#27491;&#27169;&#24577;&#30340;&#21464;&#20998;&#19978;&#30028;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#24179;&#31283;&#24615;&#30340;&#20551;&#35774;&#12290;&#22312;&#32447; Laplace &#26041;&#27861;&#21450;&#20854;&#20462;&#27491;&#27169;&#24577;&#30340;&#23545;&#24212;&#28857;&#28385;&#36275;&#20004;&#20010;&#26465;&#20214;&#65306;1. &#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26159;&#19968;&#20010;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#65292;&#28385;&#36275; Laplace &#26041;&#27861;&#30340;&#20551;&#35774;&#65307;2. &#36229;&#21442;&#25968;&#26368;&#22823;&#21270; Laplace &#35777;&#25454;&#65292;&#20174;&#32780;&#20419;&#20351;&#22312;&#32447;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20840;&#25209;&#37327;&#26799;&#24230;&#30340;&#22312;&#32447;&#31639;&#27861;&#28436;&#31034;&#20102;&#36825;&#20123;&#26368;&#20248;&#28857;&#22312;&#23454;&#36341;&#20013;&#30340;&#36817;&#20284;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Laplace approximation provides a closed-form model selection objective for neural networks (NN). Online variants, which optimise NN parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the Bayesian deep learning community. However, these methods violate Laplace's method's critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness. This work re-derives online Laplace methods, showing them to target a variational bound on a mode-corrected variant of the Laplace evidence which does not make stationarity assumptions. Online Laplace and its mode-corrected counterpart share stationary points where 1. the NN parameters are a maximum a posteriori, satisfying the Laplace method's assumption, and 2. the hyperparameters maximise the Laplace evidence, motivating online methods. We demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06092</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#37327;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#20854;&#20013;&#38544;&#34255;&#23618;&#23485;&#24230;&#19982;&#22823;&#24120;&#25968; $n$ &#25104;&#27604;&#20363;&#12290;&#22312;&#38750;&#32447;&#24615;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#26377;&#38480;&#32500;&#20998;&#24067;&#36824;&#26159;&#25972;&#20010;&#36807;&#31243;&#65292;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#65288;&#21450;&#20854;&#23548;&#25968;&#65289;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#37117;&#20250;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#20854;&#20013; $\gamma&gt;0$&#65292;&#25351;&#25968;&#21462;&#20915;&#20110;&#29992;&#20110;&#24230;&#37327;&#24046;&#24322;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20379;&#30340;&#20219;&#20309;&#30028;&#38480;&#37117;&#35201;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.06060</link><description>&lt;p&gt;
&#35299;&#35835;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#30340;&#28145;&#24230;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24739;&#32773;&#32858;&#31867;&#30340;&#32972;&#26223;&#19979;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#33521;&#22269;&#29983;&#29289;&#24211;&#30340;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach for interpreting deep embeddings in the context of patient clustering. We evaluate our approach on a dataset of participants with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful insights into disease progression patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20989;&#25968;&#31354;&#38388;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#22686;&#21152;&#28145;&#24230;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;Dirichlet&#20808;&#39564;&#22312;&#39044;&#27979;&#31354;&#38388;&#20013;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#33021;&#19982;&#19981;&#21516;&#27169;&#22411;&#30456;&#32467;&#21512;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#26550;&#26500;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.06055</link><description>&lt;p&gt;
&#28145;&#24230;&#36125;&#21494;&#26031;&#20998;&#31867;&#30340;&#20989;&#25968;&#31354;&#38388;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Function-Space Regularization for Deep Bayesian Classification. (arXiv:2307.06055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20989;&#25968;&#31354;&#38388;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#22686;&#21152;&#28145;&#24230;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;Dirichlet&#20808;&#39564;&#22312;&#39044;&#27979;&#31354;&#38388;&#20013;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#33021;&#19982;&#19981;&#21516;&#27169;&#22411;&#30456;&#32467;&#21512;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#26550;&#26500;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#27169;&#22411;&#21442;&#25968;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#24182;&#25512;&#26029;&#21518;&#39564;&#20998;&#24067;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#22686;&#21152;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#38450;&#27490;&#36807;&#20110;&#33258;&#20449;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#26435;&#37325;&#31354;&#38388;&#20808;&#39564;&#26159;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#65292;&#21487;&#33021;&#38590;&#20197;&#35299;&#37322;&#21644;&#38590;&#20197;&#25351;&#23450;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#22312;&#39044;&#27979;&#31354;&#38388;&#20013;&#24212;&#29992;Dirichlet&#20808;&#39564;&#65292;&#24182;&#25191;&#34892;&#36817;&#20284;&#20989;&#25968;&#31354;&#38388;&#21464;&#20998;&#25512;&#26029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#20256;&#32479;&#20998;&#31867;&#39044;&#27979;&#35299;&#37322;&#20026;&#26469;&#33258;&#38544;&#24335;Dirichlet&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#36890;&#36807;&#35843;&#25972;&#25512;&#26029;&#65292;&#21487;&#20197;&#23558;&#30456;&#21516;&#30340;&#20989;&#25968;&#31354;&#38388;&#20808;&#39564;&#19982;&#19981;&#21516;&#30340;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#26550;&#26500;&#25110;&#22823;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#29609;&#20855;&#23454;&#39564;&#35828;&#26126;&#20102;&#36825;&#31181;&#20808;&#39564;&#30340;&#28789;&#27963;&#24615;&#21644;&#21151;&#25928;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#23637;&#31034;&#20102;&#21487;&#25193;&#23637;&#24615;&#12289;&#25913;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian deep learning approaches assume model parameters to be latent random variables and infer posterior distributions to quantify uncertainty, increase safety and trust, and prevent overconfident and unpredictable behavior. However, weight-space priors are model-specific, can be difficult to interpret and are hard to specify. Instead, we apply a Dirichlet prior in predictive space and perform approximate function-space variational inference. To this end, we interpret conventional categorical predictions from stochastic neural network classifiers as samples from an implicit Dirichlet distribution. By adapting the inference, the same function-space prior can be combined with different models without affecting model architecture or size. We illustrate the flexibility and efficacy of such a prior with toy experiments and demonstrate scalability, improved uncertainty quantification and adversarial robustness with large-scale image classification experiments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#35774;&#32622;&#30340;&#22312;&#32447;&#24211;&#23384;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#31639;&#27861;MaxCOSD&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#32771;&#34385;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38656;&#27714;&#21644;&#26377;&#29366;&#24577;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;</title><link>http://arxiv.org/abs/2307.06048</link><description>&lt;p&gt;
&#22312;&#32447;&#24211;&#23384;&#38382;&#39064;&#65306;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Online Inventory Problems: Beyond the i.i.d. Setting with Online Convex Optimization. (arXiv:2307.06048v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#35774;&#32622;&#30340;&#22312;&#32447;&#24211;&#23384;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#31639;&#27861;MaxCOSD&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#32771;&#34385;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38656;&#27714;&#21644;&#26377;&#29366;&#24577;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22810;&#20135;&#21697;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#32463;&#29702;&#26681;&#25454;&#37096;&#20998;&#21382;&#21490;&#20449;&#24687;&#20316;&#20986;&#39034;&#24207;&#34917;&#20805;&#20915;&#31574;&#65292;&#20197;&#26368;&#23567;&#21270;&#20854;&#32047;&#31215;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#32771;&#34385;&#21040;&#19968;&#33324;&#38656;&#27714;&#12289;&#25439;&#22833;&#21644;&#21160;&#24577;&#65292;&#36229;&#36234;&#36890;&#24120;&#20381;&#36182;&#26032;&#38395;&#20379;&#24212;&#21830;&#31867;&#22411;&#25439;&#22833;&#12289;&#22266;&#23450;&#21160;&#24577;&#21644;&#19981;&#29616;&#23454;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#38656;&#27714;&#20551;&#35774;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MaxCOSD&#65292;&#19968;&#20010;&#22312;&#32447;&#31639;&#27861;&#65292;&#21363;&#20351;&#23545;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38656;&#27714;&#21644;&#26377;&#29366;&#24577;&#21160;&#24577;&#65288;&#21253;&#25324;&#26131;&#33104;&#28866;&#29289;&#21697;&#65289;&#30340;&#38382;&#39064;&#65292;&#20063;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25105;&#20204;&#25152;&#31216;&#30340;&#38656;&#27714;&#36807;&#31243;&#30340;&#38750;&#36864;&#21270;&#24615;&#20551;&#35774;&#65292;&#24182;&#35748;&#20026;&#23427;&#20204;&#26159;&#20801;&#35768;&#23398;&#20064;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multi-product inventory control problems where a manager makes sequential replenishment decisions based on partial historical information in order to minimize its cumulative losses. Our motivation is to consider general demands, losses and dynamics to go beyond standard models which usually rely on newsvendor-type losses, fixed dynamics, and unrealistic i.i.d. demand assumptions. We propose MaxCOSD, an online algorithm that has provable guarantees even for problems with non-i.i.d. demands and stateful dynamics, including for instance perishability. We consider what we call non-degeneracy assumptions on the demand process, and argue that they are necessary to allow learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;OOD&#27979;&#35797;&#22810;&#22270;&#20013;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#39044;&#27979;&#27169;&#24335;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#30340;&#23646;&#24615;&#22810;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.06046</link><description>&lt;p&gt;
&#20511;&#21161;&#26032;&#30340;&#20851;&#31995;&#31867;&#22411;&#21644;&#33410;&#28857;&#65292;&#20197;OOD&#22810;&#20219;&#21153;&#35270;&#35282;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An OOD Multi-Task Perspective for Link Prediction with New Relation Types and Nodes. (arXiv:2307.06046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;OOD&#27979;&#35797;&#22810;&#22270;&#20013;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#39044;&#27979;&#27169;&#24335;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#30340;&#23646;&#24615;&#22810;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25512;&#26029;&#20855;&#26377;&#23646;&#24615;&#30340;&#22810;&#22270;&#20013;&#26032;&#27979;&#35797;&#22810;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32570;&#22833;&#23646;&#24615;&#38142;&#25509;&#65288;&#20851;&#31995;&#65289;&#12290;&#20256;&#32479;&#30340;&#20851;&#31995;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30528;&#23545;OOD&#27979;&#35797;&#22810;&#22270;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#22810;&#22270;&#21253;&#21547;&#20102;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#26032;&#33410;&#28857;&#21644;&#26032;&#20851;&#31995;&#31867;&#22411;&#12290;&#26368;&#36817;&#65292;&#39640;&#31561;&#20154;&#65288;2023&#65289;&#22312;&#25152;&#26377;&#20851;&#31995;&#31867;&#22411;&#20849;&#20139;&#30456;&#21516;&#32467;&#26500;&#39044;&#27979;&#27169;&#24335;&#65288;&#21333;&#20010;&#20219;&#21153;&#65289;&#30340;&#21807;&#19968;&#20551;&#35774;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#29702;&#35770;&#27010;&#24565;&#65288;&#29992;&#20110;&#33410;&#28857;&#21644;&#20851;&#31995;&#31867;&#22411;&#65289;&#26469;&#36827;&#34892;OOD&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#19982;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#35774;&#35745;&#30340;&#65288;&#21333;&#20010;&#65289;&#21487;&#20132;&#25442;&#24615;&#65288;&#20165;&#29992;&#20110;&#33410;&#28857;&#65289;&#30456;&#21453;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#20219;&#21153;&#21452;&#21487;&#20132;&#25442;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#23450;&#20041;&#20102;&#23646;&#24615;&#22810;&#22270;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#65292;&#36825;&#20123;&#22270;&#21487;&#33021;&#23545;&#19981;&#21516;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#20855;&#26377;&#19981;&#21516;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#39044;&#27979;&#27169;&#24335;&#65288;&#22810;&#20010;&#20219;&#21153;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inductive link prediction in (discrete) attributed multigraphs infers missing attributed links (relations) between nodes in new test multigraphs. Traditional relational learning methods face the challenge of limited generalization to OOD test multigraphs containing both novel nodes and novel relation types not seen in training. Recently, under the only assumption that all relation types share the same structural predictive patterns (single task), Gao et al. (2023) proposed an OOD link prediction method using the theoretical concept of double exchangeability (for nodes &amp; relation types), in contrast to the (single) exchangeability (only for nodes) used to design Graph Neural Networks (GNNs). In this work we further extend the double exchangeability concept to multi-task double exchangeability, where we define link prediction in attributed multigraphs that can have distinct and potentially conflicting predictive patterns for different sets of relation types (multiple tasks). 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Urhythmic&#30340;&#26080;&#30417;&#30563;&#33410;&#22863;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#28304;&#35821;&#38899;&#36827;&#34892;&#20998;&#21106;&#21644;&#26102;&#38388;&#25289;&#20280;&#65292;&#23454;&#29616;&#20102;&#22768;&#38899;&#36716;&#25442;&#20013;&#30340;&#33410;&#22863;&#21305;&#37197;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#36136;&#37327;&#21644;&#35821;&#35843;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06040</link><description>&lt;p&gt;
&#22768;&#38899;&#36716;&#25442;&#30340;&#33410;&#22863;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Rhythm Modeling for Voice Conversion. (arXiv:2307.06040v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Urhythmic&#30340;&#26080;&#30417;&#30563;&#33410;&#22863;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#28304;&#35821;&#38899;&#36827;&#34892;&#20998;&#21106;&#21644;&#26102;&#38388;&#25289;&#20280;&#65292;&#23454;&#29616;&#20102;&#22768;&#38899;&#36716;&#25442;&#20013;&#30340;&#33410;&#22863;&#21305;&#37197;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#36136;&#37327;&#21644;&#35821;&#35843;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#36716;&#25442;&#26088;&#22312;&#23558;&#28304;&#35821;&#38899;&#36716;&#25442;&#20026;&#19981;&#21516;&#30340;&#30446;&#26631;&#22768;&#38899;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;&#22768;&#38899;&#36716;&#25442;&#31995;&#32479;&#27809;&#26377;&#32771;&#34385;&#33410;&#22863;&#65292;&#32780;&#33410;&#22863;&#26159;&#23545;&#35828;&#35805;&#20154;&#36523;&#20221;&#24863;&#30693;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#33410;&#22863;&#36716;&#25442;&#26041;&#27861;Urhythmic&#65292;&#23427;&#19981;&#38656;&#35201;&#24179;&#34892;&#25968;&#25454;&#25110;&#25991;&#26412;&#36716;&#24405;&#12290;&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#28304;&#38899;&#39057;&#20998;&#21106;&#25104;&#36817;&#20284;&#40763;&#38899;&#12289;&#38556;&#38899;&#21644;&#38745;&#38899;&#30340;&#29255;&#27573;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#29255;&#27573;&#31867;&#22411;&#30340;&#35828;&#35805;&#36895;&#29575;&#25110;&#25345;&#32493;&#26102;&#38388;&#20998;&#24067;&#26469;&#24314;&#27169;&#33410;&#22863;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#35821;&#38899;&#29255;&#27573;&#36827;&#34892;&#26102;&#38388;&#25289;&#20280;&#26469;&#21305;&#37197;&#30446;&#26631;&#35828;&#35805;&#36895;&#29575;&#25110;&#33410;&#22863;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#36136;&#37327;&#21644;&#35821;&#35843;&#26041;&#38754;&#65292;Urhythmic&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice conversion aims to transform source speech into a different target voice. However, typical voice conversion systems do not account for rhythm, which is an important factor in the perception of speaker identity. To bridge this gap, we introduce Urhythmic-an unsupervised method for rhythm conversion that does not require parallel data or text transcriptions. Using self-supervised representations, we first divide source audio into segments approximating sonorants, obstruents, and silences. Then we model rhythm by estimating speaking rate or the duration distribution of each segment type. Finally, we match the target speaking rate or rhythm by time-stretching the speech segments. Experiments show that Urhythmic outperforms existing unsupervised methods in terms of quality and prosody. Code and checkpoints: https://github.com/bshall/urhythmic. Audio demo page: https://ubisoft-laforge.github.io/speech/urhythmic.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#31168;&#35299;&#37322;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#36755;&#20837;&#23454;&#20363;&#21644;&#20854;&#30456;&#24212;&#30340;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;GradCAM&#65289;&#27169;&#22411;&#35299;&#37322;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#29992;&#25143;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#35299;&#37322;&#24615;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#21644;&#36739;&#23567;&#30340;&#20998;&#31867;&#24615;&#33021;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2307.06026</link><description>&lt;p&gt;
&#20174;&#20248;&#31168;&#35299;&#37322;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Exemplary Explanations. (arXiv:2307.06026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#31168;&#35299;&#37322;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#36755;&#20837;&#23454;&#20363;&#21644;&#20854;&#30456;&#24212;&#30340;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;GradCAM&#65289;&#27169;&#22411;&#35299;&#37322;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#29992;&#25143;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#35299;&#37322;&#24615;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#21644;&#36739;&#23567;&#30340;&#20998;&#31867;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24335;&#23398;&#20064;&#65288;XBL&#65289;&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;IML&#65289;&#24418;&#24335;&#65292;&#36890;&#36807;&#29992;&#25143;&#21453;&#39304;&#25910;&#38598;&#30340;&#27169;&#22411;&#35299;&#37322;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#27169;&#22411;&#32454;&#21270;&#26041;&#27861;&#12290;&#23613;&#31649;XBL&#30340;&#20132;&#20114;&#24615;&#20419;&#36827;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#65292;&#20294;XBL&#38656;&#35201;&#22823;&#37327;&#30340;&#29992;&#25143;&#20132;&#20114;&#65292;&#24182;&#19988;&#22312;&#39640;&#25104;&#26412;&#39046;&#22495;&#65292;&#22914;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#30001;&#20110;&#21453;&#39304;&#20197;&#35814;&#32454;&#27880;&#37322;&#24418;&#24335;&#32780;&#38750;&#31616;&#21333;&#30340;&#31867;&#21035;&#26631;&#27880;&#65292;&#36825;&#31181;&#25104;&#26412;&#20250;&#21152;&#21095;&#12290;&#20026;&#20102;&#20943;&#23569;XBL&#30340;&#24037;&#20316;&#37327;&#21644;&#25104;&#26412;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20004;&#20010;&#36755;&#20837;&#23454;&#20363;&#21450;&#20854;&#30456;&#24212;&#30340;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;GradCAM&#65289;&#27169;&#22411;&#35299;&#37322;&#20316;&#20026;&#20248;&#31168;&#35299;&#37322;&#26469;&#23454;&#29616;XBL&#12290;&#36890;&#36807;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#26368;&#23569;&#30340;&#20154;&#24037;&#36755;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#35299;&#37322;&#65288;+0.02&#65292;+3%&#65289;&#65292;&#24182;&#22312;&#19982;&#20165;&#20351;&#29992;&#27169;&#22411;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#26102;&#65292;&#23454;&#29616;&#20102;&#20998;&#31867;&#24615;&#33021;&#30340;&#38477;&#20302;&#65288;-0.04&#65292;-4%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplanation Based Learning (XBL) is a form of Interactive Machine Learning (IML) that provides a model refining approach via user feedback collected on model explanations. Although the interactivity of XBL promotes model transparency, XBL requires a huge amount of user interaction and can become expensive as feedback is in the form of detailed annotation rather than simple category labelling which is more common in IML. This expense is exacerbated in high stakes domains such as medical image classification. To reduce the effort and expense of XBL we introduce a new approach that uses two input instances and their corresponding Gradient Weighted Class Activation Mapping (GradCAM) model explanations as exemplary explanations to implement XBL. Using a medical image classification task, we demonstrate that, using minimal human input, our approach produces improved explanations (+0.02, +3%) and achieves reduced classification performance (-0.04, -4%) when compared against a model trained wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#38750;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#26041;&#38754;&#19977;&#35270;&#22270;&#26631;&#31614;&#20256;&#25773;&#12289;&#31232;&#30095;&#30456;&#20284;&#24230;&#12289;Sinkhorn&#31639;&#23376;&#21644;&#26102;&#24577;&#36845;&#20195;&#23398;&#20064;&#26469;&#25552;&#39640;&#23545;&#40784;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#27169;&#22411;&#30340;&#26102;&#38388;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2307.06013</link><description>&lt;p&gt;
&#36890;&#36807;&#20004;&#26041;&#38754;&#19977;&#35270;&#22270;&#26631;&#31614;&#20256;&#25773;&#30340;&#39640;&#25928;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Effective and Efficient Time-aware Entity Alignment Framework via Two-aspect Three-view Label Propagation. (arXiv:2307.06013v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#38750;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#26041;&#38754;&#19977;&#35270;&#22270;&#26631;&#31614;&#20256;&#25773;&#12289;&#31232;&#30095;&#30456;&#20284;&#24230;&#12289;Sinkhorn&#31639;&#23376;&#21644;&#26102;&#24577;&#36845;&#20195;&#23398;&#20064;&#26469;&#25552;&#39640;&#23545;&#40784;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#27169;&#22411;&#30340;&#26102;&#38388;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#26088;&#22312;&#23547;&#25214;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#31561;&#20215;&#23454;&#20307;&#23545;&#65292;&#36825;&#23545;&#20110;&#25512;&#21160;&#30693;&#35782;&#34701;&#21512;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#20986;&#29616;&#20197;&#22686;&#24378;&#23545;&#40784;&#25928;&#26524;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;GNN&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#24456;&#38590;&#23558;&#20854;&#24212;&#29992;&#21040;&#22823;&#35268;&#27169;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#38750;&#31070;&#32463;&#32593;&#32476;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;LightTEA&#65292;&#23427;&#30001;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;(1)&#20004;&#26041;&#38754;&#19977;&#35270;&#22270;&#26631;&#31614;&#20256;&#25773;&#65292;(2)&#24102;&#26377;&#26102;&#24577;&#32422;&#26463;&#30340;&#31232;&#30095;&#30456;&#20284;&#24230;&#65292;(3)Sinkhorn&#31639;&#23376;&#65292;&#20197;&#21450;(4)&#26102;&#24577;&#36845;&#20195;&#23398;&#20064;&#12290;&#25152;&#26377;&#36825;&#20123;&#27169;&#22359;&#20849;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#23454;&#20307;&#23545;&#40784;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#26102;&#38388;&#24320;&#38144;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA) aims to find the equivalent entity pairs between different knowledge graphs (KGs), which is crucial to promote knowledge fusion. With the wide use of temporal knowledge graphs (TKGs), time-aware EA (TEA) methods appear to enhance EA. Existing TEA models are based on Graph Neural Networks (GNN) and achieve state-of-the-art (SOTA) performance, but it is difficult to transfer them to large-scale TKGs due to the scalability issue of GNN. In this paper, we propose an effective and efficient non-neural EA framework between TKGs, namely LightTEA, which consists of four essential components: (1) Two-aspect Three-view Label Propagation, (2) Sparse Similarity with Temporal Constraints, (3) Sinkhorn Operator, and (4) Temporal Iterative Learning. All of these modules work together to improve the performance of EA while reducing the time consumption of the model. Extensive experiments on public datasets indicate that our proposed model significantly outperforms the SOTA method
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#39044;&#35757;&#32451;&#30340;Vision Transformers&#21644;Finetuned&#29256;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#24341;&#20837;&#20102;&#21487;&#36716;&#31227;&#30340;&#19981;&#21464;&#24615;&#65292;&#22312;Finetuning&#36807;&#31243;&#20013;&#65292;&#28145;&#23618;&#30340;&#19981;&#21464;&#24615;&#21521;&#27973;&#23618;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2307.06006</link><description>&lt;p&gt;
Vision Transformers&#30340;Finetuning&#36807;&#31243;&#20013;&#21457;&#29983;&#20102;&#20160;&#20040;&#65306;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation. (arXiv:2307.06006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#39044;&#35757;&#32451;&#30340;Vision Transformers&#21644;Finetuned&#29256;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#24341;&#20837;&#20102;&#21487;&#36716;&#31227;&#30340;&#19981;&#21464;&#24615;&#65292;&#22312;Finetuning&#36807;&#31243;&#20013;&#65292;&#28145;&#23618;&#30340;&#19981;&#21464;&#24615;&#21521;&#27973;&#23618;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#30740;&#31350;&#39044;&#35757;&#32451;&#30340;Vision Transformers&#21450;&#20854;&#23545;&#24212;&#30340;Finetuned&#29256;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#26032;&#30340;&#25351;&#26631;&#65292;&#24182;&#29305;&#21035;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#30340;&#19981;&#21464;&#24615;&#22312;Finetuning&#36807;&#31243;&#20013;&#30340;&#20445;&#30041;&#31243;&#24230;&#12290;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#31995;&#21015;&#23454;&#35777;&#21457;&#29616;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#22312;&#27973;&#23618;&#20013;&#24341;&#20837;&#20102;&#21487;&#36716;&#31227;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#19988;&#22312;Finetuning&#36807;&#31243;&#20013;&#65292;&#28145;&#23618;&#30340;&#19981;&#21464;&#24615;&#21521;&#27973;&#23618;&#21387;&#32553;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pretrain-finetune paradigm usually improves downstream performance over training a model from scratch on the same task, becoming commonplace across many areas of machine learning. While pretraining is empirically observed to be beneficial for a range of tasks, there is not a clear understanding yet of the reasons for this effect. In this work, we examine the relationship between pretrained vision transformers and the corresponding finetuned versions on several benchmark datasets and tasks. We present new metrics that specifically investigate the degree to which invariances learned by a pretrained model are retained or forgotten during finetuning. Using these metrics, we present a suite of empirical findings, including that pretraining induces transferable invariances in shallow layers and that invariances from deeper pretrained layers are compressed towards shallower layers during finetuning. Together, these findings contribute to understanding some of the reasons for the successes
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDNAS&#30340;&#31163;&#25955;&#21270;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26494;&#24347;&#30340;&#26550;&#26500;&#34920;&#31034;&#21644;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#31163;&#25955;&#21270;&#23618;&#65292;DDNAS&#22312;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06005</link><description>&lt;p&gt;
DDNAS: &#31163;&#25955;&#21270;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification. (arXiv:2307.06005v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06005
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDNAS&#30340;&#31163;&#25955;&#21270;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26494;&#24347;&#30340;&#26550;&#26500;&#34920;&#31034;&#21644;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#31163;&#25955;&#21270;&#23618;&#65292;DDNAS&#22312;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#22312;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;NAS&#26082;&#26410;&#23545;&#26550;&#26500;&#36827;&#34892;&#21487;&#23398;&#20064;&#30340;&#34701;&#21512;&#20197;&#20248;&#21270;&#65292;&#20063;&#26410;&#23545;&#25991;&#26412;&#36755;&#20837;&#32972;&#21518;&#30340;&#28508;&#22312;&#23618;&#32423;&#20998;&#31867;&#36827;&#34892;&#32534;&#30721;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#26041;&#27861;&#65292;&#21363;Discretized Differentiable Neural Architecture Search (DDNAS)&#65292;&#29992;&#20110;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#12290;&#36890;&#36807;&#26550;&#26500;&#34920;&#31034;&#30340;&#36830;&#32493;&#26494;&#24347;&#65292;DDNAS&#21487;&#20197;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26469;&#36827;&#34892;&#25628;&#32034;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#21270;&#23618;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#23558;&#20854;&#26045;&#21152;&#20110;&#27599;&#20010;&#25628;&#32034;&#33410;&#28857;&#19978;&#65292;&#20197;&#23545;&#25991;&#26412;&#34920;&#31034;&#20013;&#30340;&#28508;&#22312;&#23618;&#32423;&#20998;&#31867;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;DDNAS&#22987;&#32456;&#33021;&#22815;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;NAS&#26041;&#27861;&#12290;&#23613;&#31649;DDNAS&#20165;&#20381;&#36182;&#20110;&#21367;&#31215;&#65292;&#27744;&#21270;&#21644;&#26080;&#25805;&#20316;&#36825;&#19977;&#20010;&#22522;&#26412;&#25805;&#20316;&#65292;&#20316;&#20026;&#20505;&#36873;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has shown promising capability in learning text representation. However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture, nor encodes the latent hierarchical categorization behind text input. This paper presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification. With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search. We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation. Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods. While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the cand
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#33258;&#21160;&#25968;&#25454;&#26631;&#27880;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#26041;&#27861;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#26631;&#27880;&#30340;&#32791;&#26102;&#21644;&#32321;&#29712;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05988</link><description>&lt;p&gt;
&#12298;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#33258;&#21160;&#25968;&#25454;&#26631;&#27880;&#25216;&#26415;&#30340;&#32508;&#21512;&#32508;&#36848;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Automated Data Annotation Techniques in Human Activity Recognition. (arXiv:2307.05988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05988
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#33258;&#21160;&#25968;&#25454;&#26631;&#27880;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#26041;&#27861;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#26631;&#27880;&#30340;&#32791;&#26102;&#21644;&#32321;&#29712;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#24050;&#25104;&#20026;&#36807;&#21435;&#21313;&#24180;&#20013;&#39046;&#20808;&#30340;&#30740;&#31350;&#35838;&#39064;&#20043;&#19968;&#12290;&#38543;&#30528;&#20256;&#24863;&#25216;&#26415;&#30340;&#25104;&#29087;&#21644;&#32463;&#27982;&#25104;&#26412;&#30340;&#38477;&#20302;&#65292;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#24212;&#29992;&#22312;&#21307;&#30103;&#12289;&#24037;&#19994;&#12289;&#20307;&#32946;&#21644;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#31561;&#39046;&#22495;&#21464;&#24471;&#27969;&#34892;&#12290;HAR&#31995;&#32479;&#30340;&#35774;&#35745;&#38656;&#35201;&#19981;&#21516;&#32791;&#26102;&#30340;&#22788;&#29702;&#27493;&#39588;&#65292;&#22914;&#25968;&#25454;&#37319;&#38598;&#12289;&#26631;&#27880;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;&#29305;&#21035;&#26159;&#25968;&#25454;&#26631;&#27880;&#26159;HAR&#20013;&#26368;&#36153;&#26102;&#21644;&#32321;&#29712;&#30340;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#32773;&#36827;&#34892;&#22823;&#37327;&#21644;&#35814;&#32454;&#30340;&#25163;&#24037;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#24050;&#25552;&#20986;&#20102;&#20851;&#20110;HAR&#20013;&#27880;&#35299;&#36807;&#31243;&#33258;&#21160;&#21270;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#27880;&#35299;&#38382;&#39064;&#22312;&#19981;&#21516;&#30340;&#27010;&#24565;&#21644;&#22330;&#26223;&#20013;&#37117;&#23384;&#22312;&#65292;&#37117;&#38656;&#35201;&#20010;&#21035;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;HAR&#25968;&#25454;&#26631;&#27880;&#25216;&#26415;&#30340;&#39318;&#20010;&#31995;&#32479;&#32508;&#36848;&#12290;&#36890;&#36807;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#32452;&#24182;&#25552;&#20379;&#31246;onomr&#65292; &#187;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR) has become one of the leading research topics of the last decade. As sensing technologies have matured and their economic costs have declined, a host of novel applications, e.g., in healthcare, industry, sports, and daily life activities have become popular. The design of HAR systems requires different time-consuming processing steps, such as data collection, annotation, and model training and optimization. In particular, data annotation represents the most labor-intensive and cumbersome step in HAR, since it requires extensive and detailed manual work from human annotators. Therefore, different methodologies concerning the automation of the annotation procedure in HAR have been proposed. The annotation problem occurs in different notions and scenarios, which all require individual solutions. In this paper, we provide the first systematic review on data annotation techniques for HAR. By grouping existing approaches into classes and providing a taxonomy,
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#12290;Transformer&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19978;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#21253;&#25324;&#19981;&#31283;&#23450;&#35757;&#32451;&#12289;&#20449;&#29992;&#20998;&#37197;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#31561;&#26041;&#38754;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20854;&#22312;&#34920;&#31034;&#23398;&#20064;&#12289;&#29366;&#24577;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#24314;&#27169;&#20197;&#21450;&#31574;&#30053;&#20248;&#21270;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20123;&#26368;&#26032;&#30340;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.05979</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transformers in Reinforcement Learning: A Survey. (arXiv:2307.05979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#12290;Transformer&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19978;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#21253;&#25324;&#19981;&#31283;&#23450;&#35757;&#32451;&#12289;&#20449;&#29992;&#20998;&#37197;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#31561;&#26041;&#38754;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20854;&#22312;&#34920;&#31034;&#23398;&#20064;&#12289;&#29366;&#24577;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#24314;&#27169;&#20197;&#21450;&#31574;&#30053;&#20248;&#21270;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20123;&#26368;&#26032;&#30340;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#24433;&#21709;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#23427;&#34987;&#35270;&#20026;&#35299;&#20915;&#19981;&#31283;&#23450;&#35757;&#32451;&#12289;&#20449;&#29992;&#20998;&#37197;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#31561;&#25361;&#25112;&#30340;&#26377;&#24076;&#26395;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;RL&#39046;&#22495;&#30340;&#27010;&#36848;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#20256;&#32479;RL&#31639;&#27861;&#30340;&#25361;&#25112;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer&#21450;&#20854;&#21464;&#20307;&#30340;&#29305;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#20854;&#36866;&#21512;&#24212;&#23545;RL&#20013;&#22266;&#26377;&#25361;&#25112;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;Transformer&#22312;RL&#20013;&#21508;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#34920;&#31034;&#23398;&#20064;&#12289;&#29366;&#24577;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#24314;&#27169;&#20197;&#21450;&#31574;&#30053;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#25552;&#39640;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks. This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability and efficiency of transformers i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SDD&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23433;&#20840;&#33258;&#33976;&#39311;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#23548;&#22122;&#22768;&#20272;&#35745;&#19982;&#26080;&#26465;&#20214;&#27169;&#22411;&#21305;&#37197;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#28040;&#38500;&#26377;&#23475;&#20869;&#23481;&#30340;&#27604;&#20363;&#26356;&#22823;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#19988;&#20801;&#35768;&#19968;&#27425;&#31227;&#38500;&#22810;&#20010;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.05977</link><description>&lt;p&gt;
&#23454;&#29616;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23433;&#20840;&#33258;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models. (arXiv:2307.05977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SDD&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23433;&#20840;&#33258;&#33976;&#39311;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#23548;&#22122;&#22768;&#20272;&#35745;&#19982;&#26080;&#26465;&#20214;&#27169;&#22411;&#21305;&#37197;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#28040;&#38500;&#26377;&#23475;&#20869;&#23481;&#30340;&#27604;&#20363;&#26356;&#22823;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#19988;&#20801;&#35768;&#19968;&#27425;&#31227;&#38500;&#22810;&#20010;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20511;&#21161;&#20114;&#32852;&#32593;&#19978;&#20016;&#23500;&#30340;&#25968;&#25454;&#20855;&#26377;&#21331;&#36234;&#30340;&#36136;&#37327;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#31038;&#20250;&#20851;&#20999;&#65292;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20559;&#35265;&#21644;&#26377;&#23475;&#24615;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#65292;&#24182;&#19988;&#24456;&#38590;&#23436;&#20840;&#28040;&#38500;&#65292;&#36825;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SDD&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#38450;&#27490;&#38382;&#39064;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#33976;&#39311;&#25193;&#25955;&#27169;&#22411;&#26469;&#24341;&#23548;&#22522;&#20110;&#30446;&#26631;&#31227;&#38500;&#27010;&#24565;&#30340;&#22122;&#22768;&#20272;&#35745;&#19982;&#26080;&#26465;&#20214;&#27169;&#22411;&#21305;&#37197;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#26356;&#22823;&#27604;&#20363;&#30340;&#26377;&#23475;&#20869;&#23481;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20801;&#35768;&#19968;&#27425;&#31227;&#38500;&#22810;&#20010;&#27010;&#24565;&#65292;&#32780;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#33021;&#19968;&#27425;&#31227;&#38500;&#19968;&#20010;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale image generation models, with impressive quality made possible by the vast amount of data available on the Internet, raise social concerns that these models may generate harmful or copyrighted content. The biases and harmfulness arise throughout the entire training process and are hard to completely remove, which have become significant hurdles to the safe deployment of these models. In this paper, we propose a method called SDD to prevent problematic content generation in text-to-image diffusion models. We self-distill the diffusion model to guide the noise estimate conditioned on the target removal concept to match the unconditional one. Compared to the previous methods, our method eliminates a much greater proportion of harmful content from the generated images without degrading the overall image quality. Furthermore, our method allows the removal of multiple concepts at once, whereas previous works are limited to removing a single concept at a time.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22238;&#24402;&#20013;&#35299;&#20915;&#24322;&#24120;&#20540;&#26816;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20108;&#27425;&#38181;&#26494;&#24347;&#24418;&#24335;&#32780;&#19981;&#26159;&#20351;&#29992;big-M&#32422;&#26463;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;big-M&#20844;&#24335;&#24555;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2307.05975</link><description>&lt;p&gt;
&#22238;&#24402;&#20013;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#65306;&#38181;&#20108;&#27425;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
Outlier detection in regression: conic quadratic formulations. (arXiv:2307.05975v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22238;&#24402;&#20013;&#35299;&#20915;&#24322;&#24120;&#20540;&#26816;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20108;&#27425;&#38181;&#26494;&#24347;&#24418;&#24335;&#32780;&#19981;&#26159;&#20351;&#29992;big-M&#32422;&#26463;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;big-M&#20844;&#24335;&#24555;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#24403;&#26500;&#24314;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26102;&#65292;&#32771;&#34385;&#21040;&#24322;&#24120;&#20540;&#30340;&#23384;&#22312;&#8212;&#8212;&#21363;&#21463;&#25439;&#30340;&#36755;&#20837;&#25968;&#25454;&#28857;&#65292;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#36825;&#31867;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#26469;&#34920;&#36798;&#65292;&#20854;&#20013;&#27599;&#20010;&#38382;&#39064;&#30001;&#20108;&#36827;&#21046;&#21464;&#37327;&#21644;&#36830;&#32493;&#21464;&#37327;&#30340;&#20108;&#27425;&#39033;&#30340;&#20056;&#31215;&#32473;&#20986;&#65292;&#24418;&#25104;&#19977;&#27425;&#39033;&#12290;&#29616;&#26377;&#30340;&#25991;&#29486;&#26041;&#27861;&#36890;&#24120;&#20381;&#38752;&#20351;&#29992;big-M&#32422;&#26463;&#32447;&#24615;&#21270;&#19977;&#27425;&#39033;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#24369;&#25918;&#26494;&#21644;&#24615;&#33021;&#24046;&#30340;&#32570;&#28857;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19981;&#28041;&#21450;big-M&#32422;&#26463;&#30340;&#20108;&#27425;&#38181;&#26494;&#24347;&#24418;&#24335;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#35758;&#30340;&#20844;&#24335;&#27604;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;big-M&#20844;&#24335;&#24555;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications, when building linear regression models, it is important to account for the presence of outliers, i.e., corrupted input data points. Such problems can be formulated as mixed-integer optimization problems involving cubic terms, each given by the product of a binary variable and a quadratic term of the continuous variables. Existing approaches in the literature, typically relying on the linearization of the cubic terms using big-M constraints, suffer from weak relaxation and poor performance in practice. In this work we derive stronger second-order conic relaxations that do not involve big-M constraints. Our computational experiments indicate that the proposed formulations are several orders-of-magnitude faster than existing big-M formulations in the literature for this problem.
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#36716;&#21270;&#29575;&#39044;&#27979;&#30340;&#26694;&#26550;(CL4CVR)&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#26356;&#22909;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#36716;&#21270;&#29575;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05974</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#36716;&#21270;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for Conversion Rate Prediction. (arXiv:2307.05974v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05974
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#36716;&#21270;&#29575;&#39044;&#27979;&#30340;&#26694;&#26550;(CL4CVR)&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#26356;&#22909;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#36716;&#21270;&#29575;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#39044;&#27979;&#22312;&#24191;&#21578;&#31995;&#32479;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#22312;CVR&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#36739;&#39640;&#12290;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#65292;&#34429;&#28982;&#23384;&#22312;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#24191;&#21578;&#65292;&#20294;&#29992;&#25143;&#24448;&#24448;&#21482;&#28857;&#20987;&#20854;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24182;&#22312;&#20854;&#20013;&#30340;&#26356;&#23567;&#37096;&#20998;&#36827;&#34892;&#36716;&#21270;&#12290;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#38480;&#21046;&#20102;&#36825;&#20123;&#28145;&#24230;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;CVR&#39044;&#27979;&#65288;CL4CVR&#65289;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;&#30417;&#30563;CVR&#39044;&#27979;&#20219;&#21153;&#19982;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20851;&#32852;&#36215;&#26469;&#65292;&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#26356;&#22909;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;CVR&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#24212;&#29992;&#20110;CVR&#39044;&#27979;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23884;&#20837;&#24335;&#25513;&#30721;&#65288;EM&#65289;&#65292;&#32780;&#19981;&#26159;&#29305;&#24449;&#25513;&#30721;&#65292;&#26469;&#21019;&#24314;&#20004;&#20010;&#22686;&#24378;&#26679;&#26412;&#35270;&#22270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#38452;&#24615;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Conversion rate (CVR) prediction plays an important role in advertising systems. Recently, supervised deep neural network-based models have shown promising performance in CVR prediction. However, they are data hungry and require an enormous amount of training data. In online advertising systems, although there are millions to billions of ads, users tend to click only a small set of them and to convert on an even smaller set. This data sparsity issue restricts the power of these deep models. In this paper, we propose the Contrastive Learning for CVR prediction (CL4CVR) framework. It associates the supervised CVR prediction task with a contrastive learning task, which can learn better data representations exploiting abundant unlabeled data and improve the CVR prediction performance. To tailor the contrastive learning task to the CVR prediction problem, we propose embedding masking (EM), rather than feature masking, to create two views of augmented samples. We also propose a false negativ
&lt;/p&gt;</description></item><item><title>VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05973</link><description>&lt;p&gt;
VoxPoser: &#29992;&#20110;&#24102;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#32452;&#21512;&#30340;3D&#20215;&#20540;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05973
&lt;/p&gt;
&lt;p&gt;
VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#21487;&#34892;&#21160;&#30693;&#35782;&#65292;&#21487;&#20197;&#20197;&#25512;&#29702;&#21644;&#35268;&#21010;&#30340;&#24418;&#24335;&#25552;&#21462;&#20986;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#36816;&#21160;&#21407;&#35821;&#26469;&#25191;&#34892;&#19982;&#29615;&#22659;&#30340;&#29289;&#29702;&#20132;&#20114;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#24320;&#38598;&#25351;&#20196;&#21644;&#24320;&#38598;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#65292;&#21363;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;6-DoF&#26411;&#31471;&#25191;&#34892;&#22120;&#36335;&#24452;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;LLMs&#22312;&#32473;&#23450;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25351;&#20196;&#26102;&#25797;&#38271;&#25512;&#26029;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#20195;&#30721;&#32534;&#20889;&#33021;&#21147;&#65292;&#23427;&#20204;&#21487;&#20197;&#19982;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20132;&#20114;&#65292;&#20197;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#23558;&#30693;&#35782;&#25509;&#22320;&#21040;Agent&#30340;&#35266;&#27979;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#26694;&#26550;&#20013;&#20351;&#29992;&#32452;&#21512;&#30340;&#20215;&#20540;&#26144;&#23556;&#26469;&#38646;&#35797;&#21512;&#25104;&#38381;&#29615;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop ro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20107;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#23545;Transformer&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#23398;&#20064;&#37327;&#21270;&#65288;SDQ&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#32047;&#31215;&#37327;&#21270;&#35823;&#24046;&#24182;&#22312;&#22810;&#35821;&#35328;&#27169;&#22411;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;XGLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SDQ&#26041;&#27861;&#21487;&#20197;&#23558;&#27169;&#22411;&#20174;32&#20301;&#28014;&#28857;&#26435;&#37325;&#20943;&#23569;&#21040;8&#20301;&#25972;&#25968;&#26435;&#37325;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05972</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#37327;&#21270;&#65306;&#22312;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;
&lt;/p&gt;
&lt;p&gt;
Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models. (arXiv:2307.05972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20107;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#23545;Transformer&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#23398;&#20064;&#37327;&#21270;&#65288;SDQ&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#32047;&#31215;&#37327;&#21270;&#35823;&#24046;&#24182;&#22312;&#22810;&#35821;&#35328;&#27169;&#22411;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;XGLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SDQ&#26041;&#27861;&#21487;&#20197;&#23558;&#27169;&#22411;&#20174;32&#20301;&#28014;&#28857;&#26435;&#37325;&#20943;&#23569;&#21040;8&#20301;&#25972;&#25968;&#26435;&#37325;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20107;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#23545;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#23398;&#20064;&#37327;&#21270;&#65288;SDQ&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#32047;&#31215;&#37327;&#21270;&#35823;&#24046;&#24182;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;SDQ&#24212;&#29992;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;XLM-R-Base&#21644;InfoXLM-Base&#65292;&#24182;&#35777;&#26126;&#36825;&#20004;&#20010;&#27169;&#22411;&#21487;&#20197;&#20174;32&#20301;&#28014;&#28857;&#26435;&#37325;&#20943;&#23569;&#21040;8&#20301;&#25972;&#25968;&#26435;&#37325;&#65292;&#21516;&#26102;&#22312;XGLUE&#22522;&#20934;&#19978;&#20445;&#25345;&#39640;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#20984;&#26174;&#20102;&#37327;&#21270;&#22810;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#32463;&#24494;&#35843;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the effects of post-training quantization and quantization-aware training on the generalization of Transformer language models. We present a new method called self-distilled quantization (SDQ) that minimizes accumulative quantization errors and outperforms baselines. We apply SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that both models can be reduced from 32-bit floating point weights to 8-bit integer weights while maintaining a high level of performance on the XGLUE benchmark. Our results also highlight the challenges of quantizing multilingual models, which must generalize to languages they were not fine-tuned on.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#35270;&#39057;&#28436;&#31034;&#22686;&#24378;&#20102;&#30524;&#25163;&#21327;&#21516;&#30340;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#25110;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.05959</link><description>&lt;p&gt;
&#32473;&#26426;&#22120;&#20154;&#20197;&#24110;&#21161;&#65306;&#36890;&#36807;&#30524;&#25163;&#21327;&#21516;&#30340;&#20154;&#31867;&#35270;&#39057;&#28436;&#31034;&#23398;&#20064;&#36890;&#29992;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations. (arXiv:2307.05959v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05959
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#35270;&#39057;&#28436;&#31034;&#22686;&#24378;&#20102;&#30524;&#25163;&#21327;&#21516;&#30340;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#25110;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#25163;&#21327;&#21516;&#25668;&#20687;&#22836;&#22312;&#35270;&#35273;&#23548;&#21521;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#26174;&#31034;&#20986;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#20154;&#27169;&#20223;&#20013;&#65292;&#35753;&#20154;&#31867;&#36828;&#31243;&#25805;&#20316;&#21592;&#25910;&#38598;&#22823;&#37327;&#19987;&#23478;&#28436;&#31034;&#23545;&#20110;&#30495;&#23454;&#26426;&#22120;&#20154;&#26469;&#35828;&#20173;&#28982;&#24456;&#26114;&#36149;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20154;&#31867;&#36827;&#34892;&#20219;&#21153;&#30340;&#35270;&#39057;&#35201;&#20415;&#23452;&#24471;&#22810;&#65292;&#22240;&#20026;&#23427;&#20204;&#28040;&#38500;&#20102;&#23545;&#26426;&#22120;&#20154;&#36965;&#25805;&#20316;&#19987;&#19994;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#24555;&#36895;&#25429;&#25417;&#12290;&#22240;&#27492;&#65292;&#20154;&#31867;&#35270;&#39057;&#28436;&#31034;&#26159;&#23398;&#20064;&#22823;&#35268;&#27169;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#30340;&#26080;&#26631;&#31614;&#20154;&#31867;&#35270;&#39057;&#28436;&#31034;&#26469;&#22686;&#24378;&#29421;&#31364;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#30524;&#25163;&#21327;&#21516;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#35270;&#35273;&#39046;&#22495;&#24046;&#36317;&#65292;&#20294;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#26126;&#30830;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#22240;&#20026;&#25105;&#20204;&#21033;&#29992;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye-in-hand cameras have shown promise in enabling greater sample efficiency and generalization in vision-based robotic manipulation. However, for robotic imitation, it is still expensive to have a human teleoperator collect large amounts of expert demonstrations with a real robot. Videos of humans performing tasks, on the other hand, are much cheaper to collect since they eliminate the need for expertise in robotic teleoperation and can be quickly captured in a wide range of scenarios. Therefore, human video demonstrations are a promising data source for learning generalizable robotic manipulation policies at scale. In this work, we augment narrow robotic imitation datasets with broad unlabeled human video demonstrations to greatly enhance the generalization of eye-in-hand visuomotor policies. Although a clear visual domain gap exists between human and robot data, our framework does not need to employ any explicit domain adaptation method, as we leverage the partial observability of e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Newell&#29702;&#35770;&#30340;&#29305;&#24449;&#36716;&#25442;&#26041;&#27861;&#29992;&#20110;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#65292;&#29992;&#20110;&#25913;&#21892;&#27169;&#22411;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#36801;&#31227;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05949</link><description>&lt;p&gt;
&#22522;&#20110;Newell&#29702;&#35770;&#30340;&#29305;&#24449;&#36716;&#25442;&#29992;&#20110;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Newell's theory based feature transformations for spatio-temporal traffic prediction. (arXiv:2307.05949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Newell&#29702;&#35770;&#30340;&#29305;&#24449;&#36716;&#25442;&#26041;&#27861;&#29992;&#20110;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#65292;&#29992;&#20110;&#25913;&#21892;&#27169;&#22411;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#36801;&#31227;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#31354;&#20132;&#36890;&#27969;&#39044;&#27979;&#20013;&#20351;&#29992;&#21367;&#31215;&#25110;&#22270;&#21367;&#31215;&#36807;&#28388;&#22120;&#20197;&#21450;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#20123;&#27169;&#22411;, &#22914;CNN-LSTM, &#21033;&#29992;&#37051;&#36817;&#26816;&#27979;&#31449;&#30340;&#20132;&#36890;&#27969;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#30340;&#27969;&#37327;&#12290;&#28982;&#32780;, &#36825;&#20123;&#27169;&#22411;&#22312;&#25429;&#25417;&#20132;&#36890;&#31995;&#32479;&#30340;&#26356;&#24191;&#27867;&#21160;&#24577;&#26041;&#38754;&#20855;&#26377;&#23616;&#38480;&#24615;, &#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#23398;&#20064;&#29305;&#23450;&#20110;&#26816;&#27979;&#37197;&#32622;&#21644;&#30446;&#26631;&#20301;&#32622;&#20132;&#36890;&#29305;&#24449;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;, &#24403;&#22312;&#26032;&#30340;&#20301;&#32622;&#32570;&#23569;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#26102;, &#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20132;&#36890;&#27969;&#29289;&#29702;&#23398;&#30340;&#29305;&#24449;&#36716;&#25442;&#26041;&#27861;&#29992;&#20110;&#26102;&#31354;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models for spatio-temporal traffic flow forecasting employ convolutional or graph-convolutional filters along with recurrent neural networks to capture spatial and temporal dependencies in traffic data. These models, such as CNN-LSTM, utilize traffic flows from neighboring detector stations to predict flows at a specific location of interest. However, these models are limited in their ability to capture the broader dynamics of the traffic system, as they primarily learn features specific to the detector configuration and traffic characteristics at the target location. Hence, the transferability of these models to different locations becomes challenging, particularly when data is unavailable at the new location for model training. To address this limitation, we propose a traffic flow physics-based feature transformation for spatio-temporal DL models. This transformation incorporates Newell's uncongested and congested-state estimators of traffic flows at the target loc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#20551;&#35774;&#36866;&#24212;&#38382;&#39064;&#30340;&#22686;&#24378;&#22810;&#26679;&#24615;&#29983;&#25104;&#32593;&#32476;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29983;&#25104;&#25968;&#25454;&#30340;&#35821;&#20041;&#29305;&#24449;&#20043;&#38388;&#30340;HSIC&#20540;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05948</link><description>&lt;p&gt;
&#20026;&#23569;&#26679;&#26412;&#20551;&#35774;&#36866;&#24212;&#38382;&#39064;&#22686;&#24378;&#22810;&#26679;&#24615;&#30340;&#29983;&#25104;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Diversity-enhancing Generative Network for Few-shot Hypothesis Adaptation. (arXiv:2307.05948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05948
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#20551;&#35774;&#36866;&#24212;&#38382;&#39064;&#30340;&#22686;&#24378;&#22810;&#26679;&#24615;&#29983;&#25104;&#32593;&#32476;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29983;&#25104;&#25968;&#25454;&#30340;&#35821;&#20041;&#29305;&#24449;&#20043;&#38388;&#30340;HSIC&#20540;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29983;&#25104;&#26080;&#26631;&#31614;&#25968;&#25454;&#26377;&#21161;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#20551;&#35774;&#36866;&#24212;&#65288;FHA&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#26088;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30446;&#26631;&#22495;&#25968;&#25454;&#21644;&#32463;&#36807;&#35757;&#32451;&#30340;&#28304;&#22495;&#20998;&#31867;&#22120;&#65288;&#21363;&#28304;&#20551;&#35774;&#65289;&#20026;&#30446;&#26631;&#22495;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#20197;&#33719;&#21462;&#39640;&#24230;&#20860;&#23481;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#38750;&#24120;&#30456;&#20284;&#29978;&#33267;&#30456;&#21516;&#12290;&#29983;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#24378;&#20381;&#36182;&#24615;&#23558;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22810;&#26679;&#24615;&#30340;&#29983;&#25104;&#32593;&#32476;&#65288;DEG-Net&#65289;&#26469;&#35299;&#20915;FHA&#38382;&#39064;&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26680;&#29420;&#31435;&#24615;&#24230;&#37327;&#8212;&#8212;&#24076;&#23572;&#20271;&#29305;-&#26045;&#23494;&#29305;&#29420;&#31435;&#24615;&#20934;&#21017;&#65288;HSIC&#65289;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DEG-Net&#23558;&#36890;&#36807;&#26368;&#23567;&#21270;&#29983;&#25104;&#25968;&#25454;&#30340;&#35821;&#20041;&#29305;&#24449;&#20043;&#38388;&#30340;HSIC&#20540;&#65288;&#21363;&#26368;&#22823;&#21270;&#29420;&#31435;&#24615;&#65289;&#26469;&#29983;&#25104;&#25968;&#25454;&#12290;&#36890;&#36807;DEG-Net&#65292;&#29983;&#25104;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating unlabeled data has been recently shown to help address the few-shot hypothesis adaptation (FHA) problem, where we aim to train a classifier for the target domain with a few labeled target-domain data and a well-trained source-domain classifier (i.e., a source hypothesis), for the additional information of the highly-compatible unlabeled data. However, the generated data of the existing methods are extremely similar or even the same. The strong dependency among the generated data will lead the learning to fail. In this paper, we propose a diversity-enhancing generative network (DEG-Net) for the FHA problem, which can generate diverse unlabeled data with the help of a kernel independence measure: the Hilbert-Schmidt independence criterion (HSIC). Specifically, DEG-Net will generate data via minimizing the HSIC value (i.e., maximizing the independence) among the semantic features of the generated data. By DEG-Net, the generated unlabeled data are more diverse and more effective
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#23454;&#29616;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05946</link><description>&lt;p&gt;
&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#37327;&#21270;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#23454;&#29616;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25968;&#25454;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#22810;&#23618;&#26550;&#26500;&#23545;&#22797;&#26434;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#24314;&#27169;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#22823;&#22810;&#25968;&#26041;&#27861;&#19981;&#25552;&#20379;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#32780;&#36825;&#23545;&#20110;&#20132;&#36890;&#36816;&#33829;&#21644;&#25511;&#21046;&#26159;&#24517;&#38656;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35889;&#24402;&#19968;&#21270;&#21040;&#20854;&#38544;&#34255;&#23618;&#65292;&#23454;&#29616;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#34920;&#26126;&#65292;&#24402;&#19968;&#21270;&#36890;&#36807;&#25511;&#21046;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#24182;&#20943;&#23569;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#36807;&#24230;&#25311;&#21512;&#39118;&#38505;&#65292;&#25913;&#21892;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning models for traffic data prediction can have superior performance in modeling complex functions using a multi-layer architecture. However, a major drawback of these approaches is that most of these approaches do not offer forecasts with uncertainty estimates, which are essential for traffic operations and control. Without uncertainty estimates, it is difficult to place any level of trust to the model predictions, and operational strategies relying on overconfident predictions can lead to worsening traffic conditions. In this study, we propose a Bayesian recurrent neural network framework for uncertainty quantification in traffic prediction with higher generalizability by introducing spectral normalization to its hidden layers. In our paper, we have shown that normalization alters the training process of deep neural networks by controlling the model's complexity and reducing the risk of overfitting to the training data. This, in turn, helps improve the generalization perfor
&lt;/p&gt;</description></item><item><title>YOGA&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36731;&#37327;&#32423;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#37319;&#29992;&#24265;&#20215;&#32447;&#24615;&#21464;&#25442;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#12290;&#23427;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;&#36866;&#29992;&#20110;&#20302;&#31471;&#36793;&#32536;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2307.05945</link><description>&lt;p&gt;
YOGA: &#29992;&#36731;&#37327;&#32423;&#29305;&#24449;&#23398;&#20064;&#21644;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#36827;&#34892;&#37326;&#22806;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
YOGA: Deep Object Detection in the Wild with Lightweight Feature Learning and Multiscale Attention. (arXiv:2307.05945v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05945
&lt;/p&gt;
&lt;p&gt;
YOGA&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36731;&#37327;&#32423;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#37319;&#29992;&#24265;&#20215;&#32447;&#24615;&#21464;&#25442;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#12290;&#23427;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;&#36866;&#29992;&#20110;&#20302;&#31471;&#36793;&#32536;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;YOGA&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36731;&#37327;&#32423;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20302;&#31471;&#36793;&#32536;&#35774;&#22791;&#19978;&#36816;&#34892;&#65292;&#21516;&#26102;&#20173;&#28982;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;YOGA&#26550;&#26500;&#30001;&#19968;&#20010;&#20004;&#38454;&#27573;&#29305;&#24449;&#23398;&#20064;&#27969;&#27700;&#32447;&#21644;&#19968;&#20010;&#24265;&#20215;&#32447;&#24615;&#21464;&#25442;&#32452;&#25104;&#65292;&#21482;&#20351;&#29992;&#24120;&#35268;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#21367;&#31215;&#28388;&#27874;&#22120;&#25968;&#37327;&#30340;&#19968;&#21322;&#26469;&#23398;&#20064;&#29305;&#24449;&#22270;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#22312;&#20854;&#39048;&#37096;&#36827;&#34892;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#65292;&#32780;&#19981;&#26159;&#24120;&#35268;&#26816;&#27979;&#22120;&#20351;&#29992;&#30340;&#26420;&#32032;&#36830;&#25509;&#12290;YOGA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#25353;&#25968;&#37327;&#32423;&#32553;&#25918;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#30828;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#22312;COCO-val&#21644;COCO-testdev&#25968;&#25454;&#38598;&#19978;&#19982;&#20854;&#20182;10&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#35780;&#20272;&#20102;YOGA&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;YOGA&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#65288;AP&#22686;&#21152;&#20102;22&#65285;&#65292;&#21442;&#25968;&#21644;FLOP&#20943;&#23569;&#20102;23-34&#65285;&#65289;&#65292;&#36825;&#20351;&#23427;&#25104;&#20026;&#37096;&#32626;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce YOGA, a deep learning based yet lightweight object detection model that can operate on low-end edge devices while still achieving competitive accuracy. The YOGA architecture consists of a two-phase feature learning pipeline with a cheap linear transformation, which learns feature maps using only half of the convolution filters required by conventional convolutional neural networks. In addition, it performs multi-scale feature fusion in its neck using an attention mechanism instead of the naive concatenation used by conventional detectors. YOGA is a flexible model that can be easily scaled up or down by several orders of magnitude to fit a broad range of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets with other over 10 state-of-the-art object detectors. The results show that YOGA strikes the best trade-off between model size and accuracy (up to 22% increase of AP and 23-34% reduction of parameters and FLOPs), making it an ideal choice for deplo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#32500;&#29615;&#22659;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#33021;&#28304;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#38388;&#38553;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#33021;&#28304;&#31995;&#32479;&#20934;&#30830;&#39044;&#27979;&#21644;&#31649;&#29702;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#25968;&#25454;&#22312;&#20915;&#31574;&#21644;&#30740;&#31350;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05926</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#20687;&#25216;&#26415;&#22635;&#34917;&#26102;&#38388;&#24207;&#21015;&#38388;&#38553;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#22810;&#32500;&#29615;&#22659;&#33258;&#32534;&#30721;&#22120;&#30340;&#24314;&#31569;&#33021;&#28304;&#25968;&#25454;&#22635;&#34917;
&lt;/p&gt;
&lt;p&gt;
Filling time-series gaps using image techniques: Multidimensional context autoencoder approach for building energy data imputation. (arXiv:2307.05926v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#32500;&#29615;&#22659;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#33021;&#28304;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#38388;&#38553;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#33021;&#28304;&#31995;&#32479;&#20934;&#30830;&#39044;&#27979;&#21644;&#31649;&#29702;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#25968;&#25454;&#22312;&#20915;&#31574;&#21644;&#30740;&#31350;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#33021;&#28304;&#39044;&#27979;&#21644;&#31649;&#29702;&#22312;&#26368;&#36817;&#20960;&#21313;&#24180;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21463;&#21040;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#22686;&#38271;&#21644;&#26356;&#22810;&#33021;&#28304;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#30340;&#25512;&#21160;&#12290;&#28982;&#32780;&#65292;&#33021;&#28304;&#25968;&#25454;&#32463;&#24120;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#65292;&#21487;&#33021;&#19981;&#23436;&#25972;&#25110;&#19981;&#19968;&#33268;&#65292;&#36825;&#21487;&#33021;&#38459;&#30861;&#33021;&#28304;&#31995;&#32479;&#30340;&#20934;&#30830;&#39044;&#27979;&#21644;&#31649;&#29702;&#65292;&#24182;&#38480;&#21046;&#25968;&#25454;&#22312;&#20915;&#31574;&#21644;&#30740;&#31350;&#20013;&#30340;&#29992;&#36884;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22635;&#34917;&#33021;&#28304;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#38388;&#38553;&#65292;&#21253;&#25324;&#38543;&#26426;&#38388;&#38553;&#21644;&#36830;&#32493;&#38388;&#38553;&#12290;&#36825;&#19968;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#22312;&#21508;&#31181;&#24314;&#31569;&#21644;&#20202;&#34920;&#31867;&#22411;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#20934;&#30830;&#35780;&#20272;&#19981;&#21516;&#22635;&#34917;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#23558;&#26368;&#20808;&#36827;&#30340;&#22635;&#34917;&#26041;&#27861;&#24212;&#29992;&#20110;&#33021;&#28304;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#38388;&#38553;&#12290;&#29616;&#20195;&#22270;&#20687;&#20462;&#22797;&#26041;&#27861;&#65292;&#22914;&#37096;&#20998;&#21367;&#31215;(PConv)&#65292;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Building energy prediction and management has become increasingly important in recent decades, driven by the growth of Internet of Things (IoT) devices and the availability of more energy data. However, energy data is often collected from multiple sources and can be incomplete or inconsistent, which can hinder accurate predictions and management of energy systems and limit the usefulness of the data for decision-making and research. To address this issue, past studies have focused on imputing missing gaps in energy data, including random and continuous gaps. One of the main challenges in this area is the lack of validation on a benchmark dataset with various building and meter types, making it difficult to accurately evaluate the performance of different imputation methods. Another challenge is the lack of application of state-of-the-art imputation methods for missing gaps in energy data. Contemporary image-inpainting methods, such as Partial Convolution (PConv), have been widely used 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;-&#26631;&#31614;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25345;&#32493;&#25552;&#31034;&#31574;&#30053;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#30340;&#38544;&#31169;&#12289;&#26679;&#26412;&#24046;&#24322;&#21644;&#25552;&#31034;&#24046;&#24322;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05920</link><description>&lt;p&gt;
&#24102;&#26377;&#25345;&#32493;&#25552;&#31034;&#30340;&#32479;&#19968;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;-&#26631;&#31614;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt. (arXiv:2307.05920v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;-&#26631;&#31614;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25345;&#32493;&#25552;&#31034;&#31574;&#30053;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#30340;&#38544;&#31169;&#12289;&#26679;&#26412;&#24046;&#24322;&#21644;&#25552;&#31034;&#24046;&#24322;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#24615;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#21487;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#37492;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#27880;&#37322;&#26159;&#32791;&#26102;&#19988;&#36153;&#21147;&#30340;&#65292;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#22312;&#21033;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#24433;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#25968;&#25454;&#38598;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#38754;&#20020;&#20197;&#19979;&#20960;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#21487;&#29992;&#30340;&#21307;&#23398;&#25968;&#25454;&#30456;&#23545;&#36739;&#23569;&#65292;&#19982;&#33258;&#28982;&#25968;&#25454;&#30456;&#27604;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24369;&#12290;&#65288;2&#65289;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#38750;&#24120;&#30456;&#20284;&#65292;&#32454;&#24494;&#24046;&#21035;&#24456;&#22810;&#65292;&#23548;&#33268;&#27604;&#36739;&#23398;&#20064;&#20013;&#26377;&#22823;&#37327;&#30340;&#20551;&#38452;&#24615;&#26679;&#26412;&#23545;&#12290;&#65288;3&#65289;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#36890;&#24120;&#19982;&#33258;&#28982;&#30340;&#21307;&#23398;&#22270;&#20687;&#25253;&#21578;&#19981;&#21516;&#65292;&#25514;&#36766;&#19978;&#30340;&#32454;&#24494;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#22270;&#20687;-&#25991;&#26412;-&#26631;&#31614;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25345;&#32493;&#25552;&#31034;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive language-image Pre-training (CLIP) [13] can leverage large datasets of unlabeled Image-Text pairs, which have demonstrated impressive performance in various downstream tasks. Given that annotating medical data is time-consuming and laborious, Image-Text Pre-training has promising applications in exploiting large-scale medical image and radiology report datasets. However, medical Image-Text Pre-training faces several challenges, as follows: (1) Due to privacy concerns, the amount of available medical data is relatively small compared to natural data, leading to weaker generalization ability of the model. (2) Medical images are highly similar with only fine-grained differences in subtleties, resulting in a large number of false-negative sample pairs in comparison learning. (3) The hand-crafted Prompt usually differs from the natural medical image report, Subtle changes in wording can lead to significant differences in performance. In this paper, we propose a unified Image-Tex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prompt Generate Train (PGT)&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#24212;&#12289;&#23545;&#40784;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#24182;&#22312;&#29983;&#25104;&#30456;&#20851;&#31572;&#26696;&#26041;&#38754;&#20855;&#26377;&#19982;&#22522;&#20110;GPT-4&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05915</link><description>&lt;p&gt;
Prompt Generate Train (PGT): &#19968;&#31181;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#24212;&#12289;&#23545;&#40784;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Prompt Generate Train (PGT): A framework for few-shot domain adaptation, alignment, and uncertainty calibration of a retriever augmented generation (RAG) model for domain specific open book question-answering. (arXiv:2307.05915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05915
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prompt Generate Train (PGT)&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#24212;&#12289;&#23545;&#40784;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#24182;&#22312;&#29983;&#25104;&#30456;&#20851;&#31572;&#26696;&#26041;&#38754;&#20855;&#26377;&#19982;&#22522;&#20110;GPT-4&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Prompt, Generate, Train (PGT) &#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#24320;&#21457;&#19968;&#20010;&#38024;&#23545;&#19987;&#26377;&#30340;&#25991;&#26412;&#25991;&#26723;&#38598;&#21512;&#36827;&#34892;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23558;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#23545;&#40784;&#12289;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#30456;&#20851;&#31572;&#26696;&#26102;&#20855;&#26377;&#19982;&#22522;&#20110; GPT-4 &#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26381;&#21153;&#25104;&#26412;&#26356;&#20302;&#12290;&#36890;&#36807;&#20351;&#29992;&#20013;&#31561;&#35268;&#27169;&#30340;LLM (Flan-T5 XXL) &#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#26696;&#65292;&#21512;&#25104;&#29983;&#25104;&#31649;&#36947;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#31649;&#36947;&#26088;&#22312;&#29983;&#25104;&#28085;&#30422;&#25972;&#20010;&#35821;&#26009;&#24211;&#30340;&#25277;&#35937;&#21644;&#25552;&#21462;&#24335;&#38382;&#39064;&#12290;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#30340;&#26679;&#26412;&#65292;&#35813;&#26694;&#26550;&#23545;&#19968;&#20010;&#30001;&#31264;&#23494;&#26816;&#32034;&#22120;&#21644;&#36739;&#23567;&#35268;&#27169;&#30340;LLM&#32452;&#25104;&#30340;&#36739;&#23567;&#30340;RAG&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a framework - Prompt, Generate, Train (PGT) - to efficiently develop a generative question-answering model for open-book question-answering over a proprietary collection of text documents. The framework adapts a retriever augmented generation model to the target domain using supervised finetuning and reinforcement learning with synthetic feedback in a few-shot setting. This yields an aligned, uncertainty calibrated model that is competitive with GPT-4 based in-context retrieval augmented generation in generating relevant answers at lower serving costs. The synthetic generation pipeline generates high quality synthetic training data musing a medium sized LLM, Flan-T5 XXL, and a novel consistency filtering scheme. The pipeline is designed to generate both abstractive and extractive questions that span the entire corpus. Using samples from this dataset, the framework fine-tunes a smaller RAG model comprising a dense retriever and a smaller sized LLM on samples from the dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;FIS-ONE&#65292;&#19968;&#31181;&#20351;&#29992;&#19968;&#20010;&#26631;&#31614;&#30340;&#20247;&#21253;RF&#20449;&#21495;&#27004;&#23618;&#35782;&#21035;&#31995;&#32479;&#12290;&#36890;&#36807;&#20449;&#21495;&#32858;&#31867;&#21644;&#32858;&#31867;&#32034;&#24341;&#30340;&#27493;&#39588;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20165;&#20351;&#29992;&#19968;&#20010;&#24213;&#23618;&#24102;&#26377;&#26631;&#31614;&#30340;&#20449;&#21495;&#26679;&#26412;&#65292;&#20197;&#21450;&#20854;&#20313;&#30340;&#26679;&#26412;&#19981;&#24102;&#26631;&#31614;&#65292;&#20063;&#33021;&#23454;&#29616;&#26377;&#25928;&#30340;&#27004;&#23618;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2307.05914</link><description>&lt;p&gt;
FIS-ONE: &#20351;&#29992;&#19968;&#20010;&#26631;&#31614;&#30340;&#20247;&#21253;RF&#20449;&#21495;&#27004;&#23618;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FIS-ONE: Floor Identification System with One Label for Crowdsourced RF Signals. (arXiv:2307.05914v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05914
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;FIS-ONE&#65292;&#19968;&#31181;&#20351;&#29992;&#19968;&#20010;&#26631;&#31614;&#30340;&#20247;&#21253;RF&#20449;&#21495;&#27004;&#23618;&#35782;&#21035;&#31995;&#32479;&#12290;&#36890;&#36807;&#20449;&#21495;&#32858;&#31867;&#21644;&#32858;&#31867;&#32034;&#24341;&#30340;&#27493;&#39588;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20165;&#20351;&#29992;&#19968;&#20010;&#24213;&#23618;&#24102;&#26377;&#26631;&#31614;&#30340;&#20449;&#21495;&#26679;&#26412;&#65292;&#20197;&#21450;&#20854;&#20313;&#30340;&#26679;&#26412;&#19981;&#24102;&#26631;&#31614;&#65292;&#20063;&#33021;&#23454;&#29616;&#26377;&#25928;&#30340;&#27004;&#23618;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;RF&#20449;&#21495;&#30340;&#27004;&#23618;&#26631;&#31614;&#23545;&#20110;&#35768;&#22810;&#26234;&#24935;&#22478;&#24066;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#22810;&#27004;&#23618;&#23460;&#20869;&#23450;&#20301;&#12289;&#22320;&#29702;&#22260;&#26639;&#21644;&#26426;&#22120;&#20154;&#30417;&#25511;&#12290;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#20247;&#21253;RF&#20449;&#21495;&#36827;&#34892;&#27004;&#23618;&#21495;&#30721;&#30340;&#35782;&#21035;&#27169;&#22411;&#24314;&#35774;&#26102;&#65292;&#20551;&#35774;&#27599;&#20010;&#27004;&#23618;&#33267;&#23569;&#26377;&#20960;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#20449;&#21495;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#31361;&#30772;&#30028;&#38480;&#65292;&#24182;&#19988;&#35777;&#26126;&#21482;&#38656;&#35201;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#24213;&#23618;&#20449;&#21495;&#26679;&#26412;&#20197;&#21450;&#20854;&#20313;&#30340;&#20449;&#21495;&#26679;&#26412;&#19981;&#24102;&#26631;&#31614;&#20063;&#21487;&#20197;&#23454;&#29616;&#27004;&#23618;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FIS-ONE&#27004;&#23618;&#35782;&#21035;&#31995;&#32479;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;FIS-ONE&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65292;&#21363;&#20449;&#21495;&#32858;&#31867;&#21644;&#32858;&#31867;&#32034;&#24341;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#20108;&#20998;&#22270;&#26469;&#24314;&#27169;RF&#20449;&#21495;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#65288;&#27599;&#20010;&#20449;&#21495;&#26679;&#26412;&#65289;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20197;&#20415;&#23454;&#29616;&#27004;&#23618;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Floor labels of crowdsourced RF signals are crucial for many smart-city applications, such as multi-floor indoor localization, geofencing, and robot surveillance. To build a prediction model to identify the floor number of a new RF signal upon its measurement, conventional approaches using the crowdsourced RF signals assume that at least few labeled signal samples are available on each floor. In this work, we push the envelope further and demonstrate that it is technically feasible to enable such floor identification with only one floor-labeled signal sample on the bottom floor while having the rest of signal samples unlabeled.  We propose FIS-ONE, a novel floor identification system with only one labeled sample. FIS-ONE consists of two steps, namely signal clustering and cluster indexing. We first build a bipartite graph to model the RF signal samples and obtain a latent representation of each node (each signal sample) using our attention-based graph neural network model so that the R
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;CNN&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35895;&#29289;&#36793;&#30028;&#20998;&#21106;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#21516;&#26102;&#20855;&#22791;&#20102;&#25163;&#21160;&#20998;&#21106;&#30340;&#20934;&#30830;&#24230;&#21644;&#35745;&#31639;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05911</link><description>&lt;p&gt;
&#20351;&#29992;&#30495;&#23454;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35895;&#29289;&#21644;&#26230;&#30028;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Grain and Grain Boundary Segmentation using Machine Learning with Real and Generated Datasets. (arXiv:2307.05911v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;CNN&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35895;&#29289;&#36793;&#30028;&#20998;&#21106;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#21516;&#26102;&#20855;&#22791;&#20102;&#25163;&#21160;&#20998;&#21106;&#30340;&#20934;&#30830;&#24230;&#21644;&#35745;&#31639;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#30495;&#23454;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#32452;&#21512;&#19978;&#35757;&#32451;&#65292;&#25253;&#21578;&#20102;&#35895;&#29289;&#36793;&#30028;&#20998;&#21106;&#30340;&#26174;&#30528;&#25552;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#25163;&#21160;&#20998;&#21106;&#20934;&#30830;&#20294;&#32791;&#26102;&#65292;&#29616;&#26377;&#30340;&#35745;&#31639;&#26041;&#27861;&#36895;&#24230;&#36739;&#24555;&#20294;&#24120;&#24120;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26082;&#33021;&#36798;&#21040;&#25163;&#21160;&#20998;&#21106;&#30340;&#20934;&#30830;&#24230;&#65292;&#21448;&#20855;&#26377;&#35745;&#31639;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#21046;&#22791;&#12289;&#25243;&#20809;&#12289;&#34432;&#21051;&#31561;&#36807;&#31243;&#65292;&#23545;316L&#19981;&#38152;&#38050;&#26679;&#21697;&#36827;&#34892;&#20102;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#29616;&#26377;&#35745;&#31639;&#26041;&#27861;&#21644;&#25163;&#21160;&#20998;&#21106;&#21019;&#24314;&#8220;&#30495;&#23454;&#8221;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;Voronoi&#22270;&#26696;&#21644;&#38543;&#26426;&#21512;&#25104;&#22122;&#22768;&#12289;&#27169;&#25311;&#32570;&#38519;&#30340;&#20154;&#24037;&#35895;&#29289;&#22270;&#20687;&#21046;&#20316;&#26041;&#27861;&#65292;&#20026;&#25968;&#25454;&#23494;&#38598;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#35757;&#32451;&#25968;&#25454;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report significantly improved accuracy of grain boundary segmentation using Convolutional Neural Networks (CNN) trained on a combination of real and generated data. Manual segmentation is accurate but time-consuming, and existing computational methods are faster but often inaccurate. To combat this dilemma, machine learning models can be used to achieve the accuracy of manual segmentation and have the efficiency of a computational method. An extensive dataset of from 316L stainless steel samples is additively manufactured, prepared, polished, etched, and then microstructure grain images were systematically collected. Grain segmentation via existing computational methods and manual (by-hand) were conducted, to create "real" training data. A Voronoi tessellation pattern combined with random synthetic noise and simulated defects, is developed to create a novel artificial grain image fabrication method. This provided training data supplementation for data-intensive machine learning meth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65288;PPD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21551;&#21160;&#21518;&#32493;&#20196;&#29260;&#35299;&#30721;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#36138;&#23146;&#35299;&#30721;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;&#23436;&#20840;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#35299;&#30721;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;LLM&#35299;&#30721;&#31574;&#30053;&#26435;&#34913;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.05908</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65306;&#20934;&#30830;LLM&#35299;&#30721;&#20013;&#30340;&#35745;&#31639;&#24310;&#36831;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding. (arXiv:2307.05908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65288;PPD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21551;&#21160;&#21518;&#32493;&#20196;&#29260;&#35299;&#30721;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#36138;&#23146;&#35299;&#30721;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;&#23436;&#20840;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#35299;&#30721;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;LLM&#35299;&#30721;&#31574;&#30053;&#26435;&#34913;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65288;PPD&#65289;"&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#36138;&#23146;&#35299;&#30721;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#21407;&#22987;&#35299;&#30721;&#23436;&#20840;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#19982;&#20256;&#32479;&#31574;&#30053;&#19981;&#21516;&#65292;PPD&#21033;&#29992;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#22312;&#24403;&#21069;&#20196;&#29260;&#35299;&#30721;&#26399;&#38388;&#24182;&#34892;&#21551;&#21160;&#21518;&#32493;&#20196;&#29260;&#35299;&#30721;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#20943;&#23569;&#20102;&#35299;&#30721;&#24310;&#36831;&#65292;&#24182;&#37325;&#26032;&#22609;&#36896;&#20102;LLM&#35299;&#30721;&#31574;&#30053;&#20013;&#30340;&#26435;&#34913;&#29702;&#35299;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#35745;&#31639;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35780;&#20272;&#21305;&#37197;&#29575;&#65288;&#34920;&#31034;&#20026;p_correct&#65289;&#26469;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#33021;&#30340;&#24310;&#36831;&#20943;&#23569;&#36827;&#34892;&#20998;&#26512;&#20272;&#35745;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#26377;&#28508;&#21147;&#21152;&#36895;LLM&#30340;&#36138;&#23146;&#35299;&#30721;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents "Predictive Pipelined Decoding (PPD)," an approach that speeds up greedy decoding in Large Language Models (LLMs) while maintaining the exact same output as the original decoding. Unlike conventional strategies, PPD employs additional compute resources to parallelize the initiation of subsequent token decoding during the current token decoding. This innovative method reduces decoding latency and reshapes the understanding of trade-offs in LLM decoding strategies. We have developed a theoretical framework that allows us to analyze the trade-off between computation and latency. Using this framework, we can analytically estimate the potential reduction in latency associated with our proposed method, achieved through the assessment of the match rate, represented as p_correct. The results demonstrate that the use of extra computational resources has the potential to accelerate LLM greedy decoding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#23567;&#25209;&#37327;&#20248;&#21270;&#30340;&#29702;&#35770;&#26041;&#38754;&#65292;&#35777;&#26126;&#20102;&#20840;&#37096;&#36873;&#25321;&#23567;&#25209;&#37327;&#19982;&#20840;&#25209;&#37327;&#20248;&#21270;&#31561;&#25928;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#25439;&#22833;&#23567;&#25209;&#37327;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35889;&#32858;&#31867;&#35782;&#21035;&#39640;&#25439;&#22833;&#23567;&#25209;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.05906</link><description>&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#30340;&#23567;&#25209;&#37327;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Mini-Batch Optimization of Contrastive Loss. (arXiv:2307.05906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#23567;&#25209;&#37327;&#20248;&#21270;&#30340;&#29702;&#35770;&#26041;&#38754;&#65292;&#35777;&#26126;&#20102;&#20840;&#37096;&#36873;&#25321;&#23567;&#25209;&#37327;&#19982;&#20840;&#25209;&#37327;&#20248;&#21270;&#31561;&#25928;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#25439;&#22833;&#23567;&#25209;&#37327;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35889;&#32858;&#31867;&#35782;&#21035;&#39640;&#25439;&#22833;&#23567;&#25209;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30830;&#20445;&#27491;&#26679;&#26412;&#23545;&#30340;&#23884;&#20837;&#65288;&#20363;&#22914;&#65292;&#21516;&#19968;&#31867;&#21035;&#30340;&#19981;&#21516;&#26679;&#26412;&#25110;&#21516;&#19968;&#23545;&#35937;&#30340;&#19981;&#21516;&#35270;&#22270;&#65289;&#30456;&#20284;&#65292;&#32780;&#36127;&#26679;&#26412;&#23545;&#30340;&#23884;&#20837;&#19981;&#30456;&#20284;&#12290;&#23454;&#38469;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#22823;&#20869;&#23384;&#38656;&#27714;&#65292;&#20351;&#24471;&#32771;&#34385;&#25152;&#26377;&#21487;&#33021;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#20351;&#29992;&#23567;&#25209;&#37327;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#23567;&#25209;&#37327;&#20248;&#21270;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#19988;&#20165;&#24403;&#36873;&#25321;&#20102;&#25152;&#26377; $\binom{N}{B}$ &#23567;&#25209;&#37327;&#65292;&#23567;&#25209;&#37327;&#20248;&#21270;&#19982;&#20840;&#25209;&#37327;&#20248;&#21270;&#31561;&#25928;&#65292;&#32780;&#20165;&#32771;&#34385;&#23376;&#38598;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#12290;&#25105;&#20204;&#38543;&#21518;&#35777;&#26126;&#20102;&#21033;&#29992;&#39640;&#25439;&#22833;&#23567;&#25209;&#37327;&#21487;&#20197;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#36825;&#20123;&#39640;&#25439;&#22833;&#23567;&#25209;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has gained significant attention as a method for self-supervised learning. The contrastive loss function ensures that embeddings of positive sample pairs (e.g., different samples from the same class or different views of the same object) are similar, while embeddings of negative pairs are dissimilar. Practical constraints such as large memory requirements make it challenging to consider all possible positive and negative pairs, leading to the use of mini-batch optimization. In this paper, we investigate the theoretical aspects of mini-batch optimization in contrastive learning. We show that mini-batch optimization is equivalent to full-batch optimization if and only if all $\binom{N}{B}$ mini-batches are selected, while sub-optimality may arise when examining only a subset. We then demonstrate that utilizing high-loss mini-batches can speed up SGD convergence and propose a spectral clustering-based approach for identifying these high-loss mini-batches. Our experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#27169;&#22411;&#30340;Lipschitz&#24615;&#36136;&#65292;&#20445;&#35777;&#20102;&#20854;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.05902</link><description>&lt;p&gt;
&#24102;&#26377;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#31283;&#23450;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Stability Guarantees for Feature Attributions with Multiplicative Smoothing. (arXiv:2307.05902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#27169;&#22411;&#30340;Lipschitz&#24615;&#36136;&#65292;&#20445;&#35777;&#20102;&#20854;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#24448;&#24448;&#19981;&#33021;&#25552;&#20379;&#20219;&#20309;&#24418;&#24335;&#30340;&#20445;&#35777;&#65292;&#20063;&#21487;&#33021;&#19981;&#21453;&#26144;&#24213;&#23618;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31283;&#23450;&#24615;&#20316;&#20026;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#19968;&#20010;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#27169;&#22411;&#22312;&#29305;&#24449;&#23631;&#34109;&#26041;&#38754;&#20855;&#26377;&#36275;&#22815;&#30340;Lipschitz&#24615;&#36136;&#65292;&#21017;&#21487;&#20197;&#20445;&#35777;&#25918;&#26494;&#21464;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#20056;&#27861;&#24179;&#28369;&#65288;MuS&#65289;&#30340;&#24179;&#28369;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MuS&#20811;&#26381;&#20102;&#26631;&#20934;&#24179;&#28369;&#25216;&#26415;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;&#22914;LIME&#21644;SHAP&#65289;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;MuS&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;MuS&#36171;&#20104;&#20102;&#29305;&#24449;&#24402;&#22240;&#20197;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanation methods for machine learning models tend to not provide any formal guarantees and may not reflect the underlying decision-making process. In this work, we analyze stability as a property for reliable feature attribution methods. We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features. To achieve such a model, we develop a smoothing method called Multiplicative Smoothing (MuS). We show that MuS overcomes theoretical limitations of standard smoothing techniques and can be integrated with any classifier and feature attribution method. We evaluate MuS on vision and language models with a variety of feature attribution methods, such as LIME and SHAP, and demonstrate that MuS endows feature attributions with non-trivial stability guarantees.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23637;&#24320;&#31639;&#27861;&#30340;&#38750;&#20984;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#36229;&#21442;&#25968;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20154;&#33080;&#24314;&#27169;&#38382;&#39064;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#25968;&#20540;&#21644;&#35270;&#35273;&#24615;&#33021;&#12290; (arXiv:2307.05893v1 [eess.SP])</title><link>http://arxiv.org/abs/2307.05893</link><description>&lt;p&gt;
&#38750;&#20984;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#28145;&#24230;&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
Deep Unrolling for Nonconvex Robust Principal Component Analysis. (arXiv:2307.05893v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23637;&#24320;&#31639;&#27861;&#30340;&#38750;&#20984;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#36229;&#21442;&#25968;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20154;&#33080;&#24314;&#27169;&#38382;&#39064;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#25968;&#20540;&#21644;&#35270;&#35273;&#24615;&#33021;&#12290; (arXiv:2307.05893v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#29992;&#20110;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;RPCA&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#19968;&#20010;&#30697;&#38453;&#20998;&#35299;&#20026;&#19968;&#20010;&#20302;&#31209;&#30697;&#38453;&#21644;&#19968;&#20010;&#31232;&#30095;&#30697;&#38453;&#30340;&#21644;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#36895;&#20132;&#26367;&#25237;&#24433;&#31639;&#27861;&#30340;&#28145;&#24230;&#23637;&#24320;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#38750;&#20984;&#24418;&#24335;&#30340;RPCA&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#36807;&#31243;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#21644;&#21407;&#22987;&#31639;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23637;&#24320;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#20154;&#33080;&#24314;&#27169;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#25968;&#20540;&#21644;&#35270;&#35273;&#24615;&#33021;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design algorithms for Robust Principal Component Analysis (RPCA) which consists in decomposing a matrix into the sum of a low rank matrix and a sparse matrix. We propose a deep unrolled algorithm based on an accelerated alternating projection algorithm which aims to solve RPCA in its nonconvex form. The proposed procedure combines benefits of deep neural networks and the interpretability of the original algorithm and it automatically learns hyperparameters. We demonstrate the unrolled algorithm's effectiveness on synthetic datasets and also on a face modeling problem, where it leads to both better numerical and visual performances.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21463;&#21040;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65292;&#20197;&#24179;&#34913;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#28789;&#27963;&#24615;&#19982;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05891</link><description>&lt;p&gt;
&#21463;PID&#25511;&#21046;&#22120;&#21551;&#21457;&#30340;&#20559;&#24046;&#24402;&#32435;&#27861;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21463;&#21040;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65292;&#20197;&#24179;&#34913;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#28789;&#27963;&#24615;&#19982;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#36890;&#36807;&#25968;&#25454;&#33258;&#24049;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;RL&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#31995;&#32479;&#30340;&#23436;&#25972;&#29366;&#24577;&#36890;&#24120;&#19981;&#21487;&#35266;&#27979;&#12290;&#24403;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#26102;&#65292;&#31574;&#30053;&#38656;&#35201;&#21033;&#29992;&#35266;&#23519;&#21382;&#21490;&#26469;&#25512;&#26029;&#24403;&#21069;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#24046;&#24322;&#20351;&#24471;&#31574;&#30053;&#19981;&#20250;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#26102;&#35266;&#23519;&#21040;&#30340;&#24207;&#21015;&#12290;&#22240;&#27492;&#65292;&#22312;&#21382;&#21490;&#35760;&#24405;&#32534;&#30721;&#22120;&#28789;&#27963;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#35201;&#23545;&#29615;&#22659;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#24179;&#34913;&#65292;&#25105;&#20204;&#23547;&#27714;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#26029;&#23450;PID&#25511;&#21046;&#22120;&#30340;&#25104;&#21151;&#34920;&#26126;&#65292;&#35768;&#22810;&#25511;&#21046;&#20219;&#21153;&#21482;&#38656;&#35201;&#27714;&#21644;&#21644;&#27714;&#24046;&#26469;&#32047;&#31215;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#20010;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65306;&#19968;&#31181;&#30452;&#25509;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#24322;&#26500;MEC/MCC&#29615;&#22659;&#65292;&#24182;&#22522;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#21368;&#36733;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#23454;&#26102;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2307.05888</link><description>&lt;p&gt;
&#38024;&#23545;&#36793;&#32536;/&#20113;&#35745;&#31639;&#29615;&#22659;&#20013;&#25968;&#23383;&#23402;&#29983;&#30340;&#39640;&#25928;&#20219;&#21153;&#21368;&#36733;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud Computing Environment. (arXiv:2307.05888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#24322;&#26500;MEC/MCC&#29615;&#22659;&#65292;&#24182;&#22522;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#21368;&#36733;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#23454;&#26102;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#25968;&#23383;&#23402;&#29983;&#34987;&#35270;&#20026;&#36830;&#25509;&#23454;&#29289;&#23545;&#35937;&#21644;&#25968;&#23383;&#19990;&#30028;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36890;&#36807;&#34394;&#25311;&#21270;&#21644;&#27169;&#25311;&#25216;&#26415;&#65292;&#21487;&#20197;&#21033;&#29992;&#35745;&#31639;&#36164;&#28304;&#23454;&#29616;&#22810;&#31181;&#21151;&#33021;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#31227;&#21160;&#20113;&#35745;&#31639;&#21644;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#24050;&#25104;&#20026;&#23454;&#29616;&#23454;&#26102;&#21453;&#39304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#21482;&#32771;&#34385;&#20102;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#27169;&#22411;&#20013;&#30340;&#36793;&#32536;&#26381;&#21153;&#22120;&#25110;&#20113;&#26381;&#21153;&#22120;&#65292;&#21516;&#26102;&#24573;&#30053;&#20102;&#20855;&#26377;&#22810;&#20010;&#25968;&#25454;&#36164;&#28304;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24322;&#26500;MEC/MCC&#29615;&#22659;&#30340;&#26032;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#27169;&#22411;&#65292;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#25968;&#23383;&#23402;&#29983;&#37117;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#37319;&#38598;&#35774;&#22791;&#22312;&#26381;&#21153;&#22120;&#20013;&#32500;&#25252;&#12290;&#36824;&#32771;&#34385;&#20102;&#21368;&#36733;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21368;&#36733;&#26041;&#26696;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned to empower various areas as a bridge between physical objects and the digital world. Through virtualization and simulation techniques, multiple functions can be achieved by leveraging computing resources. In this process, Mobile Cloud Computing (MCC) and Mobile Edge Computing (MEC) have become two of the key factors to achieve real-time feedback. However, current works only considered edge servers or cloud servers in the DT system models. Besides, The models ignore the DT with not only one data resource. In this paper, we propose a new DT system model considering a heterogeneous MEC/MCC environment. Each DT in the model is maintained in one of the servers via multiple data collection devices. The offloading decision-making problem is also considered and a new offloading scheme is proposed based on Distributed Deep Learning (DDL). Simulation results demonstrate that our proposed algorithm can effectively and efficie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;time-dependent Cox&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;&#36827;&#34892;&#24615;&#30524;&#37096;&#30142;&#30149;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#30340;&#36827;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#32437;&#21521;&#30524;&#24213;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24314;&#31435;&#19968;&#20010;&#20010;&#20307;&#21270;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05881</link><description>&lt;p&gt;
&#21160;&#24577;&#39044;&#27979;&#20351;&#29992;&#26102;&#21464;Cox&#29983;&#23384;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dynamic Prediction using Time-Dependent Cox Survival Neural Network. (arXiv:2307.05881v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05881
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;time-dependent Cox&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;&#36827;&#34892;&#24615;&#30524;&#37096;&#30142;&#30149;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#30340;&#36827;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#32437;&#21521;&#30524;&#24213;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24314;&#31435;&#19968;&#20010;&#20010;&#20307;&#21270;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#39044;&#27979;&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#26029;&#26356;&#26032;&#30340;&#26032;&#25968;&#25454;&#21487;&#29992;&#26102;&#25552;&#20379;&#20010;&#20307;&#21270;&#30340;&#39118;&#38505;&#39044;&#27979;&#12290;&#21463;&#21040;&#24314;&#31435;&#19968;&#20010;&#38024;&#23545;&#36827;&#34892;&#24615;&#30524;&#37096;&#30142;&#30149;&#65292;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#21464;Cox&#27169;&#22411;&#30340;&#29983;&#23384;&#31070;&#32463;&#32593;&#32476;&#65288;tdCoxSNN&#65289;&#26469;&#39044;&#27979;&#20854;&#22312;&#25345;&#32493;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#36827;&#23637;&#65292;&#20351;&#29992;&#32437;&#21521;&#30524;&#24213;&#22270;&#20687;&#12290;tdCoxSNN&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#25311;&#26102;&#21464;&#21327;&#21464;&#37327;&#23545;&#29983;&#23384;&#32467;&#26524;&#30340;&#38750;&#32447;&#24615;&#24433;&#21709;&#25193;&#23637;&#20102;&#26102;&#21464;Cox&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;tdCoxSNN&#21487;&#20197;&#20197;&#32437;&#21521;&#21407;&#22987;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#65292;&#20351;&#29992;&#20004;&#20010;&#26102;&#21464;&#31934;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;Brier&#20998;&#25968;&#21644;&#21160;&#24577;AUC&#27604;&#36739;&#21644;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#32852;&#21512;&#24314;&#27169;&#21644;&#37324;&#31243;&#30865;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#19968;&#20010;&#26159;&#19968;&#20010;&#22823;&#22411;AMD&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The target of dynamic prediction is to provide individualized risk predictions over time which can be updated as new data become available. Motivated by establishing a dynamic prediction model for the progressive eye disease, age-related macular degeneration (AMD), we proposed a time-dependent Cox model-based survival neural network (tdCoxSNN) to predict its progression on a continuous time scale using longitudinal fundus images. tdCoxSNN extends the time-dependent Cox model by utilizing a neural network to model the non-linear effect of the time-dependent covariates on the survival outcome. Additionally, by incorporating the convolutional neural network (CNN), tdCoxSNN can take the longitudinal raw images as input. We evaluate and compare our proposed method with joint modeling and landmarking approaches through comprehensive simulations using two time-dependent accuracy metrics, the Brier Score and dynamic AUC. We applied the proposed approach to two real datasets. One is a large AMD
&lt;/p&gt;</description></item><item><title>&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#31995;&#32479;&#24615;&#25925;&#38556;, &#21363;&#20351;&#21333;&#20010;&#27169;&#22411;&#22312;&#24635;&#20307;&#19978;&#30340;&#25913;&#21892;&#20063;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;</title><link>http://arxiv.org/abs/2307.05862</link><description>&lt;p&gt;
&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#25581;&#31034;&#20102;&#21516;&#36136;&#21270;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes. (arXiv:2307.05862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05862
&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#31995;&#32479;&#24615;&#25925;&#38556;, &#21363;&#20351;&#21333;&#20010;&#27169;&#22411;&#22312;&#24635;&#20307;&#19978;&#30340;&#25913;&#21892;&#20063;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#22312;&#27169;&#22411;&#23618;&#38754;&#36827;&#34892;&#30740;&#31350;&#65306;&#30740;&#31350;&#20154;&#21592;&#34913;&#37327;&#21644;&#25913;&#36827;&#29305;&#23450;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#20559;&#35265;&#12289;&#25928;&#29575;&#21644;&#20854;&#20182;&#32500;&#24230;&#12290;&#23454;&#38469;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#31038;&#20250;&#24433;&#21709;&#21462;&#20915;&#20110;&#26426;&#22120;&#23398;&#20064;&#37096;&#32626;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#65306;&#19981;&#26159;&#20998;&#26512;&#21333;&#20010;&#27169;&#22411;&#65292;&#32780;&#26159;&#32771;&#34385;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#37096;&#32626;&#30340;&#25152;&#26377;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#20363;&#22914;&#65292;&#22312;&#25307;&#32856;&#20013;&#36827;&#34892;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#24847;&#21619;&#30528;&#35748;&#35782;&#21040;&#19968;&#20010;&#27714;&#32844;&#32773;&#30340;&#32467;&#26524;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#21333;&#20010;&#25307;&#32856;&#31639;&#27861;&#25110;&#20844;&#21496;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#20182;&#20204;&#30003;&#35831;&#30340;&#25152;&#26377;&#20844;&#21496;&#30340;&#38598;&#20307;&#20915;&#31574;&#12290;&#22312;&#19977;&#31181;&#27169;&#24335;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35821;&#38899;&#65289;&#21644;11&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#36235;&#21183;&#65306;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#23481;&#26131;&#20986;&#29616;&#31995;&#32479;&#24615;&#25925;&#38556;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20123;&#29992;&#25143;&#34987;&#25152;&#26377;&#21487;&#29992;&#30340;&#27169;&#22411;&#38169;&#35823;&#20998;&#31867;&#12290;&#21363;&#20351;&#22312;&#20010;&#20307;&#27169;&#22411;&#38543;&#26102;&#38388;&#22312;&#24635;&#20307;&#27700;&#24179;&#19978;&#25913;&#21892;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments. To capture this, we introduce ecosystem-level analysis: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are not only determined by a single hiring algorithm or firm but instead by the collective decisions of all the firms they applied to. Across three modalities (text, images, speech) and 11 datasets, we establish a clear trend: deployed machine learning is prone to systemic failure, meaning some users are exclusively misclassified by all models available. Even when individual models improve at the population level over time, we fin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRO&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#31639;&#27861;&#23558;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20559;&#22909;&#21464;&#21270;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05857</link><description>&lt;p&gt;
FAIRO: &#38754;&#21521;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#39034;&#24207;&#20915;&#31574;&#30340;&#20844;&#24179;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems. (arXiv:2307.05857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05857
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRO&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#31639;&#27861;&#23558;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20559;&#22909;&#21464;&#21270;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#39034;&#24207;&#20915;&#31574;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#21644;&#26399;&#26395;&#30340;&#20154;&#21463;&#21040;&#31995;&#32479;&#20013;&#30456;&#21516;&#36866;&#24212;&#20915;&#31574;&#30340;&#24433;&#21709;&#26102;&#12290;&#20154;&#30340;&#21487;&#21464;&#24615;&#22240;&#32032;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#22312;&#26576;&#19968;&#26102;&#38388;&#28857;&#34987;&#35748;&#20026;&#26159;&#20844;&#24179;&#30340;&#25919;&#31574;&#21487;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#30001;&#20110;&#20154;&#30340;&#20559;&#22909;&#21464;&#21270;&#32780;&#25104;&#20026;&#27495;&#35270;&#24615;&#25919;&#31574;&#12290;&#26412;&#25991;&#20174;&#20844;&#24179;&#24615;&#35270;&#35282;&#32771;&#34385;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20154;&#30340;&#20559;&#22909;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;FAIRO&#65292;&#29992;&#20110;&#38754;&#21521;&#20154;&#26426;&#20132;&#20114;&#36866;&#24212;&#30340;&#20844;&#24179;&#39034;&#24207;&#20915;&#31574;&#65292;&#23427;&#23558;&#36825;&#20123;&#27010;&#24565;&#32435;&#20837;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FAIRO&#23558;&#36825;&#20010;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#22522;&#20110;&#20010;&#20307;&#20154;&#30340;&#20559;&#22909;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving fairness in sequential-decision making systems within Human-in-the-Loop (HITL) environments is a critical concern, especially when multiple humans with different behavior and expectations are affected by the same adaptation decisions in the system. This human variability factor adds more complexity since policies deemed fair at one point in time may become discriminatory over time due to variations in human preferences resulting from inter- and intra-human variability. This paper addresses the fairness problem from an equity lens, considering human behavior variability, and the changes in human preferences over time. We propose FAIRO, a novel algorithm for fairness-aware sequential-decision making in HITL adaptation, which incorporates these notions into the decision-making process. In particular, FAIRO decomposes this complex fairness task into adaptive sub-tasks based on individual human preferences through leveraging the Options reinforcement learning framework. We design 
&lt;/p&gt;</description></item><item><title>PIGEON&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#31934;&#21270;&#65292;&#20197;&#21450;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;StreetCLIP&#12290;</title><link>http://arxiv.org/abs/2307.05845</link><description>&lt;p&gt;
PIGEON: &#39044;&#27979;&#22270;&#20687;&#22320;&#29702;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05845
&lt;/p&gt;
&lt;p&gt;
PIGEON&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#31934;&#21270;&#65292;&#20197;&#21450;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;StreetCLIP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;PIGEON&#65292;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#22312;&#22806;&#37096;&#22522;&#20934;&#27979;&#35797;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32467;&#21512;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#26631;&#31614;&#24179;&#28369;&#65292;&#23545;&#20855;&#26377;&#22320;&#29702;&#20449;&#24687;&#30340;&#22270;&#20687;&#36827;&#34892;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;ProtoNets&#22312;&#20505;&#36873;&#22320;&#29702;&#21333;&#20803;&#38598;&#21512;&#20013;&#25913;&#36827;&#20301;&#32622;&#39044;&#27979;&#12290;PIGEON&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;&#25968;&#25454;&#30340;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#21019;&#24314;&#21644;&#20998;&#21106;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22320;&#29702;&#21333;&#20803;&#20869;&#37096;&#31934;&#21270;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;StreetCLIP&#65292;&#20844;&#24320;&#25552;&#20379;&#65292;&#21487;&#29992;&#20110;&#19982;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#22478;&#24066;&#20065;&#26449;&#22330;&#26223;&#29702;&#35299;&#30456;&#20851;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation. Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells. The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset. Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task. Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;&#20849;&#20139;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;DistMT-LSVI&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;DistMT-LSVI&#30340;&#21333;&#20010;&#20195;&#29702;&#21487;&#20197;&#23454;&#29616;&#25152;&#26377;&#20219;&#21153;&#30340;&#949;-&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.05834</link><description>&lt;p&gt;
&#29992;&#32463;&#39564;&#20849;&#20139;&#26469;&#25193;&#23637;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing. (arXiv:2307.05834v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;&#20849;&#20139;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;DistMT-LSVI&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;DistMT-LSVI&#30340;&#21333;&#20010;&#20195;&#29702;&#21487;&#20197;&#23454;&#29616;&#25152;&#26377;&#20219;&#21153;&#30340;&#949;-&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;DARPA&#25512;&#20986;&#20102;ShELL&#35745;&#21010;&#65292;&#26088;&#22312;&#25506;&#32034;&#32463;&#39564;&#20849;&#20139;&#22914;&#20309;&#20351;&#20998;&#24067;&#24335;&#32456;&#36523;&#23398;&#20064;&#20195;&#29702;&#22312;&#36866;&#24212;&#26032;&#25361;&#25112;&#26041;&#38754;&#21463;&#30410;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#32452;N&#20010;&#20195;&#29702;&#21327;&#20316;&#35299;&#20915;&#20102;M&#20010;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#32447;&#24615;&#21442;&#25968;&#21270;&#30340;&#19978;&#19979;&#25991;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#30001;&#25351;&#23450;&#36716;&#31227;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DistMT-LSVI&#30340;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#20195;&#29702;&#35782;&#21035;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#20132;&#25442;&#20449;&#24687;&#65292;&#20026;&#20219;&#21153;&#23548;&#20986;&#949;-&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;&#20026;&#20102;&#23454;&#29616;&#25152;&#26377;M&#20010;&#20219;&#21153;&#30340;&#949;-&#26368;&#20248;&#31574;&#30053;&#65292;&#20351;&#29992;DistMT-LSVI&#30340;&#21333;&#20010;&#20195;&#29702;&#38656;&#35201;&#36816;&#34892;&#19968;&#23450;&#25968;&#37327;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, DARPA launched the ShELL program, which aims to explore how experience sharing can benefit distributed lifelong learning agents in adapting to new challenges. In this paper, we address this issue by conducting both theoretical and empirical research on distributed multi-task reinforcement learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks without prior knowledge of their identities. We approach the problem by formulating it as linearly parameterized contextual Markov decision processes (MDPs), where each task is represented by a context that specifies the transition dynamics and rewards. To tackle this problem, we propose an algorithm called DistMT-LSVI. First, the agents identify the tasks, and then they exchange information through a central server to derive $\epsilon$-optimal policies for the tasks. Our research demonstrates that to achieve $\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI needs to run a total number o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#26679;&#26412;&#19978;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#26354;&#29575;&#30340;&#26679;&#26412;&#36890;&#24120;&#26159;&#20855;&#26377;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#30340;&#38271;&#23614;&#26679;&#26412;&#65292;&#24182;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#26679;&#26412;&#36827;&#34892;&#38543;&#26426;&#26631;&#31614;&#38169;&#35823;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20986;&#36825;&#20123;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.05831</link><description>&lt;p&gt;
&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#35270;&#35282;&#25581;&#31034;&#35760;&#24518;&#21270;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Memorization Through the Lens of Curvature of Loss Function Around Samples. (arXiv:2307.05831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#26679;&#26412;&#19978;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#26354;&#29575;&#30340;&#26679;&#26412;&#36890;&#24120;&#26159;&#20855;&#26377;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#30340;&#38271;&#23614;&#26679;&#26412;&#65292;&#24182;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#26679;&#26412;&#36827;&#34892;&#38543;&#26426;&#26631;&#31614;&#38169;&#35823;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20986;&#36825;&#20123;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#36807;&#22810;&#65292;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#21487;&#20197;&#23436;&#20840;&#35760;&#24518;&#35757;&#32451;&#38598;&#65292;&#21363;&#20351;&#26631;&#31614;&#26159;&#38543;&#26426;&#30340;&#12290;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#35757;&#32451;&#26679;&#26412;&#21608;&#22260;&#30340;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#20316;&#20026;&#35760;&#24518;&#21270;&#31243;&#24230;&#30340;&#24230;&#37327;&#65292;&#23545;&#25152;&#26377;&#35757;&#32451;&#36718;&#27425;&#36827;&#34892;&#24179;&#22343;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26469;&#30740;&#31350;&#24120;&#35265;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#26679;&#26412;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20855;&#26377;&#26368;&#39640;&#25439;&#22833;&#26354;&#29575;&#30340;&#26679;&#26412;&#65292;&#21457;&#29616;&#23427;&#20204;&#36890;&#24120;&#26159;&#38271;&#23614;&#26679;&#26412;&#12289;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#26679;&#26412;&#12290;&#36825;&#31181;&#20998;&#26512;&#24110;&#21161;&#25105;&#20204;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#37325;&#22797;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#38543;&#26426;&#38169;&#35823;&#21270;&#23569;&#37327;&#26679;&#26412;&#30340;&#26631;&#31614;&#26469;&#20154;&#20026;&#22320;&#32473;&#25968;&#25454;&#38598;&#24341;&#20837;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#23637;&#31034;&#20102;&#25353;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#20986;&#26631;&#31614;&#38169;&#35823;&#26679;&#26412;&#30340;&#39640;AUROC&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are overparametrized and easily overfit the datasets they train on. In the extreme case, it is shown that they can memorize a training set with fully randomized labels. We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs. We use this to study the generalization versus memorization properties of different samples in popular image datasets. We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples. This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels. We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#65292;&#22312;&#32500;&#22522;&#30334;&#31185;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#12290;&#35813;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#32463;&#36807;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21508;&#20010;&#27169;&#22411;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.05827</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#22312;&#32500;&#22522;&#30334;&#31185;&#34920;&#26684;&#19978;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks. (arXiv:2307.05827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05827
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#65292;&#22312;&#32500;&#22522;&#30334;&#31185;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#12290;&#35813;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#32463;&#36807;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21508;&#20010;&#27169;&#22411;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#22823;&#37096;&#20998;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#20174;&#33258;&#30001;&#26684;&#24335;&#30340;&#36830;&#32493;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#20016;&#23500;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#27604;&#22914;&#34920;&#26684;&#12290;&#25105;&#20204;&#20174;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22788;&#29702;&#34920;&#26684;&#21270;&#25968;&#25454;&#30340;&#35282;&#24230;&#25506;&#32034;&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#30001;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;BiLSTM&#65289;&#32593;&#32476;&#32452;&#25104;&#65292;&#20998;&#21035;&#29992;&#20110;&#32534;&#30721;&#23454;&#20307;&#21644;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;&#20043;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38169;&#35823;&#20998;&#26512;&#21644;&#21093;&#31163;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#36129;&#29486;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) is the task of extracting relations between entities in text. Most RE methods extract relations from free-form running text and leave out other rich data sources, such as tables. We explore RE from the perspective of applying neural methods on tabularly organized data. We introduce a new model consisting of Convolutional Neural Network (CNN) and Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and learn dependencies among them, respectively. We evaluate our model on a large and recent dataset and compare results with previous neural methods. Experimental results show that our model consistently outperforms the previous model for the task of relation extraction on tabular data. We perform comprehensive error analyses and ablation study to show the contribution of various components of our model. Finally, we discuss the usefulness and trade-offs of our approach, and provide suggestions for fostering further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26680;&#20272;&#35745;&#22120;&#21644;&#32452;&#21512;&#26679;&#26465;&#65292;&#23454;&#29616;&#20102;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05825</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#20272;&#35745;&#27169;&#24335;&#30340;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Bayesian taut splines for estimating the number of modes. (arXiv:2307.05825v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#32039;&#31995;&#25968;&#26679;&#26465;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26680;&#20272;&#35745;&#22120;&#21644;&#32452;&#21512;&#26679;&#26465;&#65292;&#23454;&#29616;&#20102;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#27169;&#24335;&#30340;&#25968;&#37327;&#20195;&#34920;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#20063;&#21487;&#20197;&#30475;&#20316;&#29616;&#26377;&#20122;&#32676;&#20307;&#30340;&#25968;&#37327;&#12290;&#23613;&#31649;&#20854;&#30456;&#20851;&#24615;&#65292;&#23545;&#20854;&#20272;&#35745;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#38024;&#23545;&#21333;&#21464;&#37327;&#24773;&#20917;&#25552;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#33268;&#21147;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21463;&#21040;&#20102;&#38382;&#39064;&#30340;&#19968;&#20123;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#32467;&#26500;&#65292;&#27169;&#24335;&#30340;&#20027;&#35266;&#19988;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#23494;&#24230;&#29305;&#24615;&#30340;&#25972;&#20307;&#35270;&#22270;&#30340;&#20415;&#21033;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#26680;&#20272;&#35745;&#22120;&#21644;&#31616;&#27905;&#30340;&#32452;&#21512;&#26679;&#26465;&#12290;&#29305;&#24449;&#25506;&#32034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#24335;&#26816;&#39564;&#37117;&#22312;&#36125;&#21494;&#26031;&#25512;&#29702;&#33539;&#24335;&#20013;&#23454;&#29616;&#65292;&#20026;&#36719;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#24182;&#20801;&#35768;&#22312;&#36807;&#31243;&#20013;&#24341;&#20837;&#19987;&#23478;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#22312;&#20307;&#32946;&#20998;&#26512;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#38506;&#20276;&#30340;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of modes in a probability density function is representative of the model's complexity and can also be viewed as the number of existing subpopulations. Despite its relevance, little research has been devoted to its estimation. Focusing on the univariate setting, we propose a novel approach targeting prediction accuracy inspired by some overlooked aspects of the problem. We argue for the need for structure in the solutions, the subjective and uncertain nature of modes, and the convenience of a holistic view blending global and local density properties. Our method builds upon a combination of flexible kernel estimators and parsimonious compositional splines. Feature exploration, model selection and mode testing are implemented in the Bayesian inference paradigm, providing soft solutions and allowing to incorporate expert judgement in the process. The usefulness of our proposal is illustrated through a case study in sports analytics, showcasing multiple companion visualisation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#34394;&#25311;&#30005;&#21147;&#21378;&#22312;&#26085;&#21069;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#31454;&#26631;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#31934;&#30830;&#30340;&#24066;&#22330;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#23398;&#20064;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31454;&#26631;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#23631;&#34109;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#24809;&#32602;&#26426;&#21046;&#26469;&#32771;&#34385;&#34394;&#25311;&#30005;&#21147;&#21378;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#23433;&#20840;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.05812</link><description>&lt;p&gt;
&#34394;&#25311;&#30005;&#21147;&#21378;&#22312;&#26085;&#21069;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#31454;&#26631;&#30340;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning for Strategic Bidding of Virtual Power Plants in Day-Ahead Markets. (arXiv:2307.05812v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#34394;&#25311;&#30005;&#21147;&#21378;&#22312;&#26085;&#21069;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#31454;&#26631;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#31934;&#30830;&#30340;&#24066;&#22330;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#23398;&#20064;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31454;&#26631;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#23631;&#34109;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#24809;&#32602;&#26426;&#21046;&#26469;&#32771;&#34385;&#34394;&#25311;&#30005;&#21147;&#21378;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#21040;&#26356;&#23433;&#20840;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#34394;&#25311;&#30005;&#21147;&#21378;&#22312;&#26085;&#21069;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#31454;&#26631;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#37319;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#26041;&#27861;&#65292;&#23398;&#20064;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31454;&#26631;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#31934;&#30830;&#30340;&#24066;&#22330;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32771;&#34385;&#34394;&#25311;&#30005;&#21147;&#21378;&#30340;&#22797;&#26434;&#20869;&#37096;&#29289;&#29702;&#32422;&#26463;&#65292;&#25105;&#20204;&#23545;DDPG&#26041;&#27861;&#36827;&#34892;&#20102;&#20004;&#20010;&#22686;&#24378;&#12290;&#39318;&#20808;&#65292;&#25512;&#23548;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#24433;&#30340;&#23433;&#20840;&#23631;&#34109;&#65292;&#23558;&#20195;&#29702;&#30340;&#34892;&#20026;&#38480;&#21046;&#22312;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#30340;&#38750;&#32447;&#24615;&#21151;&#29575;&#27969;&#26041;&#31243;&#21644;&#36816;&#34892;&#32422;&#26463;&#25152;&#23450;&#20041;&#30340;&#21487;&#34892;&#31354;&#38388;&#20869;&#12290;&#20854;&#27425;&#65292;&#24341;&#20837;&#20102;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#23631;&#34109;&#28608;&#27963;&#24809;&#32602;&#65292;&#40723;&#21169;&#20195;&#29702;&#23398;&#20064;&#26356;&#23433;&#20840;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;IEEE 13-bus&#32593;&#32476;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#20351;&#20195;&#29702;&#23398;&#20064;&#21040;&#39640;&#31454;&#20105;&#21147;&#65292;&#23433;&#20840;&#30340;&#25112;&#30053;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel safe reinforcement learning algorithm for strategic bidding of Virtual Power Plants (VPPs) in day-ahead electricity markets. The proposed algorithm utilizes the Deep Deterministic Policy Gradient (DDPG) method to learn competitive bidding policies without requiring an accurate market model. Furthermore, to account for the complex internal physical constraints of VPPs we introduce two enhancements to the DDPG method. Firstly, a projection-based safety shield that restricts the agent's actions to the feasible space defined by the non-linear power flow equations and operating constraints of distributed energy resources is derived. Secondly, a penalty for the shield activation in the reward function that incentivizes the agent to learn a safer policy is introduced. A case study based on the IEEE 13-bus network demonstrates the effectiveness of the proposed approach in enabling the agent to learn a highly competitive, safe strategic policy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#30340;&#21487;&#24494;&#20998;&#21069;&#21521;&#21644;&#21518;&#21521;&#25237;&#24433;&#36719;&#20214;&#24211;&#65292;&#20197;&#30830;&#20445;&#39044;&#27979;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#27979;&#37327;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#25237;&#24433;&#20960;&#20309;&#31867;&#22411;&#65292;&#24182;&#19988;&#26368;&#23567;&#21270;&#20102;GPU&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.05801</link><description>&lt;p&gt;
X&#23556;&#32447;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#21487;&#24494;&#20998;&#21069;&#21521;&#25237;&#24433;&#22120;
&lt;/p&gt;
&lt;p&gt;
Differentiable Forward Projector for X-ray Computed Tomography. (arXiv:2307.05801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#30340;&#21487;&#24494;&#20998;&#21069;&#21521;&#21644;&#21518;&#21521;&#25237;&#24433;&#36719;&#20214;&#24211;&#65292;&#20197;&#30830;&#20445;&#39044;&#27979;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#27979;&#37327;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#25237;&#24433;&#20960;&#20309;&#31867;&#22411;&#65292;&#24182;&#19988;&#26368;&#23567;&#21270;&#20102;GPU&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#37325;&#24314;&#38382;&#39064;&#12290;&#28145;&#24230;&#25512;&#26029;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#30149;&#24577;CT&#37325;&#24314;&#20013;&#32988;&#36807;&#29616;&#26377;&#30340;&#35299;&#26512;&#21644;&#36845;&#20195;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#39044;&#27979;&#30340;&#22270;&#20687;&#19982;&#27979;&#37327;&#30340;&#25237;&#24433;&#25968;&#25454;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#30340;&#21487;&#24494;&#20998;&#21069;&#21521;&#21644;&#21518;&#21521;&#25237;&#24433;&#36719;&#20214;&#24211;&#65292;&#20197;&#30830;&#20445;&#39044;&#27979;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#27979;&#37327;&#20540;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#35813;&#36719;&#20214;&#24211;&#26377;&#25928;&#22320;&#25903;&#25345;&#21508;&#31181;&#25237;&#24433;&#20960;&#20309;&#31867;&#22411;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;GPU&#20869;&#23384;&#21344;&#29992;&#65292;&#26377;&#21033;&#20110;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#21644;&#25512;&#26029;&#27969;&#31243;&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#36719;&#20214;&#24211;&#21487;&#20197;&#20316;&#20026;&#24320;&#28304;&#24037;&#20855;&#26469;&#20351;&#29992;&#65306;https://github.com/LLNL/LEAP&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven deep learning has been successfully applied to various computed tomographic reconstruction problems. The deep inference models may outperform existing analytical and iterative algorithms, especially in ill-posed CT reconstruction. However, those methods often predict images that do not agree with the measured projection data. This paper presents an accurate differentiable forward and back projection software library to ensure the consistency between the predicted images and the original measurements. The software library efficiently supports various projection geometry types while minimizing the GPU memory footprint requirement, which facilitates seamless integration with existing deep learning training and inference pipelines. The proposed software is available as open source: https://github.com/LLNL/LEAP.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#19982;&#30140;&#30171;&#30456;&#20851;&#30340;&#38048;&#36890;&#36947;&#26500;&#24314;&#20102;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#65292;&#24182;&#24320;&#21457;&#20102;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#65292;&#20197;&#21457;&#29616;&#30140;&#30171;&#31649;&#29702;&#30340;&#28508;&#22312;&#39318;&#36873;&#21270;&#21512;&#29289;&#12290;</title><link>http://arxiv.org/abs/2307.05794</link><description>&lt;p&gt;
&#22522;&#20110;&#19982;&#30140;&#30171;&#30456;&#20851;&#30340;&#30005;&#21387;&#38376;&#25511;&#38048;&#36890;&#36947;&#30340;&#25193;&#23637;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Study of the Extended Drug-target Interaction Network informed by Pain Related Voltage-Gated Sodium Channels. (arXiv:2307.05794v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#19982;&#30140;&#30171;&#30456;&#20851;&#30340;&#38048;&#36890;&#36947;&#26500;&#24314;&#20102;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#65292;&#24182;&#24320;&#21457;&#20102;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#65292;&#20197;&#21457;&#29616;&#30140;&#30171;&#31649;&#29702;&#30340;&#28508;&#22312;&#39318;&#36873;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30140;&#30171;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20840;&#29699;&#20581;&#24247;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#30140;&#30171;&#31649;&#29702;&#27835;&#30103;&#36873;&#25321;&#22312;&#25928;&#26524;&#12289;&#21103;&#20316;&#29992;&#21644;&#28508;&#22312;&#25104;&#30270;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25913;&#36827;&#30140;&#30171;&#27835;&#30103;&#21644;&#24320;&#21457;&#26032;&#33647;&#29289;&#30340;&#38656;&#27714;&#36843;&#20999;&#12290;&#30005;&#21387;&#38376;&#25511;&#38048;&#36890;&#36947;&#65292;&#29305;&#21035;&#26159;Nav1.3&#12289;Nav1.7&#12289;Nav1.8&#21644;Nav1.9&#65292;&#22312;&#31070;&#32463;&#20852;&#22859;&#24615;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#20027;&#35201;&#34920;&#36798;&#20110;&#22806;&#21608;&#31070;&#32463;&#31995;&#32479;&#20013;&#12290;&#38774;&#21521;&#36825;&#20123;&#36890;&#36947;&#21487;&#33021;&#25552;&#20379;&#27835;&#30103;&#30140;&#30171;&#30340;&#25163;&#27573;&#65292;&#21516;&#26102;&#20943;&#23569;&#20013;&#22830;&#21644;&#24515;&#33039;&#19981;&#33391;&#21453;&#24212;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#19982;&#30140;&#30171;&#30456;&#20851;&#30340;&#38048;&#36890;&#36947;&#26500;&#24314;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;PPI&#65289;&#32593;&#32476;&#65292;&#24182;&#24320;&#21457;&#30456;&#24212;&#30340;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#65288;DTI&#65289;&#32593;&#32476;&#65292;&#20197;&#23547;&#25214;&#30140;&#30171;&#31649;&#29702;&#30340;&#28508;&#22312;&#39318;&#36873;&#21270;&#21512;&#29289;&#12290;&#20026;&#30830;&#20445;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#65292;&#25105;&#20204;&#20174;&#36229;&#36807;1,000&#20010;&#30446;&#26631;&#30340;PPI&#32593;&#32476;&#20013;&#31934;&#36873;&#20102;111&#20010;&#25233;&#21046;&#21058;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pain is a significant global health issue, and the current treatment options for pain management have limitations in terms of effectiveness, side effects, and potential for addiction. There is a pressing need for improved pain treatments and the development of new drugs. Voltage-gated sodium channels, particularly Nav1.3, Nav1.7, Nav1.8, and Nav1.9, play a crucial role in neuronal excitability and are predominantly expressed in the peripheral nervous system. Targeting these channels may provide a means to treat pain while minimizing central and cardiac adverse effects. In this study, we construct protein-protein interaction (PPI) networks based on pain-related sodium channels and develop a corresponding drug-target interaction (DTI) network to identify potential lead compounds for pain management. To ensure reliable machine learning predictions, we carefully select 111 inhibitor datasets from a pool of over 1,000 targets in the PPI network. We employ three distinct machine learning alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21521;&#21518;&#35823;&#24046;&#20998;&#26512;&#26500;&#24314;&#36830;&#32493;&#26102;&#38388;&#27969;&#37327;&#26469;&#37327;&#21270;&#31163;&#25955;&#20248;&#21270;&#22120;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;BEA&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.05789</link><description>&lt;p&gt;
&#38544;&#24335;&#27491;&#21017;&#21270;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#24212;&#29992;&#65306;&#20174;&#21333;&#30446;&#26631;&#21040;&#21452;&#20154;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Implicit regularisation in stochastic gradient descent: from single-objective to two-player games. (arXiv:2307.05789v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#21521;&#21518;&#35823;&#24046;&#20998;&#26512;&#26500;&#24314;&#36830;&#32493;&#26102;&#38388;&#27969;&#37327;&#26469;&#37327;&#21270;&#31163;&#25955;&#20248;&#21270;&#22120;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;BEA&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#21457;&#29616;&#24120;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#24102;&#26469;&#20102;&#35768;&#22810;&#26032;&#30340;&#35265;&#35299;&#12290;&#29702;&#35299;&#38544;&#24335;&#27491;&#21017;&#21270;&#19981;&#20165;&#21487;&#20197;&#25581;&#31034;&#20248;&#21270;&#21160;&#24577;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#28041;&#21450;&#21040;&#20174;&#26377;&#30417;&#30563;&#23398;&#20064;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31561;&#38382;&#39064;&#39046;&#22495;&#30340;&#20004;&#20154;&#28216;&#25103;&#12290;&#36890;&#36807;&#21521;&#21518;&#35823;&#24046;&#20998;&#26512;&#65288;BEA&#65289;&#26500;&#24314;&#30340;&#36830;&#32493;&#26102;&#38388;&#27969;&#37327;&#26469;&#37327;&#21270;&#31163;&#25955;&#20248;&#21270;&#22120;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#26159;&#25214;&#21040;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#24212;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;BEA&#30340;&#20351;&#29992;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#24182;&#19981;&#26159;&#36890;&#36807;BEA&#33719;&#24471;&#30340;&#25152;&#26377;&#36830;&#32493;&#26102;&#38388;&#27969;&#30340;&#21521;&#37327;&#22330;&#37117;&#21487;&#20197;&#20889;&#25104;&#26799;&#24230;&#65292;&#36825;&#38459;&#30861;&#20102;&#26500;&#24314;&#25581;&#31034;&#38544;&#24335;&#27491;&#21017;&#21270;&#22120;&#30340;&#20462;&#27491;&#25439;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;BEA&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#29992;&#20110;&#26500;&#24314;&#36830;&#32493;&#26102;&#38388;&#27969;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen many insights on deep learning optimisation being brought forward by finding implicit regularisation effects of commonly used gradient-based optimisers. Understanding implicit regularisation can not only shed light on optimisation dynamics, but it can also be used to improve performance and stability across problem domains, from supervised learning to two-player games such as Generative Adversarial Networks. An avenue for finding such implicit regularisation effects has been quantifying the discretisation errors of discrete optimisers via continuous-time flows constructed by backward error analysis (BEA). The current usage of BEA is not without limitations, since not all the vector fields of continuous-time flows obtained using BEA can be written as a gradient, hindering the construction of modified losses revealing implicit regularisers. In this work, we provide a novel approach to use BEA, and show how our approach can be used to construct continuous-time flows
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Nystr&#246;m&#26041;&#27861;&#29992;&#20110;&#20302;&#31209;&#36924;&#36817;&#30340;&#39640;&#31934;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#21551;&#21457;&#24335;&#31574;&#30053;&#20351;&#24471;&#35813;&#26041;&#27861;&#33021;&#22815;&#36924;&#36817;&#38750;&#23545;&#31216;&#21644;&#38271;&#23485;&#27604;&#19981;&#21516;&#30340;&#30697;&#38453;&#65292;&#24182;&#25552;&#20986;&#20102;&#24555;&#36895;&#23376;&#38598;&#26356;&#26032;&#31574;&#30053;&#21152;&#36895;&#32454;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.05785</link><description>&lt;p&gt;
&#23558;Nystr&#246;m&#26041;&#27861;&#29992;&#20110;&#20302;&#31209;&#36924;&#36817;&#30340;&#39640;&#31934;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Making the Nystr\"om method highly accurate for low-rank approximations. (arXiv:2307.05785v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Nystr&#246;m&#26041;&#27861;&#29992;&#20110;&#20302;&#31209;&#36924;&#36817;&#30340;&#39640;&#31934;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#21551;&#21457;&#24335;&#31574;&#30053;&#20351;&#24471;&#35813;&#26041;&#27861;&#33021;&#22815;&#36924;&#36817;&#38750;&#23545;&#31216;&#21644;&#38271;&#23485;&#27604;&#19981;&#21516;&#30340;&#30697;&#38453;&#65292;&#24182;&#25552;&#20986;&#20102;&#24555;&#36895;&#23376;&#38598;&#26356;&#26032;&#31574;&#30053;&#21152;&#36895;&#32454;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Nystr&#246;m&#26041;&#27861;&#26159;&#19968;&#31181;&#26041;&#20415;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#36817;&#32447;&#24615;&#30340;&#22797;&#26434;&#24230;&#33719;&#24471;&#20302;&#31209;&#36924;&#36817;&#30340;&#26680;&#30697;&#38453;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#35813;&#26041;&#27861;&#23545;&#20302;&#25110;&#20013;&#31561;&#31934;&#24230;&#30340;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#36924;&#36817;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#20351;&#24471;Nystr&#246;m&#26041;&#27861;&#33021;&#22815;&#23545;&#38750;&#23545;&#31216;&#21644;/&#25110;&#38271;&#23485;&#27604;&#19981;&#21516;&#30340;&#30697;&#38453;&#23454;&#29616;&#39640;&#31934;&#24230;&#12290;&#25152;&#24471;&#21040;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;&#39640;&#31934;&#24230;Nystr&#246;m&#26041;&#27861;&#65289;&#23558;Nystr&#246;m&#26041;&#27861;&#21644;&#30246;&#31209;&#26174;&#38706;&#20998;&#35299;&#36827;&#34892;&#20102;&#24555;&#36895;&#30340;&#26530;&#36724;&#31574;&#30053;&#65292;&#22312;&#28176;&#36827;&#20132;&#26367;&#26041;&#21521;&#32454;&#21270;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#20004;&#31181;&#32454;&#21270;&#26426;&#21046;&#65306;&#20174;&#38543;&#26426;&#36873;&#25321;&#30340;&#23569;&#25968;&#21015;&#24320;&#22987;&#20132;&#26367;&#34892;&#21644;&#21015;&#26530;&#36724;&#65292;&#24182;&#33258;&#36866;&#24212;&#22686;&#21152;&#26679;&#26412;&#25968;&#37327;&#65292;&#30452;&#21040;&#36798;&#21040;&#25152;&#38656;&#30340;&#31209;&#25110;&#31934;&#24230;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#27493;&#37319;&#26679;Schur&#34917;&#30340;&#24555;&#36895;&#23376;&#38598;&#26356;&#26032;&#31574;&#30053;&#65292;&#20197;&#21152;&#36895;&#32454;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Nystr\"om method is a convenient heuristic method to obtain low-rank approximations to kernel matrices in nearly linear complexity. Existing studies typically use the method to approximate positive semidefinite matrices with low or modest accuracies. In this work, we propose a series of heuristic strategies to make the Nystr\"om method reach high accuracies for nonsymmetric and/or rectangular matrices. The resulting methods (called high-accuracy Nystr\"om methods) treat the Nystr\"om method and a skinny rank-revealing factorization as a fast pivoting strategy in a progressive alternating direction refinement process. Two refinement mechanisms are used: alternating the row and column pivoting starting from a small set of randomly chosen columns, and adaptively increasing the number of samples until a desired rank or accuracy is reached. A fast subset update strategy based on the progressive sampling of Schur complements is further proposed to accelerate the refinement process. Effic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;$k$-WL&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22806;&#24310;&#23450;&#20041;&#21644;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.05775</link><description>&lt;p&gt;
Weisfeiler&#21644;Lehman&#36827;&#34892;&#24230;&#37327;&#24314;&#27169;&#65306;&#25506;&#32034;WL&#27979;&#35797;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of the WL Test. (arXiv:2307.05775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;$k$-WL&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22806;&#24310;&#23450;&#20041;&#21644;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#27604;&#36739;&#19968;&#20010;&#26550;&#26500;&#33021;&#22815;&#21306;&#20998;&#30340;&#38750;&#21516;&#26500;&#22270;&#25110;&#33410;&#28857;&#23545;&#30340;&#25968;&#37327;&#19982;$k$-&#32500;Weisfeiler-Lehman ($k$-WL)&#27979;&#35797;&#33021;&#22815;&#21306;&#20998;&#30340;&#25968;&#37327;&#26469;&#34913;&#37327;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;$k$-WL&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20174;&#19994;&#32773;&#23545;&#34920;&#36798;&#33021;&#21147;&#21644;$k$-WL&#30340;&#27010;&#24565;&#21270;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#23545;&#20174;&#19994;&#32773;&#30340;&#35843;&#26597;&#65288;n=18&#65289;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#27010;&#24565;&#20197;&#21450;&#23545;$k$-WL&#30340;&#20551;&#35774;&#12290;&#19982;&#20174;&#19994;&#32773;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#65288;&#20511;&#37492;&#20102;&#22270;&#35770;&#21644;&#22522;&#20934;&#23457;&#26680;&#65289;&#25581;&#31034;&#20102;$k$-WL&#19981;&#33021;&#20445;&#35777;&#31561;&#36317;&#12289;&#21487;&#33021;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20219;&#21153;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#20419;&#36827;&#27867;&#21270;&#25110;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#20027;&#24352;&#22522;&#20110;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22806;&#24310;&#23450;&#20041;&#21644;&#27979;&#37327;&#65292;&#36827;&#19968;&#27493;&#36129;&#29486;&#20102;&#26500;&#24314;&#27492;&#31867;&#22522;&#20934;&#30340;&#25351;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expressive power of graph neural networks is usually measured by comparing how many pairs of graphs or nodes an architecture can possibly distinguish as non-isomorphic to those distinguishable by the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test. In this paper, we uncover misalignments between practitioners' conceptualizations of expressive power and $k$-WL through a systematic analysis of the reliability and validity of $k$-WL. We further conduct a survey ($n = 18$) of practitioners to surface their conceptualizations of expressive power and their assumptions about $k$-WL. In contrast to practitioners' opinions, our analysis (which draws from graph theory and benchmark auditing) reveals that $k$-WL does not guarantee isometry, can be irrelevant to real-world graph tasks, and may not promote generalization or trustworthiness. We argue for extensional definitions and measurement of expressive power based on benchmarks; we further contribute guiding questions for constructing such 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05772</link><description>&lt;p&gt;
&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (arXiv:2307.05772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#38169;&#35823;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36825;&#31361;&#20986;&#20102;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#33021;&#22815;&#30830;&#23450;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25163;&#27573;&#65292;&#8220;&#30693;&#36947;&#19968;&#20010;&#27169;&#22411;&#19981;&#30693;&#36947;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#65292;&#20854;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20351;&#29992;&#38543;&#26426;&#38598;&#21512;&#30340;&#25968;&#23398;&#65292;&#21363;&#23545;&#26679;&#26412;&#31354;&#38388;&#30340;&#24130;&#38598;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#38543;&#26426;&#38598;&#27169;&#22411;&#33021;&#22815;&#34920;&#31034;&#26426;&#22120;&#23398;&#20064;&#20013;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#8220;&#35748;&#35782;&#24615;&#8221;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#36817;&#20284;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#30456;&#20851;&#32852;&#30340;&#32622;&#20449;&#38598;&#30340;&#22823;&#23567;&#26469;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is increasingly deployed in safety-critical domains where robustness against adversarial attacks is crucial and erroneous predictions could lead to potentially catastrophic consequences. This highlights the need for learning systems to be equipped with the means to determine a model's confidence in its prediction and the epistemic uncertainty associated with it, 'to know when a model does not know'. In this paper, we propose a novel Random-Set Convolutional Neural Network (RS-CNN) for classification which predicts belief functions rather than probability vectors over the set of classes, using the mathematics of random sets, i.e., distributions over the power set of the sample space. Based on the epistemic deep learning approach, random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets. We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief func
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#32447;&#24615;&#39057;&#29575;&#35843;&#35856;&#20316;&#20026;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#22312;&#23454;&#26102;&#39057;&#35889;&#30417;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#34920;&#26126;&#22312;&#38750;&#22343;&#21248;&#20449;&#21495;&#27963;&#21160;&#30340;&#22330;&#26223;&#19979;&#65292;&#20351;&#29992;Q&#23398;&#20064;&#31639;&#27861;&#30340;&#26816;&#27979;&#29575;&#26174;&#33879;&#39640;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.05763</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#23454;&#26102;&#39057;&#35889;&#30417;&#27979;--Q&#23398;&#20064;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Realtime Spectrum Monitoring via Reinforcement Learning -- A Comparison Between Q-Learning and Heuristic Methods. (arXiv:2307.05763v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#32447;&#24615;&#39057;&#29575;&#35843;&#35856;&#20316;&#20026;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#22312;&#23454;&#26102;&#39057;&#35889;&#30417;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#34920;&#26126;&#22312;&#38750;&#22343;&#21248;&#20449;&#21495;&#27963;&#21160;&#30340;&#22330;&#26223;&#19979;&#65292;&#20351;&#29992;Q&#23398;&#20064;&#31639;&#27861;&#30340;&#26816;&#27979;&#29575;&#26174;&#33879;&#39640;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26080;&#32447;&#30005;&#25216;&#26415;&#30340;&#25216;&#26415;&#36827;&#27493;&#21644;&#21487;&#29992;&#24615;&#65292;&#26080;&#32447;&#30005;&#39057;&#35889;&#20013;&#24178;&#25200;&#20449;&#21495;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#12290;&#20026;&#20102;&#20445;&#25345;&#26631;&#20934;&#24182;&#20445;&#25345;&#32039;&#24613;&#39057;&#29575;&#30340;&#24320;&#25918;&#65292;&#24517;&#39035;&#21450;&#26102;&#26816;&#27979;&#24178;&#25200;&#20449;&#21495;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#19987;&#38376;&#30340;&#65288;&#22810;&#36890;&#36947;&#65289;&#25509;&#25910;&#22120;&#36827;&#34892;&#39057;&#35889;&#30417;&#27979;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#21487;&#29992;&#25509;&#25910;&#22120;&#36164;&#28304;&#30340;&#24615;&#33021;&#12290;&#36164;&#28304;&#31649;&#29702;&#65288;ReMa&#65289;&#25152;&#20351;&#29992;&#30340;&#26041;&#27861;&#26159;&#32447;&#24615;&#39057;&#29575;&#35843;&#35856;&#20316;&#20026;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#26469;&#33258;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#27979;&#35797;&#35201;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#20004;&#20010;&#25509;&#25910;&#22120;&#36890;&#36947;&#30417;&#27979;&#21313;&#20010;&#38750;&#37325;&#21472;&#30340;&#39057;&#24102;&#65292;&#20855;&#26377;&#38750;&#22343;&#21248;&#30340;&#20449;&#21495;&#27963;&#21160;&#12290;&#23545;&#20110;&#36825;&#31181;&#35774;&#32622;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#29615;&#22659;&#19979;&#65292;Q&#23398;&#20064;&#31639;&#27861;&#27604;&#21551;&#21457;&#24335;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#26816;&#27979;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to technological advances in the field of radio technology and its availability, the number of interference signals in the radio spectrum is continuously increasing. Interference signals must be detected in a timely fashion, in order to maintain standards and keep emergency frequencies open. To this end, specialized (multi-channel) receivers are used for spectrum monitoring. In this paper, the performances of two different approaches for controlling the available receiver resources are compared. The methods used for resource management (ReMa) are linear frequency tuning as a heuristic approach and a Q-learning algorithm from the field of reinforcement learning. To test the methods to be investigated, a simplified scenario was designed with two receiver channels monitoring ten non-overlapping frequency bands with non-uniform signal activity. For this setting, it is shown that the Q-learning algorithm used has a significantly higher detection rate than the heuristic approach at the e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Fermat&#36317;&#31163;&#30340;&#25910;&#25947;&#24615;&#36136;&#21644;&#20854;&#22312;&#32858;&#31867;&#31639;&#27861;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#25955;&#37319;&#26679;&#30340;Fermat&#36317;&#31163;&#22312;&#23567;&#37051;&#22495;&#20013;&#25910;&#25947;&#20110;&#23427;&#20204;&#30340;&#36830;&#32493;&#27169;&#25311;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#22522;&#20110;&#31163;&#25955;&#37319;&#26679;&#30340;Fermat&#36317;&#31163;&#30340;&#31163;&#25955;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#20110;&#36830;&#32493;&#31639;&#23376;&#12290;</title><link>http://arxiv.org/abs/2307.05750</link><description>&lt;p&gt;
Fermat&#36317;&#31163;&#65306;&#24230;&#37327;&#36924;&#36817;&#12289;&#35889;&#25910;&#25947;&#21644;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fermat Distances: Metric Approximation, Spectral Convergence, and Clustering Algorithms. (arXiv:2307.05750v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Fermat&#36317;&#31163;&#30340;&#25910;&#25947;&#24615;&#36136;&#21644;&#20854;&#22312;&#32858;&#31867;&#31639;&#27861;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#25955;&#37319;&#26679;&#30340;Fermat&#36317;&#31163;&#22312;&#23567;&#37051;&#22495;&#20013;&#25910;&#25947;&#20110;&#23427;&#20204;&#30340;&#36830;&#32493;&#27169;&#25311;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#22522;&#20110;&#31163;&#25955;&#37319;&#26679;&#30340;Fermat&#36317;&#31163;&#30340;&#31163;&#25955;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#20110;&#36830;&#32493;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Fermat&#36317;&#31163;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#36825;&#26159;&#19968;&#31867;&#22312;&#20855;&#26377;&#20851;&#32852;&#27010;&#29575;&#27979;&#24230;&#30340;Riemann&#27969;&#24418;&#19978;&#23450;&#20041;&#30340;&#23494;&#24230;&#39537;&#21160;&#24230;&#37327;&#12290;Fermat&#36317;&#31163;&#21487;&#20197;&#22312;&#31163;&#25955;&#37319;&#26679;&#19978;&#23450;&#20041;&#65292;&#27492;&#26102;&#23427;&#20204;&#26159;&#38543;&#26426;&#30340;&#65307;&#20063;&#21487;&#20197;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#23450;&#20041;&#65292;&#27492;&#26102;&#23427;&#20204;&#30001;&#23494;&#24230;&#25197;&#26354;&#30340;Riemann&#24230;&#37327;&#19979;&#30340;&#27979;&#22320;&#32447;&#23548;&#20986;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#31163;&#25955;&#37319;&#26679;&#30340;Fermat&#36317;&#31163;&#22312;&#23567;&#37051;&#22495;&#20013;&#25910;&#25947;&#20110;&#23427;&#20204;&#30340;&#36830;&#32493;&#27169;&#25311;&#65292;&#25910;&#25947;&#36895;&#29575;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#21644;Fermat&#36317;&#31163;&#20013;&#23494;&#24230;&#21152;&#26435;&#30340;&#21442;&#25968;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#26032;&#39062;&#30340;&#20960;&#20309;&#21644;&#32479;&#35745;&#35770;&#35777;&#22312;&#28183;&#27969;&#29702;&#35770;&#20013;&#20801;&#35768;&#38750;&#22343;&#21248;&#23494;&#24230;&#21644;&#26354;&#38754;&#22495;&#30340;&#26041;&#27861;&#23436;&#25104;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#22522;&#20110;&#31163;&#25955;&#65292;&#37319;&#26679;&#39537;&#21160;&#30340;Fermat&#36317;&#31163;&#30340;&#31163;&#25955;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#20110;&#30456;&#24212;&#30340;&#36830;&#32493;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the convergence properties of Fermat distances, a family of density-driven metrics defined on Riemannian manifolds with an associated probability measure. Fermat distances may be defined either on discrete samples from the underlying measure, in which case they are random, or in the continuum setting, in which they are induced by geodesics under a density-distorted Riemannian metric. We prove that discrete, sample-based Fermat distances converge to their continuum analogues in small neighborhoods with a precise rate that depends on the intrinsic dimensionality of the data and the parameter governing the extent of density weighting in Fermat distances. This is done by leveraging novel geometric and statistical arguments in percolation theory that allow for non-uniform densities and curved domains. Our results are then used to prove that discrete graph Laplacians based on discrete, sample-driven Fermat distances converge to corresponding continuum operators. In particular, we 
&lt;/p&gt;</description></item><item><title>&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25345;&#32493;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#12289;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#20197;&#21450;&#36873;&#25321;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.05747</link><description>&lt;p&gt;
&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#30456;&#32467;&#21512;&#65306;&#23545;&#25345;&#32493;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Integrating Curricula with Replays: Its Effects on Continual Learning. (arXiv:2307.05747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05747
&lt;/p&gt;
&lt;p&gt;
&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25345;&#32493;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#12289;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#20197;&#21450;&#36873;&#25321;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#33719;&#21462;&#26032;&#25216;&#33021;&#25110;&#30693;&#35782;&#26102;&#65292;&#36890;&#36807;&#35838;&#31243;&#36827;&#34892;&#23398;&#20064;&#21644;&#22797;&#20064;&#12290;&#36825;&#31181;&#20154;&#31867;&#23398;&#20064;&#34892;&#20026;&#21551;&#21457;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#20195;&#29702;&#20013;&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#30446;&#26631;&#26159;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#30693;&#35782;&#20445;&#30041;&#21644;&#20419;&#36827;&#23398;&#20064;&#36716;&#31227;&#12290;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#22238;&#25918;&#26041;&#27861;&#28041;&#21450;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#38543;&#26426;&#36873;&#25321;&#21644;&#25490;&#24207;&#25968;&#25454;&#65292;&#24050;&#32463;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#19981;&#21516;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#20197;&#22686;&#24378;&#25345;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#27425;&#32771;&#23519;&#20102;&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#23545;&#25345;&#32493;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#19977;&#20010;&#20855;&#20307;&#26041;&#38754;&#65306;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#65292;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#65292;&#20197;&#21450;&#36873;&#25321;&#23454;&#20363;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#35838;&#31243;&#35774;&#35745;&#30340;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Humans engage in learning and reviewing processes with curricula when acquiring new skills or knowledge. This human learning behavior has inspired the integration of curricula with replay methods in continual learning agents. The goal is to emulate the human learning process, thereby improving knowledge retention and facilitating learning transfer. Existing replay methods in continual learning agents involve the random selection and ordering of data from previous tasks, which has shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design al
&lt;/p&gt;</description></item><item><title>GOKU-UI&#26159;&#19968;&#31181;&#26222;&#36866;&#25512;&#29702;&#30340;SciML&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#21644;&#22810;&#23556;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21253;&#25324;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#20869;&#30340;&#21508;&#31181;&#24494;&#20998;&#26041;&#31243;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#23427;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#35777;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.05735</link><description>&lt;p&gt;
GOKU-UI&#65306;&#36890;&#36807;&#20851;&#27880;&#21147;&#21644;&#22810;&#23556;&#20987;&#23454;&#29616;&#36830;&#32493;&#26102;&#38388;&#29983;&#25104;&#27169;&#22411;&#30340;&#26222;&#36866;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models. (arXiv:2307.05735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05735
&lt;/p&gt;
&lt;p&gt;
GOKU-UI&#26159;&#19968;&#31181;&#26222;&#36866;&#25512;&#29702;&#30340;SciML&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#21644;&#22810;&#23556;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21253;&#25324;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#20869;&#30340;&#21508;&#31181;&#24494;&#20998;&#26041;&#31243;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#23427;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#35777;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#26159;&#19968;&#20010;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#23427;&#23558;&#39046;&#22495;&#24863;&#30693;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#19982;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GOKU-UI&#65292;&#36825;&#26159;SciML&#29983;&#25104;&#27169;&#22411;GOKU-nets&#30340;&#19968;&#31181;&#28436;&#36827;&#12290;GOKU-UI&#25193;&#23637;&#20102;&#21407;&#22987;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23558;&#20854;&#20182;&#31867;&#21035;&#30340;&#24494;&#20998;&#26041;&#31243;&#65292;&#22914;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#65292;&#34701;&#20837;&#20854;&#20013;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#21644;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26032;&#22411;&#22810;&#23556;&#20987;&#35757;&#32451;&#31574;&#30053;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#30340;&#12289;&#21363;&#26080;&#22788;&#19981;&#22312;&#30340;&#25512;&#29702;&#12290;&#36825;&#20123;&#25913;&#36827;&#20351;&#20854;&#22312;&#37325;&#24314;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#35777;&#25968;&#25454;&#30340;&#35780;&#20272;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21363;&#20351;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#32553;&#23567;&#20102;32&#20493;&#65292;GOKU-UI&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#65292;&#20984;&#26174;&#20854;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#24403;&#24212;&#29992;&#20110;&#23454;&#35777;&#30340;&#20154;&#33041;&#25968;&#25454;&#26102;&#65292;&#21516;&#26102;&#34701;&#21512;&#20102;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;GOKU-UI&#20063;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific Machine Learning (SciML) is a burgeoning field that synergistically combines domain-aware and interpretable models with agnostic machine learning techniques. In this work, we introduce GOKU-UI, an evolution of the SciML generative model GOKU-nets. The GOKU-UI broadens the original model's spectrum to incorporate other classes of differential equations, such as Stochastic Differential Equations (SDEs), and integrates a distributed, i.e. ubiquitous, inference through attention mechanisms and a novel multiple shooting training strategy in the latent space. These enhancements have led to a significant increase in its performance in both reconstruction and forecast tasks, as demonstrated by our evaluation of simulated and empirical data. Specifically, GOKU-UI outperformed all baseline models on synthetic datasets even with a training set 32-fold smaller, underscoring its remarkable data efficiency. Furthermore, when applied to empirical human brain data, while incorporating stoch
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#38754;&#21521;&#32452;&#21512;&#20998;&#31867;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#24341;&#20837;&#20219;&#21153;&#36807;&#26465;&#20214;&#21270;&#21644;&#32452;&#38388;&#20132;&#26367;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#32452;&#22810;&#26631;&#31614;&#35774;&#23450;&#19979;&#23454;&#29616;&#20102;&#24658;&#23450;&#27604;&#20363;&#32553;&#25918;&#65292;&#24182;&#22312;&#23398;&#26415;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05728</link><description>&lt;p&gt;
&#38754;&#21521;&#32452;&#21512;&#20998;&#31867;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Towards A Scalable Solution for Improving Multi-Group Fairness in Compositional Classification. (arXiv:2307.05728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#38754;&#21521;&#32452;&#21512;&#20998;&#31867;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#24341;&#20837;&#20219;&#21153;&#36807;&#26465;&#20214;&#21270;&#21644;&#32452;&#38388;&#20132;&#26367;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#32452;&#22810;&#26631;&#31614;&#35774;&#23450;&#19979;&#23454;&#29616;&#20102;&#24658;&#23450;&#27604;&#20363;&#32553;&#25918;&#65292;&#24182;&#22312;&#23398;&#26415;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#25991;&#29486;&#20016;&#23500;&#65292;&#20294;&#23545;&#20110;&#20462;&#22797;&#22797;&#26434;&#31995;&#32479;&#65288;&#20854;&#20013;&#26368;&#32456;&#39044;&#27979;&#26159;&#22810;&#20010;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#65292;&#23384;&#22312;&#22810;&#20010;&#32452;&#65289;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#29992;&#20110;&#25913;&#21892;&#26426;&#20250;&#24179;&#31561;&#20844;&#24179;&#24615;&#30340;&#33258;&#28982;&#22522;&#32447;&#26041;&#27861;&#19982;&#34987;&#20462;&#22797;&#32452;&#25968;&#21644;&#34987;&#20462;&#22797;&#39044;&#27979;&#26631;&#31614;&#25968;&#30340;&#20056;&#31215;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#20351;&#20854;&#19981;&#23454;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#8220;&#20219;&#21153;&#36807;&#26465;&#20214;&#21270;&#8221;&#21644;&#8220;&#32452;&#38388;&#20132;&#26367;&#8221;&#65292;&#20197;&#22312;&#22810;&#32452;&#22810;&#26631;&#31614;&#35774;&#23450;&#20013;&#23454;&#29616;&#24658;&#23450;&#27604;&#20363;&#32553;&#25918;&#12290;&#25105;&#20204;&#22312;&#23398;&#26415;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#36825;&#20010;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rich literature on machine learning fairness, relatively little attention has been paid to remediating complex systems, where the final prediction is the combination of multiple classifiers and where multiple groups are present. In this paper, we first show that natural baseline approaches for improving equal opportunity fairness scale linearly with the product of the number of remediated groups and the number of remediated prediction labels, rendering them impractical. We then introduce two simple techniques, called {\em task-overconditioning} and {\em group-interleaving}, to achieve a constant scaling in this multi-group multi-label setup. Our experimental results in academic and real-world environments demonstrate the effectiveness of our proposal at mitigation within this environment.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#28508;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#21450;&#22522;&#20110;&#28508;&#22312;&#20998;&#24067;&#30340;&#28023;&#26862;&#30697;&#38453;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#40723;&#21169;&#28508;&#22312;&#31354;&#38388;&#36981;&#24490;&#22240;&#26524;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.05704</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#22240;&#26524;&#25490;&#24207;&#20808;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Causal Ordering Prior for Unsupervised Representation Learning. (arXiv:2307.05704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#28508;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#21450;&#22522;&#20110;&#28508;&#22312;&#20998;&#24067;&#30340;&#28023;&#26862;&#30697;&#38453;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#40723;&#21169;&#28508;&#22312;&#31354;&#38388;&#36981;&#24490;&#22240;&#26524;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20381;&#36182;&#20110;&#23545;&#28508;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#35748;&#20026;&#25968;&#25454;&#38598;&#20013;&#30340;&#21464;&#24322;&#22240;&#32032;&#23454;&#38469;&#19978;&#26159;&#22240;&#26524;&#30456;&#20851;&#30340;&#12290;&#20801;&#35768;&#28508;&#21464;&#37327;&#30001;&#20110;&#22240;&#26524;&#20851;&#31995;&#32780;&#30456;&#20851;&#24615;&#26356;&#21152;&#30495;&#23454;&#21644;&#21487;&#27867;&#21270;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#21487;&#35777;&#26126;&#21487;&#35782;&#21035;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#65306;&#36741;&#21161;&#20449;&#24687;&#12289;&#24369;&#26631;&#31614;&#65292;&#20197;&#21450;&#24178;&#39044;&#25110;&#29978;&#33267;&#23545;&#29031;&#25968;&#25454;&#12290;&#21463;&#21040;&#21151;&#33021;&#22240;&#26524;&#27169;&#22411;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#20855;&#26377;&#28508;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#22522;&#20110;&#28508;&#22312;&#20998;&#24067;&#30340;&#28023;&#26862;&#30697;&#38453;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#40723;&#21169;&#28508;&#22312;&#31354;&#38388;&#36981;&#24490;&#22240;&#26524;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning with variational inference relies heavily on independence assumptions over latent variables. Causal representation learning (CRL), however, argues that factors of variation in a dataset are, in fact, causally related. Allowing latent variables to be correlated, as a consequence of causal relationships, is more realistic and generalisable. So far, provably identifiable methods rely on: auxiliary information, weak labels, and interventional or even counterfactual data. Inspired by causal discovery with functional causal models, we propose a fully unsupervised representation learning method that considers a data generation process with a latent additive noise model (ANM). We encourage the latent space to follow a causal ordering via loss function based on the Hessian of the latent distribution.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#24110;&#21161;&#24191;&#21578;&#21830;&#22312;&#38750;&#31283;&#24577;&#37319;&#36141;&#29615;&#22659;&#19979;&#21160;&#24577;&#20248;&#21270;&#24191;&#21578;&#24179;&#21488;&#21442;&#25968;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.05698</link><description>&lt;p&gt;
&#38750;&#31283;&#24577;&#33258;&#21160;&#25237;&#26631;&#19990;&#30028;&#20013;&#30340;&#22312;&#32447;&#24191;&#21578;&#37319;&#36141;
&lt;/p&gt;
&lt;p&gt;
Online Ad Procurement in Non-stationary Autobidding Worlds. (arXiv:2307.05698v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05698
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#24110;&#21161;&#24191;&#21578;&#21830;&#22312;&#38750;&#31283;&#24577;&#37319;&#36141;&#29615;&#22659;&#19979;&#21160;&#24577;&#20248;&#21270;&#24191;&#21578;&#24179;&#21488;&#21442;&#25968;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#22312;&#32447;&#24191;&#21578;&#21830;&#36890;&#36807;&#19982;&#33258;&#21160;&#25237;&#26631;&#24179;&#21488;&#36827;&#34892;&#20132;&#20114;&#26469;&#37319;&#36141;&#25968;&#23383;&#24191;&#21578;&#23637;&#31034;&#65306;&#24191;&#21578;&#21830;&#36890;&#36807;&#35774;&#32622;&#39044;&#31639;&#12289;&#30446;&#26631;&#25237;&#36164;&#22238;&#25253;&#29575;&#12289;&#27599;&#27425;&#28857;&#20987;&#30340;&#26368;&#22823;&#25104;&#26412;&#31561;&#21442;&#25968;&#26469;&#20256;&#36798;&#39640;&#32423;&#37319;&#36141;&#30446;&#26631;&#12290;&#28982;&#21518;&#24191;&#21578;&#24179;&#21488;&#20195;&#34920;&#24191;&#21578;&#21830;&#37319;&#36141;&#23637;&#31034;&#65292;&#24182;&#21521;&#24191;&#21578;&#21830;&#25253;&#21578;&#26368;&#32456;&#37319;&#36141;&#36716;&#21270;&#32467;&#26524;&#65288;&#20363;&#22914;&#28857;&#20987;&#37327;&#65289;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24191;&#21578;&#21830;&#21487;&#33021;&#21482;&#20250;&#25509;&#25910;&#21040;&#24179;&#21488;&#37319;&#36141;&#32454;&#33410;&#30340;&#26368;&#23569;&#20449;&#24687;&#65292;&#24182;&#19988;&#37319;&#36141;&#32467;&#26524;&#21463;&#21040;&#23395;&#33410;&#24615;&#27169;&#24335;&#12289;&#20598;&#21457;&#24615;&#31995;&#32479;&#25925;&#38556;&#21644;&#24066;&#22330;&#36235;&#21183;&#31561;&#38750;&#31283;&#24577;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#24191;&#21578;&#21830;&#38590;&#20197;&#26377;&#25928;&#20248;&#21270;&#21442;&#25968;&#20915;&#31574;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#24110;&#21161;&#24191;&#21578;&#21830;&#22312;&#20855;&#26377;&#38750;&#31283;&#24577;&#37319;&#36141;&#32467;&#26524;&#30340;&#29616;&#23454;&#22810;&#33218;&#36172;&#21338;&#29615;&#22659;&#19979;&#65292;&#22312;&#21463;&#36890;&#29992;&#38271;&#26399;&#32422;&#26463;&#26465;&#20214;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#21160;&#24577;&#20248;&#21270;&#24191;&#21578;&#24179;&#21488;&#30340;&#21442;&#25968;&#20915;&#31574;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21407;&#22987;&#30340;-d
&lt;/p&gt;
&lt;p&gt;
Today's online advertisers procure digital ad impressions through interacting with autobidding platforms: advertisers convey high level procurement goals via setting levers such as budget, target return-on-investment, max cost per click, etc.. Then ads platforms subsequently procure impressions on advertisers' behalf, and report final procurement conversions (e.g. click) to advertisers. In practice, advertisers may receive minimal information on platforms' procurement details, and procurement outcomes are subject to non-stationary factors like seasonal patterns, occasional system corruptions, and market trends which make it difficult for advertisers to optimize lever decisions effectively. Motivated by this, we present an online learning framework that helps advertisers dynamically optimize ad platform lever decisions while subject to general long-term constraints in a realistic bandit feedback environment with non-stationary procurement outcomes. In particular, we introduce a primal-d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GoTogether&#65292;&#19968;&#20010;&#21033;&#29992;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#20026;&#25340;&#36710;&#26381;&#21153;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#30340;&#21382;&#21490;&#36873;&#25321;&#65292;GoTogether&#33021;&#22815;&#39044;&#27979;&#20010;&#20154;&#20849;&#20056;&#30340;&#24895;&#26395;&#65292;&#24182;&#25552;&#20379;&#39640;&#25104;&#21151;&#29575;&#30340;&#25340;&#36710;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.05697</link><description>&lt;p&gt;
&#19968;&#20010;&#21160;&#24577;&#20010;&#24615;&#21270;&#25340;&#36710;&#26381;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#25490;&#24207;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Machine-Learned Ranking Algorithm for Dynamic and Personalised Car Pooling Services. (arXiv:2307.05697v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GoTogether&#65292;&#19968;&#20010;&#21033;&#29992;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#20026;&#25340;&#36710;&#26381;&#21153;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#30340;&#21382;&#21490;&#36873;&#25321;&#65292;GoTogether&#33021;&#22815;&#39044;&#27979;&#20010;&#20154;&#20849;&#20056;&#30340;&#24895;&#26395;&#65292;&#24182;&#25552;&#20379;&#39640;&#25104;&#21151;&#29575;&#30340;&#25340;&#36710;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#21496;&#26426;&#19982;&#20855;&#26377;&#30456;&#20284;&#34892;&#31243;&#21644;&#26102;&#38388;&#23433;&#25490;&#30340;&#26053;&#23458;&#20849;&#20139;&#27773;&#36710;&#65292;&#25340;&#36710;&#34987;&#26399;&#26395;&#22312;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#21644;&#27745;&#26579;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#22312;&#19968;&#32452;&#21496;&#26426;&#21644;&#28508;&#22312;&#20056;&#23458;&#20013;&#39640;&#25928;&#22320;&#25214;&#21040;&#25104;&#21151;&#30340;&#25340;&#36710;&#21305;&#37197;&#65292;&#35774;&#35745;&#20102;&#35768;&#22810;&#25340;&#36710;&#21305;&#37197;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#22312;&#24050;&#32463;&#35748;&#35782;&#21040;&#38500;&#20102;&#31616;&#21333;&#30340;&#20986;&#34892;&#38656;&#27714;&#22806;&#65292;&#35768;&#22810;&#38750;&#36135;&#24065;&#26041;&#38754;&#21644;&#31038;&#20250;&#22240;&#32032;&#21487;&#33021;&#24433;&#21709;&#20010;&#20154;&#24895;&#24847;&#20849;&#20056;&#30340;&#24847;&#24895;&#65292;&#36825;&#20123;&#22240;&#32032;&#24456;&#38590;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GoTogether&#65292;&#36825;&#26159;&#19968;&#20010;&#25340;&#36710;&#26381;&#21153;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#20174;&#29992;&#25143;&#30340;&#36873;&#25321;&#21382;&#21490;&#65288;&#21363;&#25509;&#21463;&#25110;&#25298;&#32477;&#20849;&#20139;&#20056;&#36710;&#30340;&#31867;&#22411;&#65289;&#20013;&#33258;&#21160;&#25512;&#23548;&#20986;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#25490;&#24207;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;GoTogether&#26500;&#24314;&#25512;&#33616;&#20056;&#36710;&#21015;&#34920;&#20197;&#26368;&#22823;&#21270;&#21305;&#37197;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Car pooling is expected to significantly help in reducing traffic congestion and pollution in cities by enabling drivers to share their cars with travellers with similar itineraries and time schedules. A number of car pooling matching services have been designed in order to efficiently find successful ride matches in a given pool of drivers and potential passengers. However, it is now recognised that many non-monetary aspects and social considerations, besides simple mobility needs, may influence the individual willingness of sharing a ride, which are difficult to predict. To address this problem, in this study we propose GoTogether, a recommender system for car pooling services that leverages on learning-to-rank techniques to automatically derive the personalised ranking model of each user from the history of her choices (i.e., the type of accepted or rejected shared rides). Then, GoTogether builds the list of recommended rides in order to maximise the success rate of the offered matc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05695</link><description>&lt;p&gt;
&#20197;&#19981;&#21516;&#26041;&#24335;&#22534;&#21472;&#26356;&#22810;&#23618;&#65306;&#36890;&#36807;&#20302;&#31209;&#26356;&#26032;&#36827;&#34892;&#39640;&#31209;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#32593;&#32476;&#25317;&#26377;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#30340;&#35268;&#27169;&#24050;&#32463;&#21344;&#20027;&#23548;&#22320;&#20301;&#24182;&#19988;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#24517;&#35201;&#24615;&#20173;&#28982;&#32570;&#20047;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#32780;&#26367;&#20195;&#26041;&#27861;&#19981;&#19968;&#23450;&#33021;&#22815;&#38477;&#20302;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#39640;&#31209;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;ReLoRA&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#37327;&#39640;&#36798;350M&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#30340;&#25928;&#29575;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#30340;&#28508;&#21147;&#21450;&#20854;&#23545;&#20110;&#32553;&#25918;&#23450;&#24459;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
&lt;/p&gt;</description></item><item><title>&#12298;&#31185;&#25216;&#25991;&#26723;&#20013;&#30340;&#22270;&#24418;&#20998;&#31867;&#25216;&#26415;&#32508;&#36848;&#12299;&#23545;&#22270;&#24418;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26803;&#29702;&#65292;&#21253;&#25324;&#34920;&#26684;&#12289;&#29031;&#29255;&#12289;&#22270;&#34920;&#12289;&#22320;&#22270;&#21644;&#32472;&#22270;&#20116;&#31867;&#65292;&#24182;&#25209;&#21028;&#24615;&#22320;&#35780;&#36848;&#20102;&#29616;&#26377;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.05694</link><description>&lt;p&gt;
&#12298;&#31185;&#25216;&#25991;&#26723;&#20013;&#30340;&#22270;&#24418;&#20998;&#31867;&#25216;&#26415;&#32508;&#36848;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey on Figure Classification Techniques in Scientific Documents. (arXiv:2307.05694v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05694
&lt;/p&gt;
&lt;p&gt;
&#12298;&#31185;&#25216;&#25991;&#26723;&#20013;&#30340;&#22270;&#24418;&#20998;&#31867;&#25216;&#26415;&#32508;&#36848;&#12299;&#23545;&#22270;&#24418;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26803;&#29702;&#65292;&#21253;&#25324;&#34920;&#26684;&#12289;&#29031;&#29255;&#12289;&#22270;&#34920;&#12289;&#22320;&#22270;&#21644;&#32472;&#22270;&#20116;&#31867;&#65292;&#24182;&#25209;&#21028;&#24615;&#22320;&#35780;&#36848;&#20102;&#29616;&#26377;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#23545;&#20110;&#20256;&#36798;&#31185;&#23398;&#20107;&#23454;&#21644;&#20449;&#24687;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#22270;&#24418;&#20013;&#25552;&#21462;&#25968;&#25454;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#12290;&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#23558;&#22270;&#24418;&#20998;&#20026;&#34920;&#26684;&#12289;&#29031;&#29255;&#12289;&#22270;&#34920;&#12289;&#22320;&#22270;&#21644;&#32472;&#22270;&#20116;&#31867;&#65292;&#24182;&#23545;&#35299;&#20915;&#22270;&#24418;&#20998;&#31867;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#25209;&#21028;&#24615;&#32508;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#24403;&#21069;&#30740;&#31350;&#30340;&#31354;&#30333;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22270;&#24418;&#20998;&#31867;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Figures visually represent an essential piece of information and provide an effective means to communicate scientific facts. Recently there have been many efforts toward extracting data directly from figures, specifically from tables, diagrams, and plots, using different Artificial Intelligence and Machine Learning techniques. This is because removing information from figures could lead to deeper insights into the concepts highlighted in the scientific documents. In this survey paper, we systematically categorize figures into five classes - tables, photos, diagrams, maps, and plots, and subsequently present a critical review of the existing methodologies and data sets that address the problem of figure classification. Finally, we identify the current research gaps and provide possible directions for further research on figure classification.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20248;&#21270;&#22810;&#30446;&#26631;&#27700;&#30005;&#31449;&#27700;&#24211;&#35843;&#24230;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36816;&#34892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05643</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22810;&#30446;&#26631;&#27700;&#30005;&#31449;&#27700;&#24211;&#35843;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multiobjective Hydropower Reservoir Operation Optimization with Transformer-Based Deep Reinforcement Learning. (arXiv:2307.05643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20248;&#21270;&#22810;&#30446;&#26631;&#27700;&#30005;&#31449;&#27700;&#24211;&#35843;&#24230;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36816;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27700;&#36164;&#28304;&#30701;&#32570;&#21644;&#27700;&#38656;&#27714;&#22686;&#21152;&#65292;&#22810;&#27700;&#24211;&#31995;&#32479;&#30340;&#32852;&#21512;&#36816;&#34892;&#20197;&#24179;&#34913;&#21457;&#30005;&#12289;&#29983;&#24577;&#20445;&#25252;&#21644;&#23621;&#27665;&#29992;&#27700;&#20379;&#24212;&#24050;&#25104;&#20026;&#27700;&#30005;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#27700;&#24211;&#30340;&#20247;&#22810;&#32422;&#26463;&#21644;&#38750;&#32447;&#24615;&#24615;&#20351;&#24471;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#32791;&#26102;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;Transformer&#26694;&#26550;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#32534;&#30721;&#22120;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#26377;&#25928;&#22320;&#20174;&#27700;&#24211;&#21644;&#23621;&#27665;&#21306;&#25552;&#21462;&#20449;&#24687;&#65292;&#35299;&#30721;&#22120;&#30340;&#22810;&#27700;&#24211;&#27880;&#24847;&#32593;&#32476;&#29983;&#25104;&#36866;&#24403;&#30340;&#36816;&#34892;&#20915;&#31574;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#31185;&#32599;&#25289;&#22810;&#27827;&#27969;&#22495;&#30340;&#26757;&#24503;&#28246;&#21644;&#40077;&#23041;&#23572;&#28246;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21512;&#36866;&#30340;&#36816;&#33829;&#32467;&#26524;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#30340;&#36816;&#34892;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to shortage of water resources and increasing water demands, the joint operation of multireservoir systems for balancing power generation, ecological protection, and the residential water supply has become a critical issue in hydropower management. However, the numerous constraints and nonlinearity of multiple reservoirs make solving this problem time-consuming. To address this challenge, a deep reinforcement learning approach that incorporates a transformer framework is proposed. The multihead attention mechanism of the encoder effectively extracts information from reservoirs and residential areas, and the multireservoir attention network of the decoder generates suitable operational decisions. The proposed method is applied to Lake Mead and Lake Powell in the Colorado River Basin. The experimental results demonstrate that the transformer-based deep reinforcement learning approach can produce appropriate operational outcomes. Compared to a state-of-the-art method, the operation st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32422;&#26463;&#24341;&#23548;&#27169;&#31946;&#27979;&#35797;&#22120;ConFL&#65292;&#21487;&#20197;&#20174;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#33258;&#21160;&#25552;&#21462;&#32422;&#26463;&#65292;&#24182;&#29983;&#25104;&#33021;&#22815;&#36890;&#36807;&#39564;&#35777;&#21644;&#25506;&#32034;&#26356;&#28145;&#36335;&#24452;&#30340;&#26377;&#25928;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.05642</link><description>&lt;p&gt;
ConFL&#65306;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#32422;&#26463;&#24341;&#23548;&#27169;&#31946;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ConFL: Constraint-guided Fuzzing for Machine Learning Framework. (arXiv:2307.05642v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32422;&#26463;&#24341;&#23548;&#27169;&#31946;&#27979;&#35797;&#22120;ConFL&#65292;&#21487;&#20197;&#20174;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#33258;&#21160;&#25552;&#21462;&#32422;&#26463;&#65292;&#24182;&#29983;&#25104;&#33021;&#22815;&#36890;&#36807;&#39564;&#35777;&#21644;&#25506;&#32034;&#26356;&#28145;&#36335;&#24452;&#30340;&#26377;&#25928;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#20010;&#31038;&#20250;&#39046;&#22495;&#20013;&#33258;&#21160;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#20154;&#20204;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#30340;&#28508;&#22312;&#28431;&#27934;&#20135;&#29983;&#20102;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26694;&#26550;&#30340;&#22797;&#26434;&#23454;&#29616;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#27979;&#35797;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#20197;&#24448;&#20851;&#20110;&#23545;ML&#26694;&#26550;&#36827;&#34892;&#27169;&#31946;&#27979;&#35797;&#30340;&#30740;&#31350;&#19968;&#30452;&#38754;&#20020;&#30528;&#26377;&#25928;&#25552;&#21462;&#36755;&#20837;&#32422;&#26463;&#21644;&#29983;&#25104;&#26377;&#25928;&#36755;&#20837;&#30340;&#22256;&#38590;&#65292;&#23548;&#33268;&#28145;&#24230;&#25191;&#34892;&#30340;&#27169;&#31946;&#27979;&#35797;&#25345;&#32493;&#26102;&#38388;&#36807;&#38271;&#25110;&#26080;&#27861;&#25581;&#31034;&#30446;&#26631;&#23849;&#28291;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ConFL&#30340;&#32422;&#26463;&#24341;&#23548;&#27169;&#31946;&#22120;&#65292;&#29992;&#20110;ML&#26694;&#26550;&#12290;ConFL&#33021;&#22815;&#33258;&#21160;&#20174;&#20869;&#26680;&#20195;&#30721;&#20013;&#25552;&#21462;&#32422;&#26463;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20808;&#21069;&#30693;&#35782;&#12290;&#22312;&#32422;&#26463;&#30340;&#25351;&#23548;&#19979;&#65292;ConFL&#33021;&#22815;&#29983;&#25104;&#33021;&#22815;&#36890;&#36807;&#39564;&#35777;&#24182;&#25506;&#32034;&#26356;&#28145;&#20869;&#26680;&#20195;&#30721;&#36335;&#24452;&#30340;&#26377;&#25928;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#32452;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#31946;&#27979;&#35797;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#35777;&#26126;ConFL&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20027;&#35201;&#22312;Tensorflow&#19978;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning gains prominence in various sectors of society for automated decision-making, concerns have risen regarding potential vulnerabilities in machine learning (ML) frameworks. Nevertheless, testing these frameworks is a daunting task due to their intricate implementation. Previous research on fuzzing ML frameworks has struggled to effectively extract input constraints and generate valid inputs, leading to extended fuzzing durations for deep execution or revealing the target crash.  In this paper, we propose ConFL, a constraint-guided fuzzer for ML frameworks. ConFL automatically extracting constraints from kernel codes without the need for any prior knowledge. Guided by the constraints, ConFL is able to generate valid inputs that can pass the verification and explore deeper paths of kernel codes. In addition, we design a grouping technique to boost the fuzzing efficiency.  To demonstrate the effectiveness of ConFL, we evaluated its performance mainly on Tensorflow. We fi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#31934;&#24230;&#30697;&#38453;&#65292;&#20174;&#35757;&#32451;&#23436;&#25104;&#21518;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#21253;&#25324;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#26041;&#21521;&#21644;&#36755;&#20837;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.05639</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#27963;&#36291;&#23376;&#31354;&#38388;&#24182;&#21457;&#29616;&#37325;&#35201;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#31934;&#24230;&#30697;&#38453;&#65292;&#20174;&#35757;&#32451;&#23436;&#25104;&#21518;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#21253;&#25324;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#26041;&#21521;&#21644;&#36755;&#20837;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#19968;&#20010;&#26082;&#33021;&#36798;&#21040;&#24378;&#22823;&#39044;&#27979;&#24615;&#33021;&#65292;&#21448;&#33021;&#34987;&#20154;&#31867;&#35299;&#37322;&#30340;&#27169;&#22411;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#22256;&#38590;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#30001;&#20110;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#20914;&#31361;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20026;&#20854;&#39640;&#26031;&#26680;&#28155;&#21152;&#21487;&#23398;&#20064;&#30340;&#31934;&#24230;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35757;&#32451;&#23436;&#25104;&#21518;&#21487;&#20197;&#20174;&#31934;&#24230;&#30697;&#38453;&#30340;&#35889;&#20013;&#25552;&#21462;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#29305;&#24449;&#21521;&#37327;&#35299;&#37322;&#20102;&#27169;&#22411;&#26368;&#25935;&#24863;&#30340;&#26041;&#21521;&#65292;&#25581;&#31034;&#20102;&#27963;&#36291;&#23376;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#30417;&#30563;&#38477;&#32500;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#29305;&#24449;&#21521;&#37327;&#20984;&#26174;&#20102;&#36755;&#20837;&#21644;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#30340;&#32477;&#23545;&#21464;&#21270;&#20851;&#31995;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#22522;&#20110;&#20854;&#23545;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#25552;&#21462;&#36755;&#20837;&#21464;&#37327;&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing a model that achieves a strong predictive performance and at the same time is interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the Radial Basis Function Neural Network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the predi
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20351;&#29992;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20165;&#26377;&#23569;&#37327;&#25110;&#27809;&#26377;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#26032;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05638</link><description>&lt;p&gt;
&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#32508;&#21512;&#35843;&#26597;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions. (arXiv:2307.05638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20351;&#29992;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20165;&#26377;&#23569;&#37327;&#25110;&#27809;&#26377;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30417;&#27979;&#24037;&#19994;&#36807;&#31243;&#26377;&#28508;&#21147;&#36890;&#36807;&#21450;&#26102;&#26816;&#27979;&#24322;&#24120;&#20107;&#20214;&#24182;&#20419;&#36827;&#21450;&#26102;&#24178;&#39044;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#20248;&#21270;&#36136;&#37327;&#12290;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#35782;&#21035;&#22823;&#25968;&#25454;&#38598;&#20013;&#30340;&#38750;&#24179;&#20961;&#27169;&#24335;&#65292;&#22312;&#36825;&#19968;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#31867;&#22411;&#30340;&#25968;&#25454;&#32473;&#23450;&#29305;&#23450;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24037;&#33402;&#21644;&#29615;&#22659;&#30340;&#21160;&#24577;&#24615;&#65292;&#20026;&#27599;&#20010;&#31245;&#26377;&#19981;&#21516;&#30340;&#24773;&#20917;&#37325;&#26032;&#33719;&#24471;&#25152;&#38656;&#25968;&#25454;&#36827;&#34892;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#36825;&#20010;&#23398;&#20064;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#21363;&#20351;&#27809;&#26377;&#25110;&#21482;&#26377;&#24456;&#23569;&#30340;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, the algorithms demand large volumes of labeled training data. However, due to the dynamic nature of processes and the environment, it is impractical to acquire the needed data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, this learning framework solves new tasks even with little or no additional labeled data. The approach bypasses the need to retrain a model from scratch for ev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22768;&#38899;&#26085;&#21382;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#20027;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#34920;&#31034;&#35821;&#38899;&#20998;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22768;&#38899;&#26085;&#21382;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#25552;&#39640;&#22768;&#38899;&#26085;&#21382;&#21644;ASR&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.05637</link><description>&lt;p&gt;
&#22768;&#38899;&#26085;&#21382;&#21644;GMM&#30340;ASR&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Speech Diarization and ASR with GMM. (arXiv:2307.05637v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05637
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22768;&#38899;&#26085;&#21382;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#20027;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#34920;&#31034;&#35821;&#38899;&#20998;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22768;&#38899;&#26085;&#21382;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#25552;&#39640;&#22768;&#38899;&#26085;&#21382;&#21644;ASR&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22768;&#38899;&#26085;&#21382;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#20027;&#39064;&#12290;&#22768;&#38899;&#26085;&#21382;&#28041;&#21450;&#23558;&#38899;&#39057;&#27969;&#20013;&#30340;&#20010;&#20307;&#21457;&#35328;&#32773;&#20998;&#31163;&#20986;&#26469;&#12290;&#36890;&#36807;&#20351;&#29992;ASR&#36716;&#24405;&#65292;&#26085;&#21382;&#36807;&#31243;&#26088;&#22312;&#26681;&#25454;&#20854;&#29420;&#29305;&#30340;&#38899;&#39057;&#29305;&#24449;&#23558;&#27599;&#20010;&#21457;&#35328;&#32773;&#30340;&#21457;&#35328;&#20998;&#32452;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26159;&#25351;&#26426;&#22120;&#25110;&#31243;&#24207;&#35782;&#21035;&#21644;&#36716;&#25442;&#21475;&#35821;&#35789;&#35821;&#21644;&#30701;&#35821;&#25104;&#20026;&#26426;&#22120;&#21487;&#35835;&#26684;&#24335;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#22768;&#38899;&#26085;&#21382;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#34920;&#31034;&#35821;&#38899;&#20998;&#27573;&#12290;&#22522;&#20110;GMM&#21442;&#25968;&#35745;&#31639;&#31751;&#38388;&#36317;&#31163;&#65292;&#36317;&#31163;&#38408;&#20540;&#20316;&#20026;&#20572;&#27490;&#20934;&#21017;&#12290;ASR&#21253;&#25324;&#23558;&#26410;&#30693;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#30456;&#24212;&#30340;&#20070;&#38754;&#36716;&#24405;&#12290;&#35821;&#38899;&#20449;&#21495;&#20351;&#29992;&#21516;&#27493;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#32771;&#34385;&#21040;&#22768;&#38899;&#30340;&#38899;&#39640;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#22768;&#38899;&#26085;&#21382;&#21644;ASR&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research paper, we delve into the topics of Speech Diarization and Automatic Speech Recognition (ASR). Speech diarization involves the separation of individual speakers within an audio stream. By employing the ASR transcript, the diarization process aims to segregate each speaker's utterances, grouping them based on their unique audio characteristics. On the other hand, Automatic Speech Recognition refers to the capability of a machine or program to identify and convert spoken words and phrases into a machine-readable format. In our speech diarization approach, we utilize the Gaussian Mixer Model (GMM) to represent speech segments. The inter-cluster distance is computed based on the GMM parameters, and the distance threshold serves as the stopping criterion. ASR entails the conversion of an unknown speech waveform into a corresponding written transcription. The speech signal is analyzed using synchronized algorithms, taking into account the pitch frequency. Our primary objectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20449;&#24687;&#35770;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#36890;&#36807;&#30028;&#38480;&#23558;&#35757;&#32451;&#25968;&#25454;&#30340;&#20114;&#20449;&#24687;&#25110;&#36125;&#21494;&#26031;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#19982;&#31616;&#21333;&#32447;&#24615;&#27169;&#22411;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22522;&#26412;&#24615;&#33021;&#38480;&#21046;&#12290;&#35777;&#26126;&#26041;&#27861;&#21033;&#29992;&#20005;&#26684;&#30340;&#33258;&#26059;&#29627;&#29827;&#24037;&#20855;&#21644;&#8220;&#39640;&#26031;&#31561;&#25928;&#21407;&#29702;&#8221;&#12290;</title><link>http://arxiv.org/abs/2307.05635</link><description>&lt;p&gt;
&#34920;&#38754;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#21270;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental limits of overparametrized shallow neural networks for supervised learning. (arXiv:2307.05635v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20449;&#24687;&#35770;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#36890;&#36807;&#30028;&#38480;&#23558;&#35757;&#32451;&#25968;&#25454;&#30340;&#20114;&#20449;&#24687;&#25110;&#36125;&#21494;&#26031;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#19982;&#31616;&#21333;&#32447;&#24615;&#27169;&#22411;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22522;&#26412;&#24615;&#33021;&#38480;&#21046;&#12290;&#35777;&#26126;&#26041;&#27861;&#21033;&#29992;&#20005;&#26684;&#30340;&#33258;&#26059;&#29627;&#29827;&#24037;&#20855;&#21644;&#8220;&#39640;&#26031;&#31561;&#25928;&#21407;&#29702;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#19968;&#20010;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#36229;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#20174;&#19982;&#20854;&#32467;&#26500;&#31867;&#20284;&#30340;&#25945;&#24072;&#32593;&#32476;&#29983;&#25104;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#36827;&#34892;&#20449;&#24687;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20197;&#30028;&#38480;&#30340;&#24418;&#24335;&#32473;&#20986;&#65292;&#23558;i)&#35757;&#32451;&#25968;&#25454;&#21644;&#32593;&#32476;&#26435;&#37325;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#25110;ii)&#36125;&#21494;&#26031;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#19982;&#23545;&#24212;&#30340;&#31616;&#21333;&#65288;&#24191;&#20041;&#65289;&#32447;&#24615;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#21516;&#37327;&#32852;&#31995;&#36215;&#26469;&#65292;&#32780;&#36825;&#20010;&#32447;&#24615;&#27169;&#22411;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#24050;&#32463;&#24471;&#21040;&#20005;&#26684;&#30830;&#23450;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#20197;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#12289;&#36755;&#20837;&#32500;&#24230;&#21644;&#38544;&#34255;&#21333;&#20803;&#25968;&#37327;&#20026;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#65288;&#23454;&#38469;&#19978;&#26159;&#20219;&#20309;&#23398;&#20064;&#36807;&#31243;&#65289;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#36890;&#36807;&#25105;&#20204;&#30340;&#20004;&#23618;&#25945;&#24072;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#25552;&#20379;&#20102;&#22522;&#26412;&#24615;&#33021;&#38480;&#21046;&#12290;&#35777;&#26126;&#20381;&#36182;&#20110;&#33258;&#26059;&#29627;&#29827;&#30340;&#20005;&#26684;&#24037;&#20855;&#65292;&#24182;&#30001;&#22788;&#20110;&#20247;&#22810;&#26368;&#26032;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26680;&#24515;&#30340;&#8220;&#39640;&#26031;&#31561;&#25928;&#21407;&#29702;&#8221;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
We carry out an information-theoretical analysis of a two-layer neural network trained from input-output pairs generated by a teacher network with matching architecture, in overparametrized regimes. Our results come in the form of bounds relating i) the mutual information between training data and network weights, or ii) the Bayes-optimal generalization error, to the same quantities but for a simpler (generalized) linear model for which explicit expressions are rigorously known. Our bounds, which are expressed in terms of the number of training samples, input dimension and number of hidden units, thus yield fundamental performance limits for any neural network (and actually any learning procedure) trained from limited data generated according to our two-layer teacher neural network model. The proof relies on rigorous tools from spin glasses and is guided by ``Gaussian equivalence principles'' lying at the core of numerous recent analyses of neural networks. With respect to the existing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#32858;&#21512;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ASA-GNN&#65289;&#26469;&#25552;&#39640;&#20132;&#26131;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23398;&#20064;&#21028;&#21035;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#37051;&#23621;&#37319;&#26679;&#31574;&#30053;&#26469;&#36807;&#28388;&#22122;&#22768;&#33410;&#28857;&#21644;&#34917;&#20805;&#20449;&#24687;&#12290;&#20855;&#20307;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#36793;&#26435;&#37325;&#26469;&#33258;&#36866;&#24212;&#36873;&#25321;&#30456;&#20284;&#34892;&#20026;&#27169;&#24335;&#30340;&#37051;&#23621;&#33410;&#28857;&#65292;&#24182;&#25214;&#21040;&#22810;&#36339;&#37051;&#23621;&#26469;&#26816;&#27979;&#27450;&#35784;&#20132;&#26131;&#12290;</title><link>http://arxiv.org/abs/2307.05633</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20132;&#26131;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transaction Fraud Detection via an Adaptive Graph Neural Network. (arXiv:2307.05633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#32858;&#21512;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ASA-GNN&#65289;&#26469;&#25552;&#39640;&#20132;&#26131;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23398;&#20064;&#21028;&#21035;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#37051;&#23621;&#37319;&#26679;&#31574;&#30053;&#26469;&#36807;&#28388;&#22122;&#22768;&#33410;&#28857;&#21644;&#34917;&#20805;&#20449;&#24687;&#12290;&#20855;&#20307;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#36793;&#26435;&#37325;&#26469;&#33258;&#36866;&#24212;&#36873;&#25321;&#30456;&#20284;&#34892;&#20026;&#27169;&#24335;&#30340;&#37051;&#23621;&#33410;&#28857;&#65292;&#24182;&#25214;&#21040;&#22810;&#36339;&#37051;&#23621;&#26469;&#26816;&#27979;&#27450;&#35784;&#20132;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#20132;&#26131;&#27450;&#35784;&#26816;&#27979;&#65292;&#36825;&#23545;&#20010;&#20154;&#21644;&#38134;&#34892;&#30340;&#37329;&#34701;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#21033;&#29992;&#21407;&#22987;&#29305;&#24449;&#25110;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#29305;&#24449;&#24037;&#31243;&#12290;&#23427;&#20204;&#32570;&#20047;&#20174;&#20132;&#26131;&#25968;&#25454;&#20013;&#23398;&#20064;&#21028;&#21035;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#32780;&#19988;&#65292;&#29359;&#32618;&#20998;&#23376;&#36890;&#24120;&#36890;&#36807;&#27169;&#20223;&#25345;&#21345;&#20154;&#30340;&#34892;&#20026;&#26469;&#29359;&#32618;&#65292;&#36825;&#23548;&#33268;&#29616;&#26377;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#32858;&#21512;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ASA-GNN&#65289;&#65292;&#20854;&#23398;&#20064;&#21028;&#21035;&#34920;&#31034;&#20197;&#25552;&#39640;&#20132;&#26131;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#37051;&#23621;&#37319;&#26679;&#31574;&#30053;&#26469;&#36807;&#28388;&#22122;&#22768;&#33410;&#28857;&#65292;&#24182;&#20026;&#27450;&#35784;&#33410;&#28857;&#34917;&#20805;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#36793;&#26435;&#37325;&#26469;&#33258;&#36866;&#24212;&#36873;&#25321;&#20855;&#26377;&#30456;&#20284;&#34892;&#20026;&#27169;&#24335;&#30340;&#37051;&#23621;&#33410;&#28857;&#65292;&#24182;&#25214;&#21040;&#22810;&#36339;&#37051;&#23621;&#26469;&#26816;&#27979;&#27450;&#35784;&#20132;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning methods have been proposed to achieve accurate transaction fraud detection, which is essential to the financial security of individuals and banks. However, most existing methods leverage original features only or require manual feature engineering. They lack the ability to learn discriminative representations from transaction data. Moreover, criminals often commit fraud by imitating cardholders' behaviors, which causes the poor performance of existing detection models. In this paper, we propose an Adaptive Sampling and Aggregation-based Graph Neural Network (ASA-GNN) that learns discriminative representations to improve the performance of transaction fraud detection. A neighbor sampling strategy is performed to filter noisy nodes and supplement information for fraudulent nodes. Specifically, we leverage cosine similarity and edge weights to adaptively select neighbors with similar behavior patterns for target nodes and then find multi-hop neighbors for fraudulent 
&lt;/p&gt;</description></item><item><title>DNAGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#29420;&#29305;&#30340;&#26631;&#35760;&#35774;&#35745;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#12290;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05628</link><description>&lt;p&gt;
DNAGPT&#65306;&#29992;&#20110;&#22810;&#20010;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#30340;&#36890;&#29992;&#39044;&#35757;&#32451;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks. (arXiv:2307.05628v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05628
&lt;/p&gt;
&lt;p&gt;
DNAGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#29420;&#29305;&#30340;&#26631;&#35760;&#35774;&#35745;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#12290;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT&#31995;&#21015;&#30340;&#25104;&#21151;&#35777;&#26126;&#20102;GPT&#21487;&#20197;&#20174;&#24207;&#21015;&#20013;&#25552;&#21462;&#19968;&#33324;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#25152;&#26377;&#19979;&#28216;&#20219;&#21153;&#21463;&#30410;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25506;&#32034;DNA&#24207;&#21015;&#20013;&#30340;&#38544;&#34255;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;DNA&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#21644;&#20219;&#21153;&#38656;&#27714;&#38750;&#24120;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#65292;&#22240;&#20026;DNA&#30456;&#20851;&#25968;&#25454;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#22914;&#24207;&#21015;&#12289;&#34920;&#36798;&#27700;&#24179;&#31561;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#36825;&#20123;&#29305;&#28857;&#35774;&#35745;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DNAGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#22312;9&#20010;&#29289;&#31181;&#30340;&#36229;&#36807;100&#20159;&#20010;&#30897;&#22522;&#23545;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#38024;&#23545;&#20219;&#20309;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#25110;&#36755;&#20986;DNA&#24207;&#21015;&#21644;&#25968;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29420;&#29305;&#30340;&#26631;&#35760;&#35774;&#35745;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#20219;&#21153;&#38656;&#27714;&#26469;&#35774;&#35745;&#25552;&#31034;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of the GPT series proves that GPT can extract general information from sequences, thereby benefiting all downstream tasks. This motivates us to use pre-trained models to explore the hidden information in DNA sequences. However, data and task requirements in DNA sequence analysis are complexity and diversity as DNA relevant data includes different types of information, such as sequences, expression levels, etc, while there is currently no model specifically designed for these characteristics. Hereby, we present DNAGPT, a generalized foundation model pre-trained on over 10 billion base pairs from 9 species which can be fine-tuned for any DNA sequence analysis task. Our model can simultaneously process or output DNA sequences and numbers. In addition, our unique token design allows users to design prompts according to their own task requirements, making it applicable to any type of task. We have evaluated our model on classification, regression, and generation tasks. We demons
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#21551;&#21457;&#23398;&#20064;&#26694;&#26550;&#65288;CILF&#65289;&#65292;&#36890;&#36807;&#26126;&#30830;&#23450;&#20041;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#26469;&#35299;&#20915;&#36712;&#36857;&#39044;&#27979;&#20013;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05624</link><description>&lt;p&gt;
CILF:&#22240;&#26524;&#21551;&#21457;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CILF:Causality Inspired Learning Framework for Out-of-Distribution Vehicle Trajectory Prediction. (arXiv:2307.05624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05624
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#21551;&#21457;&#23398;&#20064;&#26694;&#26550;&#65288;CILF&#65289;&#65292;&#36890;&#36807;&#26126;&#30830;&#23450;&#20041;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#26469;&#35299;&#20915;&#36712;&#36857;&#39044;&#27979;&#20013;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20542;&#21521;&#20110;&#24314;&#27169;&#21382;&#21490;&#36712;&#36857;&#65288;&#36755;&#20837;&#65289;&#19982;&#26410;&#26469;&#36712;&#36857;&#65288;&#36755;&#20986;&#65289;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#30001;&#20110;&#30456;&#20851;&#24615;&#21482;&#26159;&#23545;&#29616;&#23454;&#30340;&#19968;&#31181;&#34920;&#38754;&#25551;&#36848;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#24182;&#23545;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#25935;&#24863;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36229;&#20986;&#20998;&#24067;&#22240;&#26524;&#22270;&#65288;OOD-CG&#65289;&#65292;&#23427;&#26126;&#30830;&#22320;&#23450;&#20041;&#20102;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#65292;&#21253;&#25324;&#19977;&#20010;&#32416;&#32544;&#30340;&#28508;&#22312;&#29305;&#24449;&#65306;1&#65289;&#39046;&#22495;&#19981;&#21464;&#30340;&#22240;&#26524;&#29305;&#24449;&#65288;IC&#65289;&#65292;2&#65289;&#39046;&#22495;&#21464;&#37327;&#30340;&#22240;&#26524;&#29305;&#24449;&#65288;VC&#65289;&#65292;3&#65289;&#39046;&#22495;&#21464;&#37327;&#30340;&#38750;&#22240;&#26524;&#29305;&#24449;&#65288;VN&#65289;&#12290;&#36825;&#20123;&#29305;&#24449;&#21463;&#21040;&#28151;&#28102;&#22240;&#23376;&#65288;C&#65289;&#21644;&#39046;&#22495;&#36873;&#25321;&#22120;&#65288;D&#65289;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#21033;&#29992;&#22240;&#26524;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#21551;&#21457;&#23398;&#20064;&#26694;&#26550;&#65288;CILF&#65289;&#65292;&#23427;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;1&#65289;&#36890;&#36807;&#19981;&#21464;&#24615;&#25439;&#22833;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#22240;&#26524;&#29305;&#24449;&#65292;2&#65289;&#36890;&#36807;&#22240;&#26524;&#24615;&#25439;&#22833;&#25552;&#21462;&#39046;&#22495;&#21464;&#37327;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Trajectory prediction is critical for autonomous driving vehicles. Most existing methods tend to model the correlation between history trajectory (input) and future trajectory (output). Since correlation is just a superficial description of reality, these methods rely heavily on the i.i.d. assumption and evince a heightened susceptibility to out-of-distribution data. To address this problem, we propose an Out-of- Distribution Causal Graph (OOD-CG), which explicitly defines the underlying causal structure of the data with three entangled latent features: 1) domain-invariant causal feature (IC), 2) domain-variant causal feature (VC), and 3) domain-variant non-causal feature (VN ). While these features are confounded by confounder (C) and domain selector (D). To leverage causal features for prediction, we propose a Causal Inspired Learning Framework (CILF), which includes three steps: 1) extracting domain-invariant causal feature by means of an invariance loss, 2) extracting domain varian
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25512;&#26029;OD&#24207;&#21015;&#30340;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#32467;&#26500;&#32422;&#26463;&#25351;&#23548;&#20256;&#32479;&#30340;&#25968;&#20540;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#39046;&#22495;&#20013;&#38745;&#24577;&#21644;&#21160;&#24577;OD&#30697;&#38453;&#20272;&#35745;&#20013;&#30340;&#27424;&#23450;&#21644;&#28382;&#21518;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05623</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#24577;&#20272;&#35745;&#20986;&#21457;&#22320;-&#30446;&#30340;&#22320;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A DeepLearning Framework for Dynamic Estimation of Origin-Destination Sequence. (arXiv:2307.05623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25512;&#26029;OD&#24207;&#21015;&#30340;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#32467;&#26500;&#32422;&#26463;&#25351;&#23548;&#20256;&#32479;&#30340;&#25968;&#20540;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#39046;&#22495;&#20013;&#38745;&#24577;&#21644;&#21160;&#24577;OD&#30697;&#38453;&#20272;&#35745;&#20013;&#30340;&#27424;&#23450;&#21644;&#28382;&#21518;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OD&#30697;&#38453;&#20272;&#35745;&#26159;&#20132;&#36890;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20027;&#35201;&#26041;&#27861;&#20351;&#29992;&#20132;&#36890;&#20256;&#24863;&#22120;&#27979;&#37327;&#20449;&#24687;&#65288;&#22914;&#20132;&#36890;&#27969;&#37327;&#65289;&#26469;&#20272;&#35745;&#30001;OD&#30697;&#38453;&#34920;&#31034;&#30340;&#20132;&#36890;&#38656;&#27714;&#12290;&#35813;&#38382;&#39064;&#20998;&#20026;&#38745;&#24577;OD&#30697;&#38453;&#20272;&#35745;&#21644;&#21160;&#24577;OD&#30697;&#38453;&#24207;&#21015;&#65288;&#31616;&#31216;OD&#24207;&#21015;&#65289;&#20272;&#35745;&#20004;&#31867;&#12290;&#19978;&#36848;&#20004;&#31181;&#26041;&#27861;&#38754;&#20020;&#30001;&#20110;&#22823;&#37327;&#20272;&#35745;&#21442;&#25968;&#21644;&#19981;&#36275;&#30340;&#32422;&#26463;&#20449;&#24687;&#36896;&#25104;&#30340;&#27424;&#23450;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;OD&#24207;&#21015;&#20272;&#35745;&#36824;&#38754;&#20020;&#28382;&#21518;&#25361;&#25112;&#65306;&#30001;&#20110;&#25317;&#22581;&#31561;&#19981;&#21516;&#20132;&#36890;&#26465;&#20214;&#65292;&#30456;&#21516;&#30340;&#36710;&#36742;&#22312;&#21516;&#19968;&#35266;&#27979;&#26102;&#27573;&#20869;&#20250;&#20986;&#29616;&#22312;&#19981;&#21516;&#30340;&#36335;&#27573;&#19978;&#65292;&#23548;&#33268;&#30456;&#21516;&#30340;OD&#38656;&#27714;&#23545;&#24212;&#19981;&#21516;&#30340;&#34892;&#31243;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25512;&#26029;OD&#24207;&#21015;&#30340;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#32467;&#26500;&#32422;&#26463;&#25351;&#23548;&#20256;&#32479;&#30340;&#25968;&#20540;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
OD matrix estimation is a critical problem in the transportation domain. The principle method uses the traffic sensor measured information such as traffic counts to estimate the traffic demand represented by the OD matrix. The problem is divided into two categories: static OD matrix estimation and dynamic OD matrices sequence(OD sequence for short) estimation. The above two face the underdetermination problem caused by abundant estimated parameters and insufficient constraint information. In addition, OD sequence estimation also faces the lag challenge: due to different traffic conditions such as congestion, identical vehicle will appear on different road sections during the same observation period, resulting in identical OD demands correspond to different trips. To this end, this paper proposes an integrated method, which uses deep learning methods to infer the structure of OD sequence and uses structural constraints to guide traditional numerical optimization. Our experiments show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LS-PIE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#32447;&#24615;&#28508;&#22312;&#31354;&#38388;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#21270;&#28508;&#22312;&#21521;&#37327;&#30340;&#32858;&#31867;&#21644;&#25490;&#24207;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#31561;&#32447;&#24615;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05620</link><description>&lt;p&gt;
&#28508;&#22312;&#31354;&#38388;&#27934;&#23519;&#21147;&#21644;&#35299;&#37322;&#22686;&#24378;&#65288;LS-PIE&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Latent Space Perspicacity and Interpretation Enhancement (LS-PIE) Framework. (arXiv:2307.05620v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LS-PIE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#32447;&#24615;&#28508;&#22312;&#31354;&#38388;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#21270;&#28508;&#22312;&#21521;&#37327;&#30340;&#32858;&#31867;&#21644;&#25490;&#24207;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#31561;&#32447;&#24615;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#28508;&#21464;&#37327;&#27169;&#22411;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#12289;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#21644;&#22240;&#23376;&#20998;&#26512;&#65288;FA&#65289;&#36890;&#24120;&#20250;&#35782;&#21035;&#26377;&#24207;&#25110;&#26080;&#24207;&#30340;&#28508;&#22312;&#26041;&#21521;&#65288;&#25110;&#36733;&#33655;&#65289;&#12290;&#28982;&#21518;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#28508;&#22312;&#26041;&#21521;&#19978;&#20197;&#33719;&#24471;&#23427;&#20204;&#30340;&#25237;&#24433;&#34920;&#31034;&#65288;&#25110;&#24471;&#20998;&#65289;&#12290;&#28982;&#32780;&#65292;ICA&#27714;&#35299;&#22120;&#36890;&#24120;&#26080;&#24207;&#22320;&#36820;&#22238;&#29420;&#31435;&#26041;&#21521;&#65292;&#24182;&#19988;&#24448;&#24448;&#23558;&#21333;&#19968;&#28304;&#20998;&#24067;&#22312;&#22810;&#20010;&#26041;&#21521;&#19978;&#20316;&#20026;&#22810;&#20010;&#23376;&#28304;&#65292;&#36825;&#23545;&#20110;&#20854;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#24456;&#19981;&#21033;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#32447;&#24615;&#28508;&#22312;&#31354;&#38388;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#23613;&#31649;&#26412;&#25991;&#20171;&#32461;&#30340;&#27010;&#24565;&#19982;&#35821;&#35328;&#26080;&#20851;&#65292;&#20294;&#35813;&#26694;&#26550;&#26159;&#29992;Python&#32534;&#20889;&#30340;&#12290;&#35813;&#26694;&#26550;&#33258;&#21160;&#21270;&#20102;&#28508;&#22312;&#21521;&#37327;&#30340;&#32858;&#31867;&#21644;&#25490;&#24207;&#65292;&#20197;&#22686;&#24378;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). For example, PCA solvers usually rank the principal directions by explaining the most to least variance, while ICA solvers usually return independent directions unordered and often with single sources spread across multiple directions as multiple sub-sources, which is of severe detriment to their usability and interpretability.  This paper proposes a general framework to enhance latent space representations for improving the interpretability of linear latent spaces. Although the concepts in this paper are language agnostic, the framework is written in Python. This framework automates the clustering and ranking of latent vectors to enhance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#23545;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#26631;&#31614;&#32534;&#30721;&#21644;&#29420;&#28909;&#32534;&#30721;&#30340;&#24615;&#33021;&#12290;&#21457;&#29616;&#34429;&#28982;&#29420;&#28909;&#32534;&#30721;&#20250;&#24102;&#26469;&#36731;&#24494;&#24615;&#33021;&#25439;&#22833;&#65292;&#20294;&#21487;&#20197;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24110;&#21161;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#12290;&#20351;&#29992;&#29420;&#28909;&#32534;&#30721;&#36824;&#21487;&#20197;&#20943;&#23567;&#35299;&#37322;&#25991;&#20214;&#22823;&#23567;&#65292;&#32553;&#30701;&#20154;&#24037;&#20998;&#26512;&#26102;&#38388;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;XAI&#30740;&#31350;&#20013;&#37325;&#35270;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05614</link><description>&lt;p&gt;
&#29305;&#24449;&#32534;&#30721;&#23545;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Feature Encoding on Malware Classification Explainability. (arXiv:2307.05614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#23545;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#26631;&#31614;&#32534;&#30721;&#21644;&#29420;&#28909;&#32534;&#30721;&#30340;&#24615;&#33021;&#12290;&#21457;&#29616;&#34429;&#28982;&#29420;&#28909;&#32534;&#30721;&#20250;&#24102;&#26469;&#36731;&#24494;&#24615;&#33021;&#25439;&#22833;&#65292;&#20294;&#21487;&#20197;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24110;&#21161;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#12290;&#20351;&#29992;&#29420;&#28909;&#32534;&#30721;&#36824;&#21487;&#20197;&#20943;&#23567;&#35299;&#37322;&#25991;&#20214;&#22823;&#23567;&#65292;&#32553;&#30701;&#20154;&#24037;&#20998;&#26512;&#26102;&#38388;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;XAI&#30740;&#31350;&#20013;&#37325;&#35270;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#23545;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65288;XAI&#65289;&#30340;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#19968;&#20010;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;XGBoost&#27169;&#22411;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#29305;&#24449;&#32534;&#30721;&#26041;&#27861;&#65306;&#26631;&#31614;&#32534;&#30721;&#65288;LE&#65289;&#21644;&#29420;&#28909;&#32534;&#30721;&#65288;OHE&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20351;&#29992;OHE&#30456;&#27604;LE&#20250;&#24102;&#26469;&#36731;&#24494;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;OHE&#25152;&#25552;&#20379;&#30340;&#26356;&#35814;&#32454;&#30340;&#35299;&#37322;&#24357;&#34917;&#20102;&#36825;&#31181;&#25439;&#22833;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;OHE&#33021;&#22815;&#26356;&#35814;&#32454;&#22320;&#25506;&#32034;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#30340;&#32454;&#33410;&#65292;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#20351;&#29992;OHE&#20250;&#23548;&#33268;&#36739;&#23567;&#30340;&#35299;&#37322;&#25991;&#20214;&#21644;&#20943;&#23569;&#20154;&#24037;&#20998;&#26512;&#30340;&#26102;&#38388;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;XAI&#30740;&#31350;&#20013;&#32771;&#34385;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24314;&#35758;&#36890;&#36807;&#32467;&#21512;&#20854;&#20182;&#32534;&#30721;&#26041;&#27861;&#21644;&#21019;&#26032;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the impact of feature encoding techniques on the explainability of XAI (Explainable Artificial Intelligence) algorithms. Using a malware classification dataset, we trained an XGBoost model and compared the performance of two feature encoding methods: Label Encoding (LE) and One Hot Encoding (OHE). Our findings reveal a marginal performance loss when using OHE instead of LE. However, the more detailed explanations provided by OHE compensated for this loss. We observed that OHE enables deeper exploration of details in both global and local contexts, facilitating more comprehensive answers. Additionally, we observed that using OHE resulted in smaller explanation files and reduced analysis time for human analysts. These findings emphasize the significance of considering feature encoding techniques in XAI research and suggest potential for further exploration by incorporating additional encoding methods and innovative visualization approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#23884;&#20837;&#20013;&#30340;&#38750;&#35821;&#20041;&#20449;&#24687;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#36716;&#25442;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20845;&#20010;&#23884;&#20837;&#21487;&#20197;&#35782;&#21035;&#20986;&#22810;&#20010;&#36716;&#25442;&#12290;&#36825;&#23545;&#20110;&#35757;&#32451;&#31639;&#27861;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.05610</link><description>&lt;p&gt;
&#29289;&#36136;&#36824;&#26159;&#39118;&#26684;&#65306;&#20320;&#30340;&#22270;&#20687;&#23884;&#20837;&#30693;&#36947;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Substance or Style: What Does Your Image Embedding Know?. (arXiv:2307.05610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#23884;&#20837;&#20013;&#30340;&#38750;&#35821;&#20041;&#20449;&#24687;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#36716;&#25442;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20845;&#20010;&#23884;&#20837;&#21487;&#20197;&#35782;&#21035;&#20986;&#22810;&#20010;&#36716;&#25442;&#12290;&#36825;&#23545;&#20110;&#35757;&#32451;&#31639;&#27861;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#38024;&#26159;&#19968;&#31181;&#20174;&#23884;&#20837;&#20013;&#39044;&#27979;&#24213;&#23618;&#25968;&#25454;&#23646;&#24615;&#30340;&#23567;&#22411;&#32593;&#32476;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#38024;&#23545;&#24615;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#23884;&#20837;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#25506;&#38024;&#36827;&#34892;&#20998;&#26512;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24050;&#32463;&#24456;&#24120;&#35265;&#20102;&#65292;&#20294;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#25506;&#32034;&#21364;&#27604;&#36739;&#23569;&#12290;&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#20027;&#35201;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#20869;&#23481;&#12290;&#26356;&#22909;&#22320;&#29702;&#35299;&#27969;&#34892;&#23884;&#20837;&#65288;&#22914;MAE&#65292;SimCLR&#25110;CLIP&#65289;&#20013;&#30340;&#38750;&#35821;&#20041;&#20449;&#24687;&#65292;&#23558;&#20026;&#35757;&#32451;&#31639;&#27861;&#21644;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#29992;&#36884;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#36716;&#25442;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#27979;&#37327;&#23884;&#20837;&#30340;&#35270;&#35273;&#20869;&#23481;&#65292;&#21253;&#25324;&#22270;&#20687;&#39118;&#26684;&#12289;&#36136;&#37327;&#20197;&#21450;&#21508;&#31181;&#33258;&#28982;&#21644;&#20154;&#24037;&#36716;&#25442;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26377;&#20845;&#20010;&#23884;&#20837;&#65288;&#21253;&#25324;SimCLR&#65289;&#32534;&#30721;&#20102;&#36275;&#22815;&#30340;&#38750;&#35821;&#20041;&#20449;&#24687;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#25968;&#21313;&#20010;&#36716;&#25442;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19968;&#20010;&#27867;&#21270;&#20219;&#21153;&#65292;&#23558;&#30456;&#20284;&#30340;&#36716;&#25442;&#20998;&#32452;&#65292;&#24182;&#30041;&#20986;&#19968;&#37096;&#20998;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probes are small networks that predict properties of underlying data from embeddings, and they provide a targeted, effective way to illuminate the information contained in embeddings. While analysis through the use of probes has become standard in NLP, there has been much less exploration in vision. Image foundation models have primarily been evaluated for semantic content. Better understanding the non-semantic information in popular embeddings (e.g., MAE, SimCLR, or CLIP) will shed new light both on the training algorithms and on the uses for these foundation models. We design a systematic transformation prediction task and measure the visual content of embeddings along many axes, including image style, quality, and a range of natural and artificial transformations. Surprisingly, six embeddings (including SimCLR) encode enough non-semantic information to identify dozens of transformations. We also consider a generalization task, where we group similar transformations and hold out seve
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#25913;&#36827;&#29616;&#26377;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#20854;&#20313;&#34892;&#32780;&#25913;&#36827;&#31243;&#24207;&#30340;&#21333;&#20010;&#34892;&#65292;&#20197;&#26174;&#30528;&#25913;&#36827;&#31243;&#24207;&#24615;&#33021;&#65292;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#21487;&#34913;&#37327;&#30446;&#26631;&#30340;&#32534;&#31243;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05603</link><description>&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#20248;&#21270;&#31243;&#24207;&#65306;&#20320;&#33021;&#25913;&#36827;&#25105;&#30340;&#20195;&#30721;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can You Improve My Code? Optimizing Programs with Local Search. (arXiv:2307.05603v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05603
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#25913;&#36827;&#29616;&#26377;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#20854;&#20313;&#34892;&#32780;&#25913;&#36827;&#31243;&#24207;&#30340;&#21333;&#20010;&#34892;&#65292;&#20197;&#26174;&#30528;&#25913;&#36827;&#31243;&#24207;&#24615;&#33021;&#65292;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#21487;&#34913;&#37327;&#30446;&#26631;&#30340;&#32534;&#31243;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#25913;&#36827;&#29616;&#26377;&#31243;&#24207;&#22312;&#21487;&#34913;&#37327;&#30446;&#26631;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;POLIS&#65288;Program Optimization with Locally Improving Search&#65289;&#21033;&#29992;&#31243;&#24207;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;&#22266;&#23450;&#20854;&#20313;&#34892;&#32780;&#25913;&#36827;&#31243;&#24207;&#30340;&#21333;&#20010;&#34892;&#65292;&#20351;&#29992;&#29616;&#26377;&#30340;&#26292;&#21147;&#21512;&#25104;&#31639;&#27861;&#65292;&#24182;&#22312;&#26080;&#27861;&#25913;&#36827;&#31243;&#24207;&#24615;&#33021;&#26102;&#32487;&#32493;&#36845;&#20195;&#12290;POLIS&#22312;&#19968;&#20010;&#30001;27&#21517;&#21442;&#19982;&#32773;&#32452;&#25104;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21442;&#19982;&#32773;&#32534;&#20889;&#31243;&#24207;&#20197;&#23581;&#35797;&#26368;&#22823;&#21270;&#20004;&#20010;&#21333;&#19968;&#20195;&#29702;&#28216;&#25103;&#65288;Lunar Lander&#21644;Highway&#65289;&#30340;&#24471;&#20998;&#12290;POLIS&#33021;&#22815;&#22312;&#28216;&#25103;&#24471;&#20998;&#26041;&#38754;&#26174;&#30528;&#25913;&#36827;&#21442;&#19982;&#32773;&#30340;&#31243;&#24207;&#12290;&#23545;&#29616;&#26377;Stack Overflow&#20195;&#30721;&#30340;&#27010;&#24565;&#39564;&#35777;&#28436;&#31034;&#27979;&#37327;&#20102;&#20854;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;POLIS&#21487;&#29992;&#20316;&#21487;&#34913;&#37327;&#30446;&#26631;&#30340;&#32534;&#31243;&#38382;&#39064;&#30340;&#26377;&#30410;&#32534;&#31243;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a local search method for improving an existing program with respect to a measurable objective. Program Optimization with Locally Improving Search (POLIS) exploits the structure of a program, defined by its lines. POLIS improves a single line of the program while keeping the remaining lines fixed, using existing brute-force synthesis algorithms, and continues iterating until it is unable to improve the program's performance. POLIS was evaluated with a 27-person user study, where participants wrote programs attempting to maximize the score of two single-agent games: Lunar Lander and Highway. POLIS was able to substantially improve the participants' programs with respect to the game scores. A proof-of-concept demonstration on existing Stack Overflow code measures applicability in real-world problems. These results suggest that POLIS could be used as a helpful programming assistant for programming problems with measurable objectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25913;&#36827;&#19981;&#21516;&#39046;&#22495;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05601</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation with Deep Neural-Network. (arXiv:2307.05601v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25913;&#36827;&#19981;&#21516;&#39046;&#22495;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#25253;&#21578;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#25913;&#36827;&#28508;&#21147;&#65292;&#20026;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#35813;&#30740;&#31350;&#30340;&#32467;&#26524;&#20026;&#39046;&#22495;&#33258;&#36866;&#24212;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#26041;&#27861;&#30340;&#28436;&#36827;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report contributes to the field of unsupervised domain adaptation by providing an analysis of existing methods, introducing a new approach, and demonstrating the potential for improving visual recognition tasks across different domains. The results of this study open up opportunities for further study and development of advanced methods in the field of domain adaptation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#32452;&#21512;&#24615;&#27867;&#21270;&#35270;&#20026;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#23548;&#20986;&#23545;&#35757;&#32451;&#20998;&#24067;&#25903;&#25345;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#26465;&#20214;&#35201;&#27714;&#65292;&#23454;&#29616;&#20102;&#32452;&#21512;&#24615;&#27867;&#21270;&#12290;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#32452;&#21512;&#24615;&#27867;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#29702;&#35770;&#24615;&#30340;&#30740;&#31350;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.05596</link><description>&lt;p&gt;
&#20174;&#31532;&#19968;&#21407;&#29702;&#20013;&#23454;&#29616;&#32452;&#21512;&#24615;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Compositional Generalization from First Principles. (arXiv:2307.05596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#32452;&#21512;&#24615;&#27867;&#21270;&#35270;&#20026;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#23548;&#20986;&#23545;&#35757;&#32451;&#20998;&#24067;&#25903;&#25345;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#26465;&#20214;&#35201;&#27714;&#65292;&#23454;&#29616;&#20102;&#32452;&#21512;&#24615;&#27867;&#21270;&#12290;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#32452;&#21512;&#24615;&#27867;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#29702;&#35770;&#24615;&#30340;&#30740;&#31350;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#25105;&#20204;&#19990;&#30028;&#30340;&#32452;&#21512;&#24615;&#36136;&#21152;&#24555;&#23398;&#20064;&#21644;&#20419;&#36827;&#27867;&#21270;&#26159;&#20154;&#31867;&#24863;&#30693;&#30340;&#19968;&#20010;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#21363;&#20351;&#23545;&#20110;&#20855;&#26377;&#26126;&#30830;&#32452;&#21512;&#24615;&#20808;&#39564;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#32452;&#21512;&#24615;&#27867;&#21270;&#20063;&#26159;&#19968;&#20010;&#38590;&#20197;&#23454;&#29616;&#30340;&#30446;&#26631;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#32452;&#21512;&#24615;&#27867;&#21270;&#65292;&#25105;&#20204;&#20174;&#24213;&#23618;&#24320;&#22987;&#36827;&#34892;&#25506;&#32034;&#65306;&#21463;&#21487;&#35782;&#21035;&#34920;&#31034;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#32452;&#21512;&#24615;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#23646;&#24615;&#65292;&#32780;&#19981;&#26159;&#25968;&#25454;&#26412;&#36523;&#12290;&#36825;&#31181;&#25913;&#36827;&#20351;&#25105;&#20204;&#33021;&#22815;&#23548;&#20986;&#20165;&#23545;&#35757;&#32451;&#20998;&#24067;&#30340;&#25903;&#25345;&#21644;&#27169;&#22411;&#26550;&#26500;&#26377;&#36731;&#24494;&#26465;&#20214;&#30340;&#35201;&#27714;&#65292;&#36825;&#20123;&#26465;&#20214;&#36275;&#20197;&#23454;&#29616;&#32452;&#21512;&#24615;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#22914;&#20309;&#24212;&#29992;&#20110;&#29616;&#23454;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#32452;&#21512;&#24615;&#27867;&#21270;&#30340;&#21407;&#21017;&#24615;&#29702;&#35770;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging the compositional nature of our world to expedite learning and facilitate generalization is a hallmark of human perception. In machine learning, on the other hand, achieving compositional generalization has proven to be an elusive goal, even for models with explicit compositional priors. To get a better handle on compositional generalization, we here approach it from the bottom up: Inspired by identifiable representation learning, we investigate compositionality as a property of the data-generating process rather than the data itself. This reformulation enables us to derive mild conditions on only the support of the training distribution and the model architecture, which are sufficient for compositional generalization. We further demonstrate how our theoretical framework applies to real-world scenarios and validate our findings empirically. Our results set the stage for a principled theoretical study of compositional generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36870;UQ&#36807;&#31243;&#65292;&#29992;&#20110;&#26102;&#38388;&#30456;&#20851;&#21709;&#24212;&#30340;&#27169;&#22411;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21151;&#33021;&#23545;&#40784;&#26041;&#27861;&#35299;&#20915;&#20102;PCT&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28201;&#24230;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05592</link><description>&lt;p&gt;
&#22522;&#20110;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;&#36870;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19982;&#30636;&#24577;&#23454;&#39564;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Functional PCA and Deep Neural Networks-based Bayesian Inverse Uncertainty Quantification with Transient Experimental Data. (arXiv:2307.05592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36870;UQ&#36807;&#31243;&#65292;&#29992;&#20110;&#26102;&#38388;&#30456;&#20851;&#21709;&#24212;&#30340;&#27169;&#22411;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21151;&#33021;&#23545;&#40784;&#26041;&#27861;&#35299;&#20915;&#20102;PCT&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28201;&#24230;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;UQ&#26159;&#22522;&#20110;&#23454;&#39564;&#25968;&#25454;&#23545;&#27169;&#22411;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#36870;&#21521;&#37327;&#21270;&#30340;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#21457;&#23637;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#30456;&#20851;&#21709;&#24212;&#30340;&#36870;UQ&#36807;&#31243;&#65292;&#21033;&#29992;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21644;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#38477;&#32500;&#12290;&#35813;&#28436;&#31034;&#22522;&#20110;&#20351;&#29992;FEBA&#30636;&#24577;&#23454;&#39564;&#25968;&#25454;&#26469;&#36870;&#21521;&#37327;&#21270;TRACE&#29289;&#29702;&#27169;&#22411;&#21442;&#25968;&#65292;&#27979;&#37327;&#25968;&#25454;&#26159;&#26102;&#38388;&#30456;&#20851;&#30340;&#26368;&#39640;&#21253;&#35206;&#28201;&#24230;&#65288;PCT&#65289;&#12290;&#30001;&#20110;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#65288;QoI&#65289;&#26159;&#26102;&#38388;&#30456;&#20851;&#30340;&#65292;&#23545;&#24212;&#20110;&#26080;&#38480;&#32500;&#21709;&#24212;&#65292;&#22240;&#27492;&#20351;&#29992;PCA&#23545;QoI&#32500;&#24230;&#36827;&#34892;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#30041;PCT&#30340;&#30636;&#24577;&#29305;&#24449;&#65292;&#20197;&#20351;&#36870;UQ&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#20256;&#32479;PCA&#21040;PCT&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#25968;&#25454;&#65292;&#22240;&#20026;&#22312;&#28140;&#28781;&#26102;&#21051;&#20250;&#20986;&#29616;&#31361;&#28982;&#30340;&#28201;&#24230;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21151;&#33021;&#23545;&#40784;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse UQ is the process to inversely quantify the model input uncertainties based on experimental data. This work focuses on developing an inverse UQ process for time-dependent responses, using dimensionality reduction by functional principal component analysis (PCA) and deep neural network (DNN)-based surrogate models. The demonstration is based on the inverse UQ of TRACE physical model parameters using the FEBA transient experimental data. The measurement data is time-dependent peak cladding temperature (PCT). Since the quantity-of-interest (QoI) is time-dependent that corresponds to infinite-dimensional responses, PCA is used to reduce the QoI dimension while preserving the transient profile of the PCT, in order to make the inverse UQ process more efficient. However, conventional PCA applied directly to the PCT time series profiles can hardly represent the data precisely due to the sudden temperature drop at the time of quenching. As a result, a functional alignment method is used
&lt;/p&gt;</description></item><item><title>SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.05591</link><description>&lt;p&gt;
SITTA: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05591
&lt;/p&gt;
&lt;p&gt;
SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#20687;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#29702;&#35299;&#23545;&#20110;&#29983;&#25104;&#36866;&#24403;&#30340;&#25551;&#36848;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#38656;&#35201;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35780;&#20272;&#22330;&#26223;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#30693;&#35782;&#34920;&#31034;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22312;&#20445;&#35777;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#30340;&#21516;&#26102;&#23454;&#29616;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34987;&#26465;&#20214;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;-&#25991;&#26412;&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#20351;&#29992;&#22270;&#20687;&#36755;&#20837;&#12290;&#36825;&#35201;&#27714;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20013;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#20256;&#36882;&#32473;LM&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36716;&#31227;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#22810;&#27169;&#24577;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35270;&#39057;&#20998;&#31867;&#20013;&#30340;&#26631;&#27880;&#20943;&#36127;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#26469;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.05587</link><description>&lt;p&gt;
&#20351;&#29992;&#24103;&#32423;&#21035;&#26597;&#35810;&#30340;&#20027;&#21160;&#23398;&#20064;&#35270;&#39057;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Video Classification with Frame Level Queries. (arXiv:2307.05587v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35270;&#39057;&#20998;&#31867;&#20013;&#30340;&#26631;&#27880;&#20943;&#36127;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#26469;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#36793;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#24378;&#22823;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#20154;&#21147;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35270;&#39057;&#20998;&#31867;&#31561;&#24212;&#29992;&#20013;&#26356;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#20154;&#24037;&#26631;&#27880;&#32773;&#24517;&#39035;&#23436;&#25972;&#22320;&#35266;&#30475;&#25972;&#20010;&#35270;&#39057;&#20197;&#25552;&#20379;&#26631;&#31614;&#12290;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#65292;&#36825;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#24037;&#20316;&#37327;&#65292;&#21482;&#38656;&#35201;&#25163;&#21160;&#26631;&#35760;&#31639;&#27861;&#35782;&#21035;&#20986;&#30340;&#23569;&#25968;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35270;&#39057;&#20998;&#31867;&#65292;&#26088;&#22312;&#36827;&#19968;&#27493;&#20943;&#36731;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35782;&#21035;&#19968;&#25209;&#22270;&#20687;&#26679;&#26412;&#35270;&#39057;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep learning algorithms have pushed the boundaries of computer vision research and have depicted commendable performance in a variety of applications. However, training a robust deep neural network necessitates a large amount of labeled training data, acquiring which involves significant time and human effort. This problem is even more serious for an application like video classification, where a human annotator has to watch an entire video end-to-end to furnish a label. Active learning algorithms automatically identify the most informative samples from large amounts of unlabeled data; this tremendously reduces the human annotation effort in inducing a machine learning model, as only the few samples that are identified by the algorithm, need to be labeled manually. In this paper, we propose a novel active learning framework for video classification, with the goal of further reducing the labeling onus on the human annotators. Our framework identifies a batch of exemplar videos, togethe
&lt;/p&gt;</description></item><item><title>DBFed&#26159;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#24615;&#30340;&#21435;&#20559;&#24046;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#25968;&#25454;&#36136;&#37327;&#24046;&#24322;&#24341;&#36215;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05582</link><description>&lt;p&gt;
DBFed: &#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#24615;&#30340;&#21435;&#20559;&#24046;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DBFed: Debiasing Federated Learning Framework based on Domain-Independent. (arXiv:2307.05582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05582
&lt;/p&gt;
&lt;p&gt;
DBFed&#26159;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#24615;&#30340;&#21435;&#20559;&#24046;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#25968;&#25454;&#36136;&#37327;&#24046;&#24322;&#24341;&#36215;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#25345;&#32493;&#36827;&#34892;&#65292;&#20225;&#19994;&#27491;&#22312;&#20135;&#29983;&#12289;&#31649;&#29702;&#21644;&#23384;&#20648;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20063;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#32473;&#20449;&#24687;&#23433;&#20840;&#21644;&#25968;&#25454;&#23433;&#20840;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25968;&#25454;&#23433;&#20840;&#26159;&#25351;&#22312;&#20854;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20869;&#65292;&#20445;&#25252;&#25968;&#23383;&#20449;&#24687;&#20813;&#21463;&#26410;&#32463;&#25480;&#26435;&#30340;&#35775;&#38382;&#12289;&#25439;&#22351;&#12289;&#30423;&#31363;&#31561;&#30340;&#25439;&#23475;&#12290;&#38543;&#30528;&#25968;&#25454;&#23433;&#20840;&#27861;&#30340;&#39041;&#24067;&#21644;&#25191;&#34892;&#20197;&#21450;&#32452;&#32455;&#21644;&#29992;&#25143;&#23545;&#25968;&#25454;&#23433;&#20840;&#21644;&#25968;&#25454;&#38544;&#31169;&#30340;&#37325;&#35270;&#65292;&#20197;&#32852;&#37030;&#23398;&#20064;&#20026;&#20195;&#34920;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#26694;&#26550;&#65292;&#20801;&#35768;&#22810;&#20010;&#20027;&#20307;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20849;&#21516;&#27169;&#22411;&#35757;&#32451;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#35299;&#20915;&#25968;&#25454;&#23396;&#23707;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#20027;&#20307;&#20043;&#38388;&#30340;&#25968;&#25454;&#24444;&#27492;&#29420;&#31435;&#65292;&#32780;&#36136;&#37327;&#19978;&#30340;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As digital transformation continues, enterprises are generating, managing, and storing vast amounts of data, while artificial intelligence technology is rapidly advancing. However, it brings challenges in information security and data security. Data security refers to the protection of digital information from unauthorized access, damage, theft, etc. throughout its entire life cycle. With the promulgation and implementation of data security laws and the emphasis on data security and data privacy by organizations and users, Privacy-preserving technology represented by federated learning has a wide range of application scenarios. Federated learning is a distributed machine learning computing framework that allows multiple subjects to train joint models without sharing data to protect data privacy and solve the problem of data islands. However, the data among multiple subjects are independent of each other, and the data differences in quality may cause fairness issues in federated learnin
&lt;/p&gt;</description></item><item><title>RidgeBase&#26159;&#19968;&#20010;&#36328;&#20256;&#24863;&#22120;&#22810;&#25351;&#38750;&#25509;&#35302;&#25351;&#32441;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#19981;&#21516;&#21305;&#37197;&#22330;&#26223;&#19979;&#38750;&#25509;&#35302;&#25351;&#32441;&#21305;&#37197;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.05563</link><description>&lt;p&gt;
RidgeBase&#65306;&#19968;&#31181;&#36328;&#20256;&#24863;&#22120;&#22810;&#25351;&#38750;&#25509;&#35302;&#25351;&#32441;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset. (arXiv:2307.05563v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05563
&lt;/p&gt;
&lt;p&gt;
RidgeBase&#26159;&#19968;&#20010;&#36328;&#20256;&#24863;&#22120;&#22810;&#25351;&#38750;&#25509;&#35302;&#25351;&#32441;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#19981;&#21516;&#21305;&#37197;&#22330;&#26223;&#19979;&#38750;&#25509;&#35302;&#25351;&#32441;&#21305;&#37197;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#30456;&#26426;&#36827;&#34892;&#38750;&#25509;&#35302;&#25351;&#32441;&#21305;&#37197;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#25351;&#32441;&#31995;&#32479;&#30340;&#19968;&#20123;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#21355;&#29983;&#37319;&#38598;&#12289;&#20415;&#25658;&#24615;&#21644;&#25915;&#20987;&#38450;&#33539;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#21644;&#40065;&#26834;&#30340;&#38750;&#25509;&#35302;&#25351;&#32441;&#21305;&#37197;&#25216;&#26415;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#28608;&#21457;&#22312;&#19981;&#21516;&#20256;&#24863;&#22120;&#20043;&#38388;&#36827;&#19968;&#27493;&#21457;&#23637;&#38750;&#25509;&#35302;&#25351;&#32441;&#21305;&#37197;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RidgeBase&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;RidgeBase&#21253;&#21547;&#20102;&#26469;&#33258;88&#20010;&#20010;&#20307;&#65292;&#22312;&#19981;&#21516;&#32972;&#26223;&#21644;&#29031;&#26126;&#26465;&#20214;&#19979;&#20351;&#29992;&#20004;&#20010;&#26234;&#33021;&#25163;&#26426;&#30456;&#26426;&#21644;&#19968;&#20010;&#24179;&#26495;&#35302;&#25720;&#20256;&#24863;&#22120;&#33719;&#24471;&#30340;&#36229;&#36807;15,000&#20010;&#38750;&#25509;&#35302;&#21644;&#25509;&#35302;&#24335;&#25351;&#32441;&#22270;&#20687;&#23545;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;RidgeBase&#26088;&#22312;&#20419;&#36827;&#22312;&#19981;&#21516;&#21305;&#37197;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#21333;&#25351;&#21305;&#37197;&#21644;&#22810;&#25351;&#21305;&#37197;&#65292;&#26082;&#21253;&#25324;&#38750;&#25509;&#35302;&#21040;&#38750;&#25509;&#35302;&#65288;CL2CL&#65289;&#30340;&#39564;&#35777;&#21644;&#35782;&#21035;&#65292;&#20063;&#21253;&#25324;&#25509;&#35302;&#21040;&#38750;&#25509;&#35302;&#65288;C2CL&#65289;&#30340;&#39564;&#35777;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contactless fingerprint matching using smartphone cameras can alleviate major challenges of traditional fingerprint systems including hygienic acquisition, portability and presentation attacks. However, development of practical and robust contactless fingerprint matching techniques is constrained by the limited availability of large scale real-world datasets. To motivate further advances in contactless fingerprint matching across sensors, we introduce the RidgeBase benchmark dataset. RidgeBase consists of more than 15,000 contactless and contact-based fingerprint image pairs acquired from 88 individuals under different background and lighting conditions using two smartphone cameras and one flatbed contact sensor. Unlike existing datasets, RidgeBase is designed to promote research under different matching scenarios that include Single Finger Matching and Multi-Finger Matching for both contactless- to-contactless (CL2CL) and contact-to-contactless (C2CL) verification and identification. 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#20013;&#30340;&#21453;&#39304;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#21644;&#35770;&#25991;&#29305;&#24449;&#65292;&#24182;&#22238;&#39038;&#20102;&#25552;&#20379;&#21453;&#39304;&#30340;&#26368;&#26032;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.05553</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#20013;&#30340;&#21453;&#39304;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of feedback in Automated Essay Scoring. (arXiv:2307.05553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#20013;&#30340;&#21453;&#39304;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#21644;&#35770;&#25991;&#29305;&#24449;&#65292;&#24182;&#22238;&#39038;&#20102;&#25552;&#20379;&#21453;&#39304;&#30340;&#26368;&#26032;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#35806;&#29983;&#20110;50&#24180;&#21069;&#12290;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#27491;&#22312;&#21457;&#23637;&#25104;&#20026;&#27604;&#20197;&#21069;&#31616;&#21333;&#35780;&#20998;&#31995;&#32479;&#26356;&#21152;&#21151;&#33021;&#20016;&#23500;&#30340;&#31995;&#32479;&#12290;&#23427;&#30340;&#30446;&#30340;&#19981;&#20165;&#20165;&#26159;&#35780;&#20998;&#65292;&#36824;&#20316;&#20026;&#19968;&#20010;&#23398;&#20064;&#24037;&#20855;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#20889;&#20316;&#33021;&#21147;&#12290;&#21453;&#39304;&#26159;&#20351;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#22312;&#23454;&#38469;&#29983;&#27963;&#20013;&#26377;&#29992;&#30340;&#26368;&#37325;&#35201;&#26041;&#38754;&#12290;&#22312;&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#20013;&#24050;&#32463;&#24378;&#35843;&#20102;&#21453;&#39304;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#20110;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#30340;&#21453;&#39304;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#21644;&#35770;&#25991;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#25552;&#20379;&#21453;&#39304;&#30340;&#26368;&#26032;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The first automated essay scoring system was developed 50 years ago. Automated essay scoring systems are developing into systems with richer functions than the previous simple scoring systems. Its purpose is not only to score essays but also as a learning tool to improve the writing skill of users. Feedback is the most important aspect of making an automated essay scoring system useful in real life. The importance of feedback was already emphasized in the first AES system. This paper reviews research on feedback including different feedback types and essay traits on automated essay scoring. We also reviewed the latest case studies of the automated essay scoring system that provides feedback.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#20840;&#23616;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05551</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization. (arXiv:2307.05551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#20840;&#23616;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#31859;&#25216;&#26415;&#21644;&#20808;&#36827;&#26448;&#26009;&#30340;&#31185;&#23398;&#36827;&#23637;&#20026;&#20307;&#20869;&#31934;&#20934;&#21307;&#23398;&#30340;&#32435;&#31859;&#23610;&#24230;&#35013;&#32622;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20854;&#20013;&#21253;&#25324;&#38598;&#25104;&#24863;&#24212;&#12289;&#35745;&#31639;&#12289;&#36890;&#20449;&#12289;&#25968;&#25454;&#21644;&#33021;&#37327;&#23384;&#20648;&#33021;&#21147;&#12290;&#22312;&#20154;&#20307;&#24515;&#34880;&#31649;&#31995;&#32479;&#20013;&#65292;&#36825;&#20123;&#35013;&#32622;&#34987;&#35774;&#24819;&#20026;&#34987;&#21160;&#27969;&#21160;&#24182;&#25345;&#32493;&#24863;&#30693;&#20197;&#20415;&#26816;&#27979;&#35786;&#26029;&#24863;&#20852;&#36259;&#30340;&#20107;&#20214;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20107;&#20214;&#30340;&#29289;&#29702;&#20301;&#32622;&#65288;&#22914;&#36523;&#20307;&#21306;&#22495;&#65289;&#20998;&#37197;&#32473;&#23427;&#20204;&#65292;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#21040;&#36825;&#20123;&#20107;&#20214;&#30340;&#35786;&#26029;&#20215;&#20540;&#65292;&#36825;&#26159;&#27969;&#23548;&#21521;&#23450;&#20301;&#30340;&#20027;&#35201;&#21629;&#39064;&#12290;&#24403;&#21069;&#30340;&#27969;&#23548;&#21521;&#23450;&#20301;&#26041;&#27861;&#23384;&#22312;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#22312;&#25972;&#20010;&#24515;&#34880;&#31649;&#31995;&#32479;&#20869;&#26412;&#22320;&#21270;&#20107;&#20214;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#36827;&#34892;&#23450;&#20301;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific advancements in nanotechnology and advanced materials are paving the way toward nanoscale devices for in-body precision medicine; comprising integrated sensing, computing, communication, data and energy storage capabilities. In the human cardiovascular system, such devices are envisioned to be passively flowing and continuously sensing for detecting events of diagnostic interest. The diagnostic value of detecting such events can be enhanced by assigning to them their physical locations (e.g., body region), which is the main proposition of flow-guided localization. Current flow-guided localization approaches suffer from low localization accuracy and they are by-design unable to localize events within the entire cardiovascular system. Toward addressing this issue, we propose the utilization of Graph Neural Networks (GNNs) for this purpose, and demonstrate localization accuracy and coverage enhancements of our proposal over the existing State of the Art (SotA) approaches. Based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;Word2Vec&#24212;&#29992;&#20110;&#26680;&#37238;&#30340;&#23884;&#20837;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23545;&#26680;&#37238;&#31867;&#21035;&#30340;&#20934;&#30830;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#26680;&#37238;&#30340;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.05537</link><description>&lt;p&gt;
NLP&#36935;&#19978;RNA&#65306;&#22522;&#20110;Word2Vec&#30340;&#33258;&#30417;&#30563;&#23884;&#20837;&#23398;&#20064;&#29992;&#20110;&#26680;&#37238;
&lt;/p&gt;
&lt;p&gt;
NLP Meets RNA: Unsupervised Embedding Learning for Ribozymes with Word2Vec. (arXiv:2307.05537v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;Word2Vec&#24212;&#29992;&#20110;&#26680;&#37238;&#30340;&#23884;&#20837;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#23545;&#26680;&#37238;&#31867;&#21035;&#30340;&#20934;&#30830;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#26680;&#37238;&#30340;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#37238;&#26159;&#20855;&#26377;&#29420;&#29305;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#20652;&#21270;&#27963;&#24615;&#30340;RNA&#20998;&#23376;&#65292;&#22312;&#21512;&#25104;&#29983;&#29289;&#23398;&#21644;&#27835;&#30103;&#23398;&#20013;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#36739;&#23569;&#30340;&#30740;&#31350;&#20851;&#27880;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22686;&#24378;&#25105;&#20204;&#23545;&#26680;&#37238;&#30340;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#23454;&#26045;&#20102;Word2Vec&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#23398;&#20064;&#26680;&#37238;&#30340;&#23884;&#20837;&#12290;Ribo2Vec&#22312;&#36229;&#36807;9000&#20010;&#22810;&#26679;&#30340;&#26680;&#37238;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23398;&#20064;&#23558;&#24207;&#21015;&#26144;&#23556;&#21040;128&#21644;256&#32500;&#30340;&#21521;&#37327;&#31354;&#38388;&#12290;&#21033;&#29992;Ribo2Vec&#65292;&#35745;&#31639;&#20102;&#20116;&#31867;&#26680;&#37238;&#65288;hatchet&#65292;pistol&#65292;hairpin&#65292;hovlinc&#21644;twister sister&#65289;&#30340;&#24207;&#21015;&#23884;&#20837;&#12290;&#20027;&#25104;&#20998;&#20998;&#26512;&#34920;&#26126;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#21306;&#20998;&#26680;&#37238;&#30340;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#29992;&#26680;&#37238;&#23884;&#20837;&#35757;&#32451;&#30340;&#31616;&#21333;SVM&#20998;&#31867;&#22120;&#22312;&#20934;&#30830;&#20998;&#31867;&#26680;&#37238;&#31867;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#23884;&#20837;&#21521;&#37327;&#21253;&#21547;&#26377;&#20851;&#26680;&#37238;&#30340;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ribozymes, RNA molecules with distinct 3D structures and catalytic activity, have widespread applications in synthetic biology and therapeutics. However, relatively little research has focused on leveraging deep learning to enhance our understanding of ribozymes. This study implements Word2Vec, an unsupervised learning technique for natural language processing, to learn ribozyme embeddings. Ribo2Vec was trained on over 9,000 diverse ribozymes, learning to map sequences to 128 and 256-dimensional vector spaces. Using Ribo2Vec, sequence embeddings for five classes of ribozymes (hatchet, pistol, hairpin, hovlinc, and twister sister) were calculated. Principal component analysis demonstrated the ability of these embeddings to distinguish between ribozyme classes. Furthermore, a simple SVM classifier trained on ribozyme embeddings showed promising results in accurately classifying ribozyme types. Our results suggest that the embedding vectors contained meaningful information about ribozymes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25353;&#38190;&#21160;&#21147;&#23398;&#22312;&#33258;&#30001;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#22810;&#31867;&#29992;&#25143;&#36523;&#20221;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;&#22270;&#20687;&#29305;&#24449;&#21644;&#22810;&#31867;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#31245;&#24494;&#20462;&#25913;&#29305;&#24449;&#30340;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.05529</link><description>&lt;p&gt;
&#29992;&#25143;&#36523;&#20221;&#35782;&#21035;&#30340;&#25353;&#38190;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Keystroke Dynamics for User Identification. (arXiv:2307.05529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25353;&#38190;&#21160;&#21147;&#23398;&#22312;&#33258;&#30001;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#22810;&#31867;&#29992;&#25143;&#36523;&#20221;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;&#22270;&#20687;&#29305;&#24449;&#21644;&#22810;&#31867;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#31245;&#24494;&#20462;&#25913;&#29305;&#24449;&#30340;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#25353;&#38190;&#21160;&#21147;&#23398;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#22522;&#20110;&#22266;&#23450;&#25991;&#26412;&#21644;&#33258;&#30001;&#25991;&#26412;&#25968;&#25454;&#30340;&#29992;&#25143;&#35748;&#35777;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20110;&#33258;&#30001;&#25991;&#26412;&#25968;&#25454;&#30340;&#22810;&#31867;&#29992;&#25143;&#36523;&#20221;&#35782;&#21035;&#38382;&#39064;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#31867;&#20284;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#35813;&#29305;&#24449;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#34987;&#29992;&#20110;&#23454;&#29616;&#33258;&#30001;&#25991;&#26412;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#30340;&#35748;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#31867;&#20284;&#22270;&#20687;&#30340;&#29305;&#24449;&#21644;&#22810;&#31867;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;148&#20010;&#29992;&#25143;&#30340;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;0.78&#30340;&#20998;&#31867;&#65288;&#21363;&#35782;&#21035;&#65289;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#32463;&#36807;&#31245;&#24494;&#20462;&#25913;&#30340;&#30456;&#21516;&#29305;&#24449;&#30340;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#20854;&#20934;&#30830;&#29575;&#21487;&#20197;&#36798;&#21040;0.93&#12290;
&lt;/p&gt;
&lt;p&gt;
In previous research, keystroke dynamics has shown promise for user authentication, based on both fixed-text and free-text data. In this research, we consider the more challenging multiclass user identification problem, based on free-text data. We experiment with a complex image-like feature that has previously been used to achieve state-of-the-art authentication results over free-text data. Using this image-like feature and multiclass Convolutional Neural Networks, we are able to obtain a classification (i.e., identification) accuracy of 0.78 over a set of 148 users. However, we find that a Random Forest classifier trained on a slightly modified version of this same feature yields an accuracy of 0.93.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#29983;&#25104;&#38899;&#39057;&#27169;&#22411;&#30340;&#20262;&#29702;&#24433;&#21709;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#21457;&#29616;&#24403;&#21069;&#26377;&#36739;&#23569;&#35770;&#25991;&#35752;&#35770;&#36127;&#38754;&#24433;&#21709;&#65292;&#20294;&#20854;&#20013;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#22914;&#27450;&#35784;&#12289;&#20551;&#33080;&#21644;&#20405;&#26435;&#28508;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05527</link><description>&lt;p&gt;
&#29983;&#25104;&#38899;&#39057;&#27169;&#22411;&#30340;&#20262;&#29702;&#24433;&#21709;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The Ethical Implications of Generative Audio Models: A Systematic Literature Review. (arXiv:2307.05527v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#29983;&#25104;&#38899;&#39057;&#27169;&#22411;&#30340;&#20262;&#29702;&#24433;&#21709;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#21457;&#29616;&#24403;&#21069;&#26377;&#36739;&#23569;&#35770;&#25991;&#35752;&#35770;&#36127;&#38754;&#24433;&#21709;&#65292;&#20294;&#20854;&#20013;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#22914;&#27450;&#35784;&#12289;&#20551;&#33080;&#21644;&#20405;&#26435;&#28508;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#38899;&#39057;&#27169;&#22411;&#36890;&#24120;&#23558;&#20854;&#24212;&#29992;&#38598;&#20013;&#22312;&#38899;&#20048;&#21644;&#35821;&#38899;&#29983;&#25104;&#19978;&#65292;&#36817;&#26399;&#30340;&#27169;&#22411;&#22312;&#38899;&#39057;&#36755;&#20986;&#26041;&#38754;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#23545;884&#31687;&#29983;&#25104;&#38899;&#39057;&#27169;&#22411;&#39046;&#22495;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#26088;&#22312;&#37327;&#21270;&#39046;&#22495;&#20869;&#30740;&#31350;&#32773;&#32771;&#34385;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#30830;&#23450;&#30740;&#31350;&#32773;&#22312;&#36825;&#20010;&#39046;&#22495;&#38656;&#35201;&#32771;&#34385;&#30340;&#20262;&#29702;&#24433;&#21709;&#31867;&#22411;&#12290;&#34429;&#28982;65%&#30340;&#29983;&#25104;&#38899;&#39057;&#30740;&#31350;&#35770;&#25991;&#25552;&#21040;&#20102;&#20182;&#20204;&#24037;&#20316;&#30340;&#31215;&#26497;&#28508;&#21147;&#65292;&#20294;&#19981;&#21040;10%&#30340;&#35770;&#25991;&#35752;&#35770;&#20102;&#20219;&#20309;&#36127;&#38754;&#24433;&#21709;&#12290;&#36825;&#20010;&#24778;&#20154;&#30340;&#23567;&#27604;&#20363;&#20351;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#23569;&#25968;&#25552;&#21040;&#36127;&#38754;&#24433;&#21709;&#30340;&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26356;&#22823;&#33539;&#22260;&#39046;&#22495;&#20869;&#30456;&#20851;&#30340;&#20005;&#37325;&#20262;&#29702;&#38382;&#39064;&#21644;&#20851;&#20999;&#65292;&#22914;&#27450;&#35784;&#12289;&#20551;&#33080;&#21644;&#20405;&#26435;&#28508;&#33021;&#12290;&#36890;&#36807;&#37327;&#21270;&#29983;&#25104;&#38899;&#39057;&#30740;&#31350;&#20013;&#32570;&#20047;&#20262;&#29702;&#32771;&#34385;&#30340;&#29616;&#29366;&#65292;&#36825;&#31687;&#35770;&#25991;&#21628;&#21505;&#30740;&#31350;&#32773;&#22312;&#26684;&#22806;&#20851;&#27880;&#36825;&#20123;&#20262;&#29702;&#38382;&#39064;&#19978;&#20570;&#20986;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative audio models typically focus their applications in music and speech generation, with recent models having human-like quality in their audio output. This paper conducts a systematic literature review of 884 papers in the area of generative audio models in order to both quantify the degree to which researchers in the field are considering potential negative impacts and identify the types of ethical implications researchers in this area need to consider. Though 65% of generative audio research papers note positive potential impacts of their work, less than 10% discuss any negative impacts. This jarringly small percentage of papers considering negative impact is particularly worrying because the issues brought to light by the few papers doing so are raising serious ethical implications and concerns relevant to the broader field such as the potential for fraud, deep-fakes, and copyright infringement. By quantifying this lack of ethical consideration in generative audio research a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20248;&#21270;&#30005;&#26497;&#21046;&#36896;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#39640;&#24615;&#33021;&#30005;&#27744;&#30005;&#26497;&#38382;&#39064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#33021;&#37327;&#21644;&#21151;&#29575;&#24212;&#29992;&#20013;&#21453;&#21521;&#35774;&#35745;&#30005;&#21270;&#23398;&#24615;&#33021;&#30340;&#21046;&#36896;&#36807;&#31243;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.05521</link><description>&lt;p&gt;
&#26397;&#30528;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20248;&#21270;&#30005;&#26497;&#21046;&#36896;&#30340;&#39640;&#24615;&#33021;&#33021;&#37327;&#21644;&#21151;&#29575;&#30005;&#27744;&#30340;&#26041;&#21521;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Toward High-Performance Energy and Power Battery Cells with Machine Learning-based Optimization of Electrode Manufacturing. (arXiv:2307.05521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20248;&#21270;&#30005;&#26497;&#21046;&#36896;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#39640;&#24615;&#33021;&#30005;&#27744;&#30005;&#26497;&#38382;&#39064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#33021;&#37327;&#21644;&#21151;&#29575;&#24212;&#29992;&#20013;&#21453;&#21521;&#35774;&#35745;&#30005;&#21270;&#23398;&#24615;&#33021;&#30340;&#21046;&#36896;&#36807;&#31243;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#26497;&#21046;&#36896;&#36807;&#31243;&#30340;&#20248;&#21270;&#23545;&#20110;&#25552;&#21319;&#38146;&#31163;&#23376;&#30005;&#27744;&#65288;LIBs&#65289;&#30340;&#24212;&#29992;&#35268;&#27169;&#20197;&#28385;&#36275;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#28304;&#38656;&#27714;&#38750;&#24120;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;LIB&#21046;&#36896;&#30340;&#20248;&#21270;&#23545;&#20110;&#30830;&#23450;&#30005;&#27744;&#22312;&#35832;&#22914;&#30005;&#21160;&#36710;&#31561;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;&#24615;&#33021;&#38750;&#24120;&#20851;&#38190;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#19968;&#31181;&#30830;&#23450;&#24615;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#30340;&#31649;&#36947;&#26469;&#20248;&#21270;&#30005;&#21270;&#23398;&#24615;&#33021;&#30340;&#21452;&#30446;&#26631;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#26399;&#26395;&#30005;&#27744;&#24212;&#29992;&#26465;&#20214;&#19979;&#30340;&#39640;&#24615;&#33021;&#30005;&#26497;&#38382;&#39064;&#12290;&#36825;&#31181;ML&#31649;&#36947;&#20801;&#35768;&#21453;&#21521;&#35774;&#35745;&#36807;&#31243;&#21442;&#25968;&#20197;&#21046;&#36896;&#36866;&#29992;&#20110;&#33021;&#37327;&#25110;&#21151;&#29575;&#24212;&#29992;&#30340;&#30005;&#26497;&#12290;&#35813;&#30740;&#31350;&#31867;&#20284;&#20110;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#25903;&#25345;&#20248;&#21270;&#30005;&#26497;&#24494;&#32467;&#26500;&#20197;&#25913;&#21892;&#21160;&#21147;&#23398;&#12289;&#31163;&#23376;&#21644;&#30005;&#23376;&#20256;&#36755;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimization of the electrode manufacturing process is important for upscaling the application of Lithium Ion Batteries (LIBs) to cater for growing energy demand. In particular, LIB manufacturing is very important to be optimized because it determines the practical performance of the cells when the latter are being used in applications such as electric vehicles. In this study, we tackled the issue of high-performance electrodes for desired battery application conditions by proposing a powerful data-driven approach supported by a deterministic machine learning (ML)-assisted pipeline for bi-objective optimization of the electrochemical performance. This ML pipeline allows the inverse design of the process parameters to adopt in order to manufacture electrodes for energy or power applications. The latter work is an analogy to our previous work that supported the optimization of the electrode microstructures for kinetic, ionic, and electronic transport properties improvement. An electr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#20986;&#20102;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05520</link><description>&lt;p&gt;
DL&#27169;&#22411;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#33021;&#28304;&#28040;&#32791;&#26377;&#24433;&#21709;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do DL models and training environments have an impact on energy consumption?. (arXiv:2307.05520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#20986;&#20102;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#27491;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#24615;&#33021;&#19978;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#20851;&#20110;&#35757;&#32451;DL&#27169;&#22411;&#24102;&#26469;&#24040;&#22823;&#30899;&#36275;&#36857;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#29615;&#22659;&#23545;&#35757;&#32451;&#26356;&#29615;&#20445;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#27169;&#22411;&#26550;&#26500;&#23545;&#23454;&#29616;&#26356;&#29615;&#20445;&#27169;&#22411;&#21516;&#26102;&#20445;&#25345;&#27491;&#30830;&#24615;&#22312;&#26368;&#20339;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#35757;&#32451;&#29615;&#22659;&#23545;&#29983;&#25104;&#26356;&#29615;&#20445;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20123;&#20851;&#31995;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25910;&#38598;&#20102;&#19982;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#30456;&#20851;&#30340;&#22810;&#20010;&#25351;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#27169;&#22411;&#26550;&#26500;&#22312;&#27979;&#37327;&#33021;&#28304;&#25928;&#29575;&#21644;&#27169;&#22411;&#27491;&#30830;&#24615;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#35757;&#32451;&#29615;&#22659;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#39564;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#36825;&#39033;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research in the computer vision field mainly focuses on improving Deep Learning (DL) correctness and inference time performance. However, there is still little work on the huge carbon footprint that has training DL models. This study aims to analyze the impact of the model architecture and training environment when training greener computer vision models. We divide this goal into two research questions. First, we analyze the effects of model architecture on achieving greener models while keeping correctness at optimal levels. Second, we study the influence of the training environment on producing greener models. To investigate these relationships, we collect multiple metrics related to energy efficiency and model correctness during the models' training. Then, we outline the trade-offs between the measured energy efficiency and the models' correctness regarding model architecture, and their relationship with the training environment. We conduct this research in the context of a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;AGC-net&#65289;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20013;GNN&#30340;&#26102;&#38388;&#21464;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#65288;AGC&#65289;&#21644;&#19978;&#19979;&#25991;&#27880;&#24847;&#26426;&#21046;&#26469;&#32771;&#34385;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05517</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Graph Convolution Networks for Traffic Flow Forecasting. (arXiv:2307.05517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;AGC-net&#65289;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20013;GNN&#30340;&#26102;&#38388;&#21464;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#65288;AGC&#65289;&#21644;&#19978;&#19979;&#25991;&#27880;&#24847;&#26426;&#21046;&#26469;&#32771;&#34385;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#21160;&#24577;&#30340;&#26102;&#31354;&#36947;&#36335;&#26465;&#20214;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;GNN&#24573;&#30053;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#36947;&#36335;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#21367;&#31215;&#24863;&#21463;&#37326;&#30340;&#33539;&#22260;&#26159;&#22266;&#23450;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;AGC-net&#65289;&#26469;&#35299;&#20915;GNN&#20013;&#30340;&#36825;&#20010;&#38382;&#39064;&#12290;AGC-net&#26159;&#36890;&#36807;&#22522;&#20110;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#27880;&#24847;&#26426;&#21046;&#30340;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#65288;AGC&#65289;&#26500;&#24314;&#30340;&#65292;&#35813;&#26426;&#21046;&#30001;&#19968;&#32452;&#20855;&#26377;&#21508;&#31181;&#21487;&#23398;&#20064;&#23610;&#24230;&#30340;&#22270;&#23567;&#27874;&#32452;&#25104;&#12290;AGC&#23558;&#31354;&#38388;&#22270;&#34920;&#31034;&#36716;&#21270;&#20026;&#32771;&#34385;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#26102;&#38388;&#25935;&#24863;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#20559;&#31227;&#22270;&#21367;&#31215;&#26680;&#20197;&#22686;&#24378;AGC&#65292;&#23427;&#35797;&#22270;&#32416;&#27491;&#30001;&#19981;&#20934;&#30830;&#30340;&#25299;&#25169;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;&#20004;&#20010;&#20844;&#20849;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;AGC-net&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic flow forecasting is a highly challenging task due to the dynamic spatial-temporal road conditions. Graph neural networks (GNN) has been widely applied in this task. However, most of these GNNs ignore the effects of time-varying road conditions due to the fixed range of the convolution receptive field. In this paper, we propose a novel Adaptive Graph Convolution Networks (AGC-net) to address this issue in GNN. The AGC-net is constructed by the Adaptive Graph Convolution (AGC) based on a novel context attention mechanism, which consists of a set of graph wavelets with various learnable scales. The AGC transforms the spatial graph representations into time-sensitive features considering the temporal context. Moreover, a shifted graph convolution kernel is designed to enhance the AGC, which attempts to correct the deviations caused by inaccurate topology. Experimental results on two public traffic datasets demonstrate the effectiveness of the AGC-net\footnote{Code is available at: 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36229;&#26448;&#26009;&#35774;&#35745;&#20026;&#23454;&#29616;&#19979;&#19968;&#20195;&#20855;&#26377;&#29305;&#27530;&#21151;&#33021;&#30340;&#35774;&#22791;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#21644;&#22797;&#26434;&#30340;&#32467;&#26500;-&#24615;&#33021;&#20851;&#31995;&#26159;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#24403;&#21069;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24378;&#35843;&#26041;&#27861;&#35770;&#32780;&#38750;&#29305;&#23450;&#39046;&#22495;&#12290;&#25968;&#25454;&#39537;&#21160;&#27169;&#22359;&#21253;&#25324;&#25968;&#25454;&#33719;&#21462;&#12289;&#26426;&#22120;&#23398;&#20064;&#21333;&#20803;&#35774;&#35745;&#21644;&#25968;&#25454;&#39537;&#21160;&#22810;&#23610;&#24230;&#20248;&#21270;&#65292;&#30740;&#31350;&#26041;&#27861;&#34987;&#22522;&#20110;&#20849;&#20139;&#21407;&#21017;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.05506</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36229;&#26448;&#26009;&#21644;&#22810;&#23610;&#24230;&#31995;&#32479;&#35774;&#35745;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Design for Metamaterials and Multiscale Systems: A Review. (arXiv:2307.05506v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05506
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36229;&#26448;&#26009;&#35774;&#35745;&#20026;&#23454;&#29616;&#19979;&#19968;&#20195;&#20855;&#26377;&#29305;&#27530;&#21151;&#33021;&#30340;&#35774;&#22791;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#21644;&#22797;&#26434;&#30340;&#32467;&#26500;-&#24615;&#33021;&#20851;&#31995;&#26159;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#24403;&#21069;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24378;&#35843;&#26041;&#27861;&#35770;&#32780;&#38750;&#29305;&#23450;&#39046;&#22495;&#12290;&#25968;&#25454;&#39537;&#21160;&#27169;&#22359;&#21253;&#25324;&#25968;&#25454;&#33719;&#21462;&#12289;&#26426;&#22120;&#23398;&#20064;&#21333;&#20803;&#35774;&#35745;&#21644;&#25968;&#25454;&#39537;&#21160;&#22810;&#23610;&#24230;&#20248;&#21270;&#65292;&#30740;&#31350;&#26041;&#27861;&#34987;&#22522;&#20110;&#20849;&#20139;&#21407;&#21017;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#26448;&#26009;&#26159;&#19968;&#31181;&#20154;&#24037;&#35774;&#35745;&#30340;&#26448;&#26009;&#65292;&#20854;&#26377;&#25928;&#26448;&#26009;&#21442;&#25968;&#36229;&#36234;&#33258;&#28982;&#26448;&#26009;&#12290;&#30001;&#20016;&#23500;&#35774;&#35745;&#24615;&#30340;&#21333;&#20803;&#21333;&#20803;&#26500;&#25104;&#30340;&#22810;&#23610;&#24230;&#31995;&#32479;&#65292;&#23427;&#20204;&#26377;&#30528;&#23454;&#29616;&#19979;&#19968;&#20195;&#20855;&#26377;&#29305;&#27530;&#21151;&#33021;&#24615;&#30340;&#35774;&#22791;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#21644;&#22797;&#26434;&#30340;&#32467;&#26500;-&#24615;&#33021;&#20851;&#31995;&#22312;&#20854;&#35774;&#35745;&#20013;&#24102;&#26469;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#25968;&#25454;&#39537;&#21160;&#35774;&#35745;&#26159;&#19968;&#31181;&#26377;&#28508;&#21147;&#23454;&#29616;&#36229;&#26448;&#26009;&#30340;&#20840;&#37096;&#28508;&#33021;&#30340;&#26032;&#20852;&#33539;&#20363;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20010;&#36805;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#25972;&#20307;&#27010;&#36848;&#65292;&#37325;&#28857;&#26159;&#24635;&#20307;&#26041;&#27861;&#35770;&#32780;&#19981;&#26159;&#29305;&#23450;&#39046;&#22495;&#21644;&#37096;&#32626;&#29615;&#22659;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30740;&#31350;&#21306;&#20998;&#20026;&#25968;&#25454;&#39537;&#21160;&#27169;&#22359;&#65292;&#21253;&#25324;&#25968;&#25454;&#33719;&#21462;&#12289;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21333;&#20803;&#21333;&#20803;&#35774;&#35745;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#23610;&#24230;&#20248;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22522;&#20110;&#20849;&#20139;&#21407;&#21017;&#23545;&#27599;&#20010;&#27169;&#22359;&#20013;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metamaterials are artificial materials designed to exhibit effective material parameters that go beyond those found in nature. Composed of unit cells with rich designability that are assembled into multiscale systems, they hold great promise for realizing next-generation devices with exceptional, often exotic, functionalities. However, the vast design space and intricate structure-property relationships pose significant challenges in their design. A compelling paradigm that could bring the full potential of metamaterials to fruition is emerging: data-driven design. In this review, we provide a holistic overview of this rapidly evolving field, emphasizing the general methodology instead of specific domains and deployment contexts. We organize existing research into data-driven modules, encompassing data acquisition, machine learning-based unit cell design, and data-driven multiscale optimization. We further categorize the approaches within each module based on shared principles, analyze
&lt;/p&gt;</description></item><item><title>HIVA &#26159;&#19968;&#20010;&#20840;&#24687;&#26234;&#33021;&#35821;&#38899;&#21161;&#25163;&#65292;&#36890;&#36807;&#35270;&#21548;&#25928;&#26524;&#21644;3D&#21160;&#30011;&#20419;&#36827;&#20154;&#26426;&#20132;&#20114;&#12290;&#23427;&#25552;&#20379;&#22823;&#23398;&#30340;&#21508;&#31181;&#20449;&#24687;&#65292;&#25903;&#25345;"&#38754;&#23545;&#38754;"&#20132;&#27969;&#65292;&#24182;&#25552;&#20379;&#22810;&#20010;&#23376;&#27169;&#22359;&#21644;&#36830;&#25509;&#20854;&#20182;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05501</link><description>&lt;p&gt;
HIVA: &#20840;&#24687;&#26234;&#33021;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
HIVA: Holographic Intellectual Voice Assistant. (arXiv:2307.05501v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05501
&lt;/p&gt;
&lt;p&gt;
HIVA &#26159;&#19968;&#20010;&#20840;&#24687;&#26234;&#33021;&#35821;&#38899;&#21161;&#25163;&#65292;&#36890;&#36807;&#35270;&#21548;&#25928;&#26524;&#21644;3D&#21160;&#30011;&#20419;&#36827;&#20154;&#26426;&#20132;&#20114;&#12290;&#23427;&#25552;&#20379;&#22823;&#23398;&#30340;&#21508;&#31181;&#20449;&#24687;&#65292;&#25903;&#25345;"&#38754;&#23545;&#38754;"&#20132;&#27969;&#65292;&#24182;&#25552;&#20379;&#22810;&#20010;&#23376;&#27169;&#22359;&#21644;&#36830;&#25509;&#20854;&#20182;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#24687;&#26234;&#33021;&#35821;&#38899;&#21161;&#25163;&#65288;HIVA&#65289;&#26088;&#22312;&#21033;&#29992;&#35270;&#21548;&#25928;&#26524;&#21644;3D&#21160;&#30011;&#26469;&#20419;&#36827;&#20154;&#26426;&#20132;&#20114;&#12290;HIVA&#25552;&#20379;&#26377;&#20851;&#22823;&#23398;&#30340;&#23436;&#25972;&#20449;&#24687;&#65292;&#21253;&#25324;&#21508;&#31181;&#33258;&#28982;&#35831;&#27714;&#65306;&#20837;&#23398;&#65292;&#23398;&#20064;&#38382;&#39064;&#65292;&#23398;&#36153;&#65292;&#38498;&#31995;&#65292;&#22823;&#23398;&#32467;&#26500;&#21644;&#21382;&#21490;&#65292;&#39135;&#22530;&#65292;&#20154;&#21147;&#36164;&#28304;&#65292;&#22270;&#20070;&#39302;&#65292;&#23398;&#29983;&#29983;&#27963;&#21644;&#27963;&#21160;&#65292;&#20197;&#21450;&#26377;&#20851;&#35813;&#22269;&#23478;&#21644;&#22478;&#24066;&#30340;&#20449;&#24687;&#31561;&#12290;&#33719;&#21462;&#19978;&#36848;&#25968;&#25454;&#30340;&#20854;&#20182;&#26041;&#24335;&#21253;&#25324;&#22823;&#23398;&#23448;&#26041;&#32593;&#31449;&#21644;&#20854;&#20182;&#25903;&#25345;&#24212;&#29992;&#31243;&#24207;&#65292;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#65288;HEI&#65289;&#23448;&#26041;&#31038;&#20132;&#23186;&#20307;&#65292;&#30452;&#25509;&#21521;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#24037;&#20316;&#20154;&#21592;&#25552;&#38382;&#31561;&#12290;&#28982;&#32780;&#65292;HIVA&#25552;&#20379;&#20102;&#19982;&#21160;&#30011;3D&#21513;&#31077;&#29289;&#30340;&#8220;&#38754;&#23545;&#38754;&#8221;&#20132;&#20114;&#30340;&#29420;&#29305;&#20307;&#39564;&#65292;&#24110;&#21161;&#20154;&#20204;&#33719;&#24471;&#8220;&#29616;&#23454;&#29983;&#27963;&#8221;&#20132;&#27969;&#30340;&#24863;&#35273;&#12290;&#35813;&#31995;&#32479;&#21253;&#25324;&#35768;&#22810;&#23376;&#27169;&#22359;&#65292;&#24182;&#36830;&#25509;&#20102;&#19968;&#31995;&#21015;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#12289;Telegram&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#24314;&#35758;&#20998;&#31867;&#21644;&#23089;&#20048;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Holographic Intellectual Voice Assistant (HIVA) aims to facilitate human computer interaction using audiovisual effects and 3D avatar. HIVA provides complete information about the university, including requests of various nature: admission, study issues, fees, departments, university structure and history, canteen, human resources, library, student life and events, information about the country and the city, etc. There are other ways for receiving the data listed above: the university's official website and other supporting apps, HEI (Higher Education Institution) official social media, directly asking the HEI staff, and other channels. However, HIVA provides the unique experience of "face-to-face" interaction with an animated 3D mascot, helping to get a sense of 'real-life' communication. The system includes many sub-modules and connects a family of applications such as mobile applications, Telegram chatbot, suggestion categorization, and entertainment services. The Voice assistant us
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#31561;&#21464;GNN&#26469;&#30740;&#31350;&#22312;&#27969;&#20307;&#27969;&#21160;&#24314;&#27169;&#20013;&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#23545;&#31216;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#27169;&#25311;&#19981;&#21464;&#25968;&#37327;&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#65292;&#24182;&#21487;&#20174;&#36895;&#24230;&#22330;&#20013;&#23398;&#20064;&#21040;&#12290;</title><link>http://arxiv.org/abs/2307.05486</link><description>&lt;p&gt;
&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#23545;&#31216;&#24615;&#23545;&#27969;&#20307;&#27969;&#21160;&#24314;&#27169;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Importance of equivariant and invariant symmetries for fluid flow modeling. (arXiv:2307.05486v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#31561;&#21464;GNN&#26469;&#30740;&#31350;&#22312;&#27969;&#20307;&#27969;&#21160;&#24314;&#27169;&#20013;&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#23545;&#31216;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#27169;&#25311;&#19981;&#21464;&#25968;&#37327;&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#65292;&#24182;&#21487;&#20174;&#36895;&#24230;&#22330;&#20013;&#23398;&#20064;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#23398;&#20064;&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#32593;&#26684;&#30340;&#29289;&#29702;&#31995;&#32479;&#27169;&#25311;&#20013;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#21253;&#25324;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#21516;&#26102;&#65292;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21407;&#29702;&#25351;&#23548;&#20102;&#24314;&#31435;&#23562;&#37325;&#24213;&#23618;&#29289;&#29702;&#23545;&#31216;&#24615;&#30340;&#31561;&#21464;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#25311;&#27969;&#20307;&#20013;&#65292;&#26059;&#36716;&#31561;&#21464;&#24615;&#30340;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#31561;&#21464;GNN&#26469;&#39044;&#27979;&#27969;&#20307;&#27969;&#21160;&#65292;&#24182;&#30740;&#31350;&#20102;&#24314;&#27169;&#27969;&#21160;&#29366;&#24577;&#30340;&#19981;&#21464;&#21644;&#38750;&#19981;&#21464;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#31561;&#21464;&#21644;&#38750;&#31561;&#21464;&#32467;&#26500;&#22312;&#39044;&#27979;&#20004;&#31181;&#27969;&#20307;&#27969;&#21160;&#65288;&#22278;&#26609;&#21608;&#22260;&#30340;&#27969;&#21160;&#21644;&#28014;&#21147;&#39537;&#21160;&#30340;&#21098;&#20999;&#27969;&#21160;&#65289;&#28436;&#21270;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#23545;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#25311;&#19981;&#21464;&#25968;&#37327;&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#65292;&#24182;&#19988;&#36825;&#20123;&#19981;&#21464;&#25968;&#37327;&#21487;&#20197;&#20174;&#36895;&#24230;&#22330;&#20013;&#23398;&#20064;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown promise in learning unstructured mesh-based simulations of physical systems, including fluid dynamics. In tandem, geometric deep learning principles have informed the development of equivariant architectures respecting underlying physical symmetries. However, the effect of rotational equivariance in modeling fluids remains unclear. We build a multi-scale equivariant GNN to forecast fluid flow and study the effect of modeling invariant and non-invariant representations of the flow state. We evaluate the model performance of several equivariant and non-equivariant architectures on predicting the evolution of two fluid flows, flow around a cylinder and buoyancy-driven shear flow, to understand the effect of equivariance and invariance on data-driven modeling approaches. Our results show that modeling invariant quantities produces more accurate long-term predictions and that these invariant quantities may be learned from the velocity field using a da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27169;&#31946;&#20102;&#21518;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#19982;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#65292;&#36890;&#36807;&#20174;&#28789;&#27963;&#30340;&#40657;&#30418;&#27169;&#22411;&#24320;&#22987;&#65292;&#36880;&#28176;&#24341;&#20837;&#21487;&#35299;&#37322;&#27169;&#22411;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#30340;&#36335;&#30001;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.05350</link><description>&lt;p&gt;
&#36335;&#30001;&#12289;&#35299;&#37322;&#12289;&#37325;&#22797;&#65306;&#27169;&#31946;&#21518;&#35299;&#37322;&#24615;&#19982;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Route, Interpret, Repeat: Blurring the line between post hoc explainability and interpretable models. (arXiv:2307.05350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27169;&#31946;&#20102;&#21518;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#19982;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#65292;&#36890;&#36807;&#20174;&#28789;&#27963;&#30340;&#40657;&#30418;&#27169;&#22411;&#24320;&#22987;&#65292;&#36880;&#28176;&#24341;&#20837;&#21487;&#35299;&#37322;&#27169;&#22411;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#30340;&#36335;&#30001;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#26041;&#27861;&#35201;&#20040;&#36873;&#25321;&#19968;&#20010;&#28789;&#27963;&#30340;&#40657;&#30418;&#27169;&#22411;&#24182;&#22312;&#21518;&#26399;&#35299;&#37322;&#23427;&#65292;&#35201;&#20040;&#20174;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;&#40657;&#30418;&#27169;&#22411;&#28789;&#27963;&#20294;&#38590;&#20197;&#35299;&#37322;&#65292;&#32780;&#21487;&#35299;&#37322;&#27169;&#22411;&#35774;&#35745;&#20026;&#21487;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#21487;&#35299;&#37322;&#27169;&#22411;&#38656;&#35201;&#28145;&#21402;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#65292;&#32780;&#24471;&#21040;&#30340;&#27169;&#22411;&#24448;&#24448;&#19981;&#22815;&#28789;&#27963;&#65292;&#21487;&#33021;&#24615;&#33021;&#19981;&#21450;&#20854;&#40657;&#30418;&#27169;&#22411;&#30340;&#31561;&#20215;&#29289;&#12290;&#26412;&#25991;&#26088;&#22312;&#27169;&#31946;&#21518;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#19982;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#25552;&#35758;&#20174;&#19968;&#20010;&#28789;&#27963;&#30340;&#40657;&#30418;&#27169;&#22411;&#24320;&#22987;&#65292;&#24182;&#36880;&#28176;&#12300;&#38613;&#21051;&#12301;&#20986;&#19968;&#31181;&#28151;&#21512;&#20102;&#21487;&#35299;&#37322;&#27169;&#22411;&#21644;&#19968;&#20010;&#12300;&#27531;&#24046;&#32593;&#32476;&#12301;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#36890;&#36807;&#21487;&#35299;&#37322;&#27169;&#22411;&#12300;&#36335;&#30001;&#12301;&#19968;&#37096;&#20998;&#26679;&#26412;&#65292;&#21097;&#20313;&#30340;&#26679;&#26412;&#21017;&#36890;&#36807;&#28789;&#27963;&#30340;&#27531;&#24046;&#32593;&#32476;&#36827;&#34892;&#36335;&#30001;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#20316;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current approach to ML model design is either to choose a flexible Blackbox model and explain it post hoc or to start with an interpretable model. Blackbox models are flexible but difficult to explain, whereas interpretable models are designed to be explainable. However, developing interpretable models necessitates extensive ML knowledge, and the resulting models tend to be less flexible, offering potentially subpar performance compared to their Blackbox equivalents. This paper aims to blur the distinction between a post hoc explanation of a BlackBox and constructing interpretable models. We propose beginning with a flexible BlackBox model and gradually \emph{carving out} a mixture of interpretable models and a \emph{residual network}. Our design identifies a subset of samples and \emph{routes} them through the interpretable models. The remaining samples are routed through a flexible residual network. We adopt First Order Logic (FOL) as the interpretable model's backbone, which pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#21040;&#35270;&#39057;&#27969;&#30340;&#35774;&#32622;&#20013;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;TTT&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#21644;&#31163;&#32447;TTT&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#20840;&#26223;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2307.05014</link><description>&lt;p&gt;
&#35270;&#39057;&#27969;&#19978;&#30340;&#27979;&#35797;&#26102;&#22521;&#35757;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training on Video Streams. (arXiv:2307.05014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#21040;&#35270;&#39057;&#27969;&#30340;&#35774;&#32622;&#20013;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;TTT&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#21644;&#31163;&#32447;TTT&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#20840;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#30830;&#23450;&#20026;&#19968;&#31181;&#22312;&#27979;&#35797;&#26102;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#27169;&#22411;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#22312;&#23545;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#36827;&#34892;&#39044;&#27979;&#20043;&#21069;&#65292;&#27169;&#22411;&#20250;&#20351;&#29992;&#33258;&#30417;&#30563;&#20219;&#21153;&#65288;&#20363;&#22914;&#20351;&#29992;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#22270;&#20687;&#37325;&#24314;&#65289;&#22312;&#21516;&#19968;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;TTT&#25193;&#23637;&#21040;&#27969;&#24335;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#22810;&#20010;&#27979;&#35797;&#23454;&#20363;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#20026;&#35270;&#39057;&#24103;&#65289;&#25353;&#26102;&#38388;&#39034;&#24207;&#21040;&#36798;&#12290;&#25105;&#20204;&#30340;&#25193;&#23637;&#26159;&#22312;&#32447;TTT&#65306;&#24403;&#21069;&#27169;&#22411;&#20174;&#19978;&#20010;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#28982;&#21518;&#22312;&#24403;&#21069;&#24103;&#21644;&#21069;&#20960;&#20010;&#24103;&#30340;&#23567;&#31383;&#21475;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#32447;TTT&#22312;&#22235;&#20010;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#65292;&#22312;&#19977;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#30456;&#23545;&#25913;&#36827;&#20998;&#21035;&#20026;45%&#21644;66%&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#32447;TTT&#20063;&#20248;&#20110;&#20854;&#31163;&#32447;&#29256;&#26412;&#65292;&#21518;&#32773;&#35775;&#38382;&#26356;&#22810;&#20449;&#24687;&#65292;&#21487;&#20197;&#35757;&#32451;&#25152;&#26377;&#24103;&#32780;&#19981;&#32771;&#34385;&#26102;&#38388;&#39034;&#24207;&#12290;&#36825;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has established test-time training (TTT) as a general framework to further improve a trained model at test time. Before making a prediction on each test instance, the model is trained on the same instance using a self-supervised task, such as image reconstruction with masked autoencoders. We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order. Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before. Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets. The relative improvement is 45% and 66% for instance and panoptic segmentation. Surprisingly, online TTT also outperforms its offline variant that accesses more information, training on all frames from the entire test video regardless of temporal order. This differs from previous findings using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#35757;&#32451;&#20013;&#30340;&#21453;&#20363;&#26469;&#35299;&#20915;&#23433;&#20840;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23558;&#36830;&#32493;&#21644;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#31995;&#32479;&#25277;&#35937;&#20026;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#27010;&#29575;&#24615;&#21453;&#20363;&#29983;&#25104;&#26500;&#24314;&#26368;&#23567;&#21270;&#20223;&#30495;&#23376;&#27169;&#22411;&#65292;&#20197;&#25581;&#31034;&#23433;&#20840;&#38656;&#27714;&#30340;&#36829;&#21453;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.04927</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#29575;&#24615;&#21453;&#20363;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Counterexample Guidance for Safer Reinforcement Learning. (arXiv:2307.04927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#35757;&#32451;&#20013;&#30340;&#21453;&#20363;&#26469;&#35299;&#20915;&#23433;&#20840;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23558;&#36830;&#32493;&#21644;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#31995;&#32479;&#25277;&#35937;&#20026;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#27010;&#29575;&#24615;&#21453;&#20363;&#29983;&#25104;&#26500;&#24314;&#26368;&#23567;&#21270;&#20223;&#30495;&#23376;&#27169;&#22411;&#65292;&#20197;&#25581;&#31034;&#23433;&#20840;&#38656;&#27714;&#30340;&#36829;&#21453;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#25506;&#32034;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#20854;&#20013;&#22312;&#35797;&#38169;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22833;&#36133;&#21487;&#33021;&#20250;&#23548;&#33268;&#39640;&#25104;&#26412;&#12290;&#23384;&#22312;&#22810;&#31181;&#26041;&#27861;&#26469;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#25110;&#20351;&#29992;&#36817;&#36317;&#31163;&#20256;&#24863;&#22120;&#25968;&#25454;&#26469;&#38480;&#21046;&#23545;&#19981;&#23433;&#20840;&#29366;&#24577;&#30340;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#20943;&#23569;&#25506;&#32034;&#39118;&#38505;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20195;&#29702;&#24517;&#39035;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#21457;&#29616;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#23433;&#20840;&#38656;&#27714;&#30340;&#21453;&#20363;&#24341;&#23548;&#35757;&#32451;&#26469;&#35299;&#20915;&#23433;&#20840;&#25506;&#32034;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#36830;&#32493;&#21644;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#31995;&#32479;&#25277;&#35937;&#20026;&#32039;&#20945;&#30340;&#25277;&#35937;&#27169;&#22411;&#65292;&#20195;&#34920;&#20195;&#29702;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#27010;&#29575;&#24615;&#21453;&#20363;&#29983;&#25104;&#26500;&#24314;&#26368;&#23567;&#21270;&#20223;&#30495;&#23376;&#27169;&#22411;&#65292;&#20197;&#25581;&#31034;&#23433;&#20840;&#38656;&#27714;&#30340;&#36829;&#21453;&#24773;&#20917;&#65292;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#20197;&#20248;&#21270;&#20854;&#31574;&#30053;&#65292;&#20943;&#23567;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#32925;&#30828;&#21270;&#39044;&#27979;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#25972;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.04617</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#65306;&#32925;&#30828;&#21270;&#20998;&#31867;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised positional contrastive learning: application to cirrhosis classification. (arXiv:2307.04617v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#32925;&#30828;&#21270;&#39044;&#27979;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#25972;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#21487;&#20197;&#36890;&#36807;&#20302;&#32622;&#20449;&#24230;&#30340;&#24369;&#26631;&#31614;&#65288;&#20363;&#22914;&#25918;&#23556;&#23398;&#35780;&#20998;&#65289;&#36827;&#34892;&#24265;&#20215;&#24555;&#36895;&#30340;&#27880;&#37322;&#12290;&#32780;&#39640;&#32622;&#20449;&#24230;&#30340;&#26631;&#31614;&#65288;&#22914;&#22522;&#20110;&#32452;&#32455;&#23398;&#30340;&#35786;&#26029;&#65289;&#24456;&#23569;&#19988;&#26114;&#36149;&#12290;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22914;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#25110;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#36739;&#22823;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65292;&#36825;&#22312;&#22823;&#22411;3D&#22270;&#20687;&#30340;&#20840;&#20998;&#36776;&#29575;&#24773;&#20917;&#19979;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;GPU&#20869;&#23384;&#26377;&#38480;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20851;&#20110;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#20307;&#31215;&#20449;&#24687;&#23545;&#20110;&#26576;&#20123;&#21307;&#23398;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#36827;&#34892;&#25972;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#65288;&#21363;&#25918;&#23556;&#23398;&#20302;&#32622;&#20449;&#24230;&#26631;&#27880;&#65289;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32925;&#30828;&#21270;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations,
&lt;/p&gt;</description></item><item><title>Solvent&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#32479;&#19968;&#30740;&#31350;&#26694;&#26550;&#65292;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#23454;&#29616;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#30340;&#26377;&#29992;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.04603</link><description>&lt;p&gt;
Solvent: &#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Solvent: A Framework for Protein Folding. (arXiv:2307.04603v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04603
&lt;/p&gt;
&lt;p&gt;
Solvent&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#32479;&#19968;&#30740;&#31350;&#26694;&#26550;&#65292;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#23454;&#29616;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#30340;&#26377;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#23545;&#20110;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#33879;&#21517;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22914;&#30446;&#26631;&#26816;&#27979;&#65292;&#24050;&#32463;&#36890;&#36807;&#31283;&#23450;&#30340;&#22522;&#20934;&#26694;&#26550;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#39564;&#35777;&#12290;&#22312;AlphaFold2&#20043;&#21518;&#65292;&#34507;&#30333;&#36136;&#25240;&#21472;&#20219;&#21153;&#24050;&#32463;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#38454;&#27573;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;AlphaFold2&#30340;&#32452;&#20214;&#25552;&#20986;&#30340;&#12290;&#22312;&#34507;&#30333;&#36136;&#25240;&#21472;&#20013;&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#30740;&#31350;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#21253;&#25324;&#23454;&#29616;&#21644;&#22522;&#20934;&#65292;&#20197;&#19968;&#33268;&#19988;&#20844;&#24179;&#22320;&#27604;&#36739;&#21508;&#31181;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Solvent&#65292;&#19968;&#20010;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#34507;&#30333;&#36136;&#25240;&#21472;&#26694;&#26550;&#12290;Solvent&#21253;&#21547;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#20195;&#30721;&#24211;&#20013;&#23454;&#29616;&#30340;&#19981;&#21516;&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#23450;&#20041;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#33879;&#21517;&#31639;&#27861;&#21450;&#20854;&#32452;&#20214;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#20197;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#25552;&#20379;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;Solvent&#33021;&#25552;&#39640;&#34507;&#30333;&#36136;&#25240;&#21472;&#30740;&#31350;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency and reliability are crucial for conducting AI research. Many famous research fields, such as object detection, have been compared and validated with solid benchmark frameworks. After AlphaFold2, the protein folding task has entered a new phase, and many methods are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve this, we present Solvent, an protein folding framework that supports significant components of state-of-th-arts models in the manner of off-the-shelf interface Solvent contains different models implemented in a unified codebase and supports training and evaluation for defined models on the same dataset. We benchmark well-known algorithms and their components and provide experiments that give helpful insights into the protein structure modeling field. We hope that Solvent will increase the reliabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;LGA&#22312;&#35745;&#31639;&#19978;&#31616;&#27905;&#19988;&#31283;&#23450;&#65292;&#33021;&#22815;&#22312;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#20013;&#23454;&#29616;&#25509;&#36817;&#22823;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#26356;&#21152;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.04149</link><description>&lt;p&gt;
&#22686;&#24378;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;LGA&#22312;&#35745;&#31639;&#19978;&#31616;&#27905;&#19988;&#31283;&#23450;&#65292;&#33021;&#22815;&#22312;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#20013;&#23454;&#29616;&#25509;&#36817;&#22823;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#26356;&#21152;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#22312;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#22270;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25429;&#25417;&#21040;&#20102;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#35745;&#31639;&#19978;&#27604;&#36739;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#33021;&#23398;&#20064;&#22270;&#20687;&#20013;&#20219;&#24847;&#20004;&#20010;&#28857;&#20043;&#38388;&#30340;&#37197;&#23545;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#35745;&#31639;&#31616;&#27905;&#65288;&#19982;&#33410;&#28857;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65289;&#21644;&#31283;&#23450;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20840;&#23616;&#19978;&#19979;&#25991;&#32435;&#20837;&#29616;&#26377;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#29305;&#21035;&#26159;&#22686;&#24378;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#23545;&#20110;&#35745;&#31639;&#33021;&#21147;&#36739;&#20302;&#21644;&#33021;&#37327;&#38656;&#27714;&#36739;&#20302;&#30340;&#36793;&#32536;&#35774;&#22791;&#26356;&#21152;&#26377;&#29992;&#12290;LGA&#20351;&#29992;&#23616;&#37096;&#36830;&#25509;&#22270;&#32593;&#32476;&#26469;&#22312;&#31354;&#38388;&#19978;&#20256;&#25773;&#20449;&#24687;&#65292;&#20174;&#32780;&#26041;&#20415;&#22320;&#26500;&#24314;&#36828;&#36317;&#31163;&#30340;&#20004;&#20010;&#31354;&#38388;&#28857;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global contexts in images are quite valuable in image-to-image translation problems. Conventional attention-based and graph-based models capture the global context to a large extent, however, these are computationally expensive. Moreover, the existing approaches are limited to only learning the pairwise semantic relation between any two points on the image. In this paper, we present Latent Graph Attention (LGA) a computationally inexpensive (linear to the number of nodes) and stable, modular framework for incorporating the global context in the existing architectures, especially empowering small-scale architectures to give performance closer to large size architectures, thus making the light-weight architectures more useful for edge devices with lower compute power and lower energy needs. LGA propagates information spatially using a network of locally connected graphs, thereby facilitating to construct a semantically coherent relation between any two spatially distant points that also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#20048;&#35889;&#20013;&#30340;&#27599;&#20010;&#38899;&#31526;&#65292;&#21033;&#29992;&#38899;&#31526;&#29305;&#24449;&#21644;&#38899;&#31526;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#26032;&#22411;&#36793;&#32553;&#20943;&#31639;&#27861;&#20135;&#29983;&#25353;&#38899;&#30340;&#34920;&#31034;&#12290;&#22312;&#21442;&#32771;&#25968;&#25454;&#38598;&#19978;&#65292;ChordGNN&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.03544</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#65306;&#20174;&#38899;&#31526;&#29305;&#24449;&#21040;&#25353;&#38899;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features. (arXiv:2307.03544v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#20048;&#35889;&#20013;&#30340;&#27599;&#20010;&#38899;&#31526;&#65292;&#21033;&#29992;&#38899;&#31526;&#29305;&#24449;&#21644;&#38899;&#31526;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#26032;&#22411;&#36793;&#32553;&#20943;&#31639;&#27861;&#20135;&#29983;&#25353;&#38899;&#30340;&#34920;&#31034;&#12290;&#22312;&#21442;&#32771;&#25968;&#25454;&#38598;&#19978;&#65292;ChordGNN&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#26159;&#22312;&#35843;&#24615;&#38899;&#20048;&#20316;&#21697;&#20013;&#35782;&#21035;&#21644;&#30830;&#23450;&#21644;&#24358;&#20197;&#21450;&#20854;&#21151;&#33021;&#32972;&#26223;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21495;&#38899;&#20048;&#30340;&#33258;&#21160;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#26032;&#26041;&#27861;&#12290;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#23545;&#20048;&#35889;&#30340;&#20013;&#38388;&#20002;&#22833;&#21387;&#32553;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#25551;&#36848;&#21644;&#22788;&#29702;&#20048;&#35889;&#20013;&#30340;&#27599;&#19968;&#20010;&#38899;&#31526;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21487;&#20197;&#21033;&#29992;&#38899;&#31526;&#29305;&#24449;&#21644;&#38899;&#31526;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#22411;&#36793;&#32553;&#20943;&#31639;&#27861;&#20135;&#29983;&#25353;&#38899;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;ChordGNN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#21442;&#32771;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;NADE&#21644;&#21518;&#22788;&#29702;&#21644;&#24358;&#39044;&#27979;&#31561;&#25216;&#26415;&#30340;&#27169;&#22411;&#21464;&#20307;&#12290;&#26412;&#25991;&#30340;&#23436;&#25972;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/mano&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music. This paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an intermediate lossy representation of the score, we propose a new method based on Graph Neural Networks (GNNs) that enable the direct description and processing of each individual note in the score. The proposed architecture can leverage notewise features and interdependencies between notes but yield onset-wise representation by virtue of our novel edge contraction algorithm. Our results demonstrate that ChordGNN outperforms existing state-of-the-art models, achieving higher accuracy in Roman Numeral analysis on the reference datasets. In addition, we investigate variants of our model using proposed techniques such as NADE, and post-processing of the chord predictions. The full source code for this work is available at https://github.com/mano
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#26469;&#20462;&#21098;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#22320;&#25214;&#21040;&#31232;&#30095;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65292;&#20855;&#26377;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03364</link><description>&lt;p&gt;
&#33976;&#39311;&#20462;&#21098;&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36194;&#24471;&#24425;&#31080;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distilled Pruning: Using Synthetic Data to Win the Lottery. (arXiv:2307.03364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#26469;&#20462;&#21098;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#22320;&#25214;&#21040;&#31232;&#30095;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65292;&#20855;&#26377;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#26469;&#20462;&#21098;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#31574;&#30053;&#20027;&#35201;&#20851;&#27880;&#20307;&#31995;&#32467;&#26500;&#25110;&#31639;&#27861;&#20248;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37325;&#26032;&#32771;&#34385;&#20102;&#25968;&#25454;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#30340;&#20316;&#29992;&#12290;&#33976;&#39311;&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#26356;&#22823;&#25968;&#25454;&#38598;&#20013;&#30340;&#37325;&#35201;&#27169;&#24335;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#26469;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20462;&#21098;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;CIFAR-10&#19978;&#27604;&#36845;&#20195;&#24133;&#20540;&#20462;&#21098;&#26356;&#24555;&#22320;&#25214;&#21040;&#31232;&#30095;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65288;&#20063;&#31216;&#20026;&#24425;&#31080;&#31080;&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#36827;&#34892;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#12289;&#27169;&#22411;&#21387;&#32553;&#21644;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a novel approach to pruning deep learning models by using distilled data. Unlike conventional strategies which primarily focus on architectural or algorithmic optimization, our method reconsiders the role of data in these scenarios. Distilled datasets capture essential patterns from larger datasets, and we demonstrate how to leverage this capability to enable a computationally efficient pruning process. Our approach can find sparse, trainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative Magnitude Pruning at comparable sparsity on CIFAR-10. The experimental results highlight the potential of using distilled data for resource-efficient neural network pruning, model compression, and neural architecture search.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#26469;&#21019;&#24314;&#33402;&#26415;&#24615;&#24433;&#21160;&#22270;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#21452;&#32990;&#32974;&#65292;&#21363;&#19968;&#23545;&#33402;&#26415;&#22270;&#20687;&#21644;&#19982;&#20043;&#23545;&#40784;&#30340;&#30495;&#23454;&#22270;&#20687;&#65292;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#33402;&#26415;&#39118;&#26684;&#21644;&#22806;&#35266;&#30340;&#35201;&#27714;&#24182;&#31616;&#21270;&#21160;&#20316;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21487;&#20197;&#20934;&#30830;&#22320;&#20998;&#21106;&#30495;&#23454;&#22270;&#20687;&#24182;&#39044;&#27979;&#21512;&#29702;&#30340;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2307.03190</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#29983;&#25104;&#33402;&#26415;&#24615;&#30340;&#24433;&#21160;&#22270;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Artistic Cinemagraphs from Text. (arXiv:2307.03190v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#26469;&#21019;&#24314;&#33402;&#26415;&#24615;&#24433;&#21160;&#22270;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#21452;&#32990;&#32974;&#65292;&#21363;&#19968;&#23545;&#33402;&#26415;&#22270;&#20687;&#21644;&#19982;&#20043;&#23545;&#40784;&#30340;&#30495;&#23454;&#22270;&#20687;&#65292;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#33402;&#26415;&#39118;&#26684;&#21644;&#22806;&#35266;&#30340;&#35201;&#27714;&#24182;&#31616;&#21270;&#21160;&#20316;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21487;&#20197;&#20934;&#30830;&#22320;&#20998;&#21106;&#30495;&#23454;&#22270;&#20687;&#24182;&#39044;&#27979;&#21512;&#29702;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#26469;&#21019;&#24314;&#33402;&#26415;&#24615;&#30340;&#24433;&#21160;&#22270;&#12290;&#22312;&#22788;&#29702;&#34394;&#26500;&#20803;&#32032;&#21644;&#33402;&#26415;&#39118;&#26684;&#30340;&#25552;&#31034;&#26102;&#65292;&#36825;&#26159;&#19968;&#39033;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#35299;&#37322;&#36825;&#20123;&#22270;&#20687;&#30340;&#35821;&#20041;&#21644;&#21160;&#20316;&#30340;&#22797;&#26434;&#24615;&#12290;&#29616;&#26377;&#30340;&#21333;&#22270;&#21160;&#30011;&#26041;&#27861;&#22312;&#33402;&#26415;&#24615;&#36755;&#20837;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#26368;&#36817;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#26041;&#27861;&#24120;&#24120;&#24341;&#20837;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#65292;&#38590;&#20197;&#20351;&#26576;&#20123;&#21306;&#22495;&#20445;&#25345;&#38745;&#24577;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#20010;&#25991;&#26412;&#25552;&#31034;&#21512;&#25104;&#22270;&#20687;&#21452;&#32990;&#32974;&#30340;&#24605;&#24819;&#65292;&#21363;&#33402;&#26415;&#22270;&#20687;&#21644;&#20854;&#19982;&#20687;&#32032;&#23545;&#40784;&#30340;&#33258;&#28982;&#22806;&#35266;&#37197;&#23545;&#12290;&#34429;&#28982;&#33402;&#26415;&#22270;&#20687;&#25551;&#32472;&#20102;&#25105;&#20204;&#22312;&#25991;&#26412;&#25552;&#31034;&#20013;&#35814;&#32454;&#25551;&#36848;&#30340;&#39118;&#26684;&#21644;&#22806;&#35266;&#65292;&#20294;&#30495;&#23454;&#30340;&#23545;&#24212;&#22270;&#20687;&#22823;&#22823;&#31616;&#21270;&#20102;&#24067;&#23616;&#21644;&#21160;&#20316;&#20998;&#26512;&#12290;&#21033;&#29992;&#29616;&#26377;&#30340;&#33258;&#28982;&#22270;&#20687;&#21644;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#20998;&#21106;&#20986;&#30495;&#23454;&#22270;&#20687;&#24182;&#26681;&#25454;&#35821;&#20041;&#20449;&#24687;&#39044;&#27979;&#20986;&#21512;&#29702;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Artistic Cinemagraph, a fully automated method for creating cinemagraphs from text descriptions - an especially challenging task when prompts feature imaginary elements and artistic styles, given the complexity of interpreting the semantics and motions of these images. Existing single-image animation methods fall short on artistic inputs, and recent text-based video methods frequently introduce temporal inconsistencies, struggling to keep certain regions static. To address these challenges, we propose an idea of synthesizing image twins from a single text prompt - a pair of an artistic image and its pixel-aligned corresponding natural-looking twin. While the artistic image depicts the style and appearance detailed in our text prompt, the realistic counterpart greatly simplifies layout and motion analysis. Leveraging existing natural image and video datasets, we can accurately segment the realistic image and predict plausible motion given the semantic information. The predi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.03042</link><description>&lt;p&gt;
LLaMA&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22914;&#20020;&#24202;&#24212;&#29992;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#24040;&#22823;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#36341;&#24615;&#36234;&#26469;&#36234;&#34987;&#35777;&#26126;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#19968;&#20010;&#23567;&#30340;&#38468;&#21152;&#21442;&#25968;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#39046;&#22495;&#36866;&#24212;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;LLaMA-LoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#26500;&#24314;&#22312;&#24320;&#28304;LLaMA&#27169;&#22411;&#19978;&#30340;PEFT&#36866;&#37197;&#22120;&#23618;&#12290;&#20020;&#24202;LLaMA-LoRA&#20351;&#29992;&#20174;MIMIC-IV&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#20026;&#20020;&#24202;&#39046;&#22495;&#35774;&#35745;&#30340;&#19987;&#29992;&#36866;&#37197;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#23558;&#20020;&#24202;LLaMA-LoRA&#19982;Downstream LLaMA-LoRA&#36827;&#34892;&#34701;&#21512;&#65292;&#21518;&#32773;&#26159;&#21478;&#19968;&#20010;&#19987;&#20026;&#19979;&#28216;&#20219;&#21153;&#35774;&#35745;&#30340;PEFT&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. W
&lt;/p&gt;</description></item><item><title>MOPO-LSI&#26159;&#19968;&#27454;&#24320;&#28304;&#30340;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#65292;&#20026;&#21487;&#25345;&#32493;&#25237;&#36164;&#25552;&#20379;&#29992;&#25143;&#25351;&#21335;&#65292;&#24182;&#20171;&#32461;&#20102;&#29256;&#26412;1.0&#30340;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01719</link><description>&lt;p&gt;
MOPO-LSI&#65306;&#29992;&#25143;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI: A User Guide. (arXiv:2307.01719v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01719
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI&#26159;&#19968;&#27454;&#24320;&#28304;&#30340;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#65292;&#20026;&#21487;&#25345;&#32493;&#25237;&#36164;&#25552;&#20379;&#29992;&#25143;&#25351;&#21335;&#65292;&#24182;&#20171;&#32461;&#20102;&#29256;&#26412;1.0&#30340;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MOPO-LSI&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#21487;&#25345;&#32493;&#25237;&#36164;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;MOPO-LSI&#29256;&#26412;1.0&#30340;&#29992;&#25143;&#25351;&#21335;&#65292;&#21253;&#25324;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#37197;&#32622;&#20013;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for Sustainable Investments. This document provides a user guide for MOPO-LSI version 1.0, including problem setup, workflow and the hyper-parameters in configurations.
&lt;/p&gt;</description></item><item><title>ESGCN&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#21644;&#24341;&#20837;&#36793;&#32536;&#29305;&#24449;&#21644;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01227</link><description>&lt;p&gt;
ESGCN: &#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting. (arXiv:2307.01227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01227
&lt;/p&gt;
&lt;p&gt;
ESGCN&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#21644;&#24341;&#20837;&#36793;&#32536;&#29305;&#24449;&#21644;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20132;&#36890;&#27969;&#30340;&#21160;&#24577;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Edge Squeeze Graph Convolutional Network (ESGCN)&#30340;&#32593;&#32476;&#26469;&#39044;&#27979;&#22810;&#20010;&#22320;&#21306;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;ESGCN&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;W&#27169;&#22359;&#21644;ES&#27169;&#22359;&#12290;W&#27169;&#22359;&#26159;&#19968;&#20010;&#23436;&#20840;&#20197;&#33410;&#28857;&#20026;&#22522;&#30784;&#30340;&#21367;&#31215;&#32593;&#32476;&#12290;&#23427;&#20998;&#21035;&#23545;&#27599;&#20010;&#20132;&#36890;&#21306;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#20998;&#35299;&#26102;&#38388;&#24207;&#21015;&#20197;&#25429;&#25417;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#30340;&#29305;&#24449;&#12290;ES&#27169;&#22359;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#65292;&#24182;&#21033;&#29992;&#26102;&#24207;&#29305;&#24449;&#29983;&#25104;&#33258;&#36866;&#24212;&#37051;&#25509;&#30697;&#38453;(AAM)&#12290;&#20026;&#20102;&#25552;&#39640;AAM&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#20851;&#38190;&#27010;&#24565;&#12290;1&#65289;&#20351;&#29992;&#36793;&#32536;&#29305;&#24449;&#30452;&#25509;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#26102;&#31354;&#27969;&#21160;&#34920;&#31034;&#12290;2&#65289;&#23558;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#24212;&#29992;&#20110;GCN&#65292;&#20174;&#36793;&#32536;&#29305;&#24449;&#20013;&#25552;&#21462;AAM&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is a highly challenging task owing to the dynamical spatio-temporal dependencies of traffic flows. To handle this, we focus on modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple regions. ESGCN consists of two modules: W-module and ES module. W-module is a fully node-wise convolutional network. It encodes the time-series of each traffic region separately and decomposes the time-series at various scales to capture fine and coarse features. The ES module models the spatio-temporal dynamics using Graph Convolutional Network (GCN) and generates an Adaptive Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM, we introduce three key concepts. 1) Using edge features to directly capture the spatiotemporal flow representation among regions. 2) Applying an edge attention mechanism to GCN to extract the AAM from the edge features. Here, the attention m
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;ReLU&#28608;&#27963;&#30340;&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#19981;&#21516;&#32500;&#24230;&#20013;&#36924;&#36817;&#21333;&#20301;&#29699;&#24418;&#65292;&#29983;&#25104;&#31070;&#32463;&#22810;&#38754;&#20307;&#65292;&#36825;&#19968;&#21457;&#29616;&#24320;&#21551;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#36924;&#36817;&#26354;&#38754;&#30340;&#29983;&#25104;&#31163;&#25955;&#20960;&#20309;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.00721</link><description>&lt;p&gt;
&#31070;&#32463;&#22810;&#38754;&#20307;
&lt;/p&gt;
&lt;p&gt;
Neural Polytopes. (arXiv:2307.00721v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00721
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ReLU&#28608;&#27963;&#30340;&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#19981;&#21516;&#32500;&#24230;&#20013;&#36924;&#36817;&#21333;&#20301;&#29699;&#24418;&#65292;&#29983;&#25104;&#31070;&#32463;&#22810;&#38754;&#20307;&#65292;&#36825;&#19968;&#21457;&#29616;&#24320;&#21551;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#36924;&#36817;&#26354;&#38754;&#30340;&#29983;&#25104;&#31163;&#25955;&#20960;&#20309;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;ReLU&#28608;&#27963;&#30340;&#31616;&#21333;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#19981;&#21516;&#32500;&#24230;&#20013;&#36924;&#36817;&#21333;&#20301;&#29699;&#24418;&#65292;&#29983;&#25104;&#22810;&#38754;&#20307;&#12290;&#22810;&#38754;&#20307;&#30340;&#31181;&#31867;&#21463;&#21040;&#32593;&#32476;&#26550;&#26500;&#30340;&#35843;&#33410;&#65292;&#22914;&#21333;&#20803;&#25968;&#21644;&#23618;&#25968;&#12290;&#23545;&#20110;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#22810;&#38754;&#20307;&#30340;&#27867;&#21270;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31070;&#32463;&#22810;&#38754;&#20307;&#12290;&#23427;&#20204;&#26159;&#22810;&#38754;&#20307;&#30340;&#24179;&#28369;&#31867;&#27604;&#65292;&#23637;&#29616;&#20102;&#20960;&#20309;&#30340;&#23545;&#20598;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24320;&#21551;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#36924;&#36817;&#26354;&#38754;&#30340;&#29983;&#25104;&#31163;&#25955;&#20960;&#20309;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We find that simple neural networks with ReLU activation generate polytopes as an approximation of a unit sphere in various dimensions. The species of polytopes are regulated by the network architecture, such as the number of units and layers. For a variety of activation functions, generalization of polytopes is obtained, which we call neural polytopes. They are a smooth analogue of polytopes, exhibiting geometric duality. This finding initiates research of generative discrete geometry to approximate surfaces by machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15951</link><description>&lt;p&gt;
&#36890;&#36807;&#36339;&#36807;&#38646;&#20803;&#32032;&#38477;&#20302;&#21367;&#31215;&#23618;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#24182;&#34892;&#22788;&#29702;&#22120;&#36827;&#34892;&#21152;&#36895;&#12290;&#20026;&#20102;&#20026;&#20854;&#35774;&#35745;&#36816;&#31639;&#31526;&#65292;&#38656;&#35201;&#19981;&#20165;&#26377;&#20248;&#21270;&#31639;&#27861;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#36824;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#30828;&#20214;&#36164;&#28304;&#12290;&#21367;&#31215;&#23618;&#20027;&#35201;&#21253;&#21547;&#19977;&#31181;&#36816;&#31639;&#31526;&#65306;&#21069;&#21521;&#20256;&#25773;&#30340;&#21367;&#31215;&#65292;&#21453;&#21521;&#20256;&#25773;&#30340;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#12290;&#24403;&#25191;&#34892;&#36825;&#20123;&#36816;&#31639;&#26102;&#65292;&#22987;&#32456;&#20250;&#21521;&#24352;&#37327;&#20013;&#28155;&#21152;0&#20803;&#32032;&#65292;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65288;ConvV2, KS-deconv, Sk-dilated&#65289;&#65292;&#20197;&#20004;&#31181;&#26041;&#24335;&#36339;&#36807;&#36825;&#20123;0&#20803;&#32032;&#65306;&#20462;&#21098;&#28388;&#27874;&#22120;&#20197;&#25490;&#38500;&#22635;&#20805;&#30340;0&#20803;&#32032;&#65307;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#20026;&#31264;&#23494;&#24352;&#37327;&#65292;&#36991;&#20813;&#22312;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#20013;&#25554;&#20837;0&#20803;&#32032;&#12290;&#19982;&#26222;&#36890;&#21367;&#31215;&#30456;&#27604;&#65292;&#21453;&#21367;&#31215;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#21152;&#36895;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;C-K-S&#30340;&#39640;&#24615;&#33021;GPU&#23454;&#29616;&#65292;&#24182;&#36890;&#36807;&#19982;PyTorch&#30340;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#37319;&#26679;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22312;&#24674;&#22797;&#32593;&#32476;&#30340;&#22270;&#24418;&#32467;&#26500;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21518;&#39564;&#27010;&#29575;&#30340;&#20934;&#30830;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2306.11380</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Take on Gaussian Process Networks. (arXiv:2306.11380v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#37319;&#26679;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22312;&#24674;&#22797;&#32593;&#32476;&#30340;&#22270;&#24418;&#32467;&#26500;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21518;&#39564;&#27010;&#29575;&#30340;&#20934;&#30830;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#65288;GPNs&#65289;&#26159;&#19968;&#31867;&#26377;&#21521;&#22270;&#27169;&#22411;&#65292;&#20854;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#20316;&#20026;&#32593;&#32476;&#20013;&#27599;&#20010;&#21464;&#37327;&#32473;&#23450;&#20854;&#29238;&#21464;&#37327;&#30340;&#26465;&#20214;&#26399;&#26395;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#20801;&#35768;&#20197;&#32039;&#20945;&#20294;&#28789;&#27963;&#30340;&#26041;&#24335;&#25551;&#36848;&#36830;&#32493;&#32852;&#21512;&#20998;&#24067;&#65292;&#23545;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20165;&#20570;&#26368;&#23569;&#30340;&#21442;&#25968;&#20551;&#35774;&#12290;GPNs&#30340;&#36125;&#21494;&#26031;&#32467;&#26500;&#23398;&#20064;&#38656;&#35201;&#35745;&#31639;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21363;&#20351;&#22312;&#20302;&#32500;&#24773;&#20917;&#19979;&#65292;&#36825;&#20063;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#33945;&#29305;&#21345;&#32599;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#26469;&#20174;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#36981;&#24490;&#36125;&#21494;&#26031;&#33539;&#24335;&#65292;&#36890;&#36807;&#36793;&#32536;&#20284;&#28982;&#27604;&#36739;&#27169;&#22411;&#65292;&#24182;&#35745;&#31639;GPN&#29305;&#24449;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#27169;&#25311;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24674;&#22797;&#32593;&#32476;&#30340;&#22270;&#24418;&#32467;&#26500;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20854;&#21518;&#39564;&#30340;&#20934;&#30830;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Networks (GPNs) are a class of directed graphical models which employ Gaussian processes as priors for the conditional expectation of each variable given its parents in the network. The model allows describing continuous joint distributions in a compact but flexible manner with minimal parametric assumptions on the dependencies between variables. Bayesian structure learning of GPNs requires computing the posterior over graphs of the network and is computationally infeasible even in low dimensions. This work implements Monte Carlo and Markov Chain Monte Carlo methods to sample from the posterior distribution of network structures. As such, the approach follows the Bayesian paradigm, comparing models via their marginal likelihood and computing the posterior probability of the GPN features. Simulation studies show that our method outperforms state-of-the-art algorithms in recovering the graphical structure of the network and provides an accurate approximation of its poste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#25351;&#23548;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#26032;&#39062;&#20294;&#21512;&#36866;&#30340;&#25351;&#23548;&#25514;&#36766;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11270</link><description>&lt;p&gt;
&#35780;&#20272;&#25351;&#23548;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Zero-shot Robustness of Instruction-tuned Language Models. (arXiv:2306.11270v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#25351;&#23548;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#26032;&#39062;&#20294;&#21512;&#36866;&#30340;&#25351;&#23548;&#25514;&#36766;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#24494;&#35843;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#19978;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#25913;&#21892;&#20013;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#29305;&#21035;&#30340;&#20248;&#21183;&#65292;&#26377;&#26102;&#29978;&#33267;&#19982;&#26356;&#22823;&#30340;&#27169;&#22411;&#21464;&#31181;&#30456;&#31454;&#20105;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#23545;&#25351;&#23548;&#30340;&#29305;&#23450;&#25514;&#36766;&#26377;&#22810;&#25935;&#24863;&#65292;&#65288;2&#65289;&#22914;&#20309;&#20351;&#23427;&#20204;&#26356;&#33021;&#25269;&#25239;&#33258;&#28982;&#35821;&#35328;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#22238;&#31572;&#21069;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#30001;NLP&#20174;&#19994;&#32773;&#25163;&#24037;&#32534;&#20889;&#30340;319&#20010;&#25351;&#23548;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;80&#22810;&#20010;&#29420;&#29305;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#25351;&#23548;&#19982;&#25351;&#23548;&#24494;&#35843;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#25351;&#23548;&#25514;&#36766;&#20043;&#38388;&#30340;&#26041;&#24046;&#21644;&#24179;&#22343;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#26032;&#39062;&#65288;&#26410;&#35266;&#23519;&#21040;&#30340;&#65289;&#20294;&#21512;&#36866;&#30340;&#25351;&#23548;&#25514;&#36766;&#20250;&#19968;&#33268;&#22320;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#29978;&#33267;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.11197</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22359;&#28608;&#27963;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411; (SSM) &#22312;&#21508;&#31181;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#24490;&#29615;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#32508;&#21512;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;SSM&#12290;&#21516;&#26102;&#20351;&#29992;SSM&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#27169;&#22411;&#36890;&#24120;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#27169;&#22359;&#38745;&#24577;&#19988;&#22343;&#21248;&#22320;&#24212;&#29992;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#20803;&#32032;&#65292;&#23548;&#33268;&#20102;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#27425;&#20248;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA)&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#31232;&#30095;&#22320;&#21160;&#24577;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#12290;&#36890;&#36807;&#20801;&#35768;&#27599;&#20010;&#20803;&#32032;&#36339;&#36807;&#38750;&#28608;&#27963;&#30340;&#23376;&#27169;&#22359;&#65292;SMA&#21487;&#20197;&#22312;&#24207;&#21015;&#24314;&#27169;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#20316;&#20026;SMA&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.10548</link><description>&lt;p&gt;
MARBLE&#65306;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33402;&#26415;&#19982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20043;&#38388;&#20132;&#21449;&#30340;&#24191;&#27867;&#26102;&#20195;&#20013;&#65292;&#20363;&#22914;&#22270;&#20687;&#29983;&#25104;&#21644;&#34394;&#26500;&#20849;&#21019;&#65292;&#38899;&#20048;&#30340;AI&#20173;&#28982;&#30456;&#23545;&#21021;&#27493;&#65292;&#29305;&#21035;&#26159;&#22312;&#38899;&#20048;&#29702;&#35299;&#26041;&#38754;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#35780;&#20272;&#22522;&#20934;MARBLE&#65292;&#26088;&#22312;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#23450;&#20041;&#21253;&#25324;&#22768;&#23398;&#65292;&#28436;&#22863;&#65292;&#20048;&#35889;&#21644;&#39640;&#32423;&#25551;&#36848;&#22312;&#20869;&#30340;&#22235;&#20010;&#23618;&#27425;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;8&#20010;&#20844;&#20849;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;14&#39033;&#20219;&#21153;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21327;&#35758;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#22522;&#20110;&#38899;&#20048;&#24405;&#38899;&#24320;&#21457;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#24449;&#30340;&#20844;&#24179;&#21644;&#26631;&#20934;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;MARBLE&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#37325;&#29992;&#30340;&#24037;&#20855;&#24211;&#65292;&#20197;&#25903;&#25345;&#31038;&#21306;&#39537;&#21160;&#30340;&#23458;&#35266;&#22522;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and re
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#30456;&#20851;&#37319;&#26679;&#25216;&#26415;&#30340;&#23454;&#29992;&#35780;&#20272;&#35774;&#32622;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10453</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#24403;&#21069;&#30340;&#38382;&#39064;&#19982;&#26032;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking. (arXiv:2306.10453v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10453
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#30456;&#20851;&#37319;&#26679;&#25216;&#26415;&#30340;&#23454;&#29992;&#35780;&#20272;&#35774;&#32622;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#35797;&#22270;&#26681;&#25454;&#22270;&#30340;&#19968;&#37096;&#20998;&#36793;&#26469;&#39044;&#27979;&#26159;&#21542;&#23384;&#22312;&#26410;&#35265;&#30340;&#36793;&#12290;&#36817;&#24180;&#26469;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#35797;&#22270;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#36825;&#19968;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#26032;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#36825;&#20123;&#26032;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#22810;&#20010;&#38382;&#39064;&#38459;&#30861;&#25105;&#20204;&#33021;&#22815;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#38382;&#39064;&#20027;&#35201;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#34920;&#29616;&#20302;&#20110;&#23454;&#38469;&#34920;&#29616;&#65292;&#65288;2&#65289;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#32570;&#20047;&#32479;&#19968;&#30340;&#25968;&#25454;&#21010;&#20998;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20351;&#29992;&#31616;&#21333;&#36127;&#26679;&#26412;&#30340;&#19981;&#29616;&#23454;&#30340;&#35780;&#20272;&#35774;&#32622;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#30693;&#21517;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#35774;&#32622;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#21551;&#21457;&#24335;&#30456;&#20851;&#37319;&#26679;&#25216;&#26415;&#65288;HeaRT&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#35780;&#20272;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction attempts to predict whether an unseen edge exists based on only a portion of edges of a graph. A flurry of methods have been introduced in recent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better evaluate the effectiveness of these new models. However, multiple pitfalls currently exist that hinder our ability to properly evaluate these new methods. These pitfalls mainly include: (1) Lower than actual performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, and (3) An unrealistic evaluation setting that uses easy negative samples. To overcome these challenges, we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset and hyperparameter search settings. We then create a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard ne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#23558;&#24403;&#20195;&#29983;&#25104;&#24314;&#27169;&#24037;&#20855;&#20013;&#30340;&#25512;&#29702;&#25216;&#26415;&#37325;&#26032;&#35299;&#37322;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#35268;&#21010;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.08810</link><description>&lt;p&gt;
Decision-Making and Control&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Decision-Making and Control. (arXiv:2306.08810v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#23558;&#24403;&#20195;&#29983;&#25104;&#24314;&#27169;&#24037;&#20855;&#20013;&#30340;&#25512;&#29702;&#25216;&#26415;&#37325;&#26032;&#35299;&#37322;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#27010;&#24565;&#19978;&#31616;&#21333;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65306;&#20351;&#29992;&#23398;&#20064;&#26469;&#20272;&#35745;&#36817;&#20284;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#23558;&#21097;&#20313;&#24037;&#20316;&#22996;&#25176;&#32473;&#32463;&#20856;&#36712;&#36857;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32452;&#21512;&#23384;&#22312;&#19968;&#20123;&#32463;&#39564;&#19978;&#30340;&#32570;&#28857;&#65292;&#38480;&#21046;&#20102;&#23454;&#38469;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#35770;&#25991;&#30340;&#21452;&#37325;&#30446;&#30340;&#26159;&#30740;&#31350;&#36825;&#20123;&#32570;&#28857;&#30340;&#21407;&#22240;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#27861;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24403;&#20195;&#29983;&#25104;&#24314;&#27169;&#24037;&#20855;&#31665;&#20013;&#30340;&#25512;&#29702;&#25216;&#26415;&#65292;&#21253;&#25324;&#26463;&#25628;&#32034;&#12289;&#20998;&#31867;&#22120;&#24341;&#23548;&#37319;&#26679;&#21644;&#22270;&#20687;&#20462;&#22797;&#65292;&#22914;&#20309;&#37325;&#26032;&#35299;&#37322;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#21487;&#34892;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep model-based reinforcement learning methods offer a conceptually simple approach to the decision-making and control problem: use learning for the purpose of estimating an approximate dynamics model, and offload the rest of the work to classical trajectory optimization. However, this combination has a number of empirical shortcomings, limiting the usefulness of model-based methods in practice. The dual purpose of this thesis is to study the reasons for these shortcomings and to propose solutions for the uncovered problems. Along the way, we highlight how inference techniques from the contemporary generative modeling toolbox, including beam search, classifier-guided sampling, and image inpainting, can be reinterpreted as viable planning strategies for reinforcement learning problems.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#36712;&#36857;&#24341;&#23548;&#21644;&#38024;&#23574;&#21147;&#21453;&#39304;&#30340;&#21327;&#20316;&#26426;&#22120;&#20154;&#27963;&#26816;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#21644;&#21307;&#29983;&#30340;&#23454;&#26102;&#25511;&#21046;&#65292;&#25552;&#20379;&#20102;&#38024;&#22836;&#25918;&#32622;&#30340;&#36741;&#21161;&#12290;&#30740;&#31350;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23454;&#26102;&#24863;&#30693;&#38024;&#23574;&#21147;&#30340;&#38024;&#22836;&#35774;&#35745;&#65292;&#36890;&#36807;&#27492;&#31995;&#32479;&#21487;&#20197;&#25913;&#21892;&#38024;&#22836;&#30340;&#23450;&#20301;&#65292;&#25552;&#39640;&#27963;&#26816;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07129</link><description>&lt;p&gt;
&#20855;&#26377;&#36712;&#36857;&#24341;&#23548;&#21644;&#38024;&#23574;&#21147;&#21453;&#39304;&#30340;&#21327;&#20316;&#26426;&#22120;&#20154;&#27963;&#26816;
&lt;/p&gt;
&lt;p&gt;
Collaborative Robotic Biopsy with Trajectory Guidance and Needle Tip Force Feedback. (arXiv:2306.07129v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#36712;&#36857;&#24341;&#23548;&#21644;&#38024;&#23574;&#21147;&#21453;&#39304;&#30340;&#21327;&#20316;&#26426;&#22120;&#20154;&#27963;&#26816;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#21644;&#21307;&#29983;&#30340;&#23454;&#26102;&#25511;&#21046;&#65292;&#25552;&#20379;&#20102;&#38024;&#22836;&#25918;&#32622;&#30340;&#36741;&#21161;&#12290;&#30740;&#31350;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23454;&#26102;&#24863;&#30693;&#38024;&#23574;&#21147;&#30340;&#38024;&#22836;&#35774;&#35745;&#65292;&#36890;&#36807;&#27492;&#31995;&#32479;&#21487;&#20197;&#25913;&#21892;&#38024;&#22836;&#30340;&#23450;&#20301;&#65292;&#25552;&#39640;&#27963;&#26816;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#26816;&#30340;&#35786;&#26029;&#20215;&#20540;&#39640;&#24230;&#20381;&#36182;&#20110;&#38024;&#22836;&#30340;&#25918;&#32622;&#12290;&#26426;&#22120;&#20154;&#36712;&#36857;&#24341;&#23548;&#24050;&#34987;&#35777;&#26126;&#33021;&#25913;&#21892;&#38024;&#22836;&#23450;&#20301;&#65292;&#20294;&#23454;&#26102;&#23548;&#33322;&#30340;&#21453;&#39304;&#26377;&#38480;&#12290;&#38024;&#23574;&#21147;&#30340;&#35302;&#35273;&#26174;&#31034;&#21487;&#20197;&#36890;&#36807;&#20351;&#32452;&#32455;&#32467;&#26500;&#22312;&#25554;&#20837;&#36335;&#24452;&#19978;&#23450;&#20301;&#65292;&#20026;&#38024;&#22836;&#23548;&#33322;&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#20316;&#30340;&#26426;&#22120;&#20154;&#27963;&#26816;&#31995;&#32479;&#65292;&#23558;&#36712;&#36857;&#24341;&#23548;&#19982;&#21160;&#35273;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#36741;&#21161;&#21307;&#29983;&#36827;&#34892;&#38024;&#22836;&#25918;&#32622;&#12290;&#26426;&#22120;&#20154;&#22312;&#25554;&#20837;&#36807;&#31243;&#20013;&#35843;&#25972;&#38024;&#22836;&#30340;&#20301;&#32622;&#65292;&#19982;&#29616;&#22330;&#25511;&#21046;&#38024;&#22836;&#20301;&#32622;&#30340;&#21307;&#30103;&#19987;&#23478;&#21327;&#20316;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23454;&#26102;&#24863;&#30693;&#38024;&#23574;&#21147;&#30340;&#38024;&#22836;&#35774;&#35745;&#65292;&#22522;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#21644;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#23454;&#26102;&#25968;&#25454;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#20154;&#35774;&#32622;&#20801;&#35768;&#25805;&#20316;&#20154;&#21592;&#29420;&#31435;&#20110;&#25705;&#25830;&#21147;&#24863;&#30693;&#28145;&#23618;&#32452;&#32455;&#30028;&#38754;&#65292;&#20197;&#25913;&#21892;&#30456;&#23545;&#20110;&#26399;&#26395;&#30446;&#26631;&#32467;&#26500;&#30340;&#38024;&#22836;&#25918;&#32622;&#12290;&#25105;&#20204;&#39318;&#20808;&#35780;&#20272;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The diagnostic value of biopsies is highly dependent on the placement of needles. Robotic trajectory guidance has been shown to improve needle positioning, but feedback for real-time navigation is limited. Haptic display of needle tip forces can provide rich feedback for needle navigation by enabling localization of tissue structures along the insertion path. We present a collaborative robotic biopsy system that combines trajectory guidance with kinesthetic feedback to assist the physician in needle placement. The robot aligns the needle while the insertion is performed in collaboration with a medical expert who controls the needle position on site. We present a needle design that senses forces at the needle tip based on optical coherence tomography and machine learning for real-time data processing. Our robotic setup allows operators to sense deep tissue interfaces independent of frictional forces to improve needle placement relative to a desired target structure. We first evaluate ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.04026</link><description>&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#21363;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65306;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#39640;&#24230;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#20294;&#39564;&#35777;&#31574;&#30053;&#34892;&#20026;&#30340;&#38590;&#24230;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20998;&#26512;&#23433;&#20840;&#32500;&#25252;&#30340;&#31616;&#21333;&#20219;&#21153;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23558;&#20540;&#20989;&#25968;&#19982;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30456;&#32852;&#31995;&#30340;&#21407;&#22987;&#23450;&#29702;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#23398;&#20064;&#30340;&#23454;&#38469;&#23454;&#26045;&#32454;&#33410;&#12290;&#38500;&#20102;&#25552;&#20986;&#35777;&#20070;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;RL&#31574;&#30053;&#35299;&#38145;&#20102;&#20016;&#23500;&#30340;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#22312;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#26102;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#23581;&#35797;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65292;&#21363;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#36880;&#20010;&#26465;&#30446;&#30340;&#31934;&#30830;&#38169;&#35823;&#30028;&#38480;&#65292;&#36825;&#26159;&#22312;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#20013;&#39318;&#27425;&#32435;&#20837;&#22122;&#22768;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.03372</link><description>&lt;p&gt;
&#22312;&#32447;&#24352;&#37327;&#23398;&#20064;&#65306;&#35745;&#31639;&#21644;&#32479;&#35745;&#26435;&#34913;&#65292;&#36866;&#24212;&#24615;&#21644;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret. (arXiv:2306.03372v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#22312;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#26102;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#23581;&#35797;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65292;&#21363;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#36880;&#20010;&#26465;&#30446;&#30340;&#31934;&#30830;&#38169;&#35823;&#30028;&#38480;&#65292;&#36825;&#26159;&#22312;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#20013;&#39318;&#27425;&#32435;&#20837;&#22122;&#22768;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65306;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#25152;&#26377;&#24212;&#29992;&#31243;&#24207;&#20013;&#37117;&#21487;&#20197;&#26681;&#25454;&#36866;&#24403;&#30340;&#26465;&#20214;&#32447;&#24615;&#25910;&#25947;&#24182;&#24674;&#22797;&#20302;&#31209;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#24314;&#31435;&#20102;&#31934;&#30830;&#30340;&#36880;&#20010;&#26465;&#30446;&#38169;&#35823;&#30028;&#38480;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20195;&#34920;&#20102;&#39318;&#27425;&#23581;&#35797;&#22312;&#22312;&#32447;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#20219;&#21153;&#20013;&#32435;&#20837;&#22122;&#22768;&#30340;&#21162;&#21147;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;&#22686;&#21152;&#27493;&#38271;&#21487;&#20197;&#21152;&#24555;&#25910;&#25947;&#65292;&#20294;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#32479;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate a generalized framework for estimating latent low-rank tensors in an online setting, encompassing both linear and generalized linear models. This framework offers a flexible approach for handling continuous or categorical variables. Additionally, we investigate two specific applications: online tensor completion and online binary tensor learning. To address these challenges, we propose the online Riemannian gradient descent algorithm, which demonstrates linear convergence and the ability to recover the low-rank component under appropriate conditions in all applications. Furthermore, we establish a precise entry-wise error bound for online tensor completion. Notably, our work represents the first attempt to incorporate noise in the online low-rank tensor recovery task. Intriguingly, we observe a surprising trade-off between computational and statistical aspects in the presence of noise. Increasing the step size accelerates convergence but leads to higher statistical error
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Polyak-{\L}ojasiewicz&#26465;&#20214;&#30340;&#21452;&#23618;&#23398;&#20064;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;&#65292;&#21363;GALET&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;$\tilde{\cal O}(\epsilon^{-1})$&#36845;&#20195;&#27425;&#25968;&#20869;&#23454;&#29616;&#38382;&#39064;&#30340;$\epsilon$-&#38745;&#24577;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.02422</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Polyak-{\L}ojasiewicz&#26465;&#20214;&#30340;&#21452;&#23618;&#23398;&#20064;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Generalized Alternating Method for Bilevel Learning under the Polyak-{\L}ojasiewicz Condition. (arXiv:2306.02422v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Polyak-{\L}ojasiewicz&#26465;&#20214;&#30340;&#21452;&#23618;&#23398;&#20064;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;&#65292;&#21363;GALET&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;$\tilde{\cal O}(\epsilon^{-1})$&#36845;&#20195;&#27425;&#25968;&#20869;&#23454;&#29616;&#38382;&#39064;&#30340;$\epsilon$-&#38745;&#24577;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#36817;&#24180;&#26469;&#22240;&#20854;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#26032;&#20852;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#32780;&#37325;&#26032;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20110;&#20855;&#26377;&#24378;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#38382;&#39064;&#65292;&#31616;&#21333;&#30340;&#20132;&#26367;&#65288;&#38544;&#24335;&#65289;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#21333;&#23618;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36229;&#20986;&#27492;&#22522;&#26412;&#35774;&#32622;&#30340;&#21452;&#23618;&#38382;&#39064;&#65292;&#23578;&#19981;&#28165;&#26970;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#35813;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28385;&#36275;Polyak-{\L}ojasiewicz (PL)&#26465;&#20214;&#30340;&#38750;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#20248;&#21270;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;&#65288;GALET&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#25152;&#32771;&#34385;&#30340;&#21452;&#23618;&#38382;&#39064;&#30340;&#19968;&#20010;&#38745;&#24577;&#24230;&#37327;&#65292;&#23427;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;&#24230;&#37327;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;GALET&#22312;$\tilde{\cal O}(\epsilon^{-1})$&#36845;&#20195;&#27425;&#25968;&#20869;&#23454;&#29616;&#20102;&#25152;&#32771;&#34385;&#38382;&#39064;&#30340;$\epsilon$-&#38745;&#24577;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization has recently regained interest owing to its applications in emerging machine learning fields such as hyperparameter optimization, meta-learning, and reinforcement learning. Recent results have shown that simple alternating (implicit) gradient-based algorithms can achieve the same convergence rate of single-level gradient descent (GD) for bilevel problems with a strongly convex lower-level objective. However, it remains unclear whether this result can be generalized to bilevel problems beyond this basic setting. In this paper, we propose a Generalized ALternating mEthod for bilevel opTimization (GALET) with a nonconvex lower-level objective that satisfies the Polyak-{\L}ojasiewicz (PL) condition. We first introduce a stationary metric for the considered bilevel problems, which generalizes the existing metric. We then establish that GALET achieves an $\epsilon$-stationary metric for the considered problem within $\tilde{\cal O}(\epsilon^{-1})$ iterations, which match
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#23376;&#32676;&#20043;&#38388;&#23384;&#22312;&#30528;&#19981;&#21516;&#30340;ICU&#21644;&#21307;&#38498;&#27515;&#20129;&#29575;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.02121</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#31639;&#27861;&#35782;&#21035;ICU&#24739;&#32773;&#23376;&#32676;
&lt;/p&gt;
&lt;p&gt;
Identifying Subgroups of ICU Patients Using End-to-End Multivariate Time-Series Clustering Algorithm Based on Real-World Vital Signs Data. (arXiv:2306.02121v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#23376;&#32676;&#20043;&#38388;&#23384;&#22312;&#30528;&#19981;&#21516;&#30340;ICU&#21644;&#21307;&#38498;&#27515;&#20129;&#29575;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;MIMIC-IV&#25968;&#25454;&#24211;&#20316;&#20026;&#25968;&#25454;&#28304;&#65292;&#30740;&#31350;&#20102;&#21160;&#24577;&#12289;&#39640;&#39057;&#29575;&#12289;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#22312;ICU&#36887;&#30041;&#26399;&#38388;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20307;&#28201;&#12289;&#24515;&#29575;&#12289;&#24179;&#22343;&#34880;&#21387;&#12289;&#21628;&#21560;&#39057;&#29575;&#21644;SpO2&#12290;&#27604;&#36739;&#20102;&#21508;&#31181;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#36873;&#25321;&#20102;&#19968;&#31181;&#21517;&#20026;Time2Feat&#30340;&#31471;&#21040;&#31471;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#31995;&#32479;&#32467;&#21512;K-Means&#20316;&#20026;&#26368;&#26377;&#25928;&#30340;ICU&#24739;&#32773;&#32858;&#31867;&#26041;&#27861;&#12290;&#22312;&#32858;&#31867;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#20102;2008&#24180;&#33267;2016&#24180;&#26399;&#38388;&#25910;&#27835;&#30340;8,080&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24320;&#21457;&#65292;&#20351;&#29992;&#20102;2017&#24180;&#33267;2019&#24180;&#26399;&#38388;&#25910;&#27835;&#30340;2,038&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#39564;&#35777;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#20020;&#24202;&#27515;&#20129;&#39044;&#21518;&#24046;&#24322;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#20122;&#32452;&#20043;&#38388;ICU&#27515;&#20129;&#29575;&#21644;&#21307;&#38498;&#27515;&#20129;&#29575;&#30340;&#39118;&#38505;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#21487;&#35270;&#21270;&#20102;&#29983;&#21629;&#20307;&#24449;&#21464;&#21270;&#30340;&#36712;&#36857;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#20026;ICU&#24739;&#32773;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study employed the MIMIC-IV database as data source to investigate the use of dynamic, high-frequency, multivariate time-series vital signs data, including temperature, heart rate, mean blood pressure, respiratory rate, and SpO2, monitored first 8 hours data in the ICU stay. Various clustering algorithms were compared, and an end-to-end multivariate time series clustering system called Time2Feat, combined with K-Means, was chosen as the most effective method to cluster patients in the ICU. In clustering analysis, data of 8,080 patients admitted between 2008 and 2016 was used for model development and 2,038 patients admitted between 2017 and 2019 for model validation. By analyzing the differences in clinical mortality prognosis among different categories, varying risks of ICU mortality and hospital mortality were found between different subgroups. Furthermore, the study visualized the trajectory of vital signs changes. The findings of this study provide valuable insights into the p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;aggVAE&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21644;MCMC&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#21464;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26144;&#23556;&#20197;&#21439;&#20026;&#23618;&#32423;&#30340;&#32858;&#21512;&#32423;&#21035;&#25968;&#25454;&#65292;&#24182;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#30340;&#21464;&#21270;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19779</link><description>&lt;p&gt;
&#21033;&#29992;aggVAE&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21644;MCMC&#20197;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#21464;&#21270;&#65306;&#20197;&#32943;&#23612;&#20122;&#30340;&#30111;&#30142;&#24739;&#30149;&#29575;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya. (arXiv:2305.19779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;aggVAE&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21644;MCMC&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#21464;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26144;&#23556;&#20197;&#21439;&#20026;&#23618;&#32423;&#30340;&#32858;&#21512;&#32423;&#21035;&#25968;&#25454;&#65292;&#24182;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#30340;&#21464;&#21270;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#30142;&#30149;&#26144;&#23556;&#26159;&#20844;&#20849;&#21355;&#29983;&#21644;&#30142;&#30149;&#30417;&#27979;&#20013;&#22522;&#26412;&#30340;&#25919;&#31574;&#20449;&#24687;&#24037;&#20855;&#65292;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#24403;&#22788;&#29702;&#21306;&#22495;&#25968;&#25454;&#65292;&#22914;&#34892;&#25919;&#21306;&#21010;&#21333;&#20301;&#65288;&#20363;&#22914;&#21439;&#25110;&#30465;&#65289;&#30340;&#32858;&#21512;&#25968;&#25454;&#26102;&#65292;&#24120;&#29992;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#21306;&#22495;&#21333;&#20803;&#30340;&#30456;&#37051;&#32467;&#26500;&#20197;&#32771;&#34385;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#30142;&#30149;&#30417;&#27979;&#31995;&#32479;&#30340;&#30446;&#26631;&#26159;&#38543;&#26102;&#38388;&#36319;&#36394;&#30142;&#30149;&#32467;&#26524;&#65292;&#20294;&#22312;&#21361;&#26426;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#25919;&#27835;&#21464;&#21270;&#23548;&#33268;&#34892;&#25919;&#36793;&#30028;&#26356;&#25913;&#65289;&#65292;&#36825;&#23558;&#24102;&#26469;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#23454;&#29992;&#21644;&#26131;&#20110;&#23454;&#26045;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20381;&#36182;&#20110;&#32452;&#21512;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#21644;&#20840;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#29616;&#26377;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE) &#24037;&#20316;&#19978;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#32858;&#21512;VAE(aggVAE)&#20307;&#31995;&#32467;&#26500;&#21487;&#29992;&#20110;&#22312;&#20197;&#21439;&#20026;&#23618;&#32423;&#30340;&#32858;&#21512;&#32423;&#21035;&#22788;&#29702;&#25968;&#25454;&#65292;&#20197;&#26144;&#23556;&#32943;&#23612;&#20122;&#30340;&#30111;&#30142;&#24739;&#30149;&#29575;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#36830;&#32493;&#30340;&#26041;&#24335;&#32771;&#34385;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30456;&#37051;&#24615;&#20551;&#35774;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#30340;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#30111;&#30142;&#24739;&#30149;&#29575;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based disease mapping remains a fundamental policy-informing tool in public health and disease surveillance with hierarchical Bayesian models being the current state-of-the-art approach. When working with areal data, e.g. aggregates at the administrative unit level such as district or province, routinely used models rely on the adjacency structure of areal units to account for spatial correlations. The goal of disease surveillance systems is to track disease outcomes over time, but this provides challenging in situations of crises, such as political changes, leading to changes of administrative boundaries. Kenya is an example of such country. Moreover, adjacency-based approach ignores the continuous nature of spatial processes and cannot solve the change-of-support problem, i.e. when administrative boundaries change. We present a novel, practical, and easy to implement solution relying on a methodology combining deep generative modelling and fully Bayesian inference. We build on 
&lt;/p&gt;</description></item><item><title>DreamWaltz&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#23548;&#21644;&#21442;&#25968;&#21270;&#30340;&#20154;&#20307;&#20808;&#39564;&#29983;&#25104;&#21644;&#21160;&#30011;&#21270;&#22797;&#26434;&#30340;3D&#35282;&#33394;&#12290;&#23427;&#25552;&#20986;&#20102;&#19977;&#32500;&#19968;&#33268;&#30340;&#36974;&#25377;&#24863;&#30693;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#26469;&#20248;&#21270;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#35268;&#33539;&#23039;&#21183;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#21160;&#30011;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#35282;&#33394;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#21019;&#24314;3D&#35282;&#33394;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12529</link><description>&lt;p&gt;
DreamWaltz&#65306;&#20351;&#29992;&#22797;&#26434;3D&#21160;&#30011;&#35282;&#33394;&#21046;&#20316;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
DreamWaltz: Make a Scene with Complex 3D Animatable Avatars. (arXiv:2305.12529v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12529
&lt;/p&gt;
&lt;p&gt;
DreamWaltz&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#23548;&#21644;&#21442;&#25968;&#21270;&#30340;&#20154;&#20307;&#20808;&#39564;&#29983;&#25104;&#21644;&#21160;&#30011;&#21270;&#22797;&#26434;&#30340;3D&#35282;&#33394;&#12290;&#23427;&#25552;&#20986;&#20102;&#19977;&#32500;&#19968;&#33268;&#30340;&#36974;&#25377;&#24863;&#30693;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#26469;&#20248;&#21270;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#35268;&#33539;&#23039;&#21183;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#21160;&#30011;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#35282;&#33394;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#21019;&#24314;3D&#35282;&#33394;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DreamWaltz&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#23548;&#21644;&#21442;&#25968;&#21270;&#30340;&#20154;&#20307;&#20808;&#39564;&#29983;&#25104;&#21644;&#21160;&#30011;&#21270;&#22797;&#26434;&#30340;3D&#35282;&#33394;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#26041;&#27861;&#22312;&#23558;&#25991;&#26412;&#36716;&#21270;&#20026;&#26222;&#36890;&#29289;&#20307;&#30340;3D&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#21019;&#24314;&#39640;&#36136;&#37327;&#19988;&#21487;&#21160;&#30011;&#30340;3D&#35282;&#33394;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;3D&#35282;&#33394;&#65292;DreamWaltz&#25552;&#20986;&#20102;&#19977;&#32500;&#19968;&#33268;&#30340;&#36974;&#25377;&#24863;&#30693;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#26469;&#20248;&#21270;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#35268;&#33539;&#23039;&#21183;&#12290;&#23427;&#36890;&#36807;&#19977;&#32500;&#24863;&#30693;&#30340;&#39592;&#26550;&#35843;&#33410;&#25552;&#20379;&#35270;&#35282;&#23545;&#40784;&#30340;&#30417;&#30563;&#65292;&#20351;&#24471;&#22797;&#26434;&#30340;&#35282;&#33394;&#29983;&#25104;&#19981;&#20250;&#20135;&#29983;&#20266;&#24433;&#21644;&#22810;&#20010;&#38754;&#12290;&#23545;&#20110;&#21160;&#30011;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#21160;&#30011;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#35282;&#33394;&#34920;&#31034;&#65292;&#21487;&#20197;&#23558;&#20219;&#24847;&#23039;&#21183;&#26144;&#23556;&#21040;&#35268;&#33539;&#23039;&#21183;&#34920;&#31034;&#20013;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DreamWaltz&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#31283;&#20581;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#20855;&#26377;&#22797;&#26434;&#24418;&#29366;&#21644;&#22806;&#35266;&#20197;&#21450;&#21160;&#30011;&#30340;3D&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DreamWaltz, a novel framework for generating and animating complex 3D avatars given text guidance and parametric human body prior. While recent methods have shown encouraging results for text-to-3D generation of common objects, creating high-quality and animatable 3D avatars remains challenging. To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural representations with canonical poses. It provides view-aligned supervision via 3D-aware skeleton conditioning which enables complex avatar generation without artifacts and multiple faces. For animation, our method learns an animatable and generalizable avatar representation which could map arbitrary poses to the canonical pose representation. Extensive evaluations demonstrate that DreamWaltz is an effective and robust approach for creating 3D avatars that can take on complex shapes and appearances as well as novel poses for animation. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.04003</link><description>&lt;p&gt;
ANTONIO:&#38754;&#21521;NLP&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39564;&#35777;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;NLP&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#19968;&#38382;&#39064;&#30340;&#25216;&#26415;&#21407;&#22240;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#23558;NLP&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20934;&#22791;&#20026;&#36866;&#21512;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#30340;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20026;&#19968;&#20010;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#35813;&#24211;&#36830;&#25509;&#21040;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;ERAN&#21644;Marabou&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;R-U-A-Robot&#30340;NLP&#25968;&#25454;&#38598;&#23545;&#24037;&#20855;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;&#25552;&#35758;&#20316;&#20026;&#39564;&#35777;&#20855;&#26377;&#27861;&#24459;&#37325;&#35201;&#24615;&#30340;NLP&#24212;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24076;&#26395;&#65292;&#30001;&#20110;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLMs&#32534;&#31243;&#21512;&#25104;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#32479;&#19968;&#27169;&#22411;&#26550;&#26500;&#12289;&#23398;&#20064;&#26041;&#27861;&#12289;&#22635;&#20805;&#37319;&#26679;&#21644;&#25968;&#25454;&#20998;&#24067;&#65292;&#26469;&#25552;&#39640;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.02309</link><description>&lt;p&gt;
CodeGen2&#65306;&#32534;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;LLM&#35757;&#32451;&#30340;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
CodeGen2: Lessons for Training LLMs on Programming and Natural Languages. (arXiv:2305.02309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLMs&#32534;&#31243;&#21512;&#25104;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#32479;&#19968;&#27169;&#22411;&#26550;&#26500;&#12289;&#23398;&#20064;&#26041;&#27861;&#12289;&#22635;&#20805;&#37319;&#26679;&#21644;&#25968;&#25454;&#20998;&#24067;&#65292;&#26469;&#25552;&#39640;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#21512;&#25104;&#21644;&#29702;&#35299;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#36136;&#37327;&#20284;&#20046;&#30001;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#20316;&#20026;&#27169;&#22411;&#21442;&#25968;&#21644;&#35266;&#23519;&#20540;&#30340;&#20989;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#36890;&#36807;&#21487;&#29992;&#25968;&#25454;&#21644;&#35745;&#31639;&#37327;&#30340;&#25968;&#37327;&#38480;&#21046;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#32479;&#19968;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#20351;LLMs&#30340;&#31243;&#24207;&#21512;&#25104;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#65306;&#65288;1&#65289;&#27169;&#22411;&#26550;&#26500;&#65292;&#65288;2&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#65288;3&#65289;&#22635;&#20805;&#37319;&#26679;&#21644;&#65288;4&#65289;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.  In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a "free lunch" hypothesis. For data distributions, the eff
&lt;/p&gt;</description></item><item><title>DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2304.14108</link><description>&lt;p&gt;
DataComp&#65306;&#23547;&#25214;&#19979;&#19968;&#20195;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14108
&lt;/p&gt;
&lt;p&gt;
DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22312;&#36817;&#26399;&#30340;&#31361;&#30772;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#27604;&#22914;CLIP&#12289;Stable Diffusion&#21644;GPT-4&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25968;&#25454;&#38598;&#24456;&#23569;&#24471;&#21040;&#19982;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31639;&#27861;&#21516;&#31561;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataComp&#65292;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#35757;&#32451;&#20195;&#30721;&#26159;&#22266;&#23450;&#30340;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Common Crawl&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#20854;&#20013;&#21253;&#21547;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#12290;&#21442;&#21152;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#65292;&#24182;&#36890;&#36807;&#36816;&#34892;&#25105;&#20204;&#26631;&#20934;&#21270;&#30340;CLIP&#35757;&#32451;&#20195;&#30721;&#24182;&#22312;38&#20010;&#19979;&#28216;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#22810;&#20010;&#35268;&#27169;&#65292;&#22235;&#20010;&#20505;&#36873;&#27744;&#22823;&#23567;&#21644;&#30456;&#24212;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#28085;&#30422;&#20102;&#20174;12.8M&#21040;12.8B&#20010;&#26679;&#26412;&#12290;&#36825;&#31181;&#22810;&#35268;&#27169;&#35774;&#35745;&#26377;&#21161;&#20110;&#30740;&#31350;&#35268;&#27169;&#36235;&#21183;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#36873;&#25321;&#20313;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.10701</link><description>&lt;p&gt;
&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#22686;&#24378;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#24182;&#20445;&#25252;&#25968;&#25454;&#29305;&#24615;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21028;&#21035;&#27169;&#22411;&#19978;&#65292;&#24573;&#30053;&#20102;&#26368;&#36817;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#21028;&#21035;&#27169;&#22411;&#31867;&#20284;&#65292;&#38656;&#35201;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#25968;&#25454;&#36129;&#29486;&#30340;&#32039;&#36843;&#38656;&#27714;&#20063;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21028;&#21035;&#27169;&#22411;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#22312;&#23454;&#38469;&#20013;&#30452;&#25509;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#36817;&#26399;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#35282;&#24230;&#23545;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20272;&#20540;&#38382;&#39064;&#36827;&#34892;&#20102;&#26500;&#24314;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;Generative Model Valuator&#8221;&#65288;GMValuator&#65289;&#8212;&#8212;&#31532;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#25968;&#25454;&#20272;&#20540;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#23454;&#20363;&#21450;&#30001;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#24212;&#21512;&#25104;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#20272;&#35745;&#21407;&#22987;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20026;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#21253;&#25324;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#35780;&#20272;&#25968;&#25454;&#23454;&#20363;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for gener
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#21270;Laplace&#36924;&#36817;(LLA)&#22312;Bayesian optimization&#20013;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;LLA&#22312;&#26500;&#24314;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26102;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#20294;&#26159;&#22312;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#38656;&#35201;&#32771;&#34385;&#20854;&#21487;&#33021;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.08309</link><description>&lt;p&gt;
Bayesian Optimization&#20013;&#32447;&#24615;&#21270;Laplace&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization. (arXiv:2304.08309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#21270;Laplace&#36924;&#36817;(LLA)&#22312;Bayesian optimization&#20013;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;LLA&#22312;&#26500;&#24314;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26102;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#20294;&#26159;&#22312;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#38656;&#35201;&#32771;&#34385;&#20854;&#21487;&#33021;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#21270;Laplace&#36924;&#36817;(LLA)&#24050;&#34987;&#35777;&#26126;&#22312;&#26500;&#24314;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26102;&#26377;&#25928;&#19988;&#39640;&#25928;&#12290;&#23427;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#20855;&#26377;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#30340;&#26368;&#22823;&#21518;&#39564;&#39044;&#27979;&#20989;&#25968;&#26368;&#22823;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24179;&#22343;&#20989;&#25968;&#65292;&#24182;&#19988;&#30001;&#32463;&#39564;&#31070;&#32463;&#26354;&#38754;&#26680;&#35825;&#23548;&#30340;&#21327;&#26041;&#24046;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24050;&#32463;&#30740;&#31350;&#36807;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#22823;&#35268;&#27169;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#35832;&#22914;Bayesian optimization&#36825;&#26679;&#30340;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#20013;&#23578;&#26410;&#23545;&#20854;&#36827;&#34892;&#30740;&#31350;&#65292;&#20854;&#20013;&#39640;&#26031;&#36807;&#31243;&#26159;&#40664;&#35748;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20855;&#26377;&#31616;&#21333;&#30340;&#24179;&#22343;&#20989;&#25968;&#21644;&#26680;&#20989;&#25968;&#65292;&#20363;&#22914;&#24452;&#21521;&#22522;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLA&#22312;Bayesian optimization&#20013;&#30340;&#26377;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#24378;&#35843;&#20854;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21487;&#33021;&#20986;&#29616;&#30340;&#19968;&#20123;&#38382;&#39064;&#21644;&#19968;&#20010;LLA&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21363;&#24403;&#25628;&#32034;&#31354;&#38388;&#26159;&#26080;&#30028;&#30340;&#26102;&#20505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The linearized-Laplace approximation (LLA) has been shown to be effective and efficient in constructing Bayesian neural networks. It is theoretically compelling since it can be seen as a Gaussian process posterior with the mean function given by the neural network's maximum-a-posteriori predictive function and the covariance function induced by the empirical neural tangent kernel. However, while its efficacy has been studied in large-scale tasks like image classification, it has not been studied in sequential decision-making problems like Bayesian optimization where Gaussian processes -- with simple mean functions and kernels such as the radial basis function -- are the de-facto surrogate models. In this work, we study the usefulness of the LLA in Bayesian optimization and highlight its strong performance and flexibility. However, we also present some pitfalls that might arise and a potential problem with the LLA when the search space is unbounded.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;&#65292;&#38024;&#23545;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#26500;&#36896;&#20102;&#19968;&#31181;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2304.08172</link><description>&lt;p&gt;
&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pointwise convergence theorem of gradient descent in sparse deep neural network. (arXiv:2304.08172v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26799;&#24230;&#19979;&#38477;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#23450;&#29702;&#65292;&#38024;&#23545;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#26500;&#36896;&#20102;&#19968;&#31181;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#29702;&#35770;&#32467;&#26500;&#36880;&#28176;&#24471;&#21040;&#20102;&#38416;&#26126;&#12290;Imaizumi-Fukumizu&#65288;2019&#65289;&#21644;Suzuki&#65288;2019&#65289;&#25351;&#20986;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#20026;&#38750;&#20809;&#28369;&#20989;&#25968;&#26102;&#65292;DNN&#30340;&#23398;&#20064;&#33021;&#21147;&#20248;&#20110;&#20808;&#21069;&#30340;&#29702;&#35770;&#12290;&#28982;&#32780;&#65292;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#20247;&#22810;&#30740;&#31350;&#23581;&#35797;&#22312;&#27809;&#26377;&#20219;&#20309;&#32479;&#35745;&#35770;&#35777;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25968;&#23398;&#30740;&#31350;&#65292;&#25506;&#31350;&#30495;&#27491;&#33021;&#22815;&#24341;&#21457;&#26799;&#24230;&#19979;&#38477;&#30340;DNN&#26550;&#26500;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#24615;&#65292;&#36825;&#19968;&#23581;&#35797;&#20284;&#20046;&#26356;&#36148;&#36817;&#23454;&#38469;DNN&#12290;&#26412;&#25991;&#23558;&#30446;&#26631;&#20989;&#25968;&#38480;&#21046;&#20026;&#38750;&#20809;&#28369;&#25351;&#31034;&#20989;&#25968;&#65292;&#24182;&#22312;ReLU-DNN&#20013;&#26500;&#36896;&#20102;&#19968;&#20010;&#31232;&#30095;&#19988;&#20855;&#26377;&#29305;&#27530;&#24418;&#29366;&#30340;DNN&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theoretical structure of deep neural network (DNN) has been clarified gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the learning ability of DNN is superior to the previous theories when the target function is non-smooth functions. However, as far as the author is aware, none of the numerous works to date attempted to mathematically investigate what kind of DNN architectures really induce pointwise convergence of gradient descent (without any statistical argument), and this attempt seems to be closer to the practical DNNs. In this paper we restrict target functions to non-smooth indicator functions, and construct a deep neural network inducing pointwise convergence provided by gradient descent process in ReLU-DNN. The DNN has a sparse and a special shape, with certain variable transformations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.06848</link><description>&lt;p&gt;
CAR-DESPOT: &#38024;&#23545;&#28151;&#26434;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#24037;&#20316;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#32771;&#34385;&#38543;&#26426;&#34892;&#20026;&#30340;&#21487;&#33021;&#32467;&#26524;&#65292;&#24182;&#26681;&#25454;&#30495;&#23454;&#30340;&#19990;&#30028;&#29366;&#24577;&#30340;&#37096;&#20998;&#35266;&#23519;&#36827;&#34892;&#20915;&#31574;&#12290;&#22240;&#26524;&#28151;&#28102;&#30340;&#38382;&#39064;&#26159;&#36827;&#34892;&#20934;&#30830;&#21644;&#24378;&#20581;&#30340;&#34892;&#20026;&#39044;&#27979;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#36825;&#20123;&#38543;&#26426;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26126;&#30830;&#30340;&#22240;&#26524;&#35821;&#20041;&#65292;POMDP&#35268;&#21010;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#28151;&#28102;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#34920;&#29616;&#19981;&#20339;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;CAR-DESPOT&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;POMDP&#35268;&#21010;&#31243;&#24207;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots operating in real-world environments must reason about possible outcomes of stochastic actions and make decisions based on partial observations of the true world state. A major challenge for making accurate and robust action predictions is the problem of confounding, which if left untreated can lead to prediction errors. The partially observable Markov decision process (POMDP) is a widely-used framework to model these stochastic and partially-observable decision-making problems. However, due to a lack of explicit causal semantics, POMDP planning methods are prone to confounding bias and thus in the presence of unobserved confounders may produce underperforming policies. This paper presents a novel causally-informed extension of "anytime regularized determinized sparse partially observable tree" (AR-DESPOT), a modern anytime online POMDP planner, using causal modelling and inference to eliminate errors caused by unmeasured confounder variables. We further propose a method to lear
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.02819</link><description>&lt;p&gt;
GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#25512;&#24191;&#24102;&#26469;&#20102;&#25968;&#23383;&#36890;&#20449;&#26041;&#38754;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#26469;&#21306;&#20998;AI&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#33521;&#35821;&#27597;&#35821;&#21644;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#30340;&#20889;&#20316;&#26679;&#26412;&#35780;&#20272;&#20102;&#20960;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;GPT&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#25345;&#32493;&#23558;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20889;&#20316;&#26679;&#26412;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#32780;&#21407;&#29983;&#20889;&#20316;&#26679;&#26412;&#21017;&#33021;&#22815;&#34987;&#20934;&#30830;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#26080;&#24847;&#20013;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21628;&#21505;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
&lt;/p&gt;</description></item><item><title>DeepGD&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.04878</link><description>&lt;p&gt;
DeepGD: &#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks. (arXiv:2303.04878v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04878
&lt;/p&gt;
&lt;p&gt;
DeepGD&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36755;&#20837;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#65292;&#27979;&#35797;DNN&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#27979;&#35797;DNN&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#29983;&#25104;&#25110;&#25506;&#32034;&#22823;&#35268;&#27169;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;DNN&#27979;&#35797;&#31070;&#35861;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#24037;&#20316;&#26469;&#26631;&#35760;&#27979;&#35797;&#25968;&#25454;&#65292;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#19987;&#23478;&#20197;&#30830;&#20445;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepGD&#65292;&#19968;&#31181;&#29992;&#20110;DNN&#27169;&#22411;&#30340;&#40657;&#30418;&#22810;&#30446;&#26631;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. Particularly, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this paper, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26631;&#31614;&#25918;&#32622;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#21487;&#35270;&#21270;&#20013;&#35299;&#20915;&#20102;&#26631;&#31614;&#37325;&#21472;&#21644;&#21487;&#35835;&#24615;&#38382;&#39064;&#65292;&#19982;&#29616;&#26377;&#30340;&#25163;&#24037;&#35774;&#35745;&#31639;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.01388</link><description>&lt;p&gt;
&#24378;&#21270;&#26631;&#31614;: &#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#28857;&#29305;&#24449;&#26631;&#31614;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Reinforced Labels: Multi-Agent Deep Reinforcement Learning for Point-Feature Label Placement. (arXiv:2303.01388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26631;&#31614;&#25918;&#32622;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#21487;&#35270;&#21270;&#20013;&#35299;&#20915;&#20102;&#26631;&#31614;&#37325;&#21472;&#21644;&#21487;&#35835;&#24615;&#38382;&#39064;&#65292;&#19982;&#29616;&#26377;&#30340;&#25163;&#24037;&#35774;&#35745;&#31639;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#20010;&#39046;&#22495;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#37329;&#34701;&#31561;&#12290;&#26412;&#25991;&#23558;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#26631;&#31614;&#25918;&#32622;&#65292;&#36825;&#26159;&#25968;&#25454;&#21487;&#35270;&#21270;&#20013;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20339;&#20301;&#32622;&#20197;&#36991;&#20813;&#37325;&#21472;&#24182;&#30830;&#20445;&#21487;&#35835;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#29305;&#24449;&#26631;&#31614;&#25918;&#32622;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(MADRL)&#26469;&#23398;&#20064;&#26631;&#31614;&#25918;&#32622;&#31574;&#30053;&#65292;&#36825;&#26159;&#19982;&#29616;&#26377;&#25163;&#24037;&#35774;&#35745;&#30340;&#31639;&#27861;&#30456;&#23545;&#24212;&#30340;&#31532;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26631;&#31614;&#25918;&#32622;&#26041;&#27861;&#12290;&#20026;&#20102;&#26041;&#20415;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29615;&#22659;&#65292;&#20854;&#20013;&#20195;&#29702;&#20316;&#20026;&#26631;&#31614;&#30340;&#20195;&#29702;&#65292;&#36825;&#20123;&#26631;&#31614;&#26159;&#19968;&#31181;&#22686;&#24378;&#21487;&#35270;&#21270;&#30340;&#30701;&#25991;&#26412;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#24471;&#21040;&#30340;&#31574;&#30053;&#26126;&#26174;&#20248;&#20110;&#26410;&#32463;&#35757;&#32451;&#30340;&#20195;&#29702;&#30340;&#38543;&#26426;&#31574;&#30053;&#20197;&#21450;&#30001;&#20154;&#31867;&#35774;&#35745;&#30340;&#27604;&#36739;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over recent years, Reinforcement Learning combined with Deep Learning techniques has successfully proven to solve complex problems in various domains, including robotics, self-driving cars, and finance. In this paper, we are introducing Reinforcement Learning (RL) to label placement, a complex task in data visualization that seeks optimal positioning for labels to avoid overlap and ensure legibility. Our novel point-feature label placement method utilizes Multi-Agent Deep Reinforcement Learning (MADRL) to learn the label placement strategy, which is the first machine-learning-driven labeling method in contrast to existing hand-crafted algorithms designed by human experts. To facilitate RL learning, we developed an environment where an agent acts as a proxy for a label, a short textual annotation that augments visualization. Our results show that the strategy trained by our method significantly outperforms the random strategy of an untrained agent and compared methods designed by human 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#25968;&#25299;&#25169;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;MALDI&#25968;&#25454;&#20013;&#33719;&#24471;&#20869;&#22312;&#20449;&#24687;&#24182;&#36716;&#21270;&#20026;&#21453;&#26144;&#25299;&#25169;&#25345;&#20037;&#24615;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#32954;&#30284;&#20122;&#22411;&#20998;&#31867;&#20013;&#30340;&#22122;&#38899;&#20449;&#21495;&#21306;&#20998;&#21644;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13948</link><description>&lt;p&gt;
&#30417;&#30563;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#22312;MALDI&#36136;&#35889;&#25104;&#20687;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Supervised topological data analysis for MALDI mass spectrometry imaging applications. (arXiv:2302.13948v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13948
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#25968;&#25299;&#25169;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;MALDI&#25968;&#25454;&#20013;&#33719;&#24471;&#20869;&#22312;&#20449;&#24687;&#24182;&#36716;&#21270;&#20026;&#21453;&#26144;&#25299;&#25169;&#25345;&#20037;&#24615;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#32954;&#30284;&#20122;&#22411;&#20998;&#31867;&#20013;&#30340;&#22122;&#38899;&#20449;&#21495;&#21306;&#20998;&#21644;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22522;&#36136;&#36741;&#21161;&#28608;&#20809;&#35299;&#21560;/&#30005;&#31163;&#36136;&#35889;&#25104;&#20687;&#65288;MALDI MSI&#65289;&#22312;&#30284;&#30151;&#30740;&#31350;&#20013;&#26174;&#31034;&#20986;&#37325;&#35201;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32959;&#30244;&#20998;&#22411;&#21644;&#20122;&#22411;&#20998;&#26512;&#20013;&#12290;&#32954;&#30284;&#26159;&#32959;&#30244;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#20854;&#20013;&#26368;&#33268;&#21629;&#30340;&#23454;&#20307;&#26159;&#33146;&#30284;&#65288;ADC&#65289;&#21644;&#40158;&#29366;&#32454;&#32990;&#30284;&#65288;SqCC&#65289;&#12290;&#21306;&#20998;&#36825;&#20004;&#31181;&#24120;&#35265;&#20122;&#22411;&#23545;&#20110;&#27835;&#30103;&#20915;&#31574;&#21644;&#24739;&#32773;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#25968;&#25299;&#25169;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;MALDI&#25968;&#25454;&#20013;&#33719;&#21462;&#20869;&#22312;&#20449;&#24687;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#21453;&#26144;&#25299;&#25169;&#25345;&#20037;&#24615;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#25299;&#25169;&#25345;&#20037;&#24615;&#26377;&#21161;&#20110;&#21306;&#20998;&#20449;&#21495;&#21644;&#22122;&#38899;&#12290;&#20854;&#27425;&#65292;&#23427;&#21387;&#32553;&#20102;MALDI&#25968;&#25454;&#65292;&#33410;&#30465;&#20102;&#23384;&#20648;&#31354;&#38388;&#65292;&#24182;&#20248;&#21270;&#20102;&#21518;&#32493;&#20998;&#31867;&#20219;&#21153;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#23454;&#29616;&#25105;&#20204;&#25299;&#25169;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#21333;&#19968;&#35843;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Matrix-assisted laser desorption/ionization mass spectrometry imaging (MALDI MSI) displays significant potential for applications in cancer research, especially in tumor typing and subtyping. Lung cancer is the primary cause of tumor-related deaths, where the most lethal entities are adenocarcinoma (ADC) and squamous cell carcinoma (SqCC). Distinguishing between these two common subtypes is crucial for therapy decisions and successful patient management.  Results: We propose a new algebraic topological framework, which obtains intrinsic information from MALDI data and transforms it to reflect topological persistence. Our framework offers two main advantages. Firstly, topological persistence aids in distinguishing the signal from noise. Secondly, it compresses the MALDI data, saving storage space and optimizes computational time for subsequent classification tasks. We present an algorithm that efficiently implements our topological framework, relying on a single tuning param
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23433;&#20840;&#38480;&#21046;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24378;&#35843;&#20102;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#21407;&#29702;&#22312;&#20855;&#26377;&#22810;&#38142;&#32467;&#26500;&#30340;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#36890;&#36807;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#34920;&#31034;&#20026;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13152</link><description>&lt;p&gt;
&#20851;&#20110;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#21407;&#29702;&#21644;&#23433;&#20840;&#38480;&#21046;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Bellman's principle of optimality and Reinforcement learning for safety-constrained Markov decision process. (arXiv:2302.13152v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23433;&#20840;&#38480;&#21046;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24378;&#35843;&#20102;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#21407;&#29702;&#22312;&#20855;&#26377;&#22810;&#38142;&#32467;&#26500;&#30340;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#36890;&#36807;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#34920;&#31034;&#20026;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23433;&#20840;&#38480;&#21046;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#26159;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#26412;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26377;&#38480;&#29366;&#24577;&#21644;&#26377;&#38480;&#21160;&#20316;&#30340;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#20915;&#31574;&#32773;&#30340;&#30446;&#26631;&#26159;&#22312;&#19968;&#23450;&#30340;&#27010;&#29575;&#20445;&#35777;&#19979;&#21040;&#36798;&#30446;&#26631;&#38598;&#21512;&#65292;&#21516;&#26102;&#36991;&#20813;&#36827;&#20837;&#19968;&#20010;&#19981;&#23433;&#20840;&#30340;&#38598;&#21512;&#12290;&#22240;&#27492;&#65292;&#20219;&#20309;&#25511;&#21046;&#31574;&#30053;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#37117;&#20250;&#34920;&#29616;&#20026;&#22810;&#38142;&#32467;&#26500;&#65292;&#22240;&#20026;&#26681;&#25454;&#23450;&#20041;&#65292;&#23384;&#22312;&#19968;&#20010;&#30446;&#26631;&#38598;&#21512;&#21644;&#19968;&#20010;&#19981;&#23433;&#20840;&#38598;&#21512;&#12290;&#20915;&#31574;&#32773;&#22312;&#23548;&#33322;&#21040;&#30446;&#26631;&#38598;&#21512;&#26102;&#36824;&#24517;&#39035;&#26159;&#26368;&#20248;&#30340;&#65288;&#22522;&#20110;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;&#65289;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24378;&#35843;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#21407;&#29702;&#22312;&#20855;&#26377;&#22810;&#38142;&#32467;&#26500;&#30340;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#65288;&#27491;&#22914;Haviv&#30340;&#21453;&#20363;&#25152;&#31034;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19978;&#36848;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#34920;&#31034;&#20026;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study optimality for the safety-constrained Markov decision process which is the underlying framework for safe reinforcement learning. Specifically, we consider a constrained Markov decision process (with finite states and finite actions) where the goal of the decision maker is to reach a target set while avoiding an unsafe set(s) with certain probabilistic guarantees. Therefore the underlying Markov chain for any control policy will be multichain since by definition there exists a target set and an unsafe set. The decision maker also has to be optimal (with respect to a cost function) while navigating to the target set. This gives rise to a multi-objective optimization problem. We highlight the fact that Bellman's principle of optimality may not hold for constrained Markov decision problems with an underlying multichain structure (as shown by the counterexample due to Haviv. We resolve the counterexample by formulating the aforementioned multi-objective optimization problem as a ze
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23436;&#20840;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#32447;&#24615;&#21270;&#31639;&#27861;&#12290;&#36890;&#36807;&#23558;&#30446;&#26631;&#20989;&#25968;&#30340;&#21487;&#24494;&#21644;&#19981;&#21487;&#24494;&#37096;&#20998;&#20998;&#24320;&#22788;&#29702;&#65292;&#20165;&#32447;&#24615;&#21270;&#24179;&#28369;&#37096;&#20998;&#65292;&#20174;&#32780;&#25512;&#24191;&#20102;Frank-Wolfe&#26041;&#27861;&#21644;Conditional Gradient Sliding&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#19968;&#31867;&#19981;&#21487;&#24494;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#22522;&#20110;&#19968;&#20010;&#26356;&#24378;&#30340;&#32447;&#24615;&#26368;&#23567;&#21270;&#39044;&#35328;&#23376;&#29256;&#26412;&#65292;&#21487;&#22312;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#20013;&#39640;&#25928;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#20984;&#21644;&#38750;&#20984;&#30446;&#26631;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#20998;&#26512;&#12290;&#21478;&#22806;&#65292;&#22312;&#20984;&#24773;&#20917;&#19979;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21152;&#36895;&#26041;&#27861;&#20197;&#25913;&#21892;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#35828;&#26126;&#24615;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.12808</link><description>&lt;p&gt;
&#32447;&#24615;&#21270;&#31639;&#27861;&#29992;&#20110;&#23436;&#20840;&#22797;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Linearization Algorithms for Fully Composite Optimization. (arXiv:2302.12808v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23436;&#20840;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#32447;&#24615;&#21270;&#31639;&#27861;&#12290;&#36890;&#36807;&#23558;&#30446;&#26631;&#20989;&#25968;&#30340;&#21487;&#24494;&#21644;&#19981;&#21487;&#24494;&#37096;&#20998;&#20998;&#24320;&#22788;&#29702;&#65292;&#20165;&#32447;&#24615;&#21270;&#24179;&#28369;&#37096;&#20998;&#65292;&#20174;&#32780;&#25512;&#24191;&#20102;Frank-Wolfe&#26041;&#27861;&#21644;Conditional Gradient Sliding&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#19968;&#31867;&#19981;&#21487;&#24494;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#22522;&#20110;&#19968;&#20010;&#26356;&#24378;&#30340;&#32447;&#24615;&#26368;&#23567;&#21270;&#39044;&#35328;&#23376;&#29256;&#26412;&#65292;&#21487;&#22312;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#20013;&#39640;&#25928;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#20984;&#21644;&#38750;&#20984;&#30446;&#26631;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#20998;&#26512;&#12290;&#21478;&#22806;&#65292;&#22312;&#20984;&#24773;&#20917;&#19979;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21152;&#36895;&#26041;&#27861;&#20197;&#25913;&#21892;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#35828;&#26126;&#24615;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#20915;&#20984;&#21644;&#32039;&#38598;&#21512;&#19978;&#30340;&#23436;&#20840;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#21035;&#22788;&#29702;&#21487;&#24494;&#21644;&#19981;&#21487;&#24494;&#32452;&#25104;&#37096;&#20998;&#30340;&#32467;&#26500;&#65292;&#20165;&#32447;&#24615;&#21270;&#24179;&#28369;&#37096;&#20998;&#65292;&#25552;&#20379;&#20102;&#32463;&#20856;Frank-Wolfe&#26041;&#27861;&#21644;Conditional Gradient Sliding&#31639;&#27861;&#30340;&#26032;&#30340;&#25512;&#24191;&#65292;&#36866;&#29992;&#20110;&#19968;&#31867;&#19981;&#21487;&#24494;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#26356;&#24378;&#30340;&#32447;&#24615;&#26368;&#23567;&#21270;&#39044;&#35328;&#23376;&#29256;&#26412;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#20013;&#39640;&#25928;&#23454;&#29616;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#26412;&#29256;&#26412;&#30340;&#20223;&#23556;&#19981;&#21464;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#20984;&#21644;&#38750;&#20984;&#30446;&#26631;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#22312;&#20984;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21152;&#36895;&#26041;&#27861;&#65292;&#22797;&#26434;&#24230;&#30456;&#24212;&#25552;&#39640;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35828;&#26126;&#24615;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies first-order algorithms for solving fully composite optimization problems over convex and compact sets. We leverage the structure of the objective by handling its differentiable and non-differentiable components separately, linearizing only the smooth parts. This provides us with new generalizations of the classical Frank-Wolfe method and the Conditional Gradient Sliding algorithm, that cater to a subclass of non-differentiable problems. Our algorithms rely on a stronger version of the linear minimization oracle, which can be efficiently implemented in several practical applications. We provide the basic version of our method with an affine-invariant analysis and prove global convergence rates for both convex and non-convex objectives. Furthermore, in the convex case, we propose an accelerated method with correspondingly improved complexity. Finally, we provide illustrative experiments to support our theoretical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35270;&#20026;&#26377;&#22122;&#22768;&#30340;&#19981;&#21160;&#28857;&#36845;&#20195;&#65292;&#20174;&#32780;&#20174;&#36825;&#19968;&#26694;&#26550;&#20013;&#25512;&#23548;&#20986;&#38544;&#31169;&#21644;&#25928;&#29992;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#27969;&#34892;&#30340;&#31169;&#26377;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#20998;&#26512;&#26032;&#30340;&#31169;&#26377;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#36890;&#29992;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#29992;&#20110;&#38598;&#20013;&#24335;&#12289;&#32852;&#21512;&#21644;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#26032;&#39062;&#31169;&#26377;ADMM&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#21644;&#23376;&#37319;&#26679;&#30340;&#38544;&#31169;&#25918;&#22823;&#24314;&#31435;&#20102;&#24378;&#38544;&#31169;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#26377;&#22122;&#22768;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#32447;&#24615;&#25910;&#25947;&#32467;&#26524;&#25552;&#20379;&#25928;&#29992;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.12559</link><description>&lt;p&gt;
&#20174;&#26377;&#22122;&#22768;&#30340;&#19981;&#21160;&#28857;&#36845;&#20195;&#21040;&#31169;&#26377;&#30340;ADMM&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#20013;&#24335;&#21644;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated Learning. (arXiv:2302.12559v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35270;&#20026;&#26377;&#22122;&#22768;&#30340;&#19981;&#21160;&#28857;&#36845;&#20195;&#65292;&#20174;&#32780;&#20174;&#36825;&#19968;&#26694;&#26550;&#20013;&#25512;&#23548;&#20986;&#38544;&#31169;&#21644;&#25928;&#29992;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#27969;&#34892;&#30340;&#31169;&#26377;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#20998;&#26512;&#26032;&#30340;&#31169;&#26377;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#36890;&#29992;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#29992;&#20110;&#38598;&#20013;&#24335;&#12289;&#32852;&#21512;&#21644;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#26032;&#39062;&#31169;&#26377;ADMM&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#21644;&#23376;&#37319;&#26679;&#30340;&#38544;&#31169;&#25918;&#22823;&#24314;&#31435;&#20102;&#24378;&#38544;&#31169;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#26377;&#22122;&#22768;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#32447;&#24615;&#25910;&#25947;&#32467;&#26524;&#25552;&#20379;&#25928;&#29992;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20316;&#20026;&#22122;&#22768;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#23454;&#20363;&#65292;&#20197;&#20415;&#20174;&#36825;&#20010;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#26694;&#26550;&#20013;&#24471;&#20986;&#38544;&#31169;&#21644;&#25928;&#29992;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#26032;&#30340;&#35270;&#35282;&#24674;&#22797;&#20102;&#27969;&#34892;&#30340;&#31169;&#26377;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#22914;DP-SGD&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#26032;&#30340;&#31169;&#26377;&#20248;&#21270;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#12290;&#19987;&#27880;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#36890;&#29992;&#26694;&#26550;&#25512;&#23548;&#20986;&#29992;&#20110;&#38598;&#20013;&#24335;&#12289;&#32852;&#21512;&#21644;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#26032;&#39062;&#31169;&#26377;ADMM&#31639;&#27861;&#12290;&#23545;&#20110;&#36825;&#19977;&#20010;&#31639;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#36845;&#20195;&#21644;&#23376;&#37319;&#26679;&#30340;&#38544;&#31169;&#25918;&#22823;&#24314;&#31435;&#20102;&#24378;&#38544;&#31169;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#26377;&#22122;&#22768;&#22266;&#23450;&#28857;&#36845;&#20195;&#30340;&#32447;&#24615;&#25910;&#25947;&#32467;&#26524;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#65292;&#25552;&#20379;&#25928;&#29992;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study differentially private (DP) machine learning algorithms as instances of noisy fixed-point iterations, in order to derive privacy and utility results from this well-studied framework. We show that this new perspective recovers popular private gradient-based methods like DP-SGD and provides a principled way to design and analyze new private optimization algorithms in a flexible manner. Focusing on the widely-used Alternating Directions Method of Multipliers (ADMM) method, we use our general framework to derive novel private ADMM algorithms for centralized, federated and fully decentralized learning. For these three algorithms, we establish strong privacy guarantees leveraging privacy amplification by iteration and by subsampling. Finally, we provide utility guarantees using a unified analysis that exploits a recent linear convergence result for noisy fixed-point iterations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#36817;&#20284;&#19981;&#21487;&#22788;&#29702;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#38469;&#26469;&#33719;&#24471;&#26435;&#37325;&#30340;&#28857;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2302.10975</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improved uncertainty quantification for neural networks with Bayesian last layer. (arXiv:2302.10975v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#36817;&#20284;&#19981;&#21487;&#22788;&#29702;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#38469;&#26469;&#33719;&#24471;&#26435;&#37325;&#30340;&#28857;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#26041;&#38754;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#26469;&#35828;&#26159;&#19968;&#20010;&#38480;&#21046;&#65292;&#22240;&#27492;&#24120;&#24120;&#20542;&#21521;&#20110;&#20351;&#29992;&#33021;&#22815;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#22914;&#39640;&#26031;&#36807;&#31243;&#25110;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#12290;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20551;&#35774;&#25152;&#26377;&#21442;&#25968;&#37117;&#26381;&#20174;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#20135;&#29983;&#20998;&#24067;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#25512;&#26029;&#36890;&#24120;&#26159;&#19981;&#21487;&#22788;&#29702;&#30340;&#65292;&#38656;&#35201;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#12290;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#36817;&#20284;&#26041;&#27861;&#26159;&#20855;&#26377;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20182;&#20204;&#20165;&#22312;&#26368;&#21518;&#19968;&#20010;&#32447;&#24615;&#23618;&#20013;&#20551;&#35774;&#20998;&#24067;&#26435;&#37325;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#27491;&#24577;&#20998;&#24067;&#30340;&#39044;&#27979;&#12290;&#20855;&#26377;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#23398;&#20064;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#20026;&#20102;&#36817;&#20284;&#19981;&#21487;&#22788;&#29702;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#38469;&#26469;&#33719;&#24471;&#38500;&#26368;&#21518;&#19968;&#23618;&#20197;&#22806;&#30340;&#25152;&#26377;&#20998;&#24067;&#26435;&#37325;&#30340;&#28857;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is an essential task in machine learning - a task in which neural networks (NNs) have traditionally not excelled. This can be a limitation for safety-critical applications, where uncertainty-aware methods like Gaussian processes or Bayesian linear regression are often preferred. Bayesian neural networks are an approach to address this limitation. They assume probability distributions for all parameters and yield distributed predictions. However, training and inference are typically intractable and approximations must be employed. A promising approximation is NNs with Bayesian last layer (BLL). They assume distributed weights only in the last linear layer and yield a normally distributed prediction. NNs with BLL can be seen as a Bayesian linear regression model with learned nonlinear features. To approximate the intractable Bayesian neural network, point estimates of the distributed weights in all but the last layer should be obtained by maximizing the margina
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#26469;&#34920;&#31034;&#25968;&#20540;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.06375</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#24207;&#21015;&#30340;Transformer&#65306;&#34920;&#31034;&#21644;&#35757;&#32451;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data. (arXiv:2302.06375v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#26469;&#34920;&#31034;&#25968;&#20540;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20197;&#22797;&#21046;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#22312;&#36825;&#19968;&#32467;&#26500;&#21270;&#39046;&#22495;&#30340;&#25104;&#21151;&#12290;&#29305;&#21035;&#26377;&#36259;&#30340;&#26159;&#65292;&#34920;&#26684;&#25968;&#25454;&#20855;&#26377;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#20363;&#22914;&#37329;&#34701;&#20132;&#26131;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#20540;&#30340;&#24322;&#36136;&#24615;&#65292;&#20854;&#20013;&#31867;&#21035;&#20803;&#32032;&#19982;&#25968;&#20540;&#39033;&#28151;&#21512;&#65292;&#20351;&#24471;&#36825;&#31181;&#36866;&#24212;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#26469;&#34920;&#31034;&#24322;&#26500;&#30340;&#26102;&#38388;&#30456;&#20851;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#25968;&#20540;&#29305;&#24449;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#34920;&#31034;&#65292;&#24182;&#19988;&#25972;&#20010;&#32593;&#32476;&#20351;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#23567;&#21270;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#26368;&#22351;&#24773;&#20917;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#30340;&#28176;&#36827;&#19979;&#30028;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;HIR&#20272;&#35745;&#30340;TS-HIR&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#25512;&#33616;&#26368;&#20248;&#33218;&#26102;&#34920;&#29616;&#20986;&#36817;&#20284;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02988</link><description>&lt;p&gt;
&#28176;&#36827;&#26368;&#20248;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20248;&#33218;&#35782;&#21035;&#26041;&#27861;&#19982;&#26041;&#24046;&#30456;&#20851;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Asymptotically Optimal Fixed-Budget Best Arm Identification with Variance-Dependent Bounds. (arXiv:2302.02988v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#23567;&#21270;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#25512;&#23548;&#26368;&#22351;&#24773;&#20917;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#30340;&#28176;&#36827;&#19979;&#30028;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;HIR&#20272;&#35745;&#30340;TS-HIR&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#25512;&#33616;&#26368;&#20248;&#33218;&#26102;&#34920;&#29616;&#20986;&#36817;&#20284;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#23567;&#21270;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#65292;&#20915;&#31574;&#32773;&#26681;&#25454;&#36807;&#21435;&#30340;&#35266;&#27979;&#32467;&#26524;&#36873;&#25321;&#22810;&#20010;&#22788;&#29702;&#33218;&#20043;&#19968;&#65292;&#24182;&#35266;&#23519;&#25152;&#36873;&#25321;&#30340;&#33218;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26463;&#21518;&#65292;&#20915;&#31574;&#32773;&#25512;&#33616;&#26399;&#26395;&#32467;&#26524;&#26368;&#39640;&#30340;&#22788;&#29702;&#33218;&#12290;&#25105;&#20204;&#22522;&#20110;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#35780;&#20272;&#20915;&#31574;&#30340;&#22909;&#22351;&#65292;&#20854;&#23450;&#20041;&#20026;&#26368;&#20248;&#33218;&#30340;&#26399;&#26395;&#32467;&#26524;&#19982;&#25512;&#33616;&#33218;&#30340;&#26399;&#26395;&#32467;&#26524;&#20043;&#24046;&#12290;&#30001;&#20110;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#26497;&#23567;&#26497;&#22823;&#20934;&#21017;&#35780;&#20272;&#36951;&#25022;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#26368;&#22351;&#24773;&#20917;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#30340;&#28176;&#36827;&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#30001;&#28508;&#22312;&#32467;&#26524;&#30340;&#26041;&#24046;&#65288;&#20027;&#23548;&#22240;&#32032;&#65289;&#25152;&#30830;&#23450;&#12290;&#22522;&#20110;&#36825;&#20123;&#19979;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Two-Stage (TS)-Hirano-Imbens-Ridder (HIR)&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#26368;&#20248;&#33218;&#26102;&#21033;&#29992;HIR&#20272;&#35745;&#65288;Hirano et al., 2003&#65289;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;TS-HIR&#31574;&#30053;&#36817;&#20284;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#33021;&#36798;&#21040;&#26497;&#23567;&#26497;&#22823;&#20934;&#21017;&#19979;&#30340;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of fixed-budget best arm identification (BAI) for minimizing expected simple regret. In an adaptive experiment, a decision maker draws one of multiple treatment arms based on past observations and observes the outcome of the drawn arm. After the experiment, the decision maker recommends the treatment arm with the highest expected outcome. We evaluate the decision based on the expected simple regret, which is the difference between the expected outcomes of the best arm and the recommended arm. Due to inherent uncertainty, we evaluate the regret using the minimax criterion. First, we derive asymptotic lower bounds for the worst-case expected simple regret, which are characterized by the variances of potential outcomes (leading factor). Based on the lower bounds, we propose the Two-Stage (TS)-Hirano-Imbens-Ridder (HIR) strategy, which utilizes the HIR estimator (Hirano et al., 2003) in recommending the best arm. Our theoretical analysis shows that the TS-HIR str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#35299;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#36890;&#36807;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#32553;&#23567;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#36890;&#36807;&#20020;&#24202;&#24212;&#29992;&#30740;&#31350;&#21644;&#24314;&#27169;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#33021;&#25552;&#20379;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#36824;&#20855;&#26377;&#36739;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#22270;&#35299;&#22411;&#35299;&#37322;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#26356;&#21463;&#25512;&#23815;&#12290;</title><link>http://arxiv.org/abs/2302.01241</link><description>&lt;p&gt;
&#22270;&#35299;&#21270;&#65306;&#21033;&#29992;&#22270;&#35299;&#22411;AI&#35299;&#37322;&#23545;&#20551;&#35774;&#24615;&#28436;&#32462;&#25512;&#29702;&#30340;&#29702;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses. (arXiv:2302.01241v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#35299;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#36890;&#36807;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#32553;&#23567;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#36890;&#36807;&#20020;&#24202;&#24212;&#29992;&#30740;&#31350;&#21644;&#24314;&#27169;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#33021;&#25552;&#20379;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#36824;&#20855;&#26377;&#36739;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#22270;&#35299;&#22411;&#35299;&#37322;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#26356;&#21463;&#25512;&#23815;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21487;&#35270;&#21270;&#24037;&#20855;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#29992;&#25143;&#36827;&#19968;&#27493;&#25512;&#29702;&#26469;&#35299;&#37322;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;XAI&#24212;&#35813;&#25903;&#25345;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#20197;&#20415;AI&#33021;&#22815;&#36827;&#34892;&#20551;&#35774;&#29983;&#25104;&#21644;&#35780;&#20272;&#65292;&#20174;&#32780;&#20943;&#23569;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#35299;&#21270;&#26041;&#27861;&#65292;&#20197;i)&#36827;&#34892;Peircean&#25512;&#23548;-&#28436;&#32462;&#25512;&#29702;&#65292;ii)&#36981;&#24490;&#39046;&#22495;&#24815;&#20363;&#65292;&#21644;iii)&#29992;&#22270;&#31034;&#25110;&#35821;&#35328;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#20020;&#24202;&#24212;&#29992;&#39046;&#22495;&#23454;&#29616;&#20102;DiagramNet&#65292;&#20197;&#39044;&#27979;&#24515;&#33039;&#21548;&#35786;&#20013;&#30340;&#24515;&#33039;&#35786;&#26029;&#65292;&#24182;&#29992;&#22522;&#20110;&#24418;&#29366;&#30340;&#26434;&#38899;&#22270;&#35299;&#36827;&#34892;&#35299;&#37322;&#12290;&#22312;&#24314;&#27169;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#25552;&#20379;&#20102;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#32780;&#19988;&#27604;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21307;&#23398;&#29983;&#30340;&#23450;&#24615;&#29992;&#25143;&#30740;&#31350;&#23637;&#31034;&#20102;&#22270;&#35299;&#22411;&#35299;&#37322;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#34920;&#26126;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#65292;&#22270;&#35299;&#24335;&#35299;&#37322;&#27604;&#20854;&#20182;&#26041;&#24335;&#26356;&#21463;&#25512;&#23815;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many visualizations have been developed for explainable AI (XAI), but they often require further reasoning by users to interpret. We argue that XAI should support diagrammatic and abductive reasoning for the AI to perform hypothesis generation and evaluation to reduce the interpretability gap. We propose Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii) follow domain conventions, and iii) explain with diagrams visually or verbally. We implemented DiagramNet for a clinical application to predict cardiac diagnoses from heart auscultation, and explain with shape-based murmur diagrams. In modeling studies, we found that DiagramNet not only provides faithful murmur shape explanations, but also has better prediction performance than baseline models. We further demonstrate the interpretability and trustworthiness of diagrammatic explanations in a qualitative user study with medical students, showing that clinically-relevant, diagrammatic explanations are preferred ov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GAN&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;MonoFlow&#65292;&#36890;&#36807;Wasserstein&#26799;&#24230;&#27969;&#33719;&#24471;&#29702;&#35770;&#27934;&#35265;&#21644;&#31639;&#27861;&#21551;&#31034;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#23494;&#24230;&#27604;&#20363;&#30340;&#21333;&#35843;&#36882;&#22686;&#26144;&#23556;&#37325;&#26032;&#32553;&#25918;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#37492;&#21035;&#22120;&#33719;&#24471;MonoFlow&#30340;&#21521;&#37327;&#22330;&#65292;&#21033;&#29992;&#30456;&#24212;&#30340;&#21521;&#37327;&#22330;&#36827;&#34892;&#31890;&#23376;&#27969;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.01075</link><description>&lt;p&gt;
MonoFlow: &#20174;Wasserstein&#26799;&#24230;&#27969;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;Divergence GANs
&lt;/p&gt;
&lt;p&gt;
MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows. (arXiv:2302.01075v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GAN&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;MonoFlow&#65292;&#36890;&#36807;Wasserstein&#26799;&#24230;&#27969;&#33719;&#24471;&#29702;&#35770;&#27934;&#35265;&#21644;&#31639;&#27861;&#21551;&#31034;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#23494;&#24230;&#27604;&#20363;&#30340;&#21333;&#35843;&#36882;&#22686;&#26144;&#23556;&#37325;&#26032;&#32553;&#25918;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#37492;&#21035;&#22120;&#33719;&#24471;MonoFlow&#30340;&#21521;&#37327;&#22330;&#65292;&#21033;&#29992;&#30456;&#24212;&#30340;&#21521;&#37327;&#22330;&#36827;&#34892;&#31890;&#23376;&#27969;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#23545;&#25239;&#35757;&#32451;&#26159;&#36890;&#36807;&#21028;&#21035;&#22120;&#26469;&#20272;&#35745;&#31163;&#25955;&#24230;&#65292;&#29983;&#25104;&#22120;&#23398;&#20064;&#26368;&#23567;&#21270;&#36825;&#20010;&#31163;&#25955;&#24230;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#35768;&#22810;GANs&#21464;&#20307;&#37117;&#26159;&#25353;&#29031;&#36825;&#20010;&#33539;&#20363;&#24320;&#21457;&#30340;&#65292;&#20294;&#24403;&#21069;GANs&#30340;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#31639;&#27861;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#23637;&#31034;&#20102;&#26679;&#26412;&#31354;&#38388;&#20869;&#31890;&#23376;&#28436;&#21270;&#30340;Wasserstein&#26799;&#24230;&#27969;&#26469;&#33719;&#24471;GANs&#30340;&#29702;&#35770;&#27934;&#35265;&#21644;&#31639;&#27861;&#21551;&#31034;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;MonoFlow&#65306;&#31890;&#23376;&#28436;&#21270;&#36890;&#36807;&#23494;&#24230;&#27604;&#20363;&#30340;&#21333;&#35843;&#36882;&#22686;&#26144;&#23556;&#36827;&#34892;&#37325;&#26032;&#32553;&#25918;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36807;&#31243;&#65292;&#39318;&#20808;&#36890;&#36807;&#35757;&#32451;&#37492;&#21035;&#22120;&#33719;&#24471;MonoFlow&#30340;&#21521;&#37327;&#22330;&#65292;&#28982;&#21518;&#29983;&#25104;&#22120;&#23398;&#20064;&#30001;&#30456;&#24212;&#21521;&#37327;&#22330;&#25152;&#23450;&#20041;&#30340;&#31890;&#23376;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional understanding of adversarial training in generative adversarial networks (GANs) is that the discriminator is trained to estimate a divergence, and the generator learns to minimize this divergence. We argue that despite the fact that many variants of GANs were developed following this paradigm, the current theoretical understanding of GANs and their practical algorithms are inconsistent. In this paper, we leverage Wasserstein gradient flows which characterize the evolution of particles in the sample space, to gain theoretical insights and algorithmic inspiration of GANs. We introduce a unified generative modeling framework - MonoFlow: the particle evolution is rescaled via a monotonically increasing mapping of the log density ratio. Under our framework, adversarial training can be viewed as a procedure first obtaining MonoFlow's vector field via training the discriminator and the generator learns to draw the particle flow defined by the corresponding vector field. We al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#30456;&#23545;&#29109;&#36793;&#30028;&#65292;&#21033;&#29992;Fokker-Planck&#26041;&#31243;&#30340;&#31283;&#23450;&#24615;&#20272;&#35745;&#32467;&#26524;&#65292;&#30740;&#31350;&#20102;&#21508;&#21521;&#24322;&#24615;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2302.00766</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#23545;&#29109;&#36793;&#30028;&#30740;&#31350;&#21508;&#21521;&#24322;&#24615;Langevin&#21160;&#21147;&#23398;&#30340;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Privacy Risk for anisotropic Langevin dynamics using relative entropy bounds. (arXiv:2302.00766v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#30456;&#23545;&#29109;&#36793;&#30028;&#65292;&#21033;&#29992;Fokker-Planck&#26041;&#31243;&#30340;&#31283;&#23450;&#24615;&#20272;&#35745;&#32467;&#26524;&#65292;&#30740;&#31350;&#20102;&#21508;&#21521;&#24322;&#24615;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20855;&#26377;&#21152;&#24615;&#21508;&#21521;&#21516;&#24615;&#22122;&#22768;&#30340;Langevin&#21160;&#21147;&#23398;&#30340;&#38544;&#31169;&#20445;&#25252;&#29305;&#24615;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#21508;&#21521;&#21516;&#24615;&#22122;&#22768;&#20551;&#35774;&#38750;&#24120;&#20005;&#26684;&#65306;&#65288;a&#65289;&#22312;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#24182;&#20445;&#25345;&#26368;&#20339;&#20934;&#30830;&#24615;&#32780;&#21521;&#29616;&#26377;&#23398;&#20064;&#31639;&#27861;&#28155;&#21152;&#22122;&#22768;&#26102;&#65292;&#24212;&#32771;&#34385;&#36755;&#20986;&#30340;&#30456;&#23545;&#24133;&#24230;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65307;&#65288;b&#65289;&#27969;&#34892;&#30340;&#31639;&#27861;&#65292;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;&#21450;&#20854;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#65289;&#65292;&#20284;&#20046;&#20855;&#26377;&#21508;&#21521;&#24322;&#24615;&#30340;&#21327;&#26041;&#24046;&#29305;&#24615;&#12290;&#35201;&#30740;&#31350;&#21508;&#21521;&#24322;&#24615;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#38656;&#35201;&#20851;&#20110;&#20855;&#26377;&#19981;&#21516;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#30340;&#20004;&#20010;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20043;&#38388;&#30340;&#30456;&#23545;&#29109;&#30340;&#19968;&#33324;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#21151;&#33021;&#19981;&#31561;&#24335;&#21033;&#29992;&#23545;Fokker-Planck&#26041;&#31243;&#30340;&#35299;&#30340;&#31283;&#23450;&#24615;&#20272;&#35745;&#26469;&#24314;&#31435;&#36825;&#26679;&#30340;&#36793;&#30028;&#12290;&#22312;&#38468;&#21152;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#29109;&#36793;&#30028;&#24847;&#21619;&#30528;&#19968;&#20010;$(\epsilon,\delta)$-differential&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The privacy preserving properties of Langevin dynamics with additive isotropic noise have been extensively studied. However, the isotropic noise assumption is very restrictive: (a) when adding noise to existing learning algorithms to preserve privacy and maintain the best possible accuracy one should take into account the relative magnitude of the outputs and their correlations; (b) popular algorithms such as stochastic gradient descent (and their continuous time limits) appear to possess anisotropic covariance properties. To study the privacy risks for the anisotropic noise case, one requires general results on the relative entropy between the laws of two Stochastic Differential Equations with different drifts and diffusion coefficients. Our main contribution is to establish such a bound using stability estimates for solutions to the Fokker-Planck equations via functional inequalities. With additional assumptions, the relative entropy bound implies an $(\epsilon,\delta)$-differential 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#20272;&#35745;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#36873;&#25321;&#24615;&#20998;&#31867;&#21644;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#65288;SCOD&#65289;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#65292;&#26377;&#25928;&#24615;&#39640;&#65292;&#21487;&#20197;&#25512;&#24191;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12386</link><description>&lt;p&gt;
&#20351;&#29992;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#30340;&#25554;&#20214;&#20272;&#35745;&#22120;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Plugin estimators for selective classification with out-of-distribution detection. (arXiv:2301.12386v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12386
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#20272;&#35745;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#36873;&#25321;&#24615;&#20998;&#31867;&#21644;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#65288;SCOD&#65289;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#65292;&#26377;&#25928;&#24615;&#39640;&#65292;&#21487;&#20197;&#25512;&#24191;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#21463;&#30410;&#20110;&#22312;&#23545;&#32622;&#20449;&#24230;&#36739;&#20302;&#30340;&#26679;&#26412;&#36827;&#34892;&#39044;&#27979;&#26102;&#36873;&#25321;&#24615;&#22320;&#25918;&#24323;&#12290;&#36825;&#31181;&#24323;&#26435;&#23545;&#20110;&#25509;&#36817;&#23398;&#20064;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#25110;&#32773;&#30456;&#23545;&#20110;&#35757;&#32451;&#26679;&#26412;&#26469;&#35828;&#26159;&#24322;&#24120;&#20540;&#30340;&#26679;&#26412;&#29305;&#21035;&#26377;&#29992;&#12290;&#36825;&#20123;&#35774;&#32622;&#24050;&#32463;&#22312;&#36873;&#25321;&#24615;&#20998;&#31867;(SC)&#21644;&#22806;&#37096;&#20998;&#24067;(OOD)&#26816;&#27979;&#30340;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#20294;&#19981;&#20851;&#32852;&#30340;&#30740;&#31350;&#25152;&#30740;&#31350;&#12290;&#26368;&#36817;&#26377;&#20851;&#36873;&#25321;&#24615;&#20998;&#31867;&#19982;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;(SCOD)&#30340;&#30740;&#31350;&#35748;&#20026;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#32479;&#19968;&#30740;&#31350;&#26159;&#26377;&#24517;&#35201;&#30340;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#27491;&#24335;&#22522;&#30784;&#20173;&#28982;&#19981;&#25104;&#29087;&#65292;&#29616;&#26377;&#30340;&#25216;&#26415;&#20063;&#37117;&#26159;&#21551;&#21457;&#24335;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SCOD&#30340;&#25554;&#20214;&#20272;&#35745;&#22120;&#65292;&#23427;&#20204;&#22312;&#29702;&#35770;&#19978;&#26159;&#26377;&#26681;&#25454;&#30340;&#12289;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#27867;&#21270;&#20102;SC&#21644;OOD&#26816;&#27979;&#39046;&#22495;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#35828;&#26126;&#20102;&#23545;&#29616;&#26377;&#30340;SC&#21644;OOD&#26816;&#27979;&#22522;&#32447;&#30340;&#22825;&#30495;&#20351;&#29992;&#21487;&#33021;&#19981;&#36275;&#20197;&#36866;&#24212;SCOD&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world classifiers can benefit from the option of abstaining from predicting on samples where they have low confidence. Such abstention is particularly useful on samples which are close to the learned decision boundary, or which are outliers with respect to the training sample. These settings have been the subject of extensive but disjoint study in the selective classification (SC) and out-of-distribution (OOD) detection literature. Recent work on selective classification with OOD detection (SCOD) has argued for the unified study of these problems; however, the formal underpinnings of this problem are still nascent, and existing techniques are heuristic in nature. In this paper, we propose new plugin estimators for SCOD that are theoretically grounded, effective, and generalise existing approaches from the SC and OOD detection literature. In the course of our analysis, we formally explicate how na\"{i}ve use of existing SC and OOD detection baselines may be inadequate for SCOD. We 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#36125;&#21494;&#26031;&#23618;&#27425;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25903;&#25345;&#20998;&#25674;&#25512;&#26029;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#21644;&#24615;&#33021;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23545;&#22235;&#20010;&#23618;&#27425;&#35777;&#25454;&#31215;&#32047;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2301.11873</link><description>&lt;p&gt;
&#27604;&#36739;&#36125;&#21494;&#26031;&#23618;&#27425;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Method for Comparing Bayesian Hierarchical Models. (arXiv:2301.11873v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11873
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#36125;&#21494;&#26031;&#23618;&#27425;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25903;&#25345;&#20998;&#25674;&#25512;&#26029;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#21644;&#24615;&#33021;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23545;&#22235;&#20010;&#23618;&#27425;&#35777;&#25454;&#31215;&#32047;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#65288;BMC&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31454;&#20105;&#35745;&#31639;&#27169;&#22411;&#30340;&#30456;&#23545;&#20248;&#21183;&#65292;&#24182;&#23558;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#21040;&#27169;&#22411;&#36873;&#25321;&#20915;&#31574;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#32500;&#23884;&#22871;&#21442;&#25968;&#32467;&#26500;&#65292;BMC&#22312;&#24120;&#35265;&#30340;&#23618;&#27425;&#27169;&#22411;&#20013;&#24120;&#24120;&#38590;&#20197;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#20219;&#20309;&#21487;&#23454;&#20363;&#21270;&#20026;&#27010;&#29575;&#31243;&#24207;&#30340;&#23618;&#27425;&#27169;&#22411;&#38598;&#36827;&#34892;BMC&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#20998;&#25674;&#25512;&#26029;&#65292;&#23427;&#21487;&#20197;&#22312;&#20219;&#20309;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#20043;&#21069;&#65292;&#23545;&#21518;&#39564;&#27169;&#22411;&#27010;&#29575;&#36827;&#34892;&#39640;&#25928;&#30340;&#37325;&#26032;&#20272;&#35745;&#21644;&#24555;&#36895;&#24615;&#33021;&#39564;&#35777;&#12290;&#22312;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#39564;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#27604;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26725;&#24335;&#25277;&#26679;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25152;&#26377;BMC&#35774;&#32622;&#20013;&#20986;&#33394;&#30340;&#20998;&#25674;&#25512;&#26029;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#20808;&#21069;&#34987;&#35748;&#20026;&#26159;&#22235;&#20010;&#23618;&#27425;&#35777;&#25454;&#31215;&#32047;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian model comparison (BMC) offers a principled approach for assessing the relative merits of competing computational models and propagating uncertainty into model selection decisions. However, BMC is often intractable for the popular class of hierarchical models due to their high-dimensional nested parameter structure. To address this intractability, we propose a deep learning method for performing BMC on any set of hierarchical models which can be instantiated as probabilistic programs. Since our method enables amortized inference, it allows efficient re-estimation of posterior model probabilities and fast performance validation prior to any real-data application. In a series of extensive validation studies, we benchmark the performance of our method against the state-of-the-art bridge sampling method and demonstrate excellent amortized inference across all BMC settings. We then showcase our method by comparing four hierarchical evidence accumulation models that have previously b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23545;&#33678;&#22763;&#27604;&#20122;&#30340;&#20316;&#21697;&#36827;&#34892;&#20102;&#36830;&#32493;&#20998;&#26512;&#65292;&#21457;&#29616;&#20182;&#30340;&#20889;&#20316;&#39118;&#26684;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#21477;&#23376;&#38271;&#24230;&#12289;&#24418;&#23481;&#35789;&#21644;&#21103;&#35789;&#30340;&#39057;&#29575;&#20197;&#21450;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#37096;&#20998;&#25103;&#21095;&#30340;&#39118;&#26684;&#29305;&#24449;&#26356;&#31867;&#20284;&#20110;&#20854;&#20889;&#20316;&#26102;&#38388;&#20043;&#21069;&#25110;&#20043;&#21518;&#30340;&#20316;&#21697;&#12290;</title><link>http://arxiv.org/abs/2301.06024</link><description>&lt;p&gt;
&#23545;&#33678;&#22763;&#27604;&#20122;&#25103;&#21095;&#30340;&#36830;&#32493;&#20998;&#26512;&#30340;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A data science and machine learning approach to continuous analysis of Shakespeare's plays. (arXiv:2301.06024v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06024
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23545;&#33678;&#22763;&#27604;&#20122;&#30340;&#20316;&#21697;&#36827;&#34892;&#20102;&#36830;&#32493;&#20998;&#26512;&#65292;&#21457;&#29616;&#20182;&#30340;&#20889;&#20316;&#39118;&#26684;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#21477;&#23376;&#38271;&#24230;&#12289;&#24418;&#23481;&#35789;&#21644;&#21103;&#35789;&#30340;&#39057;&#29575;&#20197;&#21450;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#37096;&#20998;&#25103;&#21095;&#30340;&#39118;&#26684;&#29305;&#24449;&#26356;&#31867;&#20284;&#20110;&#20854;&#20889;&#20316;&#26102;&#38388;&#20043;&#21069;&#25110;&#20043;&#21518;&#30340;&#20316;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#37327;&#21270;&#30340;&#25991;&#26412;&#20998;&#26512;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#20026;&#25991;&#23398;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#23558;&#32508;&#21512;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#24212;&#29992;&#20110;&#23041;&#24265;&#183;&#33678;&#22763;&#27604;&#20122;&#30340;&#20316;&#21697;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#33678;&#22763;&#27604;&#20122;&#30340;&#20889;&#20316;&#39118;&#26684;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21457;&#29983;&#20102;&#26126;&#26174;&#30340;&#21464;&#21270;&#65292;&#20854;&#20013;&#26368;&#26174;&#33879;&#30340;&#21464;&#21270;&#21253;&#25324;&#21477;&#23376;&#38271;&#24230;&#12289;&#24418;&#23481;&#35789;&#21644;&#21103;&#35789;&#30340;&#39057;&#29575;&#20197;&#21450;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#23545;&#25103;&#21095;&#24180;&#20221;&#30340;&#39118;&#26684;&#39044;&#27979;&#34920;&#26126;&#65292;&#23454;&#38469;&#24180;&#20221;&#21644;&#39044;&#27979;&#24180;&#20221;&#20043;&#38388;&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#20026;0.71&#65292;&#34920;&#26126;&#33678;&#22763;&#27604;&#20122;&#30340;&#20889;&#20316;&#39118;&#26684;&#22312;&#25968;&#37327;&#21270;&#27979;&#37327;&#26041;&#38754;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#26576;&#20123;&#25103;&#21095;&#30340;&#39118;&#26684;&#29305;&#24449;&#26356;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#20889;&#20316;&#24180;&#20221;&#20043;&#21069;&#25110;&#20043;&#21518;&#30340;&#20316;&#21697;&#12290;&#20363;&#22914;&#65292;"&#32599;&#23494;&#27431;&#19982;&#26417;&#20029;&#21494;"&#30340;&#26085;&#26399;&#20026;1596&#24180;&#65292;&#20294;&#22312;&#39118;&#26684;&#29305;&#24449;&#19978;&#26356;&#31867;&#20284;&#20110;1600&#24180;&#20043;&#21518;&#33678;&#22763;&#27604;&#20122;&#30340;&#20854;&#20182;&#20316;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of quantitative text analysis methods has provided new ways of analyzing literature in a manner that was not available in the pre-information era. Here we apply comprehensive machine learning analysis to the work of William Shakespeare. The analysis shows clear changes in the style of writing over time, with the most significant changes in the sentence length, frequency of adjectives and adverbs, and the sentiments expressed in the text. Applying machine learning to make a stylometric prediction of the year of the play shows a Pearson correlation of 0.71 between the actual and predicted year, indicating that Shakespeare's writing style as reflected by the quantitative measurements changed over time. Additionally, it shows that the stylometrics of some of the plays is more similar to plays written either before or after the year they were written. For instance, Romeo and Juliet is dated 1596, but is more similar in stylometrics to plays written by Shakespeare after 1600
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#31574;&#30053;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#31614;&#24179;&#28369;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#22312;&#23545;&#25239;&#26679;&#26412;&#19978;&#20986;&#29616;&#30340;&#36807;&#24230;&#33258;&#20449;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2212.10258</link><description>&lt;p&gt;
&#21033;&#29992;&#26631;&#31614;&#24179;&#28369;&#23454;&#29616;&#39046;&#22495;&#20869;&#22806;&#25991;&#26412;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
In and Out-of-Domain Text Adversarial Robustness via Label Smoothing. (arXiv:2212.10258v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#31574;&#30053;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#31614;&#24179;&#28369;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#22312;&#23545;&#25239;&#26679;&#26412;&#19978;&#20986;&#29616;&#30340;&#36807;&#24230;&#33258;&#20449;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21363;&#23545;&#36755;&#20837;&#36827;&#34892;&#32454;&#24494;&#20462;&#25913;&#65288;&#22914;&#21516;&#20041;&#35789;&#26367;&#25442;&#65289;&#20250;&#26497;&#22823;&#22320;&#25913;&#21464;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;&#25991;&#26412;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38450;&#24481;&#25216;&#26415;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#33267;&#31163;&#25955;&#24615;&#36136;&#30340;&#25991;&#26412;&#25968;&#25454;&#19978;&#65292;&#20294;&#26159;&#32508;&#21512;&#24615;&#30340;&#35268;&#21017;&#21270;&#26041;&#27861;&#65292;&#22914;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#24179;&#28369;&#23545;&#20110;&#25991;&#26412;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#30340;&#25928;&#26524;&#36824;&#27809;&#26377;&#34987;&#30740;&#31350;&#36807;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26631;&#31614;&#24179;&#28369;&#31574;&#30053;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#23545;&#22810;&#26679;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#31614;&#24179;&#28369;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#22312;&#21508;&#31181;&#27969;&#34892;&#25915;&#20987;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#39044;&#27979;&#21487;&#20449;&#24230;&#19982;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26174;&#31034;&#26631;&#31614;&#24179;&#28369;&#20943;&#23569;&#20102;&#22312;&#23545;&#25239;&#26679;&#26412;&#19978;&#20986;&#29616;&#30340;&#36807;&#24230;&#33258;&#20449;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the adversarial robustness provided by various label smoothing strategies in foundational models for diverse NLP tasks in both in-domain and out-of-domain settings. Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks. We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21551;&#29992;&#39118;&#21147;&#28065;&#36718;&#26426;&#38431;&#26412;&#22320;&#25968;&#25454;&#30340;&#33337;&#38431;&#33539;&#22260;&#23398;&#20064;&#65292;&#35299;&#20915;&#39118;&#21147;&#28065;&#36718;&#26426;&#21046;&#36896;&#21830;&#25968;&#25454;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#25913;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#28065;&#36718;&#26426;&#36816;&#32500;&#31574;&#30053;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2212.03529</link><description>&lt;p&gt;
&#24320;&#21457;&#39118;&#21147;&#28065;&#36718;&#26426;&#26465;&#20214;&#20449;&#24687;&#30340;&#33337;&#38431;&#20849;&#20139;&#65292;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Fleet-wide Sharing of Wind Turbine Condition Information through Privacy-preserving Federated Learning. (arXiv:2212.03529v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21551;&#29992;&#39118;&#21147;&#28065;&#36718;&#26426;&#38431;&#26412;&#22320;&#25968;&#25454;&#30340;&#33337;&#38431;&#33539;&#22260;&#23398;&#20064;&#65292;&#35299;&#20915;&#39118;&#21147;&#28065;&#36718;&#26426;&#21046;&#36896;&#21830;&#25968;&#25454;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#25913;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#28065;&#36718;&#26426;&#36816;&#32500;&#31574;&#30053;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#21147;&#28065;&#36718;&#26426;&#21046;&#36896;&#21830;&#27599;&#22825;&#20174;&#33258;&#24049;&#30340;&#33337;&#38431;&#20013;&#25910;&#38598;&#20102;&#25968;&#21315;&#20806;&#23383;&#33410;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#26377;&#20215;&#20540;&#30340;&#23454;&#26102;&#20449;&#24687;&#65292;&#29992;&#20110;&#28065;&#36718;&#26426;&#20581;&#24247;&#35786;&#26029;&#12289;&#24615;&#33021;&#30417;&#27979;&#12289;&#39044;&#27979;&#32597;&#35265;&#25925;&#38556;&#21644;&#20851;&#38190;&#37096;&#20214;&#30340;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#39118;&#21147;&#28065;&#36718;&#26426;&#38431;&#30340;&#36825;&#20123;&#25968;&#25454;&#36130;&#23500;&#22240;&#21046;&#36896;&#21830;&#20986;&#20110;&#21830;&#19994;&#25112;&#30053;&#21407;&#22240;&#32780;&#26080;&#27861;&#34987;&#25805;&#20316;&#32773;&#12289;&#20844;&#29992;&#20107;&#19994;&#20844;&#21496;&#21644;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#12290;&#25968;&#25454;&#35775;&#38382;&#30340;&#32570;&#20047;&#22952;&#30861;&#20102;&#21033;&#29992;&#26426;&#20250;&#65292;&#22914;&#25913;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#28065;&#36718;&#26426;&#36816;&#32500;&#31574;&#30053;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#30041;&#22312;&#39118;&#21147;&#28065;&#36718;&#26426;&#19978;&#20197;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#20173;&#28982;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#23454;&#29616;&#33337;&#38431;&#33539;&#22260;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#39118;&#21147;&#28065;&#36718;&#26426;&#30340;&#31232;&#32570;&#20195;&#34920;&#24615;&#22521;&#35757;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Terabytes of data are collected every day by wind turbine manufacturers from their fleets. The data contain valuable real-time information for turbine health diagnostics and performance monitoring, for predicting rare failures and the remaining service life of critical parts. And yet, this wealth of data from wind turbine fleets remains inaccessible to operators, utility companies, and researchers as manufacturing companies prefer the privacy of their fleets' turbine data for business strategic reasons. The lack of data access impedes the exploitation of opportunities, such as improving data-driven turbine operation and maintenance strategies and reducing downtimes. We present a distributed federated machine learning approach that leaves the data on the wind turbines to preserve the data privacy, as desired by manufacturers, while still enabling fleet-wide learning on those local data. We demonstrate in two case studies that wind turbines which are scarce in representative training dat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;TD&#23398;&#20064;&#35270;&#20026;&#36866;&#24403;&#36873;&#25321;&#20989;&#25968;&#30340;&#26799;&#24230;&#20998;&#21106;&#65292;&#23558;TD&#21644;SVRG&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2211.16237</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#20998;&#21106;&#26041;&#27861;&#32553;&#23567;SVRG&#19982;TD-SVRG&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Closing the gap between SVRG and TD-SVRG with Gradient Splitting. (arXiv:2211.16237v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;TD&#23398;&#20064;&#35270;&#20026;&#36866;&#24403;&#36873;&#25321;&#20989;&#25968;&#30340;&#26799;&#24230;&#20998;&#21106;&#65292;&#23558;TD&#21644;SVRG&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TD&#65288;&#26102;&#24207;&#24046;&#20998;&#65289;&#23398;&#20064;&#26159;&#19968;&#31181;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#22686;&#24378;&#12290;&#26368;&#36817;&#65292;&#22810;&#20010;&#24037;&#20316;&#23581;&#35797;&#23558;TD&#23398;&#20064;&#19982;SVRG&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#19968;&#31181;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#20984;&#20248;&#21270;&#35774;&#32622;&#19979;&#65292;&#25152;&#24471;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#26126;&#26174;&#19981;&#21450;SVRG&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#36817;&#23545;TD&#23398;&#20064;&#30340;&#35299;&#37322;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#36866;&#24403;&#36873;&#25321;&#20989;&#25968;&#30340;&#26799;&#24230;&#30340;&#20998;&#21106;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#31639;&#27861;&#65292;&#24182;&#23558;TD&#19982;SVRG&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#20010;&#20855;&#26377;&#39044;&#23450;&#23398;&#20064;&#36895;&#29575;&#20026;1/8&#30340;&#20960;&#20309;&#25910;&#25947;&#30028;&#38480;&#65292;&#19982;&#20984;&#35774;&#32622;&#19979;SVRG&#30340;&#25910;&#25947;&#30028;&#38480;&#30456;&#21516;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#24471;&#21040;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal difference (TD) learning is a policy evaluation in reinforcement learning whose performance can be enhanced by variance reduction techniques. Recently, multiple works have sought to fuse TD learning with SVRG to obtain a policy evaluation method with a geometric rate of convergence. However, the resulting convergence rate is significantly weaker than what is achieved by SVRG in the setting of convex optimization. In this work we utilize a recent interpretation of TD-learning as the splitting of the gradient of an appropriately chosen function, thus simplifying the algorithm and fusing TD with SVRG. Our main result is a geometric convergence bound with predetermined learning rate of $1/8$, which is identical to the convergence bound available for SVRG in the convex setting. Our theoretical findings are supported by a set of experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#21644;&#29983;&#21270;&#24615;&#36136;&#30340;&#27169;&#24577;&#32467;&#21512;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23558;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20043;&#38388;&#30340;&#21452;&#21521;&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.10590</link><description>&lt;p&gt;
&#21333;&#20010;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;&#30340;&#21452;&#21521;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Generation of Structure and Properties Through a Single Molecular Foundation Model. (arXiv:2211.10590v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#21644;&#29983;&#21270;&#24615;&#36136;&#30340;&#27169;&#24577;&#32467;&#21512;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23558;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20043;&#38388;&#30340;&#21452;&#21521;&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#20419;&#20351;&#20102;&#21270;&#23398;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#23545;&#25552;&#20379;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#34920;&#31034;&#30340;&#22823;&#20998;&#23376;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20294;&#20998;&#23376;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#23581;&#35797;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#22810;&#27169;&#24577;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#23558;&#32467;&#26500;&#21644;&#29983;&#21270;&#24615;&#36136;&#30340;&#27169;&#24577;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#31649;&#36947;&#22788;&#29702;&#25968;&#25454;&#24182;&#26681;&#25454;&#20849;&#21516;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#32467;&#26500;/&#24615;&#36136;&#29305;&#24449;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23558;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20043;&#38388;&#30340;&#21452;&#21521;&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20123;&#36129;&#29486;&#20135;&#29983;&#20102;&#21327;&#21516;&#30340;&#30693;&#35782;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#22788;&#29702;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36275;&#20197;&#29983;&#25104;&#20855;&#26377;&#21452;&#21521;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of large foundation models in artificial intelligence has prompted the emergence of chemical pre-trained models. Despite the growing interest in large molecular pre-trained models that provide informative representations for downstream tasks, attempts for multimodal pre-training approaches on the molecule domain were limited. To address this, we present a novel multimodal molecular pre-trained model that incorporates the modalities of structure and biochemical properties, drawing inspiration from recent advances in multimodal learning techniques. Our proposed model pipeline of data handling and training objectives aligns the structure/property features in a common embedding space, which enables the model to regard bidirectional information between the molecules' structure and properties. These contributions emerge synergistic knowledge, allowing us to tackle both multimodal and unimodal downstream tasks through a single model. Through extensive experiments, we demons
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#27979;&#37327;&#21644;&#39044;&#27979;&#22270;&#20687;&#23545;&#26426;&#22120;&#30340;&#35760;&#24518;&#24615;&#65292;&#24182;&#21457;&#29616;&#8220;&#22797;&#26434;&#8221;&#22270;&#20687;&#36890;&#24120;&#23545;&#26426;&#22120;&#35760;&#24518;&#26356;&#21152;&#28145;&#21051;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#35760;&#24518;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#20026;&#26426;&#22120;&#35760;&#24518;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#30740;&#31350;&#26041;&#21521;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.07625</link><description>&lt;p&gt;
&#22270;&#20687;&#23545;&#26426;&#22120;&#30340;&#35760;&#24518;&#24615;&#26377;&#20309;&#24433;&#21709;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Images are More Memorable to Machines?. (arXiv:2211.07625v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#27979;&#37327;&#21644;&#39044;&#27979;&#22270;&#20687;&#23545;&#26426;&#22120;&#30340;&#35760;&#24518;&#24615;&#65292;&#24182;&#21457;&#29616;&#8220;&#22797;&#26434;&#8221;&#22270;&#20687;&#36890;&#24120;&#23545;&#26426;&#22120;&#35760;&#24518;&#26356;&#21152;&#28145;&#21051;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#35760;&#24518;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#20026;&#26426;&#22120;&#35760;&#24518;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#30740;&#31350;&#26041;&#21521;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#27979;&#37327;&#21644;&#39044;&#27979;&#22270;&#20687;&#23545;&#27169;&#24335;&#35782;&#21035;&#26426;&#22120;&#30340;&#35760;&#24518;&#24615;&#65292;&#20197;&#25506;&#32034;&#26426;&#22120;&#26234;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#26426;&#22120;&#35760;&#24518;&#37327;&#21270;&#27969;&#31243;&#65292;&#31216;&#20026;&#8220;MachineMem measurer&#8221;&#65292;&#29992;&#20110;&#25910;&#38598;&#22270;&#20687;&#30340;&#26426;&#22120;&#35760;&#24518;&#24615;&#20998;&#25968;&#12290;&#19982;&#20154;&#31867;&#31867;&#20284;&#65292;&#26426;&#22120;&#20063;&#20542;&#21521;&#20110;&#35760;&#24518;&#26576;&#20123;&#31867;&#22411;&#30340;&#22270;&#20687;&#65292;&#20294;&#26426;&#22120;&#21644;&#20154;&#31867;&#35760;&#24518;&#30340;&#22270;&#20687;&#31867;&#22411;&#26159;&#19981;&#21516;&#30340;&#12290;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#21644;&#20840;&#38754;&#30340;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#36880;&#28176;&#25581;&#31034;&#20102;&#8220;&#22797;&#26434;&#8221;&#22270;&#20687;&#36890;&#24120;&#23545;&#26426;&#22120;&#35760;&#24518;&#26356;&#21152;&#28145;&#21051;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;11&#21488;&#19981;&#21516;&#30340;&#26426;&#22120;&#65288;&#20174;&#32447;&#24615;&#20998;&#31867;&#22120;&#21040;&#29616;&#20195;ViTs&#65289;&#21644;9&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#20197;&#20998;&#26512;&#21644;&#29702;&#35299;&#26426;&#22120;&#35760;&#24518;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26426;&#22120;&#35760;&#24518;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#26426;&#22120;&#35760;&#24518;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#20132;&#21449;&#30028;&#38754;&#19978;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of measuring and predicting how memorable an image is to pattern recognition machines, as a path to explore machine intelligence. Firstly, we propose a self-supervised machine memory quantification pipeline, dubbed ``MachineMem measurer'', to collect machine memorability scores of images. Similar to humans, machines also tend to memorize certain kinds of images, whereas the types of images that machines and humans memorize are different. Through in-depth analysis and comprehensive visualizations, we gradually unveil that``complex" images are usually more memorable to machines. We further conduct extensive experiments across 11 different machines (from linear classifiers to modern ViTs) and 9 pre-training methods to analyze and understand machine memory. This work proposes the concept of machine memorability and opens a new research direction at the interface between machine memory and visual data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#39033;&#22810;&#20013;&#24515;&#30740;&#31350;&#65292;&#26088;&#22312;&#35780;&#20272;MRI&#25968;&#25454;&#21327;&#35843;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25928;&#29992;;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35843;&#21644;&#22120;&#21464;&#21387;&#22120;&#8221;&#26041;&#27861;&#65292;&#22312;&#19981;&#27844;&#38706;&#20449;&#24687;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#21327;&#35843;&#12290;</title><link>http://arxiv.org/abs/2211.04125</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26102;&#20195;MRI&#25968;&#25454;&#21327;&#35843;&#30340;&#26377;&#25928;&#24615;&#65306;&#19968;&#39033;&#36328;36&#20010;&#25968;&#25454;&#38598;&#30340;&#22810;&#20013;&#24515;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Efficacy of MRI data harmonization in the age of machine learning. A multicenter study across 36 datasets. (arXiv:2211.04125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#39033;&#22810;&#20013;&#24515;&#30740;&#31350;&#65292;&#26088;&#22312;&#35780;&#20272;MRI&#25968;&#25454;&#21327;&#35843;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25928;&#29992;;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35843;&#21644;&#22120;&#21464;&#21387;&#22120;&#8221;&#26041;&#27861;&#65292;&#22312;&#19981;&#27844;&#38706;&#20449;&#24687;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22810;&#20010;&#32593;&#31449;&#27719;&#38598;&#20844;&#24320;&#21487;&#29992;&#30340;MRI&#25968;&#25454;&#21487;&#20197;&#32452;&#35013;&#22823;&#37327;&#21463;&#35797;&#23545;&#35937;&#65292;&#22686;&#21152;&#32479;&#35745;&#21151;&#29575;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20419;&#36827;&#25968;&#25454;&#37325;&#29992;&#12290;&#22810;&#20013;&#24515;&#25968;&#25454;&#30340;&#21327;&#35843;&#26159;&#20943;&#23569;&#25968;&#25454;&#20013;&#19982;&#38750;&#29983;&#29289;&#26469;&#28304;&#30340;&#21464;&#24322;&#24230;&#37327;&#30340;&#28151;&#26434;&#25928;&#24212;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#23558;&#21327;&#35843;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20043;&#21069;&#30340;&#25972;&#20010;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#25968;&#25454;&#27844;&#28431;&#65292;&#22240;&#20026;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#20449;&#24687;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#26500;&#24314;&#24182;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36807;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;1&#65289;&#25968;&#25454;&#21327;&#35843;&#30340;&#26377;&#25928;&#24615;&#27979;&#37327;&#26041;&#27861;&#65307;2&#65289;&#8220;&#35843;&#21644;&#22120;&#21464;&#21387;&#22120;&#8221;&#65292;&#21363;ComBat&#21327;&#35843;&#26041;&#27861;&#30340;&#19968;&#20010;&#23454;&#29616;&#65292;&#23427;&#20801;&#35768;&#23558;&#20854;&#23553;&#35013;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#65292;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;36&#20010;&#32593;&#31449;&#30340;1740&#21517;&#20581;&#24247;&#21463;&#35797;&#32773;&#30340;&#22823;&#33041;T1&#21152;&#26435;MRI&#25968;&#25454;&#26469;&#27979;&#35797;&#36825;&#20123;&#24037;&#20855;&#12290;&#32463;&#36807;&#21327;&#35843;&#21518;&#65292;&#32593;&#31449;&#25928;&#24212;&#34987;&#21024;&#38500;&#25110;&#20943;&#23569;&#20102;&#65292;
&lt;/p&gt;
&lt;p&gt;
Pooling publicly-available MRI data from multiple sites allows to assemble extensive groups of subjects, increase statistical power, and promote data reuse with machine learning techniques. The harmonization of multicenter data is necessary to reduce the confounding effect associated with non-biological sources of variability in the data. However, when applied to the entire dataset before machine learning, the harmonization leads to data leakage, because information outside the training set may affect model building, and potentially falsely overestimate performance. We propose a 1) measurement of the efficacy of data harmonization; 2) harmonizer transformer, i.e., an implementation of the ComBat harmonization allowing its encapsulation among the preprocessing steps of a machine learning pipeline, avoiding data leakage. We tested these tools using brain T1-weighted MRI data from 1740 healthy subjects acquired at 36 sites. After harmonization, the site effect was removed or reduced, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;LADDER&#65292;&#36825;&#26159;&#19968;&#20010;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#20013;&#25552;&#21462;&#25915;&#20987;&#27169;&#24335;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#23433;&#20840;&#20998;&#26512;&#24072;&#21450;&#26102;&#30830;&#23450;&#29616;&#26377;&#21644;&#26032;&#20852;&#23041;&#32961;&#30340;&#25915;&#20987;&#21521;&#37327;&#65292;&#20174;&#32780;&#20351;&#20182;&#20204;&#33021;&#22815;&#20027;&#21160;&#20934;&#22791;&#38450;&#24481;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2211.01753</link><description>&lt;p&gt;
&#36229;&#36234;IoCs&#65306;&#33258;&#21160;&#20174;&#22806;&#37096;CTI&#20013;&#25552;&#21462;&#25915;&#20987;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Looking Beyond IoCs: Automatically Extracting Attack Patterns from External CTI. (arXiv:2211.01753v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;LADDER&#65292;&#36825;&#26159;&#19968;&#20010;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#20013;&#25552;&#21462;&#25915;&#20987;&#27169;&#24335;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#23433;&#20840;&#20998;&#26512;&#24072;&#21450;&#26102;&#30830;&#23450;&#29616;&#26377;&#21644;&#26032;&#20852;&#23041;&#32961;&#30340;&#25915;&#20987;&#21521;&#37327;&#65292;&#20174;&#32780;&#20351;&#20182;&#20204;&#33021;&#22815;&#20027;&#21160;&#20934;&#22791;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#21644;&#21830;&#19994;&#32452;&#32455;&#24191;&#27867;&#20849;&#20139;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;(CTI)&#20197;&#38450;&#33539;&#29616;&#26377;&#21644;&#26032;&#20852;&#30340;&#32593;&#32476;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;CTI&#20027;&#35201;&#38598;&#20013;&#22312;&#36861;&#36394;&#24050;&#30693;&#30340;&#23041;&#32961;&#25351;&#26631;&#65288;&#22914;IP&#22320;&#22336;&#21644;&#22495;&#21517;&#65289;&#65292;&#21487;&#33021;&#22312;&#38450;&#24481;&#19981;&#26029;&#28436;&#21464;&#30340;&#25915;&#20987;&#26041;&#38754;&#25552;&#20379;&#38271;&#26399;&#20215;&#20540;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#23041;&#32961;&#24773;&#25253;&#20449;&#21495;&#65292;&#31216;&#20026;&#25915;&#20987;&#27169;&#24335;&#12290;LADDER&#26159;&#19968;&#20010;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;CTI&#25253;&#21578;&#20013;&#22823;&#35268;&#27169;&#25552;&#21462;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#27169;&#24335;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25429;&#25417;Android&#21644;&#20225;&#19994;&#32593;&#32476;&#20013;&#25915;&#20987;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23558;&#20854;&#31995;&#32479;&#22320;&#26144;&#23556;&#21040;MITRE ATT\&amp;CK&#27169;&#24335;&#26694;&#26550;&#65292;&#26469;&#23545;&#25915;&#20987;&#27169;&#24335;&#36827;&#34892;&#34920;&#24449;&#12290;LADDER&#21487;&#20197;&#34987;&#23433;&#20840;&#20998;&#26512;&#24072;&#29992;&#26469;&#30830;&#23450;&#19982;&#29616;&#26377;&#21644;&#26032;&#20852;&#23041;&#32961;&#30456;&#20851;&#30340;&#25915;&#20987;&#21521;&#37327;&#30340;&#23384;&#22312;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#20027;&#21160;&#20934;&#22791;&#38450;&#24481;&#25514;&#26045;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20960;&#20010;&#20351;&#29992;&#26696;&#20363;&#26469;&#28436;&#31034;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public and commercial organizations extensively share cyberthreat intelligence (CTI) to prepare systems to defend against existing and emerging cyberattacks. However, traditional CTI has primarily focused on tracking known threat indicators such as IP addresses and domain names, which may not provide long-term value in defending against evolving attacks. To address this challenge, we propose to use more robust threat intelligence signals called attack patterns. LADDER is a knowledge extraction framework that can extract text-based attack patterns from CTI reports at scale. The framework characterizes attack patterns by capturing the phases of an attack in Android and enterprise networks and systematically maps them to the MITRE ATT\&amp;CK pattern framework. LADDER can be used by security analysts to determine the presence of attack vectors related to existing and emerging threats, enabling them to prepare defenses proactively. We also present several use cases to demonstrate the applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LIME&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35299;&#37322;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#23450;&#20041;&#21152;&#26435;&#25439;&#22833;&#65292;&#23558;&#30495;&#23454;LIME&#35299;&#37322;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;LIME&#35299;&#37322;&#20043;&#38388;&#30340;&#36317;&#31163;&#32771;&#34385;&#22312;&#20869;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#23454;&#38469;&#35757;&#32451;&#22330;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#39034;&#24207;&#23398;&#20064;&#32780;&#19981;&#20002;&#22833;&#20808;&#21069;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.01413</link><description>&lt;p&gt;
&#21033;&#29992;&#35299;&#37322;&#30340;&#21147;&#37327;&#36827;&#34892;&#22686;&#37327;&#35757;&#32451;&#65306;&#22522;&#20110;LIME&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Explanations for Incremental Training: A LIME-Based Approach. (arXiv:2211.01413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LIME&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35299;&#37322;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#23450;&#20041;&#21152;&#26435;&#25439;&#22833;&#65292;&#23558;&#30495;&#23454;LIME&#35299;&#37322;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;LIME&#35299;&#37322;&#20043;&#38388;&#30340;&#36317;&#31163;&#32771;&#34385;&#22312;&#20869;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#23454;&#38469;&#35757;&#32451;&#22330;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#39034;&#24207;&#23398;&#20064;&#32780;&#19981;&#20002;&#22833;&#20808;&#21069;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#29702;&#35299;&#29305;&#24449;&#37325;&#35201;&#24615;&#24182;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#27934;&#23519;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#32467;&#26524;&#30340;&#35299;&#37322;&#22823;&#22810;&#23616;&#38480;&#20110;&#21487;&#35270;&#21270;&#65292;&#24182;&#19988;&#24456;&#23569;&#26377;&#30740;&#31350;&#23558;&#36825;&#20123;&#35299;&#37322;&#29992;&#20316;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#30340;&#21453;&#39304;&#12290;&#26412;&#30740;&#31350;&#23558;&#27169;&#22411;&#30340;&#35299;&#37322;&#21453;&#39304;&#21040;&#21069;&#39304;&#35757;&#32451;&#20013;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#21152;&#26435;&#25439;&#22833;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#36890;&#36807;&#32771;&#34385;&#30495;&#23454;LIME&#65288;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#65289;&#35299;&#37322;&#21644;&#27169;&#22411;&#39044;&#27979;&#30340;LIME&#35299;&#37322;&#20043;&#38388;&#30340;&#27431;&#24335;&#36317;&#31163;&#26469;&#29983;&#25104;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#38469;&#30340;&#35757;&#32451;&#22330;&#26223;&#20013;&#65292;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#24110;&#21161;&#27169;&#22411;&#39034;&#24207;&#23398;&#20064;&#32780;&#19981;&#20002;&#22833;&#20808;&#21069;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#19981;&#20250;&#19968;&#27425;&#24615;&#25552;&#20379;&#25152;&#26377;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#35813;&#26694;&#26550;&#23558;&#33258;&#23450;&#20041;&#21152;&#26435;&#25439;&#22833;&#19982;&#24377;&#24615;&#35757;&#32451;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability of neural network prediction is essential to understand feature importance and gain interpretable insight into neural network performance. However, explanations of neural network outcomes are mostly limited to visualization, and there is scarce work that looks to use these explanations as feedback to improve model performance. In this work, model explanations are fed back to the feed-forward training to help the model generalize better. To this extent, a custom weighted loss where the weights are generated by considering the Euclidean distances between true LIME (Local Interpretable Model-Agnostic Explanations) explanations and model-predicted LIME explanations is proposed. Also, in practical training scenarios, developing a solution that can help the model learn sequentially without losing information on previous data distribution is imperative due to the unavailability of all the training data at once. Thus, the framework incorporates the custom weighted loss with Elas
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#35299;&#30721;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20248;&#21270;&#23545;&#27604;&#30446;&#26631;&#26469;&#29983;&#25104;&#21512;&#29702;&#19988;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#19982;&#20165;&#20351;&#29992;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#30456;&#27604;&#65292;&#23545;&#27604;&#35299;&#30721;&#33021;&#22815;&#36991;&#20813;&#30701;&#32780;&#37325;&#22797;&#30340;&#25991;&#26412;&#21644;&#19981;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#12290;</title><link>http://arxiv.org/abs/2210.15097</link><description>&lt;p&gt;
&#23545;&#27604;&#35299;&#30721;&#65306;&#23558;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#35270;&#20026;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15097
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35299;&#30721;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20248;&#21270;&#23545;&#27604;&#30446;&#26631;&#26469;&#29983;&#25104;&#21512;&#29702;&#19988;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#19982;&#20165;&#20351;&#29992;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#30456;&#27604;&#65292;&#23545;&#27604;&#35299;&#30721;&#33021;&#22815;&#36991;&#20813;&#30701;&#32780;&#37325;&#22797;&#30340;&#25991;&#26412;&#21644;&#19981;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#65292;&#26368;&#22823;&#27010;&#29575;&#26159;&#24320;&#25918;&#24335;&#29983;&#25104;&#30340;&#36739;&#24046;&#35299;&#30721;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#20250;&#20135;&#29983;&#30701;&#32780;&#37325;&#22797;&#30340;&#25991;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#37319;&#26679;&#24448;&#24448;&#20250;&#20135;&#29983;&#19982;&#21407;&#22987;&#20027;&#39064;&#20559;&#31163;&#30340;&#19981;&#36830;&#36143;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#22312;&#28385;&#36275;&#21512;&#29702;&#24615;&#32422;&#26463;&#26465;&#20214;&#30340;&#21069;&#25552;&#19979;&#20248;&#21270;&#23545;&#27604;&#30446;&#26631;&#12290;&#23545;&#27604;&#30446;&#26631;&#36820;&#22238;&#19968;&#20010;&#22823;&#22411;LM&#65288;&#34987;&#31216;&#20026;&#19987;&#23478;&#65292;&#20363;&#22914;OPT-13B&#65289;&#21644;&#19968;&#20010;&#23567;&#22411;LM&#65288;&#34987;&#31216;&#20026;&#19994;&#20313;&#32773;&#65292;&#20363;&#22914;OPT-125M&#65289;&#20043;&#38388;&#30340;&#20284;&#28982;&#24046;&#24322;&#65292;&#24182;&#19988;&#32422;&#26463;&#26465;&#20214;&#30830;&#20445;&#36755;&#20986;&#26159;&#21512;&#29702;&#30340;&#12290;CD&#30340;&#28789;&#24863;&#26469;&#33258;&#20110;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#36739;&#22823;&#30340;LM&#65288;&#20363;&#22914;&#37325;&#22797;&#12289;&#19981;&#36830;&#36143;&#65289;&#22312;&#36739;&#23567;&#30340;LM&#20013;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#36825;&#31181;&#24046;&#24322;&#34920;&#26126;&#21738;&#20123;&#25991;&#26412;&#24212;&#20248;&#20808;&#32771;&#34385;&#12290;CD&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#22521;&#35757;&#65292;&#24182;&#19988;&#27604;&#20165;&#20174;&#36739;&#22823;&#30340;LM&#36827;&#34892;&#35299;&#30721;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;&#23427;&#36824;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#65288;OPT-13B&#21644;GPT2-1.5B&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#25311;&#21512;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#22312;&#36924;&#36817;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.13601</link><description>&lt;p&gt;
&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Single Neuron Models with Lipschitz Non-Linearities. (arXiv:2210.13601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13601
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#25311;&#21512;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#22312;&#36924;&#36817;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#23545;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#26377;&#26102;&#20063;&#34987;&#31216;&#20026;&#8220;&#23725;&#20989;&#25968;&#8221;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#24191;&#27867;&#26377;&#25928;&#22320;&#27169;&#25311;&#29289;&#29702;&#29616;&#35937;&#65292;&#24182;&#26500;&#24314;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20195;&#29702;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20855;&#26377;&#20219;&#20309;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#65288;&#22914;ReLU&#20989;&#25968;&#12289;sigmoid&#20989;&#25968;&#12289;&#32477;&#23545;&#20540;&#20989;&#25968;&#12289;&#20302;&#27425;&#22810;&#39033;&#24335;&#20989;&#25968;&#31561;&#65289;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#24050;&#30693;&#30340;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#25311;&#21512;&#8220;&#32447;&#24615;&#20989;&#25968;&#8221;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#21487;&#35777;&#26126;&#30340;&#36924;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of active learning for single neuron models, also sometimes called ``ridge functions'', in the agnostic setting (under adversarial label noise). Such models have been shown to be broadly effective in modeling physical phenomena, and for constructing surrogate data-driven models for partial differential equations.  Surprisingly, we show that for a single neuron model with any Lipschitz non-linearity (such as the ReLU, sigmoid, absolute value, low-degree polynomial, among others), strong provable approximation guarantees can be obtained using a well-known active learning strategy for fitting \emph{linear functions} in the agnostic setting. % -- i.e. for the case when there is no non-linearity. Namely, we can collect samples via statistical \emph{leverage score sampling}, which has been shown to be near-optimal in other active learning scenarios. We support our theoretical results with empirical simulations showing that our proposed active learning strategy based o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#33258;&#36866;&#24212;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.09452</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#30340;&#33258;&#36866;&#24212;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning. (arXiv:2210.09452v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#33258;&#36866;&#24212;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21482;&#26377;&#34955;&#32423;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#21333;&#20010;&#23454;&#20363;&#30340;&#34920;&#31034;&#26159;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#23545;&#27604;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;CSSL&#65289;&#21487;&#20197;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;CSSL&#23398;&#20064;&#23558;&#23545;&#24212;&#20110;&#20004;&#20010;&#19981;&#21516;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20363;&#30340;&#34920;&#31034;&#25512;&#24320;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#36890;&#24120;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#22240;&#27492;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20363;&#22823;&#37096;&#20998;&#23646;&#20110;&#21516;&#19968;&#20010;&#22810;&#25968;&#31867;&#21035;&#65292;&#36825;&#20351;&#24471;CSSL&#26080;&#27861;&#23398;&#20064;&#31867;&#38388;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#36845;&#20195;&#33258;&#36866;&#24212;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22810;&#23454;&#20363;&#34920;&#31034;&#65288;ItS2CLR&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#21253;&#32423;&#26631;&#31614;&#27966;&#29983;&#30340;&#23454;&#20363;&#32423;&#20266;&#26631;&#31614;&#26469;&#25913;&#21892;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22810;&#23454;&#20363;&#34920;&#31034;&#30340;&#31574;&#30053;&#26469;&#30830;&#20445;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;ItS2CLR&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations for individual instances when only bag-level labels are available is a fundamental challenge in multiple instance learning (MIL). Recent works have shown promising results using contrastive self-supervised learning (CSSL), which learns to push apart representations corresponding to two different randomly-selected instances. Unfortunately, in real-world applications such as medical image classification, there is often class imbalance, so randomly-selected instances mostly belong to the same majority class, which precludes CSSL from learning inter-class differences. To address this issue, we propose a novel framework, Iterative Self-paced Supervised Contrastive Learning for MIL Representations (ItS2CLR), which improves the learned representation by exploiting instance-level pseudo labels derived from the bag-level labels. The framework employs a novel self-paced sampling strategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three medical datase
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#27979;&#35797;&#20219;&#21153;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#34892;&#20026;&#36866;&#24212;&#65292;&#25552;&#20379;&#20102;&#23545;&#20998;&#24067;&#36716;&#21464;&#30340;&#25913;&#21892;&#36951;&#25022;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.03104</link><description>&lt;p&gt;
&#20998;&#24067;&#33258;&#36866;&#24212;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributionally Adaptive Meta Reinforcement Learning. (arXiv:2210.03104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#27979;&#35797;&#20219;&#21153;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#34892;&#20026;&#36866;&#24212;&#65292;&#25552;&#20379;&#20102;&#23545;&#20998;&#24067;&#36716;&#21464;&#30340;&#25913;&#21892;&#36951;&#25022;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26469;&#33719;&#21462;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#22870;&#21169;&#25110;&#21160;&#21147;&#23398;&#20989;&#25968;&#30340;&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#24471;&#21040;&#30340;&#20803;&#31574;&#30053;&#36890;&#24120;&#21482;&#22312;&#20854;&#35757;&#32451;&#26102;&#30340;&#31934;&#30830;&#20219;&#21153;&#20998;&#24067;&#19978;&#26377;&#25928;&#65292;&#22312;&#27979;&#35797;&#26102;&#30340;&#22870;&#21169;&#25110;&#36807;&#28193;&#21160;&#21147;&#23398;&#30340;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#20250;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20219;&#21153;&#31354;&#38388;&#20013;&#36866;&#24212;&#24615;&#22320;&#24212;&#23545;&#27979;&#35797;&#26102;&#30340;&#20998;&#24067;&#36716;&#21464;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20197;&#33258;&#36866;&#24212;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#26041;&#27861;&#20026;&#26680;&#24515;&#65292;&#35757;&#32451;&#19968;&#32676;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#20998;&#24067;&#36716;&#21464;&#40065;&#26834;&#24615;&#30340;&#20803;&#31574;&#30053;&#12290;&#24403;&#22312;&#21487;&#33021;&#21457;&#29983;&#20998;&#24067;&#36716;&#21464;&#30340;&#27979;&#35797;&#20219;&#21153;&#20998;&#24067;&#19978;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36873;&#25321;&#20855;&#26377;&#26368;&#21512;&#36866;&#40065;&#26834;&#24615;&#27700;&#24179;&#30340;&#20803;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#23427;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20998;&#24067;&#36716;&#21464;&#19979;&#33021;&#22815;&#25913;&#21892;&#36951;&#25022;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-reinforcement learning algorithms provide a data-driven way to acquire policies that quickly adapt to many tasks with varying rewards or dynamics functions. However, learned meta-policies are often effective only on the exact task distribution on which they were trained and struggle in the presence of distribution shift of test-time rewards or transition dynamics. In this work, we develop a framework for meta-RL algorithms that are able to behave appropriately under test-time distribution shifts in the space of tasks. Our framework centers on an adaptive approach to distributional robustness that trains a population of meta-policies to be robust to varying levels of distribution shift. When evaluated on a potentially shifted test-time distribution of tasks, this allows us to choose the meta-policy with the most appropriate level of robustness, and use it to perform fast adaptation. We formally show how our framework allows for improved regret under distribution shift, and empirica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#23481;&#37327;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#37325;&#35201;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#22810;&#20041;&#24615;&#29616;&#35937;&#22312;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20998;&#22359;&#21322;&#27491;&#20132;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.01892</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#21644;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Polysemanticity and Capacity in Neural Networks. (arXiv:2210.01892v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#23481;&#37327;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#37325;&#35201;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#22810;&#20041;&#24615;&#29616;&#35937;&#22312;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20998;&#22359;&#21322;&#27491;&#20132;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#36890;&#24120;&#20195;&#34920;&#26080;&#20851;&#29305;&#24449;&#30340;&#28151;&#21512;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#22810;&#20041;&#24615;&#65292;&#20351;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;&#20854;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#29305;&#24449;&#23481;&#37327;&#30340;&#35270;&#35282;&#26469;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#29305;&#24449;&#23481;&#37327;&#26159;&#27599;&#20010;&#29305;&#24449;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#21344;&#29992;&#30340;&#20998;&#24418;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#29609;&#20855;&#27169;&#22411;&#20013;&#65292;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65288;&#19982;&#20854;&#23545;&#25439;&#22833;&#30340;&#24433;&#21709;&#25104;&#27604;&#20363;&#65289;&#65292;&#24182;&#23436;&#20840;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#24403;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#65292;&#22810;&#20041;&#24615;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#22312;&#24471;&#21040;&#26368;&#20248;&#23481;&#37327;&#20998;&#37197;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23884;&#20837;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20998;&#22359;&#21322;&#27491;&#20132;&#30340;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. We propose doing so through the lens of feature \emph{capacity}, which is the fractional dimension each feature consumes in the embedding space. We show that in a toy model the optimal capacity allocation tends to monosemantically represent the most important features, polysemantically represent less important features (in proportion to their impact on the loss), and entirely ignore the least important features. Polysemanticity is more prevalent when the inputs have higher kurtosis or sparsity and more prevalent in some architectures than others. Given an optimal allocation of capacity, we go on to study the geometry of the embedding space. We find a block-semi-orthogonal structure, with differing block sizes in different models, highlighting the impact of model archit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#21644;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2210.01212</link><description>&lt;p&gt;
&#36890;&#36807;&#20887;&#20313;&#24615;&#23454;&#29616;&#31232;&#30095;&#24615;&#65306;&#29992;SGD&#27714;&#35299;$L_1$
&lt;/p&gt;
&lt;p&gt;
Sparsity by Redundancy: Solving $L_1$ with SGD. (arXiv:2210.01212v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#21644;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method called "spred" to minimize a generic differentiable loss function with $L_1$ penalty using redundant reparametrization and straightforward stochastic gradient descent. It is an exact solver of $L_1$ and can be used to train sparse neural networks for gene selection tasks and neural network compression tasks, bridging the gap between sparsity in deep learning and conventional statistical learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26159;$L_1$&#24809;&#32602;&#31561;&#20215;&#20110;&#24102;&#26377;&#26435;&#37325;&#34928;&#20943;&#30340;&#21487;&#24494;&#37325;&#21442;&#25968;&#21270;&#30340;&#30452;&#25509;&#25512;&#24191;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21363;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#23545;&#20110;&#36890;&#29992;&#30340;&#38750;&#20984;&#20989;&#25968;&#65292;&#37325;&#21442;&#25968;&#21270;&#25216;&#24039;&#26159;&#23436;&#20840;&#8220;&#33391;&#24615;&#8221;&#30340;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;(1)&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#22312;&#38750;&#24120;&#39640;&#32500;&#31354;&#38388;&#20013;&#25214;&#21040;&#30456;&#20851;&#29305;&#24449;&#65292;&#20197;&#21450;(2)&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#20808;&#21069;&#23581;&#35797;&#24212;&#29992;$L_1$&#24809;&#32602;&#30340;&#26041;&#27861;&#22343;&#26410;&#25104;&#21151;&#12290;&#20174;&#27010;&#24565;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to minimize a generic differentiable loss function with $L_1$ penalty with a redundant reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of a series of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \textit{spred}, is an exact solver of $L_1$ and that the reparametrization trick is completely ``benign" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26080;&#31351;&#23567;&#26799;&#24230;&#25552;&#21319;&#22312;&#22823;&#26679;&#26412;&#26497;&#38480;&#19979;&#30340;&#28176;&#36817;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#21040;&#19968;&#20010;&#30830;&#23450;&#24615;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20351;&#24471;&#27979;&#35797;&#35823;&#24046;&#20943;&#23567;&#30340;&#21160;&#21147;&#23398;&#20197;&#21450;&#20854;&#38271;&#26102;&#38388;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2210.00736</link><description>&lt;p&gt;
&#26080;&#31351;&#23567;&#26799;&#24230;&#25552;&#21319;&#30340;&#22823;&#26679;&#26412;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A large sample theory for infinitesimal gradient boosting. (arXiv:2210.00736v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26080;&#31351;&#23567;&#26799;&#24230;&#25552;&#21319;&#22312;&#22823;&#26679;&#26412;&#26497;&#38480;&#19979;&#30340;&#28176;&#36817;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#21040;&#19968;&#20010;&#30830;&#23450;&#24615;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20351;&#24471;&#27979;&#35797;&#35823;&#24046;&#20943;&#23567;&#30340;&#21160;&#21147;&#23398;&#20197;&#21450;&#20854;&#38271;&#26102;&#38388;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#31351;&#23567;&#26799;&#24230;&#25552;&#21319;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#27969;&#34892;&#30340;&#22522;&#20110;&#26641;&#30340;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#30340;&#28040;&#22833;&#23398;&#20064;&#29575;&#26497;&#38480;&#12290;&#23427;&#34987;&#23450;&#20041;&#20026;&#22312;&#26080;&#31351;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#38750;&#32447;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#20854;&#20013;&#39537;&#21160;&#21160;&#21147;&#23398;&#30340;&#26080;&#31351;&#23567;&#25552;&#21319;&#31639;&#23376;&#20381;&#36182;&#20110;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#22823;&#26679;&#26412;&#26497;&#38480;&#19979;&#30340;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#21040;&#19968;&#20010;&#30830;&#23450;&#24615;&#36807;&#31243;&#12290;&#36825;&#20010;&#31181;&#32676;&#26497;&#38480;&#20877;&#27425;&#34987;&#19968;&#20010;&#20381;&#36182;&#20110;&#31181;&#32676;&#20998;&#24067;&#30340;&#24494;&#20998;&#26041;&#31243;&#25152;&#25551;&#36848;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20010;&#31181;&#32676;&#26497;&#38480;&#30340;&#19968;&#20123;&#24615;&#36136;&#65306;&#25105;&#20204;&#35777;&#26126;&#20102;&#21160;&#21147;&#23398;&#20351;&#24471;&#27979;&#35797;&#35823;&#24046;&#20943;&#23567;&#65292;&#24182;&#32771;&#34385;&#20102;&#23427;&#22312;&#38271;&#26102;&#38388;&#34892;&#20026;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infinitesimal gradient boosting (Dombry and Duchamps, 2021) is defined as the vanishing-learning-rate limit of the popular tree-based gradient boosting algorithm from machine learning. It is characterized as the solution of a nonlinear ordinary differential equation in a infinite-dimensional function space where the infinitesimal boosting operator driving the dynamics depends on the training sample. We consider the asymptotic behavior of the model in the large sample limit and prove its convergence to a deterministic process. This population limit is again characterized by a differential equation that depends on the population distribution. We explore some properties of this population limit: we prove that the dynamics makes the test error decrease and we consider its long time behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32511;&#33394;&#37327;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#26377;&#38480;&#31934;&#24230;&#32423;&#21035;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#38024;&#23545;&#33021;&#37327;&#28040;&#32791;&#21644;&#36890;&#20449;&#36718;&#27425;&#25968;&#37327;&#30340;&#26368;&#23567;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#25512;&#23548;&#20102;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2207.09387</link><description>&lt;p&gt;
&#32511;&#33394;&#65292;&#37327;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#32852;&#21512;&#23398;&#20064;&#65306;&#19968;&#31181;&#33410;&#33021;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Green, Quantized Federated Learning over Wireless Networks: An Energy-Efficient Design. (arXiv:2207.09387v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32511;&#33394;&#37327;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#26377;&#38480;&#31934;&#24230;&#32423;&#21035;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#38024;&#23545;&#33021;&#37327;&#28040;&#32791;&#21644;&#36890;&#20449;&#36718;&#27425;&#25968;&#37327;&#30340;&#26368;&#23567;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#25512;&#23548;&#20102;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32511;&#33394;&#37327;&#21270;&#30340;&#26080;&#32447;&#32593;&#32476;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#26412;&#22320;&#35757;&#32451;&#21644;&#19978;&#34892;&#20256;&#36755;&#20013;&#20351;&#29992;&#26377;&#38480;&#31934;&#24230;&#32423;&#21035;&#26469;&#34920;&#31034;&#25968;&#25454;&#12290;&#36825;&#37324;&#65292;&#26377;&#38480;&#31934;&#24230;&#32423;&#21035;&#36890;&#36807;&#20351;&#29992;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#26469;&#23454;&#29616;&#65292;&#35813;&#32593;&#32476;&#20197;&#22266;&#23450;&#31934;&#24230;&#26684;&#24335;&#37327;&#21270;&#26435;&#37325;&#21644;&#28608;&#27963;&#20989;&#25968;&#12290;&#22312;&#32771;&#34385;&#30340;&#32852;&#21512;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#27599;&#20010;&#35774;&#22791;&#35757;&#32451;&#20854;QNN&#24182;&#23558;&#37327;&#21270;&#30340;&#35757;&#32451;&#32467;&#26524;&#20256;&#36755;&#32473;&#22522;&#31449;&#12290;&#26412;&#25991;&#20005;&#26684;&#25512;&#23548;&#20102;&#26412;&#22320;&#35757;&#32451;&#21644;&#37327;&#21270;&#20256;&#36755;&#30340;&#33021;&#37327;&#27169;&#22411;&#12290;&#20026;&#20102;&#21516;&#26102;&#26368;&#23567;&#21270;&#33021;&#37327;&#28040;&#32791;&#21644;&#36890;&#20449;&#36718;&#27425;&#25968;&#37327;&#65292;&#26412;&#25991;&#21046;&#23450;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#26412;&#22320;&#36845;&#20195;&#27425;&#25968;&#12289;&#36873;&#23450;&#35774;&#22791;&#25968;&#37327;&#20197;&#21450;&#26412;&#22320;&#35757;&#32451;&#21644;&#20256;&#36755;&#30340;&#31934;&#24230;&#32423;&#21035;&#65292;&#24182;&#30830;&#20445;&#22312;&#30446;&#26631;&#20934;&#30830;&#24230;&#32422;&#26463;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#36824;&#20998;&#26512;&#25512;&#23548;&#20102;&#25152;&#25552;&#20986;&#30340;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a green-quantized FL framework, which represents data with a finite precision level in both local training and uplink transmission, is proposed. Here, the finite precision level is captured through the use of quantized neural networks (QNNs) that quantize weights and activations in fixed-precision format. In the considered FL model, each device trains its QNN and transmits a quantized training result to the base station. Energy models for the local training and the transmission with quantization are rigorously derived. To minimize the energy consumption and the number of communication rounds simultaneously, a multi-objective optimization problem is formulated with respect to the number of local iterations, the number of selected devices, and the precision levels for both local training and transmission while ensuring convergence under a target accuracy constraint. To solve this problem, the convergence rate of the proposed FL system is analytically derived with respect t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;BDM&#65288;&#30450;&#28857;&#21457;&#29616;&#26041;&#27861;&#65289;&#35780;&#20272;&#26694;&#26550;SpotCheck&#21644;&#19968;&#20010;&#20351;&#29992;2D&#22270;&#20687;&#34920;&#31034;&#30340;BDMPlaneSpot&#12290;&#23454;&#39564;&#32467;&#26524;&#32473;&#20986;&#24433;&#21709;BDM&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#35777;&#26126;PlaneSpot&#19982;&#29616;&#26377;&#30340;BDM&#30456;&#31454;&#20105;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2207.04104</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#27169;&#22411;&#30340;&#30450;&#28857;&#21457;&#29616;&#20013;&#26397;&#30528;&#26356;&#21152;&#20005;&#35880;&#30340;&#31185;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards a More Rigorous Science of Blindspot Discovery in Image Models. (arXiv:2207.04104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;BDM&#65288;&#30450;&#28857;&#21457;&#29616;&#26041;&#27861;&#65289;&#35780;&#20272;&#26694;&#26550;SpotCheck&#21644;&#19968;&#20010;&#20351;&#29992;2D&#22270;&#20687;&#34920;&#31034;&#30340;BDMPlaneSpot&#12290;&#23454;&#39564;&#32467;&#26524;&#32473;&#20986;&#24433;&#21709;BDM&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#35777;&#26126;PlaneSpot&#19982;&#29616;&#26377;&#30340;BDM&#30456;&#31454;&#20105;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#30450;&#28857;&#21457;&#29616;&#26041;&#27861;&#65288;&#8220;BDM&#8221;&#65289;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#22270;&#20687;&#23884;&#20837;&#26469;&#26597;&#25214;&#25968;&#25454;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#23376;&#38598;&#65292;&#22312;&#36825;&#20123;&#23376;&#38598;&#20013;&#22270;&#20687;&#20998;&#31867;&#22120;&#34920;&#29616;&#26174;&#33879;&#26356;&#24046;&#65288;&#21363;&#23384;&#22312;&#30450;&#28857;&#65289;&#12290;&#21463;&#21040;&#20043;&#21069;&#24037;&#20316;&#20013;&#35266;&#23519;&#21040;&#30340;&#24046;&#36317;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;BDM&#35780;&#20272;&#26694;&#26550;&#65288;SpotCheck&#65289;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20855;&#26377;&#24050;&#30693;&#30450;&#28857;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20351;&#29992;&#26032;&#30340;BDM&#65288;PlaneSpot&#65289;&#26469;&#20351;&#29992;2D&#22270;&#20687;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;SpotCheck&#36827;&#34892;&#21463;&#25511;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#24433;&#21709;BDM&#24615;&#33021;&#30340;&#22240;&#32032;&#65288;&#20363;&#22914;&#27169;&#22411;&#20013;&#30340;&#30450;&#28857;&#25968;&#37327;&#25110;&#23450;&#20041;&#30450;&#28857;&#30340;&#29305;&#24449;&#65289;&#65292;&#24182;&#34920;&#26126;PlaneSpot&#19982;&#29616;&#26377;&#30340;BDM&#30456;&#31454;&#20105;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#39069;&#22806;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;&#36825;&#20123;&#21457;&#29616;&#65292;&#20351;&#29992;MS-COCO&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#26410;&#26469;&#22312;BDM&#35774;&#35745;&#26041;&#38754;&#25552;&#20986;&#20102;&#20960;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of work studies Blindspot Discovery Methods ("BDM"s): methods that use an image embedding to find semantically meaningful (i.e., united by a human-understandable concept) subsets of the data where an image classifier performs significantly worse. Motivated by observed gaps in prior work, we introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic image datasets to train models with known blindspots and a new BDM, PlaneSpot, that uses a 2D image representation. We use SpotCheck to run controlled experiments that identify factors that influence BDM performance (e.g., the number of blindspots in a model, or features used to define the blindspot) and show that PlaneSpot is competitive with and in many cases outperforms existing BDMs. Importantly, we validate these findings by designing additional experiments that use real image data from MS-COCO, a large image benchmark dataset. Our findings suggest several promising directions for future work on BDM des
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#21160;&#24577;&#22343;&#22330;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#38480;&#29366;&#24577;&#21644;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#27169;&#25311;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#36125;&#23572;&#26364;&#26041;&#31243;&#20316;&#20026;&#19968;&#31181;&#26080;&#24207;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#22343;&#22330;&#26041;&#31243;&#35745;&#31639;&#29366;&#24577;&#34892;&#20026;&#20540;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2206.05200</link><description>&lt;p&gt;
&#21160;&#24577;&#22343;&#22330;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dynamic mean field programming. (arXiv:2206.05200v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#21160;&#24577;&#22343;&#22330;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#38480;&#29366;&#24577;&#21644;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#27169;&#25311;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#36125;&#23572;&#26364;&#26041;&#31243;&#20316;&#20026;&#19968;&#31181;&#26080;&#24207;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#22343;&#22330;&#26041;&#31243;&#35745;&#31639;&#29366;&#24577;&#34892;&#20026;&#20540;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#38480;&#21046;&#19979;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#21160;&#24577;&#22343;&#22330;&#29702;&#35770;&#65292;&#29992;&#20110;&#26377;&#38480;&#29366;&#24577;&#21644;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#12290;&#31867;&#27604;&#20110;&#32479;&#35745;&#29289;&#29702;&#23398;&#65292;&#23545;&#36125;&#23572;&#26364;&#26041;&#31243;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#23558;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36716;&#31227;&#27010;&#29575;&#35299;&#37322;&#20026;&#32806;&#21512;&#65292;&#23558;&#20540;&#20989;&#25968;&#35299;&#37322;&#20026;&#21160;&#24577;&#28436;&#21270;&#30340;&#30830;&#23450;&#24615;&#33258;&#26059;&#12290;&#22240;&#27492;&#65292;&#24179;&#22343;&#22238;&#25253;&#21644;&#36716;&#31227;&#27010;&#29575;&#34987;&#35748;&#20026;&#26159;&#28140;&#28781;&#38543;&#26426;&#21464;&#37327;&#12290;&#35813;&#29702;&#35770;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#22312;&#28176;&#36817;&#29366;&#24577;&#31354;&#38388;&#26497;&#38480;&#19979;&#65292;&#29366;&#24577;&#34892;&#20026;&#20540;&#22312;&#29366;&#24577;&#34892;&#20026;&#23545;&#20043;&#38388;&#20855;&#26377;&#32479;&#35745;&#29420;&#31435;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#20999;&#30340;&#20998;&#24067;&#24418;&#24335;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#26377;&#38480;&#21644;&#26080;&#38480;&#25240;&#29616;&#26102;&#38388;&#35270;&#37326;&#65292;&#22312;&#20215;&#20540;&#36845;&#20195;&#21644;&#31574;&#30053;&#35780;&#20272;&#20013;&#22343;&#25104;&#31435;&#12290;&#29366;&#24577;&#34892;&#20026;&#20540;&#30340;&#32479;&#35745;&#20449;&#24687;&#21487;&#20197;&#20174;&#19968;&#32452;&#22343;&#22330;&#26041;&#31243;&#20013;&#35745;&#31639;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21160;&#24577;&#22343;&#22330;&#35268;&#21010;&#65288;DMFP&#65289;&#12290;&#23545;&#20110;&#31574;&#30053;&#35780;&#20272;&#65292;&#21487;&#20197;&#20351;&#29992;&#26399;&#26395;&#20540;&#36845;&#20195;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A dynamic mean field theory is developed for finite state and action Bayesian reinforcement learning in the large state space limit. In an analogy with statistical physics, the Bellman equation is studied as a disordered dynamical system; the Markov decision process transition probabilities are interpreted as couplings and the value functions as deterministic spins that evolve dynamically. Thus, the mean-rewards and transition probabilities are considered to be quenched random variables. The theory reveals that, under certain assumptions, the state-action values are statistically independent across state-action pairs in the asymptotic state space limit, and provides the form of the distribution exactly. The results hold in the finite and discounted infinite horizon settings, for both value iteration and policy evaluation. The state-action value statistics can be computed from a set of mean field equations, which we call dynamic mean field programming (DMFP). For policy evaluation the e
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#24403;&#21069;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#21516;&#26102;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#19982;&#20854;&#20182;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.05173</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65306;&#32508;&#36848;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Anomaly Detection: A Survey and Outlook. (arXiv:2205.05173v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05173
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#24403;&#21069;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#21516;&#26102;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#19982;&#20854;&#20182;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#32593;&#32476;&#23433;&#20840;&#12289;&#37329;&#34701;&#21644;&#21307;&#30103;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#35782;&#21035;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#27169;&#24335;&#25110;&#20107;&#20214;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26174;&#33879;&#22686;&#38271;&#20351;&#24471;&#22312;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#32508;&#36848;&#24403;&#21069;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#19982;&#20854;&#20182;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#23545;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#21253;&#25324;&#24320;&#21457;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#31561;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays a crucial role in various domains, including cybersecurity, finance, and healthcare, by identifying patterns or events that deviate from normal behaviour. In recent years, significant progress has been made in this field due to the remarkable growth of deep learning models. Notably, the advent of self-supervised learning has sparked the development of novel AD algorithms that outperform the existing state-of-the-art approaches by a considerable margin. This paper aims to provide a comprehensive review of the current methodologies in self-supervised anomaly detection. We present technical details of the standard methods and discuss their strengths and drawbacks. We also compare the performance of these models against each other and other state-of-the-art anomaly detection models. Finally, the paper concludes with a discussion of future directions for self-supervised anomaly detection, including the development of more effective and efficient algorithms and t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#35299;&#32544;&#30340;&#27493;&#24577;&#34920;&#31034;&#26469;&#35299;&#20915;&#22235;&#36275;&#26426;&#22120;&#20154;&#27493;&#24577;&#21442;&#25968;&#26080;&#27861;&#22312;&#31354;&#20013;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#30340;&#36830;&#32493;&#21512;&#25104;&#27493;&#24577;&#65292;&#22686;&#21152;&#20102;&#25511;&#21046;&#22120;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.01179</link><description>&lt;p&gt;
VAE-Loco&#65306;&#36890;&#36807;&#23398;&#20064;&#35299;&#32544;&#36367;&#27493;&#34920;&#31034;&#23454;&#29616;&#22810;&#21151;&#33021;&#22235;&#36275;&#21160;&#24577;&#27493;&#24577;
&lt;/p&gt;
&lt;p&gt;
VAE-Loco: Versatile Quadruped Locomotion by Learning a Disentangled Gait Representation. (arXiv:2205.01179v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#35299;&#32544;&#30340;&#27493;&#24577;&#34920;&#31034;&#26469;&#35299;&#20915;&#22235;&#36275;&#26426;&#22120;&#20154;&#27493;&#24577;&#21442;&#25968;&#26080;&#27861;&#22312;&#31354;&#20013;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#30340;&#36830;&#32493;&#21512;&#25104;&#27493;&#24577;&#65292;&#22686;&#21152;&#20102;&#25511;&#21046;&#22120;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#36275;&#21160;&#24577;&#27493;&#24577;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#23454;&#29616;&#39640;&#24230;&#21160;&#24577;&#30340;&#26426;&#21160;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35268;&#21010;&#22120;&#26080;&#27861;&#22312;&#31354;&#20013;&#25913;&#21464;&#25670;&#21160;&#33050;&#30340;&#20851;&#38190;&#27493;&#24577;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23398;&#20064;&#25429;&#25417;&#26500;&#25104;&#29305;&#23450;&#27493;&#24577;&#30340;&#20851;&#38190;&#25903;&#25745;&#38454;&#27573;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21487;&#20197;&#22686;&#21152;&#25511;&#21046;&#22120;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#22312;&#21333;&#19968;&#39536;&#27493;&#39118;&#26684;&#19978;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#35813;&#27169;&#22411;&#40723;&#21169;&#35299;&#32544;&#65292;&#20351;&#24471;&#23545;&#28508;&#22312;&#29366;&#24577;&#30340;&#21333;&#19968;&#32500;&#24230;&#26045;&#21152;&#39537;&#21160;&#20449;&#21495;&#33021;&#22815;&#32508;&#21512;&#36830;&#32493;&#22810;&#26679;&#21270;&#30340;&#39536;&#27493;&#39118;&#26684;&#21512;&#25104;&#20840;&#38754;&#24615;&#35745;&#21010;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39537;&#21160;&#20449;&#21495;&#30340;&#29305;&#23450;&#23646;&#24615;&#30452;&#25509;&#26144;&#23556;&#21040;&#27493;&#24577;&#21442;&#25968;&#65292;&#22914;&#33410;&#22863;&#12289;&#33050;&#27493;&#39640;&#24230;&#21644;&#20840;&#38454;&#27573;&#25345;&#32493;&#26102;&#38388;&#12290;&#30001;&#20110;&#25105;&#20204;&#26041;&#27861;&#30340;&#29305;&#28857;&#65292;&#36825;&#20123;&#21512;&#25104;&#30340;&#27493;&#24577;&#22312;&#26426;&#22120;&#20154;&#36816;&#34892;&#36807;&#31243;&#20013;&#21487;&#20197;&#22312;&#32447;&#25345;&#32493;&#21464;&#21270;&#12290;&#29983;&#25104;&#27169;&#22411;&#30340;&#20351;&#29992;&#26377;&#21161;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quadruped locomotion is rapidly maturing to a degree where robots are able to realise highly dynamic manoeuvres. However, current planners are unable to vary key gait parameters of the in-swing feet midair. In this work we address this limitation and show that it is pivotal in increasing controller robustness by learning a latent space capturing the key stance phases constituting a particular gait. This is achieved via a generative model trained on a single trot style, which encourages disentanglement such that application of a drive signal to a single dimension of the latent state induces holistic plans synthesising a continuous variety of trot styles. We demonstrate that specific properties of the drive signal map directly to gait parameters such as cadence, footstep height and full stance duration. Due to the nature of our approach these synthesised gaits are continuously variable online during robot operation. The use of a generative model facilitates the detection and mitigation o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#23567;&#22411;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#39640;&#32500;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#22320;&#22312;&#25968;&#23383;&#35782;&#21035;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#21033;&#29992;&#26356;&#23569;&#30340;&#30005;&#36335;&#35780;&#20272;&#26469;&#36817;&#20284;&#36739;&#22823;&#30005;&#36335;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2203.13739</link><description>&lt;p&gt;
&#20351;&#29992;&#23567;&#22411;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#39640;&#32500;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High Dimensional Quantum Machine Learning With Small Quantum Computers. (arXiv:2203.13739v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#23567;&#22411;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#39640;&#32500;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#22320;&#22312;&#25968;&#23383;&#35782;&#21035;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#21033;&#29992;&#26356;&#23569;&#30340;&#30005;&#36335;&#35780;&#20272;&#26469;&#36817;&#20284;&#36739;&#22823;&#30005;&#36335;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26426;&#23545;&#20110;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#38480;&#21046;&#20102;&#36825;&#19968;&#28508;&#21147;&#30340;&#23454;&#29616;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38480;&#21046;&#65292;&#21487;&#20197;&#37319;&#29992;&#19968;&#31181;&#25216;&#26415;&#65292;&#22312;&#27604;&#30005;&#36335;&#25152;&#38656;&#30340;&#27604;&#29305;&#23569;&#30340;&#26426;&#22120;&#19978;&#35780;&#20272;&#37327;&#23376;&#30005;&#36335;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#36807;&#22312;&#36739;&#23567;&#30340;&#26426;&#22120;&#19978;&#35780;&#20272;&#35768;&#22810;&#36739;&#23567;&#30340;&#30005;&#36335;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#19968;&#20010;&#22810;&#39033;&#24335;&#26469;&#22797;&#21046;&#36739;&#22823;&#26426;&#22120;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#26696;&#23545;&#20110;&#19968;&#33324;&#30005;&#36335;&#32780;&#35328;&#38656;&#35201;&#26356;&#22810;&#30340;&#30005;&#36335;&#35780;&#20272;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#21487;&#33021;&#24615;&#65292;&#21363;&#23545;&#20110;&#26576;&#20123;&#24212;&#29992;&#26469;&#35828;&#65292;&#35768;&#22810;&#36825;&#20123;&#23376;&#30005;&#36335;&#26159;&#22810;&#20313;&#30340;&#65292;&#24182;&#19988;&#19968;&#20010;&#26356;&#23567;&#30340;&#27714;&#21644;&#36275;&#20197;&#20272;&#35745;&#23436;&#25972;&#30340;&#30005;&#36335;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21033;&#29992;&#26356;&#23569;&#30340;&#30005;&#36335;&#35780;&#20272;&#26469;&#36924;&#36817;&#36739;&#22823;&#30005;&#36335;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#25968;&#23383;&#35782;&#21035;&#20219;&#21153;&#65292;&#20351;&#29992;&#27169;&#25311;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computers hold great promise to enhance machine learning, but their current qubit counts restrict the realisation of this promise. In an attempt to placate this limitation techniques can be applied for evaluating a quantum circuit using a machine with fewer qubits than the circuit naively requires. These techniques work by evaluating many smaller circuits on the smaller machine, that are then combined in a polynomial to replicate the output of the larger machine. This scheme requires more circuit evaluations than are practical for general circuits. However, we investigate the possibility that for certain applications many of these subcircuits are superfluous, and that a much smaller sum is sufficient to estimate the full circuit. We construct a machine learning model that may be capable of approximating the outputs of the larger circuit with much fewer circuit evaluations. We successfully apply our model to the task of digit recognition, using simulated quantum computers much s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#35770;&#21644;&#21160;&#24577;&#35268;&#21010;&#25216;&#26415;&#65292;&#20026;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#24314;&#31435;&#20102;&#24615;&#33021;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#36890;&#36807;&#35745;&#31639;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#21487;&#20197;&#33719;&#24471;&#19968;&#27493;&#20915;&#31574;&#20219;&#21153;&#30340;&#26368;&#39640;&#21487;&#23454;&#29616;&#39044;&#26399;&#25253;&#37228;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#23558;&#36825;&#20010;&#19978;&#30028;&#25193;&#23637;&#21040;&#22810;&#27493;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#31034;&#20363;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#20915;&#31574;&#36807;&#31243;&#12289;&#33258;&#30001;&#33853;&#20307;&#29289;&#20307;&#30340;&#25429;&#25417;&#21644;&#20351;&#29992;&#28145;&#24230;&#20256;&#24863;&#22120;&#30340;&#36991;&#38556;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.00129</link><description>&lt;p&gt;
&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits for Sensor-Based Robot Control. (arXiv:2202.00129v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00129
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#35770;&#21644;&#21160;&#24577;&#35268;&#21010;&#25216;&#26415;&#65292;&#20026;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#24314;&#31435;&#20102;&#24615;&#33021;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#36890;&#36807;&#35745;&#31639;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#21487;&#20197;&#33719;&#24471;&#19968;&#27493;&#20915;&#31574;&#20219;&#21153;&#30340;&#26368;&#39640;&#21487;&#23454;&#29616;&#39044;&#26399;&#25253;&#37228;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#23558;&#36825;&#20010;&#19978;&#30028;&#25193;&#23637;&#21040;&#22810;&#27493;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#31034;&#20363;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#20915;&#31574;&#36807;&#31243;&#12289;&#33258;&#30001;&#33853;&#20307;&#29289;&#20307;&#30340;&#25429;&#25417;&#21644;&#20351;&#29992;&#28145;&#24230;&#20256;&#24863;&#22120;&#30340;&#36991;&#38556;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#32473;&#23450;&#20219;&#21153;&#24314;&#31435;&#26426;&#22120;&#20154;&#20256;&#24863;&#22120;&#24615;&#33021;&#30340;&#22522;&#26412;&#38480;&#21046;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#25429;&#25417;&#20256;&#24863;&#22120;&#25552;&#20379;&#30340;&#19982;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#25968;&#37327;&#30340;&#37327;&#12290;&#21033;&#29992;&#20449;&#24687;&#35770;&#20013;&#24191;&#20041;Fano&#19981;&#31561;&#24335;&#30340;&#26032;&#39062;&#29256;&#26412;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#37327;&#25552;&#20379;&#20102;&#19968;&#27493;&#20915;&#31574;&#20219;&#21153;&#30340;&#26368;&#39640;&#21487;&#23454;&#29616;&#39044;&#26399;&#25253;&#37228;&#30340;&#19978;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#23558;&#36825;&#20010;&#19978;&#30028;&#25193;&#23637;&#21040;&#22810;&#27493;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#25968;&#20540;&#35745;&#31639;&#24471;&#20986;&#30340;&#19978;&#30028;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#31034;&#20363;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#65288;i&#65289;&#26469;&#33258;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#25991;&#29486;&#30340;&#29076;&#23721;&#38382;&#39064;&#65292;&#65288;ii&#65289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#35266;&#27979;&#31354;&#38388;&#30340;&#31034;&#20363;&#65292;&#23545;&#24212;&#20110;&#26426;&#22120;&#20154;&#25509;&#20303;&#33258;&#30001;&#33853;&#20307;&#29289;&#20307;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#20351;&#29992;&#38750;&#39640;&#26031;&#22122;&#22768;&#28145;&#24230;&#20256;&#24863;&#22120;&#30340;&#36991;&#38556;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Our goal is to develop theory and algorithms for establishing fundamental limits on performance imposed by a robot's sensors for a given task. In order to achieve this, we define a quantity that captures the amount of task-relevant information provided by a sensor. Using a novel version of the generalized Fano inequality from information theory, we demonstrate that this quantity provides an upper bound on the highest achievable expected reward for one-step decision making tasks. We then extend this bound to multi-step problems via a dynamic programming approach. We present algorithms for numerically computing the resulting bounds, and demonstrate our approach on three examples: (i) the lava problem from the literature on partially observable Markov decision processes, (ii) an example with continuous state and observation spaces corresponding to a robot catching a freely-falling object, and (iii) obstacle avoidance using a depth sensor with non-Gaussian noise. We demonstrate the ability
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#24230;&#37327;&#19979;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#23567;d&#26102;&#65292;&#22312;&#20551;&#35774;&#21629;&#20013;&#38598;&#21512;&#29468;&#24819;&#65288;HSC&#65289;&#25104;&#31435;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#27861;&#36890;&#36807;&#20219;&#20309;lp&#24230;&#37327;&#25110;&#32534;&#36753;&#25110;Ulam&#24230;&#37327;&#23454;&#29616;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#27425;&#20108;&#27425;&#31639;&#27861;&#12290;&#22823;d&#26102;&#65292;&#22312;&#20551;&#35774;Quantified SETH&#30340;&#24773;&#20917;&#19979;&#65292;&#25490;&#38500;&#20102;&#22522;&#20110;&#32534;&#36753;&#24230;&#37327;&#20013;&#30340;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#27425;&#22235;&#27425;&#31639;&#27861;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;Ulam&#24230;&#37327;&#20013;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#65288;1+&#949;&#65289;&#36924;&#36817;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.03222</link><description>&lt;p&gt;
&#19981;&#21516;&#24230;&#37327;&#19979;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Complexity of 1-Center in Various Metrics. (arXiv:2112.03222v2 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03222
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#24230;&#37327;&#19979;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#23567;d&#26102;&#65292;&#22312;&#20551;&#35774;&#21629;&#20013;&#38598;&#21512;&#29468;&#24819;&#65288;HSC&#65289;&#25104;&#31435;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#27861;&#36890;&#36807;&#20219;&#20309;lp&#24230;&#37327;&#25110;&#32534;&#36753;&#25110;Ulam&#24230;&#37327;&#23454;&#29616;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#27425;&#20108;&#27425;&#31639;&#27861;&#12290;&#22823;d&#26102;&#65292;&#22312;&#20551;&#35774;Quantified SETH&#30340;&#24773;&#20917;&#19979;&#65292;&#25490;&#38500;&#20102;&#22522;&#20110;&#32534;&#36753;&#24230;&#37327;&#20013;&#30340;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#27425;&#22235;&#27425;&#31639;&#27861;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;Ulam&#24230;&#37327;&#20013;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#65288;1+&#949;&#65289;&#36924;&#36817;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#20856;&#30340;1-&#20013;&#24515;&#38382;&#39064;&#65306;&#32473;&#23450;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#32452;n&#20010;&#28857;P&#65292;&#25214;&#21040;&#36317;&#31163;&#20854;&#20182;P&#20013;&#28857;&#36317;&#31163;&#26368;&#22823;&#30340;&#28857;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;d&#32500;lp&#24230;&#37327;&#20013;&#20197;&#21450;&#22522;&#20110;&#23383;&#31526;&#20018;&#38271;&#24230;&#20026;d&#30340;&#32534;&#36753;&#21644;Ulam&#24230;&#37327;&#20013;&#35813;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#23545;&#20110;1-&#20013;&#24515;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#25353;d&#36827;&#34892;&#20998;&#31867;&#12290;&#23567;d&#26102;&#65292;&#22312;&#20551;&#35774;&#21629;&#20013;&#38598;&#21512;&#29468;&#24819;&#65288;HSC&#65289;&#25104;&#31435;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;d=&#969;&#65288;logn&#65289;&#26102;&#65292;&#26080;&#27861;&#36890;&#36807;&#20219;&#20309;lp&#24230;&#37327;&#25110;&#32534;&#36753;&#25110;Ulam&#24230;&#37327;&#23454;&#29616;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#27425;&#20108;&#27425;&#31639;&#27861;&#12290;&#22823;d&#26102;&#65292;&#25105;&#20204;&#23558;&#26377;&#26465;&#20214;&#30340;&#19979;&#30028;&#25512;&#24191;&#21040;&#22522;&#20110;&#32534;&#36753;&#24230;&#37327;&#20013;&#30340;1-&#20013;&#24515;&#38382;&#39064;&#65292;&#25490;&#38500;&#20102;&#27425;&#22235;&#27425;&#31639;&#27861;&#65288;&#20551;&#35774;Quantified SETH&#65289;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;Ulam&#24230;&#37327;&#20013;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#65288;1+&#949;&#65289;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$\tilde{O_{\varepsilon}}(nd+n^2\sqrt{d})$&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20801;&#35768;&#36817;&#20284;&#26469;&#21152;&#24378;&#20102;&#19968;&#20123;&#20043;&#21069;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the classic 1-center problem: Given a set $P$ of $n$ points in a metric space find the point in $P$ that minimizes the maximum distance to the other points of $P$. We study the complexity of this problem in $d$-dimensional $\ell_p$-metrics and in edit and Ulam metrics over strings of length $d$. Our results for the 1-center problem may be classified based on $d$ as follows.  $\bullet$ Small $d$: Assuming the hitting set conjecture (HSC), we show that when $d=\omega(\log n)$, no subquadratic algorithm can solve 1-center problem in any of the $\ell_p$-metrics, or in edit or Ulam metrics.  $\bullet$ Large $d$: When $d=\Omega(n)$, we extend our conditional lower bound to rule out subquartic algorithms for 1-center problem in edit metric (assuming Quantified SETH). On the other hand, we give a $(1+\epsilon)$-approximation for 1-center in Ulam metric with running time $\tilde{O_{\varepsilon}}(nd+n^2\sqrt{d})$.  We also strengthen some of the above lower bounds by allowing approxi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20840;&#29983;&#21629;&#21608;&#26399;&#29992;&#25143;&#34920;&#31034;&#27169;&#22411;&#65288;LURM&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#23545;&#36229;&#38271;&#34892;&#20026;&#24207;&#21015;&#36827;&#34892;&#20840;&#29983;&#21629;&#21608;&#26399;&#24314;&#27169;&#12290;&#35813;&#27169;&#22411;&#30001;&#20852;&#36259;&#21253;&#32534;&#30721;&#21644;&#33258;&#30417;&#30563;&#22810;&#38170;&#28857;&#32534;&#30721;&#32593;&#32476;&#32452;&#25104;&#65292;&#33021;&#22815;&#29983;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#29992;&#25143;&#35748;&#30693;&#20219;&#21153;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2110.11337</link><description>&lt;p&gt;
&#20197;&#20840;&#29983;&#21629;&#21608;&#26399;&#34892;&#20026;&#24314;&#27169;&#36171;&#33021;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Empowering General-purpose User Representation with Full-life Cycle Behavior Modeling. (arXiv:2110.11337v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20840;&#29983;&#21629;&#21608;&#26399;&#29992;&#25143;&#34920;&#31034;&#27169;&#22411;&#65288;LURM&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#23545;&#36229;&#38271;&#34892;&#20026;&#24207;&#21015;&#36827;&#34892;&#20840;&#29983;&#21629;&#21608;&#26399;&#24314;&#27169;&#12290;&#35813;&#27169;&#22411;&#30001;&#20852;&#36259;&#21253;&#32534;&#30721;&#21644;&#33258;&#30417;&#30563;&#22810;&#38170;&#28857;&#32534;&#30721;&#32593;&#32476;&#32452;&#25104;&#65292;&#33021;&#22815;&#29983;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#29992;&#25143;&#35748;&#30693;&#20219;&#21153;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#22312;&#34892;&#19994;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#65292;&#30456;&#23545;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#33021;&#22815;&#29983;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#29992;&#25143;&#35748;&#30693;&#20219;&#21153;&#30340;&#36890;&#29992;&#34920;&#31034;&#30340;&#20219;&#21153;&#26080;&#20851;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20215;&#20540;&#21644;&#32463;&#27982;&#24615;&#12290;&#38543;&#30528;&#20114;&#32852;&#32593;&#26381;&#21153;&#24179;&#21488;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29992;&#25143;&#34892;&#20026;&#19981;&#26029;&#31215;&#32047;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30740;&#31350;&#23545;&#33258;&#29992;&#25143;&#27880;&#20876;&#20197;&#26469;&#30340;&#26497;&#38271;&#34892;&#20026;&#24207;&#21015;&#30340;&#20840;&#29983;&#21629;&#21608;&#26399;&#24314;&#27169;&#33021;&#21147;&#36739;&#24369;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20840;&#29983;&#21629;&#21608;&#26399;&#29992;&#25143;&#34920;&#31034;&#27169;&#22411;&#65288;LURM&#65289;&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LURM&#30001;&#20004;&#20010;&#32423;&#32852;&#30340;&#23376;&#27169;&#22411;&#32452;&#25104;&#65306;&#65288;I&#65289;&#20852;&#36259;&#21253;&#65288;BoI&#65289;&#23558;&#20219;&#20309;&#26102;&#38388;&#27573;&#30340;&#29992;&#25143;&#34892;&#20026;&#32534;&#30721;&#20026;&#36229;&#39640;&#32500;&#24230;&#65288;&#20363;&#22914;10^5&#65289;&#30340;&#31232;&#30095;&#21521;&#37327;&#65307;&#65288;II&#65289;&#33258;&#30417;&#30563;&#22810;&#38170;&#28857;&#32534;&#30721;&#32593;&#32476;&#65288;SMEN&#65289;&#23558;BoI&#29305;&#24449;&#24207;&#21015;&#26144;&#23556;&#21040;&#22810;&#20010;&#20302;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
User Modeling plays an essential role in industry. In this field, task-agnostic approaches, which generate general-purpose representation applicable to diverse downstream user cognition tasks, is a promising direction being more valuable and economical than task-specific representation learning. With the rapid development of Internet service platforms, user behaviors have been accumulated continuously. However, existing general-purpose user representation researches have little ability for full-life cycle modeling on extremely long behavior sequences since user registration. In this study, we propose a novel framework called full- Life cycle User Representation Model (LURM) to tackle this challenge. Specifically, LURM consists of two cascaded sub-models: (I) Bag-of-Interests (BoI) encodes user behaviors in any time period into a sparse vector with super-high dimension (e.g., 10^5); (II) Self-supervised Multi-anchor Encoder Network (SMEN) maps sequences of BoI features to multiple low-d
&lt;/p&gt;</description></item><item><title>Deep Generative Decoder (DGD) is a simple generative model that uses MAP estimation to compute model parameters and representations. It outperforms variational autoencoders (VAEs) by handling complex parameterized latent distributions and learning meaningful and well-structured latent representations on single-cell RNA data, including sub-clustering beyond provided labels.</title><link>http://arxiv.org/abs/2110.06672</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#35299;&#30721;&#22120;&#65306;MAP&#20272;&#35745;&#30340;&#34920;&#31034;&#25913;&#36827;&#20102;&#21333;&#32454;&#32990;RNA&#25968;&#25454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
The Deep Generative Decoder: MAP estimation of representations improves modeling of single-cell RNA data. (arXiv:2110.06672v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06672
&lt;/p&gt;
&lt;p&gt;
Deep Generative Decoder (DGD) is a simple generative model that uses MAP estimation to compute model parameters and representations. It outperforms variational autoencoders (VAEs) by handling complex parameterized latent distributions and learning meaningful and well-structured latent representations on single-cell RNA data, including sub-clustering beyond provided labels.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21333;&#32454;&#32990;&#36716;&#24405;&#32452;&#23398;&#30340;&#20302;&#32500;&#34920;&#31034;&#23545;&#20854;&#19979;&#28216;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#26159;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAEs)&#65292;&#23427;&#20351;&#29992;&#21464;&#20998;&#36817;&#20284;&#26469;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;&#28145;&#24230;&#29983;&#25104;&#35299;&#30721;&#22120;(DGD)&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21518;&#39564;(MAP)&#20272;&#35745;&#30452;&#25509;&#35745;&#31639;&#27169;&#22411;&#21442;&#25968;&#21644;&#34920;&#31034;&#12290;DGD&#21487;&#20197;&#33258;&#28982;&#22320;&#22788;&#29702;&#22797;&#26434;&#21442;&#25968;&#21270;&#30340;&#28508;&#22312;&#20998;&#24067;&#65292;&#32780;VAEs&#36890;&#24120;&#20351;&#29992;&#22266;&#23450;&#30340;&#39640;&#26031;&#20998;&#24067;&#65292;&#22240;&#20026;&#28155;&#21152;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#20250;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#20010;&#24120;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Fashion-MNIST&#19978;&#23637;&#31034;&#20102;&#20854;&#36890;&#29992;&#21151;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#37324;&#65292;DGD&#33021;&#22815;&#23398;&#20064;&#21040;&#20302;&#32500;&#12289;&#26377;&#24847;&#20041;&#19988;&#32467;&#26500;&#33391;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#33021;&#22312;&#25552;&#20379;&#30340;&#26631;&#31614;&#20043;&#22806;&#36827;&#34892;&#23376;&#32858;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#30340;&#31616;&#27905;&#24615;&#21644;&#28508;&#22312;&#34920;&#31034;&#30340;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning low-dimensional representations of single-cell transcriptomics has become instrumental to its downstream analysis. The state of the art is currently represented by neural network models such as variational autoencoders (VAEs) which use a variational approximation of the likelihood for inference. We here present the Deep Generative Decoder (DGD), a simple generative model that computes model parameters and representations directly via maximum a posteriori (MAP) estimation. The DGD handles complex parameterized latent distributions naturally unlike VAEs which typically use a fixed Gaussian distribution, because of the complexity of adding other types. We first show its general functionality on a commonly used benchmark set, Fashion-MNIST. Secondly, we apply the model to multiple single-cell data sets. Here the DGD learns low-dimensional, meaningful and well-structured latent representations with sub-clustering beyond the provided labels. The advantages of this approach are its s
&lt;/p&gt;</description></item><item><title>&#22810;&#31867;&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#30830;&#23450;&#24615;&#26465;&#20214;&#65292;&#24403;&#21069;&#19977;&#31181;&#31639;&#27861;&#22312;&#28385;&#36275;&#26465;&#20214;&#26102;&#20250;&#24471;&#21040;&#25554;&#20540;&#25968;&#25454;&#24182;&#20855;&#26377;&#30456;&#31561;&#20934;&#30830;&#29575;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2106.10865</link><description>&lt;p&gt;
&#22810;&#31867;&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65306;&#25152;&#26377;&#36335;&#24452;&#37117;&#36890;&#24448;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Benign Overfitting in Multiclass Classification: All Roads Lead to Interpolation. (arXiv:2106.10865v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.10865
&lt;/p&gt;
&lt;p&gt;
&#22810;&#31867;&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#30830;&#23450;&#24615;&#26465;&#20214;&#65292;&#24403;&#21069;&#19977;&#31181;&#31639;&#27861;&#22312;&#28385;&#36275;&#26465;&#20214;&#26102;&#20250;&#24471;&#21040;&#25554;&#20540;&#25968;&#25454;&#24182;&#20855;&#26377;&#30456;&#31561;&#20934;&#30830;&#29575;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#65292;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#30340;&#25991;&#29486;&#22823;&#22810;&#23616;&#38480;&#20110;&#22238;&#24402;&#25110;&#20108;&#20998;&#31867;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#31867;&#21035;&#35774;&#32622;&#20013;&#36816;&#34892;&#12290;&#21463;&#27492;&#24046;&#24322;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31867;&#32447;&#24615;&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#21487;&#20998;&#25968;&#25454;&#19978;&#30340;&#20197;&#19979;&#35757;&#32451;&#31639;&#27861;&#65306;&#65288;i&#65289;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#25910;&#25947;&#21040;&#22810;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#35299;&#65307;&#65288;ii&#65289;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#30340;ERM&#65292;&#25910;&#25947;&#21040;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#65288;MNI&#65289;&#35299;&#65307;&#21450;&#65288;iii&#65289;&#19968;&#23545;&#22810;SVM&#20998;&#31867;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20805;&#20998;&#30830;&#23450;&#24615;&#26465;&#20214;&#65292;&#22312;&#35813;&#26465;&#20214;&#19979;&#65292;&#25152;&#26377;&#19977;&#31181;&#31639;&#27861;&#37117;&#20250;&#23548;&#33268;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#24182;&#20855;&#26377;&#30456;&#31561;&#20934;&#30830;&#29575;&#30340;&#20998;&#31867;&#22120;&#12290;&#24403;&#25968;&#25454;&#26469;&#33258;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#25110;&#22810;&#39033;&#24335;&#36923;&#36753;&#27169;&#22411;&#26102;&#65292;&#22312;&#36275;&#22815;&#39640;&#30340;&#26377;&#25928;&#36229;&#21442;&#25968;&#21270;&#19979;&#65292;&#36825;&#20010;&#26465;&#20214;&#25104;&#31435;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The literature on "benign overfitting" in overparameterized models has been mostly restricted to regression or binary classification; however, modern machine learning operates in the multiclass setting. Motivated by this discrepancy, we study benign overfitting in multiclass linear classification. Specifically, we consider the following training algorithms on separable data: (i) empirical risk minimization (ERM) with cross-entropy loss, which converges to the multiclass support vector machine (SVM) solution; (ii) ERM with least-squares loss, which converges to the min-norm interpolating (MNI) solution; and, (iii) the one-vs-all SVM classifier. First, we provide a simple sufficient deterministic condition under which all three algorithms lead to classifiers that interpolate the training data and have equal accuracy. When the data is generated from Gaussian mixtures or a multinomial logistic model, this condition holds under high enough effective overparameterization. We also show that t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#30340;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#20998;&#24067;&#20013;&#23398;&#20064;&#20027;&#22270;&#65292;&#33021;&#22815;&#22788;&#29702;&#33034;&#26816;&#27979;&#20013;&#30340;&#27969;&#24418;&#23398;&#20064;&#38382;&#39064;&#20197;&#21450;&#24322;&#24120;&#20540;&#21644;&#24322;&#26041;&#24046;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2106.09035</link><description>&lt;p&gt;
&#40065;&#26834;&#20027;&#22270;&#23398;&#20064;&#30340;&#28151;&#21512;&#27169;&#22411;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regularization of Mixture Models for Robust Principal Graph Learning. (arXiv:2106.09035v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.09035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#30340;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#20998;&#24067;&#20013;&#23398;&#20064;&#20027;&#22270;&#65292;&#33021;&#22815;&#22788;&#29702;&#33034;&#26816;&#27979;&#20013;&#30340;&#27969;&#24418;&#23398;&#20064;&#38382;&#39064;&#20197;&#21450;&#24322;&#24120;&#20540;&#21644;&#24322;&#26041;&#24046;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#28151;&#21512;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#29256;&#26412;&#65292;&#29992;&#20110;&#20174;$D$&#32500;&#25968;&#25454;&#28857;&#30340;&#20998;&#24067;&#20013;&#23398;&#20064;&#20027;&#22270;&#12290;&#22312;&#29992;&#20110;&#33034;&#26816;&#27979;&#30340;&#27969;&#24418;&#23398;&#20064;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20551;&#35774;&#28508;&#22312;&#27969;&#24418;&#21487;&#20197;&#24314;&#27169;&#20026;&#25299;&#25169;&#20808;&#39564;&#20316;&#29992;&#19979;&#30340;&#39640;&#26031;&#32858;&#31867;&#22270;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#12290;&#27169;&#22411;&#30340;&#21442;&#25968;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#36827;&#34892;&#36845;&#20195;&#20272;&#35745;&#65292;&#20351;&#24471;&#32467;&#26500;&#30340;&#23398;&#20064;&#22312;&#20219;&#20309;&#22270;&#20808;&#39564;&#19979;&#37117;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#21516;&#26102;&#32467;&#21512;&#22270;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#20351;&#31639;&#27861;&#23545;&#24322;&#24120;&#20540;&#21644;&#27969;&#24418;&#37319;&#26679;&#30340;&#24322;&#26041;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30001;&#26368;&#23567;&#29983;&#25104;&#26641;&#32473;&#20986;&#30340;&#22270;&#20808;&#39564;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#38543;&#26426;&#23376;&#37319;&#26679;&#26469;&#25193;&#23637;&#22270;&#30340;&#33539;&#22260;&#65292;&#20197;&#32771;&#34385;&#22312;&#31354;&#38388;&#20998;&#24067;&#20013;&#35266;&#23519;&#21040;&#30340;&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A regularized version of Mixture Models is proposed to learn a principal graph from a distribution of $D$-dimensional data points. In the particular case of manifold learning for ridge detection, we assume that the underlying manifold can be modeled as a graph structure acting like a topological prior for the Gaussian clusters turning the problem into a maximum a posteriori estimation. Parameters of the model are iteratively estimated through an Expectation-Maximization procedure making the learning of the structure computationally efficient with guaranteed convergence for any graph prior in a polynomial time. We also embed in the formalism a natural way to make the algorithm robust to outliers of the pattern and heteroscedasticity of the manifold sampling coherently with the graph structure. The method uses a graph prior given by the minimum spanning tree that we extend using random sub-samplings of the dataset to take into account cycles that can be observed in the spatial distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#27169;&#22411;&#8212;&#8212;&#36882;&#24402;&#22343;&#34913;&#32593;&#32476;&#65288;RENs&#65289;&#65292;&#23427;&#20855;&#26377;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#8220;&#20869;&#32622;&#8221;&#20445;&#35777;&#12290;RENs&#20855;&#26377;&#24456;&#24378;&#30340;&#28789;&#27963;&#24615;&#65292;&#33021;&#34920;&#31034;&#22810;&#31181;&#31995;&#32479;&#65292;&#24182;&#29992;&#21442;&#25968;&#21270;&#26041;&#27861;&#31616;&#21270;&#20102;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2104.05942</link><description>&lt;p&gt;
&#36882;&#24402;&#22343;&#34913;&#32593;&#32476;&#65306;&#20855;&#26377;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#28789;&#27963;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed Stability and Robustness. (arXiv:2104.05942v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.05942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#27169;&#22411;&#8212;&#8212;&#36882;&#24402;&#22343;&#34913;&#32593;&#32476;&#65288;RENs&#65289;&#65292;&#23427;&#20855;&#26377;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#8220;&#20869;&#32622;&#8221;&#20445;&#35777;&#12290;RENs&#20855;&#26377;&#24456;&#24378;&#30340;&#28789;&#27963;&#24615;&#65292;&#33021;&#34920;&#31034;&#22810;&#31181;&#31995;&#32479;&#65292;&#24182;&#29992;&#21442;&#25968;&#21270;&#26041;&#27861;&#31616;&#21270;&#20102;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36882;&#24402;&#22343;&#34913;&#32593;&#32476;&#65288;RENs&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#12289;&#31995;&#32479;&#35782;&#21035;&#21644;&#25511;&#21046;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#36825;&#31181;&#26032;&#27169;&#22411;&#20855;&#26377;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#8220;&#20869;&#32622;&#8221;&#34892;&#20026;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#31867;&#20013;&#30340;&#25152;&#26377;&#27169;&#22411;&#37117;&#26159;&#25910;&#32553;&#30340;&#8212;&#8212;&#19968;&#31181;&#24378;&#38750;&#32447;&#24615;&#31283;&#23450;&#24615;&#24418;&#24335;&#8212;&#8212;&#24182;&#19988;&#21487;&#20197;&#28385;&#36275;&#35268;&#23450;&#30340;&#22686;&#37327;&#31215;&#20998;&#20108;&#27425;&#32422;&#26463;&#65288;IQC&#65289;&#65292;&#21253;&#25324;Lipschitz&#30028;&#38480;&#21644;&#22686;&#37327;&#34987;&#21160;&#24615;&#12290;RENs&#20855;&#26377;&#24456;&#24378;&#30340;&#28789;&#27963;&#24615;&#65306;&#23427;&#20204;&#21487;&#20197;&#34920;&#31034;&#25152;&#26377;&#31283;&#23450;&#30340;&#32447;&#24615;&#31995;&#32479;&#65292;&#25152;&#26377;&#20808;&#21069;&#24050;&#30693;&#30340;&#25910;&#32553;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65292;&#25152;&#26377;&#28145;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21450;&#25152;&#26377;&#31283;&#23450;&#30340;Wiener/Hammerstein&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36817;&#20284;&#25152;&#26377;&#35114;&#33394;&#35760;&#24518;&#21644;&#25910;&#32553;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;RENs&#30452;&#25509;&#30001;R^N&#20013;&#30340;&#21521;&#37327;&#21442;&#25968;&#21270;&#65292;&#21363;&#22312;&#19981;&#21463;&#21442;&#25968;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36825;&#31616;&#21270;&#20102;&#23398;&#20064;&#36807;&#31243;&#65292;&#22240;&#20026;&#36890;&#29992;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#30452;&#25509;&#23398;&#20064;RENs&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces recurrent equilibrium networks (RENs), a new class of nonlinear dynamical models} for applications in machine learning, system identification and control. The new model class admits ``built in'' behavioural guarantees of stability and robustness. All models in the proposed class are contracting -- a strong form of nonlinear stability -- and models can satisfy prescribed incremental integral quadratic constraints (IQC), including Lipschitz bounds and incremental passivity. RENs are otherwise very flexible: they can represent all stable linear systems, all previously-known sets of contracting recurrent neural networks and echo state networks, all deep feedforward neural networks, and all stable Wiener/Hammerstein models, and can approximate all fading-memory and contracting nonlinear systems. RENs are parameterized directly by a vector in R^N, i.e. stability and robustness are ensured without parameter constraints, which simplifies learning since \HL{generic methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#25968;&#25454;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#65292;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#36895;&#24230;&#36739;&#24555;&#30340;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#25351;&#25968;&#24418;&#24335;&#30340;&#21152;&#36895;&#26426;&#21046;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2102.00479</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fast Rates for the Regret of Offline Reinforcement Learning. (arXiv:2102.00479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.00479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#25968;&#25454;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#65292;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#36895;&#24230;&#36739;&#24555;&#30340;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#25351;&#25968;&#24418;&#24335;&#30340;&#21152;&#36895;&#26426;&#21046;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#34892;&#20026;&#31574;&#30053;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#29616;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#29983;&#25104;&#30340;&#31163;&#32447;&#25968;&#25454;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#12290;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#25311;&#21512;Q-&#36845;&#20195;&#65289;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23545;&#20110;&#36951;&#25022;&#30340;&#25910;&#25947;&#36895;&#29575;&#26159;O(1/&#8730;n)&#65292;&#20294;&#23454;&#35777;&#34892;&#20026;&#34920;&#29616;&#20986;&#38750;&#24120;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#36951;&#25022;&#25910;&#25947;&#36895;&#29575;&#30340;&#24555;&#36895;&#25910;&#25947;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#20934;&#30830;&#22320;&#34920;&#24449;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#32473;&#23450;&#26368;&#20248;&#36136;&#37327;&#20989;&#25968;Q*&#30340;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#23545;&#24212;&#30340;&#31574;&#30053;&#36951;&#25022;&#25353;&#29031;Q*&#20272;&#35745;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#36895;&#29575;&#30340;&#25351;&#25968;&#36827;&#34892;&#25910;&#25947;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;&#25351;&#25968;&#30340;&#32423;&#21035;&#21462;&#20915;&#20110;&#8220;&#20915;&#31574;&#38382;&#39064;&#8221;&#20013;&#30340;&#22122;&#22768;&#27700;&#24179;&#65292;&#32780;&#19981;&#26159;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#32447;&#24615;&#21644;&#34920;&#26684;&#22411;MDP&#20316;&#20026;&#31034;&#20363;&#65292;&#24314;&#31435;&#20102;&#36825;&#26679;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#25311;&#21512;Q-&#36845;&#20195;&#21644;Bellman&#27531;&#24046;&#36827;&#34892;&#20102;&#26032;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the regret of reinforcement learning from offline data generated by a fixed behavior policy in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted $Q$-iteration (FQI), suggest a $O(1/\sqrt{n})$ convergence for regret, empirical behavior exhibits \emph{much} faster convergence. In this paper, we present a finer regret analysis that exactly characterizes this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function $Q^*$, the regret of the policy it defines converges at a rate given by the exponentiation of the $Q^*$-estimate's pointwise convergence rate, thus speeding it up. The level of exponentiation depends on the level of noise in the \emph{decision-making} problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman resid
&lt;/p&gt;</description></item><item><title>B-HAR&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#28145;&#20837;&#30740;&#31350;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#35299;&#20915;&#20102;HAR&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#37197;&#32622;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#27169;&#24335;&#35782;&#21035;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2101.10870</link><description>&lt;p&gt;
B-HAR:&#19968;&#20010;&#29992;&#20110;&#28145;&#20837;&#30740;&#31350;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#21644;&#24037;&#20316;&#27969;&#31243;&#30340;&#24320;&#28304;&#22522;&#20934;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
B-HAR: an open-source baseline framework for in depth study of human activity recognition datasets and workflows. (arXiv:2101.10870v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.10870
&lt;/p&gt;
&lt;p&gt;
B-HAR&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#28145;&#20837;&#30740;&#31350;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#35299;&#20915;&#20102;HAR&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#37197;&#32622;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#27169;&#24335;&#35782;&#21035;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#34987;&#35748;&#20026;&#26159;&#26368;&#20855;&#28508;&#21147;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#29992;&#20110;&#30417;&#27979;&#19981;&#21516;&#20154;&#32676;&#65288;&#22914;&#36816;&#21160;&#21592;&#12289;&#32769;&#24180;&#20154;&#12289;&#20799;&#31461;&#12289;&#38599;&#20027;&#65289;&#30340;&#19987;&#19994;&#21644;&#26085;&#24120;&#27963;&#21160;&#65292;&#20197;&#25552;&#20379;&#21508;&#31181;&#19982;&#31119;&#31049;&#12289;&#25216;&#26415;&#24615;&#33021;&#22686;&#24378;&#12289;&#39118;&#38505;&#39044;&#38450;&#21644;&#25945;&#32946;&#30446;&#30340;&#30456;&#20851;&#30340;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;HAR&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#20998;&#26512;&#21463;&#21040;&#32570;&#20047;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#35780;&#20272;&#24320;&#21457;&#30340;&#27169;&#24335;&#35782;&#21035;&#27169;&#22411;&#36136;&#37327;&#30340;&#22522;&#20934;&#12290;&#36825;&#20351;&#24471;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#20250;&#29359;&#38169;&#35823;&#65292;&#24403;&#36825;&#20123;&#38169;&#35823;&#27809;&#26377;&#34987;&#26816;&#27979;&#21040;&#26102;&#65292;&#20250;&#23545;&#36798;&#21040;&#30340;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;B-HAR&#30340;&#24320;&#28304;&#33258;&#21160;&#21270;&#21644;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#26631;&#20934;&#21270;HAR&#23454;&#39564;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR), based on machine and deep learning algorithms is considered one of the most promising technologies to monitor professional and daily life activities for different categories of people (e.g., athletes, elderly, kids, employers) in order to provide a variety of services related, for example to well-being, empowering of technical performances, prevention of risky situation, and educational purposes. However, the analysis of the effectiveness and the efficiency of HAR methodologies suffers from the lack of a standard workflow, which might represent the baseline for the estimation of the quality of the developed pattern recognition models. This makes the comparison among different approaches a challenging task. In addition, researchers can make mistakes that, when not detected, definitely affect the achieved results. To mitigate such issues, this paper proposes an open-source automatic and highly configurable framework, named B-HAR, for the definition, stan
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#23454;&#26102;&#25511;&#21046;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#20013;&#23454;&#29616;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2010.02613</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#26102;&#25511;&#21046;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep Learning based Uncertainty Decomposition for Real-time Control. (arXiv:2010.02613v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.02613
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#23454;&#26102;&#25511;&#21046;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#20013;&#23454;&#29616;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#38656;&#35201;&#23545;&#28041;&#21450;&#30340;&#19981;&#30830;&#23450;&#24615;&#26377;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;&#34429;&#28982;&#30001;&#20110;&#27979;&#37327;&#35823;&#24046;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#22312;&#32473;&#23450;&#21442;&#25968;&#21270;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#26126;&#30830;&#22320;&#36827;&#34892;&#24314;&#27169;&#65292;&#20294;&#23545;&#20110;&#25551;&#36848;&#35757;&#32451;&#25968;&#25454;&#30340;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#24314;&#27169;&#21487;&#33021;&#26356;&#21152;&#22256;&#38590;&#12290;&#24403;&#31995;&#32479;&#21160;&#24577;&#26410;&#30693;&#26102;&#65292;&#21518;&#32773;&#22312;&#23454;&#26045;&#25506;&#32034;&#24615;&#25511;&#21046;&#31574;&#30053;&#26102;&#29305;&#21035;&#26377;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#26816;&#27979;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#22833;&#65292;&#20854;&#36755;&#20986;&#19968;&#20010;&#20171;&#20110;0&#65288;&#34920;&#31034;&#20302;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;1&#65288;&#34920;&#31034;&#39640;&#19981;&#30830;&#23450;&#24615;&#65289;&#20043;&#38388;&#30340;&#36830;&#32493;&#20540;&#26631;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26816;&#27979;&#22120;&#20316;&#20026;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#20195;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#19982;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#24182;&#20801;&#35768;&#23454;&#26102;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven control in unknown environments requires a clear understanding of the involved uncertainties for ensuring safety and efficient exploration. While aleatoric uncertainty that arises from measurement noise can often be explicitly modeled given a parametric description, it can be harder to model epistemic uncertainty, which describes the presence or absence of training data. The latter can be particularly useful for implementing exploratory control strategies when system dynamics are unknown. We propose a novel method for detecting the absence of training data using deep learning, which gives a continuous valued scalar output between $0$ (indicating low uncertainty) and $1$ (indicating high uncertainty). We utilize this detector as a proxy for epistemic uncertainty and show its advantages over existing approaches on synthetic and real-world datasets. Our approach can be directly combined with aleatoric uncertainty estimates and allows for uncertainty estimation in real-time as 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#20122;&#32447;&#24615;&#20302;&#31209;&#30697;&#38453;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#25512;&#24191;&#20102;Tang&#30340;&#37327;&#23376;&#21551;&#21457;&#31639;&#27861;&#30340;&#19968;&#31995;&#21015;&#32467;&#26524;&#12290;&#22522;&#20110;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#31639;&#27861;&#21644;&#37327;&#23376;&#22855;&#24322;&#20540;&#21464;&#25442;&#26694;&#26550;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32463;&#20856;&#30340;SVT&#31639;&#27861;&#65292;&#36816;&#34892;&#26102;&#38388;&#19982;&#36755;&#20837;&#32500;&#24230;&#26080;&#20851;&#65292;&#35777;&#26126;&#20102;&#37327;&#23376;SVT&#26080;&#27861;&#23454;&#29616;&#25351;&#25968;&#32423;&#30340;&#37327;&#23376;&#21152;&#36895;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36275;&#20197;&#25512;&#24191;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#25152;&#26377;&#26368;&#36817;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/1910.06151</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#20122;&#32447;&#24615;&#20302;&#31209;&#30697;&#38453;&#31639;&#27861;&#26694;&#26550;&#29992;&#20110;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning. (arXiv:1910.06151v4 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.06151
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#20122;&#32447;&#24615;&#20302;&#31209;&#30697;&#38453;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#25512;&#24191;&#20102;Tang&#30340;&#37327;&#23376;&#21551;&#21457;&#31639;&#27861;&#30340;&#19968;&#31995;&#21015;&#32467;&#26524;&#12290;&#22522;&#20110;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#31639;&#27861;&#21644;&#37327;&#23376;&#22855;&#24322;&#20540;&#21464;&#25442;&#26694;&#26550;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32463;&#20856;&#30340;SVT&#31639;&#27861;&#65292;&#36816;&#34892;&#26102;&#38388;&#19982;&#36755;&#20837;&#32500;&#24230;&#26080;&#20851;&#65292;&#35777;&#26126;&#20102;&#37327;&#23376;SVT&#26080;&#27861;&#23454;&#29616;&#25351;&#25968;&#32423;&#30340;&#37327;&#23376;&#21152;&#36895;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36275;&#20197;&#25512;&#24191;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#25152;&#26377;&#26368;&#36817;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#25509;&#36817;&#20302;&#31209;&#30697;&#38453;&#36827;&#34892;&#37327;&#23376;&#21551;&#21457;&#30340;&#32463;&#20856;&#31639;&#27861;&#65292;&#24182;&#19988;&#25512;&#24191;&#20102;Tang&#22312;&#25512;&#33616;&#31995;&#32479;&#19978;&#30340;&#31361;&#30772;&#24615;&#37327;&#23376;&#21551;&#21457;&#31639;&#27861;&#30340;&#19968;&#31995;&#21015;&#32467;&#26524;[STOC'19]&#12290;&#21463;&#21040;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#31639;&#27861;&#21644;Gily&#233;n&#12289;Su&#12289;Low&#21644;Wiebe [STOC'19]&#30340;&#37327;&#23376;&#22855;&#24322;&#20540;&#21464;&#25442;&#65288;SVT&#65289;&#26694;&#26550;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32463;&#20856;&#30340;SVT&#31639;&#27861;&#65292;&#22312;&#36866;&#24403;&#30340;&#37327;&#23376;&#21551;&#21457;&#25277;&#26679;&#20551;&#35774;&#19979;&#65292;&#36816;&#34892;&#26102;&#38388;&#19982;&#36755;&#20837;&#32500;&#24230;&#26080;&#20851;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#35828;&#26126;&#22312;&#30456;&#24212;&#30340;QRAM&#25968;&#25454;&#32467;&#26500;&#36755;&#20837;&#27169;&#22411;&#19979;&#65292;&#37327;&#23376;SVT&#24182;&#19981;&#20135;&#29983;&#25351;&#25968;&#32423;&#30340;&#37327;&#23376;&#21152;&#36895;&#12290;&#30001;&#20110;&#37327;&#23376;SVT&#26694;&#26550;&#22522;&#26412;&#19978;&#21253;&#21547;&#20102;&#25152;&#26377;&#24050;&#30693;&#30340;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#21069;&#26399;&#24037;&#20316;&#20013;&#30340;&#37319;&#26679;&#24341;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#36275;&#20197;&#25512;&#24191;&#25152;&#26377;&#26368;&#36817;&#20851;&#20110;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an algorithmic framework for quantum-inspired classical algorithms on close-to-low-rank matrices, generalizing the series of results started by Tang's breakthrough quantum-inspired algorithm for recommendation systems [STOC'19]. Motivated by quantum linear algebra algorithms and the quantum singular value transformation (SVT) framework of Gily\'en, Su, Low, and Wiebe [STOC'19], we develop classical algorithms for SVT that run in time independent of input dimension, under suitable quantum-inspired sampling assumptions. Our results give compelling evidence that in the corresponding QRAM data structure input model, quantum SVT does not yield exponential quantum speedups. Since the quantum SVT framework generalizes essentially all known techniques for quantum linear algebra, our results, combined with sampling lemmas from previous work, suffice to generalize all recent results about dequantizing quantum machine learning algorithms. In particular, our classical SVT framework reco
&lt;/p&gt;</description></item></channel></rss>