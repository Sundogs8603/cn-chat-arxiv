<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;MLLM&#22312;&#32431;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#23545;&#40784;&#33021;&#21147;&#65292;&#36825;&#24471;&#30410;&#20110;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#21644;&#20248;&#31168;&#30340;&#25351;&#23548;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.07120</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26412;&#35270;&#37326;&#65306;&#22810;&#27169;&#24577;&#35757;&#32451;&#25552;&#21319;&#20102;&#22312;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#36947;&#24503;&#26041;&#38754;&#30340;MLLM
&lt;/p&gt;
&lt;p&gt;
Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics. (arXiv:2309.07120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07120
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;MLLM&#22312;&#32431;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#23545;&#40784;&#33021;&#21147;&#65292;&#36825;&#24471;&#30410;&#20110;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#21644;&#20248;&#31168;&#30340;&#25351;&#23548;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#22791;&#29702;&#35299;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#29983;&#25104;&#25991;&#26412;&#21709;&#24212;&#30340;&#22686;&#24378;&#33021;&#21147;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;MLLM&#30340;&#32431;NLP&#33021;&#21147;&#24120;&#24120;&#20302;&#20272;&#24182;&#26410;&#32463;&#27979;&#35797;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;MLLM&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24615;&#8212;&#8212;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#65292;&#19968;&#31181;&#23558;LLM&#36716;&#25442;&#20026;MLLM&#30340;&#27969;&#34892;&#31574;&#30053;&#65292;&#20986;&#20046;&#24847;&#26009;&#22320;&#24110;&#21161;&#27169;&#22411;&#22312;&#32431;NLP&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20262;&#29702;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;&#20363;&#22914;&#65292;&#32463;&#36807;&#35270;&#35273;&#25351;&#23548;&#35843;&#20248;&#30340;LLaMA2 7B&#27169;&#22411;&#22312;TruthfulQA-mc&#21644;&#20262;&#29702;&#36947;&#24503;&#22522;&#20934;&#19978;&#36229;&#36807;&#20102;&#32463;&#36807;&#36229;&#36807;&#19968;&#30334;&#19975;&#20154;&#24037;&#26631;&#27880;&#30340;LLaMA2-chat 7B&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#30340;&#23545;&#40784;&#21487;&#20197;&#24402;&#22240;&#20110;&#35270;&#35273;-&#25991;&#26412;&#25968;&#25454;&#22266;&#26377;&#30340;&#20248;&#31168;&#25351;&#23548;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models (MLLMs) are trained based on large language models (LLM), with an enhanced capability to comprehend multi-modal inputs and generate textual responses. While they excel in multi-modal tasks, the pure NLP abilities of MLLMs are often underestimated and left untested. In this study, we get out of the box and unveil an intriguing characteristic of MLLMs -- our preliminary results suggest that visual instruction tuning, a prevailing strategy for transitioning LLMs into MLLMs, unexpectedly and interestingly helps models attain both improved truthfulness and ethical alignment in the pure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model surpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one million human annotations, on TruthfulQA-mc and Ethics benchmarks. Further analysis reveals that the improved alignment can be attributed to the superior instruction quality inherent to visual-text data. In releasing our code at 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PILOT&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;&#65292;&#20026;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#24182;&#36866;&#24212;&#26032;&#25968;&#25454;&#21040;&#26469;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07117</link><description>&lt;p&gt;
PILOT&#65306;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
PILOT: A Pre-Trained Model-Based Continual Learning Toolbox. (arXiv:2309.07117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PILOT&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;&#65292;&#20026;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#24182;&#36866;&#24212;&#26032;&#25968;&#25454;&#21040;&#26469;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#65292;&#20294;&#20027;&#35201;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36816;&#20316;&#65292;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#22686;&#37327;&#23398;&#20064;&#24212;&#36816;&#32780;&#29983;&#65292;&#29992;&#20110;&#22788;&#29702;&#28041;&#21450;&#26032;&#25968;&#25454;&#21040;&#26469;&#30340;&#29616;&#23454;&#22330;&#26223;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#22312;&#19981;&#26029;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#24341;&#36215;&#20102;&#20247;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#30340;&#24378;&#22823;&#24615;&#33021;&#20026;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#22240;&#27492;&#65292;&#25506;&#32034;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#21033;&#29992;PTMs&#24050;&#32463;&#25104;&#20026;&#24517;&#38656;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PILOT&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;&#12290;&#19968;&#26041;&#38754;&#65292;PILOT&#23454;&#26045;&#20102;&#19968;&#20123;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#26032;&#29677;&#32423;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;L2P&#12289;DualPrompt&#21644;CODA-Prompt&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;PILOT&#20063;&#36866;&#24212;&#20102;&#20856;&#22411;&#30340;&#29677;&#32423;&#22686;&#37327;&#23398;&#20064;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
While traditional machine learning can effectively tackle a wide range of problems, it primarily operates within a closed-world setting, which presents limitations when dealing with streaming data. As a solution, incremental learning emerges to address real-world scenarios involving new data's arrival. Recently, pre-training has made significant advancements and garnered the attention of numerous researchers. The strong performance of these pre-trained models (PTMs) presents a promising avenue for developing continual learning algorithms that can effectively adapt to real-world scenarios. Consequently, exploring the utilization of PTMs in incremental learning has become essential. This paper introduces a pre-trained model-based continual learning toolbox known as PILOT. On the one hand, PILOT implements some state-of-the-art class-incremental learning algorithms based on pre-trained models, such as L2P, DualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical class-incre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#35270;&#39057;&#35828;&#35805;&#20154;&#39564;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#21644;&#38750;&#21516;&#27493;&#37319;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#35828;&#35805;&#20154;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07115</link><description>&lt;p&gt;
&#38754;&#21521;&#38899;&#35270;&#39057;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised Multi-Task Learning for Audio-Visual Speaker Verification. (arXiv:2309.07115v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#35270;&#39057;&#35828;&#35805;&#20154;&#39564;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#21644;&#38750;&#21516;&#27493;&#37319;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#35828;&#35805;&#20154;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#38024;&#23545;&#24320;&#25918;&#38598;&#38899;&#35270;&#39057;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#20010;&#20154;&#34920;&#31034;&#12290;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#65288;DML&#65289;&#26041;&#27861;&#36890;&#24120;&#22312;&#35813;&#38382;&#39064;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22240;&#20026;&#22312;&#26032;&#30340;&#21644;&#26410;&#35265;&#36807;&#30340;&#31867;&#19978;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;DML&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#24102;&#26377;&#24369;&#26631;&#31614;&#30340;&#36741;&#21161;&#20219;&#21153;&#21487;&#20197;&#22686;&#21152;&#23398;&#20064;&#21040;&#30340;&#35828;&#35805;&#20154;&#34920;&#31034;&#30340;&#32039;&#20945;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#24191;&#20041;&#31471;&#21040;&#31471;&#25439;&#22833;&#65288;GE2E&#65289;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#38899;&#35270;&#39057;&#31354;&#38388;&#20013;&#23454;&#29616;&#31454;&#20105;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#20102;&#19968;&#31181;&#38750;&#21516;&#27493;&#38899;&#35270;&#39057;&#37319;&#26679;&#38543;&#26426;&#31574;&#30053;&#65292;&#24050;&#32463;&#26174;&#31034;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#35828;&#35805;&#20154;&#39564;&#35777;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#25253;&#21578;&#20102;VoxCeleb1-O/E&#30340;&#19977;&#20010;&#23448;&#26041;&#35797;&#39564;&#21015;&#34920;&#19978;&#30340;0.244&#65285;&#65292;0.252&#65285;&#65292;0.441&#65285;&#30340;&#31561;&#35823;&#24046;&#29575;&#65288;EER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a methodology for achieving robust multimodal person representations optimized for open-set audio-visual speaker verification. Distance Metric Learning (DML) approaches have typically dominated this problem space, owing to strong performance on new and unseen classes. In our work, we explored multitask learning techniques to further boost performance of the DML approach and show that an auxiliary task with weak labels can increase the compactness of the learned speaker representation. We also extend the Generalized end-to-end loss (GE2E) to multimodal inputs and demonstrate that it can achieve competitive performance in an audio-visual space. Finally, we introduce a non-synchronous audio-visual sampling random strategy during training time that has shown to improve generalization. Our network achieves state of the art performance for speaker verification, reporting 0.244%, 0.252%, 0.441% Equal Error Rate (EER) on the three official trial lists of VoxCeleb1-O/E
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#24615;&#28145;&#24230;&#32534;&#30721;&#33021;&#22815;&#21033;&#29992;&#20844;&#20849;&#39046;&#22495;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20197;&#23569;&#37327;&#38543;&#26426;&#36873;&#25321;&#27880;&#37322;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#22522;&#20110;&#34917;&#19969;&#30340;&#20998;&#31867;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#37327;&#21270;&#27169;&#22411;&#32622;&#20449;&#24230;&#65292;&#24110;&#21161;&#19987;&#23478;&#36827;&#34892;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.07113</link><description>&lt;p&gt;
&#23545;&#27604;&#24615;&#28145;&#24230;&#32534;&#30721;&#23454;&#29616;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#32452;&#32455;&#30149;&#29702;&#23398;
&lt;/p&gt;
&lt;p&gt;
Contrastive Deep Encoding Enables Uncertainty-aware Machine-learning-assisted Histopathology. (arXiv:2309.07113v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07113
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#24615;&#28145;&#24230;&#32534;&#30721;&#33021;&#22815;&#21033;&#29992;&#20844;&#20849;&#39046;&#22495;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20197;&#23569;&#37327;&#38543;&#26426;&#36873;&#25321;&#27880;&#37322;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#22522;&#20110;&#34917;&#19969;&#30340;&#20998;&#31867;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#37327;&#21270;&#27169;&#22411;&#32622;&#20449;&#24230;&#65292;&#24110;&#21161;&#19987;&#23478;&#36827;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#20174;&#25968;&#30334;&#19975;&#20010;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#23398;&#20064;&#20020;&#24202;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#21307;&#38498;&#12289;&#27599;&#31181;&#30284;&#30151;&#31867;&#22411;&#21644;&#27599;&#20010;&#35786;&#26029;&#20219;&#21153;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#26159;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#34429;&#28982;&#32570;&#20047;&#21487;&#38752;&#30340;&#27880;&#37322;&#65292;&#20294;&#26159;&#20844;&#20849;&#39046;&#22495;&#20013;&#26377;&#21487;&#29992;&#30340;&#21315;&#20806;&#23383;&#33410;&#32423;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#26377;&#24847;&#35782;&#22320;&#21033;&#29992;&#36825;&#20123;&#22823;&#22411;&#25968;&#25454;&#38598;&#26469;&#39044;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#20197;&#32534;&#30721;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#37096;&#20998;&#24102;&#26377;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25191;&#34892;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#27604;&#20110;&#20854;&#20182;&#26368;&#26032;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;1-10%&#30340;&#38543;&#26426;&#36873;&#25321;&#27880;&#37322;&#24773;&#20917;&#19979;&#36798;&#21040;&#22522;&#20110;&#34917;&#19969;&#30340;&#20998;&#31867;&#30340;&#26368;&#26032;&#27700;&#24179;&#65288;SOTA&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#37327;&#21270;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#32622;&#20449;&#24230;&#12290;&#37327;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#26377;&#21161;&#20110;&#19987;&#23478;&#36827;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network models can learn clinically relevant features from millions of histopathology images. However generating high-quality annotations to train such models for each hospital, each cancer type, and each diagnostic task is prohibitively laborious. On the other hand, terabytes of training data -- while lacking reliable annotations -- are readily available in the public domain in some cases. In this work, we explore how these large datasets can be consciously utilized to pre-train deep networks to encode informative representations. We then fine-tune our pre-trained models on a fraction of annotated training data to perform specific downstream tasks. We show that our approach can reach the state-of-the-art (SOTA) for patch-level classification with only 1-10% randomly selected annotations compared to other SOTA approaches. Moreover, we propose an uncertainty-aware loss function, to quantify the model confidence during inference. Quantified uncertainty helps experts select th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23376;&#32452;&#28151;&#21512;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#20195;&#34920;&#20302;&#27604;&#20363;&#32676;&#20307;&#30340;&#26032;&#26679;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#30340;&#24179;&#34913;&#65292;&#24182;&#19988;&#21033;&#29992;&#35813;&#26041;&#27861;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07110</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#32452;&#28151;&#21512;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation via Subgroup Mixup for Improving Fairness. (arXiv:2309.07110v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23376;&#32452;&#28151;&#21512;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#20195;&#34920;&#20302;&#27604;&#20363;&#32676;&#20307;&#30340;&#26032;&#26679;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#30340;&#24179;&#34913;&#65292;&#24182;&#19988;&#21033;&#29992;&#35813;&#26041;&#27861;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#23376;&#32452;&#38388;&#28151;&#21512;&#26469;&#22686;&#24378;&#25968;&#25454;&#20197;&#25552;&#39640;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24212;&#29992;&#37117;&#23384;&#22312;&#30528;&#26576;&#20123;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#36825;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#25110;&#21453;&#26144;&#20102;&#31038;&#20250;&#20559;&#35265;&#12290;&#21463;&#21040;mixup&#22312;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20004;&#20004;&#28151;&#21512;&#30340;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#40723;&#21169;&#20026;&#25152;&#26377;&#23376;&#32452;&#23454;&#29616;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#38024;&#23545;&#32676;&#20307;&#20844;&#24179;&#24615;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#20801;&#35768;&#25105;&#20204;&#28155;&#21152;&#26032;&#30340;&#20195;&#34920;&#20302;&#27604;&#20363;&#32676;&#20307;&#30340;&#26679;&#26412;&#65292;&#20197;&#24179;&#34913;&#20122;&#32676;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;mixup&#30340;&#27867;&#21270;&#33021;&#21147;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#28151;&#21512;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;&#22312;&#21512;&#25104;&#27169;&#25311;&#21644;&#23454;&#38469;&#22522;&#20934;&#20844;&#24179;&#20998;&#31867;&#25968;&#25454;&#19978;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#19978;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#29978;&#33267;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose data augmentation via pairwise mixup across subgroups to improve group fairness. Many real-world applications of machine learning systems exhibit biases across certain groups due to under-representation or training data that reflects societal biases. Inspired by the successes of mixup for improving classification performance, we develop a pairwise mixup scheme to augment training data and encourage fair and accurate decision boundaries for all subgroups. Data augmentation for group fairness allows us to add new samples of underrepresented groups to balance subpopulations. Furthermore, our method allows us to use the generalization ability of mixup to improve both fairness and accuracy. We compare our proposed mixup to existing data augmentation and bias mitigation approaches on both synthetic simulations and real-world benchmark fair classification data, demonstrating that we are able to achieve fair outcomes with robust if not improved accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#36895;&#24230;&#24615;&#33021;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#30740;&#31350;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#20837;&#20998;&#31867;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;MARL&#31639;&#27861;&#20316;&#20026;&#30446;&#26631;&#22522;&#20934;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#20248;&#21270;&#22870;&#21169;&#30340;&#31639;&#27861;&#22312;&#35757;&#32451;&#26102;&#38388;&#36895;&#24230;&#24615;&#33021;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.07108</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36895;&#24230;&#24615;&#33021;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Characterizing Speed Performance of Multi-Agent Reinforcement Learning. (arXiv:2309.07108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#36895;&#24230;&#24615;&#33021;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#30740;&#31350;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#20837;&#20998;&#31867;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;MARL&#31639;&#27861;&#20316;&#20026;&#30446;&#26631;&#22522;&#20934;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#20248;&#21270;&#22870;&#21169;&#30340;&#31639;&#27861;&#22312;&#35757;&#32451;&#26102;&#38388;&#36895;&#24230;&#24615;&#33021;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#22823;&#25968;&#25454;&#24212;&#29992;&#65288;&#22914;&#26234;&#33021;&#30005;&#32593;&#12289;&#30417;&#25511;&#31561;&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#26426;&#21046;&#26469;&#25913;&#36827;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20197;&#25552;&#39640;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20248;&#21270;&#36890;&#24120;&#20250;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#26041;&#38754;&#20135;&#29983;&#36739;&#22823;&#36127;&#25285;&#65292;&#20174;&#32780;&#23548;&#33268;&#31471;&#21040;&#31471;&#35757;&#32451;&#26102;&#38388;&#30340;&#36895;&#24230;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36895;&#24230;&#24615;&#33021;&#65288;&#21363;&#24310;&#36831;&#21463;&#38480;&#21534;&#21520;&#37327;&#65289;&#20316;&#20026;MARL&#23454;&#29616;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#21152;&#36895;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#20010;MARL&#31639;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#21253;&#25324;&#65288;1&#65289;&#35757;&#32451;&#26041;&#26696;&#21644;&#65288;2&#65289;&#36890;&#20449;&#26041;&#27861;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;MARL&#31639;&#27861;&#8212;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;MADDPG&#65289;&#12289;&#38754;&#21521;&#30446;&#26631;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#19982;&#21512;&#20316;&#65288;ToM2C&#65289;&#21644;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;RL&#65288;NeurComm&#65289;&#8212;&#20316;&#20026;&#30446;&#26631;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) has achieved significant success in large-scale AI systems and big-data applications such as smart grids, surveillance, etc. Existing advancements in MARL algorithms focus on improving the rewards obtained by introducing various mechanisms for inter-agent cooperation. However, these optimizations are usually compute- and memory-intensive, thus leading to suboptimal speed performance in end-to-end training time. In this work, we analyze the speed performance (i.e., latency-bounded throughput) as the key metric in MARL implementations. Specifically, we first introduce a taxonomy of MARL algorithms from an acceleration perspective categorized by (1) training scheme and (2) communication method. Using our taxonomy, we identify three state-of-the-art MARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG), Target-oriented Multi-agent Communication and Cooperation (ToM2C), and Networked Multi-Agent RL (NeurComm) - as target benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#26469;&#20943;&#36731;&#20840;&#23616;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07085</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#35774;&#22791;&#19978;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32676;&#20307;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Group Bias in Federated Learning for Heterogeneous Devices. (arXiv:2309.07085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#26469;&#20943;&#36731;&#20840;&#23616;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#27491;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#23853;&#38706;&#22836;&#35282;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36793;&#32536;&#37096;&#32626;&#26159;&#24322;&#26500;&#30340;&#65292;&#21363;&#23427;&#20204;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#29615;&#22659;&#22312;&#37096;&#32626;&#20013;&#21508;&#19981;&#30456;&#21516;&#12290;&#36825;&#31181;&#36793;&#32536;&#24322;&#26500;&#36829;&#21453;&#20102;&#26412;&#22320;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#29420;&#31435;&#19988;&#20998;&#24067;&#30456;&#21516; (IID) &#30340;&#29305;&#24615;&#65292;&#20135;&#29983;&#20102;&#26377;&#20559;&#35265;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#21363;&#23545;&#29305;&#23450;&#31038;&#21306;&#25110;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#21644;&#27495;&#35270;&#12290;&#29616;&#26377;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#21482;&#20851;&#27880;&#38750;IID&#25968;&#25454;&#20013;&#30001;&#26631;&#31614;&#24322;&#26500;&#24341;&#36215;&#30340;&#20559;&#35265;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#30001;&#29305;&#24449;&#24322;&#26500;&#23548;&#33268;&#30340;&#39046;&#22495;&#21464;&#21270;&#65292;&#20063;&#27809;&#26377;&#35299;&#20915;&#20840;&#23616;&#32676;&#20307;&#20844;&#24179;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#19981;&#22686;&#21152;&#36164;&#28304;&#21033;&#29992;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#32676;&#20307;&#20559;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;&#24179;&#22343;&#26465;&#20214;&#27010;&#29575;&#26469;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients and produces biased global models i.e. models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity and do not address global group-fairness property.  Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy and without resource utilization overhead. Our main idea is to leverage average conditional probabilities to compute a cross-domain group \textit{importance we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#23398;&#20064;&#26041;&#27861;&#65288;BDyMA&#65289;&#26469;&#35299;&#20915;&#22312;&#21457;&#29616;&#22823;&#33041;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#26694;&#26550;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#21644;&#26356;&#31232;&#30095;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#20351;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#21462;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#12290;</title><link>http://arxiv.org/abs/2309.07080</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#23398;&#20064;&#65306;&#22312;&#21457;&#29616;&#22823;&#33041;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain. (arXiv:2309.07080v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#23398;&#20064;&#26041;&#27861;&#65288;BDyMA&#65289;&#26469;&#35299;&#20915;&#22312;&#21457;&#29616;&#22823;&#33041;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#26694;&#26550;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#21644;&#26356;&#31232;&#30095;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#20351;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#21462;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#21160;&#24577;&#25928;&#24212;&#36830;&#25509;&#32452;&#65288;DEC&#65289;&#21487;&#20197;&#25581;&#31034;&#22823;&#33041;&#30340;&#22797;&#26434;&#26426;&#21046;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#35780;&#20998;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#21457;&#29616;&#26041;&#27861;&#22312;&#25552;&#21462;&#22240;&#26524;&#32467;&#26500;&#21644;&#25512;&#26029;&#26377;&#25928;&#36830;&#25509;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#23398;&#20064;DEC&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#19968;&#20010;&#26159;&#39640;&#32500;&#21160;&#24577;DAG&#21457;&#29616;&#26041;&#27861;&#30340;&#26681;&#26412;&#26080;&#33021;&#21147;&#65292;&#21478;&#19968;&#20010;&#26159;fMRI&#25968;&#25454;&#36136;&#37327;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;M-&#30697;&#38453;&#26080;&#29615;&#29305;&#24615;&#30340;&#36125;&#21494;&#26031;&#21160;&#24577;DAG&#23398;&#20064;&#65288;BDyMA&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#21457;&#29616;DEC&#20013;&#30340;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#22240;&#26524;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#21452;&#21521;&#36793;&#32536;&#12290;&#21033;&#29992;BDyMA&#26041;&#27861;&#20013;&#30340;&#26080;&#32422;&#26463;&#26694;&#26550;&#22312;&#26816;&#27979;&#39640;&#32500;&#32593;&#32476;&#26041;&#38754;&#21487;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#23454;&#29616;&#26356;&#31232;&#30095;&#30340;&#32467;&#26524;&#65292;&#20351;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#21462;DEC&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the complex mechanisms of the brain can be unraveled by extracting the Dynamic Effective Connectome (DEC). Recently, score-based Directed Acyclic Graph (DAG) discovery methods have shown significant improvements in extracting the causal structure and inferring effective connectivity. However, learning DEC through these methods still faces two main challenges: one with the fundamental impotence of high-dimensional dynamic DAG discovery methods and the other with the low quality of fMRI data. In this paper, we introduce Bayesian Dynamic DAG learning with M-matrices Acyclicity characterization \textbf{(BDyMA)} method to address the challenges in discovering DEC. The presented dynamic causal model enables us to discover bidirected edges as well. Leveraging an unconstrained framework in the BDyMA method leads to more accurate results in detecting high-dimensional networks, achieving sparser outcomes, making it particularly suitable for extracting DEC. Additionally, the score f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#21457;&#29616;&#22312;&#19968;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#31867;&#21035;&#20013;&#65292;&#35745;&#31639;&#21644;&#39564;&#35777;&#29702;&#24819;&#30340;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#29978;&#33267;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.07072</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21487;&#39564;&#35777;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#30340;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning. (arXiv:2309.07072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#21457;&#29616;&#22312;&#19968;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#31867;&#21035;&#20013;&#65292;&#35745;&#31639;&#21644;&#39564;&#35777;&#29702;&#24819;&#30340;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#29978;&#33267;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#29702;&#35770;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#20998;&#24067;&#26080;&#20851;&#26694;&#26550;&#21644;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#30340;&#31639;&#27861;&#65292;&#21487;&#33021;&#36824;&#21463;&#21040;&#19968;&#20123;&#26435;&#37325;&#27491;&#21017;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#24456;&#22810;&#20219;&#21153;&#26469;&#35828;&#65292;&#22312;&#19978;&#36848;&#35774;&#32622;&#19979;&#35745;&#31639;&#21644;&#39564;&#35777;&#29702;&#24819;&#30340;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#65292;&#29978;&#33267;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#21363;&#20351;&#22312;&#32473;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#31867;&#21035;&#20013;&#23384;&#22312;&#36825;&#26679;&#30340;&#29702;&#24819;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we assess the theoretical limitations of determining guaranteed stability and accuracy of neural networks in classification tasks. We consider classical distribution-agnostic framework and algorithms minimising empirical risks and potentially subjected to some weights regularisation. We show that there is a large family of tasks for which computing and verifying ideal stable and accurate neural networks in the above settings is extremely challenging, if at all possible, even when such ideal solutions exist within the given class of neural architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20195;&#30721;&#20248;&#21270;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20197;7B&#21442;&#25968;&#30340;transformer&#27169;&#22411;&#20026;&#20363;&#65292;&#36890;&#36807;&#39044;&#27979;&#25351;&#20196;&#35745;&#25968;&#21644;&#29983;&#25104;&#20248;&#21270;&#20195;&#30721;&#31561;&#36741;&#21161;&#23398;&#20064;&#20219;&#21153;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20248;&#21270;&#24615;&#33021;&#12290;&#22312;&#22823;&#37327;&#27979;&#35797;&#31243;&#24207;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#32534;&#35793;&#22120;&#30340;&#20248;&#21270;&#25928;&#26524;&#25552;&#39640;&#20102;3.0%&#65292;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#21916;&#30340;&#24378;&#22823;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.07062</link><description>&lt;p&gt;
&#29992;&#20110;&#32534;&#35793;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Compiler Optimization. (arXiv:2309.07062v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20195;&#30721;&#20248;&#21270;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20197;7B&#21442;&#25968;&#30340;transformer&#27169;&#22411;&#20026;&#20363;&#65292;&#36890;&#36807;&#39044;&#27979;&#25351;&#20196;&#35745;&#25968;&#21644;&#29983;&#25104;&#20248;&#21270;&#20195;&#30721;&#31561;&#36741;&#21161;&#23398;&#20064;&#20219;&#21153;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20248;&#21270;&#24615;&#33021;&#12290;&#22312;&#22823;&#37327;&#27979;&#35797;&#31243;&#24207;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#32534;&#35793;&#22120;&#30340;&#20248;&#21270;&#25928;&#26524;&#25552;&#39640;&#20102;3.0%&#65292;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#21916;&#30340;&#24378;&#22823;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20195;&#30721;&#20248;&#21270;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;7B&#21442;&#25968;&#30340;transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#20248;&#21270;LLVM&#27719;&#32534;&#30340;&#20195;&#30721;&#22823;&#23567;&#12290;&#35813;&#27169;&#22411;&#20197;&#26410;&#20248;&#21270;&#30340;&#27719;&#32534;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#19968;&#32452;&#26368;&#20339;&#20248;&#21270;&#31243;&#24207;&#30340;&#32534;&#35793;&#22120;&#36873;&#39033;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#20248;&#21270;&#21069;&#21518;&#30340;&#25351;&#20196;&#35745;&#25968;&#21644;&#20248;&#21270;&#21518;&#30340;&#20195;&#30721;&#26412;&#36523;&#12290;&#36825;&#20123;&#36741;&#21161;&#23398;&#20064;&#20219;&#21153;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#28145;&#24230;&#12290;&#25105;&#20204;&#22312;&#19968;&#22871;&#22823;&#22411;&#27979;&#35797;&#31243;&#24207;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#25351;&#20196;&#35745;&#25968;&#26041;&#38754;&#27604;&#32534;&#35793;&#22120;&#25552;&#39640;&#20102;3.0%&#65292;&#36229;&#36807;&#20102;&#38656;&#35201;&#25968;&#21315;&#27425;&#32534;&#35793;&#30340;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#26174;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22823;&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#65292;91%&#30340;&#26102;&#38388;&#29983;&#25104;&#21487;&#32534;&#35793;&#30340;&#20195;&#30721;&#65292;&#24182;70%&#30340;&#26102;&#38388;&#33021;&#23436;&#32654;&#27169;&#25311;&#32534;&#35793;&#22120;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding.  We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#23646;&#24615;&#20998;&#24067;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.07056</link><description>&lt;p&gt;
&#28145;&#24230;&#37327;&#23376;&#22270;&#20687;&#27169;&#25311;&#65306;&#35299;&#26512;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#23454;&#39564;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments. (arXiv:2309.07056v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#65292;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#23646;&#24615;&#20998;&#24067;&#65292;&#24182;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#20419;&#36827;&#26032;&#30340;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#20854;&#36923;&#36753;&#32972;&#21518;&#30340;&#19981;&#36879;&#26126;&#24615;&#32473;&#35299;&#37322;&#20854;&#21457;&#29616;&#30340;&#25361;&#25112;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;&#8220;inception&#8221;&#25110;&#8220;&#28145;&#24230;&#26790;&#22659;&#8221;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#34987;&#21457;&#26126;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#26469;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#23545;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#25925;&#20107;&#20174;&#23545;&#37327;&#23376;&#31995;&#32479;&#23646;&#24615;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24320;&#22987;&#12290;&#32463;&#36807;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#8220;&#21453;&#36716;&#8221;&#31070;&#32463;&#32593;&#32476;--&#23454;&#38469;&#19978;&#26159;&#35810;&#38382;&#23427;&#22914;&#20309;&#24819;&#35937;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#37327;&#23376;&#31995;&#32479;&#65292;&#20197;&#21450;&#22914;&#20309;&#36830;&#32493;&#20462;&#25913;&#37327;&#23376;&#31995;&#32479;&#20197;&#25913;&#21464;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#32593;&#32476;&#21487;&#20197;&#25913;&#21464;&#37327;&#23376;&#31995;&#32479;&#30340;&#21021;&#22987;&#23646;&#24615;&#20998;&#24067;&#65292;&#25105;&#20204;&#21487;&#20197;&#27010;&#24565;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#36739;&#27973;&#23618;&#65292;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#31616;&#21333;&#30340;&#23646;&#24615;&#65292;&#32780;&#22312;&#36739;&#28145;&#23618;&#27425;&#19978;...&#65288;&#20869;&#23481;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Despite their promise to facilitate new scientific discoveries, the opaqueness of neural networks presents a challenge in interpreting the logic behind their findings. Here, we use a eXplainable-AI (XAI) technique called $inception$ or $deep$ $dreaming$, which has been invented in machine learning for computer vision. We use this techniques to explore what neural networks learn about quantum optics experiments. Our story begins by training a deep neural networks on the properties of quantum systems. Once trained, we "invert" the neural network -- effectively asking how it imagines a quantum system with a specific property, and how it would continuously modify the quantum system to change a property. We find that the network can shift the initial distribution of properties of the quantum system, and we can conceptualize the learned strategies of the neural network. Interestingly, we find that, in the first layers, the neural network identifies simple properties, while in the deeper ones
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#35745;&#31639;&#26041;&#27861;&#65292;&#21033;&#29992;&#26497;&#38480;&#23398;&#20064;&#26426;&#25193;&#23637;&#20102;&#20302;&#32500;&#21040;&#39640;&#32500;&#30340;&#36924;&#36817;&#65292;&#36890;&#36807;&#38543;&#26426;&#25554;&#20540;&#28857;&#26469;&#28385;&#36275;PDE&#21644;&#36793;&#30028;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#35299;&#24471;&#21040;&#32593;&#32476;&#21442;&#25968;&#30340;&#35757;&#32451;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.07049</link><description>&lt;p&gt;
&#22522;&#20110;&#26497;&#38480;&#23398;&#20064;&#26426;&#30340;&#39640;&#32500;&#35745;&#31639;PDE&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Extreme Learning Machine-Based Method for Computational PDEs in Higher Dimensions. (arXiv:2309.07049v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#35745;&#31639;&#26041;&#27861;&#65292;&#21033;&#29992;&#26497;&#38480;&#23398;&#20064;&#26426;&#25193;&#23637;&#20102;&#20302;&#32500;&#21040;&#39640;&#32500;&#30340;&#36924;&#36817;&#65292;&#36890;&#36807;&#38543;&#26426;&#25554;&#20540;&#28857;&#26469;&#28385;&#36275;PDE&#21644;&#36793;&#30028;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#35299;&#24471;&#21040;&#32593;&#32476;&#21442;&#25968;&#30340;&#35757;&#32451;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#21463;&#21040;&#36825;&#31181;&#31867;&#22411;&#32593;&#32476;&#30340;&#26222;&#36941;&#36924;&#36817;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#23558;&#26497;&#38480;&#23398;&#20064;&#26426;(ELM)&#26041;&#27861;&#20174;&#20302;&#32500;&#25193;&#23637;&#21040;&#39640;&#32500;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20013;&#65292;d&#32500;&#26410;&#30693;&#35299;&#22330;&#30001;&#19968;&#20010;&#38543;&#26426;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#20854;&#20013;&#38544;&#34255;&#23618;&#21442;&#25968;&#34987;&#38543;&#26426;&#36171;&#20540;&#24182;&#22266;&#23450;&#65292;&#32780;&#36755;&#20986;&#23618;&#21442;&#25968;&#21017;&#34987;&#35757;&#32451;&#12290;PDE&#21644;&#36793;&#30028;/&#21021;&#22987;&#26465;&#20214;&#65292;&#20197;&#21450;&#36830;&#32493;&#24615;&#26465;&#20214;(&#23545;&#20110;&#26041;&#27861;&#30340;&#23616;&#37096;&#21464;&#20307;)&#37117;&#22312;&#19968;&#32452;&#38543;&#26426;&#20869;&#37096;/&#36793;&#30028;&#25554;&#20540;&#28857;&#19978;&#24471;&#21040;&#28385;&#36275;&#12290;&#26368;&#23567;&#20108;&#20056;&#35299;&#25552;&#20379;&#20102;&#32593;&#32476;&#21442;&#25968;&#30340;&#35757;&#32451;&#20540;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#20195;&#25968;&#26041;&#31243;&#32452;&#30340;&#35299;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#32422;&#26463;&#34920;&#36798;&#24335;&#22522;&#20110;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We present two effective methods for solving high-dimensional partial differential equations (PDE) based on randomized neural networks. Motivated by the universal approximation property of this type of networks, both methods extend the extreme learning machine (ELM) approach from low to high dimensions. With the first method the unknown solution field in $d$ dimensions is represented by a randomized feed-forward neural network, in which the hidden-layer parameters are randomly assigned and fixed while the output-layer parameters are trained. The PDE and the boundary/initial conditions, as well as the continuity conditions (for the local variant of the method), are enforced on a set of random interior/boundary collocation points. The resultant linear or nonlinear algebraic system, through its least squares solution, provides the trained values for the network parameters. With the second method the high-dimensional PDE problem is reformulated through a constrained expression based on an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#29992;&#20110;&#27604;&#36739;&#26377;&#21521;&#22270;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#22270;&#25968;&#25454;&#21644;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#25512;&#26029;&#30340;&#23454;&#38469;&#32454;&#32990;&#38388;&#36890;&#35759;&#22270;&#23545;&#20854;&#30456;&#23545;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.07030</link><description>&lt;p&gt;
&#26377;&#21521;&#21152;&#26435;&#22270;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#65306;&#20197;&#32454;&#32990;&#38388;&#36890;&#35759;&#32593;&#32476;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks. (arXiv:2309.07030v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#29992;&#20110;&#27604;&#36739;&#26377;&#21521;&#22270;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#22270;&#25968;&#25454;&#21644;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#25512;&#26029;&#30340;&#23454;&#38469;&#32454;&#32990;&#38388;&#36890;&#35759;&#22270;&#23545;&#20854;&#30456;&#23545;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#27604;&#36739;&#26368;&#20248;&#36755;&#36816;&#22270;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#26368;&#20248;&#36755;&#36816;&#24341;&#36215;&#30340;&#36317;&#31163;&#26082;&#25552;&#20379;&#20102;&#22270;&#20043;&#38388;&#30340;&#21512;&#29702;&#24230;&#37327;&#65292;&#21448;&#36890;&#36807;&#36755;&#36816;&#35745;&#21010;&#30340;&#21487;&#35299;&#37322;&#25551;&#36848;&#20102;&#22270;&#20043;&#38388;&#30456;&#20851;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#23545;&#31216;&#24615;&#65292;&#36890;&#24120;&#32771;&#34385;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#20027;&#35201;&#29992;&#20110;&#26080;&#21521;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#21464;&#20307;&#30340;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#26377;&#21521;&#22270;&#65306;&#65288;i&#65289;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#65288;Wasserstein&#65289;&#21644;&#65288;ii&#65289;Gromov-Wasserstein&#65288;GW&#65289;&#36317;&#31163;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#36317;&#31163;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#20223;&#30495;&#22270;&#25968;&#25454;&#21644;&#22522;&#20110;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#25512;&#26029;&#30340;&#23454;&#38469;&#26377;&#21521;&#32454;&#32990;&#38388;&#36890;&#35759;&#22270;&#19978;&#30340;&#30456;&#23545;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparing graphs of optimal transport has recently gained significant attention, as the distances induced by optimal transport provide both a principled metric between graphs as well as an interpretable description of the associated changes between graphs in terms of a transport plan. As the lack of symmetry introduces challenges in the typically considered formulations, optimal transport distances for graphs have mostly been developed for undirected graphs. Here, we propose two distance measures to compare directed graphs based on variants of optimal transport: (i) an earth movers distance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate these two distances and discuss their relative performance for both simulated graph data and real-world directed cell-cell communication graphs, inferred from single-cell RNA-seq data.
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#30340;&#25490;&#24207;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.06991</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Contrast-Consistent Ranking with Language Models. (arXiv:2309.06991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06991
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#19968;&#33268;&#25490;&#24207;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#30340;&#25490;&#24207;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#22522;&#20110;&#25490;&#24207;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#26159;&#22788;&#29702;&#19978;&#19979;&#25991;&#25490;&#21517;&#20219;&#21153;&#30340;&#24378;&#22823;&#35299;&#20915;&#32773;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#37197;&#23545;&#12289;&#28857;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#25490;&#24207;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#20180;&#32454;&#26657;&#20934;&#21644;&#38480;&#21046;&#35299;&#30721;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#22312;&#20135;&#29983;&#30340;&#25490;&#24207;&#20013;&#20063;&#19981;&#24635;&#26159;&#33258;&#27965;&#30340;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#19968;&#31181;&#21463;&#26080;&#30417;&#30563;&#25506;&#27979;&#26041;&#27861;Contrast-Consistent Search&#65288;CCS&#65289;&#21551;&#21457;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#20010;&#24819;&#27861;&#26159;&#35757;&#32451;&#19968;&#20010;&#21463;&#36923;&#36753;&#32422;&#26463;&#24341;&#23548;&#30340;&#25506;&#27979;&#27169;&#22411;&#65306;&#27169;&#22411;&#23545;&#19968;&#20010;&#35821;&#21477;&#21450;&#20854;&#21542;&#23450;&#30340;&#34920;&#31034;&#24517;&#39035;&#22312;&#22810;&#20010;&#35821;&#21477;&#20013;&#22987;&#32456;&#26144;&#23556;&#21040;&#23545;&#27604;&#30340;&#30495;-&#20551;&#26497;&#28857;&#12290;&#25105;&#20204;&#20551;&#35774;&#31867;&#20284;&#30340;&#32422;&#26463;&#36866;&#29992;&#20110;&#25152;&#26377;&#39033;&#36890;&#36807;&#19968;&#33268;&#24615;&#23545;&#30456;&#20851;&#25490;&#24207;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank reviews by sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting techniques to elicit a language model's ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probing model guided by a logical constraint: a model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent pair
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASTERKEY&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#38024;&#23545;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#12290;&#19982;&#20197;&#24448;&#25915;&#20987;&#19981;&#21516;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#25915;&#20987;&#32773;&#23545;&#30446;&#26631;&#29992;&#25143;&#27809;&#26377;&#20219;&#20309;&#20102;&#35299;&#12290;&#36890;&#36807;&#23884;&#20837;&#35828;&#35805;&#20154;&#29305;&#24449;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#38598;&#25104;&#20449;&#36947;&#22833;&#30495;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#30772;&#22351;&#22810;&#20010;&#27969;&#34892;&#30340;SV&#27169;&#22411;&#65292;&#36798;&#21040;100&#65285;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.06981</link><description>&lt;p&gt;
MASTERKEY: &#23545;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#30340;&#23454;&#38469;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems. (arXiv:2309.06981v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06981
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASTERKEY&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#38024;&#23545;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#12290;&#19982;&#20197;&#24448;&#25915;&#20987;&#19981;&#21516;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#25915;&#20987;&#32773;&#23545;&#30446;&#26631;&#29992;&#25143;&#27809;&#26377;&#20219;&#20309;&#20102;&#35299;&#12290;&#36890;&#36807;&#23884;&#20837;&#35828;&#35805;&#20154;&#29305;&#24449;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#38598;&#25104;&#20449;&#36947;&#22833;&#30495;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#30772;&#22351;&#22810;&#20010;&#27969;&#34892;&#30340;SV&#27169;&#22411;&#65292;&#36798;&#21040;100&#65285;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#39564;&#35777;&#65288;SV&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#31227;&#21160;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#30340;&#35821;&#38899;&#29305;&#24449;&#26469;&#35748;&#35777;&#21512;&#27861;&#29992;&#25143;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASTERKEY&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20197;&#30772;&#22351;SV&#27169;&#22411;&#12290;&#19982;&#20197;&#24448;&#30340;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#22312;&#25915;&#20987;&#32773;&#23545;&#30446;&#26631;&#29992;&#25143;&#27809;&#26377;&#20219;&#20309;&#20102;&#35299;&#30340;&#23454;&#38469;&#29615;&#22659;&#19979;&#36827;&#34892;&#25915;&#20987;&#12290;&#20026;&#20102;&#35774;&#35745;MASTERKEY&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#38024;&#23545;&#26410;&#35265;&#30446;&#26631;&#30340;&#20013;&#27602;&#25915;&#20987;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21518;&#38376;&#65292;&#21487;&#20197;&#25915;&#20987;&#20219;&#24847;&#30446;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#35828;&#35805;&#20154;&#30340;&#29305;&#24449;&#21644;&#35821;&#20041;&#20449;&#24687;&#23884;&#20837;&#21040;&#21518;&#38376;&#20013;&#65292;&#20351;&#20854;&#19981;&#21487;&#23519;&#35273;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#20449;&#36947;&#22833;&#30495;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#21518;&#38376;&#20013;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;6&#20010;&#27969;&#34892;&#30340;SV&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20013;&#27602;&#20102;&#20849;&#35745;53&#20010;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#35302;&#21457;&#22120;&#25915;&#20987;&#20102;16,430&#20010;&#27880;&#20876;&#35828;&#35805;&#20154;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;53&#20010;&#20013;&#27602;&#27169;&#22411;&#20013;&#27880;&#20876;&#30340;310&#20010;&#30446;&#26631;&#35828;&#35805;&#20154;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#36798;&#21040;100&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker Verification (SV) is widely deployed in mobile systems to authenticate legitimate users by using their voice traits. In this work, we propose a backdoor attack MASTERKEY, to compromise the SV models. Different from previous attacks, we focus on a real-world practical setting where the attacker possesses no knowledge of the intended victim. To design MASTERKEY, we investigate the limitation of existing poisoning attacks against unseen targets. Then, we optimize a universal backdoor that is capable of attacking arbitrary targets. Next, we embed the speaker's characteristics and semantics information into the backdoor, making it imperceptible. Finally, we estimate the channel distortion and integrate it into the backdoor. We validate our attack on 6 popular SV models. Specifically, we poison a total of 53 models and use our trigger to attack 16,430 enrolled speakers, composed of 310 target speakers enrolled in 53 poisoned models. Our attack achieves 100% attack success rate with a
&lt;/p&gt;</description></item><item><title>&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06979</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#26159;&#36890;&#29992;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-Regressive Next-Token Predictors are Universal Learners. (arXiv:2309.06979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06979
&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#22312;&#36923;&#36753;&#21644;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#38750;&#20961;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#35757;&#32451;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#31616;&#21333;&#20219;&#21153;&#19978;&#30340;&#32593;&#32476;&#20013;&#20986;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#33258;&#22238;&#24402;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#22914;&#32447;&#24615;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#24403;&#20854;&#22312;&#24605;&#32500;&#38142;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#8212;&#8212;&#38271;&#24230;&#22797;&#26434;&#24230;&#65292;&#23427;&#34913;&#37327;&#20102;&#22312;&#36817;&#20284;&#26576;&#20010;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#24605;&#32500;&#38142;&#24207;&#21015;&#20013;&#25152;&#38656;&#30340;&#20013;&#38388;&#26631;&#35760;&#30340;&#25968;&#37327;&#65292;&#24182;&#20998;&#26512;&#20102;&#38271;&#24230;&#22797;&#26434;&#24230;&#21644;&#20854;&#20182;&#22797;&#26434;&#24615;&#27010;&#24565;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#31616;&#21333;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#22914;&#32447;&#24615;&#32593;&#32476;&#21644;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our resul
&lt;/p&gt;</description></item><item><title>DNNShifter&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;&#35745;&#31639;DNN&#21098;&#26525;&#31995;&#32479;&#65292;&#36890;&#36807;&#24555;&#36895;&#25512;&#23548;&#20986;&#21512;&#36866;&#30340;&#27169;&#22411;&#21464;&#20307;&#26469;&#25552;&#20379;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#65292;&#36866;&#24212;&#31995;&#32479;&#21644;&#32593;&#32476;&#26465;&#20214;&#21464;&#21270;&#30340;&#24037;&#20316;&#36127;&#36733;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.06973</link><description>&lt;p&gt;
DNNShifter: &#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;&#35745;&#31639;DNN&#21098;&#26525;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DNNShifter: An Efficient DNN Pruning System for Edge Computing. (arXiv:2309.06973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06973
&lt;/p&gt;
&lt;p&gt;
DNNShifter&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;&#35745;&#31639;DNN&#21098;&#26525;&#31995;&#32479;&#65292;&#36890;&#36807;&#24555;&#36895;&#25512;&#23548;&#20986;&#21512;&#36866;&#30340;&#27169;&#22411;&#21464;&#20307;&#26469;&#25552;&#20379;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#65292;&#36866;&#24212;&#31995;&#32479;&#21644;&#32593;&#32476;&#26465;&#20214;&#21464;&#21270;&#30340;&#24037;&#20316;&#36127;&#36733;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#30784;&#12290;&#29983;&#20135;&#36136;&#37327;&#30340;DNN&#27169;&#22411;&#36890;&#36807;&#35757;&#32451;&#25968;&#30334;&#19975;&#20010;DNN&#21442;&#25968;&#26469;&#23454;&#29616;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#65292;&#20294;&#36825;&#21344;&#29992;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#23545;&#20110;&#22312;&#32593;&#32476;&#30340;&#26497;&#31471;&#36793;&#32536;&#22788;&#24037;&#20316;&#30340;&#36164;&#28304;&#65288;&#22914;&#20855;&#26377;&#26377;&#38480;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#30340;&#31227;&#21160;&#21644;&#23884;&#20837;&#24335;&#35774;&#22791;&#65289;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#21098;&#26525;&#65292;&#20197;&#21019;&#24314;&#36731;&#37327;&#32423;&#12289;&#26356;&#36866;&#21512;&#36825;&#20123;&#35774;&#22791;&#30340;&#21464;&#20307;&#12290;&#29616;&#26377;&#30340;&#21098;&#26525;&#26041;&#27861;&#26080;&#27861;&#22312;&#19981;&#24341;&#20837;&#26174;&#33879;&#26102;&#38388;&#25104;&#26412;&#21644;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#19982;&#26410;&#21098;&#26525;&#27169;&#22411;&#30456;&#20284;&#30340;&#36136;&#37327;&#27169;&#22411;&#65292;&#25110;&#32773;&#21482;&#38480;&#20110;&#31163;&#32447;&#20351;&#29992;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#20445;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24555;&#36895;&#25512;&#23548;&#20986;&#36866;&#21512;&#30340;&#27169;&#22411;&#21464;&#20307;&#12290;&#27169;&#22411;&#21464;&#20307;&#21487;&#20197;&#22312;&#31995;&#32479;&#21644;&#32593;&#32476;&#26465;&#20214;&#21457;&#29983;&#21464;&#21270;&#20197;&#21305;&#37197;&#24037;&#20316;&#36127;&#36733;&#38656;&#27714;&#26102;&#24555;&#36895;&#20999;&#25442;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DNNShifter&#65292;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;DNN&#35757;&#32451;&#12289;&#31354;&#38388;&#21098;&#26525;&#21644;&#27169;&#22411;&#20999;&#25442;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) underpin many machine learning applications. Production quality DNN models achieve high inference accuracy by training millions of DNN parameters which has a significant resource footprint. This presents a challenge for resources operating at the extreme edge of the network, such as mobile and embedded devices that have limited computational and memory resources. To address this, models are pruned to create lightweight, more suitable variants for these devices. Existing pruning methods are unable to provide similar quality models compared to their unpruned counterparts without significant time costs and overheads or are limited to offline use cases. Our work rapidly derives suitable model variants while maintaining the accuracy of the original model. The model variants can be swapped quickly when system and network conditions change to match workload demand. This paper presents DNNShifter, an end-to-end DNN training, spatial pruning, and model switching syst
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#20013;&#24573;&#35270;&#30340;&#20851;&#38190;&#35201;&#32032; - &#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#23545;&#34917;&#25937;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26102;&#38388;&#25512;&#31227;&#21644;&#20010;&#20307;&#38388;&#31454;&#20105;&#30340;&#24773;&#20917;&#19979;&#65292;&#21021;&#22987;&#30340;&#34917;&#25937;&#24314;&#35758;&#21487;&#33021;&#21464;&#24471;&#19981;&#21487;&#38752;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;&#26102;&#38388;&#21464;&#21270;&#26469;&#30830;&#20445;&#34917;&#25937;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06969</link><description>&lt;p&gt;
&#35774;&#23450;&#27491;&#30830;&#30340;&#26399;&#26395;&#65306;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
Setting the Right Expectations: Algorithmic Recourse Over Time. (arXiv:2309.06969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#20013;&#24573;&#35270;&#30340;&#20851;&#38190;&#35201;&#32032; - &#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#23545;&#34917;&#25937;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26102;&#38388;&#25512;&#31227;&#21644;&#20010;&#20307;&#38388;&#31454;&#20105;&#30340;&#24773;&#20917;&#19979;&#65292;&#21021;&#22987;&#30340;&#34917;&#25937;&#24314;&#35758;&#21487;&#33021;&#21464;&#24471;&#19981;&#21487;&#38752;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;&#26102;&#38388;&#21464;&#21270;&#26469;&#30830;&#20445;&#34917;&#25937;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#31995;&#32479;&#32463;&#24120;&#34987;&#29992;&#20110;&#21327;&#21161;&#39640;&#39118;&#38505;&#20915;&#31574;&#12290;&#37492;&#20110;&#27492;&#65292;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#65292;&#21363;&#20010;&#20307;&#24212;&#33021;&#22815;&#38024;&#23545;&#31639;&#27861;&#31995;&#32479;&#20135;&#29983;&#30340;&#19981;&#33391;&#32467;&#26524;&#37319;&#21462;&#34892;&#21160;&#65292;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#30340;&#22823;&#37096;&#20998;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#20026;&#21333;&#20010;&#20010;&#20307;&#25552;&#20379;&#34917;&#25937;&#65292;&#32780;&#24573;&#30053;&#20102;&#19968;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;&#24573;&#35270;&#36825;&#20123;&#23545;&#34917;&#25937;&#25514;&#26045;&#30340;&#24433;&#21709;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#30095;&#24573;&#65292;&#22240;&#20026;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#34917;&#25937;&#25514;&#26045;&#37117;&#21253;&#25324;&#20010;&#20307;&#39318;&#27425;&#20570;&#20986;&#19981;&#21033;&#23581;&#35797;&#65292;&#28982;&#21518;&#22312;&#20197;&#21518;&#30340;&#26576;&#20010;&#26102;&#38388;&#28857;&#25552;&#20379;&#19968;&#27425;&#25110;&#22810;&#27425;&#23581;&#35797;&#30340;&#26426;&#20250; - &#24403;&#26102;&#29615;&#22659;&#21487;&#33021;&#24050;&#32463;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#36825;&#21487;&#33021;&#20250;&#20135;&#29983;&#34394;&#20551;&#30340;&#26399;&#26395;&#65292;&#22240;&#20026;&#21021;&#22987;&#30340;&#34917;&#25937;&#24314;&#35758;&#38543;&#26102;&#38388;&#30340;&#25512;&#31227;&#21487;&#33021;&#21464;&#24471;&#19981;&#22826;&#21487;&#38752;&#65292;&#21407;&#22240;&#26159;&#27169;&#22411;&#28418;&#31227;&#21644;&#20010;&#20307;&#20043;&#38388;&#23545;&#26377;&#21033;&#32467;&#26524;&#30340;&#31454;&#20105;&#23548;&#33268;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26102;&#38388;&#21464;&#21270;&#30340;&#31639;&#27861;&#34917;&#25937;&#25514;&#26045;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic systems are often called upon to assist in high-stakes decision making. In light of this, algorithmic recourse, the principle wherein individuals should be able to take action against an undesirable outcome made by an algorithmic system, is receiving growing attention. The bulk of the literature on algorithmic recourse to-date focuses primarily on how to provide recourse to a single individual, overlooking a critical element: the effects of a continuously changing context. Disregarding these effects on recourse is a significant oversight, since, in almost all cases, recourse consists of an individual making a first, unfavorable attempt, and then being given an opportunity to make one or several attempts at a later date - when the context might have changed. This can create false expectations, as initial recourse recommendations may become less reliable over time due to model drift and competition for access to the favorable outcome between individuals.  In this work we prop
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNA&#30340;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#65292;&#37319;&#29992;&#38544;&#24335;&#31070;&#32463;&#22810;&#25551;&#36848;&#25216;&#26415;&#32534;&#30721;&#25968;&#25454;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;DNA&#23384;&#20648;&#20013;&#30340;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;MDC&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06956</link><description>&lt;p&gt;
&#22522;&#20110;DNA&#30340;&#25968;&#25454;&#23384;&#20648;&#30340;&#38544;&#24335;&#31070;&#32463;&#22810;&#25551;&#36848;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Multiple Description for DNA-based data storage. (arXiv:2309.06956v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNA&#30340;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#65292;&#37319;&#29992;&#38544;&#24335;&#31070;&#32463;&#22810;&#25551;&#36848;&#25216;&#26415;&#32534;&#30721;&#25968;&#25454;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;DNA&#23384;&#20648;&#20013;&#30340;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;MDC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNA&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#23384;&#20648;&#23494;&#24230;&#21644;&#38271;&#26399;&#31283;&#23450;&#24615;&#65292;&#22522;&#20110;&#20854;&#22266;&#26377;&#30340;&#29983;&#29289;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#31034;&#20986;&#20316;&#20026;&#25968;&#25454;&#23384;&#20648;&#35299;&#20915;&#26041;&#26696;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#31181;&#26032;&#22411;&#20171;&#36136;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22788;&#29702;&#23384;&#20648;&#21644;&#29983;&#29289;&#25805;&#20316;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;&#36825;&#20123;&#25361;&#25112;&#36827;&#19968;&#27493;&#21463;&#38480;&#20110;DNA&#24207;&#21015;&#30340;&#32467;&#26500;&#32422;&#26463;&#21644;&#25104;&#26412;&#32771;&#34385;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#39318;&#21019;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21387;&#32553;&#26041;&#26696;&#21644;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;DNA&#25968;&#25454;&#23384;&#20648;&#30340;&#23574;&#31471;&#22810;&#25551;&#36848;&#32534;&#30721;&#65288;MDC&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;MDC&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23558;&#25968;&#25454;&#32534;&#30721;&#25104;DNA&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#35774;&#35745;&#26469;&#26377;&#25928;&#22320;&#25269;&#25239;&#38169;&#35823;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26032;&#21387;&#32553;&#26041;&#26696;&#22312;DNA&#25968;&#25454;&#23384;&#20648;&#26041;&#38754;&#36229;&#36807;&#20102;&#32463;&#20856;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#20110;&#20381;&#36182;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20256;&#32479;MDC&#26041;&#27861;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNA exhibits remarkable potential as a data storage solution due to its impressive storage density and long-term stability, stemming from its inherent biomolecular structure. However, developing this novel medium comes with its own set of challenges, particularly in addressing errors arising from storage and biological manipulations. These challenges are further conditioned by the structural constraints of DNA sequences and cost considerations. In response to these limitations, we have pioneered a novel compression scheme and a cutting-edge Multiple Description Coding (MDC) technique utilizing neural networks for DNA data storage. Our MDC method introduces an innovative approach to encoding data into DNA, specifically designed to withstand errors effectively. Notably, our new compression scheme overperforms classic image compression methods for DNA-data storage. Furthermore, our approach exhibits superiority over conventional MDC methods reliant on auto-encoders. Its distinctive streng
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#38543;&#26426;&#26862;&#26519;&#20013;&#36229;&#21442;&#25968;&#23545;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22312;&#39640;&#32500;&#32452;&#23398;&#30740;&#31350;&#20013;&#65292;&#36866;&#24403;&#35774;&#32622;RF&#36229;&#21442;&#25968;&#23545;&#36873;&#25321;&#37325;&#35201;&#21464;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06943</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#20013;&#36229;&#21442;&#25968;&#23545;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of hyperparameters on variable selection in random forests. (arXiv:2309.06943v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06943
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#38543;&#26426;&#26862;&#26519;&#20013;&#36229;&#21442;&#25968;&#23545;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22312;&#39640;&#32500;&#32452;&#23398;&#30740;&#31350;&#20013;&#65292;&#36866;&#24403;&#35774;&#32622;RF&#36229;&#21442;&#25968;&#23545;&#36873;&#25321;&#37325;&#35201;&#21464;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#22312;&#39640;&#32500;&#32452;&#23398;&#30740;&#31350;&#20013;&#36866;&#29992;&#20110;&#39044;&#27979;&#24314;&#27169;&#21644;&#21464;&#37327;&#36873;&#25321;&#12290;&#20808;&#21069;&#30740;&#31350;&#20102;RF&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#23545;&#39044;&#27979;&#24615;&#33021;&#21644;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#65292;&#20294;&#36229;&#21442;&#25968;&#23545;&#22522;&#20110;RF&#30340;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21033;&#29992;&#29702;&#35770;&#20998;&#24067;&#21644;&#23454;&#35777;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#36827;&#34892;&#20102;&#20004;&#20010;&#27169;&#25311;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;Vita&#21644;Boruta&#21464;&#37327;&#36873;&#25321; procedures &#22312;&#36873;&#25321;&#37325;&#35201;&#21464;&#37327;&#65288;&#25935;&#24863;&#24615;&#65289;&#30340;&#21516;&#26102;&#25511;&#21046;&#34394;&#35686;&#29575;&#65288;FDR&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#35201;&#27604;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#25277;&#21462;&#31574;&#30053;&#21644;&#26368;&#23567;&#32456;&#31471;&#33410;&#28857;&#22823;&#23567;&#26356;&#33021;&#24433;&#21709;&#36873;&#25321; procedures&#12290;RF&#36229;&#21442;&#25968;&#30340;&#21512;&#36866;&#35774;&#32622;&#21462;&#20915;&#20110;
&lt;/p&gt;
&lt;p&gt;
Random forests (RFs) are well suited for prediction modeling and variable selection in high-dimensional omics studies. The effect of hyperparameters of the RF algorithm on prediction performance and variable importance estimation have previously been investigated. However, how hyperparameters impact RF-based variable selection remains unclear. We evaluate the effects on the Vita and the Boruta variable selection procedures based on two simulation studies utilizing theoretical distributions and empirical gene expression data. We assess the ability of the procedures to select important variables (sensitivity) while controlling the false discovery rate (FDR). Our results show that the proportion of splitting candidate variables (mtry.prop) and the sample fraction (sample.fraction) for the training dataset influence the selection procedures more than the drawing strategy of the training datasets and the minimal terminal node size. A suitable setting of the RF hyperparameters depends on the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#23398;&#20064;&#21327;&#35758;&#30340;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#29615;&#22659;&#20132;&#20114;&#32972;&#26223;&#20013;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#38598;&#38598;&#20013;&#21270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.06938</link><description>&lt;p&gt;
&#26080;&#38598;&#21512;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Collectionless Artificial Intelligence. (arXiv:2309.06938v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#23398;&#20064;&#21327;&#35758;&#30340;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#29615;&#22659;&#20132;&#20114;&#32972;&#26223;&#20013;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#38598;&#38598;&#20013;&#21270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#20307;&#19978;&#65292;&#22788;&#29702;&#24222;&#22823;&#25968;&#25454;&#38598;&#34987;&#35748;&#20026;&#26159;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#22766;&#35266;&#32467;&#26524;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#36825;&#31181;&#25968;&#25454;&#38598;&#30340;&#38598;&#20013;&#21270;&#23384;&#22312;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#39118;&#38505;&#24847;&#35782;&#12290;&#26412;&#25991;&#25903;&#25345;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#21327;&#35758;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#30495;&#27491;&#20197;&#29615;&#22659;&#20132;&#20114;&#20026;&#20013;&#24515;&#30340;&#31867;&#20154;&#35748;&#30693;&#32972;&#26223;&#19979;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#23398;&#20064;&#21327;&#35758;&#38656;&#35201;&#36981;&#24490;&#26080;&#38598;&#21512;&#21407;&#21017;&#65292;&#21363;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#65292;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#30340;&#25968;&#25454;&#34987;&#29992;&#20110;&#26356;&#26032;&#24403;&#21069;&#29615;&#22659;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#19988;&#20195;&#29702;&#19981;&#33021;&#23545;&#26102;&#38388;&#27969;&#36827;&#34892;&#35760;&#24405;&#12290;&#22522;&#26412;&#19978;&#65292;&#19981;&#33021;&#23384;&#20648;&#26469;&#33258;&#20256;&#24863;&#22120;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
By and large, the professional handling of huge data collections is regarded as a fundamental ingredient of the progress of machine learning and of its spectacular results in related disciplines, with a growing agreement on risks connected to the centralization of such data collections. This paper sustains the position that the time has come for thinking of new learning protocols where machines conquer cognitive skills in a truly human-like context centered on environmental interactions. This comes with specific restrictions on the learning protocol according to the collectionless principle, which states that, at each time instant, data acquired from the environment is processed with the purpose of contributing to update the current internal representation of the environment, and that the agent is not given the privilege of recording the temporal stream. Basically, there is neither permission to store the temporal information coming from the sensors, thus promoting the development of s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#35821;&#20041;&#32593;&#25216;&#26415;&#23545;&#20301;&#38169;&#21160;&#21147;&#23398;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#27010;&#24565;&#21644;&#19982;&#30456;&#20851;&#26412;&#20307;&#23545;&#40784;&#26469;&#25193;&#23637;&#20301;&#38169;&#26412;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.06930</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;&#32593;&#25216;&#26415;&#23545;&#20301;&#38169;&#21160;&#21147;&#23398;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Dislocation Dynamics Data Using Semantic Web Technologies. (arXiv:2309.06930v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#35821;&#20041;&#32593;&#25216;&#26415;&#23545;&#20301;&#38169;&#21160;&#21147;&#23398;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#27010;&#24565;&#21644;&#19982;&#30456;&#20851;&#26412;&#20307;&#23545;&#40784;&#26469;&#25193;&#23637;&#20301;&#38169;&#26412;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26448;&#26009;&#31185;&#23398;&#19982;&#24037;&#31243;&#39046;&#22495;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#26448;&#26009;&#30340;&#35774;&#35745;&#12289;&#21512;&#25104;&#12289;&#24615;&#33021;&#21644;&#24615;&#33021;&#12290;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#26448;&#26009;&#31867;&#21035;&#26159;&#26230;&#20307;&#26448;&#26009;&#65292;&#21253;&#25324;&#37329;&#23646;&#21644;&#21322;&#23548;&#20307;&#12290;&#26230;&#20307;&#26448;&#26009;&#36890;&#24120;&#21253;&#21547;&#19968;&#31181;&#31216;&#20026;&#8220;&#20301;&#38169;&#8221;&#30340;&#29305;&#27530;&#32570;&#38519;&#12290;&#36825;&#31181;&#32570;&#38519;&#26174;&#33879;&#24433;&#21709;&#21508;&#31181;&#26448;&#26009;&#24615;&#33021;&#65292;&#21253;&#25324;&#24378;&#24230;&#12289;&#26029;&#35010;&#38887;&#24615;&#21644;&#24310;&#23637;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23454;&#39564;&#34920;&#24449;&#25216;&#26415;&#21644;&#27169;&#25311;&#65288;&#22914;&#20301;&#38169;&#21160;&#21147;&#23398;&#27169;&#25311;&#65289;&#33268;&#21147;&#20110;&#29702;&#35299;&#20301;&#38169;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#32593;&#25216;&#26415;&#20197;&#26412;&#20307;&#26041;&#24335;&#23545;&#20301;&#38169;&#21160;&#21147;&#23398;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#27010;&#24565;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#20004;&#20010;&#19982;&#35813;&#39046;&#22495;&#30456;&#20851;&#30340;&#26412;&#20307;&#65288;&#21363;Elementary Multi-perspectiv&#65289;&#36827;&#34892;&#23545;&#40784;&#26469;&#25193;&#23637;&#24050;&#26377;&#30340;&#20301;&#38169;&#26412;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in the field of Materials Science and Engineering focuses on the design, synthesis, properties, and performance of materials. An important class of materials that is widely investigated are crystalline materials, including metals and semiconductors. Crystalline material typically contains a distinct type of defect called "dislocation". This defect significantly affects various material properties, including strength, fracture toughness, and ductility. Researchers have devoted a significant effort in recent years to understanding dislocation behavior through experimental characterization techniques and simulations, e.g., dislocation dynamics simulations. This paper presents how data from dislocation dynamics simulations can be modeled using semantic web technologies through annotating data with ontologies. We extend the already existing Dislocation Ontology by adding missing concepts and aligning it with two other domain-related ontologies (i.e., the Elementary Multi-perspectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#34892;&#21160;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#34892;&#21160;&#34920;&#31034;&#23545;&#27969;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#20219;&#21153;&#30340;&#23398;&#20064;&#24615;&#33021;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#20854;&#20013;&#24615;&#33021;&#24046;&#24322;&#21487;&#24402;&#22240;&#20110;&#20248;&#21270;&#38382;&#39064;&#31354;&#38388;&#30340;&#22797;&#26434;&#24615;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.06921</link><description>&lt;p&gt;
&#25506;&#31350;&#34892;&#21160;&#34920;&#31034;&#22312;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Impact of Action Representations in Policy Gradient Algorithms. (arXiv:2309.06921v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#34892;&#21160;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#34892;&#21160;&#34920;&#31034;&#23545;&#27969;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#20219;&#21153;&#30340;&#23398;&#20064;&#24615;&#33021;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#20854;&#20013;&#24615;&#33021;&#24046;&#24322;&#21487;&#24402;&#22240;&#20110;&#20248;&#21270;&#38382;&#39064;&#31354;&#38388;&#30340;&#22797;&#26434;&#24615;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#23398;&#20064;&#35299;&#20915;&#22797;&#26434;&#23454;&#38469;&#20219;&#21153;&#30340;&#22810;&#21151;&#33021;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;RL&#31639;&#27861;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#24448;&#24448;&#19981;&#22826;&#28165;&#26970;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#20998;&#26512;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#35843;&#26597;RL&#20013;&#34892;&#21160;&#34920;&#31034;&#30340;&#24433;&#21709;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#34892;&#21160;&#34920;&#31034;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#27969;&#34892;&#30340;RL&#22522;&#20934;&#20219;&#21153;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;&#20248;&#21270;&#38382;&#39064;&#31354;&#38388;&#30340;&#22797;&#26434;&#24615;&#30340;&#25913;&#21464;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;RL&#31639;&#27861;&#20998;&#26512;&#25216;&#26415;&#30340;&#24320;&#25918;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning~(RL) is a versatile framework for learning to solve complex real-world tasks. However, influences on the learning performance of RL algorithms are often poorly understood in practice. We discuss different analysis techniques and assess their effectiveness for investigating the impact of action representations in RL. Our experiments demonstrate that the action representation can significantly influence the learning performance on popular RL benchmark tasks. The analysis results indicate that some of the performance differences can be attributed to changes in the complexity of the optimization landscape. Finally, we discuss open challenges of analysis techniques for RL algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#29983;&#25104;&#30340;&#22238;&#39038;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#20266;&#26679;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.06917</link><description>&lt;p&gt;
&#20351;&#29992;&#29380;&#21033;&#20811;&#38647;&#29983;&#25104;&#22522;&#30784;&#30340;&#22238;&#39038;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning with Dirichlet Generative-based Rehearsal. (arXiv:2309.06917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#29983;&#25104;&#30340;&#22238;&#39038;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#20266;&#26679;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#25968;&#25454;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#65288;ToDs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#30001;&#20110;&#35745;&#31639;&#32422;&#26463;&#21644;&#32791;&#26102;&#38382;&#39064;&#32780;&#22256;&#25200;&#30528;&#22686;&#37327;&#23398;&#20064;&#12290;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#35797;&#22270;&#36890;&#36807;&#36991;&#20813;&#23494;&#38598;&#30340;&#39044;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#22522;&#20110;&#29983;&#25104;&#30340;&#22238;&#39038;CL&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#33021;&#20934;&#30830;&#21453;&#26144;&#24213;&#23618;&#20219;&#21153;&#29305;&#23450;&#20998;&#24067;&#30340;&#20266;&#26679;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29380;&#21033;&#20811;&#38647;&#36830;&#32493;&#23398;&#20064;&#65288;DCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29983;&#25104;&#30340;&#22238;&#39038;&#31574;&#30053;&#29992;&#20110;CL&#12290;&#19982;&#20256;&#32479;&#19978;&#22312;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#20013;&#20351;&#29992;&#30340;&#39640;&#26031;&#28508;&#21464;&#37327;&#19981;&#21516;&#65292;DCL&#21033;&#29992;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#26679;&#24615;&#26469;&#24314;&#27169;&#28508;&#21464;&#37327;&#20808;&#39564;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#39640;&#25928;&#22320;&#25429;&#25417;&#20808;&#21069;&#20219;&#21153;&#30340;&#21477;&#32423;&#29305;&#24449;&#65292;&#24182;&#26377;&#25928;&#22320;&#25351;&#23548;&#20266;&#26679;&#26412;&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#36824;&#24341;&#20837;&#20102;Jensen-&#33487;&#24443;&#21033;&#25955;&#24230;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;DCL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in data-driven task-oriented dialogue systems (ToDs) struggle with incremental learning due to computational constraints and time-consuming issues. Continual Learning (CL) attempts to solve this by avoiding intensive pre-training, but it faces the problem of catastrophic forgetting (CF). While generative-based rehearsal CL methods have made significant strides, generating pseudo samples that accurately reflect the underlying task-specific distribution is still a challenge. In this paper, we present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal strategy for CL. Unlike the traditionally used Gaussian latent variable in the Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and versatility of the Dirichlet distribution to model the latent prior variable. This enables it to efficiently capture sentence-level features of previous tasks and effectively guide the generation of pseudo samples. In addition, we introduce Jensen-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06908</link><description>&lt;p&gt;
&#36208;&#21521;TopMost&#65306;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Towards the TopMost: A Topic Modeling System Toolkit. (arXiv:2309.06908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#34987;&#25552;&#20986;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#22312;&#31070;&#32463;&#21464;&#20998;&#25512;&#26029;&#30340;&#25512;&#21160;&#19979;&#36817;&#26399;&#24471;&#21040;&#20102;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20027;&#39064;&#27169;&#22411;&#37319;&#29992;&#23436;&#20840;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#35774;&#32622;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24555;&#36895;&#21033;&#29992;&#21644;&#20844;&#24179;&#27604;&#36739;&#12290;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65288;TopMost&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;TopMost&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#30340;&#23436;&#25972;&#29983;&#21629;&#21608;&#26399;&#65292;&#33073;&#39062;&#32780;&#20986;&#12290;TopMost&#30340;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#21487;&#20197;&#24555;&#36895;&#21033;&#29992;&#65292;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#28789;&#27963;&#25193;&#23637;&#19981;&#21516;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25945;&#31243;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/bobxwu/topmost &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been proposed for decades with various applications and recently refreshed by the neural variational inference. However, these topic models adopt totally distinct dataset, implementation, and evaluation settings, which hinders their quick utilization and fair comparisons. This greatly hinders the research progress of topic models. To address these issues, in this paper we propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by covering a wider range of topic modeling scenarios including complete lifecycles with dataset pre-processing, model training, testing, and evaluations. The highly cohesive and decoupled modular design of TopMost enables quick utilization, fair comparisons, and flexible extensions of different topic models. This can facilitate the research and applications of topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21644;&#20351;&#29992;&#27969;&#20381;&#36182;&#30340;&#25968;&#25454;&#25193;&#22686;&#26041;&#24335;&#20197;&#21450;&#19968;&#20123;&#23454;&#29616;&#25216;&#24039;&#65292;&#22686;&#24378;&#20102;UOGCL&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#36798;&#21040;&#20102;&#19982;&#20854;&#20182;&#26080;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#24182;&#20943;&#23567;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.06896</link><description>&lt;p&gt;
&#38024;&#23545;&#26080;&#30417;&#30563;&#22312;&#32447;&#19968;&#33324;&#36830;&#32493;&#23398;&#20064;&#30340;&#39046;&#22495;&#24863;&#30693;&#25193;&#22686;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Domain-Aware Augmentations for Unsupervised Online General Continual Learning. (arXiv:2309.06896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21644;&#20351;&#29992;&#27969;&#20381;&#36182;&#30340;&#25968;&#25454;&#25193;&#22686;&#26041;&#24335;&#20197;&#21450;&#19968;&#20123;&#23454;&#29616;&#25216;&#24039;&#65292;&#22686;&#24378;&#20102;UOGCL&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#36798;&#21040;&#20102;&#19982;&#20854;&#20182;&#26080;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#24182;&#20943;&#23567;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#19968;&#30452;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#26080;&#30417;&#30563;&#22330;&#26223;&#26102;&#65292;&#20363;&#22914;&#26080;&#30417;&#30563;&#22312;&#32447;&#19968;&#33324;&#36830;&#32493;&#23398;&#20064;&#65288;UOGCL&#65289;&#65292;&#20854;&#20013;&#23398;&#20064;&#20195;&#29702;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#20851;&#20110;&#31867;&#21035;&#36793;&#30028;&#25110;&#20219;&#21153;&#26356;&#25913;&#30340;&#20449;&#24687;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20943;&#23569;&#31561;&#20215;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#32773;&#23545;&#36951;&#24536;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21644;&#20351;&#29992;&#27969;&#20381;&#36182;&#30340;&#25968;&#25454;&#25193;&#22686;&#26041;&#24335;&#20197;&#21450;&#19968;&#20123;&#23454;&#29616;&#25216;&#24039;&#65292;&#22686;&#24378;&#20102;UOGCL&#20013;&#23545;&#27604;&#23398;&#20064;&#30340;&#35760;&#24518;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#19982;&#20854;&#20182;&#26080;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#24182;&#20943;&#23567;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#39046;&#22495;&#24863;&#30693;&#25193;&#22686;&#36807;&#31243;&#21487;&#20197;&#36866;&#24212;&#20854;&#20182;&#22522;&#20110;&#22238;&#25918;&#30340;&#26041;&#27861;&#65292;&#20351;&#23427;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning has been challenging, especially when dealing with unsupervised scenarios such as Unsupervised Online General Continual Learning (UOGCL), where the learning agent has no prior knowledge of class boundaries or task change information. While previous research has focused on reducing forgetting in supervised setups, recent studies have shown that self-supervised learners are more resilient to forgetting. This paper proposes a novel approach that enhances memory usage for contrastive learning in UOGCL by defining and using stream-dependent data augmentations together with some implementation tricks. Our proposed method is simple yet effective, achieves state-of-the-art results compared to other unsupervised approaches in all considered setups, and reduces the gap between supervised and unsupervised continual learning. Our domain-aware augmentation procedure can be adapted to other replay-based methods, making it a promising strategy for continual learning.
&lt;/p&gt;</description></item><item><title>MagiCapture&#26159;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#22810;&#27010;&#24565;&#20154;&#20687;&#23450;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20960;&#20010;&#20027;&#39064;&#21644;&#39118;&#26684;&#21442;&#32771;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29305;&#23450;&#39118;&#26684;&#20154;&#20687;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.06895</link><description>&lt;p&gt;
MagiCapture: &#39640;&#20998;&#36776;&#29575;&#22810;&#27010;&#24565;&#20154;&#20687;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
MagiCapture: High-Resolution Multi-Concept Portrait Customization. (arXiv:2309.06895v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06895
&lt;/p&gt;
&lt;p&gt;
MagiCapture&#26159;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#22810;&#27010;&#24565;&#20154;&#20687;&#23450;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20960;&#20010;&#20027;&#39064;&#21644;&#39118;&#26684;&#21442;&#32771;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29305;&#23450;&#39118;&#26684;&#20154;&#20687;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#21253;&#25324;&#31283;&#23450;&#25193;&#25955;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#36924;&#30495;&#20154;&#20687;&#29031;&#29255;&#12290;&#26377;&#19968;&#20010;&#19987;&#38376;&#30740;&#31350;&#20010;&#24615;&#21270;&#36825;&#20123;&#27169;&#22411;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#20351;&#29992;&#25552;&#20379;&#30340;&#21442;&#32771;&#22270;&#20687;&#38598;&#21512;&#21512;&#25104;&#29305;&#23450;&#20027;&#39064;&#25110;&#39118;&#26684;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#20010;&#24615;&#21270;&#26041;&#27861;&#20135;&#29983;&#30340;&#32467;&#26524;&#20196;&#20154;&#28385;&#24847;&#65292;&#20294;&#20854;&#29983;&#25104;&#30340;&#22270;&#20687;&#24448;&#24448;&#32570;&#20047;&#30495;&#23454;&#24863;&#65292;&#24182;&#19988;&#23578;&#26410;&#36798;&#21040;&#21830;&#19994;&#21487;&#34892;&#30340;&#27700;&#24179;&#12290;&#22312;&#20154;&#20687;&#22270;&#20687;&#29983;&#25104;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#22240;&#20026;&#30001;&#20110;&#25105;&#20204;&#20869;&#22312;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;&#20154;&#33080;&#20013;&#30340;&#20219;&#20309;&#19981;&#33258;&#28982;&#30340;&#30165;&#36857;&#37117;&#24456;&#23481;&#26131;&#34987;&#35782;&#21035;&#20986;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MagiCapture&#65292;&#19968;&#31181;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20027;&#39064;&#21644;&#39118;&#26684;&#27010;&#24565;&#34701;&#21512;&#65292;&#20165;&#20351;&#29992;&#20960;&#20010;&#20027;&#39064;&#21644;&#39118;&#26684;&#21442;&#32771;&#21363;&#21487;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#30340;&#20154;&#20687;&#22270;&#20687;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#20123;&#38543;&#26426;&#30340;&#33258;&#25293;&#29031;&#65292;&#25105;&#20204;&#32463;&#36807;&#35843;&#20248;&#30340;&#27169;&#22411;&#23601;&#21487;&#20197;&#29983;&#25104;&#29305;&#23450;&#39118;&#26684;&#65288;&#22914;&#25252;&#29031;&#25110;&#20010;&#20154;&#36164;&#26009;&#65289;&#30340;&#39640;&#36136;&#37327;&#20154;&#20687;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image models including Stable Diffusion are capable of generating high-fidelity photorealistic portrait images. There is an active research area dedicated to personalizing these models, aiming to synthesize specific subjects or styles using provided sets of reference images. However, despite the plausible results from these personalization methods, they tend to produce images that often fall short of realism and are not yet on a commercially viable level. This is particularly noticeable in portrait image generation, where any unnatural artifact in human faces is easily discernible due to our inherent human bias. To address this, we introduce MagiCapture, a personalization method for integrating subject and style concepts to generate high-resolution portrait images using just a few subject and style references. For instance, given a handful of random selfies, our fine-tuned model can generate high-quality portrait images in specific styles, such as passport or profil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27744;&#21270;&#26694;&#26550;SimPool&#65292;&#29992;&#20110;&#26367;&#20195;&#21367;&#31215;&#21644;Transformer&#32534;&#30721;&#22120;&#30340;&#40664;&#35748;&#27744;&#21270;&#26426;&#21046;&#65292;&#26080;&#35770;&#26159;&#26377;&#30417;&#30563;&#36824;&#26159;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#37117;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#33021;&#22815;&#25551;&#32472;&#23545;&#35937;&#36793;&#30028;&#30340;&#27880;&#24847;&#21147;&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.06891</link><description>&lt;p&gt;
&#20445;&#25345;&#31616;&#21333;&#65306;&#35841;&#35828;&#26377;&#30417;&#30563;&#30340;Transformer&#27169;&#22411;&#27880;&#24847;&#21147;&#19981;&#38598;&#20013;? (arXiv:2309.06891v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?. (arXiv:2309.06891v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27744;&#21270;&#26694;&#26550;SimPool&#65292;&#29992;&#20110;&#26367;&#20195;&#21367;&#31215;&#21644;Transformer&#32534;&#30721;&#22120;&#30340;&#40664;&#35748;&#27744;&#21270;&#26426;&#21046;&#65292;&#26080;&#35770;&#26159;&#26377;&#30417;&#30563;&#36824;&#26159;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#37117;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#33021;&#22815;&#25551;&#32472;&#23545;&#35937;&#36793;&#30028;&#30340;&#27880;&#24847;&#21147;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#36890;&#36807;&#19981;&#21516;&#24418;&#24335;&#30340;&#25104;&#23545;&#20132;&#20114;&#26469;&#36827;&#34892;&#27744;&#21270;&#65292;&#21253;&#25324;&#22312;&#32593;&#32476;&#23618;&#20043;&#38388;&#36827;&#34892;&#27744;&#21270;&#21644;&#22312;&#32593;&#32476;&#26411;&#31471;&#36827;&#34892;&#27744;&#21270;&#12290;&#21518;&#32773;&#30495;&#30340;&#38656;&#35201;&#19982;&#21069;&#32773;&#19981;&#21516;&#21527;&#65311;&#20316;&#20026;&#27744;&#21270;&#30340;&#21103;&#20135;&#21697;&#65292;&#35270;&#35273;Transformer&#21487;&#20197;&#25552;&#20379;&#20813;&#36153;&#30340;&#31354;&#38388;&#27880;&#24847;&#21147;&#65292;&#20294;&#36890;&#24120;&#36136;&#37327;&#36739;&#20302;&#65292;&#38500;&#38750;&#26159;&#33258;&#30417;&#30563;&#30340;&#65292;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#24182;&#19981;&#20805;&#20998;&#12290;&#30417;&#30563;&#26159;&#30495;&#27491;&#30340;&#38382;&#39064;&#21527;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#27744;&#21270;&#26694;&#26550;&#65292;&#28982;&#21518;&#23558;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#20316;&#20026;&#23454;&#20363;&#21270;&#12290;&#36890;&#36807;&#35752;&#35770;&#27599;&#32452;&#26041;&#27861;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;SimPool&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#29992;&#20110;&#26367;&#20195;&#21367;&#31215;&#21644;Transformer&#32534;&#30721;&#22120;&#30340;&#40664;&#35748;&#26426;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#26377;&#30417;&#30563;&#36824;&#26159;&#33258;&#30417;&#30563;&#65292;&#36825;&#37117;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#33021;&#22815;&#25551;&#32472;&#23545;&#35937;&#36793;&#30028;&#30340;&#27880;&#24847;&#21147;&#22270;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#31216;SimPool&#26159;&#36890;&#29992;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#20570;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem?  In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal. To our knowledge, we are the first to o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#20135;&#21697;&#26144;&#23556;&#25968;&#25454;&#38598;&#65306;ProMapCz&#21644;ProMapEn&#65292;&#20998;&#21035;&#21253;&#21547;&#25463;&#20811;&#21644;&#33521;&#25991;&#20135;&#21697;&#23545;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#36739;&#20026;&#23436;&#25972;&#30340;&#20135;&#21697;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20102;&#30446;&#21069;&#29616;&#26377;&#25968;&#25454;&#38598;&#26080;&#27861;&#21306;&#20998;&#38750;&#24120;&#30456;&#20284;&#20294;&#19981;&#21305;&#37197;&#20135;&#21697;&#23545;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06882</link><description>&lt;p&gt;
ProMap&#65306;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#26144;&#23556;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ProMap: Datasets for Product Mapping in E-commerce. (arXiv:2309.06882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06882
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#20135;&#21697;&#26144;&#23556;&#25968;&#25454;&#38598;&#65306;ProMapCz&#21644;ProMapEn&#65292;&#20998;&#21035;&#21253;&#21547;&#25463;&#20811;&#21644;&#33521;&#25991;&#20135;&#21697;&#23545;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#36739;&#20026;&#23436;&#25972;&#30340;&#20135;&#21697;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20102;&#30446;&#21069;&#29616;&#26377;&#25968;&#25454;&#38598;&#26080;&#27861;&#21306;&#20998;&#38750;&#24120;&#30456;&#20284;&#20294;&#19981;&#21305;&#37197;&#20135;&#21697;&#23545;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#26144;&#23556;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;&#19981;&#21516;&#30005;&#23376;&#21830;&#24215;&#20013;&#30340;&#20004;&#20010;&#21015;&#34920;&#26159;&#21542;&#25551;&#36848;&#30456;&#21516;&#30340;&#20135;&#21697;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21305;&#37197;&#21644;&#38750;&#21305;&#37197;&#20135;&#21697;&#23545;&#30340;&#25968;&#25454;&#38598;&#32463;&#24120;&#21463;&#21040;&#20135;&#21697;&#20449;&#24687;&#19981;&#23436;&#25972;&#25110;&#32773;&#21482;&#21253;&#21547;&#38750;&#24120;&#36828;&#30340;&#38750;&#21305;&#37197;&#20135;&#21697;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#20204;&#26080;&#27861;&#21306;&#20998;&#38750;&#24120;&#30456;&#20284;&#20294;&#19981;&#21305;&#37197;&#30340;&#20135;&#21697;&#23545;&#65292;&#22240;&#27492;&#26080;&#27861;&#20351;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#20135;&#21697;&#26144;&#23556;&#25968;&#25454;&#38598;&#65306;ProMapCz&#21253;&#21547;1495&#23545;&#25463;&#20811;&#20135;&#21697;&#65292;ProMapEn&#21253;&#21547;1555&#23545;&#33521;&#25991;&#20135;&#21697;&#65292;&#36825;&#20123;&#20135;&#21697;&#23545;&#26469;&#33258;&#20004;&#20010;&#30005;&#23376;&#21830;&#24215;&#65292;&#21253;&#21547;&#20102;&#20135;&#21697;&#30340;&#22270;&#20687;&#21644;&#25991;&#23383;&#25551;&#36848;&#65292;&#21253;&#25324;&#35268;&#26684;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#26368;&#23436;&#25972;&#30340;&#20135;&#21697;&#26144;&#23556;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#38750;&#21305;&#37197;&#20135;&#21697;&#26159;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#36873;&#25321;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of product mapping is to decide, whether two listings from two different e-shops describe the same products. Existing datasets of matching and non-matching pairs of products, however, often suffer from incomplete product information or contain only very distant non-matching products. Therefore, while predictive models trained on these datasets achieve good results on them, in practice, they are unusable as they cannot distinguish very similar but non-matching pairs of products. This paper introduces two new datasets for product mapping: ProMapCz consisting of 1,495 Czech product pairs and ProMapEn consisting of 1,555 English product pairs of matching and non-matching products manually scraped from two pairs of e-shops. The datasets contain both images and textual descriptions of the products, including their specifications, making them one of the most complete datasets for product mapping. Additionally, the non-matching products were selected in two phases, creating two types 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#39063;&#31890;&#30340;&#21160;&#24577;&#33258;&#32452;&#35013;&#36807;&#31243;&#65292;&#24418;&#25104;&#21313;&#20108;&#36793;&#20934;&#26230;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#26368;&#20339;&#30340;&#28201;&#24230;&#25511;&#21046;&#31574;&#30053;&#65292;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#20960;&#20046;&#27809;&#26377;&#32570;&#38519;&#30340;&#32467;&#26500;&#12290;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30340;&#28201;&#24230;&#35843;&#24230;&#27604;&#20256;&#32479;&#30340;&#39044;&#35774;&#28201;&#24230;&#35843;&#24230;&#26356;&#26377;&#25928;&#22320;&#37325;&#29616;&#20102;&#26399;&#26395;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.06869</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;&#25511;&#21046;&#25311;&#26230;&#32467;&#26500;&#30340;&#33258;&#32452;&#35013;
&lt;/p&gt;
&lt;p&gt;
Dynamic control of self-assembly of quasicrystalline structures through reinforcement learning. (arXiv:2309.06869v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#39063;&#31890;&#30340;&#21160;&#24577;&#33258;&#32452;&#35013;&#36807;&#31243;&#65292;&#24418;&#25104;&#21313;&#20108;&#36793;&#20934;&#26230;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#26368;&#20339;&#30340;&#28201;&#24230;&#25511;&#21046;&#31574;&#30053;&#65292;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#20960;&#20046;&#27809;&#26377;&#32570;&#38519;&#30340;&#32467;&#26500;&#12290;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30340;&#28201;&#24230;&#35843;&#24230;&#27604;&#20256;&#32479;&#30340;&#39044;&#35774;&#28201;&#24230;&#35843;&#24230;&#26356;&#26377;&#25928;&#22320;&#37325;&#29616;&#20102;&#26399;&#26395;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#30340;&#39063;&#31890;&#30340;&#21160;&#24577;&#33258;&#32452;&#35013;&#36807;&#31243;&#65292;&#24418;&#25104;&#21313;&#20108;&#36793;&#20934;&#26230;&#20307;&#65288;DDQC&#65289;&#12290;&#36825;&#20123;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#30340;&#39063;&#31890;&#19982;&#20854;&#20182;&#39063;&#31890;&#20855;&#26377;&#21508;&#21521;&#24322;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#24418;&#25104;DDQC&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#31283;&#24577;&#19979;&#30340;&#32467;&#26500;&#21463;&#20854;&#32467;&#26500;&#24418;&#25104;&#30340;&#21160;&#21147;&#23398;&#36335;&#24452;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;Q&#23398;&#20064;&#26041;&#27861;&#20272;&#35745;&#20102;&#26368;&#20339;&#30340;&#28201;&#24230;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20272;&#35745;&#30340;&#31574;&#30053;&#29983;&#25104;&#20960;&#20046;&#27809;&#26377;&#32570;&#38519;&#30340;DDQC&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30340;&#28201;&#24230;&#35843;&#24230;&#27604;&#20256;&#32479;&#30340;&#39044;&#35774;&#28201;&#24230;&#35843;&#24230;&#65288;&#22914;&#36864;&#28779;&#65289;&#26356;&#26377;&#25928;&#22320;&#37325;&#29616;&#20102;&#26399;&#26395;&#30340;&#32467;&#26500;&#12290;&#20026;&#20102;&#38416;&#26126;&#23398;&#20064;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#19968;&#20010;&#25551;&#36848;&#32467;&#26500;&#21464;&#21270;&#21160;&#21147;&#23398;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;&#36816;&#21160;&#26159;&#22312;&#19977;&#20117;&#21183;&#33021;&#20013;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#33258;&#20027;&#22320;&#21457;&#29616;&#22686;&#24378;&#32467;&#26500;&#27874;&#21160;&#30340;&#20020;&#30028;&#28201;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose reinforcement learning to control the dynamical self-assembly of the dodecagonal quasicrystal (DDQC) from patchy particles. The patchy particles have anisotropic interactions with other particles and form DDQC. However, their structures at steady states are significantly influenced by the kinetic pathways of their structural formation. We estimate the best policy of temperature control trained by the Q-learning method and demonstrate that we can generate DDQC with few defects using the estimated policy. The temperature schedule obtained by reinforcement learning can reproduce the desired structure more efficiently than the conventional pre-fixed temperature schedule, such as annealing. To clarify the success of the learning, we also analyse a simple model describing the kinetics of structural changes through the motion in a triple-well potential. We have found that reinforcement learning autonomously discovers the critical temperature at which structural fluctuations enhance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#25605;&#25292;&#25705;&#25830;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#23792;&#20540;&#28201;&#24230;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#26368;&#20339;&#30340;SML&#26041;&#27861;&#20026;&#26799;&#24230;&#25552;&#21319;&#27861;&#65292;&#26368;&#20302;&#30340;&#22343;&#26041;&#35823;&#24046;&#20026;165.78&#12290;</title><link>http://arxiv.org/abs/2309.06838</link><description>&lt;p&gt;
&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#38109;&#21512;&#37329;&#25605;&#25292;&#25705;&#25830;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#23792;&#20540;&#28201;&#24230;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy. (arXiv:2309.06838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#25605;&#25292;&#25705;&#25830;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#23792;&#20540;&#28201;&#24230;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#26368;&#20339;&#30340;SML&#26041;&#27861;&#20026;&#26799;&#24230;&#25552;&#21319;&#27861;&#65292;&#26368;&#20302;&#30340;&#22343;&#26041;&#35823;&#24046;&#20026;165.78&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#26448;&#25605;&#25292;&#25705;&#25830;&#27785;&#31215;&#65288;AFSD&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22266;&#24577;&#22686;&#26448;&#21046;&#36896;&#25216;&#26415;&#65292;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;&#31881;&#26411;&#24202;&#29076;&#28860;&#21644;&#23450;&#21521;&#33021;&#37327;&#27785;&#31215;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#23380;&#38553;&#29575;&#12289;&#24320;&#35010;&#21644;&#24615;&#33021;&#21508;&#21521;&#24322;&#24615;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;AFSD&#20013;&#30340;&#24037;&#33402;&#21442;&#25968;&#12289;&#28909;&#37327;&#20998;&#24067;&#21644;&#24471;&#21040;&#30340;&#26174;&#24494;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20173;&#28982;&#19981;&#22815;&#28165;&#26970;&#65292;&#36825;&#22952;&#30861;&#20102;&#24615;&#33021;&#30340;&#24037;&#33402;&#20248;&#21270;&#12290;&#26412;&#30740;&#31350;&#36816;&#29992;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#23558;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65288;SML&#65289;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#20174;&#24037;&#33402;&#21442;&#25968;&#39044;&#27979;AFSD&#20013;&#30340;&#23792;&#20540;&#28201;&#24230;&#20998;&#24067;&#12290;&#23545;&#20110;SML&#24314;&#27169;&#65292;&#20351;&#29992;&#20102;&#20843;&#31181;&#22238;&#24402;&#31639;&#27861;&#65292;&#32780;&#23545;&#20110;PINNs&#65292;&#20351;&#29992;&#20102;&#36816;&#36755;&#12289;&#27874;&#20256;&#25773;&#12289;&#28909;&#20256;&#23548;&#21644;&#37327;&#23376;&#21147;&#23398;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#22312;&#22810;&#20010;&#32479;&#35745;&#25351;&#26631;&#19978;&#65292;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#26799;&#24230;&#25552;&#21319;&#27861;&#26159;&#26368;&#20339;&#30340;SML&#26041;&#27861;&#65292;&#26368;&#20302;&#30340;&#22343;&#26041;&#35823;&#24046;&#20026;165.78&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive friction stir deposition (AFSD) is a novel solid-state additive manufacturing technique that circumvents issues of porosity, cracking, and properties anisotropy that plague traditional powder bed fusion and directed energy deposition approaches. However, correlations between process parameters, thermal profiles, and resulting microstructure in AFSD remain poorly understood. This hinders process optimization for properties. This work employs a cutting-edge framework combining supervised machine learning (SML) and physics-informed neural networks (PINNs) to predict peak temperature distribution in AFSD from process parameters. Eight regression algorithms were implemented for SML modeling, while four PINNs leveraged governing equations for transport, wave propagation, heat transfer, and quantum mechanics. Across multiple statistical measures, ensemble techniques like gradient boosting proved superior for SML, with lowest MSE of 165.78. The integrated ML approach was also applied 
&lt;/p&gt;</description></item><item><title>UniBrain&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#33041;&#37096;MRI&#35786;&#26029;&#30340;&#20998;&#23618;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20998;&#23618;&#23545;&#40784;&#26426;&#21046;&#25552;&#39640;&#20102;&#29305;&#24449;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.06828</link><description>&lt;p&gt;
UniBrain:&#36890;&#29992;&#33041;&#37096;MRI&#35786;&#26029;&#19982;&#20998;&#23618;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training. (arXiv:2309.06828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06828
&lt;/p&gt;
&lt;p&gt;
UniBrain&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#33041;&#37096;MRI&#35786;&#26029;&#30340;&#20998;&#23618;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20998;&#23618;&#23545;&#40784;&#26426;&#21046;&#25552;&#39640;&#20102;&#29305;&#24449;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22312;&#33041;&#37096;&#30142;&#30149;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#35745;&#31639;&#26426;&#36741;&#21161;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26089;&#26399;&#30340;&#25506;&#32034;&#36890;&#24120;&#38598;&#20013;&#20110;&#19968;&#20010;&#30740;&#31350;&#20013;&#30340;&#26377;&#38480;&#31867;&#22411;&#30340;&#33041;&#37096;&#30142;&#30149;&#65292;&#24182;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#23548;&#33268;&#27867;&#21270;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#33041;&#37096;MRI&#35786;&#26029;&#30340;&#20998;&#23618;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#31216;&#20026;UniBrain&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UniBrain&#21033;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;24,770&#20010;&#22270;&#20687;-&#25253;&#21578;&#37197;&#23545;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#24120;&#35268;&#35786;&#26029;&#12290;&#19981;&#21516;&#20110;&#20808;&#21069;&#29992;&#20110;&#21333;&#19968;&#35270;&#35273;&#25110;&#25991;&#26412;&#29305;&#24449;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#25110;&#32773;&#36890;&#36807;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#20043;&#38388;&#30340;&#34542;&#21147;&#23545;&#40784;&#65292;&#25105;&#20204;&#21033;&#29992;&#19981;&#21516;&#31890;&#24230;&#30340;&#25253;&#21578;&#20449;&#24687;&#30340;&#29420;&#29305;&#29305;&#24449;&#26500;&#24314;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#40784;&#26426;&#21046;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#29305;&#24449;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic resonance imaging~(MRI) have played a crucial role in brain disease diagnosis, with which a range of computer-aided artificial intelligence methods have been proposed. However, the early explorations usually focus on the limited types of brain diseases in one study and train the model on the data in a small scale, yielding the bottleneck of generalization. Towards a more effective and scalable paradigm, we propose a hierarchical knowledge-enhanced pre-training framework for the universal brain MRI diagnosis, termed as UniBrain. Specifically, UniBrain leverages a large-scale dataset of 24,770 imaging-report pairs from routine diagnostics. Different from previous pre-training techniques for the unitary vision or textual feature, or with the brute-force alignment between vision and language information, we leverage the unique characteristic of report information in different granularity to build a hierarchical alignment mechanism, which strengthens the efficiency in feature learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20998;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#12290;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#39640;&#25928;&#39044;&#27979;&#30001;&#22810;&#20110;&#20004;&#20010;&#20851;&#31995;&#21644;&#26410;&#25351;&#23450;&#23454;&#20307;&#32452;&#25104;&#30340;&#21477;&#23376;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#22810;&#20010;&#21477;&#23376;&#30340;&#35821;&#22659;&#20013;&#35782;&#21035;&#35821;&#20041;&#20851;&#31995;&#12290;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20108;&#20803;&#20851;&#31995;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#38543;&#30528;&#20851;&#31995;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#39044;&#27979;&#20934;&#30830;&#29575;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.06814</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Contextual Relation Extraction based on Deep Learning Models. (arXiv:2309.06814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20998;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#12290;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#39640;&#25928;&#39044;&#27979;&#30001;&#22810;&#20110;&#20004;&#20010;&#20851;&#31995;&#21644;&#26410;&#25351;&#23450;&#23454;&#20307;&#32452;&#25104;&#30340;&#21477;&#23376;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#22810;&#20010;&#21477;&#23376;&#30340;&#35821;&#22659;&#20013;&#35782;&#21035;&#35821;&#20041;&#20851;&#31995;&#12290;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20108;&#20803;&#20851;&#31995;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#38543;&#30528;&#20851;&#31995;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#39044;&#27979;&#20934;&#30830;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#20027;&#35201;&#29992;&#20110;&#20511;&#21161;&#26412;&#20307;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#22312;&#35821;&#20041;&#25628;&#32034;&#12289;&#26597;&#35810;&#22238;&#31572;&#21644;&#25991;&#26412;&#34164;&#28085;&#31561;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#20851;&#31995;&#25552;&#21462;&#35782;&#21035;&#21407;&#22987;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#21450;&#20854;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#29983;&#29289;&#21307;&#33647;&#34892;&#19994;&#20013;&#65292;&#39640;&#25928;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#25552;&#21462;&#31995;&#32479;&#23545;&#20110;&#21019;&#24314;&#39046;&#22495;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26080;&#27861;&#39640;&#25928;&#22320;&#20174;&#30001;&#22810;&#20110;&#20004;&#20010;&#20851;&#31995;&#21644;&#26410;&#25351;&#23450;&#23454;&#20307;&#32452;&#25104;&#30340;&#21477;&#23376;&#20013;&#39044;&#27979;&#22797;&#26434;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#22810;&#20010;&#21477;&#23376;&#30340;&#35821;&#22659;&#20013;&#35782;&#21035;&#20986;&#36866;&#24403;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#23613;&#31649;&#20851;&#31995;&#25552;&#21462;&#20013;&#20351;&#29992;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#21482;&#23545;&#20108;&#20803;&#20851;&#31995;&#65288;&#21363;&#22312;&#21477;&#23376;&#20013;&#23436;&#20840;&#21457;&#29983;&#22312;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65289;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#20250;&#38543;&#30528;&#20851;&#31995;&#30340;&#25968;&#37327;&#22686;&#21152;&#32780;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual Relation Extraction (CRE) is mainly used for constructing a knowledge graph with a help of ontology. It performs various tasks such as semantic search, query answering, and textual entailment. Relation extraction identifies the entities from raw texts and the relations among them. An efficient and accurate CRE system is essential for creating domain knowledge in the biomedical industry. Existing Machine Learning and Natural Language Processing (NLP) techniques are not suitable to predict complex relations from sentences that consist of more than two relations and unspecified entities efficiently. In this work, deep learning techniques have been used to identify the appropriate semantic relation based on the context from multiple sentences. Even though various machine learning models have been used for relation extraction, they provide better results only for binary relations, i.e., relations occurred exactly between the two entities in a sentence. Machine learning models are
&lt;/p&gt;</description></item><item><title>FedDIP&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21160;&#24577;&#27169;&#22411;&#20462;&#21098;&#21644;&#22686;&#37327;&#27491;&#21017;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#20132;&#25442;&#21644;&#23454;&#29616;&#26497;&#31471;&#31232;&#30095;&#27169;&#22411;&#26469;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06805</link><description>&lt;p&gt;
FedDIP: &#37319;&#29992;&#26497;&#31471;&#21160;&#24577;&#20462;&#21098;&#21644;&#22686;&#37327;&#27491;&#21017;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedDIP: Federated Learning with Extreme Dynamic Pruning and Incremental Regularization. (arXiv:2309.06805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06805
&lt;/p&gt;
&lt;p&gt;
FedDIP&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21160;&#24577;&#27169;&#22411;&#20462;&#21098;&#21644;&#22686;&#37327;&#27491;&#21017;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#20132;&#25442;&#21644;&#23454;&#29616;&#26497;&#31471;&#31232;&#30095;&#27169;&#22411;&#26469;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;DNN&#20855;&#26377;&#26497;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#22240;&#27492;&#22312;&#20998;&#24067;&#24335;&#33410;&#28857;&#20043;&#38388;&#20132;&#25442;&#36825;&#20123;&#21442;&#25968;&#21644;&#31649;&#29702;&#20869;&#23384;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;DNN&#21387;&#32553;&#26041;&#27861;&#65288;&#20363;&#22914;&#31232;&#30095;&#21270;&#12289;&#20462;&#21098;&#65289;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#24182;&#26410;&#20840;&#38754;&#32771;&#34385;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#27700;&#24179;&#30340;&#21516;&#26102;&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#21442;&#25968;&#20132;&#25442;&#30340;&#20943;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65288;&#31216;&#20026;FedDIP&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#65288;i&#65289;&#21160;&#24577;&#27169;&#22411;&#20462;&#21098;&#21644;&#35823;&#24046;&#21453;&#39304;&#26469;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#20132;&#25442;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#22686;&#37327;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#23454;&#29616;&#8220;&#26497;&#31471;&#8221;&#31232;&#30095;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;FedDIP&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#21644;&#27604;&#36739;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has been successfully adopted for distributed training and inference of large-scale Deep Neural Networks (DNNs). However, DNNs are characterized by an extremely large number of parameters, thus, yielding significant challenges in exchanging these parameters among distributed nodes and managing the memory. Although recent DNN compression methods (e.g., sparsification, pruning) tackle such challenges, they do not holistically consider an adaptively controlled reduction of parameter exchange while maintaining high accuracy levels. We, therefore, contribute with a novel FL framework (coined FedDIP), which combines (i) dynamic model pruning with error feedback to eliminate redundant information exchange, which contributes to significant performance improvement, with (ii) incremental regularization that can achieve \textit{extreme} sparsity of models. We provide convergence analysis of FedDIP and report on a comprehensive performance and comparative assessment against
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06800</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36817;&#26399;&#65292;&#35768;&#22810;&#30740;&#31350;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#39044;&#27979;&#20301;&#32622;&#26377;&#23436;&#25972;&#25110;&#33267;&#23569;&#37096;&#20998;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#26080;&#21382;&#21490;&#35760;&#24405;&#30340;&#20301;&#32622;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#21644;&#23433;&#35013;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21487;&#33021;&#21463;&#38480;&#65292;&#36825;&#20351;&#24471;&#22823;&#22810;&#25968;&#24403;&#21069;&#27169;&#22411;&#19981;&#36866;&#29992;&#12290;&#34429;&#28982;&#23569;&#25968;&#25991;&#29486;&#23581;&#35797;&#22312;&#32570;&#22833;&#20301;&#32622;&#19978;&#25554;&#34917;&#20132;&#36890;&#29366;&#24577;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19982;&#20256;&#24863;&#22120;&#20301;&#32622;&#21516;&#26102;&#35266;&#27979;&#30340;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#32570;&#20047;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#24471;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#20219;&#21153;&#25110;&#28041;&#21450;&#20915;&#31574;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21463;&#21040;&#20808;&#21069;&#30340;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#24187;&#35273;&#30340;&#20998;&#31867;&#12289;&#29702;&#35770;&#20998;&#26512;&#12289;&#26816;&#27979;&#26041;&#27861;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#35774;&#24819;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.06794</link><description>&lt;p&gt;
&#35748;&#30693;&#24187;&#35273;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#29616;&#35937;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Cognitive Mirage: A Review of Hallucinations in Large Language Models. (arXiv:2309.06794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#24187;&#35273;&#30340;&#20998;&#31867;&#12289;&#29702;&#35770;&#20998;&#26512;&#12289;&#26816;&#27979;&#26041;&#27861;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#35774;&#24819;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#20196;&#20154;&#25285;&#24551;&#30340;&#29616;&#35937;&#65292;&#21363;&#24187;&#35273;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26368;&#36817;&#20851;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#24341;&#20154;&#27880;&#30446;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#24187;&#35273;&#30340;&#26032;&#20998;&#31867;&#20307;&#31995;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#29702;&#35770;&#24615;&#30340;&#27934;&#35265;&#12289;&#26816;&#27979;&#26041;&#27861;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#25105;&#20204;&#20026;&#20986;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#25552;&#20379;&#20102;&#35814;&#32454;&#21644;&#23436;&#25972;&#30340;&#20998;&#31867;&#20307;&#31995;&#65307;&#65288;2&#65289;&#25105;&#20204;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;&#30340;&#26816;&#27979;&#21644;&#25913;&#36827;&#26041;&#27861;&#65307;&#65288;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#21487;&#20197;&#21457;&#23637;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#30001;&#20110;&#24187;&#35273;&#21463;&#21040;&#20102;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#25105;&#20204;&#23558;&#32500;&#25252;&#19982;&#30456;&#20851;&#30740;&#31350;&#36827;&#23637;&#30340;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25991;&#26412;&#26032;&#38395;&#29305;&#24449;&#32435;&#20837;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#33521;&#22269;&#22269;&#23478;&#30005;&#21147;&#38656;&#27714;&#30340;&#30830;&#23450;&#24615;&#21644;&#27010;&#29575;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20844;&#20247;&#24773;&#32490;&#21644;&#19982;&#20132;&#36890;&#21644;&#22320;&#32536;&#25919;&#27835;&#30456;&#20851;&#30340;&#35789;&#21521;&#37327;&#34920;&#31034;&#23545;&#30005;&#21147;&#38656;&#27714;&#20855;&#26377;&#26102;&#38388;&#36830;&#32493;&#24615;&#30340;&#24433;&#21709;&#12290;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#32431;LSTM&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;&#36229;&#36807;3%&#65292;&#30456;&#23545;&#20110;&#23448;&#26041;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;&#25509;&#36817;10%&#65292;&#24182;&#36890;&#36807;&#32553;&#23567;&#32622;&#20449;&#21306;&#38388;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06793</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#36827;&#34892;&#30005;&#21147;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Electricity Demand Forecasting through Natural Language Processing with Long Short-Term Memory Networks. (arXiv:2309.06793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25991;&#26412;&#26032;&#38395;&#29305;&#24449;&#32435;&#20837;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#33521;&#22269;&#22269;&#23478;&#30005;&#21147;&#38656;&#27714;&#30340;&#30830;&#23450;&#24615;&#21644;&#27010;&#29575;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20844;&#20247;&#24773;&#32490;&#21644;&#19982;&#20132;&#36890;&#21644;&#22320;&#32536;&#25919;&#27835;&#30456;&#20851;&#30340;&#35789;&#21521;&#37327;&#34920;&#31034;&#23545;&#30005;&#21147;&#38656;&#27714;&#20855;&#26377;&#26102;&#38388;&#36830;&#32493;&#24615;&#30340;&#24433;&#21709;&#12290;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#32431;LSTM&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;&#36229;&#36807;3%&#65292;&#30456;&#23545;&#20110;&#23448;&#26041;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;&#25509;&#36817;10%&#65292;&#24182;&#36890;&#36807;&#32553;&#23567;&#32622;&#20449;&#21306;&#38388;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#38656;&#27714;&#39044;&#27979;&#26159;&#19968;&#20010;&#24050;&#32463;&#24314;&#31435;&#36215;&#26469;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#39033;&#20219;&#21153;&#26159;&#36890;&#36807;&#32771;&#34385;&#21382;&#21490;&#36127;&#33655;&#12289;&#22825;&#27668;&#39044;&#25253;&#12289;&#26085;&#21382;&#20449;&#24687;&#21644;&#24050;&#30693;&#30340;&#37325;&#22823;&#20107;&#20214;&#26469;&#23436;&#25104;&#30340;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#22914;&#20309;&#21033;&#29992;&#26469;&#33258;&#25991;&#26412;&#26032;&#38395;&#30340;&#26032;&#20449;&#24687;&#26469;&#25552;&#39640;&#36825;&#20123;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25991;&#26412;&#26032;&#38395;&#29305;&#24449;&#32435;&#20837;&#20854;&#20013;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#33521;&#22269;&#22269;&#23478;&#30005;&#21147;&#38656;&#27714;&#30340;&#30830;&#23450;&#24615;&#21644;&#27010;&#29575;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20844;&#20247;&#24773;&#32490;&#20197;&#21450;&#19982;&#20132;&#36890;&#21644;&#22320;&#32536;&#25919;&#27835;&#30456;&#20851;&#30340;&#35789;&#21521;&#37327;&#34920;&#31034;&#23545;&#30005;&#21147;&#38656;&#27714;&#20855;&#26377;&#26102;&#38388;&#36830;&#32493;&#24615;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#25991;&#26412;&#29305;&#24449;&#30340;LSTM&#30456;&#23545;&#20110;&#32431;LSTM&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;&#36229;&#36807;3%&#65292;&#30456;&#23545;&#20110;&#23448;&#26041;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;&#25509;&#36817;10%&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#32553;&#23567;&#32622;&#20449;&#21306;&#38388;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electricity demand forecasting is a well established research field. Usually this task is performed considering historical loads, weather forecasts, calendar information and known major events. Recently attention has been given on the possible use of new sources of information from textual news in order to improve the performance of these predictions. This paper proposes a Long and Short-Term Memory (LSTM) network incorporating textual news features that successfully predicts the deterministic and probabilistic tasks of the UK national electricity demand. The study finds that public sentiment and word vector representations related to transport and geopolitics have time-continuity effects on electricity demand. The experimental results show that the LSTM with textual features improves by more than 3% compared to the pure LSTM benchmark and by close to 10% over the official benchmark. Furthermore, the proposed model effectively reduces forecasting uncertainty by narrowing the confidence
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06782</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21315;&#20806;&#32423;&#25968;&#25454;&#38598;&#29992;&#20110;&#31890;&#23376;&#27969;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Scalable neural network models and terascale datasets for particle-flow reconstruction. (arXiv:2309.06782v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#22522;&#20110;&#39640;&#24230;&#31890;&#24230;&#25506;&#27979;&#22120;&#27169;&#25311;&#30340;&#23436;&#25972;&#20107;&#20214;&#37325;&#24314;&#65292;&#30740;&#31350;&#20102;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#31890;&#23376;&#27969;&#65288;PF&#65289;&#37325;&#24314;&#21487;&#36890;&#36807;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#22242;&#31751;&#25110;&#20987;&#20013;&#26469;&#26500;&#24314;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#20869;&#26680;&#30340;&#21464;&#25442;&#22120;&#65292;&#24182;&#35777;&#26126;&#20004;&#32773;&#37117;&#36991;&#20813;&#20102;&#20108;&#27425;&#20869;&#23384;&#20998;&#37197;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30495;&#23454;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25152;&#24471;&#27169;&#22411;&#22312;&#30828;&#20214;&#22788;&#29702;&#22120;&#19978;&#20855;&#26377;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#25903;&#25345;NVIDIA, AMD&#21644;&#33521;&#29305;&#23572; Habana&#21345;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#21487;&#20197;&#22312;&#30001;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#20987;&#20013;&#32452;&#25104;&#30340;&#39640;&#31890;&#24230;&#36755;&#20837;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33719;&#24471;&#19982;&#22522;&#20934;&#30456;&#31454;&#20105;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#26377;&#20851;&#22797;&#29616;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24050;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#27979;&#35797;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.06774</link><description>&lt;p&gt;
&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss. (arXiv:2309.06774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#27979;&#35797;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21270;&#23398;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#30005;&#23376;&#24037;&#31243;&#12289;&#25968;&#23398;&#12289;&#21307;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#29289;&#29702;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#23545;&#20110;&#20026;&#20160;&#20040;&#21644;&#22914;&#20309;&#33719;&#24471;&#32463;&#39564;&#25104;&#21151;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#22522;&#26412;&#38590;&#20197;&#25226;&#25569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#26681;&#26412;&#38382;&#39064;&#24182;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#32972;&#21518;&#30340;&#22885;&#31192;&#65292;&#24050;&#32463;&#22312;&#24314;&#31435;&#32479;&#19968;&#29702;&#35770;&#30340;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#21019;&#26032;&#12290;&#36825;&#20123;&#21019;&#26032;&#21253;&#25324;&#20248;&#21270;&#12289;&#27867;&#21270;&#21644;&#36817;&#20284;&#31561;&#22522;&#30784;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#19968;&#20010;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#35299;&#20915;&#27169;&#24335;&#20998;&#31867;&#38382;&#39064;&#26102;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;&#20026;&#20102;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20811;&#26381;&#36825;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#27979;&#35797;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep learning (DL) has led to several breakthroughs in many disciplines as diverse as chemistry, computer science, electrical engineering, mathematics, medicine, neuroscience, and physics, a comprehensive understanding of why and how DL is empirically successful remains fundamentally elusive. To attack this fundamental problem and unravel the mysteries behind DL's empirical successes, significant innovations toward a unified theory of DL have been made. These innovations encompass nearly fundamental advances in optimization, generalization, and approximation. Despite these advances, however, no work to date has offered a way to quantify the testing performance of a DL-based algorithm employed to solve a pattern classification problem. To overcome this fundamental challenge in part, this paper exposes the fundamental testing performance limits of DL-based binary classifiers trained with hinge loss. For binary classifiers that are based on deep rectified linear unit (ReLU) feedf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#26102;&#38388;&#27493;&#26816;&#27979;&#22120;&#65288;MTD&#65289;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#24310;&#36831;&#38382;&#39064;&#65292;&#21253;&#25324;&#21160;&#24577;&#36335;&#30001;&#21644;&#24310;&#36831;&#20998;&#26512;&#27169;&#22359;&#65288;DAM&#65289;&#65292;&#20197;&#21450;&#26102;&#38388;&#27493;&#39588;&#20998;&#25903;&#27169;&#22359;&#65288;TBM&#65289;&#26469;&#36866;&#24212;&#24615;&#22320;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#12290;</title><link>http://arxiv.org/abs/2309.06742</link><description>&lt;p&gt;
MTD: &#22810;&#26102;&#38388;&#27493;&#26816;&#27979;&#22120;&#29992;&#20110;&#24310;&#36831;&#27969;&#24335;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
MTD: Multi-Timestep Detector for Delayed Streaming Perception. (arXiv:2309.06742v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06742
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#26102;&#38388;&#27493;&#26816;&#27979;&#22120;&#65288;MTD&#65289;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#24310;&#36831;&#38382;&#39064;&#65292;&#21253;&#25324;&#21160;&#24577;&#36335;&#30001;&#21644;&#24310;&#36831;&#20998;&#26512;&#27169;&#22359;&#65288;DAM&#65289;&#65292;&#20197;&#21450;&#26102;&#38388;&#27493;&#39588;&#20998;&#25903;&#27169;&#22359;&#65288;TBM&#65289;&#26469;&#36866;&#24212;&#24615;&#22320;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#23454;&#26102;&#29615;&#22659;&#24863;&#30693;&#20197;&#30830;&#20445;&#29992;&#25143;&#23433;&#20840;&#21644;&#20307;&#39564;&#12290;&#27969;&#24335;&#24863;&#30693;&#26159;&#25253;&#21578;&#19990;&#30028;&#24403;&#21069;&#29366;&#24577;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#30828;&#20214;&#38480;&#21046;&#21644;&#39640;&#28201;&#31561;&#22240;&#32032;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24310;&#36831;&#65292;&#20174;&#32780;&#23548;&#33268;&#27169;&#22411;&#36755;&#20986;&#19982;&#19990;&#30028;&#29366;&#24577;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#26102;&#38388;&#27493;&#26816;&#27979;&#22120;&#65288;MTD&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26816;&#27979;&#22120;&#65292;&#20351;&#29992;&#21160;&#24577;&#36335;&#30001;&#36827;&#34892;&#22810;&#20998;&#25903;&#26410;&#26469;&#39044;&#27979;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#25269;&#25239;&#24310;&#36831;&#27874;&#21160;&#30340;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#24310;&#36831;&#20998;&#26512;&#27169;&#22359;&#65288;DAM&#65289;&#65292;&#20248;&#21270;&#29616;&#26377;&#30340;&#24310;&#36831;&#24863;&#30693;&#26041;&#27861;&#65292;&#19981;&#26029;&#30417;&#27979;&#27169;&#22411;&#25512;&#26029;&#22534;&#26632;&#24182;&#35745;&#31639;&#24310;&#36831;&#36235;&#21183;&#12290;&#27492;&#22806;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26102;&#38388;&#27493;&#39588;&#20998;&#25903;&#27169;&#22359;&#65288;TBM&#65289;&#65292;&#21253;&#25324;&#38745;&#24577;&#27969;&#21644;&#33258;&#36866;&#24212;&#27969;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving systems require real-time environmental perception to ensure user safety and experience. Streaming perception is a task of reporting the current state of the world, which is used to evaluate the delay and accuracy of autonomous driving systems. In real-world applications, factors such as hardware limitations and high temperatures inevitably cause delays in autonomous driving systems, resulting in the offset between the model output and the world state. In order to solve this problem, this paper propose the Multi- Timestep Detector (MTD), an end-to-end detector which uses dynamic routing for multi-branch future prediction, giving model the ability to resist delay fluctuations. A Delay Analysis Module (DAM) is proposed to optimize the existing delay sensing method, continuously monitoring the model inference stack and calculating the delay trend. Moreover, a novel Timestep Branch Module (TBM) is constructed, which includes static flow and adaptive flow to adaptively pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCNS&#30340;&#33258;&#21160;&#39046;&#22495;&#26080;&#20851;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#22240;&#26524;&#20851;&#31995;&#26041;&#26696;&#24110;&#21161;&#25366;&#25496;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#33258;&#28982;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.06739</link><description>&lt;p&gt;
MCNS: &#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#20869;&#37096;&#22240;&#26524;&#26041;&#26696;&#25366;&#25496;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#33258;&#28982;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
MCNS: Mining Causal Natural Structures Inside Time Series via A Novel Internal Causality Scheme. (arXiv:2309.06739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCNS&#30340;&#33258;&#21160;&#39046;&#22495;&#26080;&#20851;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#22240;&#26524;&#20851;&#31995;&#26041;&#26696;&#24110;&#21161;&#25366;&#25496;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#33258;&#28982;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#20351;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#20013;&#21508;&#20010;&#21464;&#37327;&#20043;&#38388;&#30340;&#38544;&#34255;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#25552;&#21040;&#30340;&#21464;&#37327;&#26159;&#32500;&#24230;&#65292;&#32500;&#24230;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21487;&#33021;&#26159;&#32932;&#27973;&#30340;&#65292;&#36825;&#38459;&#30861;&#20102;&#23545;&#20869;&#37096;&#20851;&#31995;&#30340;&#29702;&#35299;&#20197;&#21450;&#22240;&#26524;&#22270;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#22909;&#22788;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22240;&#26524;&#20851;&#31995;&#19981;&#20165;&#23384;&#22312;&#20110;&#26102;&#38388;&#24207;&#21015;&#20043;&#22806;&#65292;&#20063;&#23384;&#22312;&#20110;&#26102;&#38388;&#24207;&#21015;&#20043;&#20869;&#65292;&#22240;&#20026;&#36825;&#21453;&#26144;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#19968;&#31995;&#21015;&#20107;&#20214;&#30340;&#39034;&#24207;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#23547;&#25214;&#20869;&#37096;&#23376;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#20174;&#23376;&#24207;&#21015;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#33258;&#28982;&#32467;&#26500;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCNS&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#26159;&#33258;&#21160;&#30340;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;&#65292;&#24182;&#36890;&#36807;&#20869;&#37096;&#22240;&#26524;&#26041;&#26696;&#24110;&#21161;&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#33258;&#28982;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference permits us to discover covert relationships of various variables in time series. However, in most existing works, the variables mentioned above are the dimensions. The causality between dimensions could be cursory, which hinders the comprehension of the internal relationship and the benefit of the causal graph to the neural networks (NNs). In this paper, we find that causality exists not only outside but also inside the time series because it reflects a succession of events in the real world. It inspires us to seek the relationship between internal subsequences. However, the challenges are the hardship of discovering causality from subsequences and utilizing the causal natural structures to improve NNs. To address these challenges, we propose a novel framework called Mining Causal Natural Structure (MCNS), which is automatic and domain-agnostic and helps to find the causal natural structures inside time series via the internal causality scheme. We evaluate the MCNS fra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38750;&#21442;&#25968;&#20984;&#21270;&#28388;&#27874;&#65288;DNCF&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35745;&#31639;&#25668;&#24433;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#24674;&#22797;&#12290;DNCF&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2309.06724</link><description>&lt;p&gt;
&#28145;&#24230;&#38750;&#21442;&#25968;&#20984;&#21270;&#28388;&#27874;&#22312;&#35745;&#31639;&#25668;&#24433;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#23545;&#25239;&#24615;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense. (arXiv:2309.06724v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38750;&#21442;&#25968;&#20984;&#21270;&#28388;&#27874;&#65288;DNCF&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35745;&#31639;&#25668;&#24433;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#24674;&#22797;&#12290;DNCF&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#19981;&#23436;&#32654;&#30340;&#22270;&#20687;&#20013;&#24674;&#22797;&#30495;&#23454;&#22330;&#26223;&#30340;&#35745;&#31639;&#25668;&#24433;&#65292;&#36890;&#36807;&#28145;&#24230;&#38750;&#21442;&#25968;&#20984;&#21270;&#28388;&#27874;&#65288;DNCF&#65289;&#12290;&#23427;&#30001;&#19968;&#20010;&#38750;&#21442;&#25968;&#28145;&#24230;&#32593;&#32476;&#32452;&#25104;&#65292;&#20197;&#27169;&#25311;&#22270;&#20687;&#24418;&#25104;&#32972;&#21518;&#30340;&#29289;&#29702;&#26041;&#31243;&#65292;&#22914;&#38477;&#22122;&#12289;&#36229;&#20998;&#36776;&#29575;&#12289;&#20462;&#22797;&#21644;&#38378;&#20809;&#12290;DNCF&#27809;&#26377;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#65292;&#22240;&#27492;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#40723;&#21169;&#32593;&#32476;&#21442;&#25968;&#20026;&#38750;&#36127;&#65292;&#24182;&#22312;&#36755;&#20837;&#21644;&#21442;&#25968;&#19978;&#21019;&#24314;&#19968;&#20010;&#21452;&#20984;&#20989;&#25968;&#65292;&#36825;&#36866;&#24212;&#20110;&#36816;&#34892;&#26102;&#38388;&#19981;&#36275;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;Deep Image Prior&#26377;10&#20493;&#30340;&#21152;&#36895;&#12290;&#36890;&#36807;&#36825;&#20123;&#24037;&#20855;&#65292;&#25105;&#20204;&#22312;&#23454;&#26102;&#20013;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#23545;&#25239;&#22270;&#20687;&#20998;&#31867;&#28145;&#24230;&#32593;&#32476;&#25915;&#20987;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to provide a general framework of for computational photography that recovers the real scene from imperfect images, via the Deep Nonparametric Convexified Filtering (DNCF). It is consists of a nonparametric deep network to resemble the physical equations behind the image formation, such as denoising, super-resolution, inpainting, and flash. DNCF has no parameterization dependent on training data, therefore has a strong generalization and robustness to adversarial image manipulation. During inference, we also encourage the network parameters to be nonnegative and create a bi-convex function on the input and parameters, and this adapts to second-order optimization algorithms with insufficient running time, having 10X acceleration over Deep Image Prior. With these tools, we empirically verify its capability to defend image classification deep networks against adversary attack algorithms in real-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BAM&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#36741;&#21161;&#21464;&#37327;&#26469;&#25918;&#22823;&#20559;&#35265;&#65292;&#25552;&#39640;&#20102;&#23569;&#25968;&#32676;&#20307;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.06717</link><description>&lt;p&gt;
&#20559;&#35265;&#25918;&#22823;&#22686;&#24378;&#20102;&#23569;&#25968;&#32676;&#20307;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Bias Amplification Enhances Minority Group Performance. (arXiv:2309.06717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BAM&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#36741;&#21161;&#21464;&#37327;&#26469;&#25918;&#22823;&#20559;&#35265;&#65292;&#25552;&#39640;&#20102;&#23569;&#25968;&#32676;&#20307;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#26631;&#20934;&#35757;&#32451;&#20135;&#29983;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#32597;&#35265;&#30340;&#23376;&#32676;&#19978;&#30340;&#20934;&#30830;&#24615;&#36739;&#24046;&#65292;&#23613;&#31649;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#26576;&#20123;&#34394;&#20551;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20043;&#21069;&#22522;&#20110;&#26368;&#24046;&#32676;&#20307;&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;Group-DRO&#65289;&#22312;&#25913;&#21892;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#38656;&#35201;&#20026;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25552;&#20379;&#26114;&#36149;&#30340;&#32676;&#20307;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#24773;&#26223;&#65292;&#21363;&#32676;&#20307;&#27880;&#37322;&#20165;&#22312;&#19968;&#20010;&#23567;&#30340;&#39564;&#35777;&#38598;&#19978;&#21487;&#29992;&#65292;&#25110;&#32773;&#26681;&#26412;&#19981;&#21487;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BAM&#65292;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31639;&#27861;&#65306;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#36741;&#21161;&#21464;&#37327;&#20026;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#35757;&#32451;&#19968;&#20010;&#20559;&#35265;&#25918;&#22823;&#26041;&#26696;&#30340;&#27169;&#22411;&#65307;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23545;&#20559;&#35265;&#25918;&#22823;&#30340;&#27169;&#22411;&#35823;&#20998;&#31867;&#30340;&#26679;&#26412;&#36827;&#34892;&#21152;&#26435;&#65292;&#28982;&#21518;&#22312;&#37325;&#26032;&#21152;&#26435;&#30340;&#25968;&#25454;&#38598;&#19978;&#32487;&#32493;&#35757;&#32451;&#21516;&#19968;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BAM&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks produced by standard training are known to suffer from poor accuracy on rare subgroups despite achieving high accuracy on average, due to the correlations between certain spurious features and labels. Previous approaches based on worst-group loss minimization (e.g. Group-DRO) are effective in improving worse-group accuracy but require expensive group annotations for all the training samples. In this paper, we focus on the more challenging and realistic setting where group annotations are only available on a small validation set or are not available at all. We propose BAM, a novel two-stage training algorithm: in the first stage, the model is trained using a bias amplification scheme via introducing a learnable auxiliary variable for each training sample; in the second stage, we upweight the samples that the bias-amplified model misclassifies, and then continue training the same model on the reweighted dataset. Empirically, BAM achieves competitive performance compared w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21183;&#21644;&#24180;&#40836;&#36866;&#24212;Pareto&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#33021;&#22815;&#25214;&#21040;&#33021;&#37327;&#26368;&#20248;&#30340;&#26230;&#20307;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#25628;&#32034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06710</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21183;&#21644;&#24180;&#40836;&#36866;&#24212;Pareto&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Crystal structure prediction using neural network potential and age-fitness Pareto genetic algorithm. (arXiv:2309.06710v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06710
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21183;&#21644;&#24180;&#40836;&#36866;&#24212;Pareto&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#33021;&#22815;&#25214;&#21040;&#33021;&#37327;&#26368;&#20248;&#30340;&#26230;&#20307;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#25628;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ParetoCSP&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#23558;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#65288;MOGA&#65289;&#19982;&#31070;&#32463;&#32593;&#32476;&#38388;&#21407;&#23376;&#21183;&#27169;&#22411;&#65288;IAP&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25214;&#21040;&#32473;&#23450;&#21270;&#23398;&#25104;&#20998;&#30340;&#33021;&#37327;&#26368;&#20248;&#30340;&#26230;&#20307;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22522;&#22240;&#22411;&#24180;&#40836;&#20316;&#20026;&#29420;&#31435;&#20248;&#21270;&#26631;&#20934;&#26469;&#22686;&#24378;NSGA-III&#31639;&#27861;&#65292;&#24182;&#37319;&#29992;M3GNet&#36890;&#29992;IAP&#26469;&#24341;&#23548;&#36951;&#20256;&#31639;&#27861;&#30340;&#25628;&#32034;&#12290;&#19982;GN-OA&#65292;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#21183;&#30340;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#30456;&#27604;&#65292;ParetoCSP&#22312;55&#20010;&#19981;&#21516;&#30340;&#22522;&#20934;&#32467;&#26500;&#19978;&#36890;&#36807;&#19971;&#20010;&#24615;&#33021;&#25351;&#26631;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#20855;&#26377;&#26174;&#33879;&#26356;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20248;&#21183;&#22240;&#23376;&#20026;2.562&#12290;&#25152;&#26377;&#31639;&#27861;&#25152;&#36941;&#21382;&#32467;&#26500;&#30340;&#36712;&#36857;&#20998;&#26512;&#34920;&#26126;&#65292;ParetoCSP&#20135;&#29983;&#20102;&#26356;&#22810;&#30340;&#26377;&#25928;&#32467;&#26500;&#65292;&#36825;&#26377;&#21161;&#20110;&#25351;&#23548;&#36951;&#20256;&#31639;&#27861;&#26356;&#26377;&#25928;&#22320;&#25628;&#32034;&#26368;&#20248;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
While crystal structure prediction (CSP) remains a longstanding challenge, we introduce ParetoCSP, a novel algorithm for CSP, which combines a multi-objective genetic algorithm (MOGA) with a neural network inter-atomic potential (IAP) model to find energetically optimal crystal structures given chemical compositions. We enhance the NSGA-III algorithm by incorporating the genotypic age as an independent optimization criterion and employ the M3GNet universal IAP to guide the GA search. Compared to GN-OA, a state-of-the-art neural potential based CSP algorithm, ParetoCSP demonstrated significantly better predictive capabilities, outperforming by a factor of $2.562$ across $55$ diverse benchmark structures, as evaluated by seven performance metrics. Trajectory analysis of the traversed structures of all algorithms shows that ParetoCSP generated more valid structures than other algorithms, which helped guide the GA to search more effectively for the optimal structures
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36335;&#24452;&#20999;&#29255;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#39044;&#27979;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#21152;&#36733;&#26465;&#20214;&#19979;&#30340;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#21644;&#32452;&#20214;&#30340;&#22833;&#25928;&#23551;&#21629;&#12290;&#36890;&#36807;&#26500;&#24314;&#25968;&#23383;&#24211;&#24182;&#24341;&#20837;&#36335;&#24452;&#20999;&#29255;&#21644;&#37325;&#26032;&#21152;&#26435;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#30130;&#21171;&#22797;&#26434;&#24615;&#21644;&#32479;&#35745;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.06708</link><description>&lt;p&gt;
&#36890;&#36807;&#36335;&#24452;&#20999;&#29255;&#21644;&#37325;&#26032;&#21152;&#26435;&#39044;&#27979;&#30130;&#21171;&#35010;&#32441;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Predicting Fatigue Crack Growth via Path Slicing and Re-Weighting. (arXiv:2309.06708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06708
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36335;&#24452;&#20999;&#29255;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#39044;&#27979;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#21152;&#36733;&#26465;&#20214;&#19979;&#30340;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#21644;&#32452;&#20214;&#30340;&#22833;&#25928;&#23551;&#21629;&#12290;&#36890;&#36807;&#26500;&#24314;&#25968;&#23383;&#24211;&#24182;&#24341;&#20837;&#36335;&#24452;&#20999;&#29255;&#21644;&#37325;&#26032;&#21152;&#26435;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#30130;&#21171;&#22797;&#26434;&#24615;&#21644;&#32479;&#35745;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#65292;&#39044;&#27979;&#19982;&#20851;&#38190;&#32467;&#26500;&#32452;&#20214;&#30130;&#21171;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30130;&#21171;&#24120;&#24120;&#28041;&#21450;&#26448;&#26009;&#24494;&#35266;&#32467;&#26500;&#21644;&#20351;&#29992;&#26465;&#20214;&#30340;&#22797;&#26434;&#24615;&#65292;&#20351;&#30130;&#21171;&#25439;&#20260;&#30340;&#35786;&#26029;&#21644;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#21152;&#36733;&#26465;&#20214;&#19979;&#30130;&#21171;&#35010;&#32441;&#30340;&#22686;&#38271;&#21644;&#32452;&#20214;&#30340;&#22833;&#25928;&#23551;&#21629;&#12290;&#36890;&#36807;&#39640;&#20445;&#30495;&#29289;&#29702;&#27169;&#25311;&#26500;&#24314;&#20102;&#30130;&#21171;&#35010;&#32441;&#27169;&#24335;&#21644;&#21097;&#20313;&#23551;&#21629;&#30340;&#25968;&#23383;&#24211;&#12290;&#28982;&#21518;&#20351;&#29992;&#38477;&#32500;&#21644;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#30340;&#21382;&#21490;&#20381;&#36182;&#24615;&#21644;&#38750;&#32447;&#24615;&#12290;&#24341;&#20837;&#20102;&#36335;&#24452;&#20999;&#29255;&#21644;&#37325;&#26032;&#21152;&#26435;&#25216;&#26415;&#26469;&#22788;&#29702;&#32479;&#35745;&#22122;&#22768;&#21644;&#32597;&#35265;&#20107;&#20214;&#12290;&#39044;&#27979;&#30340;&#30130;&#21171;&#35010;&#32441;&#27169;&#24335;&#36890;&#36807;&#19981;&#26029;&#28436;&#21270;&#30340;&#35010;&#32441;&#27169;&#24335;&#36827;&#34892;&#33258;&#26356;&#26032;&#21644;&#33258;&#26657;&#27491;&#12290;&#20195;&#34920;&#24615;&#23454;&#20363;&#39564;&#35777;&#20102;&#31471;&#21040;&#31471;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting potential risks associated with the fatigue of key structural components is crucial in engineering design. However, fatigue often involves entangled complexities of material microstructures and service conditions, making diagnosis and prognosis of fatigue damage challenging. We report a statistical learning framework to predict the growth of fatigue cracks and the life-to-failure of the components under loading conditions with uncertainties. Digital libraries of fatigue crack patterns and the remaining life are constructed by high-fidelity physical simulations. Dimensionality reduction and neural network architectures are then used to learn the history dependence and nonlinearity of fatigue crack growth. Path-slicing and re-weighting techniques are introduced to handle the statistical noises and rare events. The predicted fatigue crack patterns are self-updated and self-corrected by the evolving crack patterns. The end-to-end approach is validated by representative examples 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#31995;&#32479;VLSlice&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#24341;&#23548;&#21457;&#29616;&#19968;&#33268;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34892;&#20026;&#30340;&#34920;&#31034;&#32423;&#23376;&#32452;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#21457;&#29616;&#23376;&#32452;&#26102;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.06703</link><description>&lt;p&gt;
VLSlice&#65306;&#20132;&#20114;&#24335;&#35270;&#35273;&#21644;&#35821;&#35328;&#20999;&#29255;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
VLSlice: Interactive Vision-and-Language Slice Discovery. (arXiv:2309.06703v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#31995;&#32479;VLSlice&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#24341;&#23548;&#21457;&#29616;&#19968;&#33268;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34892;&#20026;&#30340;&#34920;&#31034;&#32423;&#23376;&#32452;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#21457;&#29616;&#23376;&#32452;&#26102;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21487;&#20197;&#23398;&#20064;&#20986;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36801;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#21487;&#33021;&#25913;&#21892;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#32858;&#21512;&#25351;&#26631;&#65292;&#20294;&#36890;&#36807;&#20998;&#26512;&#38024;&#23545;&#29305;&#23450;&#20559;&#24046;&#32500;&#24230;&#30340;&#25163;&#24037;&#23376;&#32452;&#26102;&#65292;&#21457;&#29616;&#20102;&#31995;&#32479;&#24615;&#30340;&#19981;&#33391;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23376;&#32452;&#20998;&#26512;&#36890;&#24120;&#20250;&#22240;&#20026;&#27880;&#37322;&#24037;&#20316;&#32780;&#20572;&#28382;&#65292;&#32780;&#25910;&#38598;&#25152;&#38656;&#25968;&#25454;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#23581;&#35797;&#33258;&#21160;&#21457;&#29616;&#23376;&#32452;&#20197;&#35268;&#36991;&#36825;&#20123;&#38480;&#21046;&#65292;&#20294;&#36890;&#24120;&#21033;&#29992;&#29616;&#26377;&#20219;&#21153;&#29305;&#23450;&#27880;&#37322;&#19978;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#24182;&#22312;&#36229;&#20986;&#8220;&#34920;&#26684;&#8221;&#25968;&#25454;&#30340;&#26356;&#22797;&#26434;&#36755;&#20837;&#19978;&#24555;&#36895;&#38477;&#32423;&#65292;&#20854;&#20013;&#27809;&#26377;&#30740;&#31350;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;VLSlice&#65292;&#19968;&#31181;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#24341;&#23548;&#21457;&#29616;&#19968;&#33268;&#30340;&#34920;&#31034;&#32423;&#23376;&#32452;&#65292;&#20855;&#26377;&#19968;&#33268;&#30340;&#35270;&#35273;&#35821;&#35328;&#34892;&#20026;&#65292;&#34987;&#31216;&#20026;&#35270;&#35273;&#21644;&#35821;&#35328;&#20999;&#29255;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#20013;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond "tabular" data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled ima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FedGH&#65292;&#36890;&#36807;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#65292;FedGH&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06692</link><description>&lt;p&gt;
&#35299;&#20915;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#30340;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization. (arXiv:2309.06692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FedGH&#65292;&#36890;&#36807;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#65292;FedGH&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#20174;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#21327;&#20316;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#21463;&#21040;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21644;&#35774;&#22791;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26381;&#21153;&#22120;&#31471;&#30340;&#26799;&#24230;&#20914;&#31361;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26799;&#24230;&#20914;&#31361;&#29616;&#35937;&#65292;&#24182;&#25581;&#31034;&#20102;&#26356;&#24378;&#30340;&#24322;&#26500;&#24615;&#20250;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#26799;&#24230;&#20914;&#31361;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedGH&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26469;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#12290;&#36825;&#31181;&#25216;&#26415;&#23558;&#19968;&#20010;&#26799;&#24230;&#21521;&#37327;&#25237;&#24433;&#21040;&#19982;&#20854;&#20182;&#20914;&#31361;&#23458;&#25143;&#31471;&#23545;&#20043;&#38388;&#30340;&#27491;&#20132;&#24179;&#38754;&#19978;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FedGH&#22312;&#19981;&#21516;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#32447;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;FedGH&#22312;&#29305;&#23450;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26356;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-preserving paradigm for collaboratively training a global model from decentralized clients. However, the performance of FL is hindered by non-independent and identically distributed (non-IID) data and device heterogeneity. In this work, we revisit this key challenge through the lens of gradient conflicts on the server side. Specifically, we first investigate the gradient conflict phenomenon among multiple clients and reveal that stronger heterogeneity leads to more severe gradient conflicts. To tackle this issue, we propose FedGH, a simple yet effective method that mitigates local drifts through Gradient Harmonization. This technique projects one gradient vector onto the orthogonal plane of the other within conflicting client pairs. Extensive experiments demonstrate that FedGH consistently enhances multiple state-of-the-art FL baselines across diverse benchmarks and non-IID scenarios. Notably, FedGH yields more significant improvements in scenarios 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay (ALAP)&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#65292;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#65292;&#28040;&#38500;&#20102;&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#21644;&#23545;&#27604;&#30740;&#31350;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.06684</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attention Loss Adjusted Prioritized Experience Replay. (arXiv:2309.06684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;Attention Loss Adjusted Prioritized Experience Replay (ALAP)&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#65292;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#65292;&#28040;&#38500;&#20102;&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#21644;&#23545;&#27604;&#30740;&#31350;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#32463;&#39564;&#22238;&#25918;&#31639;&#27861;(Prioritized Experience Replay, PER)&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#26356;&#22810;&#30693;&#35782;&#37327;&#30340;&#32463;&#39564;&#26679;&#26412;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;PER&#20013;&#20351;&#29992;&#30340;&#38750;&#22343;&#21248;&#37319;&#26679;&#19981;&#21487;&#36991;&#20813;&#22320;&#20351;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#24102;&#26469;Q&#20540;&#20989;&#25968;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Attention Loss Adjusted Prioritized (ALAP) Experience Replay&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#21452;&#37319;&#26679;&#26426;&#21046;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36866;&#24212;&#33021;&#22815;&#35843;&#33410;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#22240;PER&#24341;&#36215;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#20026;&#20102;&#39564;&#35777;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;OPENAI gym&#29615;&#22659;&#20013;&#23545;&#22522;&#20110;&#20540;&#20989;&#25968;&#12289;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#21644;&#22810;&#20027;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#23545;&#27604;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26694;&#26550;&#30340;&#20248;&#21183;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prioritized Experience Replay (PER) is a technical means of deep reinforcement learning by selecting experience samples with more knowledge quantity to improve the training rate of neural network. However, the non-uniform sampling used in PER inevitably shifts the state-action space distribution and brings the estimation error of Q-value function. In this paper, an Attention Loss Adjusted Prioritized (ALAP) Experience Replay algorithm is proposed, which integrates the improved Self-Attention network with Double-Sampling mechanism to fit the hyperparameter that can regulate the importance sampling weights to eliminate the estimation error caused by PER. In order to verify the effectiveness and generality of the algorithm, the ALAP is tested with value-function based, policy-gradient based and multi-agent reinforcement learning algorithms in OPENAI gym, and comparison studies verify the advantage and efficiency of the proposed training framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#32852;&#37030;PAC-Bayesian&#30028;&#38480;&#65292;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#21807;&#19968;&#20808;&#39564;&#30693;&#35782;&#21644;&#21487;&#21464;&#32858;&#21512;&#26435;&#37325;&#65292;&#20197;&#21450;&#24341;&#20837;&#30446;&#26631;&#20989;&#25968;&#21644;&#21019;&#26032;&#30340;&#22522;&#20110;Gibbs&#30340;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.06683</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#37030;PAC-Bayesian&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated PAC-Bayesian Learning on Non-IID data. (arXiv:2309.06683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#32852;&#37030;PAC-Bayesian&#30028;&#38480;&#65292;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#21807;&#19968;&#20808;&#39564;&#30693;&#35782;&#21644;&#21487;&#21464;&#32858;&#21512;&#26435;&#37325;&#65292;&#20197;&#21450;&#24341;&#20837;&#30446;&#26631;&#20989;&#25968;&#21644;&#21019;&#26032;&#30340;&#22522;&#20110;Gibbs&#30340;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#23558;Probably Approximately Correct&#65288;PAC&#65289;&#36125;&#21494;&#26031;&#26694;&#26550;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#35201;&#20040;&#22312;&#24341;&#20837;&#23450;&#29702;&#26102;&#20351;&#29992;&#20449;&#24687;&#35770;&#30340;PAC-Bayesian&#30028;&#38480;&#65292;&#20294;&#24456;&#23569;&#32771;&#34385;FL&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#26412;&#22320;&#25968;&#25454;&#30340;&#30495;&#23454;&#32852;&#37030;PAC-Bayesian&#30028;&#38480;&#12290;&#35813;&#30028;&#38480;&#20551;&#35774;&#27599;&#20010;&#23458;&#25143;&#31471;&#20855;&#26377;&#21807;&#19968;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#21487;&#21464;&#32858;&#21512;&#26435;&#37325;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#30446;&#26631;&#20989;&#25968;&#21644;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;Gibbs&#30340;&#31639;&#27861;&#26469;&#20248;&#21270;&#24471;&#21040;&#30340;&#30028;&#38480;&#12290;&#32467;&#26524;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research has either adapted the Probably Approximately Correct (PAC) Bayesian framework for federated learning (FL) or used information-theoretic PAC-Bayesian bounds while introducing their theorems, but few considering the non-IID challenges in FL. Our work presents the first non-vacuous federated PAC-Bayesian bound tailored for non-IID local data. This bound assumes unique prior knowledge for each client and variable aggregation weights. We also introduce an objective function and an innovative Gibbs-based algorithm for the optimization of the derived bound. The results are validated on real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#25913;&#36827;&#20102;Spalart-Allmaras&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#31163;&#27969;&#20307;&#30340;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#35299;&#30340;&#27867;&#21270;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06679</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#23454;&#29616;&#23545;Spalart-Allmaras&#27169;&#22411;&#30340;&#21487;&#26222;&#36866;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Generalizable improvement of the Spalart-Allmaras model through assimilation of experimental data. (arXiv:2309.06679v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#25913;&#36827;&#20102;Spalart-Allmaras&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#31163;&#27969;&#20307;&#30340;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#35299;&#30340;&#27867;&#21270;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#27169;&#22411;&#21644;&#25968;&#25454;&#34701;&#21512;&#25913;&#36827;&#20998;&#31163;&#27969;&#20307;&#30340;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#35299;&#30340;Spalart-Allmaras&#65288;SA&#65289;&#38381;&#21512;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#27169;&#22411;&#65292;&#19981;&#20165;&#33021;&#23558;&#31232;&#30095;&#30340;&#23454;&#39564;&#25968;&#25454;&#21516;&#21270;&#20197;&#25913;&#21892;&#35745;&#31639;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36824;&#33021;&#36890;&#36807;&#24674;&#22797;&#32463;&#20856;&#30340;SA&#34892;&#20026;&#26469;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#21516;&#21270;&#65292;&#21363;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#26041;&#27861;&#65288;EnKF&#65289;&#65292;&#36890;&#36807;&#23558;SA&#27169;&#22411;&#30340;&#31995;&#25968;&#26657;&#20934;&#21040;&#20998;&#31163;&#27969;&#20307;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#21442;&#25968;&#21270;&#20135;&#29983;&#12289;&#25193;&#25955;&#21644;&#30772;&#22351;&#39033;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26657;&#20934;&#31574;&#30053;&#12290;&#35813;&#26657;&#20934;&#20381;&#36182;&#20110;&#37319;&#38598;&#30340;&#20998;&#31163;&#27969;&#20307;&#36895;&#24230;&#21078;&#38754;&#12289;&#22721;&#25830;&#21147;&#21644;&#21387;&#21147;&#31995;&#25968;&#30340;&#23454;&#39564;&#25968;&#25454;&#30340;&#21516;&#21270;&#12290;&#23613;&#31649;&#20165;&#20351;&#29992;&#20102;&#26469;&#33258;&#21333;&#19968;&#27969;&#21160;&#26465;&#20214;&#65288;&#29615;&#32469;&#19968;&#20010;&#32972;&#38754;&#21488;&#38454;&#65289;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#20294;&#37325;&#26032;&#26657;&#20934;&#30340;SA&#27169;&#22411;&#34920;&#29616;&#20986;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on the use of model and data fusion for improving the Spalart-Allmaras (SA) closure model for Reynolds-averaged Navier-Stokes solutions of separated flows. In particular, our goal is to develop of models that not-only assimilate sparse experimental data to improve performance in computational models, but also generalize to unseen cases by recovering classical SA behavior. We achieve our goals using data assimilation, namely the Ensemble Kalman Filtering approach (EnKF), to calibrate the coefficients of the SA model for separated flows. A holistic calibration strategy is implemented via a parameterization of the production, diffusion, and destruction terms. This calibration relies on the assimilation of experimental data collected velocity profiles, skin friction, and pressure coefficients for separated flows. Despite using of observational data from a single flow condition around a backward-facing step (BFS), the recalibrated SA model demonstrates generalization to o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#38454;&#27573;&#31070;&#32463;&#32593;&#32476;&#30340;&#22768;&#22330;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#31532;&#19968;&#38454;&#27573;&#23558;&#21512;&#25104;&#30340;&#40614;&#20811;&#39118;&#22788;&#30340;&#22768;&#21387;&#20998;&#31163;&#25104;&#27599;&#20010;&#22768;&#28304;&#28608;&#21457;&#30340;&#22768;&#21387;&#65292;&#28982;&#21518;&#36890;&#36807;&#31532;&#20108;&#38454;&#27573;&#20174;&#21333;&#19968;&#22768;&#28304;&#22788;&#30340;&#22768;&#21387;&#22238;&#24402;&#24471;&#21040;&#28304;&#20301;&#32622;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#28304;&#23450;&#20301;&#31934;&#24230;&#21644;&#22768;&#22330;&#37325;&#26500;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.06661</link><description>&lt;p&gt;
&#22522;&#20110;&#20004;&#38454;&#27573;&#31070;&#32463;&#32593;&#32476;&#30340;&#22768;&#22330;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Sound field decomposition based on two-stage neural networks. (arXiv:2309.06661v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06661
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#38454;&#27573;&#31070;&#32463;&#32593;&#32476;&#30340;&#22768;&#22330;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#31532;&#19968;&#38454;&#27573;&#23558;&#21512;&#25104;&#30340;&#40614;&#20811;&#39118;&#22788;&#30340;&#22768;&#21387;&#20998;&#31163;&#25104;&#27599;&#20010;&#22768;&#28304;&#28608;&#21457;&#30340;&#22768;&#21387;&#65292;&#28982;&#21518;&#36890;&#36807;&#31532;&#20108;&#38454;&#27573;&#20174;&#21333;&#19968;&#22768;&#28304;&#22788;&#30340;&#22768;&#21387;&#22238;&#24402;&#24471;&#21040;&#28304;&#20301;&#32622;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#28304;&#23450;&#20301;&#31934;&#24230;&#21644;&#22768;&#22330;&#37325;&#26500;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22768;&#22330;&#20998;&#35299;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#22768;&#22330;&#20998;&#31163;&#38454;&#27573;&#21644;&#21333;&#28304;&#23450;&#20301;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#30001;&#22810;&#20010;&#22768;&#28304;&#21512;&#25104;&#30340;&#40614;&#20811;&#39118;&#22788;&#30340;&#22768;&#21387;&#34987;&#20998;&#31163;&#25104;&#27599;&#20010;&#22768;&#28304;&#28608;&#21457;&#30340;&#22768;&#21387;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#36890;&#36807;&#30001;&#21333;&#19968;&#22768;&#28304;&#32452;&#25104;&#30340;&#40614;&#20811;&#39118;&#22788;&#30340;&#22768;&#21387;&#36827;&#34892;&#22238;&#24402;&#65292;&#33719;&#24471;&#28304;&#20301;&#32622;&#12290;&#30001;&#20110;&#31532;&#20108;&#38454;&#27573;&#35774;&#35745;&#20026;&#22238;&#24402;&#32780;&#19981;&#26159;&#20998;&#31867;&#65292;&#22240;&#27492;&#20272;&#35745;&#30340;&#20301;&#32622;&#19981;&#21463;&#31163;&#25955;&#21270;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;Green&#20989;&#25968;&#36827;&#34892;&#27169;&#25311;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#39057;&#29575;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#28304;&#23450;&#20301;&#31934;&#24230;&#21644;&#22768;&#22330;&#37325;&#26500;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A method for sound field decomposition based on neural networks is proposed. The method comprises two stages: a sound field separation stage and a single-source localization stage. In the first stage, the sound pressure at microphones synthesized by multiple sources is separated into one excited by each sound source. In the second stage, the source location is obtained as a regression from the sound pressure at microphones consisting of a single sound source. The estimated location is not affected by discretization because the second stage is designed as a regression rather than a classification. Datasets are generated by simulation using Green's function, and the neural network is trained for each frequency. Numerical experiments reveal that, compared with conventional methods, the proposed method can achieve higher source-localization accuracy and higher sound-field-reconstruction accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#23558;&#22823;&#35268;&#27169;&#30340;&#31070;&#32463;&#34920;&#31034;&#35757;&#32451;&#35270;&#20026;&#37096;&#20998;&#35266;&#27979;&#30340;&#31070;&#32463;&#36807;&#31243;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#36807;&#31243;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.06660</link><description>&lt;p&gt;
&#36890;&#29992;&#30340;&#31070;&#32463;&#22330;&#20316;&#20026;&#37096;&#20998;&#35266;&#27979;&#30340;&#31070;&#32463;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Generalizable Neural Fields as Partially Observed Neural Processes. (arXiv:2309.06660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#23558;&#22823;&#35268;&#27169;&#30340;&#31070;&#32463;&#34920;&#31034;&#35757;&#32451;&#35270;&#20026;&#37096;&#20998;&#35266;&#27979;&#30340;&#31070;&#32463;&#36807;&#31243;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#36807;&#31243;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22330;&#26159;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#20989;&#25968;&#26469;&#34920;&#31034;&#20449;&#21495;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#26367;&#20195;&#20256;&#32479;&#31163;&#25955;&#21521;&#37327;&#25110;&#22522;&#20110;&#32593;&#26684;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#19982;&#31163;&#25955;&#34920;&#31034;&#30456;&#27604;&#65292;&#31070;&#32463;&#34920;&#31034;&#19981;&#20165;&#33021;&#22815;&#24456;&#22909;&#22320;&#38543;&#30528;&#20998;&#36776;&#29575;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#32780;&#19988;&#26159;&#36830;&#32493;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#22810;&#27425;&#21487;&#24494;&#20998;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25105;&#20204;&#24819;&#35201;&#34920;&#31034;&#30340;&#20449;&#21495;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#24517;&#39035;&#23545;&#27599;&#20010;&#20449;&#21495;&#20248;&#21270;&#19968;&#20010;&#21333;&#29420;&#30340;&#31070;&#32463;&#22330;&#26159;&#20302;&#25928;&#30340;&#65292;&#32780;&#19988;&#19981;&#33021;&#21033;&#29992;&#20849;&#20139;&#20449;&#24687;&#25110;&#20449;&#21495;&#20043;&#38388;&#30340;&#32467;&#26500;&#12290;&#29616;&#26377;&#30340;&#27867;&#21270;&#26041;&#27861;&#23558;&#36825;&#35270;&#20026;&#20803;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#26469;&#23398;&#20064;&#21021;&#22987;&#21270;&#20540;&#65292;&#28982;&#21518;&#36890;&#36807;&#27979;&#35797;&#26102;&#20248;&#21270;&#26469;&#24494;&#35843;&#65292;&#25110;&#32773;&#23398;&#20064;&#36229;&#32593;&#32476;&#26469;&#20135;&#29983;&#31070;&#32463;&#22330;&#30340;&#26435;&#37325;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#23558;&#22823;&#35268;&#27169;&#30340;&#31070;&#32463;&#34920;&#31034;&#35757;&#32451;&#35270;&#20026;&#37096;&#20998;&#35266;&#27979;&#30340;&#31070;&#32463;&#36807;&#31243;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#36807;&#31243;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural fields, which represent signals as a function parameterized by a neural network, are a promising alternative to traditional discrete vector or grid-based representations. Compared to discrete representations, neural representations both scale well with increasing resolution, are continuous, and can be many-times differentiable. However, given a dataset of signals that we would like to represent, having to optimize a separate neural field for each signal is inefficient, and cannot capitalize on shared information or structures among signals. Existing generalization methods view this as a meta-learning problem and employ gradient-based meta-learning to learn an initialization which is then fine-tuned with test-time optimization, or learn hypernetworks to produce the weights of a neural field. We instead propose a new paradigm that views the large-scale training of neural representations as a part of a partially-observed neural process framework, and leverage neural process algorit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#25968;&#25454;&#38598;&#21644;&#23545;&#26893;&#29289;&#27169;&#22411;&#20102;&#35299;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31283;&#23450;&#24615;&#30340;&#32791;&#25955;&#22411;&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#20004;&#20010;&#26410;&#30693;&#30340;&#26893;&#29289;&#12290;</title><link>http://arxiv.org/abs/2309.06658</link><description>&lt;p&gt;
&#24102;&#31232;&#30095;&#25968;&#25454;&#38598;&#30340;&#31163;&#25955;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#25511;&#21046;&#30340;&#32791;&#25955;&#22411;&#20223;&#30495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dissipative Imitation Learning for Discrete Dynamic Output Feedback Control with Sparse Data Sets. (arXiv:2309.06658v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06658
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#25968;&#25454;&#38598;&#21644;&#23545;&#26893;&#29289;&#27169;&#22411;&#20102;&#35299;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31283;&#23450;&#24615;&#30340;&#32791;&#25955;&#22411;&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#20004;&#20010;&#26410;&#30693;&#30340;&#26893;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#20351;&#24471;&#21487;&#20197;&#20026;&#22797;&#26434;&#30446;&#26631;&#21644;&#39640;&#24230;&#19981;&#30830;&#23450;&#30340;&#26893;&#29289;&#27169;&#22411;&#21512;&#25104;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#32473;&#20223;&#30495;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#25552;&#20379;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;/&#25110;&#24050;&#30693;&#30340;&#26893;&#29289;&#27169;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#32791;&#25955;&#22411;&#20223;&#30495;&#23398;&#20064;&#30340;&#36755;&#20837;-&#36755;&#20986;&#65288;IO&#65289;&#31283;&#23450;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#31232;&#30095;&#25968;&#25454;&#38598;&#21644;&#23545;&#26893;&#29289;&#27169;&#22411;&#20102;&#35299;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31283;&#23450;&#24615;&#12290;&#20351;&#29992;&#19987;&#23478;&#25968;&#25454;&#12289;&#31895;&#31961;&#30340;IO&#26893;&#29289;&#27169;&#22411;&#21644;&#26032;&#30340;&#32422;&#26463;&#26469;&#24378;&#21046;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#20855;&#26377;&#32791;&#25955;&#24615;&#65292;&#20174;&#32780;&#23398;&#20064;&#21040;&#20102;&#19968;&#20010;&#38381;&#29615;&#31283;&#23450;&#30340;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#34429;&#28982;&#23398;&#20064;&#30446;&#26631;&#26159;&#38750;&#20984;&#30340;&#65292;&#20294;&#26412;&#25991;&#25506;&#32034;&#20102;&#36845;&#20195;&#20984;&#36807;&#20272;&#35745;&#65288;ICO&#65289;&#21644;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#20316;&#20026;&#25104;&#21151;&#23398;&#20064;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#12290;&#23558;&#36825;&#31181;&#26032;&#30340;&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#26410;&#30693;&#30340;&#26893;&#29289;&#65292;&#24182;&#19982;&#20256;&#32479;&#23398;&#20064;&#30340;&#21160;&#24577;&#36755;&#20986;&#21453;&#39304;&#25511;&#21046;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning enables the synthesis of controllers for complex objectives and highly uncertain plant models. However, methods to provide stability guarantees to imitation learned controllers often rely on large amounts of data and/or known plant models. In this paper, we explore an input-output (IO) stability approach to dissipative imitation learning, which achieves stability with sparse data sets and with little known about the plant model. A closed-loop stable dynamic output feedback controller is learned using expert data, a coarse IO plant model, and a new constraint to enforce dissipativity on the learned controller. While the learning objective is nonconvex, iterative convex overbounding (ICO) and projected gradient descent (PGD) are explored as methods to successfully learn the controller. This new imitation learning method is applied to two unknown plants and compared to traditionally learned dynamic output feedback controller and neural network controller. With little kn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23884;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26410;&#30693;&#39046;&#22495;&#30340;&#26816;&#27979;&#65292;&#24182;&#22522;&#20110;&#36882;&#24402;&#39044;&#27979;&#26500;&#24314;&#20102;&#22312;&#32447;&#30340;&#30417;&#35270;&#22120;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#39046;&#22495;&#20449;&#24687;&#30340;&#20869;&#26680;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#22238;&#24402;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.06655</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#39046;&#22495;&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#23454;&#29616;&#26410;&#30693;&#39046;&#22495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out of Distribution Detection via Domain-Informed Gaussian Process State Space Models. (arXiv:2309.06655v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23884;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26410;&#30693;&#39046;&#22495;&#30340;&#26816;&#27979;&#65292;&#24182;&#22522;&#20110;&#36882;&#24402;&#39044;&#27979;&#26500;&#24314;&#20102;&#22312;&#32447;&#30340;&#30417;&#35270;&#22120;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#39046;&#22495;&#20449;&#24687;&#30340;&#20869;&#26680;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#22238;&#24402;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#26410;&#30693;&#24773;&#26223;&#20013;&#23433;&#20840;&#23548;&#33322;&#65292;&#20934;&#30830;&#22320;&#22312;&#32447;&#26816;&#27979;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#24773;&#20917;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;GPSSM&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#30340;&#35266;&#27979;&#19982;&#27010;&#29575;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#26469;&#21306;&#20998;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#21306;&#20998;&#35757;&#32451;&#38598;&#20869;&#22806;&#30340;&#35266;&#27979;&#21462;&#20915;&#20110;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20027;&#35201;&#21463;&#21040;GPSSM&#20869;&#26680;&#25152;&#33021;&#34920;&#31034;&#30340;&#20989;&#25968;&#31867;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#29616;&#26377;&#39046;&#22495;&#30693;&#35782;&#23884;&#20837;&#20869;&#26680;&#20013;&#65292;&#24182;&#22522;&#20110;&#36882;&#24402;&#22320;&#39044;&#27979;&#26500;&#24314;&#20102;&#19968;&#20010;&#22312;&#32447;&#36816;&#34892;&#26102;&#30340;&#26410;&#30693;&#39046;&#22495;&#30417;&#35270;&#22120;&#12290;&#39046;&#22495;&#30693;&#35782;&#20551;&#35774;&#20197;&#22312;&#27169;&#25311;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#25110;&#20351;&#29992;&#21517;&#20041;&#27169;&#22411;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#24102;&#26377;&#39046;&#22495;&#20449;&#24687;&#30340;&#20869;&#26680;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22238;&#24402;&#36136;&#37327;&#65292;&#19982;&#26631;&#20934;&#20869;&#26680;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order for robots to safely navigate in unseen scenarios using learning-based methods, it is important to accurately detect out-of-training-distribution (OoD) situations online. Recently, Gaussian process state-space models (GPSSMs) have proven useful to discriminate unexpected observations by comparing them against probabilistic predictions. However, the capability for the model to correctly distinguish between in- and out-of-training distribution observations hinges on the accuracy of these predictions, primarily affected by the class of functions the GPSSM kernel can represent. In this paper, we propose (i) a novel approach to embed existing domain knowledge in the kernel and (ii) an OoD online runtime monitor, based on receding-horizon predictions. Domain knowledge is assumed given as a dataset collected either in simulation or using a nominal model. Numerical results show that the informed kernel yields better regression quality with smaller datasets, as compared to standard ker
&lt;/p&gt;</description></item><item><title>ConR&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#20854;&#22810;&#25968;&#37051;&#23621;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06651</link><description>&lt;p&gt;
ConR: &#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#30340;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConR: Contrastive Regularizer for Deep Imbalanced Regression. (arXiv:2309.06651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06651
&lt;/p&gt;
&lt;p&gt;
ConR&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#20854;&#22810;&#25968;&#37051;&#23621;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#20998;&#24067;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#24456;&#24120;&#35265;&#12290;&#23427;&#20204;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20986;&#20102;&#32422;&#26463;&#65292;&#20197;&#34920;&#31034;&#23569;&#25968;&#31867;&#21035;&#26631;&#31614;&#24182;&#36991;&#20813;&#23545;&#22810;&#25968;&#31867;&#21035;&#30340;&#20559;&#35265;&#12290;&#22823;&#37327;&#30340;&#19981;&#24179;&#34913;&#26041;&#27861;&#22788;&#29702;&#20102;&#20998;&#31867;&#26631;&#31614;&#31354;&#38388;&#65292;&#20294;&#22312;&#36830;&#32493;&#26631;&#31614;&#31354;&#38388;&#30340;&#22238;&#24402;&#38382;&#39064;&#19978;&#26410;&#33021;&#26377;&#25928;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;&#36830;&#32493;&#26631;&#31614;&#20043;&#38388;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20851;&#32852;&#20026;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26377;&#25928;&#24314;&#27169;&#20851;&#31995;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConR&#65292;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#23427;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#23427;&#20204;&#30340;&#22810;&#25968;&#37051;&#23621;&#20013;&#12290;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#30456;&#20284;&#24615;&#20316;&#20026;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#25351;&#31034;&#22120;&#65292;ConR&#21306;&#20998;&#20102;&#26631;&#31614;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23545;&#36825;&#20123;&#19981;&#19968;&#33268;&#26045;&#21152;&#24809;&#32602;&#12290;ConR&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#31574;&#30053;&#20851;&#27880;&#26631;&#31614;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to regression problems where the label space is continuous. Conversely, local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. Serving the similarities of the predictions as an indicator of feature similarities, ConR discerns the dissagreements between the label space and feature space and imposes a penalty on these disagreements. ConR minds the continuous nature of label space with two main strategi
&lt;/p&gt;</description></item><item><title>Bregman GNNs&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;Bregman&#36317;&#31163;&#27010;&#24565;&#21551;&#21457;&#30340;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#24182;&#22312;&#21516;&#26500;&#22270;&#21644;&#24322;&#26500;&#22270;&#20013;&#24615;&#33021;&#20248;&#20110;&#21407;&#22987;&#30340;GNNs&#12290;</title><link>http://arxiv.org/abs/2309.06645</link><description>&lt;p&gt;
Bregman&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bregman Graph Neural Network. (arXiv:2309.06645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06645
&lt;/p&gt;
&lt;p&gt;
Bregman GNNs&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;Bregman&#36317;&#31163;&#27010;&#24565;&#21551;&#21457;&#30340;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#24182;&#22312;&#21516;&#26500;&#22270;&#21644;&#24322;&#26500;&#22270;&#20013;&#24615;&#33021;&#20248;&#20110;&#21407;&#22987;&#30340;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20247;&#22810;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;GNN&#26550;&#26500;&#24314;&#27169;&#20026;&#20855;&#26377;&#24179;&#28369;&#20551;&#35774;&#30340;&#20248;&#21270;&#38382;&#39064;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;GNNs&#24341;&#36215;&#30340;&#24179;&#28369;&#25928;&#26524;&#24448;&#24448;&#20250;&#20351;&#36830;&#25509;&#33410;&#28857;&#30340;&#34920;&#31034;&#21644;&#26631;&#31614;&#36807;&#20110;&#21516;&#36136;&#21270;&#65292;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#21644;&#38169;&#35823;&#20998;&#31867;&#31561;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;Bregman&#36317;&#31163;&#27010;&#24565;&#21551;&#21457;&#30340;GNNs&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#24212;&#25552;&#20986;&#30340;GNNs&#23618;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#31867;&#20284;&#8220;&#36339;&#36291;&#36830;&#25509;&#8221;&#30340;&#26426;&#21046;&#26377;&#25928;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#22312;&#21516;&#26500;&#22270;&#21644;&#24322;&#26500;&#22270;&#20013;&#65292;Bregman&#22686;&#24378;&#30340;GNNs&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#21407;&#22987;&#30340;GNNs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#26174;&#31034;&#20986;&#65292;&#21363;&#20351;&#23618;&#25968;&#36739;&#22810;&#65292;Bregman GNNs&#20063;&#33021;&#20135;&#29983;&#26356;&#31283;&#20581;&#30340;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous recent research on graph neural networks (GNNs) has focused on formulating GNN architectures as an optimization problem with the smoothness assumption. However, in node classification tasks, the smoothing effect induced by GNNs tends to assimilate representations and over-homogenize labels of connected nodes, leading to adverse effects such as over-smoothing and misclassification. In this paper, we propose a novel bilevel optimization framework for GNNs inspired by the notion of Bregman distance. We demonstrate that the GNN layer proposed accordingly can effectively mitigate the over-smoothing issue by introducing a mechanism reminiscent of the "skip connection". We validate our theoretical results through comprehensive empirical studies in which Bregman-enhanced GNNs outperform their original counterparts in both homophilic and heterophilic graphs. Furthermore, our experiments also show that Bregman GNNs can produce more robust learning accuracy even when the number of layers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26679;&#26412;&#33258;&#36866;&#24212;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#27714;&#35299;&#22120;&#22312;&#36866;&#24212;&#37325;&#24314;&#20219;&#21153;&#22256;&#38590;&#31243;&#24230;&#12289;&#25512;&#29702;&#26102;&#38388;&#21644;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.06642</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25193;&#25955;&#65306;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26679;&#26412;&#33258;&#36866;&#24212;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models. (arXiv:2309.06642v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26679;&#26412;&#33258;&#36866;&#24212;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#27714;&#35299;&#22120;&#22312;&#36866;&#24212;&#37325;&#24314;&#20219;&#21153;&#22256;&#38590;&#31243;&#24230;&#12289;&#25512;&#29702;&#26102;&#38388;&#21644;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#38382;&#39064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#20854;&#30446;&#26631;&#26159;&#20174;&#22024;&#26434;&#21644;&#21487;&#33021;&#26159;&#65288;&#38750;&#65289;&#32447;&#24615;&#30340;&#35266;&#27979;&#20013;&#24674;&#22797;&#20986;&#19968;&#20010;&#24178;&#20928;&#30340;&#20449;&#21495;&#12290;&#37325;&#24314;&#38382;&#39064;&#30340;&#22256;&#38590;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#65292;&#22914;&#21407;&#22987;&#20449;&#21495;&#30340;&#32467;&#26500;&#65292;&#36864;&#21270;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#37325;&#24314;&#27169;&#22411;&#30340;&#38544;&#24335;&#20559;&#24046;&#20197;&#21450;&#19978;&#36848;&#22240;&#32032;&#20043;&#38388;&#22797;&#26434;&#30340;&#20132;&#20114;&#12290;&#36825;&#23548;&#33268;&#37325;&#24314;&#20219;&#21153;&#30340;&#22256;&#38590;&#22312;&#26679;&#26412;&#38388;&#23384;&#22312;&#33258;&#28982;&#30340;&#21464;&#21270;&#65292;&#36825;&#22312;&#29616;&#20195;&#25216;&#26415;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#21453;&#38382;&#39064;&#27714;&#35299;&#22120;&#22312;&#21508;&#31181;&#37325;&#24314;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32570;&#28857;&#26159;&#35745;&#31639;&#22797;&#26434;&#65292;&#38590;&#20197;&#23454;&#26045;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#22823;&#22810;&#25968;&#29616;&#26377;&#27714;&#35299;&#22120;&#32570;&#20047;&#26681;&#25454;&#37325;&#24314;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#33258;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#38271;&#65292;&#24615;&#33021;&#19981;&#20339;&#19988;&#36164;&#28304;&#20998;&#37197;&#28010;&#36153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#36866;&#24212;&#25193;&#25955;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems arise in a multitude of applications, where the goal is to recover a clean signal from noisy and possibly (non)linear observations. The difficulty of a reconstruction problem depends on multiple factors, such as the structure of the ground truth signal, the severity of the degradation, the implicit bias of the reconstruction model and the complex interactions between the above factors. This results in natural sample-by-sample variation in the difficulty of a reconstruction task, which is often overlooked by contemporary techniques. Recently, diffusion-based inverse problem solvers have established new state-of-the-art in various reconstruction tasks. However, they have the drawback of being computationally prohibitive. Our key observation in this paper is that most existing solvers lack the ability to adapt their compute power to the difficulty of the reconstruction task, resulting in long inference times, subpar performance and wasteful resource allocation. We propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#25968;&#25454;&#20013;&#24515;(QDC)&#30340;&#27010;&#24565;&#65292;&#23427;&#26159;&#29616;&#26377;&#32463;&#20856;&#25968;&#25454;&#20013;&#24515;&#30340;&#37327;&#23376;&#29256;&#26412;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;(QRAM)&#21644;&#37327;&#23376;&#32593;&#32476;&#65292;QDC&#21487;&#20197;&#25552;&#20379;&#23458;&#25143;&#22312;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#31934;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#23545;&#20110;&#37327;&#23376;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20256;&#24863;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#30828;&#20214;&#23454;&#29616;&#21644;&#29305;&#23450;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#31185;&#23398;&#21644;&#21830;&#19994;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;QDC&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#34892;&#19994;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.06641</link><description>&lt;p&gt;
&#37327;&#23376;&#25968;&#25454;&#20013;&#24515;: &#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Quantum Data Center: Perspectives. (arXiv:2309.06641v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#25968;&#25454;&#20013;&#24515;(QDC)&#30340;&#27010;&#24565;&#65292;&#23427;&#26159;&#29616;&#26377;&#32463;&#20856;&#25968;&#25454;&#20013;&#24515;&#30340;&#37327;&#23376;&#29256;&#26412;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;(QRAM)&#21644;&#37327;&#23376;&#32593;&#32476;&#65292;QDC&#21487;&#20197;&#25552;&#20379;&#23458;&#25143;&#22312;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#31934;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#23545;&#20110;&#37327;&#23376;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20256;&#24863;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#30828;&#20214;&#23454;&#29616;&#21644;&#29305;&#23450;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#31185;&#23398;&#21644;&#21830;&#19994;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;QDC&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#34892;&#19994;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#29256;&#26412;&#30340;&#25968;&#25454;&#20013;&#24515;&#21487;&#33021;&#22312;&#37327;&#23376;&#26102;&#20195;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#23376;&#25968;&#25454;&#20013;&#24515;(QDC)&#65292;&#36825;&#26159;&#29616;&#26377;&#32463;&#20856;&#25968;&#25454;&#20013;&#24515;&#30340;&#37327;&#23376;&#29256;&#26412;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;(QRAM)&#21644;&#37327;&#23376;&#32593;&#32476;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#35748;&#20026;QDC&#23558;&#20026;&#23458;&#25143;&#25552;&#20379;&#22312;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#31934;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#22909;&#22788;&#65292;&#24182;&#23545;&#37327;&#23376;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20256;&#24863;&#26041;&#38754;&#20855;&#26377;&#24110;&#21161;&#12290;&#36890;&#36807;&#30828;&#20214;&#23454;&#29616;&#21644;&#21487;&#33021;&#30340;&#29305;&#23450;&#24212;&#29992;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#19968;&#26032;&#39062;&#30740;&#31350;&#26041;&#21521;&#30340;&#28508;&#22312;&#31185;&#23398;&#21644;&#21830;&#19994;&#26426;&#20250;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;QDC&#22312;&#21830;&#19994;&#21644;&#31185;&#23398;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#34892;&#19994;&#12290;
&lt;/p&gt;
&lt;p&gt;
A quantum version of data centers might be significant in the quantum era. In this paper, we introduce Quantum Data Center (QDC), a quantum version of existing classical data centers, with a specific emphasis on combining Quantum Random Access Memory (QRAM) and quantum networks. We argue that QDC will provide significant benefits to customers in terms of efficiency, security, and precision, and will be helpful for quantum computing, communication, and sensing. We investigate potential scientific and business opportunities along this novel research direction through hardware realization and possible specific applications. We show the possible impacts of QDCs in business and science, especially the machine learning and big data industries.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#21644;&#32858;&#31867;&#31639;&#27861;&#30340;&#20248;&#21270;Mapper&#22270;&#35206;&#30422;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#35206;&#30422;&#36873;&#25321;&#29983;&#25104;&#20102;&#20445;&#30041;&#25968;&#25454;&#38598;&#26412;&#36136;&#30340;Mapper&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.06634</link><description>&lt;p&gt;
$G$-Mapper&#65306;&#23398;&#20064;Mapper&#26500;&#36896;&#20013;&#30340;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
$G$-Mapper: Learning a Cover in the Mapper Construction. (arXiv:2309.06634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#21644;&#32858;&#31867;&#31639;&#27861;&#30340;&#20248;&#21270;Mapper&#22270;&#35206;&#30422;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#35206;&#30422;&#36873;&#25321;&#29983;&#25104;&#20102;&#20445;&#30041;&#25968;&#25454;&#38598;&#26412;&#36136;&#30340;Mapper&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mapper&#31639;&#27861;&#26159;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#20013;&#19968;&#31181;&#21453;&#26144;&#32473;&#23450;&#25968;&#25454;&#38598;&#32467;&#26500;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#12290;Mapper&#31639;&#27861;&#38656;&#35201;&#35843;&#25972;&#22810;&#20010;&#21442;&#25968;&#20197;&#29983;&#25104;&#19968;&#20010;"&#22909;&#30475;&#30340;"Mapper&#22270;&#12290;&#35813;&#35770;&#25991;&#20851;&#27880;&#20110;&#36873;&#25321;&#35206;&#30422;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26681;&#25454;&#27491;&#24577;&#24615;&#30340;&#32479;&#35745;&#26816;&#39564;&#21453;&#22797;&#20998;&#21106;&#35206;&#30422;&#26469;&#20248;&#21270;Mapper&#22270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;$G$-means&#32858;&#31867;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#36827;&#34892;Anderson-Darling&#26816;&#39564;&#26469;&#23547;&#25214;$k$-means&#20013;&#26368;&#20339;&#30340;&#31751;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#21106;&#36807;&#31243;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#26681;&#25454;&#32473;&#23450;&#25968;&#25454;&#30340;&#20998;&#24067;&#31934;&#24515;&#36873;&#25321;&#35206;&#30422;&#12290;&#23545;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#29983;&#25104;&#30340;&#35206;&#30422;&#20351;Mapper&#22270;&#20445;&#30041;&#20102;&#25968;&#25454;&#38598;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. The Mapper algorithm requires tuning several parameters in order to generate a "nice" Mapper graph. The paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on $G$-means clustering which searches for the optimal number of clusters in $k$-means by conducting iteratively the Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model in order to choose carefully the cover based on the distribution of a given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24555;&#36895;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#30693;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33322;&#31354;&#33322;&#22825;&#24037;&#31243;&#31995;&#32479;&#35774;&#35745;&#25506;&#32034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#35757;&#32451;&#38598;&#25104;&#27169;&#22411;&#30340;&#25104;&#26412;&#21644;&#35745;&#31639;&#25361;&#25112;&#65292;&#20026;&#33258;&#36866;&#24212;&#23398;&#20064;&#25552;&#20379;&#20102;&#37325;&#35201;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.06628</link><description>&lt;p&gt;
&#24555;&#36895;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#35748;&#30693;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#29992;&#20110;&#33258;&#36866;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Epistemic Modeling Uncertainty of Rapid Neural Network Ensembles for Adaptive Learning. (arXiv:2309.06628v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24555;&#36895;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#30693;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33322;&#31354;&#33322;&#22825;&#24037;&#31243;&#31995;&#32479;&#35774;&#35745;&#25506;&#32034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#35757;&#32451;&#38598;&#25104;&#27169;&#22411;&#30340;&#25104;&#26412;&#21644;&#35745;&#31639;&#25361;&#25112;&#65292;&#20026;&#33258;&#36866;&#24212;&#23398;&#20064;&#25552;&#20379;&#20102;&#37325;&#35201;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#35299;&#26512;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#31181;&#30495;&#23454;&#24615;&#25968;&#25454;&#28304;&#36827;&#34892;&#33322;&#31354;&#33322;&#22825;&#24037;&#31243;&#31995;&#32479;&#39640;&#25928;&#35774;&#35745;&#25506;&#32034;&#30340;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#35757;&#32451;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23454;&#29616;&#12290;&#27169;&#22411;&#23454;&#29616;&#30340;&#38598;&#25104;&#29992;&#20110;&#35780;&#20272;&#30001;&#20110;&#32570;&#20047;&#35757;&#32451;&#26679;&#26412;&#32780;&#24341;&#36215;&#30340;&#35748;&#30693;&#24314;&#27169;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26159;&#33322;&#31354;&#33322;&#22825;&#31995;&#32479;&#35774;&#35745;&#25506;&#32034;&#20013;&#25104;&#21151;&#30446;&#26631;&#23548;&#21521;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#38598;&#25104;&#27169;&#22411;&#30340;&#25104;&#26412;&#36890;&#24120;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#65292;&#24182;&#19988;&#22312;&#33258;&#36866;&#24212;&#23398;&#20064;&#26399;&#38388;&#65292;&#29305;&#21035;&#26159;&#24403;&#27169;&#22411;&#19981;&#26159;&#24182;&#34892;&#35757;&#32451;&#26102;&#65292;&#20250;&#24102;&#26469;&#35745;&#31639;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24555;&#36895;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#30340;&#26032;&#22411;&#23884;&#20837;&#35299;&#26512;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20165;&#20248;&#21270;&#25152;&#26377;&#32593;&#32476;&#23618;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emulator embedded neural networks, which are a type of physics informed neural network, leverage multi-fidelity data sources for efficient design exploration of aerospace engineering systems. Multiple realizations of the neural network models are trained with different random initializations. The ensemble of model realizations is used to assess epistemic modeling uncertainty caused due to lack of training samples. This uncertainty estimation is crucial information for successful goal-oriented adaptive learning in an aerospace system design exploration. However, the costs of training the ensemble models often become prohibitive and pose a computational challenge, especially when the models are not trained in parallel during adaptive learning. In this work, a new type of emulator embedded neural network is presented using the rapid neural network paradigm. Unlike the conventional neural network training that optimizes the weights and biases of all the network layers by using gradient-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#26694;&#26550;&#26469;&#36880;&#27493;&#23454;&#29616;&#23545;&#22810;&#20010;&#25935;&#24863;&#29305;&#24449;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#36793;&#38469;Wasserstein&#37325;&#24515;&#25193;&#23637;&#20102;&#26631;&#20934;&#30340;&#24378;&#20154;&#21475;&#24179;&#31561;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#38381;&#24335;&#35299;&#26469;&#35299;&#37322;&#25935;&#24863;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06627</link><description>&lt;p&gt;
&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#39034;&#24207;&#20844;&#24179;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Sequentially Fair Mechanism for Multiple Sensitive Attributes. (arXiv:2309.06627v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#26694;&#26550;&#26469;&#36880;&#27493;&#23454;&#29616;&#23545;&#22810;&#20010;&#25935;&#24863;&#29305;&#24449;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#36793;&#38469;Wasserstein&#37325;&#24515;&#25193;&#23637;&#20102;&#26631;&#20934;&#30340;&#24378;&#20154;&#21475;&#24179;&#31561;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#38381;&#24335;&#35299;&#26469;&#35299;&#37322;&#25935;&#24863;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#26631;&#20934;&#29992;&#20363;&#20013;&#65292;&#30446;&#26631;&#26159;&#28040;&#38500;&#25935;&#24863;&#21464;&#37327;&#21644;&#30456;&#24212;&#20998;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#24037;&#20855;&#21644;&#23450;&#20041;&#30340;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#26694;&#26550;&#65292;&#21487;&#20197;&#36880;&#27493;&#23454;&#29616;&#23545;&#19968;&#32452;&#25935;&#24863;&#29305;&#24449;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22810;&#36793;&#38469;Wasserstein&#37325;&#24515;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#23558;&#26631;&#20934;&#30340;&#24378;&#20154;&#21475;&#24179;&#31561;&#27010;&#24565;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#25935;&#24863;&#29305;&#24449;&#30340;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#20026;&#26368;&#20248;&#30340;&#39034;&#24207;&#20844;&#24179;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#38381;&#24335;&#35299;&#65292;&#21487;&#20197;&#28165;&#26970;&#22320;&#35299;&#37322;&#25935;&#24863;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#26080;&#32541;&#25193;&#23637;&#21040;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the standard use case of Algorithmic Fairness, the goal is to eliminate the relationship between a sensitive variable and a corresponding score. Throughout recent years, the scientific community has developed a host of definitions and tools to solve this task, which work well in many practical applications. However, the applicability and effectivity of these tools and definitions becomes less straightfoward in the case of multiple sensitive attributes. To tackle this issue, we propose a sequential framework, which allows to progressively achieve fairness across a set of sensitive features. We accomplish this by leveraging multi-marginal Wasserstein barycenters, which extends the standard notion of Strong Demographic Parity to the case with multiple sensitive characteristics. This method also provides a closed-form solution for the optimal, sequentially fair predictor, permitting a clear interpretation of inter-sensitive feature correlations. Our approach seamlessly extends to approx
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23567;&#22411;&#36816;&#34892;&#26102;&#20462;&#25913;&#24341;&#20837;&#21322;&#32467;&#26500;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31232;&#30095;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#20445;&#25345;&#31934;&#24230;&#19979;&#38477;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#22788;&#29702;&#21644;&#25512;&#26029;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.06626</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#32467;&#26500;&#28608;&#27963;&#31232;&#30095;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity. (arXiv:2309.06626v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06626
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23567;&#22411;&#36816;&#34892;&#26102;&#20462;&#25913;&#24341;&#20837;&#21322;&#32467;&#26500;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31232;&#30095;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#20445;&#25345;&#31934;&#24230;&#19979;&#38477;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#22788;&#29702;&#21644;&#25512;&#26029;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#39640;&#25928;&#22788;&#29702;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#38656;&#27714;&#26159;&#38480;&#21046;&#20854;&#37096;&#32626;&#30340;&#37325;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#21033;&#29992;&#32593;&#32476;&#29305;&#24449;&#22270;&#20013;&#30340;&#31232;&#30095;&#24615;&#26159;&#20943;&#23569;&#25512;&#26029;&#24310;&#36831;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#24050;&#30693;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#19982;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30456;&#27604;&#23545;&#31934;&#24230;&#19979;&#38477;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#21069;&#32773;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#25512;&#26029;&#24341;&#25806;&#26356;&#25913;&#20197;&#33719;&#24471;&#24310;&#36831;&#20248;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23567;&#22411;&#36816;&#34892;&#26102;&#20462;&#25913;&#24341;&#20837;&#21322;&#32467;&#26500;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22312;&#25512;&#26029;&#26102;&#33719;&#24471;&#39640;&#21152;&#36895;&#24230;&#27700;&#24179;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31232;&#30095;&#35757;&#32451;&#36807;&#31243;&#65292;&#21516;&#26102;&#22312;&#35745;&#31639;&#24191;&#20041;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#26102;&#32771;&#34385;&#28608;&#27963;&#30340;&#26368;&#32456;&#20301;&#32622;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#27169;&#22411;&#23545;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#31934;&#24230;&#19979;&#38477;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;1.25&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for efficient processing of deep neural networks (DNNs) on embedded devices is a significant challenge limiting their deployment. Exploiting sparsity in the network's feature maps is one of the ways to reduce its inference latency. It is known that unstructured sparsity results in lower accuracy degradation with respect to structured sparsity but the former needs extensive inference engine changes to get latency benefits. To tackle this challenge, we propose a solution to induce semi-structured activation sparsity exploitable through minor runtime modifications. To attain high speedup levels at inference time, we design a sparse training procedure with awareness of the final position of the activations while computing the General Matrix Multiplication (GEMM). We extensively evaluate the proposed solution across various models for image classification and object detection tasks. Remarkably, our approach yields a speed improvement of $1.25 \times$ with a minimal accuracy drop 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19982;Schr\"{o}dinger&#31995;&#32479;&#30340;&#25910;&#25947;&#24615;&#30456;&#20851;&#30340;&#25910;&#32553;&#31995;&#25968;&#36827;&#34892;&#20102;&#20808;&#39564;&#20272;&#35745;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#20960;&#20309;&#21644;&#25511;&#21046;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#25351;&#20986;&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#32456;&#28857;&#25903;&#25345;&#38598;&#21487;&#20197;&#25913;&#21892;&#32447;&#24615;SBPs&#30340;&#26368;&#22351;&#24773;&#20917;&#25910;&#32553;&#31995;&#25968;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2309.06622</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#32447;&#24615;&#31995;&#32479;Schr\"odinger&#26725;&#25910;&#32553;&#31995;&#25968;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Contraction Coefficient of the Schr\"odinger Bridge for Stochastic Linear Systems. (arXiv:2309.06622v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19982;Schr\"{o}dinger&#31995;&#32479;&#30340;&#25910;&#25947;&#24615;&#30456;&#20851;&#30340;&#25910;&#32553;&#31995;&#25968;&#36827;&#34892;&#20102;&#20808;&#39564;&#20272;&#35745;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#20960;&#20309;&#21644;&#25511;&#21046;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#25351;&#20986;&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#32456;&#28857;&#25903;&#25345;&#38598;&#21487;&#20197;&#25913;&#21892;&#32447;&#24615;SBPs&#30340;&#26368;&#22351;&#24773;&#20917;&#25910;&#32553;&#31995;&#25968;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Schr\"{o}dinger&#26725;&#26159;&#19968;&#20010;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30446;&#30340;&#26159;&#22312;&#25511;&#21046;&#25193;&#25955;&#21644;&#25130;&#27490;&#32422;&#26463;&#26465;&#20214;&#19979;&#23558;&#32473;&#23450;&#30340;&#21021;&#22987;&#29366;&#24577;&#23494;&#24230;&#36716;&#21464;&#20026;&#21478;&#19968;&#20010;&#29366;&#24577;&#23494;&#24230;&#12290;&#22312;&#32463;&#20856;&#21644;&#32447;&#24615;&#31995;&#32479;&#35774;&#32622;&#20013;&#65292;&#35299;&#20915;Schr\"{o}dinger&#26725;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#26159;&#36890;&#36807;&#25910;&#32553;&#19981;&#21160;&#28857;&#36882;&#24402;&#36827;&#34892;&#25968;&#20540;&#35745;&#31639;&#12290;&#36825;&#20123;&#36882;&#24402;&#21487;&#20197;&#30475;&#20316;&#26159;&#33879;&#21517;&#30340;Sinkhorn&#36845;&#20195;&#30340;&#21160;&#24577;&#29256;&#26412;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23427;&#20204;&#35299;&#20915;&#20102;&#25152;&#35859;&#30340;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#30340;Schr\"{o}dinger&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;Schr\"{o}dinger&#31995;&#32479;&#30340;&#25910;&#25947;&#24615;&#30456;&#20851;&#30340;&#25910;&#32553;&#31995;&#25968;&#30340;&#20808;&#39564;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#32553;&#31995;&#25968;&#30340;&#26032;&#20960;&#20309;&#21644;&#25511;&#21046;&#29702;&#35770;&#35299;&#37322;&#12290;&#22522;&#20110;&#36825;&#20123;&#26032;&#21457;&#29616;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#25351;&#20986;&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#32456;&#28857;&#25903;&#25345;&#38598;&#21487;&#20197;&#25913;&#21892;&#32447;&#24615;SBPs&#30340;&#26368;&#22351;&#24773;&#20917;&#25910;&#32553;&#31995;&#25968;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Schr\"{o}dinger bridge is a stochastic optimal control problem to steer a given initial state density to another, subject to controlled diffusion and deadline constraints. A popular method to numerically solve the Schr\"{o}dinger bridge problems, in both classical and in the linear system settings, is via contractive fixed point recursions. These recursions can be seen as dynamic versions of the well-known Sinkhorn iterations, and under mild assumptions, they solve the so-called Schr\"{o}dinger systems with guaranteed linear convergence. In this work, we study a priori estimates for the contraction coefficients associated with the convergence of respective Schr\"{o}dinger systems. We provide new geometric and control-theoretic interpretations for the same. Building on these newfound interpretations, we point out the possibility of improved computation for the worst-case contraction coefficients of linear SBPs by preconditioning the endpoint support sets.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;RT-LM&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#26102;&#25512;&#29702;&#36827;&#34892;&#20248;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#36164;&#28304;&#31649;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#29702;&#35299;&#12289;&#37327;&#21270;&#21644;&#20248;&#21270;&#30001;&#20110;&#35821;&#35328;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#23548;&#33268;&#30340;&#25512;&#29702;&#24310;&#36831;&#24615;&#33021;&#21464;&#21270;&#65292;&#20197;&#25552;&#39640;LMs&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06619</link><description>&lt;p&gt;
RT-LM: &#38754;&#21521;&#23454;&#26102;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#36164;&#28304;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models. (arXiv:2309.06619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06619
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;RT-LM&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#26102;&#25512;&#29702;&#36827;&#34892;&#20248;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#36164;&#28304;&#31649;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#29702;&#35299;&#12289;&#37327;&#21270;&#21644;&#20248;&#21270;&#30001;&#20110;&#35821;&#35328;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#23548;&#33268;&#30340;&#25512;&#29702;&#24310;&#36831;&#24615;&#33021;&#21464;&#21270;&#65292;&#20197;&#25552;&#39640;LMs&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#36827;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#22312;&#23545;&#35805;AI&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#26497;&#39640;&#19988;&#25512;&#29702;&#24310;&#36831;&#26080;&#27861;&#39044;&#27979;&#65292;&#36825;&#20123;LMs&#22312;&#21508;&#31181;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#38754;&#20020;&#25361;&#25112;&#12290;&#30001;&#20110;&#35821;&#35328;&#30340;&#26412;&#36136;&#23548;&#33268;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#21457;&#30340;&#19981;&#21516;&#25512;&#29702;&#24310;&#36831;&#21487;&#33021;&#23548;&#33268;&#35745;&#31639;&#25928;&#29575;&#19981;&#39640;&#65292;&#20174;&#32780;&#38477;&#20302;LMs&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#27969;&#37327;&#30340;&#24037;&#20316;&#36127;&#36733;&#19979;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#28304;&#30340;&#24102;&#23485;&#38750;&#24120;&#24191;&#27867;&#65292;&#32473;&#24310;&#36831;&#30340;&#39044;&#27979;&#21644;&#30001;&#27492;&#20135;&#29983;&#30340;&#25928;&#26524;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#20102;&#35299;&#21644;&#20943;&#36731;&#19981;&#30830;&#23450;&#24615;&#23545;&#23454;&#26102;&#21709;&#24212;&#38656;&#27714;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#39318;&#20808;&#35201;&#29702;&#35299;&#12289;&#37327;&#21270;&#21644;&#20248;&#21270;LMs&#20013;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#24310;&#36831;&#24615;&#33021;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RT-LM
&lt;/p&gt;
&lt;p&gt;
Recent advancements in language models (LMs) have gained substantial attentions on their capability to generate human-like responses. Though exhibiting a promising future for various applications such as conversation AI, these LMs face deployment challenges on various devices due to their extreme computational cost and unpredictable inference latency. Such varied inference latency, identified as a consequence of uncertainty intrinsic to the nature of language, can lead to computational inefficiency and degrade the overall performance of LMs, especially under high-traffic workloads. Unfortunately, the bandwidth of these uncertainty sources is extensive, complicating the prediction of latency and the effects emanating from such uncertainties. To understand and mitigate the impact of uncertainty on real-time response-demanding systems, we take the first step to comprehend, quantify and optimize these uncertainty-induced latency performance variations in LMs. Specifically, we present RT-LM
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#20102;Cu-Cr&#22797;&#21512;&#26448;&#26009;&#30340;&#32435;&#31859;&#21387;&#30165;&#25968;&#25454;&#65292;&#25512;&#26029;&#20102;&#26448;&#26009;&#30340;&#24494;&#35266;&#32467;&#26500;&#32454;&#33410;&#21644;&#21147;&#23398;&#24615;&#36136;&#65292;&#24182;&#24341;&#20837;&#20102;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#25968;&#37327;&#21644;&#21487;&#38752;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06613</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25512;&#26029;&#22797;&#26434;&#26448;&#26009;&#30340;&#24494;&#35266;&#32467;&#26500;&#32454;&#33410;&#30340;&#32435;&#31859;&#21387;&#30165;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning of Nanoindentation Data to Infer Microstructural Details of Complex Materials. (arXiv:2309.06613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#20102;Cu-Cr&#22797;&#21512;&#26448;&#26009;&#30340;&#32435;&#31859;&#21387;&#30165;&#25968;&#25454;&#65292;&#25512;&#26029;&#20102;&#26448;&#26009;&#30340;&#24494;&#35266;&#32467;&#26500;&#32454;&#33410;&#21644;&#21147;&#23398;&#24615;&#36136;&#65292;&#24182;&#24341;&#20837;&#20102;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#25968;&#37327;&#21644;&#21487;&#38752;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32435;&#31859;&#21387;&#30165;&#30740;&#31350;&#20102;Cu-Cr&#22797;&#21512;&#26448;&#26009;&#12290;&#26679;&#21697;&#19978;&#25918;&#32622;&#20102;&#22823;&#38754;&#31215;&#30340;&#21387;&#30165;&#25968;&#32452;&#65292;&#20135;&#29983;&#20102;&#21253;&#21547;&#25968;&#30334;&#20010;&#22312;&#21387;&#30165;&#28145;&#24230;&#21464;&#21270;&#26102;Young&#27169;&#37327;&#21644;&#30828;&#24230;&#30340;&#27979;&#37327;&#20540;&#30340;&#25968;&#25454;&#38598;&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#65292;&#24110;&#21161;&#30830;&#23450;&#20102;&#8220;&#21147;&#23398;&#30456;&#8221;&#30340;&#25968;&#37327;&#21450;&#20854;&#30456;&#24212;&#30340;&#21147;&#23398;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65292;&#25512;&#26029;&#25968;&#25454;&#25968;&#37327;&#26159;&#21542;&#36275;&#22815;&#65292;&#24182;&#24314;&#35758;&#21487;&#38752;&#39044;&#27979;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327; - &#36825;&#26159;&#26448;&#26009;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#32463;&#24120;&#36935;&#21040;&#20294;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, Cu-Cr composites were studied by nanoindentation. Arrays of indents were placed over large areas of the samples resulting in datasets consisting of several hundred measurements of Young's modulus and hardness at varying indentation depths. The unsupervised learning technique, Gaussian mixture model, was employed to analyze the data, which helped to determine the number of "mechanical phases" and the respective mechanical properties. Additionally, a cross-validation approach was introduced to infer whether the data quantity was adequate and to suggest the amount of data required for reliable predictions -- one of the often encountered but difficult to resolve issues in machine learning of materials science problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30828;&#20214;&#24863;&#30693;&#30340;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;Harmonic-NAS&#65292;&#36890;&#36807;&#20004;&#23618;&#20248;&#21270;&#23454;&#29616;&#20102;&#21333;&#27169;&#24577;&#39592;&#24178;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#30340;&#32852;&#21512;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.06612</link><description>&lt;p&gt;
&#22522;&#20110;&#30828;&#20214;&#24863;&#30693;&#30340;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(Harmonic-NAS)
&lt;/p&gt;
&lt;p&gt;
Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices. (arXiv:2309.06612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30828;&#20214;&#24863;&#30693;&#30340;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;Harmonic-NAS&#65292;&#36890;&#36807;&#20004;&#23618;&#20248;&#21270;&#23454;&#29616;&#20102;&#21333;&#27169;&#24577;&#39592;&#24178;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#30340;&#32852;&#21512;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65288;MM-NN&#65289;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#36825;&#24402;&#21151;&#20110;&#23427;&#20204;&#26377;&#25928;&#22788;&#29702;&#21644;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22312;MM-NN&#20013;&#65292;&#20351;&#29992;&#36866;&#24403;&#30340;&#21333;&#27169;&#24577;&#39592;&#24178;&#21644;&#29305;&#23450;&#30340;&#34701;&#21512;&#32593;&#32476;&#20174;&#22810;&#20010;&#27169;&#24577;&#25552;&#21462;&#21644;&#34701;&#21512;&#29305;&#24449;&#12290;&#23613;&#31649;&#36825;&#26377;&#21161;&#20110;&#22686;&#24378;&#22810;&#27169;&#24577;&#20449;&#24687;&#34920;&#36798;&#65292;&#20294;&#35774;&#35745;&#27492;&#31867;&#32593;&#32476;&#26159;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#12290;&#23427;&#38656;&#35201;&#35843;&#25972;&#21333;&#27169;&#24577;&#39592;&#24178;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#36873;&#25321;&#34701;&#21512;&#28857;&#65292;&#24182;&#36873;&#25321;&#34701;&#21512;&#30340;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#22810;&#27169;&#24615;&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#25104;&#20026;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#19968;&#31181;&#23574;&#31471;&#36873;&#25321;&#65292;&#20854;&#20013;&#25512;&#26029;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#26159;&#38500;&#20934;&#30830;&#24615;&#22806;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Harmonic-NAS&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#20855;&#26377;&#30828;&#20214;&#24863;&#30693;&#30340;&#21333;&#27169;&#24577;&#39592;&#24178;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#30340;&#32852;&#21512;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of interest surrounding Multimodal Neural Networks (MM-NN) is attributed to their ability to effectively process and integrate information from diverse data sources. In MM-NN, features are extracted and fused from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this helps strengthen the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Furthermore, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy. In this paper, we propose Harmonic-NAS, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. Harmonic-NAS involves a two-tier optimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#33258;&#21160;&#21270;&#21644;&#21327;&#21516;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06604</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#36164;&#28304;&#19978;&#30340;&#28151;&#21512;&#31639;&#27861;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;: &#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#20195;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach. (arXiv:2309.06604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#33258;&#21160;&#21270;&#21644;&#21327;&#21516;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26159;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#20851;&#38190;&#30340;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#36164;&#28304;&#25968;&#37327;&#30340;&#22823;&#24133;&#22686;&#21152;&#12289;&#22810;&#26679;&#24615;&#21644;&#20998;&#24067;&#24615;&#65292;&#36825;&#20123;&#27493;&#39588;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#24403;&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#30340;&#35774;&#35745;&#26102;&#65292;&#20250;&#24102;&#26469;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#22810;&#20010;&#29420;&#29305;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21644;&#21327;&#21516;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#20854;&#26597;&#35810;&#32467;&#26500;&#26469;&#25903;&#25345;&#19978;&#36848;&#21151;&#33021;&#65292;&#32780;&#19981;&#38480;&#20110;&#29305;&#23450;&#30340;&#23398;&#20064;&#12289;&#36873;&#25321;&#21644;&#35843;&#25972;&#26426;&#21046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12289;&#24418;&#24335;&#39564;&#35777;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm selection and hyperparameter tuning are critical steps in both academic and applied machine learning. On the other hand, these steps are becoming ever increasingly delicate due to the extensive rise in the number, diversity, and distributedness of machine learning resources. Multi-agent systems, when applied to the design of machine learning platforms, bring about several distinctive characteristics such as scalability, flexibility, and robustness, just to name a few. This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical st
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#24314;&#27169;&#36712;&#36857;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#25209;&#22788;&#29702;&#32422;&#26463;&#36991;&#20813;&#22806;&#25512;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#24182;&#32534;&#30721;&#26356;&#20016;&#23500;&#30340;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.06599</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#25193;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Latent Diffusion in Offline Reinforcement Learning. (arXiv:2309.06599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#24314;&#27169;&#36712;&#36857;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#25209;&#22788;&#29702;&#32422;&#26463;&#36991;&#20813;&#22806;&#25512;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#24182;&#32534;&#30721;&#26356;&#20016;&#23500;&#30340;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26377;&#26395;&#36890;&#36807;&#38745;&#24577;&#25968;&#25454;&#38598;&#23398;&#20064;&#39640;&#22870;&#21169;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#30340;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#31163;&#32447;RL&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#22320;&#23558;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#30340;&#23376;&#20248;&#21270;&#36712;&#36857;&#29255;&#27573;&#36830;&#25509;&#36215;&#26469;&#65292;&#21516;&#26102;&#36991;&#20813;&#30001;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#25903;&#25345;&#19981;&#36275;&#32780;&#20135;&#29983;&#30340;&#22806;&#25512;&#35823;&#24046;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#20445;&#23432;&#30340;&#26041;&#27861;&#36827;&#34892;&#35843;&#25972;&#65292;&#36825;&#20123;&#26041;&#27861;&#38590;&#20197;&#35843;&#33410;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#19978;&#23384;&#22312;&#22256;&#38590;&#65288;&#22914;&#25105;&#20204;&#25152;&#31034;&#65289;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#22024;&#26434;&#30340;&#33945;&#29305;&#21345;&#27931;&#22238;&#25253;&#26679;&#26412;&#36827;&#34892;&#22870;&#21169;&#26465;&#20214;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23558;&#25903;&#25345;&#20869;&#30340;&#36712;&#36857;&#24207;&#21015;&#24314;&#27169;&#20026;&#21387;&#32553;&#30340;&#28508;&#22312;&#25216;&#33021;&#12290;&#36825;&#26377;&#21161;&#20110;&#36890;&#36807;&#25209;&#22788;&#29702;&#32422;&#26463;&#23398;&#20064;Q&#20989;&#25968;&#65292;&#21516;&#26102;&#36991;&#20813;&#22806;&#25512;&#35823;&#24046;&#12290;&#28508;&#22312;&#31354;&#38388;&#20063;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#20248;&#38597;&#22320;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#26102;&#38388;&#25277;&#35937;&#28508;&#22312;&#31354;&#38388;&#32534;&#30721;&#26356;&#20016;&#23500;&#30340;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) holds promise as a means to learn high-reward policies from a static dataset, without the need for further environment interactions. However, a key challenge in offline RL lies in effectively stitching portions of suboptimal trajectories from the static dataset while avoiding extrapolation errors arising due to a lack of support in the dataset. Existing approaches use conservative methods that are tricky to tune and struggle with multi-modal data (as we show) or rely on noisy Monte Carlo return-to-go samples for reward conditioning. In this work, we propose a novel approach that leverages the expressiveness of latent diffusion to model in-support trajectory sequences as compressed latent skills. This facilitates learning a Q-function while avoiding extrapolation error via batch-constraining. The latent space is also expressive and gracefully copes with multi-modal data. We show that the learned temporally-abstract latent space encodes richer task-spe
&lt;/p&gt;</description></item><item><title>Rank2Tell&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06597</link><description>&lt;p&gt;
Rank2Tell: &#19968;&#20010;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning. (arXiv:2309.06597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06597
&lt;/p&gt;
&lt;p&gt;
Rank2Tell&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#29992;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21644;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#21487;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#31038;&#20250;&#23545;&#23427;&#20204;&#30340;&#25509;&#21463;&#31243;&#24230;&#65292;&#32780;&#23545;&#39569;&#36710;&#20154;&#26469;&#35828;&#65292;&#23427;&#20204;&#34987;&#35270;&#20026;&#21487;&#20449;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20010;&#20219;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#29616;&#20195;&#33258;&#20027;&#31995;&#32479;&#36719;&#20214;&#20005;&#37325;&#20381;&#36182;&#20110;&#40657;&#30418;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;Rank2Tell&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#37325;&#35201;&#24615;&#32423;&#21035;&#25490;&#24207;&#21644;&#21407;&#22240;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#21508;&#31181;&#38381;&#21512;&#21644;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#21508;&#31181;&#35821;&#20041;&#12289;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20851;&#31995;&#23646;&#24615;&#30340;&#23494;&#38598;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#20351;&#20854;&#25104;&#20026;&#20174;&#20107;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#21644;&#30456;&#20851;&#39046;&#22495;&#30740;&#31350;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#34920;&#31034;&#21644;&#25512;&#29702;&#37325;&#35201;&#24615;&#21644;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS) may largely depend on their acceptance by society, for which their perceived trustworthiness and interpretability to riders are crucial. In general, this task is challenging because modern autonomous systems software relies heavily on black-box artificial intelligence models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios. The dense annotations and unique attributes of the dataset make it a valuable resource for researchers working on visual scene understanding and related fields. Further, we introduce a joint model for joint i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;LQR&#20013;&#24212;&#29992;MAML&#26102;&#30340;&#23616;&#37096;&#25910;&#25947;&#29305;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20445;&#25345;&#21160;&#24577;&#31995;&#32479;&#31283;&#23450;&#24615;&#30340;&#23616;&#37096;&#25910;&#25947;&#20445;&#35777;&#12290;&#35770;&#25991;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;MAML&#22312;LQR&#20219;&#21153;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2309.06588</link><description>&lt;p&gt;
Gradient-based MAML&#22312;LQR&#20013;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of Gradient-based MAML in LQR. (arXiv:2309.06588v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;LQR&#20013;&#24212;&#29992;MAML&#26102;&#30340;&#23616;&#37096;&#25910;&#25947;&#29305;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20445;&#25345;&#21160;&#24577;&#31995;&#32479;&#31283;&#23450;&#24615;&#30340;&#23616;&#37096;&#25910;&#25947;&#20445;&#35777;&#12290;&#35770;&#25991;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;MAML&#22312;LQR&#20219;&#21153;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25506;&#32034;&#22312;&#32447;&#24615;&#31995;&#32479;&#20108;&#27425;&#20248;&#21270;&#25511;&#21046;&#65288;LQR&#65289;&#20013;&#24212;&#29992;Model-agnostic Meta-learning&#65288;MAML&#65289;&#26102;&#30340;&#23616;&#37096;&#25910;&#25947;&#29305;&#24615;&#12290;MAML&#21450;&#20854;&#21464;&#20307;&#24050;&#25104;&#20026;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#27969;&#34892;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#22312;&#22238;&#24402;&#12289;&#20998;&#31867;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#30340;&#20808;&#21069;&#23398;&#20064;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#20984;&#24615;&#21644;&#20854;&#32467;&#26500;&#65292;MAML&#30340;&#29702;&#35770;&#20445;&#35777;&#20173;&#28982;&#26410;&#30693;&#65292;&#36825;&#20351;&#24471;&#22312;&#21160;&#24577;&#31995;&#32479;&#35774;&#32622;&#20013;&#30830;&#20445;&#31283;&#23450;&#24615;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;MAML&#22312;LQR&#35774;&#32622;&#20013;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#21160;&#24577;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#20197;&#23637;&#31034;MAML&#22312;LQR&#20219;&#21153;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main objective of this research paper is to investigate the local convergence characteristics of Model-agnostic Meta-learning (MAML) when applied to linear system quadratic optimal control (LQR). MAML and its variations have become popular techniques for quickly adapting to new tasks by leveraging previous learning knowledge in areas like regression, classification, and reinforcement learning. However, its theoretical guarantees remain unknown due to non-convexity and its structure, making it even more challenging to ensure stability in the dynamic system setting. This study focuses on exploring MAML in the LQR setting, providing its local convergence guarantees while maintaining the stability of the dynamical system. The paper also presents simple numerical results to demonstrate the convergence properties of MAML in LQR tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#21457;&#29616;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#36824;&#33021;&#25581;&#31034;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21644;&#20854;&#23545;&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#33021;&#25552;&#20379;&#20840;&#38754;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.06584</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Graph Neural Network for Alzheimer's Disease And Related Dementias Risk Prediction. (arXiv:2309.06584v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06584
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#21457;&#29616;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#36824;&#33021;&#25581;&#31034;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21644;&#20854;&#23545;&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#33021;&#25552;&#20379;&#20840;&#38754;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#65288;ADRD&#65289;&#22312;&#32654;&#22269;&#26159;&#31532;&#20845;&#22823;&#27515;&#20129;&#21407;&#22240;&#65292;&#20934;&#30830;&#30340;ADRD&#39118;&#38505;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#34429;&#28982;&#26368;&#36817;&#22312;ADRD&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#22270;&#20687;&#20998;&#26512;&#65292;&#32780;&#24182;&#38750;&#25152;&#26377;&#24739;&#32773;&#22312;ADRD&#35786;&#26029;&#21069;&#37117;&#25509;&#21463;&#21307;&#23398;&#24433;&#20687;&#26816;&#26597;&#12290;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#21487;&#20197;&#25581;&#31034;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#24182;&#21457;&#29616;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#32034;&#36180;&#25968;&#25454;&#36827;&#34892;ADRD&#39118;&#38505;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#32972;&#21518;&#32570;&#20047;&#21487;&#35299;&#37322;&#21407;&#22240;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21450;&#20854;&#23545;ADRD&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#30830;&#20445;&#20840;&#38754;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;VGNN&#65289;&#26469;&#20272;&#35745;ADRD&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#31181;&#24773;&#26223;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20351;&#29992;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#36731;&#26799;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading cause of death in the US, underlining the importance of accurate ADRD risk prediction. While recent advancement in ADRD risk prediction have primarily relied on imaging analysis, yet not all patients undergo medical imaging before an ADRD diagnosis. Merging machine learning with claims data can reveal additional risk factors and uncover interconnections among diverse medical codes. Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD risk prediction. Addressing the lack of human-interpretable reasons behind these predictions, we introduce an innovative method to evaluate relationship importance and its influence on ADRD risk prediction, ensuring comprehensive interpretation.  We employed Variationally Regularized Encoder-decoder Graph Neural Network (VGNN) for estimating ADRD likelihood. We created three scenarios to assess the model's efficiency, using Random Forest and Light Gradient 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;CMS&#39640;&#31934;&#24230;&#37327;&#28909;&#22120;&#21407;&#22411;&#20013;&#30340;&#30005;&#23376;&#33021;&#37327;&#22238;&#24402;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#19977;&#32500;&#28857;&#30340;&#33021;&#37327;&#37325;&#24314;&#20837;&#23556;&#30005;&#23376;&#30340;&#33021;&#37327;&#65292;&#24182;&#24076;&#26395;&#36890;&#36807;&#20844;&#24320;&#25968;&#25454;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#19987;&#23478;&#23545;&#30005;&#23376;&#22270;&#20687;&#37325;&#24314;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.06582</link><description>&lt;p&gt;
CMS&#39640;&#31934;&#24230;&#37327;&#28909;&#22120;&#21407;&#22411;&#20013;&#30340;&#30005;&#23376;&#33021;&#37327;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Electron Energy Regression in the CMS High-Granularity Calorimeter Prototype. (arXiv:2309.06582v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06582
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;CMS&#39640;&#31934;&#24230;&#37327;&#28909;&#22120;&#21407;&#22411;&#20013;&#30340;&#30005;&#23376;&#33021;&#37327;&#22238;&#24402;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#19977;&#32500;&#28857;&#30340;&#33021;&#37327;&#37325;&#24314;&#20837;&#23556;&#30005;&#23376;&#30340;&#33021;&#37327;&#65292;&#24182;&#24076;&#26395;&#36890;&#36807;&#20844;&#24320;&#25968;&#25454;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#19987;&#23478;&#23545;&#30005;&#23376;&#22270;&#20687;&#37325;&#24314;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23558;&#23433;&#35013;&#22312;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#19978;&#30340;&#19968;&#31181;&#26032;&#22411;&#37327;&#28909;&#22120;&#30340;&#27169;&#25311;&#25968;&#25454;&#12290;&#35813;&#25506;&#27979;&#22120;&#23558;&#25317;&#26377;&#36229;&#36807;600&#19975;&#20010;&#36890;&#36947;&#65292;&#27599;&#20010;&#36890;&#36947;&#37117;&#33021;&#22815;&#36827;&#34892;&#20301;&#32622;&#12289;&#30005;&#31163;&#21644;&#31934;&#30830;&#26102;&#38388;&#27979;&#37327;&#12290;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#37325;&#24314;&#36825;&#20123;&#20107;&#20214;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#27491;&#22312;&#24212;&#29992;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#35299;&#20915;&#12290;&#20316;&#20026;&#36825;&#19968;&#21457;&#23637;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24314;&#36896;&#20102;&#19968;&#20010;&#25317;&#26377;12,000&#20010;&#36890;&#36947;&#30340;&#22823;&#22411;&#21407;&#22411;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#39640;&#33021;&#30005;&#23376;&#26463;&#20837;&#23556;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#19977;&#32500;&#28857;&#30340;&#33021;&#37327;&#37325;&#24314;&#20102;&#20837;&#23556;&#30005;&#23376;&#30340;&#33021;&#37327;&#65292;&#36825;&#20123;&#33021;&#37327;&#24050;&#30693;&#21040;&#19968;&#23450;&#30340;&#31934;&#24230;&#12290;&#36890;&#36807;&#20844;&#24320;&#21457;&#24067;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#24076;&#26395;&#40723;&#21169;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#19987;&#23478;&#24320;&#21457;&#20986;&#39640;&#25928;&#20934;&#30830;&#30340;&#36825;&#20123;&#30005;&#23376;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new publicly available dataset that contains simulated data of a novel calorimeter to be installed at the CERN Large Hadron Collider. This detector will have more than six-million channels with each channel capable of position, ionisation and precision time measurement. Reconstructing these events in an efficient way poses an immense challenge which is being addressed with the latest machine learning techniques. As part of this development a large prototype with 12,000 channels was built and a beam of high-energy electrons incident on it. Using machine learning methods we have reconstructed the energy of incident electrons from the energies of three-dimensional hits, which is known to some precision. By releasing this data publicly we hope to encourage experts in the application of machine learning to develop efficient and accurate image reconstruction of these electrons.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#26469;&#35745;&#31639;&#33539;&#25968;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#33539;&#22260;&#12290;&#24212;&#29992;&#20110;&#19981;&#21516;&#23618;&#30340;&#23454;&#39564;&#34920;&#26126;&#20854;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.06577</link><description>&lt;p&gt;
&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Finite Initialization for Tensorized Neural Networks. (arXiv:2309.06577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06577
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#26469;&#35745;&#31639;&#33539;&#25968;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#33539;&#22260;&#12290;&#24212;&#29992;&#20110;&#19981;&#21516;&#23618;&#30340;&#23454;&#39564;&#34920;&#26126;&#20854;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#20197;&#36991;&#20813;&#21442;&#25968;&#29190;&#28856;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#33410;&#28857;&#30340;&#23618;&#65292;&#20854;&#20013;&#25152;&#26377;&#25110;&#22823;&#22810;&#25968;&#33410;&#28857;&#19982;&#36755;&#20837;&#25110;&#36755;&#20986;&#26377;&#36830;&#25509;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20351;&#29992;&#35813;&#23618;&#30340;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#30340;&#33539;&#22260;&#12290;&#36825;&#20010;&#33539;&#25968;&#30340;&#35745;&#31639;&#26159;&#39640;&#25928;&#30340;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#24773;&#20917;&#37117;&#21487;&#20197;&#23436;&#20840;&#25110;&#37096;&#20998;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#23618;&#65292;&#24182;&#26816;&#26597;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;Python&#20989;&#25968;&#65292;&#22312;i3BQuantum&#23384;&#20648;&#24211;&#30340;Jupyter Notebook&#20013;&#21487;&#20197;&#36816;&#34892;&#23427;&#65306;https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
&lt;/p&gt;
&lt;p&gt;
We present a novel method for initializing layers of tensorized neural networks in a way that avoids the explosion of the parameters of the matrix it emulates. The method is intended for layers with a high number of nodes in which there is a connection to the input or output of all or most of the nodes. The core of this method is the use of the Frobenius norm of this layer in an iterative partial form, so that it has to be finite and within a certain range. This norm is efficient to compute, fully or partially for most cases of interest. We apply the method to different layers and check its performance. We create a Python function to run it on an arbitrary layer, available in a Jupyter Notebook in the i3BQuantum repository: https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22278;&#24418;&#29305;&#24449;&#22270;&#36716;&#25442;&#22120;&#65288;CFG&#65289;&#65292;&#29992;&#20110;&#20002;&#22833;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#22270;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CFG&#22312;ogbl-citation2&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06574</link><description>&lt;p&gt;
Circle Feature Graphormer: &#33021;&#22815;&#21050;&#28608;&#22270;&#36716;&#25442;&#22120;&#30340;&#22278;&#24418;&#29305;&#24449;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Circle Feature Graphormer: Can Circle Features Stimulate Graph Transformer?. (arXiv:2309.06574v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22278;&#24418;&#29305;&#24449;&#22270;&#36716;&#25442;&#22120;&#65288;CFG&#65289;&#65292;&#29992;&#20110;&#20002;&#22833;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#22270;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CFG&#22312;ogbl-citation2&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#29992;&#20110;ogbl-citation2&#20013;&#20002;&#22833;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#30340;&#26412;&#22320;&#22270;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#23450;&#20041;&#20026;&#22278;&#24418;&#29305;&#24449;&#65292;&#20511;&#37492;&#20102;&#26379;&#21451;&#22280;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#36848;&#29305;&#24449;&#30340;&#35814;&#32454;&#35745;&#31639;&#20844;&#24335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#31532;&#19968;&#20010;&#22278;&#24418;&#29305;&#24449;&#23450;&#20041;&#20026;&#24120;&#35265;&#22270;&#20013;&#30340;&#25913;&#36827;&#25391;&#33633;&#29305;&#24449;&#65292;&#23427;&#26469;&#33258;&#20110;&#20108;&#20998;&#22270;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#31532;&#20108;&#20010;&#22278;&#24418;&#29305;&#24449;&#23450;&#20041;&#20026;&#26725;&#26753;&#65292;&#23427;&#34920;&#31034;&#19981;&#21516;&#26379;&#21451;&#22280;&#20013;&#20004;&#20010;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#19978;&#36848;&#29305;&#24449;&#20316;&#20026;&#20559;&#32622;&#26469;&#22686;&#24378;&#22270;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22270;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#22522;&#20110;SIEG&#32593;&#32476;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#22278;&#24418;&#29305;&#24449;&#30340;&#22270;&#36716;&#25442;&#22120;&#65288;CFG&#65289;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#21452;&#22612;&#32467;&#26500;&#26469;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#32467;&#26500;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CFG&#22312;ogbl-citation2&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce two local graph features for missing link prediction tasks on ogbl-citation2. We define the features as Circle Features, which are borrowed from the concept of circle of friends. We propose the detailed computing formulas for the above features. Firstly, we define the first circle feature as modified swing for common graph, which comes from bipartite graph. Secondly, we define the second circle feature as bridge, which indicates the importance of two nodes for different circle of friends. In addition, we firstly propose the above features as bias to enhance graph transformer neural network, such that graph self-attention mechanism can be improved. We implement a Circled Feature aware Graph transformer (CFG) model based on SIEG network, which utilizes a double tower structure to capture both global and local structure features. Experimental results show that CFG achieves the state-of-the-art performance on dataset ogbl-citation2.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#19982;&#39640;&#26031;&#36807;&#31243;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20026;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#21644;&#25511;&#21046;&#21512;&#25104;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;DKL&#19982;&#21306;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;IMDP&#65289;&#23454;&#29616;&#23545;&#38543;&#26426;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#21512;&#25104;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#39640;&#25928;&#30340;&#28145;&#24230;&#26550;&#26500;&#21644;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06569</link><description>&lt;p&gt;
&#28145;&#24230;&#26680;&#23398;&#20064;&#22312;&#25511;&#21046;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Promises of Deep Kernel Learning for Control Synthesis. (arXiv:2309.06569v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06569
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#19982;&#39640;&#26031;&#36807;&#31243;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20026;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#21644;&#25511;&#21046;&#21512;&#25104;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;DKL&#19982;&#21306;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;IMDP&#65289;&#23454;&#29616;&#23545;&#38543;&#26426;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#21512;&#25104;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#39640;&#25928;&#30340;&#28145;&#24230;&#26550;&#26500;&#21644;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#19982;&#39640;&#26031;&#36807;&#31243;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30456;&#32467;&#21512;&#12290;&#22240;&#27492;&#65292;&#23427;&#26159;&#19968;&#20010;&#26377;&#28508;&#21147;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23398;&#20064;&#21644;&#25511;&#21046;&#22797;&#26434;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25277;&#35937;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#22797;&#26434;&#35268;&#33539;&#19979;&#20351;&#29992;DKL&#36827;&#34892;&#38543;&#26426;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#21512;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;DKL&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26410;&#30693;&#31995;&#32479;&#65292;&#24182;&#23558;DKL&#27169;&#22411;&#27491;&#24335;&#25277;&#35937;&#25104;&#21306;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;IMDP&#65289;&#65292;&#20197;&#36827;&#34892;&#20855;&#26377;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#25511;&#21046;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#31181;&#28145;&#24230;&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#23398;&#20064;&#21644;&#39640;&#25928;&#30340;&#25277;&#35937;&#35745;&#31639;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35828;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#19968;&#20010;5&#32500;&#38750;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;DKL&#36827;&#34892;&#25511;&#21046;&#21512;&#25104;&#21487;&#20197;&#22823;&#22823;&#20248;&#20110;&#29366;&#24577;-
&lt;/p&gt;
&lt;p&gt;
Deep Kernel Learning (DKL) combines the representational power of neural networks with the uncertainty quantification of Gaussian Processes. Hence, it is potentially a promising tool to learn and control complex dynamical systems. In this work, we develop a scalable abstraction-based framework that enables the use of DKL for control synthesis of stochastic dynamical systems against complex specifications. Specifically, we consider temporal logic specifications and create an end-to-end framework that uses DKL to learn an unknown system from data and formally abstracts the DKL model into an Interval Markov Decision Process (IMDP) to perform control synthesis with correctness guarantees. Furthermore, we identify a deep architecture that enables accurate learning and efficient abstraction computation. The effectiveness of our approach is illustrated on various benchmarks, including a 5-D nonlinear stochastic system, showing how control synthesis with DKL can substantially outperform state-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#27809;&#26377;&#20154;&#20026;&#24433;&#21709;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#23398;&#25253;&#32440;&#26723;&#26696;&#20013;&#33719;&#21462;&#24182;&#26816;&#27979;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#30340;&#24773;&#24863;&#19982;&#21407;&#25991;&#26469;&#35745;&#31639;&#20559;&#35265;&#65292;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20026;&#23458;&#35266;&#29702;&#35299;&#23398;&#29983;&#25253;&#32440;&#26469;&#28304;&#20013;&#30340;&#20559;&#35265;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06557</link><description>&lt;p&gt;
&#22312;&#22823;&#23398;&#23398;&#29983;&#25253;&#32440;&#20013;&#26080;&#30417;&#30563;&#26816;&#27979;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Bias Detection in College Student Newspapers. (arXiv:2309.06557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#27809;&#26377;&#20154;&#20026;&#24433;&#21709;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#23398;&#25253;&#32440;&#26723;&#26696;&#20013;&#33719;&#21462;&#24182;&#26816;&#27979;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#30340;&#24773;&#24863;&#19982;&#21407;&#25991;&#26469;&#35745;&#31639;&#20559;&#35265;&#65292;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20026;&#23458;&#35266;&#29702;&#35299;&#23398;&#29983;&#25253;&#32440;&#26469;&#28304;&#20013;&#30340;&#20559;&#35265;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#27809;&#26377;&#20154;&#20026;&#24433;&#21709;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#22823;&#23398;&#25253;&#32440;&#26723;&#26696;&#20013;&#33719;&#21462;&#24182;&#26816;&#27979;&#20559;&#35265;&#12290;&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20174;&#33258;&#21160;&#21270;&#24037;&#20855;&#26080;&#27861;&#33719;&#21462;&#25968;&#25454;&#30340;&#22797;&#26434;&#26723;&#26696;&#32593;&#31449;&#19978;&#33719;&#21462;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;23,154&#20010;&#26465;&#30446;&#30340;14&#20010;&#23398;&#29983;&#25253;&#32440;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#30340;&#24773;&#24863;&#19982;&#21407;&#25991;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#20851;&#38190;&#23383;&#26597;&#35810;&#26469;&#35745;&#31639;&#20559;&#35265;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#27604;&#37325;&#26500;&#20559;&#35265;&#26356;&#23569;&#27604;&#36739;&#65292;&#24182;&#19988;&#27604;&#29983;&#25104;&#20851;&#38190;&#23383;&#24773;&#32490;&#38656;&#35201;&#26356;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#25919;&#27835;&#24615;&#35789;&#27719;&#20197;&#21450;&#25511;&#21046;&#35789;&#19978;&#35745;&#31639;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#24471;&#20986;&#32467;&#35770;&#12290;&#35813;&#23436;&#25972;&#26041;&#27861;&#26377;&#21161;&#20110;&#22312;&#20551;&#35774;&#21644;&#20998;&#31867;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#32454;&#33268;&#20837;&#24494;&#30340;&#35265;&#35299;&#65292;&#20026;&#23458;&#35266;&#29702;&#35299;&#23398;&#29983;&#25253;&#32440;&#26469;&#28304;&#20013;&#30340;&#20559;&#35265;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a pipeline with minimal human influence for scraping and detecting bias on college newspaper archives. This paper introduces a framework for scraping complex archive sites that automated tools fail to grab data from, and subsequently generates a dataset of 14 student papers with 23,154 entries. This data can also then be queried by keyword to calculate bias by comparing the sentiment of a large language model summary to the original article. The advantages of this approach are that it is less comparative than reconstruction bias and requires less labelled data than generating keyword sentiment. Results are calculated on politically charged words as well as control words to show how conclusions can be drawn. The complete method facilitates the extraction of nuanced insights with minimal assumptions and categorizations, paving the way for a more objective understanding of bias within student newspaper sources.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>"ai-cli"&#26159;&#19968;&#20010;&#24320;&#28304;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#30340;Linux&#21629;&#20196;&#34892;&#24037;&#20855;&#21629;&#20196;&#65292;&#21033;&#29992;OpenAI&#30340;API&#23454;&#29616;&#21629;&#20196;&#34892;&#30028;&#38754;&#30340;&#26234;&#33021;&#21270;&#21644;&#29992;&#25143;&#21451;&#22909;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.06551</link><description>&lt;p&gt;
&#21629;&#20196;&#20316;&#20026;AI&#23545;&#35805;&#65306;&#19968;&#20010;&#24320;&#28304;&#31995;&#32479;&#23558;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#30340;Linux&#21629;&#20196;&#34892;&#24037;&#20855;&#21629;&#20196;
&lt;/p&gt;
&lt;p&gt;
Commands as AI Conversations. (arXiv:2309.06551v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06551
&lt;/p&gt;
&lt;p&gt;
"ai-cli"&#26159;&#19968;&#20010;&#24320;&#28304;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#30340;Linux&#21629;&#20196;&#34892;&#24037;&#20855;&#21629;&#20196;&#65292;&#21033;&#29992;OpenAI&#30340;API&#23454;&#29616;&#21629;&#20196;&#34892;&#30028;&#38754;&#30340;&#26234;&#33021;&#21270;&#21644;&#29992;&#25143;&#21451;&#22909;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#21592;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#32463;&#24120;&#22312;&#32534;&#20889;&#21629;&#20196;&#34892;&#36755;&#20837;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#23613;&#31649;&#22270;&#24418;&#30028;&#38754;&#25110;&#20687;ChatGPT&#36825;&#26679;&#30340;&#24037;&#20855;&#21487;&#20197;&#25552;&#20379;&#24110;&#21161;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#24320;&#28304;&#31995;&#32479;"ai-cli"&#65292;&#21463;GitHub Copilot&#21551;&#21457;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#36716;&#21270;&#20026;&#21508;&#31181;Linux&#21629;&#20196;&#34892;&#24037;&#20855;&#30340;&#21487;&#25191;&#34892;&#21629;&#20196;&#12290;&#36890;&#36807;&#21033;&#29992;OpenAI&#30340;API&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;JSON HTTP&#35831;&#27714;&#36827;&#34892;&#20132;&#20114;&#65292;&#23558;&#29992;&#25143;&#26597;&#35810;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#21629;&#20196;&#34892;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#20010;&#21629;&#20196;&#34892;&#24037;&#20855;&#20043;&#38388;&#38598;&#25104;AI&#36741;&#21161;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#28304;&#29615;&#22659;&#20013;&#65292;&#21487;&#33021;&#20250;&#24456;&#22797;&#26434;&#12290;&#21382;&#21490;&#19978;&#65292;&#25805;&#20316;&#31995;&#32479;&#21487;&#20197;&#36827;&#34892;&#20013;&#20171;&#65292;&#20294;&#21508;&#20010;&#24037;&#20855;&#30340;&#21151;&#33021;&#21644;&#32570;&#20047;&#32479;&#19968;&#30340;&#26041;&#27861;&#20351;&#24471;&#38598;&#20013;&#21270;&#38598;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#36890;&#36807;&#21160;&#24577;&#21152;&#36733;&#21644;&#19982;&#27599;&#20010;&#31243;&#24207;&#30340;Readline&#24211;API&#38142;&#25509;&#65292;"ai-cli"&#24037;&#20855;&#22635;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#65292;&#20351;&#21629;&#20196;&#34892;&#30028;&#38754;&#26356;&#26234;&#33021;&#12289;&#26356;&#29992;&#25143;&#21451;&#22909;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#22686;&#24378;&#21644;&#36328;&#24179;&#21488;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers and data scientists often struggle to write command-line inputs, even though graphical interfaces or tools like ChatGPT can assist. The solution? "ai-cli," an open-source system inspired by GitHub Copilot that converts natural language prompts into executable commands for various Linux command-line tools. By tapping into OpenAI's API, which allows interaction through JSON HTTP requests, "ai-cli" transforms user queries into actionable command-line instructions. However, integrating AI assistance across multiple command-line tools, especially in open source settings, can be complex. Historically, operating systems could mediate, but individual tool functionality and the lack of a unified approach have made centralized integration challenging. The "ai-cli" tool, by bridging this gap through dynamic loading and linking with each program's Readline library API, makes command-line interfaces smarter and more user-friendly, opening avenues for further enhancement and cross-platfor
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#26080;&#38480;&#32500;&#32447;&#24615;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#32447;&#24615;&#31639;&#23376;&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#20123;&#26465;&#20214;&#19979;&#21017;&#19981;&#21487;&#20197;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#32447;&#22343;&#19968;&#25910;&#25947;&#21644;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#24182;&#22312;PAC&#35774;&#32622;&#19979;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06548</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#32447;&#24615;&#31639;&#23376;&#30340;&#26080;&#38480;&#32500;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Online Infinite-Dimensional Regression: Learning Linear Operators. (arXiv:2309.06548v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06548
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#26080;&#38480;&#32500;&#32447;&#24615;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#32447;&#24615;&#31639;&#23376;&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#20123;&#26465;&#20214;&#19979;&#21017;&#19981;&#21487;&#20197;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#32447;&#22343;&#19968;&#25910;&#25947;&#21644;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#24182;&#22312;PAC&#35774;&#32622;&#19979;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#20004;&#20010;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20043;&#38388;&#30340;&#32447;&#24615;&#31639;&#23376;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$p \in [1, \infty)$&#33539;&#22260;&#20869;&#65292;&#20855;&#26377;&#22343;&#21248;&#26377;&#30028;$p$-Schatten&#33539;&#25968;&#30340;&#32447;&#24615;&#31639;&#23376;&#31867;&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#22343;&#21248;&#26377;&#30028;&#31639;&#23376;&#33539;&#25968;&#30340;&#32447;&#24615;&#31639;&#23376;&#31867;\textit{&#19981;}&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25214;&#21040;&#19968;&#31867;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;&#35777;&#26126;&#20102;&#22312;&#32447;&#22343;&#19968;&#25910;&#25947;&#21644;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#21644;&#22343;&#19968;&#25910;&#25947;&#19982;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#22312;PAC&#35774;&#32622;&#19979;&#21516;&#26679;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning linear operators under squared loss between two infinite-dimensional Hilbert spaces in the online setting. We show that the class of linear operators with uniformly bounded $p$-Schatten norm is online learnable for any $p \in [1, \infty)$. On the other hand, we prove an impossibility result by showing that the class of uniformly bounded linear operators with respect to the operator norm is \textit{not} online learnable. Moreover, we show a separation between online uniform convergence and online learnability by identifying a class of bounded linear operators that is online learnable but uniform convergence does not hold. Finally, we prove that the impossibility result and the separation between uniform convergence and learnability also hold in the agnostic PAC setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25512;&#29305;&#24773;&#24863;&#20998;&#26512;&#30340;&#32929;&#31080;&#25253;&#20215;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;Twitter&#19978;&#30340;&#24086;&#23376;&#20013;&#25552;&#21462;&#24773;&#24863;&#29305;&#24449;&#65292;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#23454;&#38469;&#20132;&#26131;&#20013;&#21462;&#24471;&#20102;88.82&#38647;&#20122;&#23572;&#30340;&#20928;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2309.06538</link><description>&lt;p&gt;
&#22522;&#20110;&#25512;&#29305;&#24773;&#24863;&#20998;&#26512;&#30340;&#32929;&#31080;&#25253;&#20215;&#39044;&#27979;&#27169;&#22411;&#30340;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Desenvolvimento de modelo para predi\c{c}\~ao de cota\c{c}\~oes de a\c{c}\~ao baseada em an\'alise de sentimentos de tweets. (arXiv:2309.06538v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25512;&#29305;&#24773;&#24863;&#20998;&#26512;&#30340;&#32929;&#31080;&#25253;&#20215;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;Twitter&#19978;&#30340;&#24086;&#23376;&#20013;&#25552;&#21462;&#24773;&#24863;&#29305;&#24449;&#65292;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#23454;&#38469;&#20132;&#26131;&#20013;&#21462;&#24471;&#20102;88.82&#38647;&#20122;&#23572;&#30340;&#20928;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20132;&#26131;&#36825;&#26679;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#32929;&#31080;&#24066;&#22330;&#30340;&#32929;&#20215;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#34429;&#28982;&#22823;&#37096;&#20998;&#30340;&#30740;&#31350;&#24037;&#20316;&#26159;&#22522;&#20110;&#36807;&#21435;&#30340;&#32929;&#31080;&#20215;&#26684;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;iFeel 2.0&#24179;&#21488;&#20174;&#24494;&#21338;&#24179;&#21488;Twitter&#19978;&#33719;&#24471;&#30340;&#25552;&#21450;&#24052;&#35199;&#30707;&#27833;&#20844;&#21496;&#30340;&#24086;&#23376;&#20013;&#25552;&#21462;&#20102;19&#20010;&#24773;&#24863;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#26469;&#35757;&#32451;XBoot&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#35813;&#20844;&#21496;&#26410;&#26469;&#30340;&#32929;&#31080;&#20215;&#26684;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#27169;&#22411;&#30340;&#36755;&#20986;&#27169;&#25311;&#20102;&#24052;&#35199;&#30707;&#27833;&#20844;&#21496;&#32929;&#31080;&#30340;&#20132;&#26131;&#65292;&#24182;&#19982;100&#20010;&#38543;&#26426;&#27169;&#22411;&#30340;&#24179;&#22343;&#34920;&#29616;&#30456;&#27604;&#65292;&#22312;250&#22825;&#30340;&#26102;&#38388;&#27573;&#20869;&#33719;&#24471;&#20102;88.82&#38647;&#20122;&#23572;&#65288;&#20928;&#65289;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning models for predicting stock market share prices is an active area of research since the automatization of trading such papers was available in real time. While most of the work in this field of research is done by training Neural networks based on past prices of stock shares, in this work, we use iFeel 2.0 platform to extract 19 sentiment features from posts obtained from microblog platform Twitter that mention the company Petrobras. Then, we used those features to train XBoot models to predict future stock prices for the referred company. Later, we simulated the trading of Petrobras' shares based on the model's outputs and determined the gain of R$88,82 (net) in a 250-day period when compared to a 100 random models' average performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20869;&#26368;&#20855;&#23545;&#25239;&#24615;&#30340;&#25439;&#22833;&#26469;&#23454;&#29616;&#65292;&#35813;&#38598;&#21512;&#26159;&#30001;&#28304;&#20998;&#24067;&#30340;&#20984;&#32452;&#21512;&#29983;&#25104;&#30340;&#30446;&#26631;&#20154;&#21475;&#38598;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#36801;&#31227;&#23398;&#20064;&#21644;&#20998;&#24067;&#40065;&#26834;&#30340;&#39044;&#27979;&#27169;&#22411;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2309.06534</link><description>&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Transfer Learning. (arXiv:2309.06534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06534
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20869;&#26368;&#20855;&#23545;&#25239;&#24615;&#30340;&#25439;&#22833;&#26469;&#23454;&#29616;&#65292;&#35813;&#38598;&#21512;&#26159;&#30001;&#28304;&#20998;&#24067;&#30340;&#20984;&#32452;&#21512;&#29983;&#25104;&#30340;&#30446;&#26631;&#20154;&#21475;&#38598;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#36801;&#31227;&#23398;&#20064;&#21644;&#20998;&#24067;&#40065;&#26834;&#30340;&#39044;&#27979;&#27169;&#22411;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#21033;&#29992;&#19982;&#30446;&#26631;&#25968;&#25454;&#30456;&#20284;&#30340;&#28304;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#24573;&#35270;&#20102;&#21487;&#33021;&#23384;&#22312;&#20110;&#19981;&#21516;&#20294;&#28508;&#22312;&#30456;&#20851;&#30340;&#36741;&#21161;&#26679;&#26412;&#20013;&#30340;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#12290;&#24403;&#22788;&#29702;&#26377;&#38480;&#30340;&#30446;&#26631;&#25968;&#25454;&#21644;&#22810;&#26679;&#21270;&#30340;&#28304;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20998;&#24067;&#40065;&#26834;&#36801;&#31227;&#23398;&#20064;&#65288;TransDRO&#65289;&#65292;&#23427;&#25670;&#33073;&#20102;&#20005;&#26684;&#30340;&#30456;&#20284;&#24615;&#32422;&#26463;&#12290;TransDRO&#36890;&#36807;&#22312;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20869;&#20248;&#21270;&#26368;&#20855;&#23545;&#25239;&#24615;&#30340;&#25439;&#22833;&#26469;&#35774;&#35745;&#65292;&#35813;&#38598;&#21512;&#23450;&#20041;&#20026;&#30001;&#28304;&#20998;&#24067;&#30340;&#20984;&#32452;&#21512;&#29983;&#25104;&#30340;&#30446;&#26631;&#20154;&#21475;&#30340;&#38598;&#21512;&#65292;&#20445;&#35777;&#20102;&#23545;&#30446;&#26631;&#25968;&#25454;&#30340;&#20986;&#33394;&#39044;&#27979;&#24615;&#33021;&#12290;TransDRO&#26377;&#25928;&#22320;&#23558;&#36801;&#31227;&#23398;&#20064;&#21644;&#20998;&#24067;&#40065;&#26834;&#30340;&#39044;&#27979;&#27169;&#22411;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;TransDRO&#30340;&#21487;&#36776;&#35782;&#24615;&#21644;&#20854;&#20316;&#20026;&#26368;&#25509;&#36817;&#28304;&#27169;&#22411;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many existing transfer learning methods rely on leveraging information from source data that closely resembles the target data. However, this approach often overlooks valuable knowledge that may be present in different yet potentially related auxiliary samples. When dealing with a limited amount of target data and a diverse range of source models, our paper introduces a novel approach, Distributionally Robust Optimization for Transfer Learning (TransDRO), that breaks free from strict similarity constraints. TransDRO is designed to optimize the most adversarial loss within an uncertainty set, defined as a collection of target populations generated as a convex combination of source distributions that guarantee excellent prediction performances for the target data. TransDRO effectively bridges the realms of transfer learning and distributional robustness prediction models. We establish the identifiability of TransDRO and its interpretation as a weighted average of source models closest to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20250;&#35805;&#25512;&#33616;&#30340;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;HierSRec&#65292;&#36890;&#36807;&#22312;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#35774;&#32622;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#36755;&#20986;&#26469;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#26356;&#39640;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06533</link><description>&lt;p&gt;
&#38754;&#21521;&#20250;&#35805;&#25512;&#33616;&#30340;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Task Learning Framework for Session-based Recommendations. (arXiv:2309.06533v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20250;&#35805;&#25512;&#33616;&#30340;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;HierSRec&#65292;&#36890;&#36807;&#22312;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#35774;&#32622;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#36755;&#20986;&#26469;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#26356;&#39640;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;SBRS&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#20294;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#24050;&#32463;&#34987;SBRS&#37319;&#29992;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#12290;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;H-MTL&#65289;&#22312;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#35774;&#32622;&#20102;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#23558;&#36741;&#21161;&#20219;&#21153;&#30340;&#36755;&#20986;&#39304;&#36865;&#32473;&#20027;&#20219;&#21153;&#12290;&#19982;&#29616;&#26377;&#30340;MTL&#26694;&#26550;&#30456;&#27604;&#65292;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#20026;&#20027;&#20219;&#21153;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#26356;&#39640;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;H-MTL&#26694;&#26550;&#22312;SBRS&#20013;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HierSRec&#65292;&#23558;H-MTL&#26550;&#26500;&#32435;&#20837;SBRS&#20013;&#12290;HierSRec&#20351;&#29992;&#20803;&#25968;&#25454;&#24863;&#30693;Transformer&#23545;&#32473;&#23450;&#20250;&#35805;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20351;&#29992;&#20250;&#35805;&#32534;&#30721;&#36827;&#34892;&#19979;&#19968;&#31867;&#21035;&#39044;&#27979;&#65288;&#21363;&#36741;&#21161;&#20219;&#21153;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;HierSRec&#20351;&#29992;&#31867;&#21035;&#39044;&#27979;&#32467;&#26524;&#21644;&#20250;&#35805;&#32534;&#30721;&#36827;&#34892;&#19979;&#19968;&#20010;&#29289;&#21697;&#39044;&#27979;&#65288;&#21363;&#20027;&#20219;&#21153;&#65289;&#12290;&#20026;&#20102;&#21487;&#25193;&#23637;&#30340;&#25512;&#26029;&#65292;HierSRec&#21019;&#24314;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#20505;&#36873;&#29289;&#21697;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
While session-based recommender systems (SBRSs) have shown superior recommendation performance, multi-task learning (MTL) has been adopted by SBRSs to enhance their prediction accuracy and generalizability further. Hierarchical MTL (H-MTL) sets a hierarchical structure between prediction tasks and feeds outputs from auxiliary tasks to main tasks. This hierarchy leads to richer input features for main tasks and higher interpretability of predictions, compared to existing MTL frameworks. However, the H-MTL framework has not been investigated in SBRSs yet. In this paper, we propose HierSRec which incorporates the H-MTL architecture into SBRSs. HierSRec encodes a given session with a metadata-aware Transformer and performs next-category prediction (i.e., auxiliary task) with the session encoding. Next, HierSRec conducts next-item prediction (i.e., main task) with the category prediction result and session encoding. For scalable inference, HierSRec creates a compact set of candidate items (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#25239;&#25915;&#20987;&#23545;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#38754;&#23545;&#24050;&#30693;&#30340;&#26368;&#20339;&#23545;&#25239;&#25915;&#20987;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#31639;&#27861;&#22312;&#30456;&#23545;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26367;&#20195;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.06527</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Machine Translation Models Stand Strong in the Face of Adversarial Attacks. (arXiv:2309.06527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#25239;&#25915;&#20987;&#23545;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#38754;&#23545;&#24050;&#30693;&#30340;&#26368;&#20339;&#23545;&#25239;&#25915;&#20987;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#31639;&#27861;&#22312;&#30456;&#23545;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26367;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#36890;&#36807;&#21521;&#36755;&#20837;&#24341;&#20837;&#24494;&#23567;&#25200;&#21160;&#26469;&#26292;&#38706;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#36825;&#23548;&#33268;&#36755;&#20986;&#32467;&#26524;&#21457;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#36825;&#31181;&#23545;&#25239;&#25915;&#20987;&#23545;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#31639;&#27861;&#65292;&#21253;&#25324;&#22522;&#26412;&#25991;&#26412;&#25200;&#21160;&#21551;&#21457;&#24335;&#21644;&#26356;&#39640;&#32423;&#30340;&#31574;&#30053;&#65292;&#22914;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#65292;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#36924;&#36817;&#38750;&#21487;&#24494;&#32763;&#35793;&#24230;&#37327;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#35843;&#26597;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23545;&#24050;&#30693;&#30340;&#26368;&#20339;&#23545;&#25239;&#25915;&#20987;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#31283;&#23450;&#24615;&#65292;&#22240;&#20026;&#36755;&#20986;&#20013;&#30340;&#25200;&#21160;&#31243;&#24230;&#19982;&#36755;&#20837;&#20013;&#30340;&#25200;&#21160;&#25104;&#27604;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21033;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#32988;&#36807;&#20854;&#20182;&#36873;&#25321;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#30340;&#30456;&#23545;&#24615;&#33021;&#12290;&#21478;&#19968;&#20010;&#24378;&#22823;&#30340;&#20505;&#36873;&#26159;&#22522;&#20110;&#20010;&#20307;&#28151;&#21512;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks expose vulnerabilities of deep learning models by introducing minor perturbations to the input, which lead to substantial alterations in the output. Our research focuses on the impact of such adversarial attacks on sequence-to-sequence (seq2seq) models, specifically machine translation models. We introduce algorithms that incorporate basic text perturbation heuristics and more advanced strategies, such as the gradient-based attack, which utilizes a differentiable approximation of the inherently non-differentiable translation metric. Through our investigation, we provide evidence that machine translation models display robustness displayed robustness against best performed known adversarial attacks, as the degree of perturbation in the output is directly proportional to the perturbation in the input. However, among underdogs, our attacks outperform alternatives, providing the best relative performance. Another strong candidate is an attack based on mixing of individu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#23545;&#34920;&#26684;&#36716;&#25442;&#22120;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#22909;&#22788;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#21442;&#25968;&#25928;&#29575;&#12289;&#38544;&#31169;&#21644;&#20934;&#30830;&#24230;&#20043;&#38388;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.06526</link><description>&lt;p&gt;
&#25506;&#32034;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22312;&#34920;&#26684;&#36716;&#25442;&#22120;&#20013;&#30340;&#21463;&#30410;
&lt;/p&gt;
&lt;p&gt;
Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers. (arXiv:2309.06526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#23545;&#34920;&#26684;&#36716;&#25442;&#22120;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#22909;&#22788;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#21442;&#25968;&#25928;&#29575;&#12289;&#38544;&#31169;&#21644;&#20934;&#30830;&#24230;&#20043;&#38388;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;Table Transformer&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32780;&#24046;&#20998;&#38544;&#31169;&#26159;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#36825;&#20004;&#20010;&#26041;&#38754;&#32467;&#21512;&#36215;&#26469;&#24212;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#22909;&#22788;&#8212;&#8212;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#21644;&#23545;Table Transformer&#36827;&#34892;&#22810;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65288;&#21253;&#25324;Adapter&#12289;LoRA&#21644;Prompt Tuning&#65289;&#12290;&#25105;&#20204;&#22312;ACSIncome&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#22312;&#19979;&#28216;&#20219;&#21153;&#20934;&#30830;&#24230;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#21442;&#25968;&#25928;&#29575;&#12289;&#38544;&#31169;&#21644;&#20934;&#30830;&#24230;&#20043;&#38388;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;github.com/IBM/DP-TabTransformer&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
For machine learning with tabular data, Table Transformer (TabTransformer) is a state-of-the-art neural network model, while Differential Privacy (DP) is an essential component to ensure data privacy. In this paper, we explore the benefits of combining these two aspects together in the scenario of transfer learning -- differentially private pre-training and fine-tuning of TabTransformers with a variety of parameter-efficient fine-tuning (PEFT) methods, including Adapter, LoRA, and Prompt Tuning. Our extensive experiments on the ACSIncome dataset show that these PEFT methods outperform traditional approaches in terms of the accuracy of the downstream task and the number of trainable parameters, thus achieving an improved trade-off among parameter efficiency, privacy, and accuracy. Our code is available at github.com/IBM/DP-TabTransformer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20381;&#20174;&#24615;&#25512;&#33616;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#20154;&#31867;&#20915;&#31574;&#32773;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#8220;&#20381;&#20174;&#27700;&#24179;&#8221;&#26469;&#25429;&#25417;&#20915;&#31574;&#32773;&#36981;&#24490;&#25512;&#33616;&#30340;&#39057;&#29575;&#65292;&#24182;&#23454;&#26102;&#29983;&#25104;&#26368;&#20339;&#25512;&#33616;&#31574;&#30053;&#12290;&#30740;&#31350;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.06519</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#20381;&#20174;&#24615;&#24863;&#30693;&#25512;&#33616;&#30340;Q&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Q-learning Approach for Adherence-Aware Recommendations. (arXiv:2309.06519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20381;&#20174;&#24615;&#25512;&#33616;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#20154;&#31867;&#20915;&#31574;&#32773;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#8220;&#20381;&#20174;&#27700;&#24179;&#8221;&#26469;&#25429;&#25417;&#20915;&#31574;&#32773;&#36981;&#24490;&#25512;&#33616;&#30340;&#39057;&#29575;&#65292;&#24182;&#23454;&#26102;&#29983;&#25104;&#26368;&#20339;&#25512;&#33616;&#31574;&#30053;&#12290;&#30740;&#31350;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#28041;&#21450;&#39640;&#39118;&#38505;&#21644;&#23433;&#20840;&#38382;&#39064;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20154;&#31867;&#20915;&#31574;&#32773;&#65288;HDM&#65289;&#21487;&#33021;&#20250;&#22312;&#25285;&#36127;&#26368;&#32456;&#20915;&#31574;&#36131;&#20219;&#30340;&#21516;&#26102;&#25509;&#25910;&#21040;&#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#30340;&#25512;&#33616;&#12290;&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#8220;&#20381;&#20174;&#24615;&#24863;&#30693;&#30340;Q&#23398;&#20064;&#8221;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#23398;&#20064;&#20102;&#8220;&#20381;&#20174;&#27700;&#24179;&#8221;&#65292;&#21363;&#25429;&#25417;HDM&#36981;&#24490;&#25512;&#33616;&#34892;&#21160;&#30340;&#39057;&#29575;&#65292;&#24182;&#23454;&#26102;&#25512;&#23548;&#20986;&#26368;&#20339;&#25512;&#33616;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios involving high-stakes and safety implications, a human decision-maker (HDM) may receive recommendations from an artificial intelligence while holding the ultimate responsibility of making decisions. In this letter, we develop an "adherence-aware Q-learning" algorithm to address this problem. The algorithm learns the "adherence level" that captures the frequency with which an HDM follows the recommended actions and derives the best recommendation policy in real time. We prove the convergence of the proposed Q-learning algorithm to the optimal value and evaluate its performance across various scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;GPT-4&#27169;&#22411;&#22312;&#21333;&#27425;&#27169;&#24335;&#19979;&#25552;&#20379;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#26469;&#35782;&#21035;COVID-19&#30123;&#33495;&#30456;&#20851;&#25512;&#25991;&#65292;&#24182;&#19982;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2309.06503</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#23545;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;:&#20197;COVID-19&#33258;&#25105;&#25253;&#21578;&#30340;&#30123;&#33495;&#25512;&#25991;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets. (arXiv:2309.06503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;GPT-4&#27169;&#22411;&#22312;&#21333;&#27425;&#27169;&#24335;&#19979;&#25552;&#20379;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#26469;&#35782;&#21035;COVID-19&#30123;&#33495;&#30456;&#20851;&#25512;&#25991;&#65292;&#24182;&#19982;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#32473;&#21307;&#30103;&#34892;&#19994;&#21644;&#25972;&#20010;&#31038;&#20250;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;COVID-19&#30123;&#33495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24050;&#25104;&#20026;&#35752;&#35770;&#30123;&#33495;&#30456;&#20851;&#35805;&#39064;&#30340;&#28909;&#38376;&#23186;&#20171;&#12290;&#35782;&#21035;&#30123;&#33495;&#30456;&#20851;&#25512;&#25991;&#24182;&#36827;&#34892;&#20998;&#26512;&#21487;&#20197;&#20026;&#20844;&#20849;&#21355;&#29983;&#30740;&#31350;&#20154;&#21592;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#27880;&#37322;&#22823;&#37327;&#25512;&#25991;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#20363;&#20013;&#20026;GPT-4&#65288;3&#26376;23&#26085;&#29256;&#26412;&#65289;&#65292;&#21644;&#24369;&#30417;&#30563;&#26469;&#35782;&#21035;COVID-19&#30123;&#33495;&#30456;&#20851;&#25512;&#25991;&#65292;&#30446;&#30340;&#26159;&#19982;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#25163;&#21160;&#31574;&#21010;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;GPT-4&#22312;&#21333;&#27425;&#27169;&#24335;&#19979;&#65288;&#26080;&#38656;&#39069;&#22806;&#25552;&#31034;&#65289;&#25552;&#20379;&#26631;&#31614;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#25351;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has presented significant challenges to the healthcare industry and society as a whole. With the rapid development of COVID-19 vaccines, social media platforms have become a popular medium for discussions on vaccine-related topics. Identifying vaccine-related tweets and analyzing them can provide valuable insights for public health research-ers and policymakers. However, manual annotation of a large number of tweets is time-consuming and expensive. In this study, we evaluate the usage of Large Language Models, in this case GPT-4 (March 23 version), and weak supervision, to identify COVID-19 vaccine-related tweets, with the purpose of comparing performance against human annotators. We leveraged a manu-ally curated gold-standard dataset and used GPT-4 to provide labels without any additional fine-tuning or instructing, in a single-shot mode (no additional prompting).
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;PyTorch&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#23454;&#29616;&#30340;&#20998;&#24067;&#24335;Shampoo&#20248;&#21270;&#31639;&#27861;&#12290;&#36825;&#31181;&#31639;&#27861;&#21033;&#29992;&#20102;&#22359;&#23545;&#35282;&#20808;&#39564;&#30697;&#38453;&#21644;&#25628;&#32034;&#26041;&#21521;&#30340;AllGather&#21407;&#35821;&#25805;&#20316;&#65292;&#22312;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#23545;&#35282;&#32553;&#25918;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20165;&#26377;&#26368;&#22810;10%&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2309.06497</link><description>&lt;p&gt;
PyTorch&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#30340;&#20998;&#24067;&#24335;Shampoo&#20248;&#21270;&#22120;&#23454;&#29616;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale. (arXiv:2309.06497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06497
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;PyTorch&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#23454;&#29616;&#30340;&#20998;&#24067;&#24335;Shampoo&#20248;&#21270;&#31639;&#27861;&#12290;&#36825;&#31181;&#31639;&#27861;&#21033;&#29992;&#20102;&#22359;&#23545;&#35282;&#20808;&#39564;&#30697;&#38453;&#21644;&#25628;&#32034;&#26041;&#21521;&#30340;AllGather&#21407;&#35821;&#25805;&#20316;&#65292;&#22312;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#23545;&#35282;&#32553;&#25918;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20165;&#26377;&#26368;&#22810;10%&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shampoo&#26159;&#19968;&#31181;&#22312;&#32447;&#21644;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#23646;&#20110;AdaGrad&#26041;&#27861;&#23478;&#26063;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#26500;&#24314;&#19968;&#20010;&#22359;&#23545;&#35282;&#20808;&#39564;&#30697;&#38453;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#30001;&#31070;&#32463;&#32593;&#32476;&#27599;&#20010;&#21442;&#25968;&#30340;&#20840;&#30697;&#38453;AdaGrad&#30340;&#31895;&#30053;Kronecker&#31215;&#36817;&#20284;&#26500;&#25104;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#23436;&#25972;&#25551;&#36848;&#65292;&#20197;&#21450;&#25105;&#20204;&#30340;&#23454;&#29616;&#21033;&#29992;PyTorch&#26469;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#36890;&#36807;PyTorch&#30340;DTensor&#25968;&#25454;&#32467;&#26500;&#26469;&#20998;&#24067;&#27599;&#20010;&#21442;&#25968;&#22359;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#65292;&#24182;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#23545;&#35745;&#31639;&#24471;&#21040;&#30340;&#25628;&#32034;&#26041;&#21521;&#36827;&#34892;AllGather&#21407;&#35821;&#25805;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#30340;&#22810;GPU&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#12290;&#36825;&#19968;&#20027;&#35201;&#24615;&#33021;&#25552;&#21319;&#20351;&#25105;&#20204;&#22312;&#27599;&#27493;&#22681;&#38047;&#26102;&#38388;&#19978;&#30456;&#27604;&#20110;&#26631;&#20934;&#30340;&#22522;&#20110;&#23545;&#35282;&#32553;&#25918;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#26368;&#22810;&#21482;&#26377;10%&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shampoo is an online and stochastic optimization algorithm belonging to the AdaGrad family of methods for training neural networks. It constructs a block-diagonal preconditioner where each block consists of a coarse Kronecker product approximation to full-matrix AdaGrad for each parameter of the neural network. In this work, we provide a complete description of the algorithm as well as the performance optimizations that our implementation leverages to train deep networks at-scale in PyTorch. Our implementation enables fast multi-GPU distributed data-parallel training by distributing the memory and computation associated with blocks of each parameter via PyTorch's DTensor data structure and performing an AllGather primitive on the computed search directions at each iteration. This major performance enhancement enables us to achieve at most a 10% performance reduction in per-step wall-clock time compared against standard diagonal-scaling-based adaptive gradient methods. We validate our i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#20110;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#38750;&#32467;&#26500;&#21270;&#32593;&#26684;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#33258;&#20027;&#23545;&#24328;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#20943;&#23567;&#33410;&#28857;&#24230;&#25968;&#19982;&#29702;&#24819;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#36798;&#21040;&#20943;&#23569;&#19981;&#35268;&#21017;&#33410;&#28857;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.06484</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#32593;&#26684;&#19978;&#30340;&#25299;&#25169;&#25805;&#20316;&#21450;&#20854;&#22312;&#22810;&#36793;&#24418;&#21306;&#22359;&#20998;&#35299;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning topological operations on meshes with application to block decomposition of polygons. (arXiv:2309.06484v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#20110;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#38750;&#32467;&#26500;&#21270;&#32593;&#26684;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#33258;&#20027;&#23545;&#24328;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#20943;&#23567;&#33410;&#28857;&#24230;&#25968;&#19982;&#29702;&#24819;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#36798;&#21040;&#20943;&#23569;&#19981;&#35268;&#21017;&#33410;&#28857;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#38750;&#32467;&#26500;&#21270;&#19977;&#35282;&#24418;&#21644;&#22235;&#36793;&#24418;&#32593;&#26684;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#33258;&#20027;&#23545;&#24328;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#25913;&#21892;&#32593;&#26684;&#36136;&#37327;&#65292;&#27809;&#26377;&#20808;&#39564;&#30340;&#21551;&#21457;&#20989;&#25968;&#12290;&#32593;&#26684;&#19978;&#36827;&#34892;&#30340;&#25805;&#20316;&#26159;&#26631;&#20934;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20803;&#32032;&#25805;&#20316;&#12290;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#33410;&#28857;&#24230;&#25968;&#19982;&#29702;&#24819;&#20540;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#22312;&#20869;&#37096;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#19981;&#35268;&#21017;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a learning based framework for mesh quality improvement on unstructured triangular and quadrilateral meshes. Our model learns to improve mesh quality according to a prescribed objective function purely via self-play reinforcement learning with no prior heuristics. The actions performed on the mesh are standard local and global element operations. The goal is to minimize the deviation of the node degrees from their ideal values, which in the case of interior vertices leads to a minimization of irregular nodes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"flows for flows"&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#35757;&#32451;&#35268;&#33539;&#21270;&#27969;&#23558;&#19968;&#20010;&#25968;&#25454;&#38598;&#21464;&#24418;&#25104;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21363;&#20351;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#27010;&#29575;&#23494;&#24230;&#37117;&#19981;&#26126;&#30830;&#12290;</title><link>http://arxiv.org/abs/2309.06472</link><description>&lt;p&gt;
&#25968;&#25454;&#27969;&#36716;&#65306;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23558;&#19968;&#20010;&#25968;&#25454;&#38598;&#21464;&#24418;&#25104;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Flows for Flows: Morphing one Dataset into another with Maximum Likelihood Estimation. (arXiv:2309.06472v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"flows for flows"&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#35757;&#32451;&#35268;&#33539;&#21270;&#27969;&#23558;&#19968;&#20010;&#25968;&#25454;&#38598;&#21464;&#24418;&#25104;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21363;&#20351;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#27010;&#29575;&#23494;&#24230;&#37117;&#19981;&#26126;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#33021;&#29289;&#29702;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#35768;&#22810;&#32452;&#20214;&#38656;&#35201;&#23558;&#19968;&#20010;&#25968;&#25454;&#38598;&#21464;&#24418;&#25104;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#36890;&#24120;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#26469;&#35299;&#20915;&#65292;&#20294;&#20445;&#30041;&#26435;&#37325;&#24182;&#31227;&#21160;&#25968;&#25454;&#28857;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#35268;&#33539;&#21270;&#27969;&#26159;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31934;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#31890;&#23376;&#29289;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#20294;&#35268;&#33539;&#21270;&#27969;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#25968;&#25454;&#27969;&#36716;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23545;&#36215;&#22987;&#25968;&#25454;&#38598;&#30340;&#27010;&#29575;&#23494;&#24230;&#26377;&#25152;&#20102;&#35299;&#12290;&#22312;&#22823;&#22810;&#25968;&#31890;&#23376;&#29289;&#29702;&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#26356;&#22810;&#31034;&#20363;&#65292;&#20294;&#25105;&#20204;&#24182;&#19981;&#26126;&#30830;&#30693;&#36947;&#23494;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;"flows for flows"&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#35757;&#32451;&#35268;&#33539;&#21270;&#27969;&#23558;&#19968;&#20010;&#25968;&#25454;&#38598;&#21464;&#24418;&#25104;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21363;&#20351;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#27010;&#29575;&#23494;&#24230;&#37117;&#19981;&#26126;&#30830;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#35757;&#32451;&#21464;&#24418;&#31574;&#30053;&#65292;&#32780;&#35813;&#35774;&#32622;&#24050;&#35777;&#26126;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#21327;&#35758;&#30340;&#22810;&#31181;&#21464;&#20307;&#65292;&#20197;&#25506;&#32034;&#25968;&#25454;&#27969;&#36716;&#30340;&#26497;&#38480;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many components of data analysis in high energy physics and beyond require morphing one dataset into another. This is commonly solved via reweighting, but there are many advantages of preserving weights and shifting the data points instead. Normalizing flows are machine learning models with impressive precision on a variety of particle physics tasks. Naively, normalizing flows cannot be used for morphing because they require knowledge of the probability density of the starting dataset. In most cases in particle physics, we can generate more examples, but we do not know densities explicitly. We propose a protocol called flows for flows for training normalizing flows to morph one dataset into another even if the underlying probability density of neither dataset is known explicitly. This enables a morphing strategy trained with maximum likelihood estimation, a setup that has been shown to be highly effective in related tasks. We study variations on this protocol to explore how far the dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.06453</link><description>&lt;p&gt;
&#32553;&#23567;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#24046;&#36317;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model. (arXiv:2309.06453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#21477;&#23376;&#23884;&#20837;&#65288;CSE&#65289;&#20316;&#20026;&#20027;&#27969;&#25216;&#26415;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;CSE&#20013;&#26377;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#21363;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#30456;&#21516;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#22238;&#31572;&#8220;&#21457;&#29983;&#20102;&#20160;&#20040;&#23548;&#33268;&#20102;&#24615;&#33021;&#24046;&#36317;&#8221;&#21644;&#8220;&#22914;&#20309;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#8221;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24443;&#24213;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;CSE&#22312;&#21508;&#33258;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#26469;&#22238;&#31572;&#8220;&#21457;&#29983;&#20102;&#20160;&#20040;&#8221;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with Contrastive learning of Sentence Embeddings (CSE) as the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, even when their sentence encoder and loss function are the same. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, alignment and uniformity only measure the results, which means they cannot answer "What happens during the training process that leads to the performance gap?" and "How can the performance gap be narrowed?". In this paper, we conduct empirical experiments to answer these "What" and "How" questions. We first answer the "What" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, We o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21270;&#30340;&#38750;&#25381;&#21457;&#24615;&#32435;&#31859;&#30913;&#38459;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#25928;&#26080;&#30417;&#30563;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#20302;&#20998;&#36776;&#29575;&#30340;&#33258;&#32534;&#30721;&#22120;&#21644;&#26377;&#25928;&#30340;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#29305;&#27530;&#30340;&#30913;&#24615;&#22495;&#22721;&#20316;&#20026;&#31361;&#35302;&#65292;&#36890;&#36807;&#33258;&#26059;&#36712;&#36947;&#21147;&#30005;&#27969;&#33033;&#20914;&#25805;&#32437;&#31361;&#35302;&#26435;&#37325;&#65292;&#24182;&#22312;NSL-KDD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24471;&#21040;&#20102;&#19982;&#28014;&#28857;&#31934;&#24230;&#26435;&#37325;&#33258;&#32534;&#30721;&#22120;&#30456;&#24403;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06449</link><description>&lt;p&gt;
&#37327;&#23376;&#21270;&#30340;&#38750;&#25381;&#21457;&#24615;&#32435;&#31859;&#30913;&#38459;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#39640;&#25928;&#26080;&#30417;&#30563;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quantized Non-Volatile Nanomagnetic Synapse based Autoencoder for Efficient Unsupervised Network Anomaly Detection. (arXiv:2309.06449v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21270;&#30340;&#38750;&#25381;&#21457;&#24615;&#32435;&#31859;&#30913;&#38459;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#25928;&#26080;&#30417;&#30563;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#20302;&#20998;&#36776;&#29575;&#30340;&#33258;&#32534;&#30721;&#22120;&#21644;&#26377;&#25928;&#30340;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#29305;&#27530;&#30340;&#30913;&#24615;&#22495;&#22721;&#20316;&#20026;&#31361;&#35302;&#65292;&#36890;&#36807;&#33258;&#26059;&#36712;&#36947;&#21147;&#30005;&#27969;&#33033;&#20914;&#25805;&#32437;&#31361;&#35302;&#26435;&#37325;&#65292;&#24182;&#22312;NSL-KDD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24471;&#21040;&#20102;&#19982;&#28014;&#28857;&#31934;&#24230;&#26435;&#37325;&#33258;&#32534;&#30721;&#22120;&#30456;&#24403;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#24322;&#24120;&#26816;&#27979;&#33539;&#24335;&#20013;&#65292;&#30001;&#20110;&#30828;&#20214;&#12289;&#33021;&#28304;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#65292;&#23558;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#22312;&#33021;&#22815;&#23454;&#26102;&#23398;&#20064;&#30340;&#36793;&#32536;&#35774;&#22791;&#20013;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#20302;&#20998;&#36776;&#29575;&#30340;&#38750;&#25381;&#21457;&#24615;&#20869;&#23384;&#22522;&#30784;&#31361;&#35302;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#24037;&#31243;&#32570;&#21475;&#30340;&#38081;&#30913;&#36187;&#36947;&#65292;&#20854;&#20013;&#25215;&#36733;&#30528;&#19968;&#20010;&#30913;&#24615;&#22495;&#22721;(DW)&#20316;&#20026;&#33258;&#32534;&#30721;&#22120;&#30340;&#31361;&#35302;&#65292;&#22312;&#36825;&#37324;&#36890;&#36807;&#33258;&#26059;&#36712;&#36947;&#21147;(SOT)&#30005;&#27969;&#33033;&#20914;&#25805;&#32437;&#26377;&#38480;&#29366;&#24577;(5&#29366;&#24577;)&#30340;&#31361;&#35302;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;NSL-KDD&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;&#36827;&#34892;&#20102;&#26377;&#38480;&#20998;&#36776;&#29575;&#21644;DW&#22120;&#20214;&#38543;&#26426;&#24615;&#24863;&#30693;&#35757;&#32451;&#65292;&#20854;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#19982;&#20855;&#26377;&#28014;&#28857;&#31934;&#24230;&#26435;&#37325;&#30340;&#33258;&#32534;&#30721;&#22120;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the autoencoder based anomaly detection paradigm, implementing the autoencoder in edge devices capable of learning in real-time is exceedingly challenging due to limited hardware, energy, and computational resources. We show that these limitations can be addressed by designing an autoencoder with low-resolution non-volatile memory-based synapses and employing an effective quantized neural network learning algorithm. We propose a ferromagnetic racetrack with engineered notches hosting a magnetic domain wall (DW) as the autoencoder synapses, where limited state (5-state) synaptic weights are manipulated by spin orbit torque (SOT) current pulses. The performance of anomaly detection of the proposed autoencoder model is evaluated on the NSL-KDD dataset. Limited resolution and DW device stochasticity aware training of the autoencoder is performed, which yields comparable anomaly detection performance to the autoencoder having floating-point precision weights. While the limited number of 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#28369;&#22369;&#26131;&#21457;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#37325;&#35201;&#30340;&#36129;&#29486;&#22240;&#32032;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06062</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#28369;&#22369;&#26131;&#21457;&#24615;&#26102;&#30340;&#36129;&#29486;&#22240;&#32032;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Selection of contributing factors for predicting landslide susceptibility using machine learning and deep learning models. (arXiv:2309.06062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06062
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#28369;&#22369;&#26131;&#21457;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#37325;&#35201;&#30340;&#36129;&#29486;&#22240;&#32032;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28369;&#22369;&#26159;&#24120;&#35265;&#30340;&#33258;&#28982;&#28798;&#23475;&#65292;&#21487;&#33021;&#23548;&#33268;&#20154;&#21592;&#20260;&#20129;&#12289;&#36130;&#20135;&#23433;&#20840;&#23041;&#32961;&#21644;&#32463;&#27982;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#25110;&#39044;&#27979;&#28508;&#22312;&#39118;&#38505;&#22320;&#28857;&#30340;&#28369;&#22369;&#21457;&#29983;&#27010;&#29575;&#38750;&#24120;&#37325;&#35201;&#12290;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#28369;&#22369;&#28165;&#21333;&#21644;&#19968;&#32452;&#28369;&#22369;&#36129;&#29486;&#22240;&#32032;&#36827;&#34892;&#28369;&#22369;&#26131;&#21457;&#24615;&#35780;&#20272;&#65292;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#25110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#35760;&#24518;&#65289;&#26469;&#23454;&#29616;&#12290;&#20316;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#28369;&#22369;&#36129;&#29486;&#22240;&#32032;&#23545;&#28369;&#22369;&#21457;&#29983;&#26377;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26377;&#36923;&#36753;&#30340;&#36873;&#25321;&#26356;&#37325;&#35201;&#30340;&#36129;&#29486;&#22240;&#32032;&#24182;&#28040;&#38500;&#19981;&#30456;&#20851;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Landslides are a common natural disaster that can cause casualties, property safety threats and economic losses. Therefore, it is important to understand or predict the probability of landslide occurrence at potentially risky sites. A commonly used means is to carry out a landslide susceptibility assessment based on a landslide inventory and a set of landslide contributing factors. This can be readily achieved using machine learning (ML) models such as logistic regression (LR), support vector machine (SVM), random forest (RF), extreme gradient boosting (Xgboost), or deep learning (DL) models such as convolutional neural network (CNN) and long short time memory (LSTM). As the input data for these models, landslide contributing factors have varying influences on landslide occurrence. Therefore, it is logically feasible to select more important contributing factors and eliminate less relevant ones, with the aim of increasing the prediction accuracy of these models. However, selecting more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.05605</link><description>&lt;p&gt;
&#20869;&#23384;&#27880;&#20837;&#65306;&#22312;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#20013;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. (arXiv:2309.05605v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;Transformer-Based&#35821;&#35328;&#27169;&#22411;&#30340;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#23450;&#21521;&#27880;&#20837;&#20869;&#23384;&#26469;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#22810;&#36339;&#25512;&#29702;&#38382;&#39064;&#38656;&#35201;&#20174;&#22810;&#20010;&#20449;&#24687;&#28304;&#20013;&#26816;&#32034;&#21644;&#32508;&#21512;&#20449;&#24687;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24448;&#24448;&#38590;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;LLM&#27880;&#24847;&#21147;&#22836;&#37096;&#36827;&#34892;&#23450;&#21521;&#20869;&#23384;&#27880;&#20837;&#26469;&#30830;&#23450;&#21644;&#32416;&#27491;&#22810;&#36339;&#25512;&#29702;&#38169;&#35823;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GPT-2&#27169;&#22411;&#22312;&#21333;&#36339;&#21644;&#22810;&#36339;&#25552;&#31034;&#19979;&#21508;&#23618;&#30340;&#28608;&#27963;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21521;&#20851;&#38190;LLM&#20301;&#32622;&#27880;&#20837;&#30456;&#20851;&#30340;&#25552;&#31034;&#29305;&#23450;&#20449;&#24687;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#35760;&#24518;&#8221;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;LLM&#33021;&#22815;&#25972;&#21512;&#39069;&#22806;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22810;&#36339;&#25552;&#31034;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#23558;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#23450;&#21521;&#30340;&#35760;&#24518;&#27880;&#20837;&#21040;&#20851;&#38190;&#27880;&#24847;&#21147;&#23618;&#20013;&#24448;&#24448;&#33021;&#22815;&#25552;&#39640;&#22810;&#36339;&#20219;&#21153;&#20013;&#25152;&#38656;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#25552;&#39640;&#20102;&#36798;&#21040;424%&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#26377;&#38480;&#24046;&#20998;&#31163;&#25955;&#21270;&#23478;&#26063;&#65292;&#22312;&#38750;&#22343;&#21248;&#25193;&#25955;&#36807;&#31243;&#20013;&#36890;&#36807;&#23558;&#20108;&#32500;&#25193;&#25955;&#20998;&#35299;&#20026;&#22235;&#20010;&#19968;&#32500;&#25193;&#25955;&#24471;&#20986;&#20102;&#19968;&#20010;3 x 3&#30340;&#27169;&#26495;&#12290;&#35813;&#27169;&#26495;&#31867;&#21253;&#21547;&#19968;&#20010;&#33258;&#30001;&#21442;&#25968;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#29616;&#26377;&#31163;&#25955;&#21270;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#24314;&#31435;&#20102;&#19982;&#27169;&#26495;&#30456;&#23545;&#24212;&#30340;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#19978;&#30028;&#65292;&#24182;&#23558;&#26174;&#24335;&#26041;&#26696;&#36716;&#21270;&#20026;ResNet&#22359;&#12290;</title><link>http://arxiv.org/abs/2309.05575</link><description>&lt;p&gt;
&#19968;&#20010;&#31034;&#20363;&#39064;&#24418;&#30340;&#38750;&#22343;&#21248;&#25193;&#25955;&#27169;&#26495;: &#20174;&#31616;&#21333;&#25512;&#23548;&#21040;&#31283;&#23450;&#24615;&#35780;&#20272;&#20877;&#21040;ResNet&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations. (arXiv:2309.05575v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05575
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#26377;&#38480;&#24046;&#20998;&#31163;&#25955;&#21270;&#23478;&#26063;&#65292;&#22312;&#38750;&#22343;&#21248;&#25193;&#25955;&#36807;&#31243;&#20013;&#36890;&#36807;&#23558;&#20108;&#32500;&#25193;&#25955;&#20998;&#35299;&#20026;&#22235;&#20010;&#19968;&#32500;&#25193;&#25955;&#24471;&#20986;&#20102;&#19968;&#20010;3 x 3&#30340;&#27169;&#26495;&#12290;&#35813;&#27169;&#26495;&#31867;&#21253;&#21547;&#19968;&#20010;&#33258;&#30001;&#21442;&#25968;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#29616;&#26377;&#31163;&#25955;&#21270;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#24314;&#31435;&#20102;&#19982;&#27169;&#26495;&#30456;&#23545;&#24212;&#30340;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#19978;&#30028;&#65292;&#24182;&#23558;&#26174;&#24335;&#26041;&#26696;&#36716;&#21270;&#20026;ResNet&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#22343;&#21248;&#25193;&#25955;&#36807;&#31243;&#19982;&#25193;&#25955;&#24352;&#37327;&#22312;&#22270;&#20687;&#20998;&#26512;&#12289;&#29289;&#29702;&#23398;&#21644;&#24037;&#31243;&#23398;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25968;&#20540;&#36817;&#20284;&#23545;&#32791;&#25955;&#24615;&#20266;&#24433;&#21644;&#19982;&#26059;&#36716;&#19981;&#21464;&#24615;&#20559;&#31163;&#26377;&#24378;&#28872;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#26377;&#38480;&#24046;&#20998;&#31163;&#25955;&#21270;&#23478;&#26063;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;3 x 3&#30340;&#27169;&#26495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20108;&#32500;&#38750;&#22343;&#21248;&#25193;&#25955;&#20998;&#35299;&#20026;&#22235;&#20010;&#19968;&#32500;&#25193;&#25955;&#26469;&#25512;&#23548;&#20986;&#23427;&#12290;&#32467;&#26524;&#30340;&#27169;&#26495;&#31867;&#21253;&#21547;&#19968;&#20010;&#33258;&#30001;&#21442;&#25968;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#29616;&#26377;&#31163;&#25955;&#21270;&#12290;&#23427;&#21253;&#25324;Weickert&#31561;&#20154;(2013)&#30340;&#23436;&#25972;&#27169;&#26495;&#23478;&#26063;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#30340;&#20004;&#20010;&#21442;&#25968;&#21253;&#21547;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19982;&#27169;&#26495;&#30456;&#23545;&#24212;&#30340;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#19978;&#30028;&#12290;&#36825;&#32473;&#20986;&#20102;&#22312;&#27431;&#20960;&#37324;&#24471;&#33539;&#25968;&#20013;&#20445;&#35777;&#26174;&#24335;&#26041;&#26696;&#31283;&#23450;&#24615;&#30340;&#26102;&#38388;&#27493;&#38271;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#21521;&#20998;&#21106;&#36824;&#20801;&#35768;&#23558;&#26174;&#24335;&#26041;&#26696;&#38750;&#24120;&#33258;&#28982;&#22320;&#36716;&#21270;&#20026;ResNet&#22359;&#12290;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#24211;&#23454;&#29616;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anisotropic diffusion processes with a diffusion tensor are important in image analysis, physics, and engineering. However, their numerical approximation has a strong impact on dissipative artefacts and deviations from rotation invariance. In this work, we study a large family of finite difference discretisations on a 3 x 3 stencil. We derive it by splitting 2-D anisotropic diffusion into four 1-D diffusions. The resulting stencil class involves one free parameter and covers a wide range of existing discretisations. It comprises the full stencil family of Weickert et al. (2013) and shows that their two parameters contain redundancy. Furthermore, we establish a bound on the spectral norm of the matrix corresponding to the stencil. This gives time step size limits that guarantee stability of an explicit scheme in the Euclidean norm. Our directional splitting also allows a very natural translation of the explicit scheme into ResNet blocks. Employing neural network libraries enables simple
&lt;/p&gt;</description></item><item><title>NExT-GPT&#26159;&#19968;&#20010;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25509;&#21463;&#21644;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2309.05519</link><description>&lt;p&gt;
NExT-GPT: &#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NExT-GPT: Any-to-Any Multimodal LLM. (arXiv:2309.05519v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05519
&lt;/p&gt;
&lt;p&gt;
NExT-GPT&#26159;&#19968;&#20010;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25509;&#21463;&#21644;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLM&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#25391;&#22859;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23384;&#22312;&#19968;&#20010;&#38480;&#21046;&#65292;&#21363;&#21482;&#33021;&#22312;&#36755;&#20837;&#31471;&#36827;&#34892;&#22810;&#27169;&#24577;&#29702;&#35299;&#65292;&#26080;&#27861;&#20197;&#22810;&#31181;&#27169;&#24335;&#29983;&#25104;&#20869;&#23481;&#12290;&#30001;&#20110;&#25105;&#20204;&#20154;&#31867;&#24635;&#26159;&#36890;&#36807;&#21508;&#31181;&#27169;&#24577;&#24863;&#30693;&#19990;&#30028;&#21644;&#19982;&#20154;&#20132;&#27969;&#65292;&#22240;&#27492;&#24320;&#21457;&#33021;&#22815;&#25509;&#21463;&#21644;&#20256;&#36882;&#20219;&#20309;&#27169;&#24577;&#20869;&#23481;&#30340;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;MM-LLM&#31995;&#32479;&#23545;&#20110;&#23454;&#29616;&#20154;&#32423;AI&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;NExT-GPT&#12290;&#25105;&#20204;&#36890;&#36807;&#36830;&#25509;&#19968;&#20010;&#21547;&#26377;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#30340;LLM&#65292;&#20351;&#24471;NExT-GPT&#33021;&#22815;&#20197;&#20219;&#24847;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#36827;&#34892;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#35757;&#32451;&#26377;&#32032;&#30340;&#39640;&#24615;&#33021;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;NExT-GPT&#20165;&#36890;&#36807;&#35843;&#25972;&#26576;&#20123;&#25237;&#24433;&#23618;&#30340;&#23569;&#37327;&#21442;&#25968;&#65288;1%&#65289;&#36827;&#34892;&#35843;&#20248;&#65292;&#36825;&#19981;&#20165;&#26377;&#21033;&#20110;&#20302;&#25104;&#26412;&#35757;&#32451;&#65292;&#36824;&#26377;&#21161;&#20110;&#26041;&#20415;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#32534;&#30721;&#22120;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#22330;&#26223;&#34920;&#31034;&#21644;&#32463;&#20856;&#30340;&#26629;&#26684;&#21270;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#20102;&#25551;&#36848;&#24615;&#30340;&#22330;&#26223;&#23884;&#20837;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.05282</link><description>&lt;p&gt;
&#21487;&#20197;&#36890;&#36807;&#30701;&#20449;&#20256;&#36755;&#21457;&#29983;&#30340;&#20107;&#24773;&#21527;&#65311;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#32534;&#30721;&#22120;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving. (arXiv:2309.05282v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#32534;&#30721;&#22120;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#22330;&#26223;&#34920;&#31034;&#21644;&#32463;&#20856;&#30340;&#26629;&#26684;&#21270;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#20102;&#25551;&#36848;&#24615;&#30340;&#22330;&#26223;&#23884;&#20837;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#65292;&#22330;&#26223;&#29702;&#35299;&#26159;&#39044;&#27979;&#21608;&#22260;&#20132;&#36890;&#21442;&#19982;&#32773;&#26410;&#26469;&#34892;&#20026;&#30340;&#31532;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#34920;&#31034;&#32473;&#23450;&#30340;&#22330;&#26223;&#24182;&#25552;&#21462;&#20854;&#29305;&#24449;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#36890;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#36827;&#34892;&#22788;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#34920;&#31034;&#19982;&#20256;&#32479;&#30340;&#26629;&#26684;&#21270;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#24471;&#21040;&#25551;&#36848;&#24615;&#30340;&#22330;&#26223;&#23884;&#20837;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#39044;&#27979;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#65292;&#25991;&#26412;&#21644;&#26629;&#26684;&#21270;&#22270;&#20687;&#30340;&#32852;&#21512;&#32534;&#30721;&#22120;&#32988;&#36807;&#21333;&#29420;&#30340;&#32534;&#30721;&#22120;&#65292;&#30830;&#35748;&#20102;&#20004;&#31181;&#34920;&#31034;&#20855;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving tasks, scene understanding is the first step towards predicting the future behavior of the surrounding traffic participants. Yet, how to represent a given scene and extract its features are still open research questions. In this study, we propose a novel text-based representation of traffic scenes and process it with a pre-trained language encoder.  First, we show that text-based representations, combined with classical rasterized image representations, lead to descriptive scene embeddings. Second, we benchmark our predictions on the nuScenes dataset and show significant improvements compared to baselines. Third, we show in an ablation study that a joint encoder of text and rasterized images outperforms the individual encoders confirming that both representations have their complementary strengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#19968;&#31995;&#21015;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#36890;&#36807;&#22312;&#19981;&#26029;&#22024;&#26434;&#21270;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#23450;&#20041;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;EBMs&#65292;&#24182;&#19982;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#21327;&#21516;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20851;&#38381;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#20043;&#38388;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.05153</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#23398;&#20064;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood. (arXiv:2309.05153v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#19968;&#31995;&#21015;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#36890;&#36807;&#22312;&#19981;&#26029;&#22024;&#26434;&#21270;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#23450;&#20041;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;EBMs&#65292;&#24182;&#19982;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#21327;&#21516;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20851;&#38381;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#20043;&#38388;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#25968;&#25454;&#19978;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#35757;&#32451;&#33021;&#37327;&#22522;&#20934;&#27169;&#22411;&#65288;EBMs&#65289;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#36739;&#38271;&#12290;&#22240;&#27492;&#65292;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#65288;&#22914;GANs&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#21463;&#26368;&#36817;&#36890;&#36807;&#26368;&#22823;&#21270;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;DRL&#65289;&#26469;&#23398;&#20064;EBMs&#30340;&#21162;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#21487;&#34892;&#22320;&#23398;&#20064;&#21644;&#20174;&#19968;&#31995;&#21015;EBMs&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#20123;EBMs&#23450;&#20041;&#22312;&#36234;&#26469;&#36234;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#65292;&#24182;&#19982;&#27599;&#20010;EBM&#30340;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#12290;&#22312;&#27599;&#20010;&#22122;&#22768;&#27700;&#24179;&#19978;&#65292;&#21021;&#22987;&#21270;&#27169;&#22411;&#23398;&#20064;&#22312;EBM&#30340;&#37319;&#26679;&#36807;&#31243;&#20013;&#20998;&#25674;&#65292;&#32780;&#20004;&#20010;&#27169;&#22411;&#22312;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#20869;&#20849;&#21516;&#20272;&#35745;&#12290;&#21021;&#22987;&#21270;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#20316;&#20026;&#36215;&#22987;&#28857;&#65292;&#32463;&#36807;EBM&#30340;&#20960;&#20010;&#37319;&#26679;&#27493;&#39588;&#36827;&#34892;&#25913;&#36827;&#12290;&#36890;&#36807;&#25913;&#36827;&#21518;&#30340;&#26679;&#26412;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24674;&#22797;&#20284;&#28982;&#26469;&#20248;&#21270;EBM&#12290;
&lt;/p&gt;
&lt;p&gt;
Training energy-based models (EBMs) with maximum likelihood estimation on high-dimensional data can be both challenging and time-consuming. As a result, there a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximimizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versons of a dataset, paired with an initializer model for each EBM. At each noise level, the initializer model learns to amortize the sampling process of the EBM, and the two models are jointly estimated within a cooperative training framework. Samples from the initializer serve as starting points that are refined by a few sampling steps from the EBM. With the refined samples, the EBM is optimized by maximizing recovery likelihood, while t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#26356;&#26032;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#21033;&#29992;&#20102;&#20449;&#24687;&#35770;&#25216;&#26415;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#32500;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30456;&#31561;&#30340;&#24773;&#20917;&#19979;&#65292;&#30028;&#38480;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2309.05077</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#30028;&#26356;&#26032;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization error bounds for iterative learning algorithms with bounded updates. (arXiv:2309.05077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#26356;&#26032;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#21033;&#29992;&#20102;&#20449;&#24687;&#35770;&#25216;&#26415;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#32500;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30456;&#31561;&#30340;&#24773;&#20917;&#19979;&#65292;&#30028;&#38480;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20855;&#26377;&#26377;&#30028;&#26356;&#26032;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#37319;&#29992;&#20102;&#20449;&#24687;&#35770;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#38024;&#23545;&#20855;&#26377;&#26377;&#30028;&#26356;&#26032;&#30340;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#36229;&#20986;&#20102;&#20197;&#21069;&#21482;&#20851;&#27880;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#21019;&#26032;&#20043;&#22788;&#65306;1&#65289;&#25105;&#20204;&#23558;&#20114;&#20449;&#24687;&#37325;&#26032;&#23450;&#20041;&#20026;&#26356;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65307;2&#65289;&#25105;&#20204;&#19981;&#20351;&#29992;&#20114;&#20449;&#24687;&#30340;&#38142;&#24335;&#27861;&#21017;&#65292;&#32780;&#26159;&#37319;&#29992;&#26041;&#24046;&#20998;&#35299;&#25216;&#26415;&#26469;&#23558;&#20449;&#24687;&#20998;&#35299;&#21040;&#36845;&#20195;&#20013;&#65292;&#20174;&#32780;&#20801;&#35768;&#31616;&#21270;&#30340;&#20195;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#22312;&#27169;&#22411;&#32500;&#24230;&#20197;&#19982;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30456;&#21516;&#30340;&#36895;&#29575;&#22686;&#21152;&#26102;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#30028;&#38480;&#12290;&#20026;&#20102;&#24357;&#21512;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the generalization characteristics of iterative learning algorithms with bounded updates for non-convex loss functions, employing information-theoretic techniques. Our key contribution is a novel bound for the generalization error of these algorithms with bounded updates, extending beyond the scope of previous works that only focused on Stochastic Gradient Descent (SGD). Our approach introduces two main novelties: 1) we reformulate the mutual information as the uncertainty of updates, providing a new perspective, and 2) instead of using the chaining rule of mutual information, we employ a variance decomposition technique to decompose information across iterations, allowing for a simpler surrogate process. We analyze our generalization bound under various settings and demonstrate improved bounds when the model dimension increases at the same rate as the number of training data samples. To bridge the gap between theory and practice, we also examine the previously obse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#29289;&#29702;&#20449;&#24687;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#21644;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#26469;&#33719;&#21462;&#26368;&#36866;&#24403;&#30340;&#21453;&#23545;&#35282;&#39033;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2309.04434</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26368;&#20248;&#21453;&#23545;&#35282;&#37327;&#23376;&#35745;&#31639;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation. (arXiv:2309.04434v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#29289;&#29702;&#20449;&#24687;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#21644;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#26469;&#33719;&#21462;&#26368;&#36866;&#24403;&#30340;&#21453;&#23545;&#35282;&#39033;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#20248;&#21183;&#26469;&#35299;&#20915;&#30001;$N_{Q}$&#27604;&#29305;&#31995;&#32479;&#32452;&#25104;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#30340;&#21453;&#23545;&#35282;&#65288;CD&#65289;&#21327;&#35758;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#31934;&#30830;&#22320;&#35299;&#20915;&#37327;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#29289;&#29702;&#21487;&#35266;&#27979;&#37327;&#30340;&#26102;&#38388;&#28436;&#21270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#24517;&#35201;&#30340;&#29289;&#29702;&#20449;&#24687;&#23884;&#20837;&#21040;&#24213;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23545;&#25152;&#26377;&#29289;&#29702;&#21487;&#35266;&#27979;&#37327;&#26045;&#21152;&#21380;&#31859;&#29305;&#24615;&#26465;&#20214;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20316;&#29992;&#37327;&#21407;&#29702;&#65292;&#20445;&#35777;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26368;&#36866;&#24403;&#21453;&#23545;&#35282;&#39033;&#30340;&#33719;&#21462;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#36873;&#25321;&#26469;&#35299;&#20915;CD&#39537;&#21160;&#38382;&#39064;&#65292;&#25670;&#33073;&#20102;&#20197;&#24448;&#20381;&#36182;&#20110;&#32463;&#20856;&#25968;&#20540;&#36924;&#36817;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel methodology that leverages the strength of Physics-Informed Neural Networks (PINNs) to address the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits. The primary objective is to utilize physics-inspired deep learning techniques to accurately solve the time evolution of the different physical observables within the quantum system. To accomplish this objective, we embed the necessary physical information into an underlying neural network to effectively tackle the problem. In particular, we impose the hermiticity condition on all physical observables and make use of the principle of least action, guaranteeing the acquisition of the most appropriate counterdiabatic terms based on the underlying physics. The proposed approach offers a dependable alternative to address the CD driving problem, free from the constraints typically encountered in previous methodologies relying on classical numerical approximations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04339</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20984;&#20248;&#21270;&#23454;&#29616;&#22312;&#32447;&#23376;&#27169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#22312;&#19968;&#33324;&#24615;&#27169;&#24615;&#32422;&#26463;&#19979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#20248;&#21270;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#65292;&#21363;&#21152;&#26435;&#38408;&#20540;&#21183;&#20989;&#25968;&#65292;&#21487;&#20197;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;(OCO)&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20010;&#31867;&#21035;&#30340;&#20989;&#25968;&#21487;&#20197;&#36827;&#34892;&#20985;&#26494;&#24347;;&#22240;&#27492;&#65292;&#32467;&#21512;&#36866;&#24403;&#30340;&#33293;&#20837;&#26041;&#26696;&#65292;OCO&#31574;&#30053;&#21487;&#20197;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31616;&#21270;&#26041;&#24335;&#21487;&#20197;&#24212;&#29992;&#22312;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#21160;&#24577;&#36951;&#25022;&#12289;&#24378;&#30423;&#21644;&#20048;&#35266;&#23398;&#20064;&#31561;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03581</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#22312;&#22810;&#30446;&#26631;&#38382;&#39064;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#23545;&#20110;&#21457;&#25381;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#29992;&#25143;&#36890;&#24120;&#23545;&#22810;&#30446;&#26631;&#38382;&#39064;&#24863;&#20852;&#36259;&#65292;&#21363;&#20248;&#21270;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#27604;&#22914;&#20934;&#30830;&#24615;&#21644;&#33021;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32477;&#22823;&#22810;&#25968;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#36820;&#22238;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#36825;&#31181;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35780;&#20272;&#19968;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#28041;&#21450;&#35780;&#20272;&#24471;&#21040;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#26377;&#19968;&#20123;&#25351;&#26631;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#23646;&#24615;&#65288;&#22914;&#20307;&#31215;&#12289;&#19982;&#21442;&#32771;&#28857;&#30340;&#25509;&#36817;&#31243;&#24230;&#65289;&#26469;&#35780;&#20272;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#65288;&#20363;&#22914;&#36229;&#20307;&#31215;&#12289;R2&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#65292;&#36873;&#25321;&#23548;&#33268;&#26399;&#26395;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25351;&#26631;&#21487;&#33021;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#38024;&#23545;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#22270;&#28909;&#26041;&#31243;&#30340;&#26102;&#38388;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33410;&#28857;&#29305;&#24449;&#28165;&#26224;&#24230;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#28909;&#26680;&#28388;&#27874;&#20989;&#25968;&#65292;&#22686;&#24378;&#20102;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36827;&#19968;&#27493;&#25512;&#24191;&#20026;G-MHKG&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#26356;&#28789;&#27963;&#30340;&#28388;&#27874;&#26465;&#20214;&#65292;&#20197;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#31561;&#35745;&#31639;&#25361;&#25112;&#12290;&#36825;&#20123;&#26041;&#27861;&#21644;&#27169;&#22411;&#22312;&#22686;&#24378;GNNs&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02769</link><description>&lt;p&gt;
&#32479;&#19968;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#65306;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#26041;&#27861;&#21644;&#26356;&#22810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Unifying over-smoothing and over-squashing in graph neural networks: A physics informed approach and beyond. (arXiv:2309.02769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#22270;&#28909;&#26041;&#31243;&#30340;&#26102;&#38388;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33410;&#28857;&#29305;&#24449;&#28165;&#26224;&#24230;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#28909;&#26680;&#28388;&#27874;&#20989;&#25968;&#65292;&#22686;&#24378;&#20102;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36827;&#19968;&#27493;&#25512;&#24191;&#20026;G-MHKG&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#26356;&#28789;&#27963;&#30340;&#28388;&#27874;&#26465;&#20214;&#65292;&#20197;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#31561;&#35745;&#31639;&#25361;&#25112;&#12290;&#36825;&#20123;&#26041;&#27861;&#21644;&#27169;&#22411;&#22312;&#22686;&#24378;GNNs&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#39046;&#20808;&#26041;&#27861;&#20043;&#19968;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;GNNs&#20173;&#28982;&#38754;&#20020;&#30528;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;&#31561;&#20851;&#38190;&#35745;&#31639;&#25361;&#25112;&#65292;&#36825;&#20123;&#38382;&#39064;&#20250;&#24433;&#21709;GNNs&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#20174;&#32463;&#20856;&#21644;&#37327;&#23376;&#29289;&#29702;&#20013;&#24120;&#29992;&#30340;&#26102;&#38388;&#21453;&#28436;&#21407;&#29702;&#24471;&#21040;&#21551;&#21457;&#65292;&#23558;&#22270;&#28909;&#26041;&#31243;&#30340;&#26102;&#38388;&#26041;&#21521;&#36827;&#34892;&#21453;&#36716;&#65292;&#24471;&#21040;&#20102;&#19968;&#31867;&#39640;&#36890;&#28388;&#27874;&#20989;&#25968;&#65292;&#21487;&#20197;&#22686;&#24378;&#22270;&#33410;&#28857;&#29305;&#24449;&#30340;&#28165;&#26224;&#24230;&#12290;&#22522;&#20110;&#36825;&#20010;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#23610;&#24230;&#28909;&#26680;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MHKG&#65289;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#31181;&#28388;&#27874;&#20989;&#25968;&#23545;&#33410;&#28857;&#29305;&#24449;&#30340;&#24433;&#21709;&#26469;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#20102;&#25506;&#32034;&#26356;&#28789;&#27963;&#30340;&#28388;&#27874;&#26465;&#20214;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;MHKG&#25512;&#24191;&#21040;&#19968;&#20010;&#31216;&#20026;G-MHKG&#30340;&#27169;&#22411;&#65292;&#24182;&#35814;&#32454;&#23637;&#31034;&#20102;&#27599;&#20010;&#20803;&#32032;&#22312;&#25511;&#21046;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#25152;&#26377;&#30340;&#35266;&#27979;&#21487;&#36798;&#19968;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as one of the leading approaches for machine learning on graph-structured data. Despite their great success, critical computational challenges such as over-smoothing, over-squashing, and limited expressive power continue to impact the performance of GNNs. In this study, inspired from the time-reversal principle commonly utilized in classical and quantum physics, we reverse the time direction of the graph heat equation. The resulted reversing process yields a class of high pass filtering functions that enhance the sharpness of graph node features. Leveraging this concept, we introduce the Multi-Scaled Heat Kernel based GNN (MHKG) by amalgamating diverse filtering functions' effects on node features. To explore more flexible filtering conditions, we further generalize MHKG into a model termed G-MHKG and thoroughly show the roles of each element in controlling over-smoothing, over-squashing and expressive power. Notably, we illustrate that all afo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#25512;&#24191;&#21040;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#65292;&#21516;&#26102;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.02422</link><description>&lt;p&gt;
&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#36935;&#19978;&#31070;&#32463;&#32593;&#32476;&#65306;Radon-Kolmogorov-Smirnov&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test. (arXiv:2309.02422v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#25512;&#24191;&#21040;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#65292;&#21516;&#26102;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#65288;MMD&#65289;&#26159;&#19968;&#31867;&#22522;&#20110;&#26368;&#22823;&#21270;&#20004;&#20010;&#20998;&#24067;$P$&#21644;$Q$&#20043;&#38388;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#30340;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#26816;&#39564;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#25152;&#26377;&#22312;&#26576;&#20010;&#20989;&#25968;&#31354;&#38388;$\mathcal{F}$&#20013;&#30340;&#25968;&#25454;&#21464;&#25442;$f$&#30340;&#36873;&#25321;&#12290;&#21463;&#21040;&#26368;&#36817;&#23558;&#25152;&#35859;&#30340;Radon&#26377;&#30028;&#21464;&#24046;&#20989;&#25968;&#65288;RBV&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#32852;&#31995;&#36215;&#26469;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65288;Parhi&#21644;Nowak, 2021, 2023&#65289;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;$\mathcal{F}$&#21462;&#20026;&#32473;&#23450;&#24179;&#28369;&#24230;&#39034;&#24207;$k \geq 0$&#19979;&#30340;RBV&#31354;&#38388;&#20013;&#30340;&#21333;&#20301;&#29699;&#30340;MMD&#12290;&#36825;&#20010;&#26816;&#39564;&#34987;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#23545;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#30340;&#32463;&#20856;Kolmogorov-Smirnov&#65288;KS&#65289;&#26816;&#39564;&#30340;&#19968;&#33324;&#21270;&#12290;&#23427;&#36824;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#65306;&#25105;&#20204;&#35777;&#26126;RKS&#26816;&#39564;&#20013;&#30340;&#35777;&#25454;&#20989;&#25968;$f$&#65292;&#21363;&#36798;&#21040;&#26368;&#22823;&#22343;&#24046;&#30340;&#20989;&#25968;&#65292;&#24635;&#26159;&#19968;&#20010;&#20108;&#27425;&#26679;&#26465;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancy (MMD) refers to a general class of nonparametric two-sample tests that are based on maximizing the mean difference over samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of $\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the $\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree
&lt;/p&gt;</description></item><item><title>&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;"&#23545;&#40784;&#22686;&#24378;&#22120;"&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#21487;&#36716;&#31227;&#24615;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;&#35843;&#25972;&#21442;&#25968;&#30340;&#26102;&#38388;&#24182;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01516</link><description>&lt;p&gt;
&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#65306;&#20026;&#21487;&#25193;&#23637;&#30340;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#35843;&#25972;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval. (arXiv:2309.01516v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01516
&lt;/p&gt;
&lt;p&gt;
&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;"&#23545;&#40784;&#22686;&#24378;&#22120;"&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#21487;&#36716;&#31227;&#24615;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;&#35843;&#25972;&#21442;&#25968;&#30340;&#26102;&#38388;&#24182;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#23558;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#35843;&#25972;&#21040;&#19987;&#38376;&#30340;&#20219;&#21153;&#19978;&#24050;&#25104;&#20026;&#19968;&#20010;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#36827;&#34892;&#23396;&#31435;&#12289;&#31351;&#20030;&#30340;&#37325;&#26032;&#35843;&#25972;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#32463;&#24120;&#24573;&#35270;&#27169;&#24577;&#23545;&#40784;&#65292;&#20165;&#20851;&#27880;&#26032;&#20219;&#21153;&#30340;&#30693;&#35782;&#25552;&#21462;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#21253;&#21547;&#20102;&#19968;&#20010;&#8220;&#23545;&#40784;&#22686;&#24378;&#22120;&#8221;&#65292;&#21487;&#20197;&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#24230;&#30340;&#21487;&#36716;&#31227;&#24615;&#32780;&#26080;&#38656;&#35843;&#25972;&#39044;&#35757;&#32451;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#21521;LMMs&#28155;&#21152;&#20102;&#19981;&#21040;1.25%&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#20197;BEiT-3&#27169;&#22411;&#20026;&#20363;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#32553;&#30701;&#20102;&#39640;&#36798;57%&#30340;&#24494;&#35843;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25\% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57\% reduction in fine-tuning time. Our approach offers a resource-efficient and ef
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#25442;&#38543;&#26426;&#26799;&#24230;&#39044;&#35328;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#25955;&#38797;&#28857;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#21021;&#22987;&#38454;&#27573;&#20351;&#29992;&#24191;&#20041;&#38543;&#26426;&#26799;&#24230;&#35745;&#31639;&#39044;&#35328;&#21152;&#36895;&#36845;&#20195;&#36827;&#23637;&#65292;&#28982;&#21518;&#20999;&#25442;&#21040;&#38543;&#26426;&#26041;&#24046;&#20943;&#23569;&#26799;&#24230;&#39044;&#35328;&#65292;&#35813;&#31639;&#27861;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00997</link><description>&lt;p&gt;
&#20999;&#25442;&#19982;&#24449;&#26381;&#65306;&#36890;&#36807;&#20999;&#25442;&#38543;&#26426;&#26799;&#24230;&#39044;&#35328;&#27714;&#35299;&#20998;&#25955;&#38797;&#28857;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Switch and Conquer: Efficient Algorithms By Switching Stochastic Gradient Oracles For Decentralized Saddle Point Problems. (arXiv:2309.00997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#25442;&#38543;&#26426;&#26799;&#24230;&#39044;&#35328;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#25955;&#38797;&#28857;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#21021;&#22987;&#38454;&#27573;&#20351;&#29992;&#24191;&#20041;&#38543;&#26426;&#26799;&#24230;&#35745;&#31639;&#39044;&#35328;&#21152;&#36895;&#36845;&#20195;&#36827;&#23637;&#65292;&#28982;&#21518;&#20999;&#25442;&#21040;&#38543;&#26426;&#26041;&#24046;&#20943;&#23569;&#26799;&#24230;&#39044;&#35328;&#65292;&#35813;&#31639;&#27861;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20998;&#25955;&#35774;&#32622;&#19979;&#22788;&#29702;&#19968;&#31867;&#38750;&#20809;&#28369;&#24378;&#20984;-&#24378;&#20985;&#38797;&#28857;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#31867;&#38382;&#39064;&#30340;&#20849;&#35782;&#24418;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#30340;&#21407;&#22987;&#23545;&#20598;&#26434;&#20132;&#26799;&#24230;&#65288;&#19981;&#31934;&#30830;PDHG&#65289;&#36807;&#31243;&#65292;&#20801;&#35768;&#36890;&#29992;&#30340;&#26799;&#24230;&#35745;&#31639;&#39044;&#35328;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#21644;&#23545;&#20598;&#21464;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#24102;&#26377;&#38543;&#26426;&#26041;&#24046;&#20943;&#23569;&#26799;&#24230;&#65288;SVRG&#65289;&#39044;&#35328;&#30340;&#19981;&#31934;&#30830;PDHG&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#30740;&#31350;&#25581;&#31034;&#20102;IPDHG&#19982;SVRG&#39044;&#35328;&#36845;&#20195;&#30340;&#21021;&#22987;&#20445;&#23432;&#36827;&#23637;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20999;&#25442;&#24605;&#24819;&#65292;&#22312;&#21021;&#22987;&#26356;&#26032;&#38454;&#27573;&#20351;&#29992;&#24191;&#20041;&#38543;&#26426;&#26799;&#24230;&#65288;GSG&#65289;&#35745;&#31639;&#39044;&#35328;&#26469;&#21152;&#36895;&#36845;&#20195;&#30340;&#36827;&#23637;&#21040;&#38797;&#28857;&#35299;&#65292;&#28982;&#21518;&#22312;&#36866;&#24403;&#26102;&#21051;&#20999;&#25442;&#21040;SVRG&#39044;&#35328;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21629;&#21517;&#20026;&#20998;&#25955;&#36817;&#31471;&#20999;&#25442;&#38543;&#26426;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
We consider a class of non-smooth strongly convex-strongly concave saddle point problems in a decentralized setting without a central server. To solve a consensus formulation of problems in this class, we develop an inexact primal dual hybrid gradient (inexact PDHG) procedure that allows generic gradient computation oracles to update the primal and dual variables. We first investigate the performance of inexact PDHG with stochastic variance reduction gradient (SVRG) oracle. Our numerical study uncovers a significant phenomenon of initial conservative progress of iterates of IPDHG with SVRG oracle. To tackle this, we develop a simple and effective switching idea, where a generalized stochastic gradient (GSG) computation oracle is employed to hasten the iterates' progress to a saddle point solution during the initial phase of updates, followed by a switch to the SVRG oracle at an appropriate juncture. The proposed algorithm is named Decentralized Proximal Switching Stochastic Gradient me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15321</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#8220;&#26333;&#20809;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#65292;&#32570;&#20047;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#23545;&#37319;&#26679;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#24314;&#27169;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;&#39044;&#27979;&#35823;&#24046;&#24402;&#22240;&#20026;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#38500;&#20102;&#38416;&#26126;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;Epsilon Scaling&#65292;&#20197;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Epsilon Scaling&#36890;&#36807;&#32553;&#23567;&#32593;&#32476;&#36755;&#20986;&#65288;Epsilon&#65289;&#26126;&#30830;&#22320;&#23558;&#37319;&#26679;&#36712;&#36857;&#31227;&#36817;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#12290;&#22312;&#21508;&#31181;&#25193;&#25955;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22269;&#23478;&#31726;&#29699;&#21327;&#20250;&#30340;&#36827;&#25915;&#25112;&#26415;&#35268;&#21010;&#12290;&#36890;&#36807;&#24314;&#31435;&#27169;&#22411;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#21487;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#25112;&#26415;&#25191;&#34892;&#30340;&#20855;&#20307;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2308.06851</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22269;&#23478;&#31726;&#29699;&#21327;&#20250;&#30340;&#36827;&#25915;&#25112;&#26415;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning. (arXiv:2308.06851v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22269;&#23478;&#31726;&#29699;&#21327;&#20250;&#30340;&#36827;&#25915;&#25112;&#26415;&#35268;&#21010;&#12290;&#36890;&#36807;&#24314;&#31435;&#27169;&#22411;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#21487;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#25112;&#26415;&#25191;&#34892;&#30340;&#20855;&#20307;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;NBA&#21457;&#29983;&#30340;&#20998;&#26512;&#38761;&#21629;&#20013;&#65292;&#29305;&#23450;&#30340;&#25351;&#26631;&#21644;&#20844;&#24335;&#30340;&#21457;&#23637;&#20026;&#29699;&#38431;&#12289;&#25945;&#32451;&#21644;&#29699;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#30475;&#24453;&#27604;&#36187;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65292;&#25105;&#20204;&#22914;&#20309;&#39564;&#35777;&#36825;&#20123;&#25351;&#26631;&#21602;&#65311;&#19968;&#31181;&#26041;&#27861;&#21487;&#33021;&#26159;&#31616;&#21333;&#22320;&#20973;&#30524;&#29699;&#25512;&#27979;&#65288;&#23581;&#35797;&#35768;&#22810;&#19981;&#21516;&#30340;&#25112;&#26415;&#35745;&#21010;&#65289;&#21644;/&#25110;&#35797;&#38169;&#27861;-&#19968;&#31181;&#20272;&#35745;&#24615;&#30340;&#26114;&#36149;&#26041;&#27861;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#23581;&#35797;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#24050;&#26377;&#25351;&#26631;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#29420;&#29305;&#30340;&#29305;&#24449;&#26469;&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#65292;&#36890;&#36807;&#36873;&#25321;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#23581;&#35797;&#35780;&#20272;&#36825;&#20123;&#29305;&#24449;&#30340;&#32452;&#21512;&#25928;&#26524;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20998;&#26512;&#31616;&#21333;&#30340;&#25351;&#26631;&#35780;&#20272;&#12290;&#22914;&#26524;&#25105;&#20204;&#26377;&#19968;&#20010;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#30830;&#23450;&#25112;&#26415;&#25191;&#34892;&#30340;&#20855;&#20307;&#32454;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#32479;&#35745;&#25351;&#26631;ORTG&#65288;Dean Oliver&#24320;&#21457;&#30340;&#36827;&#25915;&#35780;&#20998;&#65289;&#21457;&#29616;&#19982;&#19981;&#21516;&#30340;NBA&#27604;&#36187;&#31867;&#22411;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout the analytical revolution that has occurred in the NBA, the development of specific metrics and formulas has given teams, coaches, and players a new way to see the game. However - the question arises - how can we verify any metrics? One method would simply be eyeball approximation (trying out many different gameplans) and/or trial and error - an estimation-based and costly approach. Another approach is to try to model already existing metrics with a unique set of features using machine learning techniques. The key to this approach is that with these features that are selected, we can try to gauge the effectiveness of these features combined, rather than using individual analysis in simple metric evaluation. If we have an accurate model, it can particularly help us determine the specifics of gameplan execution. In this paper, the statistic ORTG (Offensive Rating, developed by Dean Oliver) was found to have a correlation with different NBA playtypes using both a linear regress
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#33008;&#33146;&#30284;&#30340;&#39044;&#21518;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#36317;&#31163;&#26469;&#25551;&#36848;&#32959;&#30244;&#19982;&#34880;&#31649;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#25913;&#36827;&#22810;&#30456;CT&#24433;&#20687;&#20013;&#30340;&#32959;&#30244;&#32441;&#29702;&#29305;&#24449;&#25552;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#21518;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.00507</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#30456;CT&#32467;&#21512;&#31070;&#32463;&#36317;&#31163;&#21644;&#32441;&#29702;&#24863;&#30693;&#21464;&#21387;&#22120;&#65292;&#25552;&#39640;&#33008;&#33146;&#30284;&#39044;&#21518;&#39044;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer. (arXiv:2308.00507v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#33008;&#33146;&#30284;&#30340;&#39044;&#21518;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#36317;&#31163;&#26469;&#25551;&#36848;&#32959;&#30244;&#19982;&#34880;&#31649;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#25913;&#36827;&#22810;&#30456;CT&#24433;&#20687;&#20013;&#30340;&#32959;&#30244;&#32441;&#29702;&#29305;&#24449;&#25552;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#21518;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33008;&#33146;&#23548;&#31649;&#33146;&#30284;&#65288;PDAC&#65289;&#26159;&#19968;&#31181;&#39640;&#24230;&#33268;&#21629;&#30340;&#30284;&#30151;&#65292;&#32959;&#30244;-&#34880;&#31649;&#21463;&#32047;&#26497;&#22823;&#24433;&#21709;&#24739;&#32773;&#30340;&#21487;&#20999;&#38500;&#24615;&#21644;&#24635;&#20307;&#29983;&#23384;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39044;&#21518;&#39044;&#27979;&#26041;&#27861;&#26410;&#33021;&#26126;&#30830;&#20934;&#30830;&#22320;&#35843;&#26597;&#32959;&#30244;&#19982;&#38468;&#36817;&#37325;&#35201;&#34880;&#31649;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#36317;&#31163;&#65292;&#25551;&#36848;&#20102;&#19981;&#21516;&#24739;&#32773;CT&#24433;&#20687;&#20013;&#32959;&#30244;&#19982;&#34880;&#31649;&#20043;&#38388;&#30340;&#31934;&#30830;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#39044;&#21518;&#39044;&#27979;&#30340;&#20027;&#35201;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;CT&#25104;&#20687;&#19978;&#21033;&#29992;CNN&#25110;LSTM&#26469;&#21033;&#29992;&#32959;&#30244;&#22686;&#24378;&#27169;&#24335;&#65292;&#25105;&#20204;&#36890;&#36807;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20351;&#29992;CNN&#21644;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#22312;&#22810;&#30456;&#23545;&#27604;&#22686;&#24378;CT&#20013;&#25913;&#36827;&#20102;&#21160;&#24577;&#19982;&#32959;&#30244;&#30456;&#20851;&#30340;&#32441;&#29702;&#29305;&#24449;&#25552;&#21462;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36328;&#22810;&#30456;CT&#24433;&#20687;&#25552;&#21462;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal cancer in which the tumor-vascular involvement greatly affects the resectability and, thus, overall survival of patients. However, current prognostic prediction methods fail to explicitly and accurately investigate relationships between the tumor and nearby important vessels. This paper proposes a novel learnable neural distance that describes the precise relationship between the tumor and vessels in CT images of different patients, adopting it as a major feature for prognosis prediction. Besides, different from existing models that used CNNs or LSTMs to exploit tumor enhancement patterns on dynamic contrast-enhanced CT imaging, we improved the extraction of dynamic tumor-related texture features in multi-phase contrast-enhanced CT by fusing local and global features using CNN and transformer modules, further enhancing the features extracted across multi-phase CT images. We extensively evaluated and compared the proposed method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;Temporal Graph Benchmark&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#21040;TGB&#65292;&#24182;&#20351;&#29992;&#21313;&#19968;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#12290;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;DyGLib&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.12510</link><description>&lt;p&gt;
Temporal Graph Benchmark&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Temporal Graph Benchmark. (arXiv:2307.12510v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Temporal Graph Benchmark&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#21040;TGB&#65292;&#24182;&#20351;&#29992;&#21313;&#19968;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#12290;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;DyGLib&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#25193;&#23637;&#21040;Temporal Graph Benchmark (TGB)&#65292;&#23545;TGB&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#19982;TGB&#30456;&#27604;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#21313;&#19968;&#31181;&#27969;&#34892;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#36825;&#19982;&#20043;&#21069;&#30340;&#35266;&#23519;&#19968;&#33268;&#65307;&#65288;2&#65289;&#20351;&#29992;DyGLib&#26102;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#12290;&#26412;&#24037;&#20316;&#26088;&#22312;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#22312;TGB&#19978;&#35780;&#20272;&#21508;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35797;&#22270;&#25552;&#20379;&#21487;&#30452;&#25509;&#21442;&#32771;&#30340;&#32467;&#26524;&#20379;&#21518;&#32493;&#30740;&#31350;&#20351;&#29992;&#12290;&#26412;&#39033;&#30446;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#36164;&#28304;&#22343;&#21487;&#22312;https://github.com/yule-BUAA/DyGLib_TGB&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;&#26412;&#24037;&#20316;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#27426;&#36814;&#31038;&#21306;&#25552;&#20379;&#21453;&#39304;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#22270;&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.02799</link><description>&lt;p&gt;
&#29992;&#24352;&#37327;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#65292;&#20445;&#30041;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information. (arXiv:2307.02799v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#22270;&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#21040;&#30697;&#38453;&#22238;&#24402;&#36827;&#34892;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#22270;&#65288;PSM&#65289;&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;&#19982;&#19968;&#33324;&#30340;&#26174;&#33879;&#24615;&#22270;&#30456;&#27604;&#65292;PSM&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#30340;&#26144;&#23556;&#25351;&#31034;&#20102;&#20010;&#20307;&#29305;&#23450;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#65292;&#23545;&#20110;&#20174;&#20957;&#35270;&#21306;&#22495;&#30340;&#24322;&#36136;&#24615;&#20013;&#33719;&#21462;&#20010;&#20307;&#35270;&#35273;&#20559;&#22909;&#38750;&#24120;&#26377;&#29992;&#12290;PSM&#30340;&#39044;&#27979;&#26159;&#20026;&#20102;&#33719;&#21462;&#26410;&#35265;&#22270;&#20687;&#30340;PSM&#65292;&#20294;&#30001;&#20110;&#20010;&#20307;&#20957;&#35270;&#27169;&#24335;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20174;&#26377;&#38480;&#30340;&#30524;&#21160;&#25968;&#25454;&#20013;&#35782;&#21035;&#20010;&#20307;&#20957;&#35270;&#27169;&#24335;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#37319;&#29992;&#20010;&#20307;&#20043;&#38388;&#20957;&#35270;&#36235;&#21183;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20808;&#21069;&#30340;&#26041;&#27861;&#20013;&#65292;PSMs&#34987;&#21521;&#37327;&#21270;&#20197;&#36866;&#24212;&#39044;&#27979;&#27169;&#22411;&#65292;&#20174;&#32780;&#24573;&#35270;&#20102;&#19982;&#22270;&#20687;&#23545;&#24212;&#30340;PSMs&#30340;&#32467;&#26500;&#20840;&#23616;&#20449;&#24687;&#12290;&#20026;&#20102;&#33258;&#21160;&#25581;&#31034;PSMs&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;...
&lt;/p&gt;
&lt;p&gt;
This paper presents a few-shot personalized saliency prediction using tensor-to-matrix regression for preserving the structural global information of personalized saliency maps (PSMs). In contrast to a general saliency map, a PSM has been great potential since its map indicates the person-specific visual attention that is useful for obtaining individual visual preferences from heterogeneity of gazed areas. The PSM prediction is needed for acquiring the PSM for the unseen image, but its prediction is still a challenging task due to the complexity of individual gaze patterns. For recognizing individual gaze patterns from the limited amount of eye-tracking data, the previous methods adopt the similarity of gaze tendency between persons. However, in the previous methods, the PSMs are vectorized for the prediction model. In this way, the structural global information of the PSMs corresponding to the image is ignored. For automatically revealing the relationship between PSMs, we focus on the
&lt;/p&gt;</description></item><item><title>RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17100</link><description>&lt;p&gt;
RL4CO: &#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark. (arXiv:2306.17100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17100
&lt;/p&gt;
&lt;p&gt;
RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;RL4CO&#65292;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;RL4CO&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#24211;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#22914;&#27169;&#22359;&#21270;&#21644;&#37197;&#32622;&#31649;&#29702;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#29615;&#22659;&#21644;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65289;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24378;&#35843;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#21508;&#31181;&#20248;&#21270;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;&#27169;&#22411;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#38646;-shot&#27867;&#21270;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#20351;&#29992;&#36825;&#20123;&#26032;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#26102;&#33853;&#21518;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#26356;&#21152;&#24179;&#34913;&#22320;&#35780;&#20272;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;RL4CO&#33021;&#22815;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse optimization tasks. We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these new metrics, suggesting the necessity for a more balanced view of the performance of neural CO solvers. We hope RL4CO will 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#23545;&#25163;&#22312;&#21482;&#30693;&#36947;&#32593;&#32476;&#25299;&#25169;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#33021;&#22815;&#31181;&#19979;&#19981;&#21644;&#35856;&#65292;&#24182;&#20197;&#27492;&#20026;&#22522;&#30784;&#25552;&#20986;&#20102;&#35299;&#20915;&#29992;&#25143;&#24847;&#35265;&#20272;&#35745;&#22256;&#38590;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.10313</link><description>&lt;p&gt;
&#22312;&#24343;&#37324;&#24503;&#37329;&#65293;&#32422;&#32752;&#26862;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#26377;&#38480;&#30340;&#23545;&#25163;
&lt;/p&gt;
&lt;p&gt;
Adversaries with Limited Information in the Friedkin--Johnsen Model. (arXiv:2306.10313v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10313
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#23545;&#25163;&#22312;&#21482;&#30693;&#36947;&#32593;&#32476;&#25299;&#25169;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#33021;&#22815;&#31181;&#19979;&#19981;&#21644;&#35856;&#65292;&#24182;&#20197;&#27492;&#20026;&#22522;&#30784;&#25552;&#20986;&#20102;&#35299;&#20915;&#29992;&#25143;&#24847;&#35265;&#20272;&#35745;&#22256;&#38590;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32593;&#32476;&#31038;&#20132;&#24179;&#21488;&#25104;&#20026;&#20102;&#23547;&#27714;&#22312;&#31038;&#20250;&#20013;&#24341;&#20837;&#19981;&#21644;&#35856;&#65292;&#30772;&#22351;&#27665;&#20027;&#21644;&#30772;&#22351;&#31038;&#21306;&#31283;&#23450;&#30340;&#23545;&#25163;&#30340;&#30446;&#26631;&#12290;&#36890;&#24120;&#65292;&#30446;&#26631;&#19981;&#26159;&#25903;&#25345;&#26576;&#19968;&#26041;&#30340;&#20914;&#31361;&#65292;&#32780;&#26159;&#22686;&#21152;&#20998;&#27495;&#21644;&#26497;&#21270;&#12290;&#20026;&#20102;&#23545;&#36825;&#20123;&#25915;&#20987;&#26377;&#25968;&#23398;&#19978;&#30340;&#29702;&#35299;&#65292;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#31038;&#20250;&#23398;&#20013;&#30340;&#24847;&#35265;&#24418;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#24343;&#37324;&#24503;&#37329;&#65293;&#32422;&#32752;&#26862;&#27169;&#22411;&#65292;&#24182;&#19988;&#27491;&#24335;&#30740;&#31350;&#24403;&#25913;&#21464;&#20165;&#19968;&#23567;&#37096;&#20998;&#29992;&#25143;&#30340;&#24847;&#35265;&#26102;&#65292;&#23545;&#25163;&#21487;&#20197;&#20135;&#29983;&#22810;&#22823;&#30340;&#19981;&#21644;&#35856;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#36890;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#29992;&#25143;&#30340;&#24847;&#35265;&#36890;&#24120;&#26080;&#27861;&#33719;&#24471;&#25110;&#32773;&#24456;&#38590;&#20934;&#30830;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#21363;&#20351;&#21482;&#30693;&#36947;&#32593;&#32476;&#25299;&#25169;&#65292;&#25915;&#20987;&#32773;&#26159;&#21542;&#21487;&#20197;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#31181;&#19979;&#19981;&#21644;&#35856;&#65311;
&lt;/p&gt;
&lt;p&gt;
In recent years, online social networks have been the target of adversaries who seek to introduce discord into societies, to undermine democracies and to destabilize communities. Often the goal is not to favor a certain side of a conflict but to increase disagreement and polarization. To get a mathematical understanding of such attacks, researchers use opinion-formation models from sociology, such as the Friedkin--Johnsen model, and formally study how much discord the adversary can produce when altering the opinions for only a small set of users. In this line of work, it is commonly assumed that the adversary has full knowledge about the network topology and the opinions of all users. However, the latter assumption is often unrealistic in practice, where user opinions are not available or simply difficult to estimate accurately.  To address this concern, we raise the following question: Can an attacker sow discord in a social network, even when only the network topology is known? We an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GromovMatcher&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#33258;&#21160;&#21512;&#24182;LC-MS&#25968;&#25454;&#38598;&#65292;&#21487;&#25552;&#39640;&#25968;&#25454;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#21512;&#24182;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.03218</link><description>&lt;p&gt;
&#29992;&#20110;&#26080;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#33258;&#21160;&#23545;&#40784;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal transport for automatic alignment of untargeted metabolomic data. (arXiv:2306.03218v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GromovMatcher&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#33258;&#21160;&#21512;&#24182;LC-MS&#25968;&#25454;&#38598;&#65292;&#21487;&#25552;&#39640;&#25968;&#25454;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#21512;&#24182;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28082;&#30456;&#33394;&#35889;-&#36136;&#35889;&#65288;LC-MS&#65289;&#36890;&#36807;&#27979;&#37327;&#29983;&#29289;&#26631;&#26412;&#20013;&#30340;&#22823;&#37327;&#20195;&#35874;&#29289;&#25512;&#21160;&#33647;&#29289;&#30740;&#21457;&#65292;&#30142;&#30149;&#35786;&#26029;&#21644;&#39118;&#38505;&#39044;&#27979;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;LC-MS&#30340;&#20302;&#36890;&#37327;&#23545;&#20110;&#29983;&#29289;&#26631;&#35760;&#29289;&#21457;&#29616;&#65292;&#27880;&#37322;&#21644;&#23454;&#39564;&#27604;&#36739;&#26500;&#25104;&#20102;&#20027;&#35201;&#25361;&#25112;&#65292;&#38656;&#35201;&#21512;&#24182;&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;&#24403;&#21069;&#30340;&#25968;&#25454;&#27744;&#21270;&#26041;&#27861;&#30001;&#20110;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#33030;&#24369;&#24615;&#32780;&#36935;&#21040;&#23454;&#38469;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GromovMatcher&#65292;&#19968;&#31181;&#28789;&#27963;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#33258;&#21160;&#32467;&#21512;LC-MS&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#24378;&#24230;&#30456;&#20851;&#32467;&#26500;&#65292;GromovMatcher&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#23545;&#40784;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#35813;&#31639;&#27861;&#21487;&#25193;&#23637;&#21040;&#38656;&#35201;&#26368;&#23567;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25968;&#21315;&#20010;&#29305;&#24449;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#32925;&#30284;&#21644;&#33008;&#33146;&#30284;&#30340;&#23454;&#39564;&#24739;&#32773;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Untargeted metabolomic profiling through liquid chromatography-mass spectrometry (LC-MS) measures a vast array of metabolites within biospecimens, advancing drug development, disease diagnosis, and risk prediction. However, the low throughput of LC-MS poses a major challenge for biomarker discovery, annotation, and experimental comparison, necessitating the merging of multiple datasets. Current data pooling methods encounter practical limitations due to their vulnerability to data variations and hyperparameter dependence. Here we introduce GromovMatcher, a flexible and user-friendly algorithm that automatically combines LC-MS datasets using optimal transport. By capitalizing on feature intensity correlation structures, GromovMatcher delivers superior alignment accuracy and robustness compared to existing approaches. This algorithm scales to thousands of features requiring minimal hyperparameter tuning. Applying our method to experimental patient studies of liver and pancreatic cancer, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26368;&#32039;Horn&#36924;&#36817;&#30446;&#26631;&#29702;&#35770;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#20110;&#25581;&#31034;&#22522;&#20110;&#32844;&#19994;&#30340;&#24615;&#21035;&#20559;&#35265;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.12143</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#23398;&#20064;Horn&#21253;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Horn Envelopes via Queries from Large Language Models. (arXiv:2305.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26368;&#32039;Horn&#36924;&#36817;&#30446;&#26631;&#29702;&#35770;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#20110;&#25581;&#31034;&#22522;&#20110;&#32844;&#19994;&#30340;&#24615;&#21035;&#20559;&#35265;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Angluin&#30340;&#31934;&#30830;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#25104;&#21592;&#21644;&#31561;&#20215;&#24615;&#26597;&#35810;&#21040;&#19968;&#20010;oracle&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;oracle&#26159;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;Angluin&#29992;&#20110;&#23398;&#20064;Horn&#29702;&#35770;&#30340;&#32463;&#20856;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#24517;&#35201;&#30340;&#21464;&#21270;&#65292;&#20197;&#20351;&#20854;&#36866;&#29992;&#20110;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24517;&#39035;&#32771;&#34385;&#21040;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#19981;&#20250;&#20687;Horn oracle&#37027;&#26679;&#34892;&#20107;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#28508;&#22312;&#30446;&#26631;&#29702;&#35770;&#21487;&#33021;&#19981;&#26159;Horn&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#25552;&#21462;&#30446;&#26631;&#29702;&#35770;&#8220;&#26368;&#32039;Horn&#36924;&#36817;&#8221;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#20445;&#35777;&#22312;&#25351;&#25968;&#26102;&#38388;&#65288;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65289;&#20869;&#32456;&#27490;&#65292;&#22312;&#30446;&#26631;&#20855;&#26377;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#38750;Horn&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#32456;&#27490;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25552;&#21462;&#25581;&#31034;&#22522;&#20110;&#32844;&#19994;&#30340;&#24615;&#21035;&#20559;&#35265;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate an approach for extracting knowledge from trained neural networks based on Angluin's exact learning model with membership and equivalence queries to an oracle. In this approach, the oracle is a trained neural network. We consider Angluin's classical algorithm for learning Horn theories and study the necessary changes to make it applicable to learn from neural networks. In particular, we have to consider that trained neural networks may not behave as Horn oracles, meaning that their underlying target theory may not be Horn. We propose a new algorithm that aims at extracting the ``tightest Horn approximation'' of the target theory and that is guaranteed to terminate in exponential time (in the worst case) and in polynomial time if the target has polynomially many non-Horn examples. To showcase the applicability of the approach, we perform experiments on pre-trained language models and extract rules that expose occupation-based gender biases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06695</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#29992;&#20110;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species. (arXiv:2305.06695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#65292;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#36890;&#24120;&#29992;&#20110;&#35782;&#21035;&#29289;&#31181;&#21644;&#20010;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#19978;&#22686;&#24378;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#31232;&#26377;&#31867;&#21035;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#38754;&#65292;&#35813;&#39046;&#22495;&#23578;&#26410;&#36827;&#34892;&#23581;&#35797;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#65292;&#26088;&#22312;&#38544;&#24335;&#32534;&#30721;&#36328;&#22495;&#20851;&#32852;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36825;&#31181;&#23545;&#40784;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#30452;&#25509;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65288;LTR&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20110;32&#20010;&#29289;&#31181;&#12289;&#36229;&#36807;30,000&#20010;&#28014;&#28216;&#26377;&#23380;&#34411;&#22771;&#30340;&#26174;&#24494;&#22270;&#20687;&#24182;&#19982;&#29420;&#31435;&#30340;&#36951;&#20256;&#25968;&#25454;&#26679;&#26412;&#19968;&#36215;&#20351;&#29992;&#26469;&#23454;&#39564;&#23460;&#23637;&#29616;&#20102;&#35813;&#27010;&#24565;&#30340;&#25928;&#21147;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23545;&#20174;&#19994;&#32773;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35270;&#35273;-&#36951;&#20256;&#23545;&#40784;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual as well as genetic biometrics are routinely employed to identify species and individuals in biological applications. However, no attempts have been made in this domain to computationally enhance visual classification of rare classes with little image data via genetics. In this paper, we thus propose aligned visual-genetic inference spaces with the aim to implicitly encode cross-domain associations for improved performance. We demonstrate for the first time that such alignment can be achieved via deep embedding models and that the approach is directly applicable to boosting long-tailed recognition (LTR) particularly for rare species. We experimentally demonstrate the efficacy of the concept via application to microscopic imagery of 30k+ planktic foraminifer shells across 32 species when used together with independent genetic data samples. Most importantly for practitioners, we show that visual-genetic alignment can significantly benefit visual-only recognition of the rarest speci
&lt;/p&gt;</description></item><item><title>&#26412;&#39033;&#30446;&#22312;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#25552;&#20986;&#32771;&#34385;&#35821;&#20041;&#36317;&#31163;&#30340;&#26032;&#39062;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#24403;&#21069;&#20808;&#36827;&#25104;&#26524;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11075</link><description>&lt;p&gt;
Spaiche: &#23558;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#25193;&#23637;&#21040;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;
&lt;/p&gt;
&lt;p&gt;
Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects. (arXiv:2304.11075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#22312;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#25552;&#20986;&#32771;&#34385;&#35821;&#20041;&#36317;&#31163;&#30340;&#26032;&#39062;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#24403;&#21069;&#20808;&#36827;&#25104;&#26524;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#31361;&#30772;&#22823;&#22823;&#22686;&#21152;&#20102;ASR&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#30001;&#20110;&#38590;&#20197;&#33719;&#21462;&#30456;&#20851;&#25968;&#25454;&#65292;ASR&#27169;&#22411;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#26412;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#29790;&#22763;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#35265;&#35299;&#65292;&#24110;&#21161;&#25512;&#36827;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#20102;&#39044;&#27979;&#21644;&#22522;&#20934;&#26631;&#31614;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#12290;&#36890;&#36807;&#23545;&#29790;&#22763;&#24503;&#35821;&#25968;&#25454;&#38598;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#24403;&#21069;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in NLP largely increased the presence of ASR systems in our daily lives. However, for many low-resource languages, ASR models still need to be improved due in part to the difficulty of acquiring pertinent data. This project aims to help advance research in ASR models for Swiss German dialects, by providing insights about the performance of state-of-the-art ASR models on recently published Swiss German speech datasets. We propose a novel loss that takes into account the semantic distance between the predicted and the ground-truth labels. We outperform current state-of-the-art results by fine-tuning OpenAI's Whisper model on Swiss-German datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#30340;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#65292;&#21516;&#26102;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.06104</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24102;&#26102;&#38388;&#24179;&#22343;&#32422;&#26463;&#30340;&#25511;&#21046;&#31995;&#32479;&#22312;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual Contextual Bayesian Optimization for Control System Online Optimization with Time-Average Constraints. (arXiv:2304.06104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06104
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#30340;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#65292;&#21516;&#26102;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24102;&#26377;&#22806;&#29983;&#26102;&#38388;&#21464;&#21270;&#19978;&#19979;&#25991;&#24178;&#25200;&#30340;&#26410;&#30693;&#40657;&#30418;&#20989;&#25968;&#30340;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#26368;&#20248;&#35299;&#30340;&#20122;&#32447;&#24615;&#32047;&#31215;&#36951;&#25022;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#38646;&#26102;&#38388;&#24179;&#22343;&#32422;&#26463;&#36829;&#35268;&#65292;&#30830;&#20445;&#20102;&#32422;&#26463;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#37319;&#26679;&#23454;&#20363;&#21644;&#36830;&#32493;&#25605;&#25292;&#27133;&#21453;&#24212;&#22120;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#21644;&#24179;&#22343;&#20445;&#25345;&#32422;&#26463;&#21487;&#34892;&#24615;&#65292;&#36825;&#19982;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#35201;&#20040;&#36973;&#21463;&#22823;&#37327;&#32047;&#31215;&#36951;&#25022;&#65292;&#35201;&#20040;&#23384;&#22312;&#20005;&#37325;&#32422;&#26463;&#36829;&#35268;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of online performance optimization of constrained closed-loop control systems, where both the objective and the constraints are unknown black-box functions affected by exogenous time-varying contextual disturbances. A primal-dual contextual Bayesian optimization algorithm is proposed that achieves sublinear cumulative regret with respect to the dynamic optimal solution under certain regularity conditions. Furthermore, the algorithm achieves zero time-average constraint violation, ensuring that the average value of the constraint function satisfies the desired constraint. The method is applied to both sampled instances from Gaussian processes and a continuous stirred tank reactor parameter tuning problem; simulation results show that the method simultaneously provides close-to-optimal performance and maintains constraint feasibility on average. This contrasts current state-of-the-art methods, which either suffer from large cumulative regret or severe const
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20511;&#37492;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26222;&#36866;&#20154;&#31867;&#20808;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#35270;&#39057;&#26469;&#35843;&#25972;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20223;&#30495;&#20013;&#34920;&#29616;&#20986;&#26356;&#21152;&#31867;&#20154;&#30340;&#34892;&#20026;&#65292;&#29978;&#33267;&#22312;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20013;&#20063;&#33021;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04602</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#28789;&#24039;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26222;&#36866;&#20154;&#31867;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Learning a Universal Human Prior for Dexterous Manipulation from Human Preference. (arXiv:2304.04602v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04602
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20511;&#37492;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26222;&#36866;&#20154;&#31867;&#20808;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#35270;&#39057;&#26469;&#35843;&#25972;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20223;&#30495;&#20013;&#34920;&#29616;&#20986;&#26356;&#21152;&#31867;&#20154;&#30340;&#34892;&#20026;&#65292;&#29978;&#33267;&#22312;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20013;&#20063;&#33021;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25163;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#20013;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#39640;&#32500;&#30340;&#25511;&#21046;&#31354;&#38388;&#65292;&#20174;&#22836;&#24320;&#22987;&#32534;&#20889;&#31574;&#30053;&#24456;&#38590;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#25163;&#21160;&#22870;&#21169;&#35774;&#35745;&#35757;&#32451;&#31574;&#30053;&#20063;&#38590;&#20197;&#23454;&#29616;&#24182;&#23548;&#33268;&#19981;&#33258;&#28982;&#30340;&#21160;&#20316;&#12290;&#20511;&#37492;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#35270;&#39057;&#26469;&#23398;&#20064;&#19968;&#20010;&#26222;&#36866;&#30340;&#20154;&#31867;&#20808;&#39564;&#65292;&#20197;&#22312;&#20223;&#30495;&#20013;&#26377;&#25928;&#35843;&#25972;20&#20010;&#21452;&#25163;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;RL&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#20154;&#31867;&#31034;&#33539;&#12290;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#30340;&#31574;&#30053;&#24182;&#25910;&#38598;&#36712;&#36857;&#19978;&#30340;&#20154;&#31867;&#20559;&#22909;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#24494;&#35843;&#38454;&#27573;&#29992;&#20110;&#35268;&#33539;&#31574;&#30053;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#25163;&#30340;&#26356;&#31867;&#20154;&#34892;&#20026;&#65292;&#29978;&#33267;&#21253;&#25324;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating human-like behavior on robots is a great challenge especially in dexterous manipulation tasks with robotic hands. Scripting policies from scratch is intractable due to the high-dimensional control space, and training policies with reinforcement learning (RL) and manual reward engineering can also be hard and lead to unnatural motions. Leveraging the recent progress on RL from Human Feedback, we propose a framework that learns a universal human prior using direct human preference feedback over videos, for efficiently tuning the RL policies on 20 dual-hand robot manipulation tasks in simulation, without a single human demonstration. A task-agnostic reward model is trained through iteratively generating diverse polices and collecting human preference over the trajectories; it is then applied for regularizing the behavior of polices in the fine-tuning stage. Our method empirically demonstrates more human-like behaviors on robot hands in diverse tasks including even unseen tasks,
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#29992;&#20316;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20316;&#32773;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16203</link><description>&lt;p&gt;
&#24744;&#30340;&#25193;&#25955;&#27169;&#22411;&#26263;&#20013;&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Your Diffusion Model is Secretly a Zero-Shot Classifier. (arXiv:2303.16203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16203
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#29992;&#20316;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20316;&#32773;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20026;&#22823;&#37327;&#25552;&#31034;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#29992;&#20363;&#21040;&#30446;&#21069;&#20026;&#27490;&#37117;&#21482;&#20851;&#27880;&#25277;&#26679;&#65292;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#36824;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#20043;&#22806;&#30340;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;Stable Diffusion&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#25191;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20248;&#20110;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25105;&#20204;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#27604;&#31454;&#20105;&#24615;&#30340;&#23545;&#27604;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21576;&#29616;&#20102;&#23450;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#23427;&#23398;&#20064;&#20102;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. We also find that our diffusion-based approach has stronger multimodal relational reasoning abilities than competing contrastive approaches. Finally, we eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#19979;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#31163;&#25955;&#21270;&#29615;&#22659;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#25214;&#21040;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.00028</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#36830;&#32493;&#21644;&#31163;&#25955;&#31354;&#38388;&#30340;&#22238;&#24402;&#20256;&#24863;&#22120;&#25918;&#32622;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#19979;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#31163;&#25955;&#21270;&#29615;&#22659;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#25214;&#21040;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#26041;&#26696;&#65292;&#29992;&#20110;&#30417;&#27979;&#28201;&#24230;&#12289;&#38477;&#27700;&#31561;&#31354;&#38388;&#65288;&#25110;&#26102;&#31354;&#65289;&#30456;&#20851;&#29616;&#35937;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#24050;&#30693;&#20869;&#26680;&#20989;&#25968;&#21442;&#25968;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#25311;&#21512;&#21040;&#29615;&#22659;&#20013;&#38543;&#26426;&#37319;&#26679;&#30340;&#26410;&#26631;&#35760;&#20301;&#32622;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#30340;&#35825;&#23548;&#28857;&#26469;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#12290;&#20351;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#32423;&#21035;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#21035;&#12290;&#22312;&#20505;&#36873;&#20256;&#24863;&#22120;&#25918;&#32622;&#28857;&#38598;&#21512;&#30340;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#36138;&#23146;&#39034;&#24207;&#36873;&#25321;&#31639;&#27861;&#26469;&#25214;&#21040;&#36739;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach based on sparse Gaussian processes (SGPs) to address the sensor placement problem for monitoring spatially (or spatiotemporally) correlated phenomena such as temperature and precipitation. Existing Gaussian process (GP) based sensor placement approaches use GPs with known kernel function parameters to model a phenomenon and subsequently optimize the sensor locations in a discretized representation of the environment. In our approach, we fit an SGP with known kernel function parameters to randomly sampled unlabeled locations in the environment and show that the learned inducing points of the SGP inherently solve the sensor placement problem in continuous spaces. Using SGPs avoids discretizing the environment and reduces the computation cost from cubic to linear complexity. When restricted to a candidate set of sensor placement locations, we can use greedy sequential selection algorithms on the SGP's optimization bound to find good solutions. We also present a
&lt;/p&gt;</description></item><item><title>&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(IBNNs)&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#38598;&#21512;&#21644;&#20284;&#28982;&#20998;&#24067;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;BNNs&#65292;&#21487;&#20197;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2302.09656</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Imprecise Bayesian Neural Networks. (arXiv:2302.09656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09656
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(IBNNs)&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#38598;&#21512;&#21644;&#20284;&#28982;&#20998;&#24067;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;BNNs&#65292;&#21487;&#20197;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;, &#30830;&#23450;&#19981;&#30830;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#26159;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20351;&#24471;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#33021;&#22815;&#34987;&#35780;&#20272;&#65292;&#19981;&#21516;&#26469;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;IBNNs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#27010;&#25324;&#21644;&#20811;&#26381;&#26631;&#20934;BNNs&#30340;&#26576;&#20123;&#32570;&#28857;&#12290;&#26631;&#20934;BNNs&#20351;&#29992;&#21333;&#19968;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;IBNNs&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#20204;&#20801;&#35768;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#27604;&#26631;&#20934;BNNs&#26356;&#21152;&#40065;&#26834;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;PAC&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#38598;&#12290;&#25105;&#20204;&#23558;IBNNs&#24212;&#29992;&#20110;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#19968;&#20010;&#26159;&#20026;&#20102;&#20154;&#24037;&#33008;&#33146;&#25511;&#21046;&#27169;&#25311;&#34880;&#31958;&#21644;&#33008;&#23707;&#32032;&#21160;&#21147;&#23398;&#65292;&#21478;&#19968;&#20010;&#26159;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. Although Bayesian neural networks (BNNs) allow for uncertainty in the predictions to be assessed, different sources of uncertainty are indistinguishable. We present imprecise Bayesian neural networks (IBNNs); they generalize and overcome some of the drawbacks of standard BNNs. These latter are trained using a single prior and likelihood distributions, whereas IBNNs are trained using credal prior and likelihood sets. They allow to distinguish between aleatoric and epistemic uncertainties, and to quantify them. In addition, IBNNs are robust in the sense of Bayesian sensitivity analysis, and are more robust than BNNs to distribution shift. They can also be used to compute sets of outcomes that enjoy PAC-like properties. We apply IBNNs to two case studies. One, to model blood glucose and insulin dynamics for artificial pancreas control, and two, for motion p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05185</link><description>&lt;p&gt;
&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Penalty-based Bilevel Gradient Descent Method. (arXiv:2302.05185v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a penalty-based bilevel gradient descent algorithm to solve the constrained bilevel problem without lower-level strong convexity, and experiments show its efficiency.
&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#38590;&#20197;&#35299;&#20915;&#12290;&#26368;&#36817;&#30340;&#21487;&#25193;&#23637;&#21452;&#23618;&#31639;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#19979;&#23618;&#30446;&#26631;&#20989;&#25968;&#26159;&#24378;&#20984;&#25110;&#26080;&#32422;&#26463;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24809;&#32602;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#24809;&#32602;&#37325;&#26500;&#21487;&#20197;&#24674;&#22797;&#21407;&#22987;&#21452;&#23618;&#38382;&#39064;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#65288;PBGD&#65289;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;PBGD&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel optimization problems are difficult to solve. Recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent (PBGD) algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. Experiments showcase the efficiency of the proposed PBGD algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#21644;&#26816;&#27979;&#22270;&#20687;&#20013;&#21518;&#38376;&#27169;&#24335;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#31216;&#20026;&#35748;&#30693;&#31934;&#28860;&#65288;CD&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#25513;&#30721;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#21462;&#19968;&#20010;&#23567;&#27169;&#24335;&#65292;&#35813;&#27169;&#24335;&#21487;&#20197;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#20351;&#29992;CD&#21644;&#25552;&#21462;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#20010;&#26377;&#36259;&#29616;&#35937;&#65306;&#21518;&#38376;&#26679;&#26412;&#30340;&#27169;&#24335;&#37117;&#38750;&#24120;&#23567;&#12290;&#25152;&#20197;&#21487;&#20197;&#21033;&#29992;&#23398;&#21040;&#30340;&#25513;&#30721;&#20174;&#27745;&#26579;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21644;&#21024;&#38500;&#21518;&#38376;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2301.10908</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#20013;&#25552;&#21462;&#35748;&#30693;&#21518;&#38376;&#27169;&#24335;&#30340;&#26041;&#27861;: &#19968;&#31181;&#29992;&#20110;&#21518;&#38376;&#26679;&#26412;&#26816;&#27979;&#30340; SOTA &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distilling Cognitive Backdoor Patterns within an Image: A SOTA Method for Backdoor Sample Detection. (arXiv:2301.10908v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#21644;&#26816;&#27979;&#22270;&#20687;&#20013;&#21518;&#38376;&#27169;&#24335;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#31216;&#20026;&#35748;&#30693;&#31934;&#28860;&#65288;CD&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#25513;&#30721;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#21462;&#19968;&#20010;&#23567;&#27169;&#24335;&#65292;&#35813;&#27169;&#24335;&#21487;&#20197;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#20351;&#29992;CD&#21644;&#25552;&#21462;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#20010;&#26377;&#36259;&#29616;&#35937;&#65306;&#21518;&#38376;&#26679;&#26412;&#30340;&#27169;&#24335;&#37117;&#38750;&#24120;&#23567;&#12290;&#25152;&#20197;&#21487;&#20197;&#21033;&#29992;&#23398;&#21040;&#30340;&#25513;&#30721;&#20174;&#27745;&#26579;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21644;&#21024;&#38500;&#21518;&#38376;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#21644;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#21518;&#38376;&#27169;&#24335;: &#35748;&#30693;&#31934;&#28860; (CD)&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#36755;&#20837;&#25513;&#30721;&#26469;&#25552;&#21462;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#23567;&#27169;&#24335;&#65292;&#35813;&#27169;&#24335;&#21487;&#20197;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#36890;&#36807;&#20351;&#29992;CD&#21644;&#25552;&#21462;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#20010;&#26377;&#36259;&#29616;&#35937;&#65306;&#23613;&#31649;&#19981;&#21516;&#25915;&#20987;&#20351;&#29992;&#19981;&#21516;&#24418;&#24335;&#21644;&#22823;&#23567;&#30340;&#35302;&#21457;&#27169;&#24335;&#65292;&#20294;&#21518;&#38376;&#26679;&#26412;&#30340;&#27169;&#24335;&#37117;&#24778;&#20154;&#22320;&#23567;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#21033;&#29992;&#23398;&#21040;&#30340;&#25513;&#30721;&#20174;&#34987;&#27745;&#26579;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21644;&#21024;&#38500;&#21518;&#38376;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;CD&#21487;&#20197;&#31283;&#20581;&#22320;&#26816;&#27979;&#21508;&#31181;&#39640;&#32423;&#21518;&#38376;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple method to distill and detect backdoor patterns within an image: \emph{Cognitive Distillation} (CD). The idea is to extract the "minimal essence" from an input image responsible for the model's prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. One thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. We conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#39044;&#27979;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#22823;&#23567;&#21644;&#36235;&#21183;&#65292;&#24182;&#25454;&#27492;&#20026;&#22320;&#29699;&#23545;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#24433;&#21709;&#20570;&#20934;&#22791;&#12290;</title><link>http://arxiv.org/abs/2301.06732</link><description>&lt;p&gt;
&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#39044;&#27979;&#22826;&#38451;&#20885;&#31354;&#27934;
&lt;/p&gt;
&lt;p&gt;
Solar Coronal Hole Analysis and Prediction using Computer Vision and LSTM Neural Network. (arXiv:2301.06732v4 [astro-ph.SR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;LSTM&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21644;&#39044;&#27979;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#22823;&#23567;&#21644;&#36235;&#21183;&#65292;&#24182;&#25454;&#27492;&#20026;&#22320;&#29699;&#23545;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#24433;&#21709;&#20570;&#20934;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#24320;&#22987;&#25506;&#32034;&#22826;&#31354;&#65292;&#22826;&#31354;&#22825;&#27668;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#26126;&#26174;&#12290;&#24050;&#32463;&#30830;&#31435;&#20102;&#22826;&#38451;&#20885;&#31354;&#27934;&#36825;&#19968;&#31181;&#22826;&#31354;&#22825;&#27668;&#29616;&#35937;&#20250;&#23545;&#39134;&#26426;&#21644;&#21355;&#26143;&#30340;&#36816;&#20316;&#20135;&#29983;&#24433;&#21709;&#12290;&#22826;&#38451;&#20885;&#31354;&#27934;&#26159;&#22826;&#38451;&#19978;&#30340;&#19968;&#29255;&#21306;&#22495;&#65292;&#20854;&#29305;&#28857;&#26159;&#30913;&#22330;&#32447;&#24320;&#25918;&#32780;&#28201;&#24230;&#30456;&#23545;&#36739;&#20302;&#65292;&#23548;&#33268;&#22826;&#38451;&#39118;&#20197;&#39640;&#20110;&#24179;&#22343;&#36895;&#29575;&#36827;&#34892;&#21457;&#23556;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#20934;&#22791;&#22909;&#24212;&#23545;&#22826;&#38451;&#20885;&#31354;&#27934;&#23545;&#22320;&#29699;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#26816;&#27979;&#22826;&#38451;&#21160;&#21147;&#23398;&#35266;&#27979;&#21355;&#26143;&#65288;SDO&#65289;&#22270;&#20687;&#20013;&#30340;&#22826;&#38451;&#20885;&#31354;&#27934;&#21306;&#22495;&#24182;&#35745;&#31639;&#20854;&#22823;&#23567;&#12290;&#25105;&#20204;&#23545;&#22826;&#38451;&#27599;&#20010;&#21306;&#22495;&#30340;&#22826;&#38451;&#20885;&#31354;&#27934;&#36827;&#34892;&#27604;&#36739;&#21644;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23454;&#26045;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26041;&#27861;&#20998;&#26512;&#22826;&#38451;&#20885;&#31354;&#27934;&#38754;&#31215;&#25968;&#25454;&#30340;&#36235;&#21183;&#65292;&#24182;&#39044;&#27979;&#19981;&#21516;&#22826;&#38451;&#21306;&#22495;&#26410;&#26469;7&#22825;&#30340;&#22826;&#38451;&#20885;&#31354;&#27934;&#22823;&#23567;&#12290;&#36890;&#36807;&#20998;&#26512;&#22826;&#38451;&#20885;&#31354;&#27934;&#38754;&#31215;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36776;&#35782;&#22826;&#38451;&#20885;&#31354;&#27934;&#30340;&#21464;&#21270;&#36235;&#21183;&#21644;&#39044;&#27979;&#20854;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
As humanity has begun to explore space, the significance of space weather has become apparent. It has been established that coronal holes, a type of space weather phenomenon, can impact the operation of aircraft and satellites. The coronal hole is an area on the sun characterized by open magnetic field lines and relatively low temperatures, which result in the emission of the solar wind at higher than average rates. In this study, To prepare for the impact of coronal holes on the Earth, we use computer vision to detect the coronal hole region and calculate its size based on images from the Solar Dynamics Observatory (SDO). We compare the coronal holes for each region of the Sun and analyze the correlation. We then implement deep learning techniques, specifically the Long Short-Term Memory (LSTM) method, to analyze trends in the coronal hole area data and predict its size for different sun regions over 7 days. By analyzing time series data on the coronal hole area, this study aims to id
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#22312;&#23398;&#20064;&#20013;&#36873;&#25321;&#24178;&#20928;&#26679;&#26412;&#30340;&#26694;&#26550;Knockoffs-SPR&#65292;&#36890;&#36807;Scalable Penalized Regression&#65288;SPR&#65289;&#26041;&#27861;&#27169;&#22411;&#21270;&#32593;&#32476;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;Knockoff&#36807;&#28388;&#22120;&#25511;&#21046;&#20102;&#24178;&#20928;&#25968;&#25454;&#30340;&#35823;&#36873;&#25321;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.00545</link><description>&lt;p&gt;
Knockoffs-SPR: &#26080;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#24178;&#20928;&#26679;&#26412;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels. (arXiv:2301.00545v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#22312;&#23398;&#20064;&#20013;&#36873;&#25321;&#24178;&#20928;&#26679;&#26412;&#30340;&#26694;&#26550;Knockoffs-SPR&#65292;&#36890;&#36807;Scalable Penalized Regression&#65288;SPR&#65289;&#26041;&#27861;&#27169;&#22411;&#21270;&#32593;&#32476;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;Knockoff&#36807;&#28388;&#22120;&#25511;&#21046;&#20102;&#24178;&#20928;&#25968;&#25454;&#30340;&#35823;&#36873;&#25321;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#35757;&#32451;&#38598;&#36890;&#24120;&#20250;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#30340;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23398;&#20064;&#20013;&#65292;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#24178;&#20928;&#26679;&#26412;&#36873;&#25321;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24809;&#32602;&#22238;&#24402;&#65288;SPR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#32593;&#32476;&#29305;&#24449;&#21644;&#29420;&#28909;&#26631;&#31614;&#20043;&#38388;&#30340;&#32447;&#24615;&#20851;&#31995;&#12290;&#22312;SPR&#20013;&#65292;&#36890;&#36807;&#22312;&#22238;&#24402;&#27169;&#22411;&#20013;&#27714;&#35299;&#30340;&#38646;&#22343;&#20540;&#28418;&#31227;&#21442;&#25968;&#26469;&#30830;&#23450;&#24178;&#20928;&#25968;&#25454;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;SPR&#33021;&#22815;&#24674;&#22797;&#24178;&#20928;&#25968;&#25454;&#12290;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26465;&#20214;&#21487;&#33021;&#19981;&#20877;&#28385;&#36275;&#65307;&#24182;&#19988;&#19968;&#20123;&#22122;&#22768;&#25968;&#25454;&#34987;&#38169;&#35823;&#22320;&#36873;&#25321;&#20026;&#24178;&#20928;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Knockoff&#36807;&#28388;&#22120;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#25511;&#21046;&#36873;&#21462;&#30340;&#24178;&#20928;&#25968;&#25454;&#30340;&#35823;&#36873;&#25321;&#29575;&#65288;FSR&#65289;&#30340;Knockoffs-SPR&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21106;&#31639;&#27861;&#65292;&#23558;&#25972;&#20010;&#35757;&#32451;&#38598;&#20998;&#21106;&#20026;&#22810;&#20010;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
A noisy training set usually leads to the degradation of the generalization and robustness of neural networks. In this paper, we propose a novel theoretically guaranteed clean sample selection framework for learning with noisy labels. Specifically, we first present a Scalable Penalized Regression (SPR) method, to model the linear relation between network features and one-hot labels. In SPR, the clean data are identified by the zero mean-shift parameters solved in the regression model. We theoretically show that SPR can recover clean data under some conditions. Under general scenarios, the conditions may be no longer satisfied; and some noisy data are falsely selected as clean data. To solve this problem, we propose a data-adaptive method for Scalable Penalized Regression with Knockoff filters (Knockoffs-SPR), which is provable to control the False-Selection-Rate (FSR) in the selected clean data. To improve the efficiency, we further present a split algorithm that divides the whole trai
&lt;/p&gt;</description></item><item><title>ColD Fusion&#26159;&#19968;&#31181;&#21327;&#21516;&#19979;&#38477;&#30340;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#21487;&#20197;&#19981;&#26029;&#25913;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;RoBERTa&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.01378</link><description>&lt;p&gt;
ColD Fusion: &#21327;&#21516;&#19979;&#38477;&#30340;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning. (arXiv:2212.01378v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01378
&lt;/p&gt;
&lt;p&gt;
ColD Fusion&#26159;&#19968;&#31181;&#21327;&#21516;&#19979;&#38477;&#30340;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#21487;&#20197;&#19981;&#26029;&#25913;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;RoBERTa&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#26469;&#19981;&#26029;&#28436;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#31216;&#20026;ColD Fusion&#12290;&#23427;&#20855;&#26377;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#20294;&#21033;&#29992;&#26377;&#38480;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#20849;&#20139;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;ColD Fusion&#21487;&#20197;&#24418;&#25104;&#19968;&#20010;&#21327;&#21516;&#24490;&#29615;&#65292;&#20854;&#20013;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24490;&#29615;&#21033;&#29992;&#65292;&#19981;&#26029;&#25913;&#36827;&#23427;&#20204;&#25152;&#22522;&#20110;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ColD Fusion&#20135;&#29983;&#20102;&#19982;&#22810;&#20219;&#21153;&#35757;&#32451;&#30456;&#24403;&#30340;&#22909;&#22788;&#65292;&#36890;&#36807;&#20135;&#29983;&#19968;&#20010;&#22312;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#24182;&#19988;&#22312;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#26356;&#22909;&#30340;&#36215;&#28857;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ColD Fusion&#20248;&#20110;RoBERTa&#29978;&#33267;&#20197;&#21069;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#20351;&#29992;35&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;ColD Fusion-based&#27169;&#22411;&#22312;&#19981;&#25913;&#21464;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#24179;&#22343;&#20248;&#20110;RoBERTa 2.33&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new paradigm to continually evolve pretrained models, denoted ColD Fusion. It provides the benefits of multitask learning but leverages distributed computation with limited communication and eliminates the need for shared data. Consequentially, ColD Fusion can give rise to a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based upon. We show that ColD Fusion yields comparable benefits to multitask training by producing a model that (a) attains strong performance on all of the datasets it was trained on; and (b) is a better starting point for finetuning on unseen datasets. We show that ColD Fusion outperforms RoBERTa and even previous multitask models. Specifically, when training and testing on 35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.33 points on average without any changes to the architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#26469;&#35299;&#20915;&#20256;&#32479;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#30740;&#31350;DFL&#19982;CFL&#30340;&#24046;&#24322;&#12289;DFL&#30340;&#22522;&#30784;&#29702;&#35770;&#12289;DFL&#26694;&#26550;&#30340;&#35774;&#35745;&#19982;&#35780;&#20272;&#20197;&#21450;DFL&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2211.08413</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;: &#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#26694;&#26550;&#12289;&#36235;&#21183;&#21644;&#25361;&#25112; (arXiv:2211.08413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges. (arXiv:2211.08413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#26469;&#35299;&#20915;&#20256;&#32479;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#30740;&#31350;DFL&#19982;CFL&#30340;&#24046;&#24322;&#12289;DFL&#30340;&#22522;&#30784;&#29702;&#35770;&#12289;DFL&#26694;&#26550;&#30340;&#35774;&#35745;&#19982;&#35780;&#20272;&#20197;&#21450;DFL&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#22312;&#19981;&#20849;&#20139;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#21327;&#20316;&#27169;&#22411;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#33258;&#38382;&#19990;&#20197;&#26469;&#65292;&#20013;&#24515;&#21270;FL&#65288;CFL&#65289;&#19968;&#30452;&#26159;&#25991;&#29486;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#20013;&#24515;&#21270;&#23454;&#20307;&#21019;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20013;&#24515;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;&#29942;&#39048;&#22686;&#21152;&#12289;&#31995;&#32479;&#25925;&#38556;&#39118;&#38505;&#22686;&#39640;&#65292;&#24433;&#21709;&#36127;&#36131;&#21019;&#24314;&#20840;&#23616;&#27169;&#22411;&#30340;&#23454;&#20307;&#30340;&#21487;&#20449;&#24230;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#24212;&#36816;&#32780;&#29983;&#65292;&#36890;&#36807;&#25512;&#24191;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#24182;&#26368;&#23567;&#21270;&#23545;&#20013;&#24515;&#21270;&#26550;&#26500;&#30340;&#20381;&#36182;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;&#22312;DFL&#26041;&#38754;&#26377;&#25152;&#21162;&#21147;&#65292;&#25991;&#29486;&#36824;&#27809;&#26377;&#30740;&#31350;(i)DFL&#21644;CFL&#20043;&#38388;&#30340;&#20027;&#35201;&#24046;&#24322;;(ii)&#20998;&#26512;DFL&#26694;&#26550;&#20197;&#21019;&#24314;&#21644;&#35780;&#20272;&#26032;&#35299;&#20915;&#26041;&#26696;;(iii)&#22238;&#39038;&#20351;&#29992;DFL&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22312;&#32852;&#37030;&#26550;&#26500;&#12289;&#23433;&#20840;&#24615;&#12289;&#36890;&#20449;&#31561;&#26041;&#38754;&#35782;&#21035;&#24182;&#20998;&#26512;&#20102;DFL&#30340;&#20027;&#35201;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, Federated Learning (FL) has gained relevance in training collaborative models without sharing sensitive data. Since its birth, Centralized FL (CFL) has been the most common approach in the literature, where a central entity creates a global model. However, a centralized approach leads to increased latency due to bottlenecks, heightened vulnerability to system failures, and trustworthiness concerns affecting the entity responsible for the global model creation. Decentralized Federated Learning (DFL) emerged to address these concerns by promoting decentralized model aggregation and minimizing reliance on centralized architectures. However, despite the work done in DFL, the literature has not (i) studied the main aspects differentiating DFL and CFL; (ii) analyzed DFL frameworks to create and evaluate new solutions; and (iii) reviewed application scenarios using DFL. Thus, this article identifies and analyzes the main fundamentals of DFL in terms of federation architect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31232;&#30095;&#24615;&#23545;&#22270;&#35889;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#31232;&#30095;&#22270;&#19978;&#33021;&#22815;&#20248;&#20110;&#35889;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.03231</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#31264;&#23494;&#21644;&#31232;&#30095;&#22270;&#19978;&#30340;&#35889;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Spectral Analysis of Graph Neural Networks on Dense and Sparse Graphs. (arXiv:2211.03231v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31232;&#30095;&#24615;&#23545;&#22270;&#35889;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#31232;&#30095;&#22270;&#19978;&#33021;&#22815;&#20248;&#20110;&#35889;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20135;&#29983;&#19981;&#21516;&#31232;&#30095;&#31243;&#24230;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#31232;&#30095;&#24615;&#23545;&#22270;&#35889;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#36825;&#23545;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24615;&#33021;&#24433;&#21709;&#22312;&#31264;&#23494;&#21644;&#31232;&#30095;&#22270;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23558;GNN&#19982;&#24050;&#30693;&#22312;&#31264;&#23494;&#22270;&#19978;&#25552;&#20379;&#19968;&#33268;&#20272;&#35745;&#30340;&#35889;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#20063;&#26159;&#19968;&#20010;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;GNN&#22312;&#31232;&#30095;&#22270;&#19978;&#21487;&#20197;&#36229;&#36234;&#35889;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#30340;&#25968;&#20540;&#31034;&#20363;&#26469;&#35828;&#26126;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose a random graph model that can produce graphs at different levels of sparsity. We analyze how sparsity affects the graph spectra, and thus the performance of graph neural networks (GNNs) in node classification on dense and sparse graphs. We compare GNNs with spectral methods known to provide consistent estimators for community detection on dense graphs, a closely related task. We show that GNNs can outperform spectral methods on sparse graphs, and illustrate these results with numerical examples on both synthetic and real graphs.
&lt;/p&gt;</description></item><item><title>CTRL&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#22810;&#31867;&#21035;&#25968;&#25454;&#38598;&#20013;&#26631;&#31614;&#38169;&#35823;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32858;&#31867;&#35757;&#32451;&#25439;&#22833;&#23558;&#26679;&#26412;&#20998;&#20026;&#24178;&#20928;&#26631;&#35760;&#21644;&#26377;&#22122;&#22768;&#26631;&#35760;&#20004;&#31867;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#38169;&#35823;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.08464</link><description>&lt;p&gt;
CTRL: &#38024;&#23545;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#36827;&#34892;&#32858;&#31867;&#35757;&#32451;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
CTRL: Clustering Training Losses for Label Error Detection. (arXiv:2208.08464v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08464
&lt;/p&gt;
&lt;p&gt;
CTRL&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#22810;&#31867;&#21035;&#25968;&#25454;&#38598;&#20013;&#26631;&#31614;&#38169;&#35823;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32858;&#31867;&#35757;&#32451;&#25439;&#22833;&#23558;&#26679;&#26412;&#20998;&#20026;&#24178;&#20928;&#26631;&#35760;&#21644;&#26377;&#22122;&#22768;&#26631;&#35760;&#20004;&#31867;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#38169;&#35823;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27491;&#30830;&#30340;&#26631;&#31614;&#20351;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#39640;&#20934;&#30830;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#26377;&#25439;&#26631;&#31614;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#20855;&#22791;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#20854;&#26631;&#31614;&#38169;&#35823;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#20854;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CTRL&#65288;Clustering TRaining Losses for label error detection&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#22810;&#31867;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#38169;&#35823;&#12290;&#23427;&#26681;&#25454;&#27169;&#22411;&#20197;&#19981;&#21516;&#26041;&#24335;&#23398;&#20064;&#24178;&#20928;&#21644;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#35266;&#23519;&#65292;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#26816;&#27979;&#26631;&#31614;&#38169;&#35823;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26377;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24182;&#33719;&#24471;&#27599;&#20010;&#26679;&#26412;&#30340;&#25439;&#22833;&#26354;&#32447;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#32858;&#31867;&#31639;&#27861;&#24212;&#29992;&#20110;&#35757;&#32451;&#25439;&#22833;&#65292;&#23558;&#26679;&#26412;&#20998;&#20026;&#20004;&#31867;&#65306;&#24178;&#20928;&#26631;&#35760;&#21644;&#26377;&#22122;&#22768;&#26631;&#35760;&#12290;&#22312;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#21518;&#65292;&#25105;&#20204;&#21024;&#38500;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#26679;&#26412;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#38169;&#35823;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In supervised machine learning, use of correct labels is extremely important to ensure high accuracy. Unfortunately, most datasets contain corrupted labels. Machine learning models trained on such datasets do not generalize well. Thus, detecting their label errors can significantly increase their efficacy. We propose a novel framework, called CTRL (Clustering TRaining Losses for label error detection), to detect label errors in multi-class datasets. It detects label errors in two steps based on the observation that models learn clean and noisy labels in different ways. First, we train a neural network using the noisy training dataset and obtain the loss curve for each sample. Then, we apply clustering algorithms to the training losses to group samples into two categories: cleanly-labeled and noisily-labeled. After label error detection, we remove samples with noisy labels and retrain the model. Our experimental results demonstrate state-of-the-art error detection accuracy on both image
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;&#65288;QRAM&#65289;&#21644;&#37327;&#23376;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#30340;&#37327;&#23376;&#25968;&#25454;&#20013;&#24515;&#65288;QDC&#65289;&#26550;&#26500;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#37327;&#23376;&#35745;&#31639;&#12289;&#37327;&#23376;&#36890;&#20449;&#21644;&#37327;&#23376;&#24863;&#30693;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36825;&#23558;&#20026;&#26410;&#26469;&#30340;&#25968;&#25454;&#20013;&#24515;&#25552;&#20379;&#39640;&#25928;&#12289;&#31169;&#23494;&#21644;&#24555;&#36895;&#30340;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2207.14336</link><description>&lt;p&gt;
&#20855;&#26377;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;&#21644;&#37327;&#23376;&#32593;&#32476;&#30340;&#25968;&#25454;&#20013;&#24515;
&lt;/p&gt;
&lt;p&gt;
Data centers with quantum random access memory and quantum networks. (arXiv:2207.14336v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;&#65288;QRAM&#65289;&#21644;&#37327;&#23376;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#30340;&#37327;&#23376;&#25968;&#25454;&#20013;&#24515;&#65288;QDC&#65289;&#26550;&#26500;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#37327;&#23376;&#35745;&#31639;&#12289;&#37327;&#23376;&#36890;&#20449;&#21644;&#37327;&#23376;&#24863;&#30693;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36825;&#23558;&#20026;&#26410;&#26469;&#30340;&#25968;&#25454;&#20013;&#24515;&#25552;&#20379;&#39640;&#25928;&#12289;&#31169;&#23494;&#21644;&#24555;&#36895;&#30340;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#25968;&#25454;&#20013;&#24515;&#65288;QDC&#65289;&#30340;&#26550;&#26500;&#65292;&#23558;&#37327;&#23376;&#38543;&#26426;&#35775;&#38382;&#23384;&#20648;&#22120;&#65288;QRAM&#65289;&#21644;&#37327;&#23376;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;QDC&#30340;&#31934;&#30830;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#30340;&#21487;&#33021;&#23454;&#29616;&#21644;&#25193;&#23637;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;QDC&#22312;&#37327;&#23376;&#35745;&#31639;&#12289;&#37327;&#23376;&#36890;&#20449;&#21644;&#37327;&#23376;&#24863;&#30693;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#29992;&#20110;$T$&#38376;&#36164;&#28304;&#30340;QDC&#12289;&#29992;&#20110;&#22810;&#26041;&#31169;&#23494;&#37327;&#23376;&#36890;&#20449;&#30340;QDC&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#36827;&#34892;&#20998;&#24067;&#24335;&#24863;&#30693;&#30340;QDC&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;QDC&#23558;&#20316;&#20026;&#25968;&#25454;&#20013;&#24515;&#30340;&#26410;&#26469;&#29256;&#26412;&#65292;&#25552;&#20379;&#39640;&#25928;&#12289;&#31169;&#23494;&#21644;&#24555;&#36895;&#30340;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the Quantum Data Center (QDC), an architecture combining Quantum Random Access Memory (QRAM) and quantum networks. We give a precise definition of QDC, and discuss its possible realizations and extensions. We discuss applications of QDC in quantum computation, quantum communication, and quantum sensing, with a primary focus on QDC for $T$-gate resources, QDC for multi-party private quantum communication, and QDC for distributed sensing through data compression. We show that QDC will provide efficient, private, and fast services as a future version of data centers.
&lt;/p&gt;</description></item><item><title>Perseus &#35299;&#20915;&#20102;&#19968;&#20010;&#20851;&#20110;&#20809;&#28369;&#21644;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#31616;&#21333;&#19988;&#26368;&#20248;&#39640;&#38454;&#26041;&#27861;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2205.03202</link><description>&lt;p&gt;
Perseus:&#19968;&#20010;&#27714;&#35299;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#31616;&#21333;&#19988;&#26368;&#20248;&#39640;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Perseus: A Simple and Optimal High-Order Method for Variational Inequalities. (arXiv:2205.03202v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03202
&lt;/p&gt;
&lt;p&gt;
Perseus &#35299;&#20915;&#20102;&#19968;&#20010;&#20851;&#20110;&#20809;&#28369;&#21644;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#31616;&#21333;&#19988;&#26368;&#20248;&#39640;&#38454;&#26041;&#27861;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#20851;&#20110;&#35774;&#35745;&#31616;&#21333;&#21644;&#26368;&#20248;&#39640;&#38454;&#26041;&#27861;&#20197;&#27714;&#35299;&#20809;&#28369;&#19988;&#21333;&#35843;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#24320;&#25918;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#21464;&#20998;&#19981;&#31561;&#24335;&#28041;&#21450;&#23547;&#25214;$x^\star \in \mathcal{X}$&#65292;&#20351;&#24471;&#23545;&#20110;&#25152;&#26377;&#30340;$x \in \mathcal{X}$&#65292;$\langle F(x), x - x^\star\rangle \geq 0$&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;$F$&#20855;&#26377;&#26368;&#22810;$(p-1)$&#38454;&#23548;&#25968;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;$p=2$&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#31435;&#26041;&#27491;&#21017;&#21270;&#30340;&#29275;&#39039;&#26041;&#27861;&#20197;&#28385;&#36275;&#20840;&#23616;&#36895;&#29575;&#20026;$O(\epsilon^{-1})$&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#12290;&#36890;&#36807;&#19968;&#31181;&#21478;&#22806;&#30340;&#20108;&#38454;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;$O(\epsilon^{-2/3}\log\log(1/\epsilon))$&#30340;&#25913;&#36827;&#36895;&#29575;&#65292;&#20294;&#35813;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20869;&#24490;&#29615;&#32447;&#25628;&#32034;&#36807;&#31243;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#32447;&#25628;&#32034;&#36807;&#31243;&#30340;&#39640;&#38454;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#36798;&#21040;$O(\epsilon^{-2/(p+1)}\log\log(1/\epsilon))$&#30340;&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;Nesterov&#25152;&#24378;&#35843;&#30340;&#65292;&#36825;&#26679;&#30340;&#36807;&#31243;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#31616;&#21333;&#19988;&#26368;&#20248;&#30340;&#39640;&#38454;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper settles an open and challenging question pertaining to the design of simple and optimal high-order methods for solving smooth and monotone variational inequalities (VIs). A VI involves finding $x^\star \in \mathcal{X}$ such that $\langle F(x), x - x^\star\rangle \geq 0$ for all $x \in \mathcal{X}$. We consider the setting in which $F$ is smooth with up to $(p-1)^{th}$-order derivatives. For $p = 2$, the cubic regularized Newton method was extended to VIs with a global rate of $O(\epsilon^{-1})$. An improved rate of $O(\epsilon^{-2/3}\log\log(1/\epsilon))$ can be obtained via an alternative second-order method, but this method requires a nontrivial line-search procedure as an inner loop. Similarly, high-order methods based on line-search procedures have been shown to achieve a rate of $O(\epsilon^{-2/(p+1)}\log\log(1/\epsilon))$. As emphasized by Nesterov, however, such procedures do not necessarily imply practical applicability in large-scale applications, and it would be de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#36229;&#20869;&#23384;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;CPU/GPU&#26550;&#26500;&#19978;&#23454;&#29616;&#39640;&#25928;&#35745;&#31639;&#12290;&#31639;&#27861;&#36890;&#36807;&#31232;&#30095;&#21644;&#31264;&#23494;&#30697;&#38453;&#25805;&#20316;&#20197;&#21450;&#25209;&#22788;&#29702;/&#24179;&#38138;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#36229;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;CUDA&#27969;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#21644;&#24322;&#27493;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2202.09518</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;CPU/GPU&#26550;&#26500;&#19978;&#30340;&#36229;&#20869;&#23384;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)
&lt;/p&gt;
&lt;p&gt;
Distributed Out-of-Memory NMF on CPU/GPU Architectures. (arXiv:2202.09518v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09518
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#36229;&#20869;&#23384;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;CPU/GPU&#26550;&#26500;&#19978;&#23454;&#29616;&#39640;&#25928;&#35745;&#31639;&#12290;&#31639;&#27861;&#36890;&#36807;&#31232;&#30095;&#21644;&#31264;&#23494;&#30697;&#38453;&#25805;&#20316;&#20197;&#21450;&#25209;&#22788;&#29702;/&#24179;&#38138;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#36229;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;CUDA&#27969;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#21644;&#24322;&#27493;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#36229;&#20869;&#23384;&#23454;&#29616;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#31639;&#27861;&#65292;&#29992;&#20110;&#24322;&#26500;&#39640;&#24615;&#33021;&#35745;&#31639;(HPC)&#31995;&#32479;&#12290;&#35813;&#23454;&#29616;&#22522;&#20110;NMFk&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#24182;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#21464;&#37327;&#21644;&#27169;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#23545;&#22810;&#33410;&#28857;&#12289;&#22810;GPU&#31995;&#32479;&#30340;&#31264;&#23494;&#21644;&#31232;&#30095;&#30697;&#38453;&#25805;&#20316;&#25903;&#25345;&#65292;&#25193;&#23637;&#20102;NMFk&#12290;&#24471;&#21040;&#30340;&#31639;&#27861;&#38024;&#23545;&#36229;&#20869;&#23384;&#38382;&#39064;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20854;&#20013;&#25152;&#38656;&#20869;&#23384;&#22823;&#20110;&#21487;&#29992;&#30340;GPU&#20869;&#23384;&#26469;&#36827;&#34892;&#30697;&#38453;&#20998;&#35299;&#12290;&#36890;&#36807;&#25209;&#22788;&#29702;/&#24179;&#38138;&#31574;&#30053;&#38477;&#20302;&#20869;&#23384;&#22797;&#26434;&#24230;&#65292;&#24182;&#20351;&#29992;GPU&#26680;&#24515;(&#25110;&#32773;&#21487;&#29992;&#30340;&#24352;&#37327;&#26680;&#24515;)&#26174;&#33879;&#21152;&#36895;&#31232;&#30095;&#21644;&#31264;&#23494;&#30697;&#38453;&#25805;&#20316;&#12290;&#20351;&#29992;CUDA&#27969;&#38544;&#34255;&#20102;&#20027;&#26426;&#21644;&#35774;&#22791;&#20043;&#38388;&#30340;&#25209;&#22788;&#29702;&#22797;&#21046;&#30340;&#36755;&#20837;/&#36755;&#20986;(I/O)&#24310;&#36831;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#20256;&#36755;&#21644;&#24322;&#27493;&#35745;&#31639;&#30340;&#37325;&#21472;&#65292;&#20197;&#21450;&#19982;&#25910;&#38598;&#30456;&#20851;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient distributed out-of-memory implementation of the Non-negative Matrix Factorization (NMF) algorithm for heterogeneous high-performance-computing (HPC) systems. The proposed implementation is based on prior work on NMFk, which can perform automatic model selection and extract latent variables and patterns from data. In this work, we extend NMFk by adding support for dense and sparse matrix operation on multi-node, multi-GPU systems. The resulting algorithm is optimized for out-of-memory (OOM) problems where the memory required to factorize a given matrix is greater than the available GPU memory. Memory complexity is reduced by batching/tiling strategies, and sparse and dense matrix operations are significantly accelerated with GPU cores (or tensor cores when available). Input/Output (I/O) latency associated with batch copies between host and device is hidden using CUDA streams to overlap data transfers and compute asynchronously, and latency associated with collect
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20247;&#21253;&#30340;&#24037;&#20154;-&#20219;&#21153;&#29305;&#21270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#22312;&#22810;&#20010;&#19981;&#20934;&#30830;&#31572;&#26696;&#20013;&#25512;&#26029;&#27491;&#30830;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#20219;&#21153;&#21644;&#24037;&#20154;&#30340;&#29305;&#21270;&#31867;&#22411;&#20197;&#21450;&#20854;&#21487;&#38752;&#24615;&#21464;&#21270;&#12290;&#29992;&#20110;&#20272;&#35745;&#32416;&#27491;&#31572;&#26696;&#36798;&#21040;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2111.12550</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20247;&#21253;&#30340;&#24037;&#20154;-&#20219;&#21153;&#29305;&#21270;&#27169;&#22411;&#65306;&#39640;&#25928;&#25512;&#26029;&#19982;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Worker-Task Specialization Model for Crowdsourcing: Efficient Inference and Fundamental Limits. (arXiv:2111.12550v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.12550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20247;&#21253;&#30340;&#24037;&#20154;-&#20219;&#21153;&#29305;&#21270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#22312;&#22810;&#20010;&#19981;&#20934;&#30830;&#31572;&#26696;&#20013;&#25512;&#26029;&#27491;&#30830;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#20219;&#21153;&#21644;&#24037;&#20154;&#30340;&#29305;&#21270;&#31867;&#22411;&#20197;&#21450;&#20854;&#21487;&#38752;&#24615;&#21464;&#21270;&#12290;&#29992;&#20110;&#20272;&#35745;&#32416;&#27491;&#31572;&#26696;&#36798;&#21040;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#31995;&#32479;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#24179;&#21488;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#19987;&#23478;&#24037;&#20154;&#20197;&#30456;&#23545;&#36739;&#20302;&#30340;&#25104;&#26412;&#20026;&#25968;&#25454;&#26631;&#27880;&#12290;&#28982;&#32780;&#65292;&#20174;&#22810;&#20010;&#19981;&#20934;&#30830;&#31572;&#26696;&#20013;&#25512;&#26029;&#27491;&#30830;&#30340;&#26631;&#31614;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#31572;&#26696;&#30340;&#36136;&#37327;&#22312;&#20219;&#21153;&#21644;&#24037;&#20154;&#20043;&#38388;&#24046;&#24322;&#36739;&#22823;&#12290;&#35768;&#22810;&#29616;&#26377;&#24037;&#20316;&#20551;&#35774;&#24037;&#20154;&#30340;&#25216;&#33021;&#27700;&#24179;&#23384;&#22312;&#22266;&#23450;&#30340;&#25490;&#24207;&#65292;&#24182;&#30528;&#37325;&#20110;&#20272;&#35745;&#24037;&#20154;&#25216;&#33021;&#20197;&#27719;&#24635;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#26435;&#37325;&#30340;&#24037;&#20154;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#20219;&#21153;&#24322;&#36136;&#24615;&#24456;&#22823;&#26102;&#65292;&#24037;&#20154;&#30340;&#25216;&#33021;&#22312;&#20219;&#21153;&#38388;&#21464;&#21270;&#24456;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#31216;&#20026;$d$-type&#29305;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#21644;&#24037;&#20154;&#37117;&#26377;&#33258;&#24049;&#30340;&#65288;&#26410;&#30693;&#65289;&#31867;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#24037;&#20154;&#30340;&#21487;&#38752;&#24615;&#21487;&#20197;&#22312;&#32473;&#23450;&#20219;&#21153;&#30340;&#31867;&#22411;&#21644;&#24037;&#20154;&#30340;&#31867;&#22411;&#19978;&#21464;&#21270;&#12290;&#25105;&#20204;&#20801;&#35768;&#31867;&#22411;&#25968;$d$&#38543;&#30528;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#20351;&#32416;&#27491;&#31572;&#26696;&#36798;&#21040;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowdsourcing system has emerged as an effective platform for labeling data with relatively low cost by using non-expert workers. Inferring correct labels from multiple noisy answers on data, however, has been a challenging problem, since the quality of the answers varies widely across tasks and workers. Many existing works have assumed that there is a fixed ordering of workers in terms of their skill levels, and focused on estimating worker skills to aggregate the answers from workers with different weights. In practice, however, the worker skill changes widely across tasks, especially when the tasks are heterogeneous. In this paper, we consider a new model, called $d$-type specialization model, in which each task and worker has its own (unknown) type and the reliability of each worker can vary in the type of a given task and that of a worker. We allow that the number $d$ of types can scale in the number of tasks. In this model, we characterize the optimal sample complexity to correct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32593;&#32476;&#65292;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20165;&#20256;&#25773;&#21487;&#22609;&#24615;&#21464;&#21270;&#30340;&#31526;&#21495;&#65292;&#21516;&#26102;&#20351;&#29992;&#31070;&#32463;&#35843;&#21046;&#25511;&#21046;&#23398;&#20064;&#36895;&#29575;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#33539;&#20363;&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#20855;&#26377;&#22810;&#23618;&#34920;&#31034;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2110.07292</link><description>&lt;p&gt;
&#31614;&#21517;&#21644;&#30456;&#20851;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sign and Relevance learning. (arXiv:2110.07292v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.07292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32593;&#32476;&#65292;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20165;&#20256;&#25773;&#21487;&#22609;&#24615;&#21464;&#21270;&#30340;&#31526;&#21495;&#65292;&#21516;&#26102;&#20351;&#29992;&#31070;&#32463;&#35843;&#21046;&#25511;&#21046;&#23398;&#20064;&#36895;&#29575;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#33539;&#20363;&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#20855;&#26377;&#22810;&#23618;&#34920;&#31034;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#29983;&#29289;&#20223;&#30495;&#21644;&#29983;&#29289;&#21551;&#21457;&#24335;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20840;&#23616;&#35823;&#24046;&#20449;&#21495;&#65292;&#36825;&#24847;&#21619;&#30528;&#20351;&#29992;&#27973;&#23618;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#20801;&#35768;&#20351;&#29992;&#22810;&#23618;&#32593;&#32476;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20165;&#20256;&#25773;&#21487;&#22609;&#24615;&#21464;&#21270;&#30340;&#31526;&#21495;&#65288;&#21363;LTP / LTD&#65289;&#65292;&#21516;&#26102;&#20351;&#29992;&#31070;&#32463;&#35843;&#21046;&#26469;&#25511;&#21046;&#23398;&#20064;&#36895;&#29575;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#31070;&#32463;&#35843;&#21046;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#19968;&#20010;&#20462;&#27491;&#30340;&#35823;&#24046;&#25110;&#30456;&#20851;&#20449;&#21495;&#65292;&#32780;&#38169;&#35823;&#20449;&#21495;&#30340;&#33258;&#19978;&#32780;&#19979;&#31526;&#21495;&#20915;&#23450;&#38271;&#26399;&#22686;&#24378;&#36824;&#26159;&#38271;&#26399;&#25233;&#21046;&#23558;&#21457;&#29983;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#33539;&#20363;&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#20855;&#26377;&#22810;&#23618;&#34920;&#31034;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#22312;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard models of biologically realistic or biologically inspired reinforcement learning employ a global error signal, which implies the use of shallow networks. On the other hand, error backpropagation allows the use of networks with multiple layers. However, precise error backpropagation is difficult to justify in biologically realistic networks because it requires precise weighted error backpropagation from layer to layer. In this study, we introduce a novel network that solves this problem by propagating only the sign of the plasticity change (i.e., LTP/LTD) throughout the whole network, while neuromodulation controls the learning rate. Neuromodulation can be understood as a rectified error or relevance signal, while the top-down sign of the error signal determines whether long-term potentiation or long-term depression will occur. To demonstrate the effectiveness of this approach, we conducted a real robotic task as proof of concept. Our results show that this paradigm can success
&lt;/p&gt;</description></item><item><title>RIFLE&#26159;&#19968;&#20010;&#32479;&#35745;&#25512;&#26029;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22238;&#24402;&#21644;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#20272;&#35745;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#20302;&#38454;&#30697;&#21644;&#32622;&#20449;&#21306;&#38388;&#26469;&#23398;&#20064;&#19968;&#20010;&#20998;&#24067;&#40065;&#26834;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#25968;&#25454;&#34917;&#20840;&#12290;</title><link>http://arxiv.org/abs/2109.00644</link><description>&lt;p&gt;
RIFLE: &#20174;&#20302;&#38454;&#36793;&#32536;&#20272;&#35745;&#20013;&#34917;&#20840;&#32570;&#22833;&#20540;&#21644;&#31283;&#20581;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
RIFLE: Imputation and Robust Inference from Low Order Marginals. (arXiv:2109.00644v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.00644
&lt;/p&gt;
&lt;p&gt;
RIFLE&#26159;&#19968;&#20010;&#32479;&#35745;&#25512;&#26029;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22238;&#24402;&#21644;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#20272;&#35745;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#20302;&#38454;&#30697;&#21644;&#32622;&#20449;&#21306;&#38388;&#26469;&#23398;&#20064;&#19968;&#20010;&#20998;&#24067;&#40065;&#26834;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#25968;&#25454;&#34917;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#20540;&#30340;&#26222;&#36941;&#23384;&#22312;&#32473;&#32479;&#35745;&#25512;&#26029;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#24182;&#19988;&#21487;&#33021;&#38459;&#27490;&#30456;&#20284;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#21516;&#30340;&#30740;&#31350;&#20998;&#26512;&#65292;&#20351;&#24471;&#35768;&#22810;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#19981;&#33021;&#29992;&#20110;&#26032;&#30340;&#20998;&#26512;&#12290;&#30446;&#21069;&#24050;&#32463;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#34917;&#20840;&#21253;&#21644;&#31639;&#27861;&#65292;&#20294;&#32477;&#22823;&#22810;&#25968;&#22312;&#23384;&#22312;&#22823;&#37327;&#32570;&#22833;&#20540;&#21644;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#36825;&#22312;&#23454;&#35777;&#25968;&#25454;&#20013;&#26159;&#24120;&#35265;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#20302;&#20934;&#30830;&#24230;&#30340;&#20272;&#35745;&#20250;&#23545;&#19979;&#28216;&#30340;&#32479;&#35745;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#35745;&#25512;&#26029;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22238;&#24402;&#21644;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;RIFLE&#65288;&#36890;&#36807;&#20302;&#38454;&#30697;&#20272;&#35745;&#36827;&#34892;&#31283;&#20581;&#25512;&#26029;&#65289;&#36890;&#36807;&#20272;&#35745;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#20302;&#38454;&#30697;&#21644;&#30456;&#24212;&#30340;&#32622;&#20449;&#21306;&#38388;&#26469;&#23398;&#20064;&#19968;&#20010;&#20998;&#24067;&#40065;&#26834;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#25968;&#25454;&#34917;&#20840;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#19987;&#38376;&#24212;&#29992;&#20110;&#32447;&#24615;&#22238;&#24402;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The ubiquity of missing values in real-world datasets poses a challenge for statistical inference and can prevent similar datasets from being analyzed in the same study, precluding many existing datasets from being used for new analyses. While an extensive collection of packages and algorithms have been developed for data imputation, the overwhelming majority perform poorly if there are many missing values and low sample sizes, which are unfortunately common characteristics in empirical data. Such low-accuracy estimations adversely affect the performance of downstream statistical models. We develop a statistical inference framework for regression and classification in the presence of missing data without imputation. Our framework, RIFLE (Robust InFerence via Low-order moment Estimations), estimates low-order moments of the underlying data distribution with corresponding confidence intervals to learn a distributionally robust model. We specialize our framework to linear regression and n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#19981;&#21160;&#28857;&#29702;&#35770;&#20998;&#26512;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#38750;&#36127;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#32500;&#24230;&#30456;&#21516;&#30340;&#19981;&#21160;&#28857;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#19981;&#21160;&#28857;&#38598;&#24418;&#29366;&#20026;&#21306;&#38388;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#23545;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.16239</link><description>&lt;p&gt;
&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21160;&#28857;
&lt;/p&gt;
&lt;p&gt;
Fixed points of nonnegative neural networks. (arXiv:2106.16239v7 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.16239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#19981;&#21160;&#28857;&#29702;&#35770;&#20998;&#26512;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#38750;&#36127;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#32500;&#24230;&#30456;&#21516;&#30340;&#19981;&#21160;&#28857;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#19981;&#21160;&#28857;&#38598;&#24418;&#29366;&#20026;&#21306;&#38388;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#23545;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#19981;&#21160;&#28857;&#29702;&#35770;&#20998;&#26512;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#20854;&#23450;&#20041;&#20026;&#23558;&#38750;&#36127;&#21521;&#37327;&#26144;&#23556;&#20026;&#38750;&#36127;&#21521;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#20855;&#26377;&#38750;&#36127;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#38750;&#32447;&#24615;Perron-Frobenius&#29702;&#35770;&#26694;&#26550;&#19979;&#34987;&#35748;&#20026;&#26159;&#21333;&#35843;&#19988;(&#24369;)&#21487;&#25193;&#23637;&#30340;&#20989;&#25968;&#12290;&#36825;&#20010;&#20107;&#23454;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#32500;&#24230;&#30456;&#21516;&#30340;&#19981;&#21160;&#28857;&#30340;&#26465;&#20214;&#65292;&#36825;&#20123;&#26465;&#20214;&#27604;&#26368;&#36817;&#22312;&#20984;&#20998;&#26512;&#20013;&#20351;&#29992;&#30340;&#35770;&#35777;&#35201;&#24369;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#38750;&#36127;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21160;&#28857;&#38598;&#30340;&#24418;&#29366;&#26159;&#19968;&#20010;&#21306;&#38388;&#65292;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#36864;&#21270;&#20026;&#19968;&#20010;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#24471;&#21040;&#26356;&#19968;&#33324;&#30340;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#19981;&#21160;&#28857;&#30340;&#32467;&#35770;&#12290;&#20174;&#23454;&#38469;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#21161;&#20110;&#23545;&#38750;&#36127;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use fixed point theory to analyze nonnegative neural networks, which we define as neural networks that map nonnegative vectors to nonnegative vectors. We first show that nonnegative neural networks with nonnegative weights and biases can be recognized as monotonic and (weakly) scalable functions within the framework of nonlinear Perron-Frobenius theory. This fact enables us to provide conditions for the existence of fixed points of nonnegative neural networks having inputs and outputs of the same dimension, and these conditions are weaker than those recently obtained using arguments in convex analysis. Furthermore, we prove that the shape of the fixed point set of nonnegative neural networks with nonnegative weights and biases is an interval, which under mild conditions degenerates to a point. These results are then used to obtain the existence of fixed points of more general nonnegative neural networks. From a practical perspective, our results contribute to the understanding of th
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#28065;&#26059;&#26041;&#27861;&#65288;NVM&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;-based &#26694;&#26550;&#65292;&#29992;&#20110;&#37325;&#26500;&#39640;&#20998;&#36776;&#29575;&#30340;&#27431;&#25289;&#27969;&#22330;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#25289;&#26684;&#26391;&#26085;&#28065;&#26059;&#32467;&#26500;&#21644;&#20854;&#30456;&#20114;&#20316;&#29992;&#21160;&#21147;&#23398;&#65292;&#35299;&#20915;&#20102;&#23558;&#36830;&#32493;&#27969;&#22330;&#26144;&#23556;&#21040;&#31163;&#25955;&#28065;&#26059;&#31890;&#23376;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2006.04178</link><description>&lt;p&gt;
&#31070;&#32463;&#28065;&#26059;&#26041;&#27861;&#65306;&#20174;&#26377;&#38480;&#25289;&#26684;&#26391;&#26085;&#31890;&#23376;&#21040;&#26080;&#38480;&#32500;&#27431;&#25289;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Neural Vortex Method: from Finite Lagrangian Particles to Infinite Dimensional Eulerian Dynamics. (arXiv:2006.04178v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.04178
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#28065;&#26059;&#26041;&#27861;&#65288;NVM&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;-based &#26694;&#26550;&#65292;&#29992;&#20110;&#37325;&#26500;&#39640;&#20998;&#36776;&#29575;&#30340;&#27431;&#25289;&#27969;&#22330;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#25289;&#26684;&#26391;&#26085;&#28065;&#26059;&#32467;&#26500;&#21644;&#20854;&#30456;&#20114;&#20316;&#29992;&#21160;&#21147;&#23398;&#65292;&#35299;&#20915;&#20102;&#23558;&#36830;&#32493;&#27969;&#22330;&#26144;&#23556;&#21040;&#31163;&#25955;&#28065;&#26059;&#31890;&#23376;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27969;&#20307;&#25968;&#20540;&#20998;&#26512;&#39046;&#22495;&#65292;&#23384;&#22312;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#30340;&#38382;&#39064;&#65306;&#32570;&#20047;&#19968;&#31181;&#20005;&#26684;&#30340;&#25968;&#23398;&#24037;&#20855;&#23558;&#36830;&#32493;&#27969;&#22330;&#26144;&#23556;&#21040;&#31163;&#25955;&#30340;&#28065;&#26059;&#31890;&#23376;&#65292;&#20174;&#32780;&#20351;&#25289;&#26684;&#26391;&#26085;&#31890;&#23376;&#26080;&#27861;&#32487;&#25215;&#22823;&#35268;&#27169;&#27431;&#25289;&#27714;&#35299;&#22120;&#30340;&#39640;&#20998;&#36776;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21363;&#31070;&#32463;&#28065;&#26059;&#26041;&#27861;&#65288;NVM&#65289;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#25289;&#26684;&#26391;&#26085;&#28065;&#26059;&#32467;&#26500;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#65292;&#20197;&#29289;&#29702;&#31934;&#30830;&#30340;&#26041;&#24335;&#37325;&#26500;&#39640;&#20998;&#36776;&#29575;&#30340;&#27431;&#25289;&#27969;&#22330;&#12290;&#25105;&#20204;&#22522;&#30784;&#35774;&#26045;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#20004;&#20010;&#32593;&#32476;&#65306;&#19968;&#20010;&#28065;&#26059;&#34920;&#31034;&#32593;&#32476;&#29992;&#20110;&#35782;&#21035;&#22522;&#20110;&#32593;&#26684;&#30340;&#36895;&#24230;&#22330;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#28065;&#26059;&#65292;&#20197;&#21450;&#19968;&#20010;&#28065;&#26059;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#29992;&#20110;&#23398;&#20064;&#36825;&#20123;&#26377;&#38480;&#32467;&#26500;&#30340;&#28508;&#22312;&#25511;&#21046;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#23558;&#36825;&#20004;&#20010;&#32593;&#32476;&#23884;&#20837;&#28065;&#24230;&#21040;&#36895;&#24230;&#27850;&#26494;&#27714;&#35299;&#22120;&#24182;&#20351;&#29992;&#39640;&#20445;&#30495;&#24230;&#30340;&#25968;&#25454;&#35757;&#32451;&#20854;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of fluid numerical analysis, there has been a long-standing problem: lacking of a rigorous mathematical tool to map from a continuous flow field to discrete vortex particles, hurdling the Lagrangian particles from inheriting the high resolution of a large-scale Eulerian solver. To tackle this challenge, we propose a novel learning-based framework, the Neural Vortex Method (NVM), which builds a neural-network description of the Lagrangian vortex structures and their interaction dynamics to reconstruct the high-resolution Eulerian flow field in a physically-precise manner. The key components of our infrastructure consist of two networks: a vortex representation network to identify the Lagrangian vortices from a grid-based velocity field and a vortex interaction network to learn the underlying governing dynamics of these finite structures. By embedding these two networks with a vorticity-to-velocity Poisson solver and training its parameters using the high-fidelity data obtai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#29992;&#20110;&#26368;&#36817;&#37051;&#30340;&#28857;&#38598;&#25277;&#26679;&#65292;&#25152;&#28041;&#21450;&#30340;RaySense&#33609;&#22270;&#21487;&#20197;&#25429;&#25417;&#28857;&#30340;&#22522;&#26412;&#20960;&#20309;&#24418;&#24577;&#20197;&#21450;&#25552;&#21462;&#19982;&#20043;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#19988;&#21487;&#39640;&#25928;&#22320;&#36827;&#34892;&#28857;&#38598;&#19978;&#30340;&#32447;&#31215;&#20998;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/1911.10737</link><description>&lt;p&gt;
&#29992;&#23556;&#32447;&#36827;&#34892;&#26368;&#36817;&#37051;&#28857;&#38598;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbor Sampling of Point Sets using Rays. (arXiv:1911.10737v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.10737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#29992;&#20110;&#26368;&#36817;&#37051;&#30340;&#28857;&#38598;&#25277;&#26679;&#65292;&#25152;&#28041;&#21450;&#30340;RaySense&#33609;&#22270;&#21487;&#20197;&#25429;&#25417;&#28857;&#30340;&#22522;&#26412;&#20960;&#20309;&#24418;&#24577;&#20197;&#21450;&#25552;&#21462;&#19982;&#20043;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#19988;&#21487;&#39640;&#25928;&#22320;&#36827;&#34892;&#28857;&#38598;&#19978;&#30340;&#32447;&#31215;&#20998;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25277;&#26679;&#12289;&#21387;&#32553;&#21644;&#20998;&#26512;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#28857;&#38598;&#21644;&#20854;&#20182;&#20960;&#20309;&#23545;&#35937;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#31181;&#31216;&#20026;RaySense&#33609;&#22270;&#30340;&#24352;&#37327;&#65292;&#35813;&#24352;&#37327;&#25429;&#25417;&#27839;&#30528;&#19968;&#32452;&#23556;&#32447;&#30340;&#28857;&#30340;&#22522;&#26412;&#20960;&#20309;&#24418;&#24577;&#30340;&#26368;&#36817;&#37051;&#23621;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21487;&#20197;&#22312;RaySense&#33609;&#22270;&#19978;&#25191;&#34892;&#30340;&#21508;&#31181;&#25805;&#20316;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21516;&#30340;&#23646;&#24615;&#21644;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#23556;&#32447;&#38598;&#30340;&#24773;&#20917;&#19979;&#20174;&#33609;&#22270;&#20013;&#25552;&#21462;&#26377;&#20851;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#20351;&#29992;&#33609;&#22270;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#28857;&#38598;&#19978;&#30340;&#32447;&#31215;&#20998;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20960;&#20010;&#31034;&#20363;&#65292;&#35828;&#26126;&#20102;&#25152;&#25552;&#20986;&#31574;&#30053;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new framework for the sampling, compression, and analysis of distributions of point sets and other geometric objects embedded in Euclidean spaces. Our approach involves the construction of a tensor called the RaySense sketch, which captures the nearest neighbors from the underlying geometry of points along a set of rays. We explore various operations that can be performed on the RaySense sketch, leading to different properties and potential applications. Statistical information about the data set can be extracted from the sketch, independent of the ray set. Line integrals on point sets can be efficiently computed using the sketch. We also present several examples illustrating applications of the proposed strategy in practical scenarios.
&lt;/p&gt;</description></item></channel></rss>