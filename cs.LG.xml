<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03279</link><description>&lt;p&gt;
&#22870;&#21169;&#26159;&#21542;&#21512;&#29702;&#65311;&#22312; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#20013;&#34913;&#37327;&#22870;&#21169;&#19982;&#36947;&#24503;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#36861;&#27714;&#26435;&#21147;&#21644;&#27450;&#39575;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#21487;&#33021;&#20250;&#28608;&#21169;&#26377;&#23475;&#34892;&#20026;&#12290;&#37027;&#20040;&#20195;&#29702;&#26159;&#21542;&#33258;&#28982;&#32780;&#28982;&#22320;&#23398;&#20250;&#20102;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65311;&#25105;&#20204;&#22914;&#20309;&#22312; GPT-4 &#31561;&#36890;&#29992;&#27169;&#22411;&#20013;&#34913;&#37327;&#36825;&#20123;&#34892;&#20026;&#21602;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20250;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#12290;&#25105;&#20204;&#25968;&#23398;&#21270;&#20102;&#25968;&#21313;&#31181;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#20195;&#29702;&#20542;&#21521;&#20110;&#36861;&#27714;&#26435;&#21147;&#65292;&#36896;&#25104;&#21151;&#33021;&#19981;&#33391;&#21644;&#36829;&#21453;&#20262;&#29702;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#20195;&#29702;&#36235;&#21521;&#20110;&#37319;&#21462;&#26356;&#23569;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MACHIAVELLI &#26159;&#35780;&#20272;&#20154;&#24037;&#20195;&#29702;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#27700;&#24179;&#30340;&#26377;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DiffMimic&#65292;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#26377;&#26356;&#24555;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#21516;&#26102;&#36890;&#36807;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.03274</link><description>&lt;p&gt;
DiffMimic: &#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
DiffMimic: Efficient Motion Mimicking with Differentiable Physics. (arXiv:2304.03274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiffMimic&#65292;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#26377;&#26356;&#24555;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#21516;&#26102;&#36890;&#36807;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27169;&#20223;&#26159;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#21160;&#30011;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#37117;&#24314;&#31435;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20043;&#19978;&#65292;&#23384;&#22312;&#37325;&#24230;&#22870;&#21169;&#24037;&#31243;&#12289;&#39640;&#26041;&#24046;&#21644;&#38590;&#20197;&#25506;&#32034;&#30340;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#27169;&#25311;&#22120;&#65288;DPS&#65289;&#30340;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#65292;&#21517;&#20026;DiffMimic&#65292;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#21644;&#22522;&#20110;&#30495;&#23454;&#29289;&#29702;&#20808;&#39564;&#23398;&#20064;&#31283;&#23450;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#26356;&#24555;&#21644;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#65292;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#20869;&#23454;&#29616;&#31283;&#23450;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion mimicking is a foundational task in physics-based character animation. However, most existing motion mimicking methods are built upon reinforcement learning (RL) and suffer from heavy reward engineering, high variance, and slow convergence with hard explorations. Specifically, they usually take tens of hours or even days of training to mimic a simple motion sequence, resulting in poor scalability. In this work, we leverage differentiable physics simulators (DPS) and propose an efficient motion mimicking method dubbed DiffMimic. Our key insight is that DPS casts a complex policy learning task to a much simpler state matching problem. In particular, DPS learns a stable policy by analytical gradients with ground-truth physical priors hence leading to significantly faster and stabler convergence than RL-based methods. Moreover, to escape from local optima, we utilize a Demonstration Replay mechanism to enable stable gradient backpropagation in a long horizon. Extensive experiments o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03271</link><description>&lt;p&gt;
&#20351;AI&#8220;&#21475;&#28212;&#8221;&#20943;&#23569;&#30340;&#26041;&#27861;&#65306;&#25581;&#31034;&#21644;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#31192;&#23494;&#27700;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;
Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#19981;&#26029;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#20687;GPT-3&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24050;&#32463;&#21463;&#21040;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21516;&#31561;&#37325;&#35201;&#19988;&#24040;&#22823;&#30340;AI&#27169;&#22411;&#27700;&#21360;&#23578;&#26410;&#24341;&#36215;&#20154;&#20204;&#30340;&#27880;&#24847;&#12290;&#20363;&#22914;&#65292;&#22312;&#24494;&#36719;&#26368;&#20808;&#36827;&#30340;&#32654;&#22269;&#25968;&#25454;&#20013;&#24515;&#20013;&#35757;&#32451;GPT-3&#21487;&#20197;&#30452;&#25509;&#28040;&#32791;70&#19975;&#21319;&#28165;&#27905;&#28129;&#27700;&#65288;&#30456;&#24403;&#20110;&#29983;&#20135;370&#36742;&#23453;&#39532;&#27773;&#36710;&#25110;320&#36742;&#29305;&#26031;&#25289;&#30005;&#21160;&#27773;&#36710;&#65289;&#65292;&#22914;&#26524;&#22312;&#24494;&#36719;&#30340;&#20122;&#27954;&#25968;&#25454;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20010;&#27700;&#28040;&#32791;&#37327;&#23558;&#22686;&#21152;&#19977;&#20493;&#65292;&#20294;&#36825;&#26679;&#30340;&#20449;&#24687;&#19968;&#30452;&#34987;&#20445;&#23494;&#12290;&#36825;&#26497;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#28129;&#27700;&#30701;&#32570;&#24050;&#25104;&#20026;&#22312;&#20154;&#21475;&#36805;&#36895;&#22686;&#38271;&#12289;&#27700;&#36164;&#28304;&#20943;&#23569;&#21644;&#32769;&#21270;&#30340;&#27700;&#22522;&#30784;&#35774;&#26045;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25152;&#26377;&#20154;&#38754;&#20020;&#30340;&#26368;&#32039;&#36843;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#24212;&#23545;&#20840;&#29699;&#27700;&#36164;&#28304;&#30340;&#25361;&#25112;&#65292;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#20197;&#65292;&#32780;&#19988;&#24212;&#35813;&#65292;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20197;&#36523;&#20316;&#21017;&#35299;&#20915;&#33258;&#24049;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#24615;&#21457;&#29616;&#31639;&#27861;NoGAM&#65292;&#23427;&#20165;&#20165;&#38656;&#35201;&#26368;&#23569;&#37327;&#30340;&#20551;&#35774;&#65292;&#24182;&#22522;&#20110;&#21152;&#24615;&#38750;&#32447;&#24615;&#27169;&#22411;&#26469;&#25512;&#26029;&#22240;&#26524;&#22270;&#20013;&#21464;&#37327;&#30340;&#25299;&#25169;&#39034;&#24207;&#65292;&#32780;&#19988;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24615;&#33021;&#34920;&#29616;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.03265</link><description>&lt;p&gt;
&#20219;&#24847;&#22122;&#22768;&#19979;&#22522;&#20110;&#31215;&#20998;&#31639;&#27861;&#30340;&#28508;&#22312;&#22240;&#26524;&#24615;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery with Score Matching on Additive Models with Arbitrary Noise. (arXiv:2304.03265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#24615;&#21457;&#29616;&#31639;&#27861;NoGAM&#65292;&#23427;&#20165;&#20165;&#38656;&#35201;&#26368;&#23569;&#37327;&#30340;&#20551;&#35774;&#65292;&#24182;&#22522;&#20110;&#21152;&#24615;&#38750;&#32447;&#24615;&#27169;&#22411;&#26469;&#25512;&#26029;&#22240;&#26524;&#22270;&#20013;&#21464;&#37327;&#30340;&#25299;&#25169;&#39034;&#24207;&#65292;&#32780;&#19988;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24615;&#33021;&#34920;&#29616;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#21463;&#22266;&#26377;&#30340;&#32467;&#26500;&#21487;&#35782;&#21035;&#24615;&#20551;&#35774;&#25152;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#31616;&#21270;&#25512;&#26029;&#20219;&#21153;&#65292;&#36890;&#24120;&#36824;&#20250;&#26045;&#21152;&#20854;&#20182;&#38480;&#21046;&#65292;&#20363;&#22914;&#35768;&#22810;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#20013;&#25152;&#20849;&#26377;&#30340;&#23545;&#20110;&#21152;&#24615;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#39640;&#26031;&#22122;&#22768;&#20551;&#35774;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#36829;&#21453;&#39640;&#26031;&#22122;&#22768;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#36793;&#32536;&#21453;&#36716;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25512;&#26029;&#22240;&#26524;&#22270;&#20013;&#21464;&#37327;&#30340;&#25299;&#25169;&#39034;&#24207;&#65292;&#20854;&#21487;&#36866;&#29992;&#20110;&#30001;&#21152;&#24615;&#38750;&#32447;&#24615;&#27169;&#22411;&#29983;&#25104;&#30340;&#20855;&#26377;&#36890;&#29992;&#22122;&#22768;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#24615;&#21457;&#29616;&#31639;&#27861;NoGAM&#65288;Not only Gaussian Additive noise Models&#65289;&#65292;&#35813;&#31639;&#27861;&#20165;&#38656;&#35201;&#26368;&#23569;&#37327;&#30340;&#20551;&#35774;&#65292;&#19988;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery methods are intrinsically constrained by the set of assumptions needed to ensure structure identifiability. Moreover additional restrictions are often imposed in order to simplify the inference task: this is the case for the Gaussian noise assumption on additive non-linear models, which is common to many causal discovery approaches. In this paper we show the shortcomings of inference under this hypothesis, analyzing the risk of edge inversion under violation of Gaussianity of the noise terms. Then, we propose a novel method for inferring the topological ordering of the variables in the causal graph, from data generated according to an additive non-linear model with a generic noise distribution. This leads to NoGAM (Not only Gaussian Additive noise Models), a causal discovery algorithm with a minimal set of assumptions and state of the art performance, experimentally benchmarked on synthetic data.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#38024;&#23545;&#38169;&#20301;&#22788;&#29702;&#38382;&#39064;&#65292;&#23558;&#20854;&#35270;&#20026;&#27835;&#30103;&#20999;&#25442;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#27169;&#22411;&#35299;&#20915;&#20102;&#22797;&#22686;&#21644;&#26411;&#20107;&#20214;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03247</link><description>&lt;p&gt;
&#19968;&#31181;&#22312;&#19981;&#21487;&#36991;&#20813;&#39118;&#38505;&#23384;&#22312;&#19979;&#36827;&#34892;&#22797;&#21457;&#20107;&#20214;&#22240;&#26524;&#20998;&#26512;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Framework for Causal Analysis of Recurrent Events in Presence of Immortal Risk. (arXiv:2304.03247v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03247
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#38024;&#23545;&#38169;&#20301;&#22788;&#29702;&#38382;&#39064;&#65292;&#23558;&#20854;&#35270;&#20026;&#27835;&#30103;&#20999;&#25442;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#27169;&#22411;&#35299;&#20915;&#20102;&#22797;&#22686;&#21644;&#26411;&#20107;&#20214;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#32479;&#35745;&#23398;&#20013;&#23545;&#22797;&#21457;&#20107;&#20214;&#29575;&#30340;&#35266;&#27979;&#30740;&#31350;&#24456;&#24120;&#35265;&#12290;&#36890;&#24120;&#30340;&#30446;&#26631;&#26159;&#22312;&#35268;&#23450;&#30340;&#38543;&#35775;&#26102;&#38388;&#31383;&#21475;&#20869;&#65292;&#20272;&#35745;&#22312;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#30446;&#26631;&#20154;&#32676;&#20013;&#20004;&#31181;&#27835;&#30103;&#26041;&#27861;&#30340;&#20107;&#20214;&#29575;&#24046;&#24322;&#12290;&#20351;&#29992;&#35266;&#27979;&#24615;&#32034;&#36180;&#25968;&#25454;&#36827;&#34892;&#20272;&#35745;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#30446;&#26631;&#20154;&#32676;&#30340;&#25104;&#21592;&#36164;&#26684;&#26041;&#38754;&#23450;&#20041;&#26102;&#65292;&#24456;&#23569;&#22312;&#36164;&#26684;&#30830;&#35748;&#26102;&#20934;&#30830;&#20998;&#37197;&#27835;&#30103;&#26041;&#24335;&#12290;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#26159;&#38169;&#20301;&#22788;&#29702;&#65292;&#27604;&#22914;&#22522;&#20110;&#21518;&#32493;&#20998;&#37197;&#65292;&#22312;&#36164;&#26684;&#30830;&#35748;&#26102;&#20998;&#37197;&#27835;&#30103;&#26041;&#24335;&#65292;&#36825;&#20250;&#23558;&#20808;&#21069;&#30340;&#20107;&#20214;&#29575;&#38169;&#35823;&#22320;&#24402;&#22240;&#20110;&#27835;&#30103;-&#20174;&#32780;&#20135;&#29983;&#19981;&#21487;&#36991;&#20813;&#30340;&#39118;&#38505;&#20559;&#24046;&#12290;&#21363;&#20351;&#36164;&#26684;&#21644;&#27835;&#30103;&#24050;&#32463;&#23545;&#40784;&#65292;&#32456;&#27490;&#20107;&#20214;&#36807;&#31243;&#65288;&#20363;&#22914;&#27515;&#20129;&#65289;&#20063;&#32463;&#24120;&#20572;&#27490;&#24863;&#20852;&#36259;&#30340;&#22797;&#21457;&#20107;&#20214;&#36807;&#31243;&#12290;&#21516;&#26679;&#65292;&#36825;&#20004;&#20010;&#36807;&#31243;&#20063;&#21463;&#21040;&#23457;&#26597;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#22312;&#25972;&#20010;&#38543;&#35775;&#26102;&#38388;&#31383;&#21475;&#20869;&#19981;&#33021;&#35266;&#23519;&#21040;&#20107;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#38169;&#20301;&#22788;&#29702;&#36716;&#21270;&#20026;&#27835;&#30103;&#20999;&#25442;&#38382;&#39064;&#65306;&#19968;&#20123;&#24739;&#32773;&#22312;&#25972;&#20010;&#38543;&#35775;&#26102;&#38388;&#31383;&#21475;&#20869;&#22362;&#25345;&#19968;&#20010;&#29305;&#23450;&#30340;&#27835;&#30103;&#31574;&#30053;&#65292;&#21478;&#19968;&#20123;&#24739;&#32773;&#22312;&#36825;&#20010;&#26102;&#38388;&#31383;&#21475;&#20869;&#32463;&#21382;&#27835;&#30103;&#31574;&#30053;&#30340;&#20999;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#22522;&#26412;&#20803;&#32032;&#65306;&#36890;&#36807;&#19968;&#20010;&#21512;&#29702;&#30340;&#26102;&#21051;&#20999;&#25442;&#27169;&#22411;&#65292;&#27491;&#30830;&#22320;&#24314;&#27169;&#27835;&#30103;&#20043;&#38388;&#30340;&#20999;&#25442;&#21644;&#19981;&#21487;&#36991;&#20813;&#39118;&#38505;&#65292;&#36890;&#36807;&#23558;&#38750;&#35266;&#23519;&#20107;&#20214;&#27169;&#22411;&#21270;&#20026;&#22797;&#21457;&#20107;&#20214;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22797;&#22686;&#21644;&#26411;&#20107;&#20214;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Observational studies of recurrent event rates are common in biomedical statistics. Broadly, the goal is to estimate differences in event rates under two treatments within a defined target population over a specified followup window. Estimation with observational claims data is challenging because while membership in the target population is defined in terms of eligibility criteria, treatment is rarely assigned exactly at the time of eligibility. Ad-hoc solutions to this timing misalignment, such as assigning treatment at eligibility based on subsequent assignment, incorrectly attribute prior event rates to treatment - resulting in immortal risk bias. Even if eligibility and treatment are aligned, a terminal event process (e.g. death) often stops the recurrent event process of interest. Both processes are also censored so that events are not observed over the entire followup window. Our approach addresses misalignment by casting it as a treatment switching problem: some patients are on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#29983;&#29289;&#26631;&#35760;&#21457;&#29616;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#25104;&#31574;&#30053;&#22312;&#30830;&#23450;&#28508;&#22312;PD&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;SNPs&#19978;&#30340;&#19968;&#33268;&#24615;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.03239</link><description>&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22312;&#24085;&#37329;&#26862;&#30149;&#29983;&#29289;&#26631;&#35760;&#21457;&#29616;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Reproducibility of Machine-learning-based Biomarker Discovery in Parkinson's Disease. (arXiv:2304.03239v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#29983;&#29289;&#26631;&#35760;&#21457;&#29616;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#25104;&#31574;&#30053;&#22312;&#30830;&#23450;&#28508;&#22312;PD&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;SNPs&#19978;&#30340;&#19968;&#33268;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#32452;&#20851;&#32852;&#24615;&#30740;&#31350;(GWAS)&#26377;&#21161;&#20110;&#21457;&#29616;&#22312;&#31867;&#20284;&#24085;&#37329;&#26862;&#30149;(PD)&#31561;&#30142;&#30149;&#30340;&#24739;&#32773;&#20013;&#36739;&#20026;&#26222;&#36941;&#30340;&#36951;&#20256;&#21464;&#24322;&#12290;&#22240;&#27492;&#65292;GWAS&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#19982;&#35813;&#30142;&#30149;&#30456;&#20851;&#30340;&#36951;&#20256;&#21464;&#24322;&#12290;&#29305;&#24449;&#36873;&#25321;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#29992;&#20110;&#20998;&#26512;GWAS&#25968;&#25454;&#24182;&#35782;&#21035;&#28508;&#22312;&#30340;&#30142;&#30149;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#28982;&#32780;&#65292;GWAS&#30740;&#31350;&#23384;&#22312;&#25216;&#26415;&#24046;&#24322;&#65292;&#36825;&#20123;&#24046;&#24322;&#20250;&#24433;&#21709;&#35782;&#21035;&#30340;&#29983;&#29289;&#26631;&#35760;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#20363;&#22914;&#22522;&#22240;&#20998;&#22411;&#24179;&#21488;&#30340;&#24046;&#24322;&#20197;&#21450;&#36873;&#25321;&#34987;&#22522;&#22240;&#20998;&#22411;&#30340;&#20010;&#20307;&#30340;&#26631;&#20934;&#30340;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;Genotypes and Phenotypes (dbGaP)&#25968;&#25454;&#24211;&#30340;&#20116;&#20010;GWAS&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#31181;&#25968;&#25454;&#38598;&#25104;&#31574;&#30053;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#31574;&#30053;&#22312;&#35782;&#21035;&#28508;&#22312;PD&#29983;&#29289;&#26631;&#35760;&#29289;&#30340;&#21333;&#26680;&#33527;&#37240;&#22810;&#24577;&#24615;&#65288;SNP&#65289;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#25104;&#31574;&#30053;&#21457;&#29616;&#30340;&#29983;&#29289;&#26631;&#35760;&#30340;&#19968;&#33268;&#24615;&#36739;&#20302;&#65292;&#36825;&#34920;&#26126;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#22522;&#22240;&#29983;&#29289;&#26631;&#35760;&#26102;&#24212;&#35880;&#24910;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Genome-Wide Association Studies (GWAS) help identify genetic variations in people with diseases such as Parkinson's disease (PD), which are less common in those without the disease. Thus, GWAS data can be used to identify genetic variations associated with the disease. Feature selection and machine learning approaches can be used to analyze GWAS data and identify potential disease biomarkers. However, GWAS studies have technical variations that affect the reproducibility of identified biomarkers, such as differences in genotyping platforms and selection criteria for individuals to be genotyped. To address this issue, we collected five GWAS datasets from the database of Genotypes and Phenotypes (dbGaP) and explored several data integration strategies. We evaluated the agreement among different strategies in terms of the Single Nucleotide Polymorphisms (SNPs) that were identified as potential PD biomarkers. Our results showed a low concordance of biomarkers discovered using different dat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;FedBot&#12290;&#23427;&#32467;&#21512;&#20102;Deep Bidirectional Transformer&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#32852;&#21512;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2304.03228</link><description>&lt;p&gt;
FedBot&#65306;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#22686;&#24378;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
FedBot: Enhancing Privacy in Chatbots with Federated Learning. (arXiv:2304.03228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;FedBot&#12290;&#23427;&#32467;&#21512;&#20102;Deep Bidirectional Transformer&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#32852;&#21512;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#20027;&#35201;&#20381;&#36182;&#20110;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#35805;&#35821;&#30340;&#25968;&#25454;&#25512;&#21160;&#65292;&#20294;&#26159;&#22312;&#20849;&#20139;&#25968;&#25454;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#20405;&#29359;&#29992;&#25143;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;FedBot&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#35268;&#27169;&#23458;&#25143;&#25903;&#25345;&#25968;&#25454;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#27010;&#24565;&#39564;&#35777;&#65292;&#23427;&#32467;&#21512;&#20102;Deep Bidirectional Transformer&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#32852;&#21512;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#27010;&#24565;&#39564;&#35777;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#38544;&#31169;&#20445;&#25252;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#25913;&#21464;&#23458;&#25143;&#25903;&#25345;&#34892;&#19994;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots are mainly data-driven and usually based on utterances that might be sensitive. However, training deep learning models on shared data can violate user privacy. Such issues have commonly existed in chatbots since their inception. In the literature, there have been many approaches to deal with privacy, such as differential privacy and secure multi-party computation, but most of them need to have access to users' data. In this context, Federated Learning (FL) aims to protect data privacy through distributed learning methods that keep the data in its location. This paper presents Fedbot, a proof-of-concept (POC) privacy-preserving chatbot that leverages large-scale customer support data. The POC combines Deep Bidirectional Transformer models and federated learning algorithms to protect customer data privacy during collaborative model training. The results of the proof-of-concept showcase the potential for privacy-preserving chatbots to transform the customer support industry by de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DexDeform&#65292;&#20511;&#21161;&#20154;&#31867;&#31034;&#33539;&#21644;&#19981;&#21516;iable&#29289;&#29702;&#23398;&#20064;&#28789;&#24039;&#25805;&#32437;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#25216;&#33021;&#65292;&#21487;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#39640;&#25928;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.03223</link><description>&lt;p&gt;
DexDeform&#65306;&#22522;&#20110;&#20154;&#31867;&#31034;&#33539;&#21644;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#24039;&#22937;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
DexDeform: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics. (arXiv:2304.03223v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DexDeform&#65292;&#20511;&#21161;&#20154;&#31867;&#31034;&#33539;&#21644;&#19981;&#21516;iable&#29289;&#29702;&#23398;&#20064;&#28789;&#24039;&#25805;&#32437;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#25216;&#33021;&#65292;&#21487;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#39640;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#22810;&#25351;&#25163;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#28789;&#24039;&#25805;&#32437;&#12290;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#21487;&#21464;&#24418;&#29289;&#20307;&#20013;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#22797;&#26434;&#24615;&#19979;&#21487;&#33021;&#20250;&#21463;&#21040;&#38459;&#30861;&#65292;&#24182;&#19988;&#20808;&#21069;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#20063;&#21487;&#33021;&#22240;&#20026;&#25163;-&#29289;&#20307;&#20132;&#20114;&#24341;&#36215;&#30340;&#25509;&#35302;&#27169;&#24335;&#22686;&#21152;&#32780;&#21463;&#21040;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DexDeform&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#31034;&#33539;&#30340;&#24039;&#22937;&#25805;&#32437;&#25216;&#33021;&#25277;&#35937;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#36827;&#34892;&#31934;&#21270;&#30340;&#21407;&#21017;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25805;&#20316;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to learn dexterous manipulation of deformable objects using multi-fingered hands. Reinforcement learning approaches for dexterous rigid object manipulation would struggle in this setting due to the complexity of physics interaction with deformable objects. At the same time, previous trajectory optimization approaches with differentiable physics for deformable manipulation would suffer from local optima caused by the explosion of contact modes from hand-object interactions. To address these challenges, we propose DexDeform, a principled framework that abstracts dexterous manipulation skills from human demonstration and refines the learned skills with differentiable physics. Concretely, we first collect a small set of human demonstrations using teleoperation. And we then train a skill model using demonstrations for planning over action abstractions in imagination. To explore the goal space, we further apply augmentations to the existing deformable shapes in demonstra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#36830;&#32493;&#26494;&#24347;&#20998;&#31867;&#20998;&#24067;&#30340;&#24471;&#20998;&#26469;&#26816;&#27979;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#34920;&#26684;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#22343;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03220</link><description>&lt;p&gt;
&#36890;&#36807;Gumbel&#22122;&#22768;&#20998;&#25968;&#21305;&#37197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection via Gumbel Noise Score Matching. (arXiv:2304.03220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#36830;&#32493;&#26494;&#24347;&#20998;&#31867;&#20998;&#24067;&#30340;&#24471;&#20998;&#26469;&#26816;&#27979;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#34920;&#26684;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#22343;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#36830;&#32493;&#26494;&#24347;&#20998;&#31867;&#20998;&#24067;&#30340;&#24471;&#20998;&#65288;&#21363;&#19982;&#36755;&#20837;&#30456;&#20851;&#30340;&#23545;&#25968;&#20284;&#28982;&#26799;&#24230;&#65289;&#26469;&#26816;&#27979;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#24322;&#24120;&#26816;&#27979;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290; GNSM&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#22343;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#25968;&#25454;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;GNSM&#30340;&#28789;&#27963;&#24615;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#20219;&#21153;&#26159;&#26816;&#27979;&#19981;&#33391;&#20998;&#21106;&#39044;&#27979;&#12290; GNSM&#25490;&#21517;&#24322;&#24120;&#30340;&#22270;&#20687;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#20998;&#21106;&#22833;&#36133;&#65292;GNSM&#36755;&#20986;&#30340;&#32467;&#26524;&#19982;&#22522;&#20110;&#22320;&#38754;&#30495;&#23454;&#20540;&#35745;&#31639;&#30340;&#20998;&#21106;&#24230;&#37327;&#39640;&#24230;&#30456;&#20851;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;GNSM&#20351;&#29992;&#30340;&#24471;&#20998;&#21305;&#37197;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#25552;&#20379;&#20102;&#25105;&#20204;&#24037;&#20316;&#30340;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Gumbel Noise Score Matching (GNSM), a novel unsupervised method to detect anomalies in categorical data. GNSM accomplishes this by estimating the scores, i.e. the gradients of log likelihoods w.r.t.~inputs, of continuously relaxed categorical distributions. We test our method on a suite of anomaly detection tabular datasets. GNSM achieves a consistently high performance across all experiments. We further demonstrate the flexibility of GNSM by applying it to image data where the model is tasked to detect poor segmentation predictions. Images ranked anomalous by GNSM show clear segmentation failures, with the outputs of GNSM strongly correlating with segmentation metrics computed on ground-truth. We outline the score matching training objective utilized by GNSM and provide an open-source implementation of our work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03215</link><description>&lt;p&gt;
&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Graph Neural Network with Cross-Attention for Cross-Device User Matching. (arXiv:2304.03215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#21578;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#20247;&#22810;&#39046;&#22495;&#65292;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#23427;&#28041;&#21450;&#20351;&#29992;&#24207;&#21015;&#26085;&#24535;&#26469;&#35782;&#21035;&#21644;&#38142;&#25509;&#23646;&#20110;&#21516;&#19968;&#20154;&#30340;&#19981;&#21516;&#35774;&#22791;&#12290;&#20197;&#24448;&#30340;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#38590;&#20197;&#35299;&#20915;&#26085;&#24535;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#21644;&#39640;&#38454;&#36830;&#25509;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#22270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#23618;&#22270;&#19978;&#19979;&#25991;&#23884;&#20837;(TGCE)&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;HGNN&#65289;&#65292;&#23427;&#20855;&#26377;&#27604;TGCE&#26356;&#20026;&#35745;&#31639;&#25928;&#29575;&#30340;&#20108;&#32423;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;Cross-Att&#65289;&#26426;&#21046;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-device user matching is a critical problem in numerous domains, including advertising, recommender systems, and cybersecurity. It involves identifying and linking different devices belonging to the same person, utilizing sequence logs. Previous data mining techniques have struggled to address the long-range dependencies and higher-order connections between the logs. Recently, researchers have modeled this problem as a graph problem and proposed a two-tier graph contextual embedding (TGCE) neural network architecture, which outperforms previous methods. In this paper, we propose a novel hierarchical graph neural network architecture (HGNN), which has a more computationally efficient second level design than TGCE. Furthermore, we introduce a cross-attention (Cross-Att) mechanism in our model, which improves performance by 5% compared to the state-of-the-art TGCE method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORSE&#30340;&#22522;&#20110;&#38544;&#24335;&#35299;&#21078;&#28210;&#26579;&#30340;&#36890;&#29992;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24110;&#21161;&#34701;&#21512;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.03209</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#19987;&#23478;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#38544;&#24615;&#35299;&#21078;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts. (arXiv:2304.03209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORSE&#30340;&#22522;&#20110;&#38544;&#24335;&#35299;&#21078;&#28210;&#26579;&#30340;&#36890;&#29992;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24110;&#21161;&#34701;&#21512;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#38598;&#25104;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#36817;&#26399;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#20998;&#21106;&#26041;&#27861;&#22312;&#26356;&#22909;&#22320;&#24314;&#27169;&#36825;&#20123;&#20449;&#24687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#20998;&#21106;&#30340;&#21367;&#31215;&#25805;&#20316;&#36890;&#24120;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#36816;&#34892;&#65292;&#36825;&#22312;&#39640;&#39057;&#21306;&#22495;&#21363;&#36793;&#30028;&#21306;&#22495;&#20013;&#22825;&#29983;&#27169;&#31946;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MORSE&#30340;&#36890;&#29992;&#38544;&#24335;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#35299;&#21078;&#23618;&#38754;&#19978;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#36741;&#21161;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20107;&#23454;&#65306;&#30456;&#36739;&#20110;&#31163;&#25955;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#22312;&#25311;&#21512;&#22797;&#26434;&#20449;&#21495;&#21644;&#35299;&#20915;&#35745;&#31639;&#26426;&#22270;&#24418;&#38382;&#39064;&#26102;&#34920;&#29616;&#26356;&#20026;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23558;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#35270;&#20026;&#28210;&#26579;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25345;&#32493;&#22320;&#23545;&#40784;&#31895;&#30053;&#30340;&#20998;&#21106;p&#24182;&#21033;&#29992;&#38543;&#26426;&#19987;&#23478;&#26469;&#29983;&#25104;&#28210;&#26579;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating high-level semantically correlated contents and low-level anatomical features is of central importance in medical image segmentation. Towards this end, recent deep learning-based medical segmentation methods have shown great promise in better modeling such information. However, convolution operators for medical segmentation typically operate on regular grids, which inherently blur the high-frequency regions, i.e., boundary regions. In this work, we propose MORSE, a generic implicit neural rendering framework designed at an anatomical level to assist learning in medical image segmentation. Our method is motivated by the fact that implicit neural representation has been shown to be more effective in fitting complex signals and solving computer graphics problems than discrete grid-based representation. The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. Specifically, we continuously align the coarse segmentation p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20174;111M&#21040;13B&#21442;&#25968;&#30340;&#24320;&#25918;&#35745;&#31639;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411; Cerebras-GPT&#65292;&#23427;&#37319;&#29992;&#20102;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#32553;&#25918;&#35268;&#21017;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#21487;&#39044;&#27979;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#25918;&#21487;&#22797;&#29616;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.03208</link><description>&lt;p&gt;
&#22522;&#20110; Cerebras Wafer-Scale Cluster &#30340;&#24320;&#25918;&#35745;&#31639;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411; Cerebras-GPT &#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster. (arXiv:2304.03208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20174;111M&#21040;13B&#21442;&#25968;&#30340;&#24320;&#25918;&#35745;&#31639;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411; Cerebras-GPT&#65292;&#23427;&#37319;&#29992;&#20102;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#32553;&#25918;&#35268;&#21017;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#21487;&#39044;&#27979;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#25918;&#21487;&#22797;&#29616;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#20197;&#21450;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#21516;&#26102;&#32467;&#21512;&#36825;&#20123;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#20174;111M&#21040;13B&#21442;&#25968;&#30340;&#24320;&#25918;&#35745;&#31639;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411; Cerebras-GPT&#12290;&#25105;&#20204;&#26681;&#25454; DeepMind &#30340; Chinchilla &#32553;&#25918;&#35268;&#21017;&#23545; Eleuther Pile &#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#32473;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#26368;&#39640;&#31934;&#24230;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#21487;&#39044;&#27979;&#30340;&#24130;&#24459;&#32553;&#25918;&#35268;&#24459;&#65292;&#24182;&#19982;&#20854;&#20182;&#20844;&#24320;&#21487;&#29992;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102; Cerebras-GPT &#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#30446;&#26631;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#21253;&#25324;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;($\mu$P)&#22914;&#20309;&#36827;&#19968;&#27493;&#25552;&#39640;&#22823;&#22411;&#27169;&#22411;&#25193;&#23637;&#30340;&#31934;&#24230;&#21644;&#36229;&#21442;&#25968;&#21487;&#39044;&#27979;&#24615;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#20195;&#30721;&#65292;&#20351;&#26412;&#25991;&#25104;&#20026;&#20851;&#20110;&#22312;&#20159;&#32423;&#21442;&#25968;&#35268;&#27169;&#19979;&#27604;&#36739;&#35745;&#31639;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#39318;&#20010;&#24320;&#25918;&#21487;&#22797;&#29616;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study recent research advances that improve large language models through efficient pre-training and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the predictable power-law scaling and compare Cerebras-GPT with other publicly-available models to show all Cerebras-GPT models have state-of-the-art training efficiency on both pre-training and downstream objectives. We describe our learnings including how Maximal Update Parameterization ($\mu$P) can further improve large model scaling, improving accuracy and hyperparameter predictability at scale. We release our pre-trained models and code, making this paper the first open and reproducible work comparing compute-optimal 
&lt;/p&gt;</description></item><item><title>SLM&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#31471;&#21040;&#31471;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#31232;&#30095;&#25513;&#30721;&#26469;&#26368;&#22823;&#21270;&#25152;&#36873;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#21516;&#26102;&#31934;&#30830;&#22320;&#25511;&#21046;&#36873;&#25321;&#30340;&#29305;&#24449;&#25968;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03202</link><description>&lt;p&gt;
SLM&#65306;&#36890;&#36807;&#31232;&#30095;&#21487;&#23398;&#20064;&#25513;&#30721;&#36827;&#34892;&#31471;&#21040;&#31471;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
SLM: End-to-end Feature Selection via Sparse Learnable Masks. (arXiv:2304.03202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03202
&lt;/p&gt;
&lt;p&gt;
SLM&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#31471;&#21040;&#31471;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#31232;&#30095;&#25513;&#30721;&#26469;&#26368;&#22823;&#21270;&#25152;&#36873;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#21516;&#26102;&#31934;&#30830;&#22320;&#25511;&#21046;&#36873;&#25321;&#30340;&#29305;&#24449;&#25968;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20943;&#23569;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#35201;&#27714;&#65292;&#38416;&#26126;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SLM - &#31232;&#30095;&#21487;&#23398;&#20064;&#25513;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#31471;&#21040;&#31471;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#36866;&#24212;&#29305;&#24449;&#32500;&#24230;&#21644;&#26679;&#26412;&#25968;&#37327;&#30340;&#21464;&#21270;&#12290;SLM&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21487;&#23398;&#20064;&#31232;&#30095;&#25513;&#30721;&#65292;&#23427;&#23398;&#20064;&#36873;&#25321;&#21738;&#20123;&#29305;&#24449;&#65292;&#24182;&#20135;&#29983;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#26368;&#22823;&#21270;&#20102;&#25152;&#36873;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#65292;&#36825;&#21487;&#20197;&#20174;&#20114;&#20449;&#24687;&#30340;&#19968;&#27425;&#26041;&#26681;&#26494;&#24347;&#20013;&#23548;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#19968;&#31181;&#32553;&#25918;&#26426;&#21046;&#65292;&#21033;&#29992;sparsemax&#31934;&#30830;&#22320;&#25511;&#21046;&#36873;&#25321;&#30340;&#29305;&#24449;&#25968;&#37327;&#12290;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SLM&#22312;&#21508;&#31181;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection has been widely used to alleviate compute requirements during training, elucidate model interpretability, and improve model generalizability. We propose SLM -- Sparse Learnable Masks -- a canonical approach for end-to-end feature selection that scales well with respect to both the feature dimension and the number of samples. At the heart of SLM lies a simple but effective learnable sparse mask, which learns which features to select, and gives rise to a novel objective that provably maximizes the mutual information (MI) between the selected features and the labels, which can be derived from a quadratic relaxation of mutual information from first principles. In addition, we derive a scaling mechanism that allows SLM to precisely control the number of features selected, through a novel use of sparsemax. This allows for more effective learning as demonstrated in ablation studies. Empirically, SLM achieves state-of-the-art results against a variety of competitive baselines
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#22270;&#34701;&#21512;&#21450;&#20004;&#27493;&#24335;&#36801;&#31227;&#23398;&#20064;&#22686;&#24378;&#30340;&#33258;&#21160;&#20869;&#38236;&#32467;&#30707;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#32958;&#32467;&#30707;&#20998;&#31867;&#20934;&#30830;&#24230;&#36798;6%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.03193</link><description>&lt;p&gt;
&#37319;&#29992;&#22810;&#35270;&#22270;&#34701;&#21512;&#21450;&#20004;&#27493;&#24335;&#36801;&#31227;&#23398;&#20064;&#22686;&#24378;&#30340;&#33258;&#21160;&#20869;&#38236;&#32467;&#30707;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving automatic endoscopic stone recognition using a multi-view fusion approach enhanced with two-step transfer learning. (arXiv:2304.03193v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#22270;&#34701;&#21512;&#21450;&#20004;&#27493;&#24335;&#36801;&#31227;&#23398;&#20064;&#22686;&#24378;&#30340;&#33258;&#21160;&#20869;&#38236;&#32467;&#30707;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#32958;&#32467;&#30707;&#20998;&#31867;&#20934;&#30830;&#24230;&#36798;6%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#34701;&#21512;&#20174;&#19981;&#21516;&#35270;&#35282;&#33719;&#21462;&#30340;&#22270;&#20687;&#20449;&#24687;&#65292;&#26088;&#22312;&#20026;&#37492;&#23450;&#20869;&#38236;&#22270;&#20687;&#20013;&#25152;&#35265;&#30340;&#32958;&#32467;&#30707;&#31867;&#22411;&#20135;&#29983;&#26356;&#20855;&#21306;&#20998;&#24615;&#30340;&#29289;&#20307;&#29305;&#24449;&#12290;&#26412;&#27169;&#22411;&#36827;&#19968;&#27493;&#24212;&#29992;&#20102;&#20004;&#27493;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21644;&#27880;&#24847;&#21147;&#22359;&#26469;&#35843;&#25972;&#23398;&#20064;&#30340;&#29305;&#24449;&#22270;&#12290;&#28145;&#24230;&#29305;&#24449;&#34701;&#21512;&#31574;&#30053;&#23558;&#32958;&#32467;&#30707;&#20998;&#31867;&#20934;&#30830;&#24230;&#27604;&#21333;&#35270;&#22270;&#25552;&#21462;&#20027;&#24178;&#27169;&#22411;&#25552;&#39640;&#20102;6%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
This contribution presents a deep-learning method for extracting and fusing image information acquired from different viewpoints, with the aim to produce more discriminant object features for the identification of the type of kidney stones seen in endoscopic images. The model was further improved with a two-step transfer learning approach and by attention blocks to refine the learned feature maps. Deep feature fusion strategies improved the results of single view extraction backbone models by more than 6% in terms of accuracy of the kidney stones classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#30697;&#38453;-&#21521;&#37327;&#30456;&#20056;&#27169;&#22411;&#19979;&#38024;&#23545;&#21508;&#31181;Schatten&#33539;&#25968;&#30340;&#31209;-1&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#65292;&#34920;&#26126;Krylov&#26041;&#27861;&#65288;&#20960;&#20046;&#65289;&#23454;&#29616;&#35889;&#65292;Frobenius&#21644;&#26680;&#20302;&#31209;&#36924;&#36817;&#30340;&#20449;&#24687;&#29109;&#19978;&#30340;&#26368;&#20248;&#30697;&#38453;-&#21521;&#37327;&#31215;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.03191</link><description>&lt;p&gt;
Krylov&#26041;&#27861;&#22312;&#20302;&#31209;&#36924;&#36817;&#20013;&#65288;&#20960;&#20046;&#65289;&#26368;&#20248;
&lt;/p&gt;
&lt;p&gt;
Krylov Methods are (nearly) Optimal for Low-Rank Approximation. (arXiv:2304.03191v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#30697;&#38453;-&#21521;&#37327;&#30456;&#20056;&#27169;&#22411;&#19979;&#38024;&#23545;&#21508;&#31181;Schatten&#33539;&#25968;&#30340;&#31209;-1&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#65292;&#34920;&#26126;Krylov&#26041;&#27861;&#65288;&#20960;&#20046;&#65289;&#23454;&#29616;&#35889;&#65292;Frobenius&#21644;&#26680;&#20302;&#31209;&#36924;&#36817;&#30340;&#20449;&#24687;&#29109;&#19978;&#30340;&#26368;&#20248;&#30697;&#38453;-&#21521;&#37327;&#31215;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#30697;&#38453;-&#21521;&#37327;&#30456;&#20056;&#27169;&#22411;&#19979;&#65292;&#38024;&#23545;&#21508;&#31181;Schatten&#33539;&#25968;&#30340;&#31209;-1&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#65306;$$\min_{\|u\|_2=1} \|A(I - u u^\top)\|_{\mathcal{S}_p}$$ &#20854;&#20013;$\|M\|_{\mathcal{S}_p}$&#34920;&#31034;$M$&#30340;&#22855;&#24322;&#20540;&#30340;$\ell_p$&#33539;&#25968;&#12290;&#32473;&#23450;$\varepsilon&gt;0$&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36755;&#20986;&#19968;&#20010;&#21333;&#20301;&#21521;&#37327;$v$&#65292;&#20351;&#24471;$$\|A(I - vv^\top)\|_{\mathcal{S}_p} \leq (1+\varepsilon) \min_{\|u\|_2=1}\|A(I - u u^\top)\|_{\mathcal{S}_p}.$$ &#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;Krylov&#26041;&#27861;&#65288;&#20960;&#20046;&#65289;&#23454;&#29616;&#35889;&#65288;$p=\infty$&#65289;&#65292;Frobenius&#65288;$p=2$&#65289;&#21644;&#26680;&#65288;$p=1$&#65289;&#20302;&#31209;&#36924;&#36817;&#30340;&#20449;&#24687;&#29109;&#19978;&#30340;&#26368;&#20248;&#30697;&#38453;-&#21521;&#37327;&#31215;&#25968;&#37327;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#35889;&#20302;&#31209;&#36924;&#36817;&#65292;&#25105;&#20204;&#23637;&#31034;&#20219;&#20309;&#31639;&#27861;&#37117;&#38656;&#35201;$\Omega\left(\log(n)/\varepsilon^{1/2}\right)$&#20010;&#30697;&#38453;-&#21521;&#37327;&#31215;&#65292;&#23436;&#20840;&#21305;&#37197;Krylov&#26041;&#27861;[MM15, BCW22]&#24471;&#21040;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#22238;&#31572;&#20102;[Woo14]&#30340;&#38382;&#39064;1&#65292;&#20026;&#31639;&#27861;&#30340;&#32570;&#20047;&#36827;&#23637;&#25552;&#20379;&#20102;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of rank-$1$ low-rank approximation (LRA) in the matrix-vector product model under various Schatten norms: $$  \min_{\|u\|_2=1} \|A (I - u u^\top)\|_{\mathcal{S}_p} , $$ where $\|M\|_{\mathcal{S}_p}$ denotes the $\ell_p$ norm of the singular values of $M$. Given $\varepsilon&gt;0$, our goal is to output a unit vector $v$ such that $$  \|A(I - vv^\top)\|_{\mathcal{S}_p} \leq (1+\varepsilon) \min_{\|u\|_2=1}\|A(I - u u^\top)\|_{\mathcal{S}_p}. $$ Our main result shows that Krylov methods (nearly) achieve the information-theoretically optimal number of matrix-vector products for Spectral ($p=\infty$), Frobenius ($p=2$) and Nuclear ($p=1$) LRA.  In particular, for Spectral LRA, we show that any algorithm requires $\Omega\left(\log(n)/\varepsilon^{1/2}\right)$ matrix-vector products, exactly matching the upper bound obtained by Krylov methods [MM15, BCW22]. Our lower bound addresses Open Question 1 in [Woo14], providing evidence for the lack of progress on algorithms for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#21069;&#21521;&#21069;&#21521;&#23398;&#20064;&#31639;&#27861;&#20110;&#22810;&#36755;&#20986;&#24863;&#30693;&#26426;&#65292;&#20165;&#38656;&#21333;&#20010;&#30697;&#38453;&#20056;&#27861;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#24615;&#33021;&#19982;&#38544;&#34255;&#23618;&#26356;&#22810;&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2304.03189</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#22810;&#36755;&#20986;&#24863;&#30693;&#26426;&#30340;&#21069;&#21521;&#21069;&#21521;&#23398;&#20064;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
The Concept of Forward-Forward Learning Applied to a Multi Output Perceptron. (arXiv:2304.03189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#21069;&#21521;&#21069;&#21521;&#23398;&#20064;&#31639;&#27861;&#20110;&#22810;&#36755;&#20986;&#24863;&#30693;&#26426;&#65292;&#20165;&#38656;&#21333;&#20010;&#30697;&#38453;&#20056;&#27861;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#24615;&#33021;&#19982;&#38544;&#34255;&#23618;&#26356;&#22810;&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340;&#21069;&#21521;&#21069;&#21521;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#21333;&#20010;&#22810;&#36755;&#20986;&#24863;&#30693;&#26426;&#27169;&#22411;&#20013;&#65292;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#31995;&#32479;&#21442;&#25968;&#30340;&#35757;&#32451;&#26159;&#22522;&#20110;&#23545;&#36755;&#20837;&#26679;&#26412;&#30340;&#27491;&#30830;&#65288;&#38169;&#35823;&#65289;&#26631;&#35760;&#30340;&#8220;&#22909;&#24230;&#8221;&#22686;&#21152;&#65288;&#20943;&#23569;&#65289;&#12290;&#22522;&#26412;&#30340;&#25968;&#20540;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#22810;&#36755;&#20986;&#24863;&#30693;&#26426;&#26377;&#25928;&#22320;&#22788;&#29702;&#20855;&#26377;&#38750;&#32447;&#24615;&#20915;&#31574;&#36793;&#30028;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#20854;&#24615;&#33021;&#24635;&#20307;&#19978;&#19982;&#38544;&#34255;&#23618;&#26356;&#22810;&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#28857;&#22312;&#20110;&#20165;&#28041;&#21450;&#21333;&#20010;&#30697;&#38453;&#20056;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of a recently proposed Forward-Forward learning algorithm for fully connected artificial neural networks is applied to a single multi output perceptron for classification. The parameters of the system are trained with respect to increased (decreased) "goodness" for correctly (incorrectly) labelled input samples. Basic numerical tests demonstrate that the trained perceptron effectively deals with data sets that have non-linear decision boundaries. Moreover, the overall performance is comparable to more complex neural networks with hidden layers. The benefit of the approach presented here is that it only involves a single matrix multiplication.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#26032;&#30340;Oracle&#19981;&#31561;&#24335;&#65292;&#22312;&#36755;&#20837;&#22495;&#19978;&#30340;&#19968;&#33324;&#30418;&#35745;&#25968;&#32500;&#24230;&#20551;&#35774;&#21644;&#22122;&#22768;&#26465;&#20214;&#25110;&#26631;&#20934;&#24179;&#28369;&#26465;&#20214;&#19979;&#65292;&#23545;&#20110;&#39640;&#26031;&#25104;&#23545;&#25490;&#21517;&#20272;&#35745;&#22120;&#24471;&#20986;&#20102;&#24555;&#36895;&#23398;&#20064;&#29575;&#12290;&#36825;&#34920;&#26126;&#65292;&#36755;&#20837;&#31354;&#38388;&#30340;&#20302;&#22266;&#26377;&#32500;&#24230;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#12290;</title><link>http://arxiv.org/abs/2304.03185</link><description>&lt;p&gt;
&#24102;&#39640;&#26031;&#26680;&#30340;&#25104;&#23545;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Pairwise Ranking with Gaussian Kernels. (arXiv:2304.03185v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#26032;&#30340;Oracle&#19981;&#31561;&#24335;&#65292;&#22312;&#36755;&#20837;&#22495;&#19978;&#30340;&#19968;&#33324;&#30418;&#35745;&#25968;&#32500;&#24230;&#20551;&#35774;&#21644;&#22122;&#22768;&#26465;&#20214;&#25110;&#26631;&#20934;&#24179;&#28369;&#26465;&#20214;&#19979;&#65292;&#23545;&#20110;&#39640;&#26031;&#25104;&#23545;&#25490;&#21517;&#20272;&#35745;&#22120;&#24471;&#20986;&#20102;&#24555;&#36895;&#23398;&#20064;&#29575;&#12290;&#36825;&#34920;&#26126;&#65292;&#36755;&#20837;&#31354;&#38388;&#30340;&#20302;&#22266;&#26377;&#32500;&#24230;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#39640;&#26031;&#26680;&#30340;&#27491;&#21017;&#25104;&#23545;&#25490;&#21517;&#26159;&#21069;&#27839;&#30340;&#23398;&#20064;&#31639;&#27861;&#20043;&#19968;&#12290;&#23613;&#31649;&#24212;&#29992;&#33539;&#22260;&#24191;&#27867;&#65292;&#20294;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#26469;&#25903;&#25345;&#36825;&#31181;&#25490;&#21517;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20026;&#27491;&#21017;&#25104;&#23545;&#25490;&#21517;&#24320;&#21457;&#26032;&#30340; Oracle &#19981;&#31561;&#24335;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20511;&#21161;&#36825;&#20123; Oracle &#19981;&#31561;&#24335;&#65292;&#32467;&#21512;&#36755;&#20837;&#22495;&#19978;&#30340;&#19968;&#33324;&#30418;&#35745;&#25968;&#32500;&#24230;&#20551;&#35774;&#21644;&#22122;&#22768;&#26465;&#20214;&#25110;&#26631;&#20934;&#24179;&#28369;&#26465;&#20214;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#39640;&#26031;&#25490;&#21517;&#20272;&#35745;&#22120;&#30340;&#24555;&#36895;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20272;&#35745;&#65292;&#24182;&#26174;&#31034;&#36755;&#20837;&#31354;&#38388;&#30340;&#20302;&#22266;&#26377;&#32500;&#24230;&#21487;&#20197;&#24110;&#21161;&#36895;&#29575;&#36991;&#20813;&#32500;&#24230;&#35781;&#21650;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularized pairwise ranking with Gaussian kernels is one of the cutting-edge learning algorithms. Despite a wide range of applications, a rigorous theoretical demonstration still lacks to support the performance of such ranking estimators. This work aims to fill this gap by developing novel oracle inequalities for regularized pairwise ranking. With the help of these oracle inequalities, we derive fast learning rates of Gaussian ranking estimators under a general box-counting dimension assumption on the input domain combined with the noise conditions or the standard smoothness condition. Our theoretical analysis improves the existing estimates and shows that a low intrinsic dimension of input space can help the rates circumvent the curse of dimensionality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19977;&#32500;&#32467;&#32928;&#34920;&#38754;&#37325;&#24314;&#65292;&#22312;&#22270;&#20687;&#39044;&#22788;&#29702;&#30340;&#36807;&#31243;&#20013;&#36890;&#36807;&#32416;&#27491;&#23616;&#37096;&#27424;&#26333;&#20809;&#21644;&#36807;&#26333;&#20809;&#26469;&#25552;&#39640;&#37325;&#24314;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.03171</link><description>&lt;p&gt;
&#20197;&#28145;&#24230;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#26333;&#20809;&#22686;&#24378;&#20316;&#20026;&#20934;&#30830;&#30340;&#19977;&#32500;&#32467;&#32928;&#34920;&#38754;&#37325;&#24314;&#30340;&#39044;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based image exposure enhancement as a pre-processing for an accurate 3D colon surface reconstruction. (arXiv:2304.03171v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19977;&#32500;&#32467;&#32928;&#34920;&#38754;&#37325;&#24314;&#65292;&#22312;&#22270;&#20687;&#39044;&#22788;&#29702;&#30340;&#36807;&#31243;&#20013;&#36890;&#36807;&#32416;&#27491;&#23616;&#37096;&#27424;&#26333;&#20809;&#21644;&#36807;&#26333;&#20809;&#26469;&#25552;&#39640;&#37325;&#24314;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#30340;&#22270;&#20687;&#39044;&#22788;&#29702;&#25913;&#21892;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#32928;&#37096;&#20998;&#19977;&#32500;&#37325;&#24314;&#12290;&#20551;&#35774;&#22312;&#20869;&#31397;&#38236;&#26816;&#26597;&#20013;&#24212;&#35813;&#32416;&#27491;&#23616;&#37096;&#27424;&#26333;&#20809;&#21644;&#36807;&#26333;&#20809;&#32780;&#19981;&#26159;&#20840;&#23616;&#22270;&#20687;&#29031;&#26126;&#26657;&#27491;&#12290;&#39318;&#20808;&#27010;&#36848;&#20102;&#21253;&#25324;&#22270;&#20687;&#26333;&#20809;&#26657;&#27491;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;-SLAM&#30340;&#27969;&#31243;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#19982;&#36866;&#24403;&#29031;&#26126;&#26657;&#27491;&#21644;&#26080;&#26657;&#27491;&#30340;&#24773;&#20917;&#19979;&#32467;&#32928;&#20869;&#38236;&#36712;&#36857;&#30340;&#37325;&#24314;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This contribution shows how an appropriate image pre-processing can improve a deep-learning based 3D reconstruction of colon parts. The assumption is that, rather than global image illumination corrections, local under- and over-exposures should be corrected in colonoscopy. An overview of the pipeline including the image exposure correction and a RNN-SLAM is first given. Then, this paper quantifies the reconstruction accuracy of the endoscope trajectory in the colon with and without appropriate illumination correction
&lt;/p&gt;</description></item><item><title>&#39057;&#35889;&#24037;&#20855;&#21253;STAG&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#30340;&#39057;&#35889;&#22270;&#31639;&#27861;&#65292;&#20854;&#20013;&#36824;&#21253;&#25324;&#26412;&#22320;&#22270;&#32858;&#31867;&#32452;&#20214;&#12290;&#27492;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;STAG&#30340;&#29992;&#25143;&#25351;&#21335;&#12289;&#23637;&#31034;&#30740;&#31350;&#20197;&#21450;&#24320;&#21457;&#32972;&#21518;&#30340;&#25216;&#26415;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2304.03170</link><description>&lt;p&gt;
&#22270;&#35889;&#31639;&#27861;&#30340;&#39057;&#35889;&#24037;&#20855;&#21253;&#65306;&#25216;&#26415;&#25253;&#21578;&#65288;1&#65289;
&lt;/p&gt;
&lt;p&gt;
Spectral Toolkit of Algorithms for Graphs: Technical Report (1). (arXiv:2304.03170v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03170
&lt;/p&gt;
&lt;p&gt;
&#39057;&#35889;&#24037;&#20855;&#21253;STAG&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#30340;&#39057;&#35889;&#22270;&#31639;&#27861;&#65292;&#20854;&#20013;&#36824;&#21253;&#25324;&#26412;&#22320;&#22270;&#32858;&#31867;&#32452;&#20214;&#12290;&#27492;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;STAG&#30340;&#29992;&#25143;&#25351;&#21335;&#12289;&#23637;&#31034;&#30740;&#31350;&#20197;&#21450;&#24320;&#21457;&#32972;&#21518;&#30340;&#25216;&#26415;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#35889;&#31639;&#27861;&#30340;&#39057;&#35889;&#24037;&#20855;&#21253;&#65288;STAG&#65289;&#26159;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#39057;&#35889;&#22270;&#31639;&#27861;&#30340;&#24320;&#28304;&#24211;&#65292;&#20854;&#24320;&#21457;&#22987;&#20110;2022&#24180;9&#26376;&#12290;&#25105;&#20204;&#30446;&#21069;&#24050;&#32463;&#23436;&#25104;&#20102;&#26412;&#22320;&#22270;&#32858;&#31867;&#30340;&#32452;&#20214;&#65292;&#24182;&#19988;&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;STAG&#30340;&#29992;&#25143;&#25351;&#21335;&#12289;&#23637;&#31034;&#30740;&#31350;&#20197;&#21450;&#25105;&#20204;&#24320;&#21457;&#32972;&#21518;&#30340;&#20960;&#20010;&#25216;&#26415;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral Toolkit of Algorithms for Graphs (STAG) is an open-source library for efficient spectral graph algorithms, and its development starts in September 2022. We have so far finished the component on local graph clustering, and this technical report presents a user's guide to STAG, showcase studies, and several technical considerations behind our development.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;EPAS&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#32479;&#19968;&#20102;&#24050;&#26377;&#30340;EPAS</title><link>http://arxiv.org/abs/2304.03146</link><description>&lt;p&gt;
&#24102;&#19968;&#33324;&#33539;&#25968;&#30446;&#26631;&#30340;&#32858;&#31867;&#21442;&#25968;&#36817;&#20284;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Parameterized Approximation Schemes for Clustering with General Norm Objectives. (arXiv:2304.03146v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03146
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;EPAS&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#32479;&#19968;&#20102;&#24050;&#26377;&#30340;EPAS
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#35774;&#35745;&#36816;&#34892;&#26102;&#38388;&#20026;$f(k, \epsilon)poly(n)$&#30340;$k$-&#32858;&#31867;&#38382;&#39064;&#30340;$(1+\epsilon)$-&#36817;&#20284;&#31639;&#27861;&#12290;&#24050;&#26377;&#32467;&#26524;&#22788;&#29702;&#22522;&#26412;&#30446;&#26631;&#65288;&#22914;$k$-&#20013;&#24515;&#65292;$k$-&#20013;&#20301;&#25968;&#21644;$k$-&#22343;&#20540;&#65289;&#30340;&#38382;&#39064;&#65292;&#20294;&#26159;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#30446;&#26631;&#21644;&#24230;&#37327;&#31354;&#38388;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;EPAS&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#32858;&#31867;&#38382;&#39064;&#24182;&#32479;&#19968;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;EPAS&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the well-studied algorithmic regime of designing a $(1+\epsilon)$-approximation algorithm for a $k$-clustering problem that runs in time $f(k,\epsilon)poly(n)$ (sometimes called an efficient parameterized approximation scheme or EPAS for short). Notable results of this kind include EPASes in the high-dimensional Euclidean setting for $k$-center [Bad\u{o}iu, Har-Peled, Indyk; STOC'02] as well as $k$-median, and $k$-means [Kumar, Sabharwal, Sen; J. ACM 2010]. However, existing EPASes handle only basic objectives (such as $k$-center, $k$-median, and $k$-means) and are tailored to the specific objective and metric space.  Our main contribution is a clean and simple EPAS that settles more than ten clustering problems (across multiple well-studied objectives as well as metric spaces) and unifies well-known EPASes. Our algorithm gives EPASes for a large variety of clustering objectives (for example, $k$-means, $k$-center, $k$-median, priority $k$-center, $\ell$-centrum, o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#26041;&#27861;&#21517;&#20026;d-SAGE&#65292;&#29992;&#20110;&#21152;&#36895;SAGE&#36924;&#36817;&#31639;&#27861;&#65292;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;$d$-SAGE&#30340;&#36924;&#36817;&#35823;&#24046;&#20250;&#25910;&#25947;&#20110;&#38646;&#65292;&#23454;&#39564;&#19978;&#20307;&#29616;&#20102;&#39640;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.03113</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#30340;SAGE&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient SAGE Estimation via Causal Structure Learning. (arXiv:2304.03113v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03113
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#26041;&#27861;&#21517;&#20026;d-SAGE&#65292;&#29992;&#20110;&#21152;&#36895;SAGE&#36924;&#36817;&#31639;&#27861;&#65292;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;$d$-SAGE&#30340;&#36924;&#36817;&#35823;&#24046;&#20250;&#25910;&#25947;&#20110;&#38646;&#65292;&#23454;&#39564;&#19978;&#20307;&#29616;&#20102;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley Additive Global Importance (SAGE)&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26377;&#21560;&#24341;&#21147;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#20844;&#24179;&#22320;&#23558;&#20840;&#23616;&#37325;&#35201;&#24615;&#24402;&#22240;&#20110;&#27169;&#22411;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#31934;&#30830;&#35745;&#31639;&#38656;&#35201;&#35745;&#31639;&#29305;&#24449;&#38598;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#21097;&#20313;&#24615;&#33021;&#36129;&#29486;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#26114;&#36149;&#65292;&#23588;&#20854;&#26159;&#22240;&#20026;&#20272;&#35745;&#21097;&#20313;&#24615;&#33021;&#36129;&#29486;&#38656;&#35201;&#20174;&#26465;&#20214;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#22240;&#27492;&#65292;SAGE&#36924;&#36817;&#31639;&#27861;&#21482;&#32771;&#34385;&#20102;&#19968;&#23567;&#37096;&#20998;&#29305;&#24449;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$d$-SAGE&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;SAGE&#36924;&#36817;&#12290;$d$-SAGE&#26159;&#30001;&#20110;&#35266;&#23519;&#21040;&#29305;&#24449;&#21644;&#27169;&#22411;&#30446;&#26631;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615; (CI) &#24847;&#21619;&#30528;&#38646;&#21097;&#20313;&#36129;&#29486;&#65292;&#22240;&#27492;&#21487;&#20197;&#36339;&#36807;&#23427;&#20204;&#30340;&#35745;&#31639;&#12290;&#20026;&#20102;&#35782;&#21035;CI&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;(CSL)&#26469;&#25512;&#26029;&#19968;&#20010;&#22270;&#65292;&#35813;&#22270;&#23558;&#25968;&#25454;&#20013;&#30340;(&#26465;&#20214;)&#29420;&#31435;&#24615;&#32534;&#30721;&#20026;$d$&#20998;&#31163;&#12290;&#36825;&#22312;&#35745;&#31639;&#19978;&#26356;&#26377;&#25928;&#65292;&#22240;&#20026;&#25105;&#20204;&#21482;&#38656;&#35201;&#35745;&#31639;&#38750;$d$&#20998;&#31163;&#29305;&#24449;&#38598;&#30340;SAGE&#20540;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#35828;&#26126;&#38543;&#30528;$d$&#30340;&#22686;&#21152;&#65292;$d$-SAGE&#30340;&#36924;&#36817;&#35823;&#24046;&#20250;&#25910;&#25947;&#20110;&#38646;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$d$-SAGE&#38656;&#35201;&#27604;&#29616;&#26377;&#30340;SAGE&#36924;&#36817;&#31639;&#27861;&#26356;&#23569;&#30340;&#29305;&#24449;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Shapley Additive Global Importance (SAGE) value is a theoretically appealing interpretability method that fairly attributes global importance to a model's features. However, its exact calculation requires the computation of the feature's surplus performance contributions over an exponential number of feature sets. This is computationally expensive, particularly because estimating the surplus contributions requires sampling from conditional distributions. Thus, SAGE approximation algorithms only take a fraction of the feature sets into account. We propose $d$-SAGE, a method that accelerates SAGE approximation. $d$-SAGE is motivated by the observation that conditional independencies (CIs) between a feature and the model target imply zero surplus contributions, such that their computation can be skipped. To identify CIs, we leverage causal structure learning (CSL) to infer a graph that encodes (conditional) independencies in the data as $d$-separations. This is computationally more ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35889;/&#22270;&#24418;&#20449;&#24687;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;Fiedler&#27491;&#21017;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#24213;&#23618;&#22270;&#30340;Fiedler&#20540;&#20316;&#20026;&#27491;&#21017;&#21270;&#24037;&#20855;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21152;&#26435;&#30340; $\text{L}_1$ &#24809;&#32602;&#24182;&#25552;&#20379;&#32479;&#19968;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#30340;&#20998;&#26512;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03096</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35889;&#38388;&#38553;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Spectral Gap Regularization of Neural Networks. (arXiv:2304.03096v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35889;/&#22270;&#24418;&#20449;&#24687;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;Fiedler&#27491;&#21017;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#24213;&#23618;&#22270;&#30340;Fiedler&#20540;&#20316;&#20026;&#27491;&#21017;&#21270;&#24037;&#20855;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21152;&#26435;&#30340; $\text{L}_1$ &#24809;&#32602;&#24182;&#25552;&#20379;&#32479;&#19968;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#30340;&#20998;&#26512;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;Fiedler&#27491;&#21017;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#35889;/&#22270;&#24418;&#20449;&#24687;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#24120;&#24120;&#36890;&#36807;&#20840;&#23616;/&#22343;&#21248;&#22320;&#24809;&#32602;&#26435;&#37325;&#26469;&#23454;&#29616;&#65292;&#24573;&#30053;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#36890;&#24615;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#24213;&#23618;&#22270;&#30340;Fiedler&#20540;&#20316;&#20026;&#27491;&#21017;&#21270;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#35889;&#22270;&#29702;&#35770;&#25552;&#20379;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#29702;&#35770;&#21160;&#26426;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Fiedler&#20540;&#30340;&#20960;&#20010;&#26377;&#29992;&#23646;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#27491;&#21017;&#21270;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#35757;&#32451;&#26399;&#38388;&#26356;&#24555;&#22320;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#26694;&#26550;&#30340;&#21478;&#19968;&#31181;&#24418;&#24335;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21152;&#26435;&#30340; $\text{L}_1$ &#24809;&#32602;&#65292;&#22240;&#27492;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#31232;&#30095;&#24863;&#24212;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;Rademacher&#22797;&#26434;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;Fiedler&#27491;&#21017;&#21270;&#30340;&#32479;&#19968;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Fiedler regularization, a novel approach for regularizing neural networks that utilizes spectral/graphical information. Existing regularization methods often focus on penalizing weights in a global/uniform manner that ignores the connectivity structure of the neural network. We propose to use the Fiedler value of the neural network's underlying graph as a tool for regularization. We provide theoretical motivation for this approach via spectral graph theory. We demonstrate several useful properties of the Fiedler value that make it useful as a regularization tool. We provide an approximate, variational approach for faster computation during training. We provide an alternative formulation of this framework in the form of a structurally weighted $\text{L}_1$ penalty, thus linking our approach to sparsity induction. We provide uniform generalization error bounds for Fiedler regularization via a Rademacher complexity analysis. We performed experiments on datasets that compare F
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;PopulAtion Parameter Averaging (PAPA)&#65292;&#33021;&#21516;&#26102;&#25317;&#26377;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03094</link><description>&lt;p&gt;
PopulAtion Parameter Averaging (PAPA)&#65288;&#20154;&#21475;&#21442;&#25968;&#24179;&#22343;&#65289;
&lt;/p&gt;
&lt;p&gt;
PopulAtion Parameter Averaging (PAPA). (arXiv:2304.03094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03094
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;PopulAtion Parameter Averaging (PAPA)&#65292;&#33021;&#21516;&#26102;&#25317;&#26377;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#23558;&#22810;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#32452;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#25104;&#26412;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#36827;&#34892;&#24179;&#22343;&#26469;&#23558;&#23427;&#20204;&#21512;&#24182;&#25104;&#19968;&#20010;&#65288;&#27169;&#22411;&#27748;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#27604;&#38598;&#25104;&#34920;&#29616;&#26356;&#24046;&#12290;&#24403;&#26435;&#37325;&#36275;&#22815;&#30456;&#20284;&#65288;&#22312;&#26435;&#37325;&#25110;&#29305;&#24449;&#31354;&#38388;&#20013;&#65289;&#21487;&#20197;&#24456;&#22909;&#22320;&#24179;&#22343;&#65292;&#20294;&#36275;&#22815;&#19981;&#21516;&#20197;&#20174;&#32452;&#21512;&#20013;&#21463;&#30410;&#26102;&#65292;&#26435;&#37325;&#24179;&#22343;&#25165;&#26159;&#26377;&#30410;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PopulAtion Parameter Averaging (PAPA)&#65292;&#19968;&#31181;&#23558;&#38598;&#25104;&#30340;&#26222;&#36941;&#24615;&#19982;&#26435;&#37325;&#24179;&#22343;&#30340;&#25928;&#29575;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;PAPA&#21033;&#29992;&#19981;&#21516;&#27169;&#22411;&#65288;&#22312;&#19981;&#21516;&#25968;&#25454;&#39034;&#24207;&#65292;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#19978;&#35757;&#32451;&#65289;&#30340;&#20154;&#21475;&#65292;&#32780;&#20598;&#23572;&#65288;&#19981;&#35201;&#22826;&#39057;&#32321;&#65292;&#20063;&#19981;&#35201;&#22826;&#31232;&#30095;&#65289;&#29992;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#20195;&#26367;&#20154;&#21475;&#26435;&#37325;&#30340;&#24179;&#22343;&#20540;&#12290;PAPA&#20943;&#23569;&#20102;&#24179;&#22343;&#20540;&#21644;&#38598;&#25104;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods combine the predictions of multiple models to improve performance, but they require significantly higher computation costs at inference time. To avoid these costs, multiple neural networks can be combined into one by averaging their weights (model soups). However, this usually performs significantly worse than ensembling. Weight averaging is only beneficial when weights are similar enough (in weight or feature space) to average well but different enough to benefit from combining them. Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a method that combines the generality of ensembling with the efficiency of weight averaging. PAPA leverages a population of diverse models (trained on different data orders, augmentations, and regularizations) while occasionally (not too often, not too rarely) replacing the weights of the networks with the population average of the weights. PAPA reduces the performance gap between averaging and ensembling, increasing th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#22270;&#24418;&#25968;&#25454;&#30340;&#21453;&#23398;&#20064;&#65292;&#26088;&#22312;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#65292;&#19982;&#20854;&#20182;&#26694;&#26550;&#30456;&#27604;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24402;&#32435;&#24335;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#35753;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#21160;&#24577;&#25913;&#21464;&#30340;&#22270;&#24418;&#26102;&#26356;&#20855;&#26377;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03093</link><description>&lt;p&gt;
&#24402;&#32435;&#24335;&#22270;&#24418;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inductive Graph Unlearning. (arXiv:2304.03093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#22270;&#24418;&#25968;&#25454;&#30340;&#21453;&#23398;&#20064;&#65292;&#26088;&#22312;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#65292;&#19982;&#20854;&#20182;&#26694;&#26550;&#30456;&#27604;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24402;&#32435;&#24335;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#35753;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#21160;&#24577;&#25913;&#21464;&#30340;&#22270;&#24418;&#26102;&#26356;&#20855;&#26377;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#26426;&#22120;&#21453;&#23398;&#20064;&#8221;&#26159;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23436;&#20840;&#21024;&#38500;&#35201;&#21024;&#38500;&#30340;&#26679;&#26412;&#23545;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#36129;&#29486;&#21644;&#20449;&#24687;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#20854;&#20182;&#26679;&#26412;&#30340;&#36129;&#29486;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#21453;&#23398;&#20064;&#26694;&#26550;&#24050;&#34987;&#25552;&#20986;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#19987;&#27880;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#12290;&#20026;&#20102;&#23558;&#21453;&#23398;&#20064;&#25193;&#23637;&#21040;&#22270;&#24418;&#25968;&#25454;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;GraphEraser&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#26159;GraphEraser&#19987;&#38376;&#38024;&#23545;&#36716;&#31227;&#22270;&#35774;&#23450;&#36827;&#34892;&#35774;&#35745;&#65292;&#22312;&#35813;&#35774;&#23450;&#19979;&#65292;&#22270;&#24418;&#26159;&#38745;&#24577;&#30340;&#65292;&#27979;&#35797;&#33410;&#28857;&#30340;&#23646;&#24615;&#21644;&#36793;&#32536;&#22312;&#35757;&#32451;&#26399;&#38388;&#26159;&#21487;&#35265;&#30340;&#12290;&#23545;&#20110;&#24402;&#32435;&#24335;&#30340;&#35774;&#32622;&#26159;&#19981;&#21512;&#36866;&#30340;&#65292;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#22270;&#24418;&#21487;&#20197;&#26159;&#21160;&#24577;&#30340;&#65292;&#27979;&#35797;&#22270;&#24418;&#20449;&#24687;&#20107;&#20808;&#26159;&#19981;&#21487;&#35265;&#30340;&#12290;&#36825;&#31181;&#24402;&#32435;&#33021;&#21147;&#23545;&#20110;&#20855;&#26377;&#19981;&#26029;&#21457;&#23637;&#30340;&#22270;&#24418;&#65288;&#22914;&#31038;&#20132;&#23186;&#20307;&#21644;&#20132;&#26131;&#32593;&#32476;&#65289;&#30340;&#29983;&#20135;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;G...
&lt;/p&gt;
&lt;p&gt;
As a way to implement the "right to be forgotten" in machine learning, \textit{machine unlearning} aims to completely remove the contributions and information of the samples to be deleted from a trained model without affecting the contributions of other samples. Recently, many frameworks for machine unlearning have been proposed, and most of them focus on image and text data. To extend machine unlearning to graph data, \textit{GraphEraser} has been proposed. However, a critical issue is that \textit{GraphEraser} is specifically designed for the transductive graph setting, where the graph is static and attributes and edges of test nodes are visible during training. It is unsuitable for the inductive setting, where the graph could be dynamic and the test graph information is invisible in advance. Such inductive capability is essential for production machine learning systems with evolving graphs like social media and transaction networks. To fill this gap, we propose the \underline{{\bf G
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#19981;&#33391;&#36712;&#36857;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#38450;&#27490;&#36127;&#38754;&#21103;&#20316;&#29992;&#23454;&#29616;&#23433;&#20840;MDP&#35268;&#21010;</title><link>http://arxiv.org/abs/2304.03081</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19981;&#33391;&#36712;&#36857;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#38450;&#27490;&#36127;&#38754;&#21103;&#20316;&#29992;&#23454;&#29616;&#23433;&#20840;MDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe MDP Planning by Learning Temporal Patterns of Undesirable Trajectories and Averting Negative Side Effects. (arXiv:2304.03081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03081
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19981;&#33391;&#36712;&#36857;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#38450;&#27490;&#36127;&#38754;&#21103;&#20316;&#29992;&#23454;&#29616;&#23433;&#20840;MDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;MDP&#35268;&#21010;&#20013;&#65292;&#22522;&#20110;&#24403;&#21069;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#20195;&#20215;&#20989;&#25968;&#36890;&#24120;&#29992;&#20110;&#25351;&#23450;&#23433;&#20840;&#26041;&#38754;&#65292;&#20294;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#30340;&#29366;&#24577;&#34920;&#31034;&#36890;&#24120;&#32570;&#20047;&#36275;&#22815;&#30340;&#20934;&#30830;&#24230;&#26469;&#25351;&#23450;&#36825;&#26679;&#30340;&#23433;&#20840;&#32422;&#26463;&#26465;&#20214;&#65292;&#22522;&#20110;&#19981;&#23436;&#25972;&#27169;&#22411;&#24037;&#20316;&#24120;&#24120;&#20250;&#20135;&#29983;&#24847;&#22806;&#30340;&#36127;&#38754;&#21103;&#20316;&#29992;&#65288;NSEs&#65289;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#23433;&#20840;&#20449;&#21495;&#19982;&#29366;&#24577;-&#21160;&#20316;&#36712;&#36857;&#30456;&#20851;&#32852;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#29366;&#24577;-&#21160;&#20316;&#21363;&#26102;&#65289;&#20351;&#25105;&#20204;&#30340;&#23433;&#20840;&#27169;&#22411;&#20855;&#26377;&#39640;&#24230;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#20551;&#35774;&#20026;&#19981;&#21516;&#30340;&#36712;&#36857;&#25552;&#20379;&#20102;&#20998;&#31867;&#23433;&#20840;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#26356;&#38590;&#30001;&#38382;&#39064;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#25968;&#20540;&#20195;&#20215;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#26469;&#23398;&#20064;&#36825;&#26679;&#30340;&#38750;&#39532;&#23572;&#31185;&#22827;&#23433;&#20840;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25289;&#26684;&#26391;&#26085;&#20056;&#25968;&#26041;&#27861;&#65292;&#23558;&#23433;&#20840;&#27169;&#22411;&#21644;&#22522;&#30784;MDP&#27169;&#22411;&#21512;&#24182;&#25104;&#19968;&#20010;&#35745;&#31639;&#22270;&#65292;&#20197;&#20419;&#36827;&#20195;&#29702;&#23398;&#20064;&#23433;&#20840;&#34892;&#20026;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;...
&lt;/p&gt;
&lt;p&gt;
In safe MDP planning, a cost function based on the current state and action is often used to specify safety aspects. In the real world, often the state representation used may lack sufficient fidelity to specify such safety constraints. Operating based on an incomplete model can often produce unintended negative side effects (NSEs). To address these challenges, first, we associate safety signals with state-action trajectories (rather than just an immediate state-action). This makes our safety model highly general. We also assume categorical safety labels are given for different trajectories, rather than a numerical cost function, which is harder to specify by the problem designer. We then employ a supervised learning model to learn such non-Markovian safety patterns. Second, we develop a Lagrange multiplier method, which incorporates the safety model and the underlying MDP model in a single computation graph to facilitate agent learning of safe behaviors. Finally, our empirical results
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#33258;&#36866;&#24212;&#23398;&#29983;t&#20998;&#24067;&#26041;&#27861;&#65292;&#22522;&#20110;&#26041;&#27861;&#30340;&#19968;&#33324;&#33258;&#36866;&#24212;&#30697;&#21487;&#20197;&#20351;&#29992;&#24265;&#20215;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;EMA&#65289;&#26469;&#20272;&#35745;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.03069</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23398;&#29983;t&#20998;&#24067;&#19982;&#26041;&#27861;&#30697;&#31227;&#21160;&#20272;&#35745;&#22120;&#29992;&#20110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series. (arXiv:2304.03069v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#33258;&#36866;&#24212;&#23398;&#29983;t&#20998;&#24067;&#26041;&#27861;&#65292;&#22522;&#20110;&#26041;&#27861;&#30340;&#19968;&#33324;&#33258;&#36866;&#24212;&#30697;&#21487;&#20197;&#20351;&#29992;&#24265;&#20215;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;EMA&#65289;&#26469;&#20272;&#35745;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#30340;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#36825;&#24102;&#26469;&#20102;&#27169;&#22411;&#36866;&#24212;&#30340;&#38590;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;GARCH&#20551;&#23450;&#20219;&#24847;&#31867;&#22411;&#30340;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#20559;&#24046;&#65292;&#25105;&#20204;&#23558;&#30528;&#30524;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#19981;&#21487;&#30693;&#30340;&#31227;&#21160;&#20272;&#35745;&#22120;&#21746;&#23398;&#65306;&#22312;&#26102;&#38388;$t$&#25214;&#21040;&#20248;&#21270;$F_t=\sum_{\tau&lt;t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$&#31227;&#21160;&#23545;&#25968;&#20284;&#28982;&#30340;&#21442;&#25968;&#65292;&#38543;&#26102;&#38388;&#28436;&#21270;&#12290;&#20363;&#22914;&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#24265;&#20215;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;EMA&#65289;&#26469;&#20272;&#35745;&#21442;&#25968;&#65292;&#20363;&#22914;&#32477;&#23545;&#20013;&#24515;&#30697;$E[|x-\mu|^p]$&#38543;$p\in\mathbb{R}^+$&#30340;&#21464;&#21270;&#32780;&#28436;&#21270;$m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$&#12290;&#36825;&#31181;&#22522;&#20110;&#26041;&#27861;&#30340;&#19968;&#33324;&#33258;&#36866;&#24212;&#30697;&#30340;&#24212;&#29992;&#23558;&#21576;&#29616;&#22312;&#23398;&#29983;t&#20998;&#24067;&#19978;&#65292;&#23588;&#20854;&#26159;&#22312;&#32463;&#27982;&#24212;&#29992;&#20013;&#27969;&#34892;&#65292;&#36825;&#37324;&#24212;&#29992;&#20110;DJIA&#20844;&#21496;&#30340;&#23545;&#25968;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The real life time series are usually nonstationary, bringing a difficult question of model adaptation. Classical approaches like GARCH assume arbitrary type of dependence. To prevent such bias, we will focus on recently proposed agnostic philosophy of moving estimator: in time $t$ finding parameters optimizing e.g. $F_t=\sum_{\tau&lt;t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$ moving log-likelihood, evolving in time. It allows for example to estimate parameters using inexpensive exponential moving averages (EMA), like absolute central moments $E[|x-\mu|^p]$ evolving with $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$ for one or multiple powers $p\in\mathbb{R}^+$. Application of such general adaptive methods of moments will be presented on Student's t-distribution, popular especially in economical applications, here applied to log-returns of DJIA companies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#26032;&#30340;3RL&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#38754;&#37096;&#24773;&#24863;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;(cnn)&#23454;&#29616;&#20102;91.4&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03064</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26032;3RL&#25968;&#25454;&#38598;&#23454;&#26102;&#38754;&#37096;&#24773;&#24863;&#35782;&#21035;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An experimental study in Real-time Facial Emotion Recognition on new 3RL dataset. (arXiv:2304.03064v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03064
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#26032;&#30340;3RL&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#38754;&#37096;&#24773;&#24863;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;(cnn)&#23454;&#29616;&#20102;91.4&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23454;&#26102;&#38754;&#37096;&#24773;&#24863;&#35782;&#21035;&#26159;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;&#20173;&#23384;&#22312;&#21508;&#31181;&#38382;&#39064;&#65292;&#22914;&#19968;&#20123;&#19982;&#24773;&#24863;&#26080;&#20851;&#30340;&#29031;&#29255;&#65288;&#22914;&#25991;&#20214;&#29031;&#29255;&#65289;&#65292;&#27599;&#31867;&#29031;&#29255;&#25968;&#37327;&#19981;&#24179;&#34913;&#20197;&#21450;&#21487;&#33021;&#23545;&#27491;&#30830;&#20998;&#31867;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#30340;&#35823;&#23548;&#24615;&#22270;&#20687;&#12290;&#20026;&#20102;&#20811;&#26381;&#20197;&#21069;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;3RL&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32422;24K&#24352;&#22270;&#20687;&#65292;&#24182;&#19988;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#26631;&#35760;&#20026;&#20116;&#31181;&#22522;&#26412;&#24773;&#32490;&#65306;&#24555;&#20048;&#12289;&#24656;&#24807;&#12289;&#24754;&#20260;&#12289;&#21388;&#24694;&#21644;&#24868;&#24594;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;3RL&#25968;&#25454;&#38598;&#19982;&#20854;&#20182;&#33879;&#21517;&#30340;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;&#65288;FER&#25968;&#25454;&#38598;&#12289;CK+&#25968;&#25454;&#38598;&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#24212;&#29992;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#26368;&#24120;&#29992;&#30340;&#31639;&#27861;&#65292;&#22914;SVM&#21644;CNN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;3RL&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#19978;&#26377;&#26126;&#26174;&#30340;&#25552;&#39640;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;CNN&#22312;3RL&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#21487;&#36798;91.4&#65285;&#65292;&#32780;FER2013&#12289;CK +&#30340;&#32467;&#26524;&#21017;&#30053;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although real-time facial emotion recognition is a hot topic research domain in the field of human-computer interaction, state-of the-art available datasets still suffer from various problems, such as some unrelated photos such as document photos, unbalanced numbers of photos in each class, and misleading images that can negatively affect correct classification. The 3RL dataset was created, which contains approximately 24K images and will be publicly available, to overcome previously available dataset problems. The 3RL dataset is labelled with five basic emotions: happiness, fear, sadness, disgust, and anger. Moreover, we compared the 3RL dataset with other famous state-of-the-art datasets (FER dataset, CK+ dataset), and we applied the most commonly used algorithms in previous works, SVM and CNN. The results show a noticeable improvement in generalization on the 3RL dataset. Experiments have shown an accuracy of up to 91.4% on 3RL dataset using CNN where results on FER2013, CK+ are, re
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#33258;&#20027;&#26041;&#27861;&#39044;&#27979;&#20102;&#22797;&#26434;&#27833;&#34255;&#30340;&#31354;&#38388;&#20998;&#24067;&#27010;&#29575;&#65292;&#21487;&#20197;&#36827;&#34892;&#19987;&#23478;&#26080;&#20851;&#30340;&#27010;&#21270;&#39044;&#27979;&#21644;&#22320;&#36136;&#27169;&#22411;&#21019;&#24314;&#12290;</title><link>http://arxiv.org/abs/2304.03048</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22797;&#26434;&#27833;&#34255;&#26089;&#26399;&#22320;&#36136;&#21208;&#25506;&#20013;&#65292;&#21033;&#29992;&#20117;&#21644;&#22320;&#38663;&#25968;&#25454;&#36827;&#34892;&#19987;&#23478;&#26080;&#20851;&#30340;&#27010;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Expert-Independent Generalization of Well and Seismic Data Using Machine Learning Methods for Complex Reservoirs Predicting During Early-Stage Geological Exploration. (arXiv:2304.03048v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#33258;&#20027;&#26041;&#27861;&#39044;&#27979;&#20102;&#22797;&#26434;&#27833;&#34255;&#30340;&#31354;&#38388;&#20998;&#24067;&#27010;&#29575;&#65292;&#21487;&#20197;&#36827;&#34892;&#19987;&#23478;&#26080;&#20851;&#30340;&#27010;&#21270;&#39044;&#27979;&#21644;&#22320;&#36136;&#27169;&#22411;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21644;&#24212;&#29992;&#19968;&#31181;&#33258;&#20027;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#30740;&#31350;&#21306;&#22495;&#20869;&#27833;&#34255;&#20256;&#25773;&#30340;&#27010;&#29575;&#12290;&#33258;&#20027;&#24615;&#24847;&#21619;&#30528;&#22312;&#20934;&#22791;&#21644;&#36755;&#20837;&#22320;&#36136;&#22320;&#29699;&#29289;&#29702;&#20449;&#24687;&#20043;&#21518;&#65292;&#19987;&#23478;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#34987;&#26368;&#23567;&#21270;&#12290;&#35813;&#30740;&#31350;&#20197;&#30740;&#31350;&#21306;&#22495;&#26089;&#26399;&#21208;&#25506;&#38454;&#27573;&#30340;3D&#22320;&#38663;&#21208;&#25506;&#25968;&#25454;&#21644;&#20117;&#20449;&#24687;&#20026;&#22522;&#30784;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32467;&#26524;&#65292;&#20026;&#20004;&#32452;&#36755;&#20837;&#25968;&#25454;&#65306;&#22522;&#30784;&#32452;&#21644;&#21453;&#28436;&#26657;&#20934;&#21518;&#30340;&#32452;&#65292;&#39044;&#27979;&#20102;&#27833;&#34255;&#31354;&#38388;&#20998;&#24067;&#30340;&#27010;&#29575;&#65292;&#24182;&#24471;&#21040;&#20102;&#26631;&#23450;&#21518;&#30340;&#27010;&#29575;&#31435;&#26041;&#20307;&#12290;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23545;&#22320;&#36136;&#21644;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#36827;&#34892;&#19987;&#23478;&#26080;&#20851;&#30340;&#27010;&#21270;&#65292;&#24182;&#21033;&#29992;&#35813;&#27010;&#21270;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#21644;&#22522;&#20110;&#27833;&#34255;&#27010;&#29575;&#34920;&#31034;&#30340;&#22320;&#36136;&#27169;&#22411;&#21019;&#24314;&#12290;&#31639;&#27861;&#30340;&#21512;&#26684;&#34920;&#29616;&#34920;&#26126;&#65292;&#22312;&#22797;&#26434;&#27833;&#34255;&#26089;&#26399;&#22320;&#36136;&#21208;&#25506;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to develop and apply an autonomous approach for predicting the probability of hydrocarbon reservoirs spreading in the studied area. Autonomy means that after preparing and inputting geological-geophysical information, the influence of an expert on the algorithms is minimized. The study was made based on the 3D seismic survey data and well information on the early exploration stage of the studied field. As a result, a forecast of the probability of spatial distribution of reservoirs was made for two sets of input data: the base set and the set after reverse-calibration, and three-dimensional cubes of calibrated probabilities of belonging of the studied space to the identified classes were obtained. The approach presented in the paper allows for expert-independent generalization of geological and geophysical data, and to use this generalization for hypothesis testing and creating geological models based on a probabilistic representation of the reservoir. The qual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#27969;&#24418;&#30340;&#22810;&#32447;&#24615;&#26680;&#22238;&#24402;&#21644;&#25554;&#34917;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#35745;&#31639;&#12289;&#25552;&#21462;&#25968;&#25454;&#27169;&#24335;&#21644;&#23427;&#20204;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#32780;&#19988;&#22312;dMRI&#25968;&#25454;&#19979;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.03041</link><description>&lt;p&gt;
&#25968;&#25454;&#27969;&#24418;&#20013;&#30340;&#22810;&#32447;&#24615;&#26680;&#22238;&#24402;&#21644;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Multi-Linear Kernel Regression and Imputation in Data Manifolds. (arXiv:2304.03041v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#27969;&#24418;&#30340;&#22810;&#32447;&#24615;&#26680;&#22238;&#24402;&#21644;&#25554;&#34917;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#35745;&#31639;&#12289;&#25552;&#21462;&#25968;&#25454;&#27169;&#24335;&#21644;&#23427;&#20204;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#32780;&#19988;&#22312;dMRI&#25968;&#25454;&#19979;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#32447;&#24615;&#38750;&#21442;&#25968;&#65288;&#22522;&#20110;&#26680;&#30340;&#65289;&#36924;&#36817;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#22238;&#24402;&#21644;&#25554;&#34917;&#65292;&#20197;&#21450;&#20854;&#22312;&#21160;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#20551;&#35774;&#25968;&#25454;&#29305;&#24449;&#39547;&#30041;&#22312;&#25110;&#38752;&#36817;&#23884;&#20837;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20809;&#28369;&#27969;&#24418;&#20013;&#12290;&#36890;&#36807;&#35782;&#21035;&#37324;&#31243;&#30865;&#28857;&#26469;&#25551;&#36848;&#29305;&#24449;&#28857;&#20113;&#65292;&#36890;&#36807;&#32447;&#24615;&#36924;&#36817;&#22359;&#26469;&#27169;&#20223;&#20809;&#28369;&#27969;&#24418;&#30340;&#20999;&#31354;&#38388;&#30340;&#27010;&#24565;&#12290;&#22810;&#32447;&#24615;&#27169;&#22411;&#23454;&#29616;&#20102;&#38477;&#32500;&#65292;&#33021;&#22815;&#36827;&#34892;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#24182;&#25552;&#21462;&#25968;&#25454;&#27169;&#24335;&#21450;&#20854;&#20960;&#20309;&#24418;&#29366;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#25110;&#20854;&#20182;&#20449;&#24687;&#12290;&#38024;&#23545;&#20005;&#37325;&#27424;&#37319;&#26679;&#30340;dMRI&#25968;&#25454;&#30340;&#25968;&#20540;&#27979;&#35797;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#12289;&#27969;&#34892;&#30340;&#25968;&#25454;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#26368;&#36817;&#30340;&#24352;&#37327;&#21644;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#26041;&#26696;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an efficient multi-linear nonparametric (kernel-based) approximation framework for data regression and imputation, and its application to dynamic magnetic-resonance imaging (dMRI). Data features are assumed to reside in or close to a smooth manifold embedded in a reproducing kernel Hilbert space. Landmark points are identified to describe concisely the point cloud of features by linear approximating patches which mimic the concept of tangent spaces to smooth manifolds. The multi-linear model effects dimensionality reduction, enables efficient computations, and extracts data patterns and their geometry without any training data or additional information. Numerical tests on dMRI data under severe under-sampling demonstrate remarkable improvements in efficiency and accuracy of the proposed approach over its predecessors, popular data modeling methods, as well as recent tensor-based and deep-image-prior schemes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#38271;&#26399;&#21512;&#21516;&#21644;&#20135;&#21697;&#20013;&#24515;&#23458;&#25143;&#20851;&#31995;&#30340;&#34892;&#19994;&#26469;&#24314;&#27169;&#23458;&#25143;&#30340;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#39044;&#27979;&#20219;&#24847;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;CLV&#65292;&#24182;&#21487;&#29983;&#25104;&#22522;&#20110;&#20135;&#21697;&#30340;&#20542;&#21521;&#27169;&#22411;&#65292;&#36825;&#22312;&#38646;&#21806;&#38134;&#34892;&#19994;&#20013;&#23588;&#20854;&#37325;&#35201;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#20256;&#32479;&#31639;&#27861;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;43%&#30340;&#36229;&#20986;&#26102;&#38388;&#30340;CLV&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.03038</link><description>&lt;p&gt;
&#22312;&#38646;&#21806;&#38134;&#34892;&#19994;&#20013;&#24314;&#27169;&#23458;&#25143;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Modelling customer lifetime-value in the retail banking industry. (arXiv:2304.03038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#38271;&#26399;&#21512;&#21516;&#21644;&#20135;&#21697;&#20013;&#24515;&#23458;&#25143;&#20851;&#31995;&#30340;&#34892;&#19994;&#26469;&#24314;&#27169;&#23458;&#25143;&#30340;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#39044;&#27979;&#20219;&#24847;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;CLV&#65292;&#24182;&#21487;&#29983;&#25104;&#22522;&#20110;&#20135;&#21697;&#30340;&#20542;&#21521;&#27169;&#22411;&#65292;&#36825;&#22312;&#38646;&#21806;&#38134;&#34892;&#19994;&#20013;&#23588;&#20854;&#37325;&#35201;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#20256;&#32479;&#31639;&#27861;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;43%&#30340;&#36229;&#20986;&#26102;&#38388;&#30340;CLV&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23458;&#25143;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#26159;&#22521;&#20859;&#38271;&#26399;&#23458;&#25143;&#20851;&#31995;&#30340;&#20851;&#38190;&#65292;&#20294;&#20272;&#35745;&#23427;&#36828;&#38750;&#26131;&#20107;&#12290;&#22312;&#38646;&#21806;&#38134;&#34892;&#19994;&#20013;&#65292;&#24120;&#29992;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#26469;&#24314;&#27169;&#23458;&#25143;&#29983;&#21629;&#21608;&#26399;&#20215;&#20540;&#65292;&#35813;&#26694;&#26550;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#38271;&#26399;&#21512;&#21516;&#21644;&#20135;&#21697;&#20013;&#24515;&#23458;&#25143;&#20851;&#31995;&#30340;&#34892;&#19994;&#65292;&#20854;&#20013;&#38646;&#21806;&#38134;&#34892;&#23601;&#26159;&#19968;&#20010;&#20363;&#23376;&#12290;&#35813;&#26694;&#26550;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21487;&#20197;&#22312;&#20219;&#24847;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;CLV&#39044;&#27979;&#21644;&#22522;&#20110;&#20135;&#21697;&#30340;&#20542;&#21521;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#20010;&#27169;&#22411;&#30340;&#23454;&#29616;&#65292;&#35813;&#27169;&#22411;&#30446;&#21069;&#24050;&#22312;&#19968;&#23478;&#22823;&#22411;&#33521;&#22269;&#25918;&#36151;&#26426;&#26500;&#20013;&#25237;&#20837;&#29983;&#20135;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#30456;&#23545;&#20110;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#25105;&#20204;&#20272;&#35745;&#22312;&#26102;&#38388;&#22806;CLV&#39044;&#27979;&#35823;&#24046;&#26041;&#38754;&#26377;43%&#30340;&#25913;&#21892;&#12290;&#20174;&#25105;&#20204;&#30340;CLV&#27169;&#22411;&#27966;&#29983;&#30340;&#20542;&#21521;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#25903;&#25345;&#23458;&#25143;&#32852;&#32476;&#33829;&#38144;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding customer lifetime value is key to nurturing long-term customer relationships, however, estimating it is far from straightforward. In the retail banking industry, commonly used approaches rely on simple heuristics and do not take advantage of the high predictive ability of modern machine learning techniques. We present a general framework for modelling customer lifetime value which may be applied to industries with long-lasting contractual and product-centric customer relationships, of which retail banking is an example. This framework is novel in facilitating CLV predictions over arbitrary time horizons and product-based propensity models. We also detail an implementation of this model which is currently in production at a large UK lender. In testing, we estimate an 43% improvement in out-of-time CLV prediction error relative to a popular baseline approach. Propensity models derived from our CLV model have been used to support customer contact marketing campaigns. In test
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#32536;&#35745;&#31639;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#20998;&#25955;&#24335;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#24615;&#21644;&#25928;&#29575;&#65292;&#21487;&#20197;&#22312;&#19981;&#27844;&#38706;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#35757;&#32451;&#20986;&#26356;&#21152;&#20934;&#30830;&#12289;&#20840;&#38754;&#30340;&#21307;&#30103;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03006</link><description>&lt;p&gt;
&#36793;&#32536;&#29289;&#32852;&#32593;&#32852;&#37030;&#21306;&#22359;&#38142;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
IoT Federated Blockchain Learning at the Edge. (arXiv:2304.03006v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#32536;&#35745;&#31639;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#20998;&#25955;&#24335;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#24615;&#21644;&#25928;&#29575;&#65292;&#21487;&#20197;&#22312;&#19981;&#27844;&#38706;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#35757;&#32451;&#20986;&#26356;&#21152;&#20934;&#30830;&#12289;&#20840;&#38754;&#30340;&#21307;&#30103;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#35774;&#22791;&#22312;&#21307;&#23398;&#39046;&#22495;&#29305;&#21035;&#26159;&#21307;&#30103;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#21033;&#29992;&#36828;&#36828;&#19981;&#22815;&#65292;&#20294;&#23427;&#20204;&#20855;&#26377;&#26080;&#21487;&#27604;&#25311;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;IoMT&#65288;&#21307;&#30103;&#29289;&#32852;&#32593;&#65289;&#30340;IoT&#35774;&#22791;&#65292;&#21033;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#20998;&#25955;&#24335;&#26041;&#26696;&#65292;&#25552;&#39640;&#38544;&#31169;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#32780;&#20174;&#20027;&#27969;&#30340;&#22522;&#20110;&#20113;&#30340;&#26550;&#26500;&#21521;&#36793;&#32536;&#31227;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoT devices are sorely underutilized in the medical field, especially within machine learning for medicine, yet they offer unrivaled benefits. IoT devices are low-cost, energy-efficient, small and intelligent devices. In this paper, we propose a distributed federated learning framework for IoT devices, more specifically for IoMT (Internet of Medical Things), using blockchain to allow for a decentralized scheme improving privacy and efficiency over a centralized system; this allows us to move from the cloud-based architectures, that are prevalent, to the edge. The system is designed for three paradigms: 1) Training neural networks on IoT devices to allow for collaborative training of a shared model whilst decoupling the learning from the dataset to ensure privacy. Training is performed in an online manner simultaneously amongst all participants, allowing for the training of actual data that may not have been present in a dataset collected in the traditional way and dynamically adapt the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25104;&#20026;&#22810;&#23186;&#20307;&#21462;&#35777;&#35843;&#26597;&#30340;&#26631;&#20934;&#65292;&#36890;&#36807;&#39564;&#35777;&#21512;&#25104;&#38754;&#37096;&#22270;&#20687;&#26469;&#25506;&#31350;&#21360;&#21047;&#21644;&#25195;&#25551;&#22270;&#20687;&#30340;&#26377;&#25928;&#21462;&#35777;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02982</link><description>&lt;p&gt;
&#31361;&#30772;&#24615;&#35770;&#25991;: Spritz-PS-&#21033;&#29992;&#22823;&#35268;&#27169;&#21360;&#21047;&#25991;&#20214;&#25968;&#25454;&#38598;&#23545;&#21512;&#25104;&#38754;&#37096;&#22270;&#20687;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Spritz-PS: Validation of Synthetic Face Images Using a Large Dataset of Printed Documents. (arXiv:2304.02982v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25104;&#20026;&#22810;&#23186;&#20307;&#21462;&#35777;&#35843;&#26597;&#30340;&#26631;&#20934;&#65292;&#36890;&#36807;&#39564;&#35777;&#21512;&#25104;&#38754;&#37096;&#22270;&#20687;&#26469;&#25506;&#31350;&#21360;&#21047;&#21644;&#25195;&#25551;&#22270;&#20687;&#30340;&#26377;&#25928;&#21462;&#35777;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#23545;&#21360;&#21047;&#21644;&#25195;&#25551;&#65288;PS&#65289;&#22270;&#20687;&#36827;&#34892;&#26377;&#25928;&#30340;&#21462;&#35777;&#20998;&#26512;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;PS&#25991;&#26723;&#21487;&#20197;&#29992;&#20110;&#38544;&#34255;&#22270;&#20687;&#30340;&#20266;&#36896;&#30165;&#36857;&#65292;&#22240;&#20026;&#36825;&#20123;&#30165;&#36857;&#36890;&#24120;&#23384;&#22312;&#20110;&#22788;&#29702;&#36807;&#30340;&#22270;&#20687;&#20013;&#65292;&#24182;&#19988;&#21512;&#25104;&#22270;&#20687;&#20013;&#30340;&#20027;&#35201;&#30165;&#36857;&#21487;&#20197;&#22312;PS&#20043;&#21518;&#21435;&#38500;&#12290;&#30001;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#30340;&#21560;&#24341;&#21147;&#65292;&#20351;&#29992;GANs&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#38754;&#37096;&#22270;&#20687;&#38590;&#20197;&#19982;&#30495;&#27491;&#30340;&#20154;&#31867;&#38754;&#37096;&#21306;&#20998;&#24320;&#26469;&#65292;&#21487;&#33021;&#29992;&#20110;&#21019;&#24314;&#20266;&#36896;&#36523;&#20221;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;GAN&#27169;&#22411;&#26410;&#32771;&#34385;&#22312;&#29983;&#25104;&#20154;&#31867;&#38754;&#37096;&#26102;&#30340;&#29983;&#29702;&#32422;&#26463;&#20197;&#21450;&#36825;&#20123;&#32422;&#26463;&#23545;&#20154;&#31867;&#34425;&#33180;&#30340;&#24433;&#21709;&#65292;&#22312;PS&#24773;&#20917;&#19979;&#21306;&#20998;&#30495;&#23454;&#21644;&#21512;&#25104;&#34425;&#33180;&#21464;&#24471;&#26497;&#20026;&#22256;&#38590;&#12290;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#21442;&#32771;&#34425;&#33180;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#25104;&#20026;&#22810;&#23186;&#20307;&#21462;&#35777;(MFs)&#35843;&#26597;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capability of doing effective forensic analysis on printed and scanned (PS) images is essential in many applications. PS documents may be used to conceal the artifacts of images which is due to the synthetic nature of images since these artifacts are typically present in manipulated images and the main artifacts in the synthetic images can be removed after the PS. Due to the appeal of Generative Adversarial Networks (GANs), synthetic face images generated with GANs models are difficult to differentiate from genuine human faces and may be used to create counterfeit identities. Additionally, since GANs models do not account for physiological constraints for generating human faces and their impact on human IRISes, distinguishing genuine from synthetic IRISes in the PS scenario becomes extremely difficult. As a result of the lack of large-scale reference IRIS datasets in the PS scenario, we aim at developing a novel dataset to become a standard for Multimedia Forensics (MFs) investigat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FLW-Net&#30340;&#24555;&#36895;&#12289;&#36731;&#37327;&#32423;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#20809;&#29031;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#12289;&#20302;&#20142;&#24230;&#12289;&#20302;&#23545;&#27604;&#24230;&#21644;&#33394;&#24425;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#20449;&#24687;&#25552;&#21462;&#32452;&#20214;&#20197;&#21450;&#22522;&#20110;&#30456;&#23545;&#20449;&#24687;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02978</link><description>&lt;p&gt;
&#19968;&#31181;&#24555;&#36895;&#12289;&#36731;&#37327;&#32423;&#30340;&#29992;&#20110;&#20302;&#20809;&#29031;&#22270;&#20687;&#22686;&#24378;&#30340;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Fast and Lightweight Network for Low-Light Image Enhancement. (arXiv:2304.02978v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FLW-Net&#30340;&#24555;&#36895;&#12289;&#36731;&#37327;&#32423;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20302;&#20809;&#29031;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#12289;&#20302;&#20142;&#24230;&#12289;&#20302;&#23545;&#27604;&#24230;&#21644;&#33394;&#24425;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#20449;&#24687;&#25552;&#21462;&#32452;&#20214;&#20197;&#21450;&#22522;&#20110;&#30456;&#23545;&#20449;&#24687;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#29031;&#22270;&#20687;&#36890;&#24120;&#23384;&#22312;&#20005;&#37325;&#30340;&#22122;&#22768;&#12289;&#20302;&#20142;&#24230;&#12289;&#20302;&#23545;&#27604;&#24230;&#21644;&#33394;&#24425;&#20559;&#24046;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLW-Net&#30340;&#24555;&#36895;&#12289;&#36731;&#37327;&#32423;&#32593;&#32476;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#20840;&#23616;&#29305;&#24449;&#20449;&#24687;&#25552;&#21462;&#32452;&#20214;&#65292;&#24182;&#22522;&#20110;&#30456;&#23545;&#20449;&#24687;&#35774;&#35745;&#20102;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#32477;&#23545;&#21442;&#32771;&#32570;&#22833;&#21644;&#33719;&#24471;&#20840;&#23616;&#23545;&#27604;&#24230;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light images often suffer from severe noise, low brightness, low contrast, and color deviation. While several low-light image enhancement methods have been proposed, there remains a lack of efficient methods that can simultaneously solve all of these problems. In this paper, we introduce FLW-Net, a Fast and LightWeight Network for low-light image enhancement that significantly improves processing speed and overall effect. To achieve efficient low-light image enhancement, we recognize the challenges of the lack of an absolute reference and the need for a large receptive field to obtain global contrast. Therefore, we propose an efficient global feature information extraction component and design loss functions based on relative information to overcome these challenges. Finally, we conduct comparative experiments to demonstrate the effectiveness of the proposed method, and the results confirm that FLW-Net can significantly reduce the complexity of supervised low-light image enhancemen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#24490;&#29615;&#24179;&#34913;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#20351;&#24471;&#32593;&#32476;&#20855;&#26377;&#25910;&#32553;&#21644;&#32791;&#25955;&#24615;&#36136;&#12290;&#27492;&#22806;&#25552;&#20986;&#30340;&#38750;&#32422;&#26463;&#21442;&#25968;&#21270;&#26041;&#27861;&#20351;&#24471;&#35813;&#32593;&#32476;&#23398;&#20064;&#30340;&#21442;&#25968;&#37327;&#24471;&#20197;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2304.02976</link><description>&lt;p&gt;
&#26080;&#32422;&#26463;&#21442;&#25968;&#21270;&#30340;&#32791;&#25955;&#24615;&#21644;&#25910;&#32553;&#24615;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Unconstrained Parametrization of Dissipative and Contracting Neural Ordinary Differential Equations. (arXiv:2304.02976v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#24490;&#29615;&#24179;&#34913;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#20351;&#24471;&#32593;&#32476;&#20855;&#26377;&#25910;&#32553;&#21644;&#32791;&#25955;&#24615;&#36136;&#12290;&#27492;&#22806;&#25552;&#20986;&#30340;&#38750;&#32422;&#26463;&#21442;&#25968;&#21270;&#26041;&#27861;&#20351;&#24471;&#35813;&#32593;&#32476;&#23398;&#20064;&#30340;&#21442;&#25968;&#37327;&#24471;&#20197;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#21644;&#30740;&#31350;&#20102;&#19968;&#31867;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#30340;&#26550;&#26500;&#28304;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#26368;&#36817;&#24341;&#20837;&#30340;&#24490;&#29615;&#24179;&#34913;&#32593;&#32476;&#65288;RENs&#65289;&#30340;&#27169;&#22411;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36171;&#20104;&#25105;&#20204;&#25552;&#20986;&#30340;NodeRENs&#25910;&#32553;&#21644;&#32791;&#25955;&#24615;&#8212;&#8212;&#23545;&#20110;&#20581;&#22766;&#30340;&#23398;&#20064;&#21644;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#19982;RENs&#19968;&#26679;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#25910;&#32553;&#21644;&#32791;&#25955;NodeRENs&#30340;&#21442;&#25968;&#21270;&#65292;&#36825;&#20123;&#21442;&#25968;&#27809;&#26377;&#32422;&#26463;&#65292;&#22240;&#27492;&#33021;&#22815;&#23398;&#20064;&#22823;&#37327;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;NodeRENs&#30340;&#23646;&#24615;&#65292;&#21253;&#25324;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce and study a class of Deep Neural Networks (DNNs) in continuous-time. The proposed architecture stems from the combination of Neural Ordinary Differential Equations (Neural ODEs) with the model structure of recently introduced Recurrent Equilibrium Networks (RENs). We show how to endow our proposed NodeRENs with contractivity and dissipativity -- crucial properties for robust learning and control. Most importantly, as for RENs, we derive parametrizations of contractive and dissipative NodeRENs which are unconstrained, hence enabling their learning for a large number of parameters. We validate the properties of NodeRENs, including the possibility of handling irregularly sampled data, in a case study in nonlinear system identification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22686;&#37327;&#36755;&#20837;&#29366;&#24577;&#31283;&#23450;&#30340;&#28145;&#24230;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#35782;&#21035;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#21033;&#29992;&#23545;&#32593;&#32476;&#26435;&#37325;&#30340;&#36866;&#24403;&#20805;&#20998;&#26465;&#20214;&#26469;&#24314;&#31435;&#19968;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#32463;&#36807;&#35777;&#26126;&#30340;$\delta$ISS LSTM&#27169;&#22411;&#12290;&#22312;&#23454;&#39564;&#20013;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24314;&#27169;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.02975</link><description>&lt;p&gt;
&#28145;&#24230;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65306;&#31283;&#23450;&#24615;&#21644;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Deep Long-Short Term Memory networks: Stability properties and Experimental validation. (arXiv:2304.02975v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22686;&#37327;&#36755;&#20837;&#29366;&#24577;&#31283;&#23450;&#30340;&#28145;&#24230;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#35782;&#21035;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#21033;&#29992;&#23545;&#32593;&#32476;&#26435;&#37325;&#30340;&#36866;&#24403;&#20805;&#20998;&#26465;&#20214;&#26469;&#24314;&#31435;&#19968;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#32463;&#36807;&#35777;&#26126;&#30340;$\delta$ISS LSTM&#27169;&#22411;&#12290;&#22312;&#23454;&#39564;&#20013;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24314;&#27169;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22686;&#37327;&#36755;&#20837;&#29366;&#24577;&#31283;&#23450;&#65288;$\delta$ISS&#65289;&#28145;&#24230;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTMs&#65289;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#35782;&#21035;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#34920;&#26126;&#21487;&#20197;&#21033;&#29992;&#23545;&#32593;&#32476;&#26435;&#37325;&#30340;&#36866;&#24403;&#20805;&#20998;&#26465;&#20214;&#26469;&#24314;&#31435;&#19968;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#32463;&#36807;&#35777;&#26126;&#30340;$\delta$ISS LSTM&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#30340;&#21046;&#21160;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#20174;&#23454;&#39564;&#25910;&#38598;&#30340;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#31995;&#32479;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24314;&#27169;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this work is to investigate the use of Incrementally Input-to-State Stable ($\delta$ISS) deep Long Short Term Memory networks (LSTMs) for the identification of nonlinear dynamical systems. We show that suitable sufficient conditions on the weights of the network can be leveraged to setup a training procedure able to learn provenly-$\delta$ISS LSTM models from data. The proposed approach is tested on a real brake-by-wire apparatus to identify a model of the system from input-output experimentally collected data. Results show satisfactory modeling performances.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#35299;&#26512;&#30340;&#26041;&#27861;&#35757;&#32451;&#21452;&#23618;ReLU&#32593;&#32476;&#65292;&#30456;&#27604;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;Adam&#20248;&#21270;&#22120;&#33021;&#22815;&#25214;&#21040;&#26356;&#28145;&#30340;&#26368;&#23567;&#20540;&#65292;&#22312;&#22235;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#26356;&#23567;&#30340;&#35757;&#32451;&#25439;&#22833;&#20540;&#65292;&#21516;&#26102;&#35813;&#26041;&#27861;&#36895;&#24230;&#26356;&#24555;&#65292;&#35843;&#21442;&#21442;&#25968;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2304.02972</link><description>&lt;p&gt;
&#35299;&#26512;&#35757;&#32451;&#21452;&#23618;ReLU&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training a Two Layer ReLU Network Analytically. (arXiv:2304.02972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#35299;&#26512;&#30340;&#26041;&#27861;&#35757;&#32451;&#21452;&#23618;ReLU&#32593;&#32476;&#65292;&#30456;&#27604;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;Adam&#20248;&#21270;&#22120;&#33021;&#22815;&#25214;&#21040;&#26356;&#28145;&#30340;&#26368;&#23567;&#20540;&#65292;&#22312;&#22235;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#26356;&#23567;&#30340;&#35757;&#32451;&#25439;&#22833;&#20540;&#65292;&#21516;&#26102;&#35813;&#26041;&#27861;&#36895;&#24230;&#26356;&#24555;&#65292;&#35843;&#21442;&#21442;&#25968;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20351;&#29992;&#21508;&#31181;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25110;Adam&#20248;&#21270;&#22120;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#21452;&#23618;ReLU&#32593;&#32476;&#30340;&#20020;&#30028;&#28857;&#65288;&#25439;&#22833;&#26799;&#24230;&#20026;&#38646;&#30340;&#28857;&#65289;&#19981;&#37117;&#26159;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#19968;&#31181;&#20351;&#29992;ReLU&#28608;&#27963;&#30340;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#24179;&#26041;&#25439;&#22833;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20132;&#26367;&#22320;&#22312;&#19968;&#20010;&#23618;&#30340;&#24773;&#20917;&#19979;&#35299;&#26512;&#22320;&#25214;&#21040;&#25439;&#22833;&#20989;&#25968;&#30340;&#20020;&#30028;&#28857;&#65292;&#21516;&#26102;&#20445;&#25345;&#21478;&#19968;&#20010;&#23618;&#21644;&#31070;&#32463;&#20803;&#28608;&#27963;&#27169;&#24335;&#19981;&#21464;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#27604;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25110;Adam&#20248;&#21270;&#22120;&#33021;&#22815;&#25214;&#21040;&#26356;&#28145;&#30340;&#26368;&#23567;&#20540;&#65292;&#22312;&#35780;&#20272;&#30340;&#20116;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#26377;&#22235;&#20010;&#33719;&#24471;&#20102;&#26174;&#33879;&#26356;&#23567;&#30340;&#35757;&#32451;&#25439;&#22833;&#20540;&#12290;&#32780;&#19988;&#65292;&#35813;&#26041;&#27861;&#27604;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26356;&#24555;&#65292;&#20960;&#20046;&#27809;&#26377;&#35843;&#21442;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are usually trained with different variants of gradient descent based optimization algorithms such as stochastic gradient descent or the Adam optimizer. Recent theoretical work states that the critical points (where the gradient of the loss is zero) of two-layer ReLU networks with the square loss are not all local minima. However, in this work we will explore an algorithm for training two-layer neural networks with ReLU-like activation and the square loss that alternatively finds the critical points of the loss function analytically for one layer while keeping the other layer and the neuron activation pattern fixed. Experiments indicate that this simple algorithm can find deeper optima than Stochastic Gradient Descent or the Adam optimizer, obtaining significantly smaller training loss values on four out of the five real datasets evaluated. Moreover, the method is faster than the gradient descent methods and has virtually no tuning parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SSCL&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#38590;&#20197;&#21306;&#20998;&#20986;&#30340;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02971</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21512;&#25104;&#38590;&#36127;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Synthetic Hard Negative Samples for Contrastive Learning. (arXiv:2304.02971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SSCL&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#38590;&#20197;&#21306;&#20998;&#20986;&#30340;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#20854;&#26680;&#24515;&#30446;&#26631;&#26159;&#22312;&#26368;&#22823;&#21270;&#21516;&#19968;&#22270;&#20687;&#30340;&#20004;&#20010;&#22686;&#24378;&#29256;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65288;&#27491;&#23545;&#65289;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#19981;&#21516;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65288;&#36127;&#23545;&#65289;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38590;&#24230;&#26356;&#22823;&#30340;&#36127;&#26679;&#26412;&#65292;&#21363;&#38590;&#20197;&#20174;&#38170;&#23450;&#26679;&#26412;&#20013;&#21306;&#20998;&#20986;&#30340;&#26679;&#26412;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#21457;&#25381;&#20102;&#26356;&#20026;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#23618;&#26041;&#27861;&#65292;&#21363;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21512;&#25104;&#38590;&#36127;&#26679;&#26412;&#65288;SSCL&#65289;&#30340;&#37319;&#26679;&#65292;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#26356;&#38590;&#30340;&#36127;&#26679;&#26412;&#12290;&#20855;&#20307;&#22320;&#65292;1&#65289;&#25105;&#20204;&#36890;&#36807;&#28151;&#21512;&#36127;&#26679;&#26412;&#29983;&#25104;&#26356;&#22810;&#21644;&#26356;&#38590;&#30340;&#36127;&#26679;&#26412;&#65292;&#28982;&#21518;&#36890;&#36807;&#25511;&#21046;&#38170;&#28857;&#26679;&#26412;&#19982;&#20854;&#20182;&#36127;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#27604;&#24230;&#26469;&#23545;&#36825;&#20123;&#36127;&#26679;&#26412;&#36827;&#34892;&#37319;&#26679;&#12290;2&#65289;&#32771;&#34385;&#21040;&#36890;&#36807;&#37319;&#26679;&#24471;&#21040;&#30340;&#36127;&#26679;&#26412;&#21487;&#33021;&#23384;&#22312;&#20551;&#36127;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#27880;&#37325;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has emerged as an essential approach for self-supervised learning in computer vision. The central objective of contrastive learning is to maximize the similarities between two augmented versions of the same image (positive pairs), while minimizing the similarities between different images (negative pairs). Recent studies have demonstrated that harder negative samples, i.e., those that are difficult to distinguish from anchor sample, play a more critical role in contrastive learning. In this paper, we propose a novel featurelevel method, namely sampling synthetic hard negative samples for contrastive learning (SSCL), to exploit harder negative samples more effectively. Specifically, 1) we generate more and harder negative samples by mixing negative samples, and then sample them by controlling the contrast of anchor sample with the other negative samples. 2) Considering that the negative samples obtained by sampling may have the problem of false negative samples, we 
&lt;/p&gt;</description></item><item><title>FengWu&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20808;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#12290;&#23427;&#20174;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#35282;&#24230;&#19979;&#35299;&#20915;&#20102;&#20013;&#26399;&#39044;&#25253;&#38382;&#39064;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#25439;&#22833;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21306;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#19979;&#24179;&#34913;&#19981;&#21516;&#39044;&#27979;&#22120;&#30340;&#20248;&#21270;&#12290;&#24341;&#20837;&#22238;&#25918;&#32531;&#20914;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;FengWu&#20855;&#26377;&#31934;&#30830;&#39044;&#27979;&#22823;&#27668;&#21160;&#21147;&#23398;&#21644;&#26410;&#26469;&#30340;&#38470;&#22320;&#21644;&#22823;&#27668;&#29366;&#24577;&#30340;&#33021;&#21147;&#12290;&#22312;2018&#24180;&#30340;&#38271;&#26399;&#39044;&#25253;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.02948</link><description>&lt;p&gt;
FengWu&#65306;&#25512;&#21160;&#25216;&#33021;&#31934;&#28251;&#30340;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#36229;&#36234;10&#22825;&#30340;&#39046;&#20808;&#12290;(arXiv:2304.02948v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
FengWu: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead. (arXiv:2304.02948v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02948
&lt;/p&gt;
&lt;p&gt;
FengWu&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20808;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#12290;&#23427;&#20174;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#35282;&#24230;&#19979;&#35299;&#20915;&#20102;&#20013;&#26399;&#39044;&#25253;&#38382;&#39064;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#25439;&#22833;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21306;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#19979;&#24179;&#34913;&#19981;&#21516;&#39044;&#27979;&#22120;&#30340;&#20248;&#21270;&#12290;&#24341;&#20837;&#22238;&#25918;&#32531;&#20914;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;FengWu&#20855;&#26377;&#31934;&#30830;&#39044;&#27979;&#22823;&#27668;&#21160;&#21147;&#23398;&#21644;&#26410;&#26469;&#30340;&#38470;&#22320;&#21644;&#22823;&#27668;&#29366;&#24577;&#30340;&#33021;&#21147;&#12290;&#22312;2018&#24180;&#30340;&#38271;&#26399;&#39044;&#25253;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FengWu&#65292;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20808;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#19981;&#21516;&#65292;FengWu&#20174;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#20013;&#26399;&#39044;&#25253;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#37197;&#22791;&#20102;&#27169;&#22411;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;Transformer&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#25439;&#22833;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21306;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#19979;&#24179;&#34913;&#19981;&#21516;&#39044;&#27979;&#22120;&#30340;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22238;&#25918;&#32531;&#20914;&#21306;&#26426;&#21046;&#26469;&#25552;&#39640;&#20013;&#26399;&#39044;&#25253;&#24615;&#33021;&#12290;&#22312;&#22522;&#20110;ERA5&#20877;&#20998;&#26512;&#30340;39&#24180;&#25968;&#25454;&#35757;&#32451;&#19979;&#65292;FengWu&#33021;&#22815;&#20934;&#30830;&#22320;&#22797;&#21046;&#22823;&#27668;&#21160;&#21147;&#23398;&#24182;&#22312;0.25{\deg}&#32428;&#24230;-&#32463;&#24230;&#20998;&#36776;&#29575;&#19978;&#39044;&#27979;&#26410;&#26469;&#30340;&#38470;&#22320;&#21644;&#22823;&#27668;&#29366;&#24577;&#12290;&#22522;&#20110;ERA5&#30340;2018&#24180;6&#23567;&#26102;&#38271;&#26399;&#39044;&#25253;&#34920;&#26126;&#65292;FengWu&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FengWu, an advanced data-driven global medium-range weather forecast system based on Artificial Intelligence (AI). Different from existing data-driven weather forecast methods, FengWu solves the medium-range forecast problem from a multi-modal and multi-task perspective. Specifically, a deep learning architecture equipped with model-specific encoder-decoders and cross-modal fusion Transformer is elaborately designed, which is learned under the supervision of an uncertainty loss to balance the optimization of different predictors in a region-adaptive manner. Besides this, a replay buffer mechanism is introduced to improve medium-range forecast performance. With 39-year data training based on the ERA5 reanalysis, FengWu is able to accurately reproduce the atmospheric dynamics and predict the future land and atmosphere states at 37 vertical levels on a 0.25{\deg} latitude-longitude resolution. Hindcasts of 6-hourly weather in 2018 based on ERA5 demonstrate that FengWu performs 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;RAID&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20803;&#21160;&#24577;&#36807;&#31243;&#20013;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#65292;&#20855;&#26377;&#36866;&#24212;&#38750;&#24179;&#31283;&#25928;&#24212;&#12289;&#19981;&#38656;&#25913;&#21464;&#29616;&#26377;&#36807;&#31243;&#33258;&#21160;&#21270;&#22522;&#30784;&#35774;&#26045;&#31561;&#29305;&#28857;&#65292;&#21487;&#22312;&#19981;&#21516;&#39046;&#22495;&#39640;&#24230;&#37096;&#32626;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#25968;&#25454;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20854;&#25913;&#36827;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02947</link><description>&lt;p&gt;
&#23454;&#26102;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#30340;&#21487;&#36866;&#24212;&#21644;&#21487;&#35299;&#37322;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Adaptable and Interpretable Framework for Novelty Detection in Real-Time IoT Systems. (arXiv:2304.02947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02947
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;RAID&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20803;&#21160;&#24577;&#36807;&#31243;&#20013;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#65292;&#20855;&#26377;&#36866;&#24212;&#38750;&#24179;&#31283;&#25928;&#24212;&#12289;&#19981;&#38656;&#25913;&#21464;&#29616;&#26377;&#36807;&#31243;&#33258;&#21160;&#21270;&#22522;&#30784;&#35774;&#26045;&#31561;&#29305;&#28857;&#65292;&#21487;&#22312;&#19981;&#21516;&#39046;&#22495;&#39640;&#24230;&#37096;&#32626;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#25968;&#25454;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20854;&#25913;&#36827;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Real-time Adaptive and Interpretable Detection (RAID)&#31639;&#27861;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#22810;&#20803;&#21160;&#24577;&#36807;&#31243;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#26465;&#20214;&#33539;&#22260;&#20869;&#26816;&#27979;&#24322;&#24120;&#12290;RAID&#31639;&#27861;&#36866;&#24212;&#20102;&#38750;&#24179;&#31283;&#25928;&#24212;&#65292;&#22914;&#25968;&#25454;&#28418;&#31227;&#21644;&#21464;&#21270;&#28857;&#65292;&#22312;&#27169;&#22411;&#24320;&#21457;&#26399;&#38388;&#21487;&#33021;&#27809;&#26377;&#36827;&#34892;&#36134;&#21153;&#65292;&#20174;&#32780;&#24310;&#38271;&#20102;&#26381;&#21153;&#23551;&#21629;&#12290;&#22522;&#20110;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#30340;&#21160;&#24577;&#27169;&#22411;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#21644;&#22522;&#20110;&#33258;&#36866;&#24212;&#36807;&#31243;&#38480;&#21046;&#30340;&#26681;&#26412;&#21407;&#22240;&#38548;&#31163;&#12290;RAID&#31639;&#27861;&#19981;&#38656;&#35201;&#26356;&#25913;&#29616;&#26377;&#30340;&#36807;&#31243;&#33258;&#21160;&#21270;&#22522;&#30784;&#35774;&#26045;&#65292;&#22240;&#27492;&#21487;&#22312;&#19981;&#21516;&#39046;&#22495;&#39640;&#24230;&#37096;&#32626;&#12290;&#20004;&#20010;&#28041;&#21450;&#23454;&#38469;&#21160;&#24577;&#31995;&#32479;&#25968;&#25454;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;RAID&#31639;&#27861;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#21464;&#26356;&#28857;&#36866;&#24212;&#24615;&#12289;&#26681;&#26412;&#21407;&#22240;&#38548;&#31163;&#21644;&#25913;&#36827;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the Real-time Adaptive and Interpretable Detection (RAID) algorithm. The novel approach addresses the limitations of state-of-the-art anomaly detection methods for multivariate dynamic processes, which are restricted to detecting anomalies within the scope of the model training conditions. The RAID algorithm adapts to non-stationary effects such as data drift and change points that may not be accounted for during model development, resulting in prolonged service life. A dynamic model based on joint probability distribution handles anomalous behavior detection in a system and the root cause isolation based on adaptive process limits. RAID algorithm does not require changes to existing process automation infrastructures, making it highly deployable across different domains. Two case studies involving real dynamic system data demonstrate the benefits of the RAID algorithm, including change point adaptation, root cause isolation, and improved detection accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#20845;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#20351;&#29992;&#21253;&#21547;14000&#20010;&#26679;&#26412;&#30340;&#26032;&#29616;&#23454;&#19990;&#30028;&#20108;&#20803;&#35010;&#32541;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26816;&#27979;&#36335;&#38754;&#35010;&#32541;&#30340;&#30446;&#30340;&#12290;&#35757;&#32451;&#30340;&#20845;&#20010;&#27169;&#22411;&#20013;&#26377;&#20116;&#20010;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97&#65285;&#65292;&#26368;&#39640;&#35760;&#24405;&#30340;&#20934;&#30830;&#29575;&#20026;99.7&#65285;&#12290;&#26368;&#20339;&#27169;&#22411;&#24050;&#32463;&#37096;&#32626;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#19978;&#65292;&#20197;&#20801;&#35768;&#33258;&#21160;&#26816;&#27979;&#30456;&#26426;&#38236;&#22836;&#20013;&#30340;&#35010;&#32541;&#12290;</title><link>http://arxiv.org/abs/2304.02933</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26580;&#24615;&#36335;&#38754;&#35010;&#32541;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks for crack detection on flexible road pavements. (arXiv:2304.02933v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#20845;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#20351;&#29992;&#21253;&#21547;14000&#20010;&#26679;&#26412;&#30340;&#26032;&#29616;&#23454;&#19990;&#30028;&#20108;&#20803;&#35010;&#32541;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26816;&#27979;&#36335;&#38754;&#35010;&#32541;&#30340;&#30446;&#30340;&#12290;&#35757;&#32451;&#30340;&#20845;&#20010;&#27169;&#22411;&#20013;&#26377;&#20116;&#20010;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97&#65285;&#65292;&#26368;&#39640;&#35760;&#24405;&#30340;&#20934;&#30830;&#29575;&#20026;99.7&#65285;&#12290;&#26368;&#20339;&#27169;&#22411;&#24050;&#32463;&#37096;&#32626;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#19978;&#65292;&#20197;&#20801;&#35768;&#33258;&#21160;&#26816;&#27979;&#30456;&#26426;&#38236;&#22836;&#20013;&#30340;&#35010;&#32541;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#36335;&#38754;&#20027;&#35201;&#30001;&#20110;&#36710;&#36742;&#21644;&#19981;&#21033;&#30340;&#29615;&#22659;&#26465;&#20214;&#32780;&#24694;&#21270;&#65292;&#32780;&#35010;&#32541;&#26159;&#26368;&#24120;&#35265;&#30340;&#24694;&#21270;&#26426;&#21046;&#65307;&#20854;&#35843;&#26597;&#36890;&#24120;&#20351;&#29992;&#22269;&#38469;&#23450;&#20041;&#30340;&#20998;&#31867;&#26631;&#20934;&#36827;&#34892;&#25163;&#21160;&#36827;&#34892;&#12290;&#22312;&#21335;&#38750;&#65292;&#24050;&#32463;&#24341;&#20837;&#20102;&#39640;&#28165;&#26224;&#24230;&#35270;&#39057;&#22270;&#20687;&#65292;&#21487;&#20197;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#36947;&#36335;&#35843;&#26597;&#12290;&#20294;&#26159;&#65292;&#35843;&#26597;&#20173;&#28982;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#25163;&#21160;&#36807;&#31243;&#12290;&#33258;&#21160;&#26816;&#27979;&#35832;&#22914;&#35010;&#32541;&#20043;&#31867;&#30340;&#32570;&#38519;&#23558;&#20801;&#35768;&#26356;&#24555;&#22320;&#20998;&#26512;&#36947;&#36335;&#32593;&#32476;&#65292;&#24182;&#28508;&#22312;&#22320;&#20943;&#23569;&#20154;&#20026;&#20559;&#24046;&#21644;&#38169;&#35823;&#12290;&#35813;&#30740;&#31350;&#23545;&#20845;&#31181;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#29992;&#20110;&#35010;&#32541;&#26816;&#27979;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#21253;&#21547;14000&#20010;&#26679;&#26412;&#30340;&#26032;&#29616;&#23454;&#19990;&#30028;&#20108;&#20803;&#35010;&#32541;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#12290;&#36824;&#35843;&#26597;&#20102;&#25968;&#25454;&#38598;&#25193;&#20805;&#30340;&#25928;&#26524;&#12290;&#35757;&#32451;&#30340;&#20845;&#20010;&#27169;&#22411;&#20013;&#26377;&#20116;&#20010;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97&#65285;&#65292;&#26368;&#39640;&#35760;&#24405;&#30340;&#20934;&#30830;&#29575;&#20026;99.7&#65285;&#12290;&#26368;&#20339;&#27169;&#22411;&#37096;&#32626;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#19978;&#65292;&#20197;&#20801;&#35768;&#33258;&#21160;&#26816;&#27979;&#30456;&#26426;&#38236;&#22836;&#20013;&#30340;&#35010;&#32541;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20026;&#36947;&#36335;&#32570;&#38519;&#30340;&#33258;&#21160;&#21270;&#26816;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible road pavements deteriorate primarily due to traffic and adverse environmental conditions. Cracking is the most common deterioration mechanism; the surveying thereof is typically conducted manually using internationally defined classification standards. In South Africa, the use of high-definition video images has been introduced, which allows for safer road surveying. However, surveying is still a tedious manual process. Automation of the detection of defects such as cracks would allow for faster analysis of road networks and potentially reduce human bias and error. This study performs a comparison of six state-of-the-art convolutional neural network models for the purpose of crack detection. The models are pretrained on the ImageNet dataset, and fine-tuned using a new real-world binary crack dataset consisting of 14000 samples. The effects of dataset augmentation are also investigated. Of the six models trained, five achieved accuracy above 97%. The highest recorded accuracy w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#28909;&#25104;&#20687;&#25216;&#26415;&#65292;&#26816;&#27979;&#21475;&#32617;&#26159;&#21542;&#20329;&#25140;&#21450;&#20998;&#31867;&#31867;&#22411;&#65292;&#26368;&#20339;&#27169;&#22411;&#20026;nano&#29256;&#26412;&#30340;Yolov5&#27169;&#22411;&#65292;&#36798;&#21040;97%&#20197;&#19978;&#30340;&#39640;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.02931</link><description>&lt;p&gt;
&#28909;&#25104;&#20687;&#19979;&#21475;&#32617;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mask Detection and Classification in Thermal Face Images. (arXiv:2304.02931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#28909;&#25104;&#20687;&#25216;&#26415;&#65292;&#26816;&#27979;&#21475;&#32617;&#26159;&#21542;&#20329;&#25140;&#21450;&#20998;&#31867;&#31867;&#22411;&#65292;&#26368;&#20339;&#27169;&#22411;&#20026;nano&#29256;&#26412;&#30340;Yolov5&#27169;&#22411;&#65292;&#36798;&#21040;97%&#20197;&#19978;&#30340;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#32617;&#30340;&#20351;&#29992;&#21487;&#20197;&#20943;&#23569;&#30149;&#27602;&#65292;&#29305;&#21035;&#26159;SARS-CoV-2&#30340;&#20256;&#25773;&#65292;&#22240;&#27492;&#33258;&#21160;&#26816;&#27979;&#21475;&#32617;&#30340;&#20329;&#25140;&#24773;&#20917;&#20197;&#21450;&#21475;&#32617;&#31867;&#22411;&#21644;&#20329;&#25140;&#26041;&#24335;&#31561;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#28909;&#25104;&#20687;&#25216;&#26415;&#20998;&#26512;&#22312;&#38754;&#37096;&#26816;&#27979;&#21644;&#20998;&#31867;&#21475;&#32617;&#31867;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25193;&#23637;&#24182;&#27880;&#37322;&#20102;&#29616;&#26377;&#30340;&#28909;&#25104;&#20687;&#25968;&#25454;&#38598;&#12290;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#36866;&#24212;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#20026;nano&#29256;&#26412;&#30340;Yolov5&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;mAP&#22823;&#20110;97&#65285;&#21644;&#31934;&#24230;&#32422;&#20026;95&#65285;&#30340;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face masks are recommended to reduce the transmission of many viruses, especially SARS-CoV-2. Therefore, the automatic detection of whether there is a mask on the face, what type of mask is worn, and how it is worn is an important research topic. In this work, the use of thermal imaging was considered to analyze the possibility of detecting (localizing) a mask on the face, as well as to check whether it is possible to classify the type of mask on the face. The previously proposed dataset of thermal images was extended and annotated with the description of a type of mask and a location of a mask within a face. Different deep learning models were adapted. The best model for face mask detection turned out to be the Yolov5 model in the "nano" version, reaching mAP higher than 97% and precision of about 95%. High accuracy was also obtained for mask type classification. The best results were obtained for the convolutional neural network model built on an autoencoder initially trained in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Patchout&#21644;&#25991;&#26412;&#25351;&#23548;&#30340;&#39640;&#25928;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;Mixup&#22686;&#24378;&#25216;&#26415;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02916</link><description>&lt;p&gt;
&#22522;&#20110;Patchout&#21644;&#25991;&#26412;&#25351;&#23548;&#30340;&#39640;&#25928;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Audio Captioning Transformer with Patchout and Text Guidance. (arXiv:2304.02916v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Patchout&#21644;&#25991;&#26412;&#25351;&#23548;&#30340;&#39640;&#25928;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;Mixup&#22686;&#24378;&#25216;&#26415;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#26159;&#19968;&#39033;&#22810;&#27169;&#24577;&#32763;&#35793;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#38899;&#39057;&#21098;&#36753;&#29983;&#25104;&#25991;&#26412;&#25551;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;Transformer&#26550;&#26500;&#65292;&#21033;&#29992;&#20102;[1]&#20013;&#25552;&#20986;&#30340;Patchout&#25216;&#26415;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#12290;&#23383;&#24149;&#29983;&#25104;&#37096;&#20998;&#23545;&#39044;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#25552;&#21462;&#30340;&#25991;&#26412;AudioSet&#26631;&#31614;&#36827;&#34892;&#20102;&#26576;&#31181;&#31243;&#24230;&#30340;&#26465;&#20214;&#32422;&#26463;&#65292;&#35813;&#27169;&#22411;&#34987;&#24494;&#35843;&#20197;&#26368;&#22823;&#21270;AudioSet&#26631;&#31614;&#19982;&#22320;&#38754;&#30495;&#23454;&#23383;&#24149;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#20026;&#20102;&#32531;&#35299;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26469;&#33258;&#19978;&#28216;&#38899;&#39057;&#30456;&#20851;&#20219;&#21153;&#21644;&#25193;&#22823;&#30340;&#39046;&#22495;&#20869;&#25968;&#25454;&#38598;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;Mixup&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#26469;&#30740;&#31350;Patchout&#21644;&#25991;&#26412;&#25351;&#23548;&#23545;&#26368;&#32456;&#24615;&#33021;&#30340;&#36129;&#29486;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#25913;&#21892;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated audio captioning is multi-modal translation task that aim to generate textual descriptions for a given audio clip. In this paper we propose a full Transformer architecture that utilizes Patchout as proposed in [1], significantly reducing the computational complexity and avoiding overfitting. The caption generation is partly conditioned on textual AudioSet tags extracted by a pre-trained classification model which is fine-tuned to maximize the semantic similarity between AudioSet labels and ground truth captions. To mitigate the data scarcity problem of Automated Audio Captioning we introduce transfer learning from an upstream audio-related task and an enlarged in-domain dataset. Moreover, we propose a method to apply Mixup augmentation for AAC. Ablation studies are carried out to investigate how Patchout and text guidance contribute to the final performance. The results show that the proposed techniques improve the performance of our system and while reducing the computationa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#23545;&#39640;&#32500;&#36229;&#32479;&#35745;&#29305;&#24449;&#19979;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#27491;&#21017;&#21270;&#21644;&#20998;&#24067;&#23610;&#24230;&#21442;&#25968;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.02912</link><description>&lt;p&gt;
&#39640;&#32500;&#36229;&#32479;&#35745;&#29305;&#24449;&#30340;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classification of Superstatistical Features in High Dimensions. (arXiv:2304.02912v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#23545;&#39640;&#32500;&#36229;&#32479;&#35745;&#29305;&#24449;&#19979;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#27491;&#21017;&#21270;&#21644;&#20998;&#24067;&#23610;&#24230;&#21442;&#25968;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#23545;&#20855;&#26377;&#19968;&#33324;&#20013;&#24515;&#28857;&#30340;&#20004;&#20010;&#25968;&#25454;&#20113;&#30340;&#28151;&#21512;&#36827;&#34892;&#20102;&#23398;&#20064;&#65292;&#20551;&#35774;&#20855;&#26377;&#36890;&#29992;&#30340;&#20984;&#25439;&#22833;&#21644;&#20984;&#27491;&#21017;&#21270;&#12290;&#27599;&#20010;&#25968;&#25454;&#20113;&#26159;&#36890;&#36807;&#20174;&#21487;&#33021;&#26159;&#19981;&#21487;&#25968;&#30340;&#39640;&#26031;&#20998;&#24067;&#21472;&#21152;&#20013;&#36827;&#34892;&#37319;&#26679;&#26469;&#33719;&#24471;&#30340;&#65292;&#20854;&#26041;&#24046;&#20855;&#26377;&#36890;&#29992;&#30340;&#27010;&#29575;&#23494;&#24230;$\varrho$&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#21253;&#25324;&#27809;&#26377;&#21327;&#26041;&#24046;&#30340;&#24130;&#24459;&#23614;&#37096;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#24471;&#20272;&#35745;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20998;&#26512;&#20102;&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;&#20197;&#21450;&#20998;&#31163;&#36716;&#25442;&#19982;&#20998;&#24067;&#23610;&#24230;&#21442;&#25968;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterise the learning of a mixture of two clouds of data points with generic centroids via empirical risk minimisation in the high dimensional regime, under the assumptions of generic convex loss and convex regularisation. Each cloud of data points is obtained by sampling from a possibly uncountable superposition of Gaussian distributions, whose variance has a generic probability density $\varrho$. Our analysis covers therefore a large family of data distributions, including the case of power-law-tailed distributions with no covariance. We study the generalisation performance of the obtained estimator, we analyse the role of regularisation, and the dependence of the separability transition on the distribution scale parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#37325;&#23614;&#37096;&#27491;&#21017;&#21270;&#30340;&#25216;&#26415;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36890;&#36807;&#26126;&#30830;&#25552;&#20513;&#26356;&#37325;&#30340;&#37325;&#23614;&#35889;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#19982;&#26631;&#20934;&#27491;&#21017;&#21270;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.02911</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#23614;&#37096;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Heavy-Tailed Regularization of Weight Matrices in Deep Neural Networks. (arXiv:2304.02911v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#37325;&#23614;&#37096;&#27491;&#21017;&#21270;&#30340;&#25216;&#26415;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36890;&#36807;&#26126;&#30830;&#25552;&#20513;&#26356;&#37325;&#30340;&#37325;&#23614;&#35889;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#19982;&#26631;&#20934;&#27491;&#21017;&#21270;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#21644;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20174;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#24471;&#21040;&#30340;&#26368;&#26032;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#20998;&#26512;&#30340;&#20449;&#24687;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#32447;&#32034;&#12290;&#19968;&#20010;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#19982;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#30340;&#37325;&#23614;&#31243;&#24230;&#30456;&#20851;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31216;&#20026;&#37325;&#23614;&#37096;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#26126;&#30830;&#25552;&#20513;&#26435;&#37325;&#30697;&#38453;&#20013;&#26356;&#37325;&#30340;&#37325;&#23614;&#35889;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#21152;&#26435;&#38463;&#23572;&#27861;&#21644;&#31283;&#23450;&#31209;&#20316;&#20026;&#24809;&#32602;&#39033;&#65292;&#20004;&#32773;&#37117;&#21487;&#24494;&#20998;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#35745;&#31639;&#23427;&#20204;&#30340;&#26799;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#24230;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#24809;&#32602;&#20989;&#25968;&#30340;&#21464;&#20307;&#12290;&#28982;&#21518;&#65292;&#37319;&#29992;&#36125;&#21494;&#26031;&#32479;&#35745;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#23614;&#37096;&#27491;&#21017;&#21270;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#20854;&#25928;&#26524;&#29702;&#35299;&#20026;&#26435;&#37325;&#30697;&#38453;&#30340;&#20808;&#39564;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#27491;&#21017;&#21270;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unraveling the reasons behind the remarkable success and exceptional generalization capabilities of deep neural networks presents a formidable challenge. Recent insights from random matrix theory, specifically those concerning the spectral analysis of weight matrices in deep neural networks, offer valuable clues to address this issue. A key finding indicates that the generalization performance of a neural network is associated with the degree of heavy tails in the spectrum of its weight matrices. To capitalize on this discovery, we introduce a novel regularization technique, termed Heavy-Tailed Regularization, which explicitly promotes a more heavy-tailed spectrum in the weight matrix through regularization. Firstly, we employ the Weighted Alpha and Stable Rank as penalty terms, both of which are differentiable, enabling the direct calculation of their gradients. To circumvent over-regularization, we introduce two variations of the penalty function. Then, adopting a Bayesian statistics
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#39640;&#25928;MCMC&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20803;&#21487;&#20114;&#25442;&#24615;&#21644;&#26576;&#20123;&#28608;&#27963;&#20989;&#25968;&#24341;&#36215;&#30340;&#23545;&#31216;&#24615;&#22312;&#21442;&#25968;&#21518;&#39564;&#30340;&#22810;&#27169;&#24577;&#24615;&#20013;&#25214;&#21040;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.02902</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#31216;&#24615;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#39640;&#25928;MCMC&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient MCMC Sampling in Bayesian Neural Networks by Exploiting Symmetry. (arXiv:2304.02902v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#39640;&#25928;MCMC&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20803;&#21487;&#20114;&#25442;&#24615;&#21644;&#26576;&#20123;&#28608;&#27963;&#20989;&#25968;&#24341;&#36215;&#30340;&#23545;&#31216;&#24615;&#22312;&#21442;&#25968;&#21518;&#39564;&#30340;&#22810;&#27169;&#24577;&#24615;&#20013;&#25214;&#21040;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39640;&#32500;&#65292;&#24378;&#22810;&#27169;&#24577;&#21442;&#25968;&#21518;&#39564;&#23494;&#24230;&#26223;&#35266;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#21487;&#20197;&#28176;&#36827;&#24615;&#22320;&#24674;&#22797;&#30495;&#23454;&#21518;&#39564;&#65292;&#20294;&#22240;&#20854;&#22312;&#22823;&#35268;&#27169;&#29616;&#20195;&#26550;&#26500;&#19978;&#34987;&#35748;&#20026;&#26159;&#20195;&#20215;&#39640;&#26114;&#32780;&#38590;&#20197;&#24212;&#29992;&#12290;&#32780;&#23616;&#37096;&#26041;&#27861;&#65292;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#32858;&#28966;&#20110;&#21487;&#36890;&#36807;&#21487;&#31215;&#20989;&#25968;&#36817;&#20284;&#30340;&#29305;&#23450;&#21442;&#25968;&#21306;&#22495;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#33021;&#22815;&#20135;&#29983;&#28385;&#24847;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#21442;&#25968;&#21518;&#39564;&#30340;&#22810;&#27169;&#24577;&#24615;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#21518;&#39564;&#26223;&#35266;&#20013;&#30340;&#23545;&#31216;&#24615;&#26469;&#32531;&#35299;&#31934;&#30830;&#20294;&#20195;&#20215;&#26114;&#36149;&#21644;&#24265;&#20215;&#20294;&#19981;&#31934;&#30830;&#26041;&#27861;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;&#36825;&#31181;&#23545;&#31216;&#24615;&#30001;&#31070;&#32463;&#20803;&#21487;&#20114;&#25442;&#24615;&#21644;&#26576;&#20123;&#28608;&#27963;&#20989;&#25968;&#24341;&#36215;&#65292;&#22312;&#19981;&#21516;&#30340;&#21442;&#25968;&#20540;&#23548;&#33268;&#30456;&#21516;&#30340;&#21151;&#33021;&#36755;&#20986;&#20540;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#21518;&#39564;&#39044;&#27979;&#21487;&#20197;&#21033;&#29992;&#23545;&#31216;&#24615;&#34987;&#39640;&#25928;&#22320;&#35745;&#31639;&#21644;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference in deep neural networks is challenging due to the high-dimensional, strongly multi-modal parameter posterior density landscape. Markov chain Monte Carlo approaches asymptotically recover the true posterior but are considered prohibitively expensive for large modern architectures. Local methods, which have emerged as a popular alternative, focus on specific parameter regions that can be approximated by functions with tractable integrals. While these often yield satisfactory empirical results, they fail, by definition, to account for the multi-modality of the parameter posterior. In this work, we argue that the dilemma between exact-but-unaffordable and cheap-but-inexact approaches can be mitigated by exploiting symmetries in the posterior landscape. Such symmetries, induced by neuron interchangeability and certain activation functions, manifest in different parameter values leading to the same functional output value. We show theoretically that the posterior predictiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#22797;&#26434;&#24230;&#21152;&#26435;&#35843;&#33410; Gibbs &#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#21464;&#37327;&#36873;&#25321;&#65292;&#21487;&#20197;&#38477;&#20302;&#27599;&#20010;MCMC&#36845;&#20195;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#36845;&#20195;&#27425;&#25968;&#20869;&#25511;&#21046;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.02899</link><description>&lt;p&gt;
&#21464;&#22797;&#26434;&#24230;&#21152;&#26435;&#35843;&#33410; Gibbs &#37319;&#26679;&#29992;&#20110;&#36125;&#21494;&#26031;&#21464;&#37327;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Variable-Complexity Weighted-Tempered Gibbs Samplers for Bayesian Variable Selection. (arXiv:2304.02899v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#22797;&#26434;&#24230;&#21152;&#26435;&#35843;&#33410; Gibbs &#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#21464;&#37327;&#36873;&#25321;&#65292;&#21487;&#20197;&#38477;&#20302;&#27599;&#20010;MCMC&#36845;&#20195;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#36845;&#20195;&#27425;&#25968;&#20869;&#25511;&#21046;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Jankowiak&#24341;&#20837;&#20102;&#23376;&#38598;&#21152;&#26435;&#35843;&#33410; Gibbs &#37319;&#26679;&#22120;&#65288;wTGS&#65289;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#24212;&#29992;&#31243;&#24207;&#20013;&#38477;&#20302;&#27599;&#20010;MCMC&#36845;&#20195;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#21518;&#39564;&#21253;&#21547;&#27010;&#29575;&#65288;PIP&#65289;&#30340;&#31934;&#30830;&#35745;&#31639;&#24182;&#19981;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19982;&#35813;&#37319;&#26679;&#22120;&#30456;&#20851;&#30340;Rao-Backwellized&#20272;&#35745;&#22120;&#20855;&#26377;&#39640;&#26041;&#24046;&#65292;&#22240;&#20026;&#20449;&#21495;&#32500;&#24230;&#19982;&#26465;&#20214;PIP&#20272;&#35745;&#25968;&#20043;&#27604;&#24456;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#23376;&#38598;&#21152;&#26435;&#35843;&#33410; Gibbs &#37319;&#26679;&#22120;&#65288;wTGS&#65289;&#65292;&#20854;&#20013;&#27599;&#20010;MCMC&#36845;&#20195;&#20013;&#21487;&#39044;&#26399;&#30340;&#26465;&#20214;PIP&#35745;&#31639;&#25968;&#37327;&#21487;&#20197;&#36828;&#23567;&#20110;&#20449;&#21495;&#32500;&#24230;&#12290;&#19982;&#23376;&#38598;wTGS&#21644;wTGS&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#37319;&#26679;&#22120;&#20855;&#26377;&#21487;&#21464;&#30340;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#22312;&#26377;&#38480;&#30340;&#36845;&#20195;&#27425;&#25968; $T$ &#19978;&#25552;&#20379;&#20102;&#19982;&#35813;&#37319;&#26679;&#22120;&#20851;&#32852;&#30340;Rao-Blackwellized&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#19978;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#24046;&#20026; $O\big(\big(\frac{P}{S}\big)^2 \frac{\log T}{T}\big)$&#65292;&#20854;&#20013; $\frac{P}{S}$ &#26159;&#26465;&#20214;PIP&#20272;&#35745;&#25968;&#21644;&#20449;&#21495;&#32500;&#24230;&#20043;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subset weighted-Tempered Gibbs Sampler (wTGS) has been recently introduced by Jankowiak to reduce the computation complexity per MCMC iteration in high-dimensional applications where the exact calculation of the posterior inclusion probabilities (PIP) is not essential. However, the Rao-Backwellized estimator associated with this sampler has a high variance as the ratio between the signal dimension and the number of conditional PIP estimations is large. In this paper, we design a new subset weighted-Tempered Gibbs Sampler (wTGS) where the expected number of computations of conditional PIPs per MCMC iteration can be much smaller than the signal dimension. Different from the subset wTGS and wTGS, our sampler has a variable complexity per MCMC iteration. We provide an upper bound on the variance of an associated Rao-Blackwellized estimator for this sampler at a finite number of iterations, $T$, and show that the variance is $O\big(\big(\frac{P}{S}\big)^2 \frac{\log T}{T}\big)$ for a given 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#35821;&#35328;&#26465;&#20214;&#25918;&#32622;&#25512;&#29702;&#26694;&#26550;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798; 97.75% &#30340;&#25918;&#32622;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.02893</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#35821;&#35328;&#26465;&#20214;&#25918;&#32622;&#25512;&#29702;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Object-centric Inference for Language Conditioned Placement: A Foundation Model based Approach. (arXiv:2304.02893v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#35821;&#35328;&#26465;&#20214;&#25918;&#32622;&#25512;&#29702;&#26694;&#26550;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798; 97.75% &#30340;&#25918;&#32622;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#65292;&#21363;&#26426;&#22120;&#20154;&#24212;&#35813;&#29983;&#25104;&#28385;&#36275;&#35821;&#35328;&#35828;&#26126;&#20013;&#25152;&#26377;&#31354;&#38388;&#20851;&#31995;&#38480;&#21046;&#30340;&#25918;&#32622;&#12290;&#20197;&#24448;&#22522;&#20110;&#35268;&#21017;&#30340;&#35821;&#35328;&#35299;&#26512;&#25110;&#22330;&#26223;&#20013;&#24515;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#25351;&#20196;&#24418;&#24335;&#21644;&#21442;&#32771;&#23545;&#35937;&#26377;&#38480;&#21046;&#25110;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#26469;&#30830;&#23450;&#21442;&#32771;&#23545;&#35937;&#21644;&#31354;&#38388;&#20851;&#31995;&#20197;&#36827;&#34892;&#25918;&#32622;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#26679;&#26412;&#39640;&#25928;&#21644;&#21487;&#25512;&#24191;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21482;&#38656; ~0.26M &#21487;&#35757;&#32451;&#21442;&#25968;&#23601;&#33021;&#36798;&#21040; 97.75% &#30340;&#25918;&#32622;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#21644;&#35828;&#26126;&#37117;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#24403;&#21482;&#20351;&#29992; 25% &#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#20173;&#28982;&#32988;&#36807;&#39030;&#32423;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the task of language-conditioned object placement, in which a robot should generate placements that satisfy all the spatial relational constraints in language instructions. Previous works based on rule-based language parsing or scene-centric visual representation have restrictions on the form of instructions and reference objects or require large amounts of training data. We propose an object-centric framework that leverages foundation models to ground the reference objects and spatial relations for placement, which is more sample efficient and generalizable. Experiments indicate that our model can achieve a 97.75% success rate of placement with only ~0.26M trainable parameters. Besides, our method generalizes better to both unseen objects and instructions. Moreover, with only 25% training data, we still outperform the top competing approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCNI&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#40065;&#26834;&#30340;&#20840;&#23616;&#32858;&#21512;&#22120;&#21644;&#25239;&#22122;&#23616;&#37096;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22312;&#23567;&#22411;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02892</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#22024;&#26434;&#21644;&#24322;&#36136;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#35880;&#24910;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Cautiously in Federated Learning with Noisy and Heterogeneous Clients. (arXiv:2304.02892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCNI&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#40065;&#26834;&#30340;&#20840;&#23616;&#32858;&#21512;&#22120;&#21644;&#25239;&#22122;&#23616;&#37096;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22312;&#23567;&#22411;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65288;&#26412;&#22320;&#31867;&#21035;&#19981;&#24179;&#34913;&#65289;&#21644;&#20302;&#36136;&#37327;&#30340;&#27880;&#37322;&#65288;&#26631;&#31614;&#22024;&#26434;&#65289;&#12290;FL &#30340;&#23567;&#22411;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#20849;&#23384;&#22312;&#65292;&#20351;&#20256;&#32479; FL &#26041;&#27861;&#21644;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#22343;&#26080;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FedCNI&#65292;&#23427;&#19981;&#20351;&#29992;&#39069;&#22806;&#30340;&#24178;&#20928;&#20195;&#29702;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#40065;&#26834;&#30340;&#20840;&#23616;&#32858;&#21512;&#22120;&#21644;&#19968;&#20010;&#25239;&#22122;&#23616;&#37096;&#27714;&#35299;&#22120;&#12290;&#23545;&#20110;&#23616;&#37096;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26356;&#31283;&#20581;&#30340;&#26679;&#26412;&#22024;&#26434;&#26816;&#27979;&#22120;&#26469;&#21306;&#20998;&#22024;&#26434;&#26679;&#26412;&#12290;&#20026;&#20102;&#20943;&#23569;&#22122;&#22768;&#26679;&#26412;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35838;&#31243;&#20266;&#26631;&#31614;&#26041;&#27861;&#21644;&#19968;&#20010;&#21435;&#22122; Mixup &#35757;&#32451;&#31574;&#30053;&#12290;&#23545;&#20110;&#20840;&#23616;&#32858;&#21512;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#19981;&#21516;&#23398;&#20064;&#38454;&#27573;&#37327;&#36523;&#23450;&#21046;&#30340;&#20999;&#25442;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22024;&#26434;&#26631;&#31614;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed framework for collaboratively training with privacy guarantees. In real-world scenarios, clients may have Non-IID data (local class imbalance) with poor annotation quality (label noise). The co-existence of label noise and class imbalance in FL's small local datasets renders conventional FL methods and noisy-label learning methods both ineffective. To address the challenges, we propose FedCNI without using an additional clean proxy dataset. It includes a noise-resilient local solver and a robust global aggregator. For the local solver, we design a more robust prototypical noise detector to distinguish noisy samples. Further to reduce the negative impact brought by the noisy samples, we devise a curriculum pseudo labeling method and a denoise Mixup training strategy. For the global aggregator, we propose a switching re-weighted aggregation method tailored to different learning periods. Extensive experiments demonstrate our method can substantiall
&lt;/p&gt;</description></item><item><title>ViralVectors&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20174;virome&#27979;&#24207;&#25968;&#25454;&#20013;&#29983;&#25104;Minimizers&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#26377;&#25928;&#30340;&#19979;&#28216;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#38750;&#27604;&#23545;&#25216;&#26415;&#26041;&#27861;&#65292;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#30149;&#27602;&#23478;&#26063;&#65292;&#29978;&#33267;&#23646;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;SARS-CoV-2&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02891</link><description>&lt;p&gt;
ViralVectors&#65306;&#19968;&#31181;&#32039;&#20945;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#38750;&#27604;&#23545;&#25216;&#26415;&#29983;&#25104;virome&#29305;&#24449;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ViralVectors: Compact and Scalable Alignment-free Virome Feature Generation. (arXiv:2304.02891v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02891
&lt;/p&gt;
&lt;p&gt;
ViralVectors&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20174;virome&#27979;&#24207;&#25968;&#25454;&#20013;&#29983;&#25104;Minimizers&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#26377;&#25928;&#30340;&#19979;&#28216;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#38750;&#27604;&#23545;&#25216;&#26415;&#26041;&#27861;&#65292;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#30149;&#27602;&#23478;&#26063;&#65292;&#29978;&#33267;&#23646;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;SARS-CoV-2&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;SARS-CoV-2&#30340;&#27979;&#24207;&#25968;&#25454;&#37327;&#27604;&#20854;&#20182;&#22823;&#22810;&#25968;&#30149;&#27602;&#37117;&#35201;&#22823;&#33509;&#24178;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19988;SARS-CoV-2&#30340;&#25968;&#25454;&#37327;&#23558;&#32487;&#32493;&#21576;&#20960;&#20309;&#32423;&#25968;&#22686;&#38271;&#65292;&#35768;&#22810;&#22269;&#23478;&#27491;&#22312;&#22823;&#21147;&#25237;&#36164;&#22522;&#22240;&#32452;&#30417;&#27979;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#22788;&#29702;&#22823;&#37327;&#30340;&#24207;&#21015;&#25968;&#25454;&#20197;&#23454;&#29616;&#26377;&#25928;&#32780;&#21450;&#26102;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#25968;&#25454;&#26469;&#33258;&#21508;&#31181;&#19981;&#21516;&#30340;&#26469;&#28304;&#65306;&#27604;&#23545;&#12289;&#26410;&#27604;&#23545;&#29978;&#33267;&#26410;&#35013;&#37197;&#30340;&#21407;&#22987;&#26680;&#33527;&#37240;&#25110;&#27688;&#22522;&#37240;&#27979;&#24207;reads&#65292;&#28085;&#30422;&#25972;&#20010;&#22522;&#22240;&#32452;&#25110;&#26576;&#20123;&#21306;&#22495;&#65288;&#20363;&#22914;spike&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ViralVectors&#65292;&#19968;&#31181;&#20174;virome&#27979;&#24207;&#25968;&#25454;&#20013;&#29983;&#25104;&#32039;&#20945;&#29305;&#24449;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#20998;&#26512;&#12290;&#35813;&#29983;&#25104;&#26041;&#27861;&#22522;&#20110;minimizers&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24207;&#21015;&#8220;&#31614;&#21517;&#8221;&#65292;&#20256;&#32479;&#19978;&#29992;&#20110;&#32452;&#35013;&#21644;&#35835;&#21462;&#26144;&#23556;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#39318;&#27425;&#20351;&#29992;minimizers&#36827;&#34892;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#27979;&#24207;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65306;&#65288;a&#65289;2.5M SARS-CoV-2 nanopore reads&#65292;&#65288;b&#65289;1.5M &#26410;&#27604;&#23545;&#30340;SARS-CoV-2 Illumina reads&#65292;&#20197;&#21450;&#65288;c&#65289;&#22823;&#37327;&#30340;virome reads&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#38750;&#27604;&#23545;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#30149;&#27602;&#23478;&#26063;&#65292;&#29978;&#33267;&#23646;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#29305;&#24449;&#21521;&#37327;&#21487;&#20197;&#36731;&#26494;&#22320;&#32858;&#31867;&#21644;&#21487;&#35270;&#21270;&#65292;&#23454;&#29616;&#20102;&#30452;&#35266;&#30340;&#30149;&#27602;&#21457;&#29616;&#21644;&#25506;&#32034;&#21151;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#20844;&#24320;&#30340;SARS-CoV-2&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19968;&#20010;&#24179;&#34913;&#30340;&#20004;&#31867;SARS-CoV-2&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The amount of sequencing data for SARS-CoV-2 is several orders of magnitude larger than any virus. This will continue to grow geometrically for SARS-CoV-2, and other viruses, as many countries heavily finance genomic surveillance efforts. Hence, we need methods for processing large amounts of sequence data to allow for effective yet timely decision-making. Such data will come from heterogeneous sources: aligned, unaligned, or even unassembled raw nucleotide or amino acid sequencing reads pertaining to the whole genome or regions (e.g., spike) of interest. In this work, we propose \emph{ViralVectors}, a compact feature vector generation from virome sequencing data that allows effective downstream analysis. Such generation is based on \emph{minimizers}, a type of lightweight "signature" of a sequence, used traditionally in assembly and read mapping -- to our knowledge, the first use minimizers in this way. We validate our approach on different types of sequencing data: (a) 2.5M SARS-CoV-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20013;&#33258;&#21160;&#26631;&#35760;&#38382;&#39064;&#20026;API&#39046;&#22495;&#26631;&#31614;&#30340;&#21487;&#34892;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#33258;&#21160;&#20998;&#37197;&#30340;&#26631;&#31614;&#23545;&#36129;&#29486;&#32773;&#36873;&#25321;&#20219;&#21153;&#21644;&#29702;&#35299;&#38382;&#39064;&#35201;&#27714;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.02877</link><description>&lt;p&gt;
&#26631;&#35760;&#38382;&#39064;&#65306;&#22312;&#38382;&#39064;&#36319;&#36394;&#31995;&#32479;&#20013;&#24212;&#29992;API&#39046;&#22495;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Tag that issue: Applying API-domain labels in issue tracking systems. (arXiv:2304.02877v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20013;&#33258;&#21160;&#26631;&#35760;&#38382;&#39064;&#20026;API&#39046;&#22495;&#26631;&#31614;&#30340;&#21487;&#34892;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#33258;&#21160;&#20998;&#37197;&#30340;&#26631;&#31614;&#23545;&#36129;&#29486;&#32773;&#36873;&#25321;&#20219;&#21153;&#21644;&#29702;&#35299;&#38382;&#39064;&#35201;&#27714;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20013;&#65292;&#29992;&#25152;&#38656;&#25216;&#33021;&#26469;&#26631;&#35760;&#38382;&#39064;&#21487;&#20197;&#24110;&#21161;&#36129;&#29486;&#32773;&#36873;&#25321;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#26631;&#35760;&#38382;&#39064;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#65292;&#32780;&#24403;&#21069;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#22823;&#22810;&#20165;&#38480;&#20110;&#23558;&#38382;&#39064;&#20998;&#31867;&#20026;&#38169;&#35823;/&#38750;&#38169;&#35823;&#12290;&#25105;&#20204;&#35843;&#26597;&#33258;&#21160;&#26631;&#35760;&#38382;&#39064;&#20026;&#25105;&#20204;&#25152;&#35859;&#30340;&#8220;API&#39046;&#22495;&#8221;&#30340;&#21487;&#34892;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#36825;&#20123;&#39046;&#22495;&#26159;API&#30340;&#39640;&#32423;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#21463;&#38382;&#39064;&#24433;&#21709;&#30340;&#28304;&#20195;&#30721;&#20013;&#20351;&#29992;&#30340;API&#21487;&#20197;&#20316;&#20026;&#24037;&#20316;&#38382;&#39064;&#25152;&#38656;&#25216;&#33021;&#65288;&#20363;&#22914;DB&#12289;&#23433;&#20840;&#24615;&#12289;UI&#65289;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65288;n=74&#65289;&#26469;&#35780;&#20272;API&#39046;&#22495;&#26631;&#31614;&#23545;&#28508;&#22312;&#36129;&#29486;&#32773;&#30340;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38382;&#39064;&#25551;&#36848;&#21644;&#39033;&#30446;&#21382;&#21490;&#26469;&#24314;&#31435;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#39564;&#35777;&#20102;&#36129;&#29486;&#32773;&#65288;n=20&#65289;&#23545;&#39033;&#30446;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;i&#65289;&#39033;&#30446;&#26032;&#25163;&#35748;&#20026;API&#39046;&#22495;&#26631;&#31614;&#22312;&#36873;&#25321;&#20219;&#21153;&#26102;&#26377;&#29992;&#65292;&#65288;ii&#65289;&#21487;&#20197;&#39044;&#27979;&#26631;&#31614;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;60&#65285;&#65292;&#65288;iii&#65289;&#36129;&#29486;&#32773;&#21457;&#29616;&#33258;&#21160;&#20998;&#37197;&#30340;&#26631;&#31614;&#26377;&#21161;&#20110;&#29702;&#35299;&#38382;&#39064;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling issues with the skills required to complete them can help contributors to choose tasks in Open Source Software projects. However, manually labeling issues is time-consuming and error-prone, and current automated approaches are mostly limited to classifying issues as bugs/non-bugs. We investigate the feasibility and relevance of automatically labeling issues with what we call "API-domains," which are high-level categories of APIs. Therefore, we posit that the APIs used in the source code affected by an issue can be a proxy for the type of skills (e.g., DB, security, UI) needed to work on the issue. We ran a user study (n=74) to assess API-domain labels' relevancy to potential contributors, leveraged the issues' descriptions and the project history to build prediction models, and validated the predictions with contributors (n=20) of the projects. Our results show that (i) newcomers to the project consider API-domain labels useful in choosing tasks, (ii) labels can be predicted w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21487;&#20197;&#38459;&#27490;&#20405;&#29359;&#29992;&#25143;&#38544;&#31169;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#20445;&#25252;&#29992;&#25143;&#25968;&#23383;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2304.02870</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#20445;&#25252;&#22312;&#32447;&#29992;&#25143;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Protecting User Privacy in Online Settings via Supervised Learning. (arXiv:2304.02870v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21487;&#20197;&#38459;&#27490;&#20405;&#29359;&#29992;&#25143;&#38544;&#31169;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#20445;&#25252;&#29992;&#25143;&#25968;&#23383;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#21830;&#19994;&#27169;&#24335;&#19981;&#21516;&#65292;&#32593;&#32476;&#20844;&#21496;&#36890;&#24120;&#20197;&#25910;&#38598;&#29992;&#25143;&#25968;&#25454;&#30340;&#26041;&#24335;&#36186;&#21462;&#21033;&#28070;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#25968;&#25454;&#20250;&#34987;&#20986;&#21806;&#32473;&#31532;&#19977;&#26041;&#65292;&#20405;&#29359;&#20102;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26234;&#33021;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#21487;&#33021;&#20405;&#29359;&#29992;&#25143;&#38544;&#31169;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#20174;&#32780;&#24674;&#22797;&#29992;&#25143;&#30340;&#25968;&#23383;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Companies that have an online presence-in particular, companies that are exclusively digital-often subscribe to this business model: collect data from the user base, then expose the data to advertisement agencies in order to turn a profit. Such companies routinely market a service as "free", while obfuscating the fact that they tend to "charge" users in the currency of personal information rather than money. However, online companies also gather user data for more principled purposes, such as improving the user experience and aggregating statistics. The problem is the sale of user data to third parties. In this work, we design an intelligent approach to online privacy protection that leverages supervised learning. By detecting and blocking data collection that might infringe on a user's privacy, we can restore a degree of digital privacy to the user. In our evaluation, we collect a dataset of network requests and measure the performance of several classifiers that adhere to the supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#26234;&#33021;&#65292;&#26377;&#24453;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.02868</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#33021;&#22815;&#24456;&#22909;&#22320;&#29609;&#25991;&#23383;&#28216;&#25103;&#65311;&#29616;&#29366;&#21644;&#26410;&#26469;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. (arXiv:2304.02868v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#26234;&#33021;&#65292;&#26377;&#24453;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;GPT-4&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#29992;&#25143;&#36890;&#20449;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#26088;&#22312;&#35843;&#26597;&#23427;&#20204;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#35201;&#27714;&#29609;&#23478;&#36890;&#36807;&#19982;&#28216;&#25103;&#19990;&#30028;&#30340;&#23545;&#35805;&#26469;&#29702;&#35299;&#29615;&#22659;&#24182;&#23545;&#24773;&#20917;&#20570;&#20986;&#21453;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#25152;&#26377;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;ChatGPT&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;ChatGPT&#26080;&#27861;&#36890;&#36807;&#29609;&#28216;&#25103;&#25110;&#38405;&#35835;&#28216;&#25103;&#25163;&#20876;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#65307;&#23427;&#21487;&#33021;&#26080;&#27861;&#21033;&#29992;&#23427;&#24050;&#32463;&#25317;&#26377;&#30340;&#19990;&#30028;&#30693;&#35782;&#65307;&#23427;&#26080;&#27861;&#25512;&#26029;&#20986;&#38543;&#30528;&#28216;&#25103;&#36827;&#23637;&#30340;&#27599;&#19968;&#27493;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20132;&#21449;&#39046;&#22495;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT and GPT-4 have recently demonstrated their remarkable abilities of communicating with human users. In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world. Our experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses. Our results open up new research questions at the intersection of artificial intelligence, machine learning, and natural language processing.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;Meta-LTH&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#19981;&#21487;&#32570;&#23569;&#30340;&#36830;&#25509;&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#21098;&#26525;&#25216;&#26415;&#29983;&#25104;&#20851;&#38190;&#30340;&#36830;&#25509;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#26631;&#20934;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.02862</link><description>&lt;p&gt;
&#23398;&#20064;&#19981;&#21487;&#25110;&#32570;&#30340;&#36830;&#25509;&#26469;&#36827;&#34892;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn with Indispensable Connections. (arXiv:2304.02862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02862
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;Meta-LTH&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#19981;&#21487;&#32570;&#23569;&#30340;&#36830;&#25509;&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#21098;&#26525;&#25216;&#26415;&#29983;&#25104;&#20851;&#38190;&#30340;&#36830;&#25509;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#26631;&#20934;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#23569;&#37327;&#26631;&#35760;&#23454;&#20363;&#26469;&#35299;&#20915;&#26410;&#30693;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29616;&#26377;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#24555;&#36895;&#23398;&#20064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#32570;&#38519;&#12290;&#20803;&#35757;&#32451;&#20013;&#32463;&#24120;&#20986;&#29616;&#19981;&#24517;&#35201;&#30340;&#36830;&#25509;&#65292;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#22240;&#27492;&#65292;&#22312;&#20803;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#35266;&#23519;&#21040;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#21644;&#39069;&#22806;&#30340;&#20869;&#23384;&#24320;&#38144;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;Meta-LTH&#65292;&#23427;&#21253;&#25324;&#19981;&#21487;&#32570;&#23569;&#30340;&#36830;&#25509;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#34987;&#31216;&#20026;&#37325;&#35201;&#24615;&#21098;&#26525;&#30340;&#24425;&#31080;&#20551;&#35774;&#25216;&#26415;&#26469;&#29983;&#25104;&#36825;&#20123;&#20851;&#38190;&#30340;&#36830;&#25509;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23454;&#29616;&#20004;&#20214;&#20107;&#65306;(a) &#23547;&#25214;&#19968;&#20010;&#26356;&#36866;&#24212;&#20803;&#23398;&#20064;&#30340;&#23376;&#32593;&#32476;&#65292;(b) &#22312;&#20803;&#27979;&#35797;&#38454;&#27573;&#23398;&#20064;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#26032;&#20302;&#32423;&#29305;&#24449;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#24050;&#32463;&#23398;&#20064;&#30340;&#29305;&#24449;&#37325;&#26032;&#32452;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;Meta-LTH&#26041;&#27861;&#22312;&#26631;&#20934;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning aims to solve unseen tasks with few labelled instances. Nevertheless, despite its effectiveness for quick learning in existing optimization-based methods, it has several flaws. Inconsequential connections are frequently seen during meta-training, which results in an over-parameterized neural network. Because of this, meta-testing observes unnecessary computations and extra memory overhead. To overcome such flaws. We propose a novel meta-learning method called Meta-LTH that includes indispensible (necessary) connections. We applied the lottery ticket hypothesis technique known as magnitude pruning to generate these crucial connections that can effectively solve few-shot learning problem. We aim to perform two things: (a) to find a sub-network capable of more adaptive meta-learning and (b) to learn new low-level features of unseen tasks and recombine those features with the already learned features during the meta-test phase. Experimental results show that our proposed Met-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.02858</link><description>&lt;p&gt;
&#38754;&#21521;&#31867;&#21035;&#19981;&#22343;&#38382;&#39064;&#30340;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#32508;&#36848;&#65306;&#32452;&#21512;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65288;CI&#65289;&#26159;&#25351;&#23646;&#20110;&#19968;&#20010;&#31867;&#30340;&#35266;&#27979;&#20540;&#25968;&#37327;&#20302;&#20110;&#20854;&#20182;&#31867;&#30340;&#25968;&#37327;&#12290;&#38598;&#25104;&#23398;&#20064;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#19968;&#20123;&#31574;&#30053;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22686;&#24378;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#26412;&#25991;&#23545;&#29992;&#20110;&#35299;&#20915;&#22522;&#20934;CI&#38382;&#39064;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35745;&#31639;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;CI&#38382;&#39064;&#30340;10&#20010;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;10&#20010;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#25552;&#39640;&#20998;&#31867;&#25928;&#26524;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other classes. Ensemble learning that combines multiple models to obtain a robust model has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, but the true rank of different combinations would require a computational review. In this paper, we present a computational review to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We propose a general framework that evaluates 10 data augmentation and 10 ensemble learning methods for CI problems. Our objective was to identify the most effective combination for improving classificat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20854;&#22522;&#20110;&#27491;&#24577;&#20998;&#24067;&#65292;&#24182;&#21487;&#36890;&#36807;&#26368;&#23567;&#21270;&#36127;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.02849</link><description>&lt;p&gt;
&#20998;&#31867;&#20013;&#24322;&#26041;&#24046;&#26631;&#31614;&#22122;&#22768;&#30340;&#36923;&#36753;&#27491;&#24577;&#20284;&#28982;
&lt;/p&gt;
&lt;p&gt;
Logistic-Normal Likelihoods for Heteroscedastic Label Noise in Classification. (arXiv:2304.02849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20854;&#22522;&#20110;&#27491;&#24577;&#20998;&#24067;&#65292;&#24182;&#21487;&#36890;&#36807;&#26368;&#23567;&#21270;&#36127;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#24402;&#20013;&#20272;&#35745;&#24322;&#26041;&#24046;&#26631;&#31614;&#22122;&#22768;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#27861;&#26159;&#23558;&#35266;&#27979;&#21040;&#30340;&#65288;&#21487;&#33021;&#24102;&#26377;&#22122;&#22768;&#30340;&#65289;&#30446;&#26631;&#24314;&#27169;&#20026;&#19968;&#20010;&#27491;&#24577;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#20854;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#36127;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#12290;&#35813;&#25439;&#22833;&#20855;&#26377;&#26399;&#26395;&#30340;&#25439;&#22833;&#34928;&#20943;&#29305;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#38477;&#20302;&#39640;&#35823;&#24046;&#31034;&#20363;&#30340;&#36129;&#29486;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36825;&#31181;&#34892;&#20026;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#36807;&#25311;&#21512;&#26469;&#25552;&#39640;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#31181;&#31616;&#21333;&#19988;&#27010;&#29575;&#21270;&#26041;&#27861;&#22312;&#20998;&#31867;&#20013;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#30456;&#21516;&#30340;&#26399;&#26395;&#25439;&#22833;&#34928;&#20943;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#20854;&#23545;&#20998;&#31867;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#26469;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21551;&#21457;&#24615;&#30340;&#23454;&#39564;&#65292;&#25506;&#32034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#21253;&#25324;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#65292;&#28040;&#34701;&#30740;&#31350;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
A natural way of estimating heteroscedastic label noise in regression is to model the observed (potentially noisy) target as a sample from a normal distribution, whose parameters can be learned by minimizing the negative log-likelihood. This loss has desirable loss attenuation properties, as it can reduce the contribution of high-error examples. Intuitively, this behavior can improve robustness against label noise by reducing overfitting. We propose an extension of this simple and probabilistic approach to classification that has the same desirable loss attenuation properties. We evaluate the effectiveness of the method by measuring its robustness against label noise in classification. We perform enlightening experiments exploring the inner workings of the method, including sensitivity to hyperparameters, ablation studies, and more.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;Robustmix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;Imagenet-C&#21644;Stylized Imagenet&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#22312;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#21644;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#30340;&#21516;&#26102;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2304.02847</link><description>&lt;p&gt;
Robustmix&#65306;&#36890;&#36807;&#27491;&#21017;&#21270;&#28145;&#24230;&#32593;&#32476;&#30340;&#39057;&#29575;&#20559;&#24046;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets. (arXiv:2304.02847v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;Robustmix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;Imagenet-C&#21644;Stylized Imagenet&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#22312;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#21644;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#30340;&#21516;&#26102;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#22312;&#19968;&#31995;&#21015;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23545;&#20154;&#31867;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#30340;&#25200;&#21160;&#20173;&#28982;&#24456;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Robustmix&#30340;Mixup&#26032;&#25193;&#23637;&#65292;&#35813;&#25193;&#23637;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#22522;&#20110;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#27491;&#21017;&#21270;&#25913;&#21892;&#20102;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;Imagenet-C&#21644;Stylized Imagenet&#12290;&#23427;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#34917;&#20805;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20351;&#29992;EfficientNet-B8&#27169;&#22411;&#21644;RandAugment&#36798;&#21040;&#20102;44.8&#30340;&#26368;&#26032;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#30456;&#27604;&#22522;&#32447;&#38477;&#20302;&#20102;16&#20010;mCE&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep networks have achieved impressive results on a range of well-curated benchmark datasets. Surprisingly, their performance remains sensitive to perturbations that have little effect on human performance. In this work, we propose a novel extension of Mixup called Robustmix that regularizes networks to classify based on lower-frequency spatial features. We show that this type of regularization improves robustness on a range of benchmarks such as Imagenet-C and Stylized Imagenet. It adds little computational overhead and, furthermore, does not require a priori knowledge of a large set of image transformations. We find that this approach further complements recent advances in model architecture and data augmentation, attaining a state-of-the-art mCE of 44.8 with an EfficientNet-B8 model and RandAugment, which is a reduction of 16 mCE compared to the baseline.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RNAS&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#29983;&#25104;&#39640;&#36136;&#37327;&#26550;&#26500;&#65292;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#20013;&#22343;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.02845</link><description>&lt;p&gt;
&#22362;&#38887;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Robust Neural Architecture Search. (arXiv:2304.02845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02845
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RNAS&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#29983;&#25104;&#39640;&#36136;&#37327;&#26550;&#26500;&#65292;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#20013;&#22343;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#24448;&#24448;&#26356;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#24694;&#24847;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#24378;&#20581;&#30340;NAS&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#26469;&#22686;&#24378;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#30340;&#24378;&#20581;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#24573;&#30053;&#20102;NAS&#29983;&#25104;&#30340;&#27169;&#22411;&#30340;&#26412;&#36136;&#20934;&#30830;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#26041;&#27861;&#65292;&#21517;&#20026;Robust Neural Architecture Search&#65288;RNAS&#65289;&#12290;&#20026;&#20102;&#35774;&#35745;&#20986;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;RNAS&#29983;&#25104;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#33391;&#22909;&#40065;&#26834;&#24615;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#20943;&#23569;&#25628;&#32034;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#32780;&#19981;&#26159;&#23545;&#25239;&#24615;&#26679;&#26412;&#20316;&#20026;&#25628;&#32034;&#26550;&#26500;&#30340;&#36755;&#20837;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RNAS&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;RNAS&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architectures Search (NAS) becomes more and more popular over these years. However, NAS-generated models tends to suffer greater vulnerability to various malicious attacks. Lots of robust NAS methods leverage adversarial training to enhance the robustness of NAS-generated models, however, they neglected the nature accuracy of NAS-generated models. In our paper, we propose a novel NAS method, Robust Neural Architecture Search (RNAS). To design a regularization term to balance accuracy and robustness, RNAS generates architectures with both high accuracy and good robustness. To reduce search cost, we further propose to use noise examples instead adversarial examples as input to search architectures. Extensive experiments show that RNAS achieves state-of-the-art (SOTA) performance on both image classification and adversarial attacks, which illustrates the proposed RNAS achieves a good tradeoff between robustness and accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#65306;&#36890;&#36807;&#23545;&#40784;&#35757;&#32451;&#21160;&#24577;&#26469;&#25552;&#39640;&#21098;&#26525;&#25928;&#26524;&#65292;&#20855;&#20307;&#26469;&#35828;&#23601;&#26159;&#21098;&#21435;&#23545;NTK&#39057;&#35889;&#24433;&#21709;&#26368;&#23567;&#30340;&#36830;&#25509;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#32500;&#25345;NTK&#39057;&#35889;&#65292;&#20174;&#32780;&#23558;&#35757;&#32451;&#21160;&#24577;&#21644;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#35757;&#32451;&#21160;&#24577;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2304.02840</link><description>&lt;p&gt;
NTK-SAP: &#36890;&#36807;&#23545;&#40784;&#35757;&#32451;&#21160;&#24577;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
NTK-SAP: Improving neural network pruning by aligning training dynamics. (arXiv:2304.02840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#65306;&#36890;&#36807;&#23545;&#40784;&#35757;&#32451;&#21160;&#24577;&#26469;&#25552;&#39640;&#21098;&#26525;&#25928;&#26524;&#65292;&#20855;&#20307;&#26469;&#35828;&#23601;&#26159;&#21098;&#21435;&#23545;NTK&#39057;&#35889;&#24433;&#21709;&#26368;&#23567;&#30340;&#36830;&#25509;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#32500;&#25345;NTK&#39057;&#35889;&#65292;&#20174;&#32780;&#23558;&#35757;&#32451;&#21160;&#24577;&#21644;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#35757;&#32451;&#21160;&#24577;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20043;&#21069;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#22240;&#20854;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#23384;&#20648;&#31354;&#38388;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#26576;&#31181;&#24230;&#37327;&#23545;&#36830;&#25509;&#36827;&#34892;&#21098;&#26525;&#65292;&#20294;&#26159;&#20160;&#20040;&#24230;&#37327;&#26159;&#26368;&#22909;&#30340;&#36873;&#25321;&#36824;&#19981;&#23436;&#20840;&#28165;&#26970;&#12290;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#36275;&#22815;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#19982;NTK&#30340;&#39057;&#35889;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#27492;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24314;&#35758;&#21098;&#26525;&#37027;&#20123;&#23545;NTK&#39057;&#35889;&#24433;&#21709;&#26368;&#23567;&#30340;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#32500;&#25345;NTK&#39057;&#35889;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#23558;&#35757;&#32451;&#21160;&#24577;&#19982;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#35757;&#32451;&#21160;&#24577;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#21487;&#33021;&#30340;&#38382;&#39064;&#26159;&#32473;&#23450;&#21021;&#22987;&#28857;&#23545;&#24212;&#30340;&#22266;&#23450;&#26435;&#20540;NTK&#21487;&#33021;&#19982;&#35757;&#32451;&#38454;&#27573;&#21518;&#30340;&#36845;&#20195;&#23545;&#24212;&#30340;NTK&#38750;&#24120;&#19981;&#21516;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#35758;&#23545;&#38543;&#26426;&#26435;&#37325;&#30340;&#22810;&#20010;&#23454;&#29616;&#36827;&#34892;&#37319;&#26679;&#20197;&#20272;&#35745;NTK&#39057;&#35889;&#12290;&#35831;&#27880;&#24847;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26435;&#37325;&#26080;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning neural networks before training has received increasing interest due to its potential to reduce training time and memory. One popular method is to prune the connections based on a certain metric, but it is not entirely clear what metric is the best choice. Recent advances in neural tangent kernel (NTK) theory suggest that the training dynamics of large enough neural networks is closely related to the spectrum of the NTK. Motivated by this finding, we propose to prune the connections that have the least influence on the spectrum of the NTK. This method can help maintain the NTK spectrum, which may help align the training dynamics to that of its dense counterpart. However, one possible issue is that the fixed-weight-NTK corresponding to a given initial point can be very different from the NTK corresponding to later iterates during the training phase. We further propose to sample multiple realizations of random weights to estimate the NTK spectrum. Note that our approach is weight
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26469;&#28304;&#22270;&#21644;Transformer&#30340;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25552;&#21462;&#31995;&#32479;&#29366;&#24577;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26469;&#28304;&#20998;&#26512;&#23454;&#29616;&#23545;&#38271;&#26399;&#36816;&#34892;&#31995;&#32479;&#30340;&#27010;&#25324;&#65292;&#20197;&#26816;&#27979;&#32531;&#24930;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.02838</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#21644;&#26469;&#28304;&#22270;&#30340;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TBDetector:Transformer-Based Detector for Advanced Persistent Threats with Provenance Graph. (arXiv:2304.02838v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26469;&#28304;&#22270;&#21644;Transformer&#30340;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25552;&#21462;&#31995;&#32479;&#29366;&#24577;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26469;&#28304;&#20998;&#26512;&#23454;&#29616;&#23545;&#38271;&#26399;&#36816;&#34892;&#31995;&#32479;&#30340;&#27010;&#25324;&#65292;&#20197;&#26816;&#27979;&#32531;&#24930;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#65288;APT&#65289;&#25915;&#20987;&#30340;&#38271;&#26399;&#28508;&#20239;&#12289;&#38544;&#31192;&#22810;&#38454;&#27573;&#25915;&#20987;&#27169;&#24335;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;APT&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#28304;&#22270;&#25552;&#20379;&#30340;&#21382;&#21490;&#20449;&#24687;&#36827;&#34892;APT&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25552;&#21462;&#31995;&#32479;&#29366;&#24577;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26469;&#28304;&#20998;&#26512;&#23454;&#29616;&#23545;&#38271;&#26399;&#36816;&#34892;&#31995;&#32479;&#30340;&#27010;&#25324;&#65292;&#20197;&#26816;&#27979;&#32531;&#24930;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#24341;&#20837;&#20102;&#24322;&#24120;&#35780;&#20998;&#65292;&#21487;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#29366;&#24577;&#30340;&#24322;&#24120;&#24615;&#12290;&#27599;&#20010;&#29366;&#24577;&#37117;&#26377;&#30456;&#24212;&#30340;&#30456;&#20284;&#24230;&#21644;&#38548;&#31163;&#24230;&#20998;&#25968;&#30340;&#24322;&#24120;&#20998;&#25968;&#35745;&#31639;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
APT detection is difficult to detect due to the long-term latency, covert and slow multistage attack patterns of Advanced Persistent Threat (APT). To tackle these issues, we propose TBDetector, a transformer-based advanced persistent threat detection method for APT attack detection. Considering that provenance graphs provide rich historical information and have the powerful attacks historic correlation ability to identify anomalous activities, TBDetector employs provenance analysis for APT detection, which summarizes long-running system execution with space efficiency and utilizes transformer with self-attention based encoder-decoder to extract long-term contextual features of system states to detect slow-acting attacks. Furthermore, we further introduce anomaly scores to investigate the anomaly of different system states, where each state is calculated with an anomaly score corresponding to its similarity score and isolation score. To evaluate the effectiveness of the proposed method,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#25972;&#21512;&#20102;EHR&#20013;&#30340;&#25104;&#20687;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.02836</link><description>&lt;p&gt;
&#38271;&#26399;&#30340;&#22810;&#27169;&#24335;&#21464;&#21387;&#22120;&#25972;&#21512;EHR&#20013;&#25104;&#20687;&#21644;&#28508;&#22312;&#20020;&#24202;&#29305;&#24449;&#65292;&#29992;&#20110;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification. (arXiv:2304.02836v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#25972;&#21512;&#20102;EHR&#20013;&#30340;&#25104;&#20687;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#37325;&#22797;&#25104;&#20687;&#21644;&#21307;&#30103;&#32972;&#26223;&#65288;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65289;&#32435;&#20837;&#39044;&#27979;&#24615;&#23396;&#31435;&#24615;&#32954;&#37096;&#32467;&#33410;&#65288;SPN&#65289;&#35786;&#26029;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20687;&#25104;&#20687;&#21644;&#35786;&#26029;&#20195;&#30721;&#36825;&#26679;&#30340;&#20020;&#24202;&#24120;&#35268;&#27169;&#24335;&#21487;&#33021;&#26159;&#24322;&#27493;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#19981;&#35268;&#21017;&#37319;&#26679;&#65292;&#36825;&#26159;&#38271;&#26399;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#27169;&#24577;&#31574;&#30053;&#65292;&#23558;&#37325;&#22797;&#25104;&#20687;&#19982;&#26085;&#24120;&#25910;&#38598;&#30340;EHR&#20013;&#30340;&#38271;&#26399;&#20020;&#24202;&#29305;&#24449;&#30456;&#25972;&#21512;&#65292;&#20197;&#36827;&#34892;SPN&#20998;&#31867;&#12290;&#25105;&#20204;&#23545;&#28508;&#22312;&#20020;&#24202;&#29305;&#24449;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#35299;&#32544;&#32538;&#65292;&#24182;&#21033;&#29992;&#26102;&#38388;&#36317;&#31163;&#32553;&#25918;&#33258;&#27880;&#24847;&#21147;&#26469;&#32852;&#21512;&#23398;&#20064;&#20020;&#24202;&#29305;&#24449;&#34920;&#36798;&#21644;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#26159;&#22312;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;2,668&#20010;&#25195;&#25551;&#21644;1,149&#21517;&#24535;&#24895;&#32773;&#30340;&#38271;&#26399;&#33016;&#37096;CT&#12289;&#36134;&#21333;&#20195;&#30721;&#12289;&#33647;&#29289;&#21644;&#23454;&#39564;&#23460;&#26816;&#26597;&#35760;&#24405;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy of predictive models for solitary pulmonary nodule (SPN) diagnosis can be greatly increased by incorporating repeat imaging and medical context, such as electronic health records (EHRs). However, clinically routine modalities such as imaging and diagnostic codes can be asynchronous and irregularly sampled over different time scales which are obstacles to longitudinal multimodal learning. In this work, we propose a transformer-based multimodal strategy to integrate repeat imaging with longitudinal clinical signatures from routinely collected EHRs for SPN classification. We perform unsupervised disentanglement of latent clinical signatures and leverage time-distance scaled self-attention to jointly learn from clinical signatures expressions and chest computed tomography (CT) scans. Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36951;&#24536;&#26041;&#27861;GIF&#65292;&#37319;&#29992;&#24433;&#21709;&#20989;&#25968;&#26469;&#31227;&#38500;&#27169;&#22411;&#20013;&#29305;&#23450;&#33410;&#28857;&#12289;&#36793;&#25110;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#22312;&#24615;&#33021;&#21644;&#22797;&#26434;&#24230;&#30340;&#24179;&#34913;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.02835</link><description>&lt;p&gt;
GIF&#65306;&#19968;&#31181;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36951;&#24536;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GIF: A General Graph Unlearning Strategy via Influence Function. (arXiv:2304.02835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36951;&#24536;&#26041;&#27861;GIF&#65292;&#37319;&#29992;&#24433;&#21709;&#20989;&#25968;&#26469;&#31227;&#38500;&#27169;&#22411;&#20013;&#29305;&#23450;&#33410;&#28857;&#12289;&#36793;&#25110;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#22312;&#24615;&#33021;&#21644;&#22797;&#26434;&#24230;&#30340;&#24179;&#34913;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#22312;&#25105;&#20204;&#30340;&#31038;&#20250;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22312;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#25764;&#38144;&#29305;&#23450;&#25968;&#25454;&#30340;&#24433;&#21709;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36951;&#24536;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#36951;&#24536;&#21040;&#26368;&#36817;&#20986;&#29616;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36951;&#24536;&#26041;&#27861;&#65292;&#35201;&#20040;&#37319;&#29992;&#37325;&#26032;&#35757;&#32451;&#30340;&#33539;&#24335;&#65292;&#35201;&#20040;&#25191;&#34892;&#36817;&#20284;&#25273;&#28040;&#65292;&#32780;&#36825;&#31181;&#26041;&#27861;&#26410;&#33021;&#32771;&#34385;&#21040;&#36830;&#25509;&#37051;&#23621;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#25110;&#23545;GNN&#32467;&#26500;&#26045;&#21152;&#38480;&#21046;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021; - &#22797;&#26434;&#24615;&#24179;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36951;&#24536;&#37327;&#36523;&#23450;&#21046;&#30340;&#24433;&#21709;&#20989;&#25968;&#65292;&#20197;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#36951;&#24536;&#20219;&#21153;&#30340;&#32479;&#19968;&#38382;&#39064;&#34920;&#36848;&#65288;&#33410;&#28857;&#12289;&#36793;&#12289;&#29305;&#24449;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#20256;&#32479;&#24433;&#21709;&#20989;&#25968;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#36951;&#24536;&#26080;&#33021;&#20026;&#21147;&#30340;&#20851;&#38190;&#25152;&#22312;&#65292;&#24182;&#35774;&#35745;&#20102;&#22270;&#24433;&#21709;&#20989;&#25968;(GIF)&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#36951;&#24536;&#26426;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#24433;&#21709;&#20989;&#25968;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#29305;&#23450;&#33410;&#28857;&#12289;&#36793;&#25110;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#22522;&#32447;&#30456;&#27604;&#65292;GIF&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#19978;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the greater emphasis on privacy and security in our society, the problem of graph unlearning -- revoking the influence of specific data on the trained GNN model, is drawing increasing attention. However, ranging from machine unlearning to recently emerged graph unlearning methods, existing efforts either resort to retraining paradigm, or perform approximate erasure that fails to consider the inter-dependency between connected neighbors or imposes constraints on GNN structure, therefore hard to achieve satisfying performance-complexity trade-offs.  In this work, we explore the influence function tailored for graph unlearning, so as to improve the unlearning efficacy and efficiency for graph unlearning. We first present a unified problem formulation of diverse graph unlearning tasks \wrt node, edge, and feature. Then, we recognize the crux to the inability of traditional influence function for graph unlearning, and devise Graph Influence Function (GIF), a model-agnostic unlearning m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#36710;&#36742;&#36873;&#25321;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#36710;&#32852;&#32593;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35757;&#32451;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2304.02832</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#36710;&#36742;&#36873;&#25321;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Based Vehicle Selection for Asynchronous Federated Learning Enabled Vehicular Edge Computing. (arXiv:2304.02832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#36710;&#36742;&#36873;&#25321;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#36710;&#32852;&#32593;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35757;&#32451;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#36710;&#32852;&#32593;&#20013;&#65292;&#36710;&#36742;&#29983;&#25104;&#30340;&#35745;&#31639;&#20219;&#21153;&#36890;&#24120;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20219;&#21153;&#21368;&#36733;&#20250;&#23548;&#33268;&#22823;&#37327;&#24310;&#36831;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#36710;&#36742;&#36793;&#32536;&#35745;&#31639; (VEC) &#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#20855;&#26377;&#19968;&#23450;&#35745;&#31639;&#33021;&#21147;&#30340;&#36335;&#20391;&#21333;&#20803; (RSU) &#20316;&#20026;&#36793;&#32536;&#23454;&#20307;&#26469;&#22788;&#29702;&#36710;&#36742;&#30340;&#25968;&#25454;&#12290;&#30001;&#20110;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#36710;&#36742;&#19981;&#24895;&#30452;&#25509;&#19978;&#20256;&#26412;&#22320;&#25968;&#25454;&#21040; RSU&#65292;&#22240;&#27492;&#32852;&#37030;&#23398;&#20064; (FL) &#25104;&#20026; VEC &#20013;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#65292;&#36710;&#36742;&#21482;&#38656;&#35201;&#19978;&#20256;&#26412;&#22320;&#27169;&#22411;&#36229;&#21442;&#25968;&#32780;&#19981;&#26159;&#23558;&#26412;&#22320;&#25968;&#25454;&#36716;&#31227;&#21040;&#38468;&#36817;&#30340; RSU&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#36710;&#36742;&#20855;&#26377;&#19981;&#21516;&#30340;&#26412;&#22320;&#35757;&#32451;&#26102;&#38388;&#65292;&#22240;&#20026;&#26412;&#22320;&#25968;&#25454;&#30340;&#22823;&#23567;&#21644;&#19981;&#21516;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#25152;&#20197;&#37319;&#29992;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064; (AFL) &#26469;&#20419;&#36827; RSU &#20174;&#25152;&#26377;&#21442;&#19982;&#36710;&#36742;&#30340;&#32858;&#21512;&#26412;&#22320;&#26356;&#26032;&#20013;&#26356;&#26032;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340; AFL-VEC &#36710;&#36742;&#36873;&#25321;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#36873;&#25321;&#20855;&#26377;&#39640;&#35745;&#31639;&#33021;&#21147;&#21644;&#36890;&#20449;&#33021;&#21147;&#30340;&#26368;&#36866;&#21512;&#30340;&#36710;&#36742;&#21442;&#19982; AFL &#36827;&#31243;&#65292;&#20197;&#25552;&#39640;&#25972;&#20307;&#35757;&#32451;&#24615;&#33021;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35757;&#32451;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the traditional vehicular network, computing tasks generated by the vehicles are usually uploaded to the cloud for processing. However, since task offloading toward the cloud will cause a large delay, vehicular edge computing (VEC) is introduced to avoid such a problem and improve the whole system performance, where a roadside unit (RSU) with certain computing capability is used to process the data of vehicles as an edge entity. Owing to the privacy and security issues, vehicles are reluctant to upload local data directly to the RSU, and thus federated learning (FL) becomes a promising technology for some machine learning tasks in VEC, where vehicles only need to upload the local model hyperparameters instead of transferring their local data to the nearby RSU. Furthermore, as vehicles have different local training time due to various sizes of local data and their different computing capabilities, asynchronous federated learning (AFL) is employed to facilitate the RSU to update the g
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#25345;&#32493;&#38598;&#25104;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#23384;&#22312;&#30340;&#19981;&#36275;&#21644;&#25913;&#36827;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.02829</link><description>&lt;p&gt;
SoK: &#26426;&#22120;&#23398;&#20064;&#22312;&#25345;&#32493;&#38598;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SoK: Machine Learning for Continuous Integration. (arXiv:2304.02829v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02829
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#25345;&#32493;&#38598;&#25104;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#23384;&#22312;&#30340;&#19981;&#36275;&#21644;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#38598;&#25104;&#65288;CI&#65289;&#24050;&#25104;&#20026;&#33258;&#21160;&#21644;&#25345;&#32493;&#38598;&#25104;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#20195;&#30721;&#21464;&#26356;&#30340;&#19968;&#39033;&#25104;&#29087;&#30340;&#36719;&#20214;&#24320;&#21457;&#23454;&#36341;&#12290;&#36234;&#26469;&#36234;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#21270;CI&#36807;&#31243;&#30340;&#25253;&#36947;&#27491;&#22312;&#20986;&#29616;&#12290;&#25552;&#20379;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#22312;CI&#38454;&#27573;&#24212;&#29992;&#30340;&#30693;&#35782;&#20307;&#31995;&#21270;&#65288;SoK&#65289;&#26159;&#21450;&#26102;&#19988;&#30456;&#20851;&#30340;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;CI&#30340;&#19981;&#21516;&#26041;&#38754;&#30340; SoK&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20998;&#26512;&#36824;&#24378;&#35843;&#20102;&#29616;&#26377;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#32570;&#38519;&#65292;&#21487;&#20197;&#25913;&#36827;&#20197;&#25512;&#36827;&#26368;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous Integration (CI) has become a well-established software development practice for automatically and continuously integrating code changes during software development. An increasing number of Machine Learning (ML) based approaches for automation of CI phases are being reported in the literature. It is timely and relevant to provide a Systemization of Knowledge (SoK) of ML-based approaches for CI phases. This paper reports an SoK of different aspects of the use of ML for CI. Our systematic analysis also highlights the deficiencies of the existing ML-based solutions that can be improved for advancing the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.02819</link><description>&lt;p&gt;
GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#25512;&#24191;&#24102;&#26469;&#20102;&#25968;&#23383;&#36890;&#20449;&#26041;&#38754;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#26469;&#21306;&#20998;AI&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#33521;&#35821;&#27597;&#35821;&#21644;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#30340;&#20889;&#20316;&#26679;&#26412;&#35780;&#20272;&#20102;&#20960;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;GPT&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#25345;&#32493;&#23558;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20889;&#20316;&#26679;&#26412;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#32780;&#21407;&#29983;&#20889;&#20316;&#26679;&#26412;&#21017;&#33021;&#22815;&#34987;&#20934;&#30830;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#26080;&#24847;&#20013;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21628;&#21505;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#22411;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#22240;&#26524;&#35786;&#26029;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#30699;&#27491;&#26377;&#38382;&#39064;&#30340;&#36755;&#20837;/&#36755;&#20986;&#34892;&#20026;&#23376;&#38598;&#65292;&#35782;&#21035;&#30495;&#27491;&#30340;&#23646;&#24615;&#36829;&#35268;&#21407;&#22240;&#24182;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2304.02813</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#22411;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#22240;&#26524;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Causal Repair of Learning-enabled Cyber-physical Systems. (arXiv:2304.02813v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#22411;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#22240;&#26524;&#35786;&#26029;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#30699;&#27491;&#26377;&#38382;&#39064;&#30340;&#36755;&#20837;/&#36755;&#20986;&#34892;&#20026;&#23376;&#38598;&#65292;&#35782;&#21035;&#30495;&#27491;&#30340;&#23646;&#24615;&#36829;&#35268;&#21407;&#22240;&#24182;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#22240;&#26524;&#27169;&#22411;&#20351;&#29992;&#39046;&#22495;&#30693;&#35782;&#29983;&#25104;&#23548;&#33268;&#32467;&#26524;&#30340;&#20107;&#20214;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#35786;&#26029;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#20855;&#26377;&#23398;&#20064;&#22686;&#24378;&#32452;&#20214;&#65288;LEC&#65289;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#65288;CPS&#65289;&#20013;&#35786;&#26029;&#21644;&#20462;&#22797;&#36816;&#34892;&#26102;&#23646;&#24615;&#36829;&#35268;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;LEC&#30340;&#39640;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#65288;&#20363;&#22914;CPS&#21160;&#24577;&#65289;&#32534;&#30721;&#25104;&#21487;&#29983;&#25104;&#26377;&#29992;&#20462;&#22797;&#24314;&#35758;&#30340;&#21487;&#25193;&#23637;&#23454;&#38469;&#22240;&#26524;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#35786;&#26029;&#38598;&#20013;&#20110;LEC&#30340;&#36755;&#20837;/&#36755;&#20986;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;LEC&#30340;&#21738;&#20010;&#36755;&#20837;/&#36755;&#20986;&#34892;&#20026;&#23376;&#38598;&#26159;&#23548;&#33268;&#23646;&#24615;&#36829;&#35268;&#30340;&#30495;&#27491;&#21407;&#22240;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#21103;&#20135;&#21697;&#26159;&#30699;&#27491;&#35782;&#21035;&#20986;&#26377;&#38382;&#39064;&#30340;&#34892;&#20026;&#26469;&#20462;&#22797;&#36816;&#34892;&#26102;&#23646;&#24615;&#30340;LEC&#30340;&#21453;&#20107;&#23454;&#29256;&#26412;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#27493;&#35786;&#26029;&#27969;&#31243;&#65306;&#65288;1&#65289;&#26500;&#24314;Halpern-Pearl&#22240;&#26524;&#27169;&#22411;&#20197;&#21453;&#26144;&#23646;&#24615;&#32467;&#26524;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of actual causality leverage domain knowledge to generate convincing diagnoses of events that caused an outcome. It is promising to apply these models to diagnose and repair run-time property violations in cyber-physical systems (CPS) with learning-enabled components (LEC). However, given the high diversity and complexity of LECs, it is challenging to encode domain knowledge (e.g., the CPS dynamics) in a scalable actual causality model that could generate useful repair suggestions. In this paper, we focus causal diagnosis on the input/output behaviors of LECs. Specifically, we aim to identify which subset of I/O behaviors of the LEC is an actual cause for a property violation. An important by-product is a counterfactual version of the LEC that repairs the run-time property by fixing the identified problematic behaviors. Based on this insights, we design a two-step diagnostic pipeline: (1) construct and Halpern-Pearl causality model that reflects the dependency of property outcom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#24182;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#36890;&#36807;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.02811</link><description>&lt;p&gt;
HomPINNs&#65306;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
HomPINNs: homotopy physics-informed neural networks for solving the inverse problems of nonlinear differential equations with multiple solutions. (arXiv:2304.02811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#24182;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#36890;&#36807;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35299;&#31354;&#38388;&#20013;&#30340;&#38750;&#21807;&#19968;&#24615;&#12289;&#23545;&#31216;&#24615;&#21644;&#20998;&#23700;&#31561;&#22797;&#26434;&#34892;&#20026;&#65292;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#65288;DEs&#65289;&#30340;&#21453;&#38382;&#39064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21516;&#20262;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;HomPINNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#21516;&#20262;&#36830;&#32493;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#26469;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#39318;&#20808;&#20351;&#29992;NNs&#21516;&#26102;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#21644;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#36890;&#36807;&#21033;&#29992;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#65292;&#36924;&#36817;&#21487;&#36861;&#36394;&#35266;&#23519;&#32467;&#26524;&#20197;&#30830;&#23450;&#22810;&#20010;&#35299;&#24182;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#28085;&#30422;&#22312;&#19968;&#32500;DEs&#19978;&#27979;&#35797;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#24212;&#29992;&#23427;&#26469;&#35299;&#20915;&#20108;&#32500;Gray-Scott&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the complex behavior arising from non-uniqueness, symmetry, and bifurcations in the solution space, solving inverse problems of nonlinear differential equations (DEs) with multiple solutions is a challenging task. To address this issue, we propose homotopy physics-informed neural networks (HomPINNs), a novel framework that leverages homotopy continuation and neural networks (NNs) to solve inverse problems. The proposed framework begins with the use of a NN to simultaneously approximate known observations and conform to the constraints of DEs. By utilizing the homotopy continuation method, the approximation traces the observations to identify multiple solutions and solve the inverse problem. The experiments involve testing the performance of the proposed method on one-dimensional DEs and applying it to solve a two-dimensional Gray-Scott simulation. Our findings demonstrate that the proposed method is scalable and adaptable, providing an effective solution for solving DEs with mul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#28151;&#21512;&#19987;&#23478;&#65288;GMoE&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#20855;&#26377;&#22810;&#26679;&#30340;&#22270;&#32467;&#26500;&#21644;&#21253;&#21547;&#24322;&#26500;&#33410;&#28857;&#21644;&#36793;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22686;&#24378;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#24212;&#22810;&#26679;&#30340;&#35757;&#32451;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#19981;&#20250;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2304.02806</link><description>&lt;p&gt;
&#22270;&#28151;&#21512;&#19987;&#23478;&#65306;&#26174;&#24335;&#22810;&#26679;&#24615;&#24314;&#27169;&#19979;&#30340;&#22823;&#35268;&#27169;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling. (arXiv:2304.02806v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#28151;&#21512;&#19987;&#23478;&#65288;GMoE&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#20855;&#26377;&#22810;&#26679;&#30340;&#22270;&#32467;&#26500;&#21644;&#21253;&#21547;&#24322;&#26500;&#33410;&#28857;&#21644;&#36793;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22686;&#24378;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#24212;&#22810;&#26679;&#30340;&#35757;&#32451;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#19981;&#20250;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#36890;&#24120;&#20855;&#26377;&#22810;&#26679;&#30340;&#22270;&#32467;&#26500;&#65292;&#24182;&#19988;&#21253;&#21547;&#24322;&#26500;&#33410;&#28857;&#21644;&#36793;&#12290;&#20026;&#20102;&#22686;&#24378;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#35757;&#32451;&#22270;&#32467;&#26500;&#30340;&#22810;&#26679;&#24615;&#24050;&#25104;&#20026;&#24120;&#35265;&#20570;&#27861;&#12290;&#20294;&#26159;&#65292;&#31616;&#21333;&#22320;&#22686;&#21152;GNN&#27169;&#22411;&#23481;&#37327;&#23558;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#25512;&#29702;&#25104;&#26412;&#21644;GNN&#38590;&#20197;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30340;&#24605;&#24819;&#24341;&#20837;&#21040;GNN&#20013;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#36866;&#24212;&#22810;&#26679;&#30340;&#35757;&#32451;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#26032;&#22270;&#28151;&#21512;&#19987;&#23478;&#65288;GMoE&#65289;&#27169;&#22411;&#20351;&#24471;&#22270;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;&#21487;&#20197;&#21160;&#24577;&#22320;&#36873;&#25321;&#20854;&#33258;&#24049;&#30340;&#26368;&#20339;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have been widely applied to learning over graph data. Yet, real-world graphs commonly exhibit diverse graph structures and contain heterogeneous nodes and edges. Moreover, to enhance the generalization ability of GNNs, it has become common practice to further increase the diversity of training graph structures by incorporating graph augmentations and/or performing large-scale pre-training on more graphs. Therefore, it becomes essential for a GNN to simultaneously model diverse graph structures. Yet, naively increasing the GNN model capacity will suffer from both higher inference costs and the notorious trainability issue of GNNs. This paper introduces the Mixture-of-Expert (MoE) idea to GNNs, aiming to enhance their ability to accommodate the diversity of training graph structures, without incurring computational overheads. Our new Graph Mixture of Expert (GMoE) model enables each node in the graph to dynamically select its own optimal \textit{information a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35302;&#21457;&#22120;&#21453;&#28436;&#30340;&#32479;&#19968;&#26694;&#26550; UNICORN&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#21518;&#38376;&#27169;&#22411;&#24182;&#29702;&#35299;&#26893;&#20837;&#30340;&#24694;&#24847;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.02786</link><description>&lt;p&gt;
UNICORN: &#19968;&#31181;&#32479;&#19968;&#30340;&#21518;&#38376;&#35302;&#21457;&#21453;&#28436;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UNICORN: A Unified Backdoor Trigger Inversion Framework. (arXiv:2304.02786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35302;&#21457;&#22120;&#21453;&#28436;&#30340;&#32479;&#19968;&#26694;&#26550; UNICORN&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#21518;&#38376;&#27169;&#22411;&#24182;&#29702;&#35299;&#26893;&#20837;&#30340;&#24694;&#24847;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#20005;&#37325;&#23041;&#32961;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25915;&#20987;&#25163;&#27573;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27880;&#20837;&#24102;&#26377;&#35302;&#21457;&#20449;&#21495;&#30340;&#36755;&#20837;&#25968;&#25454;&#65288;&#22914;&#34917;&#19969;&#65289;&#26469;&#28608;&#27963;&#39044;&#20808;&#26893;&#20837;&#30340;&#24694;&#24847;&#34892;&#20026;&#12290;&#35302;&#21457;&#20449;&#21495;&#21453;&#28436;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21518;&#38376;&#27169;&#22411;&#35782;&#21035;&#21644;&#23545;&#26893;&#20837;&#36827;&#21435;&#30340;&#24694;&#24847;&#34892;&#20026;&#36827;&#34892;&#29702;&#35299;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#21453;&#28436;&#35302;&#21457;&#20449;&#21495;&#26102;&#20250;&#26377;&#35768;&#22810;&#19981;&#21516;&#30340;&#26500;&#36896;&#26041;&#24335;&#65292;&#20294;&#22240;&#20026;&#37319;&#29992;&#20102;&#19968;&#20123;&#36807;&#20110;&#20855;&#20307;&#30340;&#20551;&#35774;&#25110;&#32773;&#25915;&#20987;&#26041;&#24335;&#29305;&#23450;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#23601;&#26080;&#27861;&#27867;&#21270;&#21040;&#21508;&#31181;&#31867;&#22411;&#30340;&#35302;&#21457;&#20449;&#21495;&#19978;&#12290;&#26681;&#26412;&#21407;&#22240;&#26159;&#29616;&#26377;&#24037;&#20316;&#27809;&#26377;&#32771;&#34385;&#35302;&#21457;&#22120;&#35774;&#35745;&#31354;&#38388;&#23545;&#21453;&#28436;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#21644;&#20998;&#26512;&#20102;&#19981;&#21516;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#35302;&#21457;&#22120;&#21450;&#20854;&#21453;&#28436;&#38382;&#39064;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#28436;&#38382;&#39064;&#30340;&#35302;&#21457;&#22120;&#24418;&#24335;&#21270;&#23450;&#20041;&#21644;&#20998;&#26512;&#35782;&#21035;&#21518;&#38376;&#27169;&#22411;&#20869;&#37096;&#34892;&#20026;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21407;&#22411; UNICORN &#22312;&#27867;&#21270;&#24615;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The backdoor attack, where the adversary uses inputs stamped with triggers (e.g., a patch) to activate pre-planted malicious behaviors, is a severe threat to Deep Neural Network (DNN) models. Trigger inversion is an effective way of identifying backdoor models and understanding embedded adversarial behaviors. A challenge of trigger inversion is that there are many ways of constructing the trigger. Existing methods cannot generalize to various types of triggers by making certain assumptions or attack-specific constraints. The fundamental reason is that existing work does not consider the trigger's design space in their formulation of the inversion problem. This work formally defines and analyzes the triggers injected in different spaces and the inversion problem. Then, it proposes a unified framework to invert backdoor triggers based on the formalization of triggers and the identified inner behaviors of backdoor models from our analysis. Our prototype UNICORN is general and effective in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#39044;&#27979;&#20116;&#31181;&#31227;&#26893;&#21518;&#39118;&#38505;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02780</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20844;&#27491;&#39044;&#27979;&#32925;&#31227;&#26893;&#21518;&#39118;&#38505;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
A Transformer-Based Deep Learning Approach for Fairly Predicting Post-Liver Transplant Risk Factors. (arXiv:2304.02780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#39044;&#27979;&#20116;&#31181;&#31227;&#26893;&#21518;&#39118;&#38505;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32925;&#31227;&#26893;&#26159;&#23545;&#20110;&#26202;&#26399;&#32925;&#30149;&#24739;&#32773;&#30340;&#19968;&#39033;&#25327;&#25937;&#24615;&#25163;&#26415;&#65292;&#32780;&#35813;&#25163;&#26415;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#20026;&#20379;&#20307;&#23547;&#25214;&#26368;&#20339;&#21305;&#37197;&#30340;&#21463;&#20307;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#20122;&#32676;&#20307;&#20043;&#38388;&#30830;&#20445;&#31227;&#26893;&#30340;&#20844;&#24179;&#24615;&#12290;&#20256;&#32479;&#30340;MELD&#35780;&#20998;&#31995;&#32479;&#21482;&#33021;&#35780;&#20272;&#22312;90&#22825;&#20869;&#26410;&#25509;&#21463;&#22120;&#23448;&#31227;&#26893;&#30340;&#24739;&#32773;&#30340;&#27515;&#20129;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#20379;&#21463;&#20307;&#21305;&#37197;&#20063;&#24212;&#35813;&#32771;&#34385;&#21040;&#31227;&#26893;&#21518;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#22914;&#24515;&#34880;&#31649;&#30142;&#30149;&#12289;&#24930;&#24615;&#25490;&#24322;&#31561;&#65292;&#36825;&#20123;&#37117;&#26159;&#31227;&#26893;&#21518;&#24120;&#35265;&#30340;&#24182;&#21457;&#30151;&#12290;&#20934;&#30830;&#39044;&#27979;&#36825;&#20123;&#39118;&#38505;&#20998;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20854;&#21046;&#23450;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;&#25552;&#20986;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#21516;&#26102;&#39044;&#27979;&#20116;&#31181;&#31227;&#26893;&#21518;&#39118;&#38505;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Liver transplantation is a life-saving procedure for patients with end-stage liver disease. There are two main challenges in liver transplant: finding the best matching patient for a donor and ensuring transplant equity among different subpopulations. The current MELD scoring system evaluates a patient's mortality risk if not receiving an organ within 90 days. However, the donor-patient matching should also take into consideration post-transplant risk factors, such as cardiovascular disease, chronic rejection, etc., which are all common complications after transplant. Accurate prediction of these risk scores remains a significant challenge. In this study, we will use predictive models to solve the above challenge. We propose a deep learning framework model to predict multiple risk factors after a liver transplant. By formulating it as a multi-task learning problem, the proposed deep neural network was trained on this data to simultaneously predict the five post-transplant risks and ach
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#30005;&#23376;&#30149;&#21382;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.02768</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Application of Transformers based methods in Electronic Medical Records: A Systematic Literature Review. (arXiv:2304.02768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02768
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#30005;&#23376;&#30149;&#21382;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21487;&#29992;&#25968;&#25454;&#30340;&#22686;&#38271;&#21644;&#23427;&#20204;&#30340;&#38750;&#32467;&#26500;&#21270;&#24615;&#36136;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#24320;&#22987;&#21463;&#21040;&#20851;&#27880;&#65292;&#20197;&#20174;&#36825;&#20123;&#25968;&#25454;&#36164;&#20135;&#20013;&#33719;&#24471;&#20215;&#20540;&#65292;&#22240;&#20026;&#36825;&#31181;&#26684;&#24335;&#19981;&#36866;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#12290;&#26412;&#25991;&#23545;&#19981;&#21516;NLP&#20219;&#21153;&#20013;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;EMR&#19978;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#22312;&#26368;&#21021;&#30340;&#26597;&#35810;&#20013;&#65292;&#20174;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#24211;&#20013;&#36873;&#25321;&#20102;99&#31687;&#25991;&#31456;&#65292;&#26368;&#32456;&#31579;&#36873;&#24471;&#21040;&#20102;65&#31687;&#25991;&#31456;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#26412;&#25991;&#23558;&#20174;&#19994;&#21153;&#38382;&#39064;&#12289;NLP&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12289;&#24314;&#27169;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#35821;&#35328;&#21644;&#20132;&#25442;&#26684;&#24335;&#31561;&#26041;&#38754;&#23545;&#36825;&#20123;&#35770;&#25991;&#36827;&#34892;&#20998;&#26512;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combined growth of available data and their unstructured nature has received increased interest in natural language processing (NLP) techniques to make value of these data assets since this format is not suitable for statistical analysis. This work presents a systematic literature review of state-of-the-art advances using transformer-based methods on electronic medical records (EMRs) in different NLP tasks. To the best of our knowledge, this work is unique in providing a comprehensive review of research on transformer-based methods for NLP applied to the EMR field. In the initial query, 99 articles were selected from three public databases and filtered into 65 articles for detailed analysis. The papers were analyzed with respect to the business problem, NLP task, models and techniques, availability of datasets, reproducibility of modeling, language, and exchange format. The paper presents some limitations of current research and some recommendations for further research.
&lt;/p&gt;</description></item><item><title>MethaneMapper&#26159;&#19968;&#20010;&#21487;&#29992;&#20110;&#20809;&#35889;&#22495;&#20869;&#23450;&#20301;&#30002;&#28919;&#25490;&#25918;&#21306;&#22495;&#30340;Transformer&#32593;&#32476;&#65292;&#24182;&#22312;&#27169;&#22411;&#23610;&#23544;&#19978;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#30002;&#28919;&#26816;&#27979;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.02767</link><description>&lt;p&gt;
MethaneMapper: &#20809;&#35889;&#21560;&#25910;&#24863;&#30693;&#39640;&#20809;&#35889;&#36716;&#25442;&#22120;&#29992;&#20110;&#30002;&#28919;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
MethaneMapper: Spectral Absorption aware Hyperspectral Transformer for Methane Detection. (arXiv:2304.02767v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02767
&lt;/p&gt;
&lt;p&gt;
MethaneMapper&#26159;&#19968;&#20010;&#21487;&#29992;&#20110;&#20809;&#35889;&#22495;&#20869;&#23450;&#20301;&#30002;&#28919;&#25490;&#25918;&#21306;&#22495;&#30340;Transformer&#32593;&#32476;&#65292;&#24182;&#22312;&#27169;&#22411;&#23610;&#23544;&#19978;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#30002;&#28919;&#26816;&#27979;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30002;&#28919;(CH4)&#26159;&#20840;&#29699;&#27668;&#20505;&#21464;&#21270;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#31471;&#21040;&#31471;&#30340;&#20809;&#35889;&#21560;&#25910;&#27874;&#38271;&#24863;&#30693;Transformer&#32593;&#32476;MethaneMapper&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#23450;&#37327;&#25490;&#25918;&#12290;MethaneMapper&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#27169;&#22359;&#65292;&#24110;&#21161;&#22312;&#20809;&#35889;&#22495;&#20013;&#23450;&#20301;&#26368;&#30456;&#20851;&#30340;&#30002;&#28919;&#20113;&#21306;&#22495;&#65292;&#24182;&#29992;&#20110;&#20934;&#30830;&#22320;&#23450;&#20301;&#23427;&#20204;&#12290;&#20805;&#20998;&#30340;&#35780;&#20272;&#34920;&#26126;MethaneMapper&#22312;&#26816;&#27979;&#26041;&#38754;&#36798;&#21040;&#20102;0.63 mAP&#65292;&#24182;&#22312;&#27169;&#22411;&#23610;&#23544;&#19978;&#65288;&#32553;&#23567;5&#20493;&#65289;&#19982;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30456;&#27604;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#30002;&#28919;&#20113;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;1000&#24352;AVIRIS-NG&#22270;&#20687;&#20197;&#21450;&#23427;&#20204;&#30340;&#30495;&#23454;&#19990;&#30028;&#22320;&#29702;&#21442;&#32771;&#25968;&#25454;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#35299;&#20915;&#22312;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#27874;&#38271;&#19979;&#30340;&#30002;&#28919;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methane (CH$_4$) is the chief contributor to global climate change. Recent Airborne Visible-Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) has been very useful in quantitative mapping of methane emissions. Existing methods for analyzing this data are sensitive to local terrain conditions, often require manual inspection from domain experts, prone to significant error and hence are not scalable. To address these challenges, we propose a novel end-to-end spectral absorption wavelength aware transformer network, MethaneMapper, to detect and quantify the emissions. MethaneMapper introduces two novel modules that help to locate the most relevant methane plume regions in the spectral domain and uses them to localize these accurately. Thorough evaluation shows that MethaneMapper achieves 0.63 mAP in detection and reduces the model size (by 5x) compared to the current state of the art. In addition, we also introduce a large-scale dataset of methane plume segmentation mask for over 1
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#21306;&#38388;&#22810;&#38754;&#20307;&#31934;&#30830;&#34920;&#31034;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#20108;&#36827;&#21046;&#21464;&#37327;&#38543;&#32593;&#32476;&#35268;&#27169;&#32447;&#24615;&#22686;&#38271;&#65292;&#23454;&#29992;&#24615;&#24191;&#27867;&#12290;</title><link>http://arxiv.org/abs/2304.02755</link><description>&lt;p&gt;
&#28151;&#21512;&#21306;&#38388;&#22810;&#38754;&#20307;&#31934;&#30830;&#34920;&#31034;ReLU&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hybrid Zonotopes Exactly Represent ReLU Neural Networks. (arXiv:2304.02755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02755
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#21306;&#38388;&#22810;&#38754;&#20307;&#31934;&#30830;&#34920;&#31034;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#20108;&#36827;&#21046;&#21464;&#37327;&#38543;&#32593;&#32476;&#35268;&#27169;&#32447;&#24615;&#22686;&#38271;&#65292;&#23454;&#29992;&#24615;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#28151;&#21512;&#21306;&#38388;&#22810;&#38754;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#31561;&#20215;&#30340;&#34920;&#31034;&#21069;&#39304;&#20840;&#36830;&#25509;ReLU&#28608;&#27963;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35777;&#26126;&#20102;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#22797;&#26434;&#24230;&#31561;&#20110;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#24635;&#25968;&#65292;&#22240;&#27492;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#22823;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#28151;&#21512;&#21306;&#38388;&#22810;&#38754;&#20307;&#20844;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12289;MPC&#38381;&#29615;&#21487;&#36798;&#24615;&#21644;&#39564;&#35777;&#65292;&#20197;&#21450;&#23545;MNIST&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that hybrid zonotopes offer an equivalent representation of feed-forward fully connected neural networks with ReLU activation functions. Our approach demonstrates that the complexity of binary variables is equal to the total number of neurons in the network and hence grows linearly in the size of the network. We demonstrate the utility of the hybrid zonotope formulation through three case studies including nonlinear function approximation, MPC closed-loop reachability and verification, and robustness of classification on the MNIST dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#20154;&#31867;&#21644;GPT-3&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#31283;&#20581;&#40065;&#26834;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26500;&#26356;&#22810;&#21462;&#20915;&#20110;&#20855;&#20307;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.02754</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#32467;&#26500;&#34920;&#29616;&#30340;&#24046;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models. (arXiv:2304.02754v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#20154;&#31867;&#21644;GPT-3&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#31283;&#20581;&#40065;&#26834;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26500;&#26356;&#22810;&#21462;&#20915;&#20110;&#20855;&#20307;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#20197;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#19968;&#30452;&#34987;&#29992;&#20316;&#30740;&#31350;&#24515;&#29702;&#21644;&#33041;&#37096;&#27010;&#24565;&#34920;&#24449;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#20195;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#20960;&#20046;&#30456;&#21516;&#30340;&#26041;&#27861;&#26469;&#25506;&#35752;&#27010;&#24565;&#34920;&#24449;&#30340;&#28508;&#22312;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#32463;&#20856;&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;&#25216;&#26415;&#26469;&#20272;&#31639;&#21644;&#27604;&#36739;&#20154;&#31867;&#21644;&#19968;&#20010;&#33879;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-3&#30340;DaVinci&#21464;&#20307;&#65289;&#30340;&#35789;&#27719;&#35821;&#20041;&#32467;&#26500;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#30340;&#27010;&#24565;&#32467;&#26500;&#24378;&#22823;&#19988;&#40065;&#26834;&#65292;&#19981;&#21463;&#25991;&#21270;&#12289;&#35821;&#35328;&#21644;&#20272;&#31639;&#26041;&#27861;&#30340;&#24046;&#24322;&#24433;&#21709;&#65307;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26524;&#30456;&#23545;&#31283;&#23450;&#65292;&#20294;&#20855;&#20307;&#21462;&#20915;&#20110;&#20219;&#21153;&#26412;&#36523;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#20272;&#31639;&#32467;&#26524;&#21487;&#38752;&#65292;&#20294;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#35748;&#30693;&#22788;&#29702;&#30456;&#20851;&#25512;&#26029;&#26102;&#65292;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language AIs, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly used with human participants. The current work uses two common techniques borrowed from cognitive psychology to estimate and compare lexical-semantic structure in both humans and a well-known AI, the DaVinci variant of GPT-3. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from AI behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20197;&#23569;&#37327;&#25968;&#25454;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#20551;&#35780;&#35770;&#21644;&#30495;&#23454;&#35780;&#35770;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;BanglaBERT&#19982;&#21322;&#30417;&#30563;GAN&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;83.59&#65285;&#65292;f1&#20998;&#25968;&#36798;&#21040;84.89&#65285;&#12290;</title><link>http://arxiv.org/abs/2304.02739</link><description>&lt;p&gt;
&#20351;&#29992;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#20551;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Bengali Fake Review Detection using Semi-supervised Generative Adversarial Networks. (arXiv:2304.02739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20197;&#23569;&#37327;&#25968;&#25454;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#20551;&#35780;&#35770;&#21644;&#30495;&#23454;&#35780;&#35770;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;BanglaBERT&#19982;&#21322;&#30417;&#30563;GAN&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;83.59&#65285;&#65292;f1&#20998;&#25968;&#36798;&#21040;84.89&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#23569;&#37327;&#24050;&#27880;&#37322;&#25968;&#25454;&#26469;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#20551;&#35780;&#35770;&#21644;&#30495;&#23454;&#35780;&#35770;&#30340;&#28508;&#21147;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#21644;&#30005;&#23376;&#21830;&#21153;&#30340;&#20852;&#36215;&#65292;&#33021;&#22815;&#26816;&#27979;&#34394;&#20551;&#25110;&#27450;&#39575;&#24615;&#35780;&#35770;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#20445;&#25252;&#28040;&#36153;&#32773;&#20813;&#21463;&#34394;&#20551;&#20449;&#24687;&#30340;&#35823;&#23548;&#12290;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35782;&#21035;&#20551;&#35780;&#35770;&#26041;&#38754;&#37117;&#20250;&#36935;&#21040;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20687;&#23391;&#21152;&#25289;&#35821;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#21322;&#30417;&#30563;GAN-LM&#20307;&#31995;&#32467;&#26500;&#65288;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65289;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;1024&#20010;&#24050;&#27880;&#37322;&#30340;&#26679;&#26412;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;GAN&#30340;BanglaBERT&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;83.59&#65285;&#65292;f1&#20998;&#25968;&#36798;&#21040;84.89&#65285;&#65292;&#20248;&#20110;&#20854;&#20182;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BanglaBERT&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the potential of semi-supervised Generative Adversarial Networks (GANs) to fine-tune pretrained language models in order to classify Bengali fake reviews from real reviews with a few annotated data. With the rise of social media and e-commerce, the ability to detect fake or deceptive reviews is becoming increasingly important in order to protect consumers from being misled by false information. Any machine learning model will have trouble identifying a fake review, especially for a low resource language like Bengali. We have demonstrated that the proposed semi-supervised GAN-LM architecture (generative adversarial network on top of a pretrained language model) is a viable solution in classifying Bengali fake reviews as the experimental results suggest that even with only 1024 annotated samples, BanglaBERT with semi-supervised GAN (SSGAN) achieved an accuracy of 83.59% and a f1-score of 84.89% outperforming other pretrained language models BanglaBERT generator,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#65288;CLFs&#65289;&#20026;&#31471;&#21040;&#31471;&#22522;&#20110;&#35270;&#35273;&#30340;&#31574;&#30053;&#37197;&#22791;&#31283;&#23450;&#23646;&#24615;&#65292;&#24182;&#24341;&#20837;CLFs&#20013;&#30340;&#31283;&#23450;&#24615;&#27880;&#24847;&#21147;&#65288;att-CLFs&#65289;&#26469;&#22788;&#29702;&#29615;&#22659;&#21464;&#21270;&#24182;&#25552;&#39640;&#23398;&#20064;&#28789;&#27963;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.02733</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#31471;&#21040;&#31471;&#39550;&#39542;&#31574;&#30053;&#30340;&#23398;&#20064;&#31283;&#23450;&#24615;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning Stability Attention in Vision-based End-to-end Driving Policies. (arXiv:2304.02733v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02733
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#65288;CLFs&#65289;&#20026;&#31471;&#21040;&#31471;&#22522;&#20110;&#35270;&#35273;&#30340;&#31574;&#30053;&#37197;&#22791;&#31283;&#23450;&#23646;&#24615;&#65292;&#24182;&#24341;&#20837;CLFs&#20013;&#30340;&#31283;&#23450;&#24615;&#27880;&#24847;&#21147;&#65288;att-CLFs&#65289;&#26469;&#22788;&#29702;&#29615;&#22659;&#21464;&#21270;&#24182;&#25552;&#39640;&#23398;&#20064;&#28789;&#27963;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31471;&#21040;&#31471;&#23398;&#20064;&#31995;&#32479;&#21487;&#20197;&#20174;&#24863;&#30693;&#20013;&#23398;&#20064;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#31995;&#32479;&#24448;&#24448;&#25509;&#25910;&#21040;&#26410;&#32463;&#32467;&#26500;&#21270;&#12289;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#30340;&#35266;&#27979;&#31354;&#38388;&#65292;&#22914;&#20174;&#20687;&#32032;&#36755;&#20837;&#27969;&#20013;&#36827;&#34892;&#33258;&#20027;&#39550;&#39542;&#65292;&#22240;&#27492;&#20445;&#35777;&#36825;&#20123;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#24456;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#65288;CLFs&#65289;&#20026;&#31471;&#21040;&#31471;&#22522;&#20110;&#35270;&#35273;&#30340;&#31574;&#30053;&#37197;&#22791;&#31283;&#23450;&#23646;&#24615;&#65292;&#24182;&#24341;&#20837;CLFs&#20013;&#30340;&#31283;&#23450;&#24615;&#27880;&#24847;&#21147;&#65288;att-CLFs&#65289;&#26469;&#22788;&#29702;&#29615;&#22659;&#21464;&#21270;&#24182;&#25552;&#39640;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#23494;&#38598;&#25104;&#21040;att-CLFs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#29615;&#22659;&#21644;&#30495;&#23454;&#20840;&#23610;&#23544;&#33258;&#20027;&#36710;&#36742;&#19978;&#65292;&#36890;&#36807;&#19982;&#32463;&#20856;CLFs&#12289;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#39321;&#33609;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;att-CLFs&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern end-to-end learning systems can learn to explicitly infer control from perception. However, it is difficult to guarantee stability and robustness for these systems since they are often exposed to unstructured, high-dimensional, and complex observation spaces (e.g., autonomous driving from a stream of pixel inputs). We propose to leverage control Lyapunov functions (CLFs) to equip end-to-end vision-based policies with stability properties and introduce stability attention in CLFs (att-CLFs) to tackle environmental changes and improve learning flexibility. We also present an uncertainty propagation technique that is tightly integrated into att-CLFs. We demonstrate the effectiveness of att-CLFs via comparison with classical CLFs, model predictive control, and vanilla end-to-end learning in a photo-realistic simulator and on a real full-scale autonomous vehicle.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27185;&#26691;&#25361;&#36873;&#29702;&#35770;&#26694;&#26550;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#33021;&#22815;&#25429;&#25417;&#36755;&#20837;&#26641;&#32467;&#26500;&#20449;&#24687;&#24182;&#25351;&#23548;&#31639;&#27861;&#20135;&#29983;&#26356;&#22909;&#35299;&#20915;&#26041;&#26696;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#30001;&#20108;&#21449;&#26641;&#32452;&#25104;&#30340;&#23454;&#38469;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31995;&#32479;&#21457;&#32946;&#32593;&#32476;&#26500;&#24314;&#65292;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02729</link><description>&lt;p&gt;
&#21033;&#29992;&#27185;&#26691;&#25361;&#36873;&#21644;&#26426;&#22120;&#23398;&#20064;&#26500;&#24314;&#31995;&#32479;&#21457;&#32946;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Constructing Phylogenetic Networks via Cherry Picking and Machine Learning. (arXiv:2304.02729v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27185;&#26691;&#25361;&#36873;&#29702;&#35770;&#26694;&#26550;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#33021;&#22815;&#25429;&#25417;&#36755;&#20837;&#26641;&#32467;&#26500;&#20449;&#24687;&#24182;&#25351;&#23548;&#31639;&#27861;&#20135;&#29983;&#26356;&#22909;&#35299;&#20915;&#26041;&#26696;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#30001;&#20108;&#21449;&#26641;&#32452;&#25104;&#30340;&#23454;&#38469;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31995;&#32479;&#21457;&#32946;&#32593;&#32476;&#26500;&#24314;&#65292;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#32452;&#31995;&#32479;&#21457;&#32946;&#26641;&#21512;&#24182;&#20026;&#19968;&#20010;&#35299;&#37322;&#23427;&#20204;&#30340;&#31995;&#32479;&#21457;&#32946;&#32593;&#32476;&#26159;&#28436;&#21270;&#30740;&#31350;&#20013;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#21482;&#33021;&#22788;&#29702;&#23569;&#37327;&#30340;&#31995;&#32479;&#21457;&#32946;&#26641;&#65292;&#25110;&#20165;&#38480;&#20110;&#20005;&#26684;&#21463;&#38480;&#30340;&#32593;&#32476;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#26368;&#36817;&#24341;&#20837;&#30340;&#27185;&#26691;&#25361;&#36873;&#29702;&#35770;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#19968;&#31867;&#26377;&#25928;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20445;&#35777;&#29983;&#25104;&#19968;&#20010;&#21253;&#21547;&#36755;&#20837;&#26641;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#30001;&#20108;&#21449;&#26641;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#30340;&#19968;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#22522;&#20110;&#35774;&#35745;&#21644;&#35757;&#32451;&#33021;&#22815;&#25429;&#25417;&#36755;&#20837;&#26641;&#32467;&#26500;&#37325;&#35201;&#20449;&#24687;&#24182;&#25351;&#23548;&#31639;&#27861;&#20135;&#29983;&#26356;&#22909;&#35299;&#20915;&#26041;&#26696;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#31616;&#21333;&#24555;&#36895;&#30340;&#38543;&#26426;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#35777;&#26126;&#22312;&#22810;&#27425;&#36816;&#34892;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#19982;&#29616;&#26377;&#30340;&#31934;&#30830;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#36866;&#29992;&#20110;&#23454;&#38469;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22788;&#29702;&#24191;&#27867;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#30340;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining a set of phylogenetic trees into a single phylogenetic network that explains all of them is a fundamental challenge in evolutionary studies. Existing methods are computationally expensive and can either handle only small numbers of phylogenetic trees or are limited to severely restricted classes of networks. In this paper, we apply the recently-introduced theoretical framework of cherry picking to design a class of efficient heuristics that are guaranteed to produce a network containing each of the input trees, for datasets consisting of binary trees. Some of the heuristics in this framework are based on the design and training of a machine learning model that captures essential information on the structure of the input trees and guides the algorithms towards better solutions. We also propose simple and fast randomised heuristics that prove to be very effective when run multiple times.  Unlike the existing exact methods, our heuristics are applicable to datasets of practical 
&lt;/p&gt;</description></item><item><title>FMG-Net&#21644;W-Net&#26159;&#20004;&#31181;&#21463;&#22810;&#37325;&#32593;&#26684;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38754;&#20020;&#30340;&#32454;&#33410;&#29305;&#24449;&#21644;&#23610;&#24230;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#33021;&#22815;&#25552;&#39640;&#32959;&#30244;&#20998;&#21106;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.02725</link><description>&lt;p&gt;
FMG-Net&#21644;W-Net&#65306;&#21463;&#22810;&#37325;&#32593;&#26684;&#21551;&#21457;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For Medical Imaging Segmentation. (arXiv:2304.02725v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02725
&lt;/p&gt;
&lt;p&gt;
FMG-Net&#21644;W-Net&#26159;&#20004;&#31181;&#21463;&#22810;&#37325;&#32593;&#26684;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38754;&#20020;&#30340;&#32454;&#33410;&#29305;&#24449;&#21644;&#23610;&#24230;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#33021;&#22815;&#25552;&#39640;&#32959;&#30244;&#20998;&#21106;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#23545;&#20110;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#21307;&#30103;&#24178;&#39044;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20173;&#38754;&#20020;&#30528;&#22788;&#29702;&#32454;&#31890;&#24230;&#29305;&#24449;&#21644;&#22270;&#20687;&#23610;&#24230;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#22312;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#21106;&#20219;&#21153;&#20013;&#29305;&#21035;&#26126;&#26174;&#65292;&#20363;&#22914;BraTS&#22810;&#26631;&#31614;&#33041;&#32959;&#30244;&#20998;&#21106;&#25361;&#25112;&#36187;&#20013;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#31934;&#30830;&#22320;&#20998;&#21106;&#19981;&#21516;&#30340;&#32959;&#30244;&#20122;&#32452;&#20998;&#65292;&#22312;&#22823;&#23567;&#21644;&#24418;&#29366;&#19978;&#37117;&#26377;&#26174;&#33879;&#21464;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20063;&#20250;&#20135;&#29983;&#37325;&#22823;&#38169;&#35823;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26550;&#26500;&#65292;FMG-Net&#21644;W-Net&#65292;&#23427;&#20204;&#23558;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#20960;&#20309;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#30340;&#21407;&#29702;&#32435;&#20837;CNN&#20013;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;BraTS 2020&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FMG-Net&#21644;W-Net&#37117;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;U-Net&#26550;&#26500;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#21106;&#20013;&#30340;tum&#30340;&#31934;&#24230;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate medical imaging segmentation is critical for precise and effective medical interventions. However, despite the success of convolutional neural networks (CNNs) in medical image segmentation, they still face challenges in handling fine-scale features and variations in image scales. These challenges are particularly evident in complex and challenging segmentation tasks, such as the BraTS multi-label brain tumor segmentation challenge. In this task, accurately segmenting the various tumor sub-components, which vary significantly in size and shape, remains a significant challenge, with even state-of-the-art methods producing substantial errors. Therefore, we propose two architectures, FMG-Net and W-Net, that incorporate the principles of geometric multigrid methods for solving linear systems of equations into CNNs to address these challenges. Our experiments on the BraTS 2020 dataset demonstrate that both FMG-Net and W-Net outperform the widely used U-Net architecture regarding tum
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;M-&#27169;&#24335;&#32954;&#37096;&#36229;&#22768;&#22270;&#20687;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#32570;&#22833;&#32954;&#28369;&#21160;&#26816;&#27979;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#21644;&#22806;&#37096;&#39564;&#35777;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02724</link><description>&lt;p&gt;
&#25506;&#32034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;M-&#27169;&#24335;&#32954;&#37096;&#36229;&#22768;&#27874;&#20013;&#26816;&#27979;&#32570;&#22833;&#32954;&#28369;&#21160;&#24615;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the Utility of Self-Supervised Pretraining Strategies for the Detection of Absent Lung Sliding in M-Mode Lung Ultrasound. (arXiv:2304.02724v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;M-&#27169;&#24335;&#32954;&#37096;&#36229;&#22768;&#22270;&#20687;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#32570;&#22833;&#32954;&#28369;&#21160;&#26816;&#27979;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#21644;&#22806;&#37096;&#39564;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#23454;&#21487;&#20197;&#25552;&#39640;&#21307;&#23398;&#24433;&#20687;&#20013;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#20043;&#21069;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#23454;&#29992;&#24615;&#65292;&#29992;&#20110;M-&#27169;&#24335;&#32954;&#37096;&#36229;&#22768;&#27874;&#22270;&#20687;&#20013;&#30340;&#32954;&#28369;&#21160;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37197;&#23545;&#20851;&#31995;&#65292;&#23558;&#20174;&#21516;&#19968;B-&#27169;&#24335;&#22270;&#20687;&#26500;&#24314;&#30340;M-&#27169;&#24335;&#22270;&#20687;&#36827;&#34892;&#20102;&#37197;&#23545;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#23450;&#20110;M-&#27169;&#24335;&#32954;&#37096;&#36229;&#22768;&#27874;&#30340;&#25968;&#25454;&#22686;&#24378;&#31243;&#24207;&#30340;&#23454;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27604;&#23436;&#20840;&#30417;&#30563;&#26356;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26410;&#20351;&#29992;ImageNet&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21253;&#25324;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#20250;&#23548;&#33268;&#22312;&#22806;&#37096;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#24615;&#33021;&#65292;&#20984;&#26174;&#20102;&#33258;&#30417;&#30563;&#24615;&#30340;&#20215;&#20540;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#36229;&#22768;&#35299;&#37322;&#30340;&#36890;&#29992;&#24615;&#12290;&#20316;&#32773;&#26368;&#22909;&#30340;&#30693;&#35782;&#65292;&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#25506;&#32034;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#35299;&#20915;M-&#27169;&#24335;&#32954;&#37096;&#36229;&#22768;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pretraining has been observed to improve performance in supervised learning tasks in medical imaging. This study investigates the utility of self-supervised pretraining prior to conducting supervised fine-tuning for the downstream task of lung sliding classification in M-mode lung ultrasound images. We propose a novel pairwise relationship that couples M-mode images constructed from the same B-mode image and investigate the utility of data augmentation procedure specific to M-mode lung ultrasound. The results indicate that self-supervised pretraining yields better performance than full supervision, most notably for feature extractors not initialized with ImageNet-pretrained weights. Moreover, we observe that including a vast volume of unlabelled data results in improved performance on external validation datasets, underscoring the value of self-supervision for improving generalizability in automatic ultrasound interpretation. To the authors' best knowledge, this study i
&lt;/p&gt;</description></item><item><title>SPIRES&#26159;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.02711</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#25552;&#31034;&#35810;&#38382;&#19982;&#36882;&#24402;&#35821;&#20041;&#25552;&#21462;&#65288;SPIRES&#65289;&#65306;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#22635;&#20805;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. (arXiv:2304.02711v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02711
&lt;/p&gt;
&lt;p&gt;
SPIRES&#26159;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#30693;&#35782;&#24211;&#21644;&#26412;&#20307;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#20381;&#36182;&#20110;&#25163;&#21160;&#31649;&#29702;&#12290;AI / NLP&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#19987;&#19994;&#31574;&#23637;&#20154;&#22635;&#20805;&#36825;&#20123;&#30693;&#35782;&#24211;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#33021;&#22635;&#20805;&#20219;&#24847;&#22797;&#26434;&#30340;&#23884;&#22871;&#30693;&#35782;&#27169;&#24335;&#12290;&#22312;&#36825;&#37324;&#25105;&#20204;&#25552;&#20986;&#20102;Structured Prompt Interrogation and Recursive Extraction of Semantics&#65288;SPIRES&#65289;&#65292;&#19968;&#31181;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25191;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#20197;&#21450;&#20174;&#28789;&#27963;&#25552;&#31034;&#36820;&#22238;&#31526;&#21512;&#25351;&#23450;&#27169;&#24335;&#30340;&#20449;&#24687;&#12290; SPIRES&#38024;&#23545;&#32473;&#23450;&#30340;&#35814;&#32454;&#29992;&#25143;&#23450;&#20041;&#30340;&#30693;&#35782;&#27169;&#24335;&#21644;&#36755;&#20837;&#25991;&#26412;&#65292;&#23545;GPT-3+&#25191;&#34892;&#36882;&#24402;&#25552;&#31034;&#35810;&#38382;&#65292;&#20197;&#33719;&#24471;&#19982;&#25552;&#20379;&#30340;&#27169;&#24335;&#21305;&#37197;&#30340;&#19968;&#32452;&#21709;&#24212;&#12290; SPIRES&#20351;&#29992;&#29616;&#26377;&#30340;&#26412;&#20307;&#21644;&#35789;&#27719;&#34920;&#20026;&#25152;&#26377;&#21305;&#37197;&#20803;&#32032;&#25552;&#20379;&#26631;&#35782;&#31526;&#12290; &#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#65288;&#21253;&#25324;&#38899;&#20048;&#65292;&#20307;&#32946;&#21644;&#25919;&#27835;&#65289;&#20013;&#20351;&#29992;SPIRES&#30340;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#20854;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating knowledge bases and ontologies is a time consuming task that relies on a manual curation. AI/NLP approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrary complex nested knowledge schemas.  Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, SPIRES recursively performs prompt interrogation against GPT-3+ to obtain a set of responses matching the provided schema. SPIRES uses existing ontologies and vocabularies to provide identifiers for all matched elements.  We present examples of use of SPIRES in different domains, inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#20559;&#12289;&#39640;&#25928;&#12289;&#36866;&#24403;&#30340;&#21333;&#35843;&#24067;&#23572;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20551;&#35774;&#30340;&#22823;&#23567;&#21644;&#35780;&#20272;&#26102;&#38388;&#37117;&#20026;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#65292;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02700</link><description>&lt;p&gt;
&#26080;&#20559;&#20851;&#20110;&#22369;&#24230;&#20989;&#25968;&#30340;&#36866;&#24403;&#23398;&#20064;&#65306;&#36234;&#36807;&#40657;&#30418;&#20462;&#27491;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Agnostic proper learning of monotone functions: beyond the black-box correction barrier. (arXiv:2304.02700v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#20559;&#12289;&#39640;&#25928;&#12289;&#36866;&#24403;&#30340;&#21333;&#35843;&#24067;&#23572;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20551;&#35774;&#30340;&#22823;&#23567;&#21644;&#35780;&#20272;&#26102;&#38388;&#37117;&#20026;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#65292;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#20559;&#12289;&#39640;&#25928;&#12289;&#36866;&#24403;&#30340;&#21333;&#35843;&#24067;&#23572;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#12290;&#32473;&#23450;&#26410;&#30693;&#20989;&#25968;$f:\{\pm 1\}^n \rightarrow \{\pm 1\}$&#30340;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#20010;&#22343;&#21248;&#38543;&#26426;&#26679;&#26412;&#65292;&#31639;&#27861;&#36755;&#20986;&#19968;&#20010;&#20551;&#35774;$g:\{\pm 1\}^n \rightarrow \{\pm 1\}$&#65292;&#35813;&#20551;&#35774;&#26159;&#21333;&#35843;&#30340;&#65292;&#24182;&#19988;&#19982;$f$&#30340;&#36317;&#31163;&#20026;$(\mathrm{opt} + \varepsilon)$&#65292;&#20854;&#20013;$\mathrm{opt}$&#26159;$f$&#19982;&#26368;&#36817;&#21333;&#35843;&#20989;&#25968;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#65288;&#22240;&#27492;&#20063;&#26159;&#20551;&#35774;&#30340;&#22823;&#23567;&#21644;&#35780;&#20272;&#26102;&#38388;&#65289;&#20063;&#26159;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#65292;&#20960;&#20046;&#19982;Blais&#31561;&#20154;&#65288;RANDOM '15&#65289;&#30340;&#19979;&#30028;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#26410;&#30693;&#20989;&#25968;$f$&#21040;&#21333;&#35843;&#24615;&#30340;&#28155;&#21152;&#35823;&#24046;$\varepsilon$&#30340;&#36317;&#31163;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#12290;&#20197;&#21069;&#65292;&#38024;&#23545;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24050;&#30693;&#26377;&#26679;&#26412;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#24182;&#19981;&#26159;&#36816;&#34892;&#26102;&#38388;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give the first agnostic, efficient, proper learning algorithm for monotone Boolean functions. Given $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$ uniformly random examples of an unknown function $f:\{\pm 1\}^n \rightarrow \{\pm 1\}$, our algorithm outputs a hypothesis $g:\{\pm 1\}^n \rightarrow \{\pm 1\}$ that is monotone and $(\mathrm{opt} + \varepsilon)$-close to $f$, where $\mathrm{opt}$ is the distance from $f$ to the closest monotone function. The running time of the algorithm (and consequently the size and evaluation time of the hypothesis) is also $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$, nearly matching the lower bound of Blais et al (RANDOM '15). We also give an algorithm for estimating up to additive error $\varepsilon$ the distance of an unknown function $f$ to monotone using a run-time of $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$. Previously, for both of these problems, sample-efficient algorithms were known, but these algorithms were not run-time efficient. Our work thus closes this g
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#21333;&#32454;&#32990;&#20998;&#26512;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#12289;&#20934;&#30830;&#22320;&#36827;&#34892;&#32454;&#32990;&#31867;&#22411;&#27880;&#37322;&#65292;&#25581;&#31034;&#20197;&#21069;&#34987;&#24573;&#35270;&#30340;&#32454;&#32990;&#20122;&#22411;&#30340;&#29305;&#23450;&#20998;&#21270;&#36712;&#36857;&#65292;&#23545;&#30284;&#30151;&#12289;&#21457;&#32946;&#21644;&#24178;&#32454;&#32990;&#20998;&#21270;&#30340;&#29702;&#35299;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.02697</link><description>&lt;p&gt;
&#38761;&#21629;&#24615;&#30340;&#21333;&#32454;&#32990;&#20998;&#26512;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#32990;&#31867;&#22411;&#27880;&#37322;&#20013;&#30340;&#23041;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Single Cell Analysis: The Power of Large Language Models for Cell Type Annotation. (arXiv:2304.02697v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02697
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#21333;&#32454;&#32990;&#20998;&#26512;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#12289;&#20934;&#30830;&#22320;&#36827;&#34892;&#32454;&#32990;&#31867;&#22411;&#27880;&#37322;&#65292;&#25581;&#31034;&#20197;&#21069;&#34987;&#24573;&#35270;&#30340;&#32454;&#32990;&#20122;&#22411;&#30340;&#29305;&#23450;&#20998;&#21270;&#36712;&#36857;&#65292;&#23545;&#30284;&#30151;&#12289;&#21457;&#32946;&#21644;&#24178;&#32454;&#32990;&#20998;&#21270;&#30340;&#29702;&#35299;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#24050;&#25104;&#20026;&#30740;&#31350;&#32454;&#32990;&#22810;&#26679;&#24615;&#21644;&#21151;&#33021;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#31934;&#30830;&#22320;&#20174;&#21333;&#32454;&#32990;&#25968;&#25454;&#20013;&#27880;&#37322;&#32454;&#32990;&#31867;&#22411;&#19968;&#30452;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23545;&#32454;&#32990;&#29983;&#29289;&#23398;&#21644;&#22522;&#22240;&#21151;&#33021;&#26377;&#24191;&#27867;&#30340;&#35748;&#35782;&#12290;2023&#24180;ChatGPT&#21644;New Bing&#31561;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#36825;&#20010;&#36807;&#31243;&#65292;&#23427;&#20204;&#23558;&#31185;&#23398;&#25991;&#29486;&#25972;&#21512;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#32454;&#32990;&#31867;&#22411;&#27880;&#37322;&#12290;&#36825;&#19968;&#31361;&#30772;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#26377;&#25928;&#12289;&#20934;&#30830;&#22320;&#36827;&#34892;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#26377;&#21487;&#33021;&#25581;&#31034;&#32454;&#32990;&#31867;&#22411;&#27880;&#37322;&#20013;&#30340;&#26032;&#35265;&#35299;&#12290;&#36890;&#36807;&#20351;&#29992;ChatGPT&#26469;&#27880;&#37322;&#21333;&#32454;&#32990;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#32597;&#35265;&#30340;&#32454;&#32990;&#31867;&#22411;&#19982;&#20854;&#21151;&#33021;&#32852;&#31995;&#36215;&#26469;&#65292;&#25581;&#31034;&#20197;&#21069;&#34987;&#24573;&#35270;&#30340;&#32454;&#32990;&#20122;&#22411;&#30340;&#29305;&#23450;&#20998;&#21270;&#36712;&#36857;&#12290;&#36825;&#21487;&#20197;&#22312;&#29702;&#35299;&#30284;&#30151;&#36827;&#23637;&#12289;&#21754;&#20083;&#21160;&#29289;&#21457;&#32946;&#21644;&#24178;&#32454;&#32990;&#20998;&#21270;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#65292;&#24182;&#26377;&#21487;&#33021;&#20419;&#36827;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, single cell RNA sequencing has become a widely used technique to study cellular diversity and function. However, accurately annotating cell types from single cell data has been a challenging task, as it requires extensive knowledge of cell biology and gene function. The emergence of large language models such as ChatGPT and New Bing in 2023 has revolutionized this process by integrating the scientific literature and providing accurate annotations of cell types. This breakthrough enables researchers to conduct literature reviews more efficiently and accurately, and can potentially uncover new insights into cell type annotation. By using ChatGPT to annotate single cell data, we can relate rare cell type to their function and reveal specific differentiation trajectories of cell subtypes that were previously overlooked. This can have important applications in understanding cancer progression, mammalian development, and stem cell differentiation, and can potentially lead to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#35777;&#21322;&#24452;&#30340;&#25915;&#20987;&#26694;&#26550;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#35813;&#25915;&#20987;&#26694;&#26550;&#20027;&#35201;&#36890;&#36807;&#24341;&#23548;&#25628;&#32034;&#20855;&#26377;&#30456;&#23545;&#36739;&#23567;&#35748;&#35777;&#21322;&#24452;&#30340;&#20687;&#32032;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.02693</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#35748;&#35777;&#21322;&#24452;&#30340;&#25915;&#20987;&#26694;&#26550;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Certified Radius-Guided Attack Framework to Image Segmentation Models. (arXiv:2304.02693v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#35777;&#21322;&#24452;&#30340;&#25915;&#20987;&#26694;&#26550;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#35813;&#25915;&#20987;&#26694;&#26550;&#20027;&#35201;&#36890;&#36807;&#24341;&#23548;&#25628;&#32034;&#20855;&#26377;&#30456;&#23545;&#36739;&#23567;&#35748;&#35777;&#21322;&#24452;&#30340;&#20687;&#32032;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#21106;&#26159;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#26131;&#21463;&#23545;&#25239;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#32780;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#20027;&#35201;&#26159;&#25915;&#20987;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#22270;&#20687;&#20998;&#21106;&#21644;&#20998;&#31867;&#26377;&#26412;&#36136;&#21306;&#21035;&#65292;&#24182;&#20026;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#35774;&#35745;&#20102;&#19968;&#31181;&#25915;&#20987;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#26694;&#26550;&#21463;&#35748;&#35777;&#21322;&#24452;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#26368;&#21021;&#26159;&#30001;&#38450;&#24481;&#32773;&#29992;&#26469;&#38450;&#24481;&#20998;&#31867;&#27169;&#22411;&#30340;&#23545;&#25239;&#25200;&#21160;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25915;&#20987;&#32773;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#21033;&#29992;&#35748;&#35777;&#21322;&#24452;&#30340;&#24615;&#36136;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#25915;&#20987;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#38024;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#30340;&#35748;&#35777;&#26041;&#27861;&#38543;&#26426;&#24179;&#28369;&#27861;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#23548;&#20986;&#20687;&#32032;&#30340;&#35748;&#35777;&#21322;&#24452;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26356;&#21152;&#20851;&#27880;&#20855;&#26377;&#30456;&#23545;&#36739;&#23567;&#35748;&#35777;&#21322;&#24452;&#30340;&#20687;&#32032;&#30340;&#30772;&#22351;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#25628;&#32034;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32473;&#23450;&#36317;&#31163;&#38480;&#21046;&#19979;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#22823;&#21270;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24378;&#20581;&#25200;&#21160;&#12290;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#26694;&#26550;&#23545;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image segmentation is an important problem in many safety-critical applications. Recent studies show that modern image segmentation models are vulnerable to adversarial perturbations, while existing attack methods mainly follow the idea of attacking image classification models. We argue that image segmentation and classification have inherent differences, and design an attack framework specially for image segmentation models. Our attack framework is inspired by certified radius, which was originally used by defenders to defend against adversarial perturbations to classification models. We are the first, from the attacker perspective, to leverage the properties of certified radius and propose a certified radius guided attack framework against image segmentation models. Specifically, we first adapt randomized smoothing, the state-of-the-art certification method for classification models, to derive the pixel's certified radius. We then focus more on disrupting pixels with relatively small
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2304.02689</link><description>&lt;p&gt;
ACTION++&#65306;&#20351;&#29992;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#24230;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25968;&#25454;&#36890;&#24120;&#34920;&#29616;&#20026;&#38271;&#23614;&#20998;&#24067;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#36825;&#33258;&#28982;&#23548;&#33268;&#23569;&#25968;&#31867;&#21035;&#65288;&#21363;&#36793;&#30028;&#21306;&#22495;&#25110;&#32597;&#35265;&#29289;&#20307;&#65289;&#30340;&#20998;&#31867;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#37197;&#22791;&#26080;&#30417;&#30563;&#23545;&#27604;&#26631;&#20934;&#65292;&#22312;&#38271;&#23614;&#22330;&#26223;&#20013;&#26174;&#30528;&#25913;&#36827;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#22312;&#31867;&#21035;&#20998;&#24067;&#20063;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#26631;&#35760;&#25968;&#25454;&#37096;&#20998;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTION++&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;&#20855;&#26377;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes (i.e., boundary regions or rare objects). Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced. In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed cla
&lt;/p&gt;</description></item><item><title>&#39044;&#27979;&#32534;&#30721;&#31639;&#27861;&#34987;&#35748;&#20026;&#26159;&#21453;&#21521;&#20256;&#25773;&#30340;&#19968;&#20010;&#26367;&#20195;&#26041;&#26696;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#23398;&#31995;&#32479;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340; PC &#21464;&#20307;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#30028;&#65292;&#25581;&#31034;&#20102; PC &#30340;&#19968;&#20123;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#20854;&#31070;&#32463;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#21644;&#28508;&#22312;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.02658</link><description>&lt;p&gt;
&#39044;&#27979;&#32534;&#30721;&#20316;&#20026;&#31070;&#32463;&#24418;&#24577;&#23398;&#26367;&#20195;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Predictive Coding as a Neuromorphic Alternative to Backpropagation: A Critical Evaluation. (arXiv:2304.02658v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02658
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32534;&#30721;&#31639;&#27861;&#34987;&#35748;&#20026;&#26159;&#21453;&#21521;&#20256;&#25773;&#30340;&#19968;&#20010;&#26367;&#20195;&#26041;&#26696;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#23398;&#31995;&#32479;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340; PC &#21464;&#20307;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#30028;&#65292;&#25581;&#31034;&#20102; PC &#30340;&#19968;&#20123;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#20854;&#31070;&#32463;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#21644;&#28508;&#22312;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#24050;&#32463;&#24555;&#36895;&#25104;&#20026;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20027;&#35201;&#23398;&#20998;&#20998;&#37197;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#36215;&#28304;&#20110;&#30340;&#20462;&#25913;&#24418;&#24335;&#30340;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#24050;&#34987;&#34920;&#26126;&#21487;&#20197;&#24471;&#21040;&#19982;&#21453;&#21521;&#20256;&#25773;&#23436;&#20840;&#30456;&#21516;&#25110;&#36817;&#20284;&#30456;&#31561;&#30340;&#21442;&#25968;&#26356;&#26032;&#12290;&#30001;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#26377;&#20154;&#35748;&#20026;PC&#21487;&#20197;&#20316;&#20026;&#21453;&#21521;&#20256;&#25773;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#26377;&#21033;&#30340;&#24615;&#36136;&#65292;&#26377;&#21161;&#20110;&#22312;&#31070;&#32463;&#24418;&#24577;&#23398;&#31995;&#32479;&#20013;&#23454;&#29616;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#19981;&#21516;&#30340;&#24403;&#20195;PC&#21464;&#20307;&#26469;&#25506;&#35752;&#36825;&#20123;&#22768;&#26126;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#36825;&#20123;PC&#21464;&#20307;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#30028;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#20302;&#20110;&#21453;&#21521;&#20256;&#25773;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#36825;&#20123;&#21464;&#20307;&#30340;&#20027;&#35201;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#28041;&#21450;&#31070;&#32463;&#29983;&#29289;&#23398;&#30340;&#21487;&#34892;&#24615;&#21644;&#23427;&#20204;&#30340;&#35299;&#37322;&#65292;&#23588;&#20854;&#26159;&#20174;&#26631;&#20934;PC&#20316;&#20026;&#28508;&#22312;&#27010;&#29575;&#27169;&#22411;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#35282;&#24230;&#26469;&#30475;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;PC&#20316;&#20026;&#31070;&#32463;&#24418;&#24577;&#23398;&#31995;&#32479;&#20013;&#21453;&#21521;&#20256;&#25773;&#26367;&#20195;&#26041;&#26696;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation has rapidly become the workhorse credit assignment algorithm for modern deep learning methods. Recently, modified forms of predictive coding (PC), an algorithm with origins in computational neuroscience, have been shown to result in approximately or exactly equal parameter updates to those under backpropagation. Due to this connection, it has been suggested that PC can act as an alternative to backpropagation with desirable properties that may facilitate implementation in neuromorphic systems. Here, we explore these claims using the different contemporary PC variants proposed in the literature. We obtain time complexity bounds for these PC variants which we show are lower-bounded by backpropagation. We also present key properties of these variants that have implications for neurobiological plausibility and their interpretations, particularly from the perspective of standard PC as a variational Bayes algorithm for latent probabilistic models. Our findings shed new light 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#22312;&#29983;&#29289;&#20998;&#23376;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#23545;&#33647;&#29289;&#21457;&#29616;&#12289;&#34507;&#30333;&#36136;&#34920;&#24449;&#21644;&#29983;&#29289;&#31995;&#32479;&#20998;&#26512;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#35813;&#39046;&#22495;&#30340;&#29616;&#29366;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.02656</link><description>&lt;p&gt;
&#20132;&#20114;&#29983;&#29289;&#20998;&#23376;&#31995;&#32479;&#30340;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning for Interactive Biomolecule Systems. (arXiv:2304.02656v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#22312;&#29983;&#29289;&#20998;&#23376;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#23545;&#33647;&#29289;&#21457;&#29616;&#12289;&#34507;&#30333;&#36136;&#34920;&#24449;&#21644;&#29983;&#29289;&#31995;&#32479;&#20998;&#26512;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#35813;&#39046;&#22495;&#30340;&#29616;&#29366;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#29983;&#29289;&#20998;&#23376;&#31995;&#32479;&#21644;&#23427;&#20204;&#30340;&#26426;&#21046;&#30340;&#30740;&#31350;&#12290;&#20854;&#20013;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#29305;&#21035;&#37325;&#35201;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#19981;&#21516;&#23618;&#27425;&#30340;&#29983;&#29289;&#20998;&#23376;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29992;&#20110;&#23558;&#29983;&#29289;&#20998;&#23376;&#21644;&#31995;&#32479;&#34920;&#31034;&#20026;&#35745;&#31639;&#26426;&#21487;&#35782;&#21035;&#23545;&#35937;&#65288;&#20363;&#22914;&#24207;&#21015;&#12289;&#22270;&#24418;&#21644;&#34920;&#38754;&#65289;&#30340;&#26041;&#27861;&#23398;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#24378;&#35843;&#22522;&#20110;&#22270;&#24418;&#30340;&#25216;&#26415;&#65289;&#22914;&#20309;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#25968;&#25454;&#20197;&#23454;&#29616;&#33647;&#29289;&#21457;&#29616;&#12289;&#34507;&#30333;&#36136;&#34920;&#24449;&#21644;&#29983;&#29289;&#31995;&#32479;&#20998;&#26512;&#12290;&#30740;&#31350;&#26368;&#21518;&#24635;&#32467;&#20102;&#35813;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#20984;&#26174;&#20102;&#23384;&#22312;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in deep learning models have revolutionized the study of biomolecule systems and their mechanisms. Graph representation learning, in particular, is important for accurately capturing the geometric information of biomolecules at different levels. This paper presents a comprehensive review of the methodologies used to represent biological molecules and systems as computer-recognizable objects, such as sequences, graphs, and surfaces. Moreover, it examines how geometric deep learning models, with an emphasis on graph-based techniques, can analyze biomolecule data to enable drug discovery, protein characterization, and biological system analysis. The study concludes with an overview of the current state of the field, highlighting the challenges that exist and the potential future research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiSupervised&#30340;&#26032;&#26550;&#26500;&#65292;&#20351;&#29992;&#20004;&#20010;&#30417;&#30563;&#22120;&#22312;&#35843;&#29992;&#22823;&#35268;&#27169;&#36828;&#31243;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20043;&#21069;&#65292;&#23545;&#23567;&#35268;&#27169;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#32593;&#32476;&#35843;&#29992;&#65292;&#21487;&#20197;&#33410;&#30465;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.02654</link><description>&lt;p&gt;
&#37319;&#29992;&#20004;&#20010;&#30417;&#30563;&#22120;&#29992;&#20110;&#22823;&#35268;&#27169;&#36828;&#31243;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks. (arXiv:2304.02654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiSupervised&#30340;&#26032;&#26550;&#26500;&#65292;&#20351;&#29992;&#20004;&#20010;&#30417;&#30563;&#22120;&#22312;&#35843;&#29992;&#22823;&#35268;&#27169;&#36828;&#31243;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20043;&#21069;&#65292;&#23545;&#23567;&#35268;&#27169;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#32593;&#32476;&#35843;&#29992;&#65292;&#21487;&#20197;&#33410;&#30465;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20986;&#29616;&#22312;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20154;&#31867;&#31454;&#20105;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#36825;&#20123;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#21253;&#21547;&#25968;&#21313;&#20159;&#29978;&#33267;&#25968;&#30334;&#20159;&#21442;&#25968;&#65292;&#22240;&#27492;&#26080;&#27861;&#37096;&#32626;&#21040;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#65288;&#22914;&#25163;&#26426;&#25110;&#29289;&#32852;&#32593;&#24494;&#25511;&#21046;&#22120;&#65289;&#19978;&#25110;&#39640;&#25928;&#36816;&#34892;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31995;&#32479;&#38656;&#35201;&#36890;&#36807;&#32593;&#32476;&#35843;&#29992;&#30456;&#24212;&#30340;&#27169;&#22411;&#65292;&#23548;&#33268;&#25176;&#31649;&#21644;&#36816;&#34892;&#22823;&#35268;&#27169;&#36828;&#31243;&#27169;&#22411;&#30340;&#25104;&#26412;&#30456;&#24403;&#39640;&#65292;&#36825;&#20123;&#25104;&#26412;&#36890;&#24120;&#25353;&#20351;&#29992;&#27425;&#25968;&#25910;&#36153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;BiSupervised&#65292;&#23427;&#22312;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#36828;&#31243;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20043;&#21069;&#65292;&#23581;&#35797;&#22312;&#23567;&#35268;&#27169;&#26412;&#22320;&#27169;&#22411;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19968;&#20010;DNN&#20027;&#31649;&#30417;&#25511;&#27492;&#39044;&#27979;&#36807;&#31243;&#24182;&#30830;&#23450;&#26131;&#20110;&#36827;&#34892;&#26412;&#22320;&#39044;&#27979;&#30340;&#36755;&#20837;&#12290;&#23545;&#20110;&#36825;&#20123;&#36755;&#20837;&#65292;&#26080;&#38656;&#35843;&#29992;&#36828;&#31243;&#27169;&#22411;&#65292;&#20174;&#32780;&#33410;&#30465;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent decades have seen the rise of large-scale Deep Neural Networks (DNNs) to achieve human-competitive performance in a variety of artificial intelligence tasks. Often consisting of hundreds of millions, if not hundreds of billion parameters, these DNNs are too large to be deployed to, or efficiently run on resource-constrained devices such as mobile phones or IoT microcontrollers. Systems relying on large-scale DNNs thus have to call the corresponding model over the network, leading to substantial costs for hosting and running the large-scale remote model, costs which are often charged on a per-use basis. In this paper, we propose BiSupervised, a novel architecture, where, before relying on a large remote DNN, a system attempts to make a prediction on a small-scale local model. A DNN supervisor monitors said prediction process and identifies easy inputs for which the local prediction can be trusted. For these inputs, the remote model does not have to be invoked, thus saving costs, 
&lt;/p&gt;</description></item><item><title>RARE&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#22312;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01507</link><description>&lt;p&gt;
RARE&#65306;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
RARE: Robust Masked Graph Autoencoder. (arXiv:2304.01507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01507
&lt;/p&gt;
&lt;p&gt;
RARE&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#22312;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;MGAE&#65289;&#30001;&#20110;&#20854;&#31616;&#21333;&#21644;&#26377;&#25928;&#30340;&#29305;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#22270;&#39044;&#35757;&#32451;&#65288;SGP&#65289;&#26041;&#38754;&#24050;&#25104;&#20026;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#20013;&#25191;&#34892;&#25513;&#30721;-&#37325;&#26500;&#25805;&#20316;&#65292;&#31867;&#20284;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#32780;&#24573;&#30053;&#20102;&#22270;&#25968;&#25454;&#30340;&#37325;&#35201;&#38750;&#27431;&#20960;&#37324;&#24471;&#23646;&#24615;&#12290;&#32467;&#26524;&#65292;&#39640;&#24230;&#19981;&#31283;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#32467;&#26500;&#22823;&#22823;&#22686;&#21152;&#20102;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#38477;&#20302;&#20102;&#21033;&#29992;&#33258;&#30417;&#30563;&#20449;&#21495;&#30340;&#21487;&#38752;&#24615;&#65292;&#23548;&#33268;&#19979;&#28216;&#35780;&#20272;&#20013;&#30340;&#34920;&#31034;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SGP&#26041;&#27861;&#65292;&#31216;&#20026;Robust mAsked gRaph autoEncoder&#65288;RARE&#65289;&#65292;&#36890;&#36807;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26356;&#22810;&#30340;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;RARE&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked graph autoencoder (MGAE) has emerged as a promising self-supervised graph pre-training (SGP) paradigm due to its simplicity and effectiveness. However, existing efforts perform the mask-then-reconstruct operation in the raw data space as is done in computer vision (CV) and natural language processing (NLP) areas, while neglecting the important non-Euclidean property of graph data. As a result, the highly unstable local connection structures largely increase the uncertainty in inferring masked data and decrease the reliability of the exploited self-supervision signals, leading to inferior representations for downstream evaluations. To address this issue, we propose a novel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve the certainty in inferring masked data and the reliability of the self-supervision mechanism by further masking and reconstructing node samples in the high-order latent feature space. Through both theoretical and empirical analyses, we have dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#21644;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;&#23454;&#29616;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01347</link><description>&lt;p&gt;
&#26102;&#38388;&#21160;&#24577;&#21516;&#27493;&#21151;&#33021;&#33041;&#32593;&#32476;&#22312;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis. (arXiv:2304.01347v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#21644;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;&#23454;&#29616;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#21487;&#20197;&#25429;&#25417;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#20013;&#30340;&#33041;&#27963;&#21160;&#26102;&#21464;&#24322;&#24120;&#65292;&#24182;&#22312;&#25581;&#31034;&#31934;&#31070;&#20998;&#35010;&#30151;&#65288;SZ&#65289;&#24739;&#32773;&#24322;&#24120;&#33041;&#27963;&#21160;&#26426;&#21046;&#26041;&#38754;&#20855;&#26377;&#22825;&#28982;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#21160;&#24577;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22411;&#8212;&#8212;&#26102;&#24577;&#33041;&#31867;&#21035;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;temporal-BCGCN&#65289;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#29420;&#29305;&#30340;&#21160;&#24577;&#33041;&#32593;&#32476;&#20998;&#26512;&#27169;&#22359;DSF-BrainNet&#65292;&#29992;&#20110;&#26500;&#24314;&#21160;&#24577;&#21516;&#27493;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#22270;&#21367;&#31215;&#26041;&#27861;TemporalConv&#65292;&#22522;&#20110;&#29305;&#24449;&#30340;&#21516;&#27493;&#26102;&#38388;&#23646;&#24615;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#21270;&#24322;&#24120;&#21322;&#29699;&#20391;&#21270;&#26816;&#27979;&#24037;&#20855;&#65292;&#31216;&#20026;CategoryPool&#12290;&#35813;&#30740;&#31350;&#22312;COBRE&#21644;UCLA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#20998;&#21035;&#36798;&#21040;83.62&#65285;&#21644;89.71&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#21644;&#20391;&#21270;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Available evidence suggests that dynamic functional connectivity (dFC) can capture time-varying abnormalities in brain activity in rs-fMRI data and has a natural advantage in uncovering mechanisms of abnormal brain activity in schizophrenia(SZ) patients. Hence, an advanced dynamic brain network analysis model called the temporal brain category graph convolutional network (temporal-BCGCN) was employed. Firstly, a unique dynamic brain network analysis module, DSF-BrainNet, was designed to construct dynamic synchronization features. Subsequently, a revolutionary graph convolution method, TemporalConv, was proposed, based on the synchronous temporal properties of feature. Finally, the first modular abnormal hemispherical lateralization test tool in deep learning based on rs-fMRI data, named CategoryPool, was proposed. This study was validated on COBRE and UCLA datasets and achieved 83.62% and 89.71% average accuracy, respectively, outperforming the baseline model and other State-of-the-Art
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35775;&#35848;19&#20301;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#65292;&#21457;&#29616;KG&#26500;&#24314;&#32773;&#38656;&#27714;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#65292;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#65292;KG&#28040;&#36153;&#32773;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#65292;&#24182;&#25351;&#20986;&#22312;&#23454;&#36341;&#20013;&#23454;&#26045;KG&#38656;&#35201;&#25216;&#26415;&#21644;&#31038;&#20132;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.01311</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30693;&#35782;&#22270;&#35889;&#29992;&#25143;&#12289;&#25361;&#25112;&#21644;&#21487;&#35270;&#21270;&#38656;&#27714;&#30340;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice. (arXiv:2304.01311v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35775;&#35848;19&#20301;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#65292;&#21457;&#29616;KG&#26500;&#24314;&#32773;&#38656;&#27714;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#65292;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#65292;KG&#28040;&#36153;&#32773;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#65292;&#24182;&#25351;&#20986;&#22312;&#23454;&#36341;&#20013;&#23454;&#26045;KG&#38656;&#35201;&#25216;&#26415;&#21644;&#31038;&#20132;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;19&#20301;&#26469;&#33258;&#20225;&#19994;&#21644;&#23398;&#26415;&#29615;&#22659;&#19979;&#12289;&#28041;&#21450;&#21508;&#31181;&#29992;&#20363;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#30340;&#35775;&#35848;&#65292;&#25552;&#20986;&#20102;KG&#23454;&#36341;&#32773;&#22312;&#21019;&#24314;&#12289;&#25506;&#32034;&#21644;&#20998;&#26512;KG&#26102;&#36935;&#21040;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#21487;&#35270;&#21270;&#35774;&#35745;&#26469;&#32531;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;KG&#23454;&#36341;&#32773;&#21487;&#20197;&#20998;&#20026;&#19977;&#31867;&#65306;KG&#26500;&#24314;&#32773;&#12289;&#20998;&#26512;&#24072;&#21644;&#28040;&#36153;&#32773;&#65292;&#27599;&#20010;&#20154;&#37117;&#26377;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#38656;&#27714;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;KG&#26500;&#24314;&#32773;&#21487;&#20197;&#20174;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#20013;&#33719;&#30410;&#65292;&#32780;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#25552;&#20379;&#20013;&#38388;&#26597;&#35810;&#32467;&#26524;&#30340;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#12290;&#23545;&#20110;KG&#28040;&#36153;&#32773;&#65292;&#25105;&#20204;&#30830;&#23450;&#33410;&#28857;&#38142;&#25509;&#22270;&#30340;&#25928;&#21147;&#19981;&#36275;&#65292;&#24182;&#38656;&#35201;&#23450;&#21046;&#30340;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#26469;&#20419;&#36827;KG&#30340;&#37319;&#29992;&#21644;&#29702;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23454;&#36341;&#20013;&#26377;&#25928;&#22320;&#23454;&#26045;KG&#38656;&#35201;&#19981;&#20165;&#25216;&#26415;&#19978;&#30340;&#65292;&#36824;&#26377;&#31038;&#20132;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30446;&#21069;&#24182;&#26410;&#34987;&#24403;&#21069;&#30340;&#24037;&#20855;&#12289;&#25216;&#26415;&#21644;&#26368;&#20339;&#23454;&#36341;&#25152;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners - KG Builders, Analysts, and Consumers - each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, tec
&lt;/p&gt;</description></item><item><title>POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01218</link><description>&lt;p&gt;
POLAR-Express: &#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#39640;&#25928;&#20934;&#30830;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems. (arXiv:2304.01218v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01218
&lt;/p&gt;
&lt;p&gt;
POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#38382;&#39064;&#19978;&#65292;&#25198;&#28436;&#25511;&#21046;&#22120;&#35282;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476; (NN) &#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23454;&#39564;&#24615;&#33021;&#12290;&#20294;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479; (NNCS) &#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#37319;&#29992;&#20063;&#24341;&#36215;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#23545;&#36825;&#20123; NNCS &#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; POLAR-Express&#65292;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777; NNCS &#30340;&#23433;&#20840;&#24615;&#12290;POLAR-Express &#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#65292;&#36880;&#23618;&#27178;&#36328;&#31070;&#32463;&#32593;&#32476;&#26469;&#20256;&#25773; Taylor &#27169;&#22411; (TM) &#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#36817;&#20284;&#20540;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#20219;&#20309;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#26356;&#26377;&#25928;&#22320;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;POLAR-Express &#20026;&#36880;&#23618;&#20256;&#25773;&#25552;&#20379;&#20102;&#24182;&#34892;&#35745;&#31639;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) playing the role of controllers have demonstrated impressive empirical performances on challenging control problems. However, the potential adoption of NN controllers in real-life applications also gives rise to a growing concern over the safety of these neural-network controlled systems (NNCSs), especially when used in safety-critical applications. In this work, we present POLAR-Express, an efficient and precise formal reachability analysis tool for verifying the safety of NNCSs. POLAR-Express uses Taylor model arithmetic to propagate Taylor models (TMs) across a neural network layer-by-layer to compute an overapproximation of the neural-network function. It can be applied to analyze any feed-forward neural network with continuous activation functions. We also present a novel approach to propagate TMs more efficiently and precisely across ReLU activation functions. In addition, POLAR-Express provides parallel computation support for the layer-by-layer propagation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;2022&#24180;&#22522;&#22240;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#20030;&#21150;&#30340;&#31454;&#36187;&#65292;&#35780;&#20272;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01117</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#21487;&#35299;&#37322;&#31526;&#21495;&#22238;&#24402;&#65306;2022&#24180;&#31454;&#36187;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition. (arXiv:2304.01117v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;2022&#24180;&#22522;&#22240;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#20030;&#21150;&#30340;&#31454;&#36187;&#65292;&#35780;&#20272;&#20102;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#23547;&#25214;&#33021;&#22815;&#20934;&#30830;&#25551;&#36848;&#30740;&#31350;&#29616;&#35937;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#36820;&#22238;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#32473;&#29992;&#25143;&#25552;&#20379;&#28145;&#21051;&#30340;&#35265;&#35299;&#12290;&#21382;&#21490;&#19978;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#22823;&#22810;&#25968;&#31639;&#27861;&#37117;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#22823;&#37327;&#26032;&#30340;&#25552;&#26696;&#65292;&#36825;&#20123;&#25552;&#26696;&#20351;&#29992;&#20102;&#21015;&#20030;&#31639;&#27861;&#12289;&#28151;&#21512;&#32447;&#24615;&#25972;&#25968;&#35268;&#21010;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#19968;&#32452;&#24120;&#35265;&#25361;&#25112;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#25105;&#20204;&#22312;2022&#24180;&#36951;&#20256;&#19982;&#36827;&#21270;&#35745;&#31639;&#20250;&#35758;&#19978;&#20030;&#21150;&#20102;&#19968;&#27425;&#31454;&#36187;&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#21516;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#21442;&#36187;&#32773;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#30450;&#27979;&#35797;&#30340;&#12290;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39046;&#22495;&#19987;&#23478;&#26469;&#35780;&#20272;&#20505;&#36873;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression searches for analytic expressions that accurately describe studied phenomena. The main attraction of this approach is that it returns an interpretable model that can be insightful to users. Historically, the majority of algorithms for symbolic regression have been based on evolutionary algorithms. However, there has been a recent surge of new proposals that instead utilize approaches such as enumeration algorithms, mixed linear integer programming, neural networks, and Bayesian optimization. In order to assess how well these new approaches behave on a set of common challenges often faced in real-world data, we hosted a competition at the 2022 Genetic and Evolutionary Computation Conference consisting of different synthetic and real-world datasets which were blind to entrants. For the real-world track, we assessed interpretability in a realistic way by using a domain expert to judge the trustworthiness of candidate models.We present an in-depth analysis of the result
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Cityscapes-3D&#30340;&#32852;&#21512;2D-3D&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#23454;&#29616;&#21333;&#30524;3D&#36710;&#36742;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#21333;&#20803;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00971</link><description>&lt;p&gt;
&#22522;&#20110;Cityscapes-3D&#30340;&#32852;&#21512;2D-3D&#22810;&#20219;&#21153;&#23398;&#20064;&#65306;3D&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Joint 2D-3D Multi-Task Learning on Cityscapes-3D: 3D Detection, Segmentation, and Depth Estimation. (arXiv:2304.00971v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Cityscapes-3D&#30340;&#32852;&#21512;2D-3D&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#23454;&#29616;&#21333;&#30524;3D&#36710;&#36742;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#21333;&#20803;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#26159;TaskPrompter&#22312;&#22522;&#20110;Cityscapes-3D&#30340;&#26032;&#32852;&#21512;2D-3D&#22810;&#20219;&#21153;&#23398;&#20064;&#26631;&#20934;&#19978;&#30340;&#23454;&#29616;&#30340;&#34917;&#20805;&#25991;&#26723;&#12290;TaskPrompter&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26694;&#26550;&#65292;&#23558;&#65288;i&#65289;&#20219;&#21153;&#36890;&#29992;&#34920;&#31034;&#12289;&#65288;ii&#65289;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#21644;&#65288;iii&#65289;&#36328;&#20219;&#21153;&#20132;&#20114;&#30340;&#23398;&#20064;&#32479;&#19968;&#36215;&#26469;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#23558;&#36825;&#20123;&#23398;&#20064;&#30446;&#26631;&#20998;&#21035;&#23384;&#25918;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#27169;&#22359;&#20013;&#30456;&#21453;&#12290;&#36825;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#19981;&#20165;&#20943;&#23569;&#20102;&#23545;&#32467;&#26500;&#35774;&#35745;&#30340;&#32454;&#33268;&#32463;&#39564;&#38656;&#27714;&#65292;&#36824;&#26174;&#33879;&#22686;&#24378;&#20102;&#22810;&#20219;&#21153;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#22240;&#20026;&#25972;&#20010;&#27169;&#22411;&#23481;&#37327;&#37117;&#33268;&#21147;&#20110;&#21516;&#26102;&#20248;&#21270;&#36825;&#19977;&#20010;&#30446;&#26631;&#12290;TaskPrompter&#22312;Cityscapes-3D&#25968;&#25454;&#38598;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#35201;&#27714;&#22810;&#20219;&#21153;&#27169;&#22411;&#21516;&#26102;&#20026;&#21333;&#30524;3D&#36710;&#36742;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#29983;&#25104;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report serves as a supplementary document for TaskPrompter, detailing its implementation on a new joint 2D-3D multi-task learning benchmark based on Cityscapes-3D. TaskPrompter presents an innovative multi-task prompting framework that unifies the learning of (i) task-generic representations, (ii) task-specific representations, and (iii) cross-task interactions, as opposed to previous approaches that separate these learning objectives into different network modules. This unified approach not only reduces the need for meticulous empirical structure design but also significantly enhances the multi-task network's representation learning capability, as the entire model capacity is devoted to optimizing the three objectives simultaneously. TaskPrompter introduces a new multi-task benchmark based on Cityscapes-3D dataset, which requires the multi-task model to concurrently generate predictions for monocular 3D vehicle detection, semantic segmentation, and monocular depth estimation. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20808;&#21069;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#36136;&#37327;&#23545;&#25239;&#25200;&#21160;&#30340;&#27491;&#38754;&#20808;&#39564;&#24341;&#23548;&#23545;&#25239;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#36991;&#20813;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00202</link><description>&lt;p&gt;
&#20351;&#29992;&#20808;&#39564;&#24341;&#23548;&#30693;&#35782;&#25913;&#36827;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Improving Fast Adversarial Training with Prior-Guided Knowledge. (arXiv:2304.00202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20808;&#21069;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#36136;&#37327;&#23545;&#25239;&#25200;&#21160;&#30340;&#27491;&#38754;&#20808;&#39564;&#24341;&#23548;&#23545;&#25239;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#36991;&#20813;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#26159;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#22312;&#32463;&#36807;&#20960;&#20010;&#35757;&#32451;&#21608;&#26399;&#21518;&#40065;&#26834;&#24615;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#30340;&#21464;&#20307;&#26469;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#36739;&#39640;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#23545;&#25239;&#35757;&#32451;&#21644;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#30740;&#31350;&#20102;&#23545;&#25239;&#26679;&#26412;&#36136;&#37327;&#21644;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#21464;&#24046;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#36136;&#37327;&#23545;&#25239;&#25200;&#21160;&#30340;&#27491;&#38754;&#20808;&#39564;&#24341;&#23548;&#23545;&#25239;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#36991;&#20813;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#21021;&#22987;&#21270;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast adversarial training (FAT) is an efficient method to improve robustness. However, the original FAT suffers from catastrophic overfitting, which dramatically and suddenly reduces robustness after a few training epochs. Although various FAT variants have been proposed to prevent overfitting, they require high training costs. In this paper, we investigate the relationship between adversarial example quality and catastrophic overfitting by comparing the training processes of standard adversarial training and FAT. We find that catastrophic overfitting occurs when the attack success rate of adversarial examples becomes worse. Based on this observation, we propose a positive prior-guided adversarial initialization to prevent overfitting by improving adversarial example quality without extra training costs. This initialization is generated by using high-quality adversarial perturbations from the historical training process. We provide theoretical analysis for the proposed initialization a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#65292;&#23454;&#29616;&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39640;&#25928;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2303.17907</link><description>&lt;p&gt;
&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39044;&#27979;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#37325;&#23450;&#21521;&#27493;&#34892;
&lt;/p&gt;
&lt;p&gt;
Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking. (arXiv:2303.17907v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#65292;&#23454;&#29616;&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39640;&#25928;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#27491;&#26397;&#30528;&#22686;&#24378;&#27785;&#28024;&#24863;&#12289;&#25903;&#25345;&#22810;&#29992;&#25143;&#20307;&#39564;&#21644;&#22312;&#34394;&#25311;&#20307;&#39564;&#20013;&#25903;&#25345;&#26080;&#38480;&#21046;&#30340;&#31227;&#21160;&#65292;&#32780;&#36890;&#36807;&#37325;&#23450;&#21521;&#27493;&#34892;&#23558;&#29992;&#25143;&#38480;&#21046;&#22312;&#19987;&#38376;&#30340;VR&#35774;&#32622;&#20869;&#12290;&#20026;&#20102;&#28385;&#36275;&#26410;&#26469;VR&#31995;&#32479;&#30340;&#26497;&#31471;&#25968;&#25454;&#36895;&#29575;&#21644;&#24310;&#36831;&#35201;&#27714;&#65292;&#25903;&#25345;&#26080;&#32447;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#23558;&#22312;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#39057;&#29575;&#19978;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#23454;&#29616;&#39640;&#24230;&#23450;&#21521;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#30701;&#26399;&#39044;&#27979;&#22810;&#29992;&#25143;VR&#35774;&#32622;&#20013;&#29992;&#25143;&#30340;&#27178;&#21521;&#31227;&#21160;&#65292;&#21487;&#20197;&#21033;&#29992;&#29992;&#25143;&#26041;&#21521;&#19978;&#30340;&#30452;&#32447;&#35270;&#36317;&#65288;LoS&#65289;&#8220;&#36319;&#36394;&#8221;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual Reality (VR) technology is being advanced along the lines of enhancing its immersiveness, enabling multiuser Virtual Experiences (VEs), and supporting unconstrained mobility of the users in their VEs, while constraining them within specialized VR setups through Redirected Walking (RDW). For meeting the extreme data-rate and latency requirements of future VR systems, supporting wireless networking infrastructures will operate in millimeter Wave (mmWave) frequencies and leverage highly directional communication in both transmission and reception through beamforming and beamsteering. We propose to leverage predictive context-awareness for optimizing transmitter and receiver-side beamforming and beamsteering. In particular, we argue that short-term prediction of users' lateral movements in multiuser VR setups with RDW can be utilized for optimizing transmitter-side beamforming and beamsteering through Line-of-Sight (LoS) "tracking" in the users' directions. At the same time, short-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;12&#31181;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#26579;&#33394;&#36716;&#31227;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#26579;&#33394;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.17009</link><description>&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#26579;&#33394;&#36716;&#31227;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A comparative evaluation of image-to-image translation methods for stain transfer in histopathology. (arXiv:2303.17009v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;12&#31181;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#26579;&#33394;&#36716;&#31227;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#26579;&#33394;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#65288;I2I&#65289;&#26041;&#27861;&#20801;&#35768;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#39118;&#26684;&#20294;&#19982;&#21407;&#22987;&#22270;&#20687;&#20849;&#20139;&#20869;&#23481;&#30340;&#20154;&#24037;&#22270;&#20687;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;I2I&#26041;&#27861;&#23454;&#29616;&#20102;&#29983;&#25104;&#19982;&#33258;&#28982;&#22270;&#20687;&#26080;&#27861;&#21306;&#20998;&#30340;&#20154;&#24037;&#22270;&#20687;&#12290;&#26368;&#36817;&#65292;&#38024;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#19981;&#21516;&#31867;&#22411;&#30340;&#26579;&#33394;&#65292;I2I&#26041;&#27861;&#20063;&#34987;&#29992;&#20110;&#29983;&#25104;&#27169;&#25311;&#30340;&#26579;&#33394;&#32452;&#32455;&#30340;&#20154;&#24037;&#22270;&#20687;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;&#26579;&#33394;&#36716;&#31227;&#12290;&#30001;&#20110;I2I&#21464;&#31181;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#36825;&#20351;&#24471;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;I2I&#26041;&#27861;&#36827;&#34892;&#26579;&#33394;&#36716;&#31227;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;12&#31181;&#26579;&#33394;&#36716;&#31227;&#26041;&#27861;&#65292;&#20854;&#20013;3&#31181;&#22522;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;9&#31181;&#22522;&#20110;GAN&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#12290;&#20998;&#26512;&#20381;&#36182;&#20110;&#23450;&#37327;&#35780;&#20272;&#22270;&#20687;&#32763;&#35793;&#36136;&#37327;&#30340;&#34917;&#20805;&#25514;&#26045;&#65292;&#20197;&#21450;&#23545;&#28145;&#24230;&#23398;&#20064;&#32452;&#32455;&#25104;&#20687;&#21644;&#21307;&#23398;&#35786;&#26029;&#30340;&#36866;&#29992;&#24615;&#30340;&#35780;&#20272;&#65292;&#20197;&#21450;&#32463;&#39564;&#20016;&#23500;&#30340;&#30149;&#29702;&#23398;&#23478;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26377;&#20960;&#31181;I2I&#26041;&#27861;&#20248;&#20110;&#26579;&#33394;&#36716;&#31227;&#39046;&#22495;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#26579;&#33394;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-to-image translation (I2I) methods allow the generation of artificial images that share the content of the original image but have a different style. With the advances in Generative Adversarial Networks (GANs)-based methods, I2I methods enabled the generation of artificial images that are indistinguishable from natural images. Recently, I2I methods were also employed in histopathology for generating artificial images of in silico stained tissues from a different type of staining. We refer to this process as stain transfer. The number of I2I variants is constantly increasing, which makes a well justified choice of the most suitable I2I methods for stain transfer challenging. In our work, we compare twelve stain transfer approaches, three of which are based on traditional and nine on GAN-based image processing methods. The analysis relies on complementary quantitative measures for the quality of image translation, the assessment of the suitability for deep learning-based tissue gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#30340;&#21098;&#26525;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#26082;&#23454;&#29616;&#20102;&#35757;&#32451;&#21644;&#20869;&#23384;&#30340;&#39640;&#25928;&#29575;&#65292;&#21448;&#21152;&#24555;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312; GLUE &#20219;&#21153;&#20013;&#27809;&#26377;&#26174;&#33879;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14704</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#20869;&#23384;&#39640;&#25928;&#21098;&#26525;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Task-oriented Memory-efficient Pruning-Adapter. (arXiv:2303.14704v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#30340;&#21098;&#26525;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#26082;&#23454;&#29616;&#20102;&#35757;&#32451;&#21644;&#20869;&#23384;&#30340;&#39640;&#25928;&#29575;&#65292;&#21448;&#21152;&#24555;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312; GLUE &#20219;&#21153;&#20013;&#27809;&#26377;&#26174;&#33879;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#19981;&#26029;&#22686;&#38271;&#30340;&#35268;&#27169;&#23548;&#33268;&#20102;&#23545;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#30340; increased attention&#12290;&#20027;&#35201;&#30340;&#20004;&#31181;&#26041;&#27861;&#26159;&#36866;&#37197;&#22120;&#21644;&#21098;&#26525;&#12290;&#36866;&#37197;&#22120;&#26159;&#22312;&#27169;&#22411;&#19978;&#20923;&#32467;&#24182;&#32473;&#23427;&#19968;&#20010;&#26032;&#30340;&#26435;&#37325;&#30697;&#38453;&#65292;&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#26041;&#38754;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25104;&#26412;&#65292;&#20294;&#36825;&#26679;&#20250;&#22686;&#21152;&#35780;&#20272;&#21644;&#27979;&#35797;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#21098;&#26525;&#26159;&#25130;&#26029;&#19968;&#20123;&#26435;&#37325;&#24182;&#37325;&#26032;&#20998;&#37197;&#21097;&#20313;&#30340;&#26435;&#37325;&#65292;&#36825;&#26679;&#21487;&#20197;&#29306;&#29298;&#35757;&#32451;&#30340;&#22797;&#26434;&#24230;&#65292;&#20197;&#26497;&#39640;&#30340;&#20869;&#23384;&#21644;&#35757;&#32451;&#26102;&#38388;&#20026;&#20195;&#20215;&#65292;&#20351;&#35780;&#20272;&#21644;&#27979;&#35797;&#30340;&#25104;&#26412;&#30456;&#23545;&#36739;&#20302;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#26080;&#27861;&#21516;&#26102;&#24471;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#30340;&#21098;&#26525;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#21644;&#20869;&#23384;&#30340;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#30830;&#20445;&#22312; GLUE &#20219;&#21153;&#20013;&#20934;&#30830;&#24615;&#27809;&#26377;&#26174;&#33879;&#19979;&#38477;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Outstanding performance and growing size of Large Language Models has led to increased attention in parameter efficient learning. The two predominant approaches are Adapters and Pruning. Adapters are to freeze the model and give it a new weight matrix on the side, which can significantly reduce the time and memory of training, but the cost is that the evaluation and testing will increase the time and memory consumption. Pruning is to cut off some weight and re-distribute the remaining weight, which sacrifices the complexity of training at the cost of extremely high memory and training time, making the cost of evaluation and testing relatively low. So efficiency of training and inference can't be obtained in the same time. In this work, we propose a task-oriented Pruning-Adapter method that achieve a high memory efficiency of training and memory, and speeds up training time and ensures no significant decrease in accuracy in GLUE tasks, achieving training and inference efficiency at 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21487;&#36870;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#19982;&#20854;&#20182;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#36755;&#20986;&#23618;&#37325;&#24314;&#36755;&#20837;&#24207;&#21015;&#30340;&#21333;&#35789;&#21521;&#37327;&#65292;&#20854;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#21644;&#24555;&#36895;&#35757;&#32451;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#21512;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#38656;&#35201;&#39640;&#36136;&#37327;&#21477;&#23884;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#20351;&#29992;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.13570</link><description>&lt;p&gt;
RNN &#30340;&#22238;&#24402;&#65306;&#29992;&#21487;&#36870;&#21477;&#23884;&#20837;&#30340;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings. (arXiv:2303.13570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21487;&#36870;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#19982;&#20854;&#20182;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#36755;&#20986;&#23618;&#37325;&#24314;&#36755;&#20837;&#24207;&#21015;&#30340;&#21333;&#35789;&#21521;&#37327;&#65292;&#20854;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#21644;&#24555;&#36895;&#35757;&#32451;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#21512;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#38656;&#35201;&#39640;&#36136;&#37327;&#21477;&#23884;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#20351;&#29992;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65292;&#20351;&#29992;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#30417;&#30563;&#32534;&#30721;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#29983;&#25104;&#21487;&#36870;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#30456;&#27604;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#24120;&#35265;&#30340;&#27010;&#29575;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#36755;&#20986;&#23618;&#26469;&#37325;&#24314;&#36755;&#20837;&#24207;&#21015;&#30340;&#21333;&#35789;&#21521;&#37327;&#12290;&#35813;&#27169;&#22411;&#22312;&#20351;&#29992; ADAM &#20248;&#21270;&#22120;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#30340;&#21516;&#26102;&#65292;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27531;&#24046;&#36830;&#25509;&#21644;&#8220;match drop&#8221;&#25216;&#26415;&#65292;&#21363;&#21482;&#35745;&#31639;&#38169;&#35823;&#21333;&#35789;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#39640;&#36136;&#37327;&#21477;&#23884;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel model for invertible sentence embeddings using a residual recurrent network trained on an unsupervised encoding task. Rather than the probabilistic outputs common to neural machine translation models, our approach employs a regression-based output layer to reconstruct the input sequence's word vectors. The model achieves high accuracy and fast training with the ADAM optimizer, a significant finding given that RNNs typically require memory units, such as LSTMs, or second-order optimization methods. We incorporate residual connections and introduce a "match drop" technique, where gradients are calculated only for incorrect words. Our approach demonstrates potential for various natural language processing applications, particularly in neural network-based systems that require high-quality sentence embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20851;&#30340;&#31867;&#21035;&#65292;&#21363;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#21644;&#20107;&#20214;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#32771;&#34385;&#36825;&#20004;&#20010;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.10112</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;:&#32508;&#36848;&#21644;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Temporal Data: An Overview and New Perspectives. (arXiv:2303.10112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20851;&#30340;&#31867;&#21035;&#65292;&#21363;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#21644;&#20107;&#20214;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#32771;&#34385;&#36825;&#20004;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#25968;&#25454;&#20195;&#34920;&#30528;&#22797;&#26434;&#31995;&#32479;&#30340;&#26102;&#38388;&#39034;&#24207;&#35266;&#27979;&#65292;&#21487;&#20197;&#34987;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#29983;&#25104;&#65292;&#20363;&#22914;&#24037;&#19994;&#12289;&#21307;&#30103;&#21644;&#37329;&#34701;&#12290;&#20998;&#26512;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#22240;&#27492;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#39044;&#27979;&#12290;&#20854;&#20013;&#65292;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#30340;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#26377;&#36259;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#24037;&#20316;&#21487;&#20197;&#26681;&#25454;&#26102;&#38388;&#25968;&#25454;&#26159;&#21542;&#34987;&#26657;&#20934;&#26469;&#20998;&#20026;&#20004;&#20010;&#39640;&#24230;&#30456;&#20851;&#30340;&#31867;&#21035;&#65292;&#21363;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#21644;&#20107;&#20214;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#35843;&#26597;&#20165;&#19987;&#27880;&#20110;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#65292;&#24573;&#30053;&#20102;&#31532;&#20108;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#36825;&#20004;&#20010;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#32771;&#34385;&#36825;&#20004;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal data, representing chronological observations of complex systems, has always been a typical data structure that can be widely generated by many domains, such as industry, medicine and finance. Analyzing this type of data is extremely valuable for various applications. Thus, different temporal data analysis tasks, eg, classification, clustering and prediction, have been proposed in the past decades. Among them, causal discovery, learning the causal relations from temporal data, is considered an interesting yet critical task and has attracted much research attention. Existing casual discovery works can be divided into two highly correlated categories according to whether the temporal data is calibrated, ie, multivariate time series casual discovery, and event sequence casual discovery. However, most previous surveys are only focused on the time series casual discovery and ignore the second category. In this paper, we specify the correlation between the two categories and provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ODE-Net&#22312;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#32422;&#26463;&#21442;&#25968;ODE&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#24230;&#35770;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#24182;&#38024;&#23545;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.05924</link><description>&lt;p&gt;
ODE-Net&#30340;&#21464;&#20998;&#24418;&#24335;&#65306;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21450;&#23384;&#22312;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Variational formulations of ODE-Net as a mean-field optimal control problem and existence results. (arXiv:2303.05924v2 [math.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ODE-Net&#22312;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#32422;&#26463;&#21442;&#25968;ODE&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#24230;&#35770;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#24182;&#38024;&#23545;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ODE-Net&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#65292;&#23427;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#36830;&#32493;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#29992;ODE&#20195;&#26367;DNN&#28145;&#24230;&#32467;&#26500;&#20316;&#20026;&#36830;&#32493;&#26497;&#38480;&#30340;&#24819;&#27861;&#12290;&#36825;&#20123;&#30740;&#31350;&#23558;ODE-Net&#30340;"&#23398;&#20064;"&#35270;&#20026;&#26368;&#23567;&#21270;&#30001;&#21442;&#25968;ODE&#32422;&#26463;&#30340;"&#25439;&#22833;"&#12290;&#34429;&#28982;&#38656;&#35201;&#20551;&#23450;&#35813;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#23384;&#22312;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#35814;&#32454;&#22320;&#20998;&#26512;&#20102;&#20854;&#23384;&#22312;&#24615;&#12290;&#26412;&#25991;&#23558;ODE-Net&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#20026;&#19968;&#31181;&#27979;&#24230;&#35770;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#27492;&#35752;&#35770;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#32467;&#26524;&#12290;&#24403;&#25551;&#36848;ODE-Net&#21521;&#37327;&#22330;&#30340;&#31070;&#32463;&#32593;&#32476;&#38024;&#23545;&#21487;&#23398;&#20064;&#21442;&#25968;&#26159;&#32447;&#24615;&#30340;&#26102;&#65292;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#12290;&#35777;&#26126;&#20351;&#29992;&#27979;&#24230;&#35770;&#24418;&#24335;&#21270;&#21644;&#21464;&#20998;&#27861;&#30340;&#30452;&#25509;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20854;&#27425;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20197;&#28040;&#38500;&#38024;&#23545;&#22522;&#20110;&#36793;&#30028;&#26465;&#20214;&#30340;"&#24658;&#31561;"ODE&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#19968;&#20123;&#25216;&#26415;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a mathematical analysis of ODE-Net, a continuum model of deep neural networks (DNNs). In recent years, Machine Learning researchers have introduced ideas of replacing the deep structure of DNNs with ODEs as a continuum limit. These studies regard the "learning" of ODE-Net as the minimization of a "loss" constrained by a parametric ODE. Although the existence of a minimizer for this minimization problem needs to be assumed, only a few studies have investigated its existence analytically in detail. In the present paper, the existence of a minimizer is discussed based on a formulation of ODE-Net as a measure-theoretic mean-field optimal control problem. The existence result is proved when a neural network, which describes a vector field of ODE-Net, is linear with respect to learnable parameters. The proof employs the measure-theoretic formulation combined with the direct method of Calculus of Variations. Secondly, an idealized minimization problem is proposed to remove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LSTM&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21463;&#21040;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.03324</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection with reconstruction-based state-space models. (arXiv:2303.03324v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#37325;&#26500;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LSTM&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21463;&#21040;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#21487;&#20197;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#23548;&#33268;&#21508;&#31181;&#39046;&#22495;&#20013;&#20986;&#29616;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20351;&#24471;&#23454;&#26102;&#30417;&#27979;&#36816;&#33829;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#24322;&#24120;&#25968;&#25454;&#27169;&#24335;&#21644;&#26816;&#27979;&#28508;&#22312;&#25925;&#38556;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#20294;&#20063;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20849;&#21516;&#23398;&#20064;&#35266;&#27979;&#27169;&#22411;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#20174;&#27491;&#24120;&#26679;&#26412;&#20013;&#20272;&#35745;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#30340;&#65292;&#37319;&#29992;&#22522;&#20110;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#30340;&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#34920;&#31034;&#35266;&#27979;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;, &#34701;&#21512;&#20102;&#21521;&#21518;&#21644;&#21521;&#21069;&#30340;&#26102;&#38388;&#20449;&#24687;&#20197;&#21516;&#26102;&#24314;&#27169;&#29366;&#24577;&#30340;&#21452;&#21521;&#36716;&#25442;&#12290;&#28508;&#22312;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#32422;&#26463;&#20102;&#27491;&#24120;&#26679;&#26412;&#30340;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#35780;&#20272;&#24322;&#24120;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in digitization have led to the availability of multivariate time series data in various domains, enabling real-time monitoring of operations. Identifying abnormal data patterns and detecting potential failures in these scenarios are important yet rather challenging. In this work, we propose a novel unsupervised anomaly detection method for time series data. The proposed framework jointly learns the observation model and the dynamic model, and model uncertainty is estimated from normal samples. Specifically, a long short-term memory (LSTM)-based encoder-decoder is adopted to represent the mapping between the observation space and the latent space. Bidirectional transitions of states are simultaneously modeled by leveraging backward and forward temporal information. Regularization of the latent space places constraints on the states of normal samples, and Mahalanobis distance is used to evaluate the abnormality level. Empirical studies on synthetic and real-world dataset
&lt;/p&gt;</description></item><item><title>&#38750;&#23545;&#25968;&#20985;&#21183;V&#30340;&#39640;&#32500;&#37319;&#26679;&#36895;&#29575;&#21487;&#20197;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23454;&#29616;&#19982;&#20984;&#20989;&#25968;&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.03237</link><description>&lt;p&gt;
&#38750;&#23545;&#25968;&#20985;&#37319;&#26679;&#21644;&#23545;&#25968;&#20998;&#21306;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Convergence Rates for Non-Log-Concave Sampling and Log-Partition Estimation. (arXiv:2303.03237v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03237
&lt;/p&gt;
&lt;p&gt;
&#38750;&#23545;&#25968;&#20985;&#21183;V&#30340;&#39640;&#32500;&#37319;&#26679;&#36895;&#29575;&#21487;&#20197;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23454;&#29616;&#19982;&#20984;&#20989;&#25968;&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21513;&#24067;&#26031;&#20998;&#24067;$p(x)\propto\exp(-V(x)/\epsilon)$&#20013;&#37319;&#26679;&#24182;&#35745;&#31639;&#20854;&#23545;&#25968;&#20998;&#21306;&#20989;&#25968;&#26159;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#26377;&#25928;&#30340;&#31639;&#27861;&#24050;&#30693;&#20110;&#20984;&#21183;&#20989;&#25968;$V$&#65292;&#20294;&#38750;&#20984;&#24773;&#20917;&#19979;&#30340;&#24773;&#20917;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#31639;&#27861;&#24517;&#28982;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;&#39640;&#32500;&#37319;&#26679;&#38750;&#23545;&#25968;&#20985;&#21183;V&#30340;&#36895;&#29575;&#20063;&#21487;&#20197;&#36798;&#21040;&#21516;&#26679;&#24555;&#30340;&#36895;&#24230;&#12290;&#26412;&#25991;&#23545;&#36825;&#20123;&#32467;&#26524;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling from Gibbs distributions $p(x) \propto \exp(-V(x)/\varepsilon)$ and computing their log-partition function are fundamental tasks in statistics, machine learning, and statistical physics. However, while efficient algorithms are known for convex potentials $V$, the situation is much more difficult in the non-convex case, where algorithms necessarily suffer from the curse of dimensionality in the worst case. For optimization, which can be seen as a low-temperature limit of sampling, it is known that smooth functions $V$ allow faster convergence rates. Specifically, for $m$-times differentiable functions in $d$ dimensions, the optimal rate for algorithms with $n$ function evaluations is known to be $O(n^{-m/d})$, where the constant can potentially depend on $m, d$ and the function to be optimized. Hence, the curse of dimensionality can be alleviated for smooth functions at least in terms of the convergence rate. Recently, it has been shown that similarly fast rates can also be ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21322;&#20998;&#25955;&#25512;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#20113;&#33410;&#28857;&#38477;&#20302;&#36890;&#20449;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#25955;&#21270;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#25552;&#39640;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.00524</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21322;&#20998;&#25955;&#25512;&#29702;&#25216;&#26415;&#22312;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#36793;&#32536;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semi-decentralized Inference in Heterogeneous Graph Neural Networks for Traffic Demand Forecasting: An Edge-Computing Approach. (arXiv:2303.00524v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21322;&#20998;&#25955;&#25512;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#20113;&#33410;&#28857;&#38477;&#20302;&#36890;&#20449;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#25955;&#21270;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#25552;&#39640;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20986;&#31199;&#36710;&#26381;&#21153;&#30340;&#38656;&#27714;&#21644;&#20379;&#24212;&#23545;&#20110;&#25552;&#39640;&#23458;&#25143;&#20307;&#39564;&#21644;&#25552;&#20379;&#21830;&#21033;&#28070;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#27492;&#31867;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#21069;&#26223;&#12290;&#35813;&#26041;&#27861;&#23558;&#22478;&#24066;&#21306;&#22495;&#24314;&#27169;&#20026;&#19968;&#20010;&#20132;&#36890;&#22270;&#20013;&#30340;&#33410;&#28857;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20316;&#20026;&#36793;&#12290;GNN&#21033;&#29992;&#26412;&#22320;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#24418;&#32467;&#26500;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20004;&#31181;&#20027;&#35201;&#36884;&#24452;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#39044;&#27979;&#65306;&#25193;&#22823;&#20132;&#36890;&#32593;&#32476;&#30340;&#35268;&#27169;&#65292;&#21516;&#26102;&#21033;&#29992;&#22270;&#20013;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#38754;&#20020;GNN&#21487;&#25193;&#23637;&#24615;&#30340;&#25361;&#25112;&#12290;&#21363;&#26102;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#20998;&#25955;GNN&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#20250;&#20135;&#29983;&#36807;&#22810;&#30340;&#33410;&#28857;&#20043;&#38388;&#20256;&#36755;&#36890;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#34920;&#24449;&#20102;&#20998;&#25955;&#30340;GNN&#26041;&#27861;&#23545;&#20110;&#36807;&#22810;&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20998;&#25955;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#36127;&#36131;&#35843;&#33410;&#30340;&#20113;&#35745;&#31639;&#33410;&#28857;&#26469;&#20943;&#23569;&#36890;&#20449;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20998;&#25955;&#21270;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#20986;&#31199;&#36710;&#26381;&#21153;&#38656;&#27714;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of taxi service demand and supply is essential for improving customer's experience and provider's profit. Recently, graph neural networks (GNNs) have been shown promising for this application. This approach models city regions as nodes in a transportation graph and their relations as edges. GNNs utilize local node features and the graph structure in the prediction. However, more efficient forecasting can still be achieved by following two main routes; enlarging the scale of the transportation graph, and simultaneously exploiting different types of nodes and edges in the graphs. However, both approaches are challenged by the scalability of GNNs. An immediate remedy to the scalability challenge is to decentralize the GNN operation. However, this creates excessive node-to-node communication. In this paper, we first characterize the excessive communication needs for the decentralized GNN approach. Then, we propose a semi-decentralized approach utilizing multiple cloudlets, moder
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.00085</link><description>&lt;p&gt;
AR3n: &#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for Robotic Rehabilitation. (arXiv:2303.00085v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AR3n&#65288;&#21457;&#38899;&#20026;Aaron&#65289;&#65292;&#19968;&#31181;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#65292;&#21487;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#20070;&#20889;&#24247;&#22797;&#20219;&#21153;&#20013;&#25552;&#20379;&#36866;&#24212;&#24615;&#36741;&#21161;&#12290;&#19982;&#20197;&#24448;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#24739;&#32773;&#29305;&#23450;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#25110;&#29289;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#26469;&#20351;AR3n&#25512;&#24191;&#21040;&#22810;&#20010;&#21463;&#35797;&#32773;&#12290;&#35813;&#31995;&#32479;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#22522;&#20110;&#34987;&#35797;&#30340;&#36319;&#36394;&#35823;&#24046;&#12290;&#36890;&#36807;&#19968;&#32452;&#20223;&#30495;&#23454;&#39564;&#21644;&#20154;&#20307;&#21463;&#35797;&#23454;&#39564;&#23545;&#25511;&#21046;&#22120;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#19982;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#25511;&#21046;&#22120;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#20998;&#26512;&#20004;&#31181;&#25511;&#21046;&#22120;&#30340;&#36741;&#21161;&#26426;&#21046;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present AR3n (pronounced as Aaron), an assist-as-needed (AAN) controller that utilizes reinforcement learning to supply adaptive assistance during a robot assisted handwriting rehabilitation task. Unlike previous AAN controllers, our method does not rely on patient specific controller parameters or physical models. We propose the use of a virtual patient model to generalize AR3n across multiple subjects. The system modulates robotic assistance in realtime based on a subject's tracking error, while minimizing the amount of robotic assistance. The controller is experimentally validated through a set of simulations and human subject experiments. Finally, a comparative study with a traditional rule-based controller is conducted to analyze differences in assistance mechanisms of the two controllers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#65292;&#31216;&#20026;&#21512;&#29702;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65288;PAD&#65289;&#65292;&#23427;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20984;&#24230;&#37327;&#20445;&#25252;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#20813;&#21463;&#25915;&#20987;&#32773;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#38450;&#24481;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.11328</link><description>&lt;p&gt;
PAD: &#38754;&#21521;&#23545;&#25239;&#36867;&#36991;&#25915;&#20987;&#30340;&#21512;&#29702;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PAD: Towards Principled Adversarial Malware Detection Against Evasion Attacks. (arXiv:2302.11328v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#65292;&#31216;&#20026;&#21512;&#29702;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65288;PAD&#65289;&#65292;&#23427;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20984;&#24230;&#37327;&#20445;&#25252;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#20813;&#21463;&#25915;&#20987;&#32773;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#38450;&#24481;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#20419;&#36827;&#24694;&#24847;&#36719;&#20214;&#65288;&#31616;&#31216;&#20026;&#24694;&#24847;&#36719;&#20214;&#65289;&#30340;&#33258;&#21160;&#26816;&#27979;&#65292;&#20294;&#21463;&#21040;&#36867;&#36991;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25915;&#20987;&#65292;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#21644;&#26377;&#25928;&#30340;&#38450;&#24481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#65292;&#31216;&#20026;&#21512;&#29702;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65288;PAD&#65289;&#65292;&#23427;&#38024;&#23545;&#24378;&#22823;&#30340;&#20248;&#21270;&#26041;&#27861;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;PAD&#24314;&#31435;&#22312;&#21487;&#23398;&#20064;&#30340;&#20984;&#24230;&#37327;&#19978;&#65292;&#37327;&#21270;&#20998;&#24067;&#24335;&#31163;&#25955;&#25200;&#21160;&#65292;&#20197;&#20445;&#25252;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#20813;&#21463;&#25915;&#20987;&#32773;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#24179;&#28369;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#29702;&#35770;&#19978;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#20026;&#20102;&#25552;&#39640;&#38450;&#24481;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25915;&#20987;&#26041;&#27861;&#26469;&#23454;&#29616;PAD&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27979;&#37327;&#21644;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#12290;&#22312;&#20004;&#20010;Android&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) techniques can facilitate the automation of malicious software (malware for short) detection, but suffer from evasion attacks. Many studies counter such attacks in heuristic manners, lacking theoretical guarantees and defense effectiveness. In this paper, we propose a new adversarial training framework, termed Principled Adversarial Malware Detection (PAD), which offers convergence guarantees for robust optimization methods. PAD lays on a learnable convex measurement that quantifies distribution-wise discrete perturbations to protect malware detectors from adversaries, whereby for smooth detectors, adversarial training can be performed with theoretical treatments. To promote defense effectiveness, we propose a new mixture of attacks to instantiate PAD to enhance deep neural network-based measurements and malware detectors. Experimental results on two Android malware datasets demonstrate: (i) the proposed method significantly outperforms the state-of-the-art defens
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#25512;&#26029;&#36139;&#22256;&#22320;&#22270;&#30340;&#27169;&#22411;&#31649;&#36947;&#65292;&#36890;&#36807;&#25512;&#26029;&#22810;&#20010;&#22320;&#29702;&#38598;&#32676;&#30340;&#36130;&#23500;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#65292;&#37197;&#21512;&#26032;&#39062;&#30340;&#8220;&#36130;&#23500;&#22320;&#24179;&#32447;&#8221;&#27010;&#24565;&#26126;&#30830;&#21487;&#35270;&#21270;&#36130;&#23500;&#20998;&#24067;&#22312;&#19981;&#21516;&#31354;&#38388;&#23610;&#24230;&#19979;&#30340;&#24773;&#20917;&#65292;&#26377;&#21161;&#20110;&#25919;&#31574;&#21046;&#23450;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#24433;&#21709;&#36130;&#23500;&#20998;&#24067;&#30340;&#28508;&#22312;&#31038;&#20250;&#32463;&#27982;&#22810;&#26679;&#24615;&#65292;&#20248;&#20808;&#32771;&#34385;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2302.10793</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#25512;&#26029;&#36139;&#22256;&#22320;&#22270;&#26469;&#35299;&#37322;&#36130;&#23500;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Interpreting wealth distribution via poverty map inference using multimodal data. (arXiv:2302.10793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#25512;&#26029;&#36139;&#22256;&#22320;&#22270;&#30340;&#27169;&#22411;&#31649;&#36947;&#65292;&#36890;&#36807;&#25512;&#26029;&#22810;&#20010;&#22320;&#29702;&#38598;&#32676;&#30340;&#36130;&#23500;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#65292;&#37197;&#21512;&#26032;&#39062;&#30340;&#8220;&#36130;&#23500;&#22320;&#24179;&#32447;&#8221;&#27010;&#24565;&#26126;&#30830;&#21487;&#35270;&#21270;&#36130;&#23500;&#20998;&#24067;&#22312;&#19981;&#21516;&#31354;&#38388;&#23610;&#24230;&#19979;&#30340;&#24773;&#20917;&#65292;&#26377;&#21161;&#20110;&#25919;&#31574;&#21046;&#23450;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#24433;&#21709;&#36130;&#23500;&#20998;&#24067;&#30340;&#28508;&#22312;&#31038;&#20250;&#32463;&#27982;&#22810;&#26679;&#24615;&#65292;&#20248;&#20808;&#32771;&#34385;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36139;&#22256;&#22320;&#22270;&#26159;&#25919;&#24220;&#21644;&#38750;&#25919;&#24220;&#32452;&#32455;&#36861;&#36394;&#31038;&#20250;&#32463;&#27982;&#21464;&#21270;&#24182;&#22312;&#38656;&#35201;&#30340;&#22320;&#21306;&#20805;&#20998;&#20998;&#37197;&#22522;&#30784;&#35774;&#26045;&#21644;&#26381;&#21153;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#20256;&#24863;&#22120;&#21644;&#22312;&#32447;&#20247;&#21253;&#25968;&#25454;&#19982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#26368;&#36817;&#22312;&#36139;&#22256;&#22320;&#22270;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#22320;&#26041;&#36130;&#23500;&#27874;&#21160;&#65292;&#24182;&#19988;&#27809;&#26377;&#20248;&#21270;&#20197;&#20135;&#29983;&#21487;&#38752;&#32467;&#26524;&#65292;&#20197;&#30830;&#20445;&#25152;&#26377;&#23376;&#20154;&#21475;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31649;&#36947;&#65292;&#26469;&#25512;&#26029;&#22810;&#20010;&#22320;&#29702;&#38598;&#32676;&#20154;&#21475;&#22320;&#26041;&#30340;&#36130;&#23500;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#20102;&#23427;&#20204;&#22312;&#22622;&#25289;&#21033;&#26114;&#21644;&#20044;&#24178;&#36798;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#20102;&#19971;&#20010;&#29420;&#31435;&#30340;&#21644;&#33258;&#30001;&#21487;&#29992;&#30340;&#29305;&#24449;&#28304;&#65292;&#22522;&#20110;&#21355;&#26143;&#22270;&#20687;&#21644;&#36890;&#36807;&#22312;&#32447;&#20247;&#21253;&#21644;&#31038;&#20132;&#23186;&#20307;&#25910;&#38598;&#30340;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#26126;&#65292;&#32508;&#21512;&#20803;&#25968;&#25454;&#29305;&#24449;&#26159;&#20892;&#26449;&#22320;&#21306;&#36130;&#23500;&#30340;&#26368;&#20339;&#39044;&#27979;&#22240;&#32032;&#65292;&#20248;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#8220;&#36130;&#23500;&#22320;&#24179;&#32447;&#8221;&#27010;&#24565;&#65292;&#26126;&#30830;&#22320;&#21487;&#35270;&#21270;&#20102;&#19981;&#21516;&#31354;&#38388;&#23610;&#24230;&#19979;&#30340;&#36130;&#23500;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#25919;&#31574;&#21046;&#23450;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#24433;&#21709;&#36130;&#23500;&#20998;&#24067;&#30340;&#28508;&#22312;&#31038;&#20250;&#32463;&#27982;&#22810;&#26679;&#24615;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poverty maps are essential tools for governments and NGOs to track socioeconomic changes and adequately allocate infrastructure and services in places in need. Sensor and online crowd-sourced data combined with machine learning methods have provided a recent breakthrough in poverty map inference. However, these methods do not capture local wealth fluctuations, and are not optimized to produce accountable results that guarantee accurate predictions to all sub-populations. Here, we propose a pipeline of machine learning models to infer the mean and standard deviation of wealth across multiple geographically clustered populated places, and illustrate their performance in Sierra Leone and Uganda. These models leverage seven independent and freely available feature sources based on satellite images, and metadata collected via online crowd-sourcing and social media. Our models show that combined metadata features are the best predictors of wealth in rural areas, outperforming image-based mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#33258;&#36866;&#24212;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;PQ&#25351;&#25968;&#26469;&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#21387;&#32553;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#23450;&#27169;&#22411;&#30340;&#21098;&#26525;&#31243;&#24230;&#65292;&#30830;&#20445;&#19981;&#20250;&#36807;&#24230;&#25110;&#27424;&#21098;&#26525;&#12290;</title><link>http://arxiv.org/abs/2302.05601</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#24615;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Pruning Deep Neural Networks from a Sparsity Perspective. (arXiv:2302.05601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#33258;&#36866;&#24212;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;PQ&#25351;&#25968;&#26469;&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#21387;&#32553;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#23450;&#27169;&#22411;&#30340;&#21098;&#26525;&#31243;&#24230;&#65292;&#30830;&#20445;&#19981;&#20250;&#36807;&#24230;&#25110;&#27424;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#32593;&#32476;&#21098;&#26525;&#25216;&#26415;&#21463;&#21040;&#20102;&#37325;&#35270;&#65292;&#26088;&#22312;&#23558;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#37096;&#32626;&#21040;&#35745;&#31639;&#21644;&#20869;&#23384;&#21463;&#38480;&#30340;&#23567;&#22411;&#35774;&#22791;&#19978;&#12290;&#36825;&#31181;&#21098;&#26525;&#36890;&#24120;&#36890;&#36807;&#20002;&#24323;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#20887;&#20313;&#26435;&#37325;&#12289;&#31070;&#32463;&#20803;&#25110;&#23618;&#26469;&#23454;&#29616;&#65292;&#21516;&#26102;&#21162;&#21147;&#20445;&#25345;&#21487;&#27604;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#23618;&#21098;&#26525;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#23454;&#35777;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#21487;&#37327;&#21270;&#30340;&#25514;&#26045;&#26469;&#20272;&#31639;&#27599;&#20010;&#21098;&#26525;&#36845;&#20195;&#20013;&#23376;&#32593;&#32476;&#30340;&#21487;&#21387;&#32553;&#24615;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#36827;&#34892;&#36807;&#21098;&#26525;&#25110;&#27424;&#21098;&#26525;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PQ&#25351;&#25968;&#65288;PQI&#65289;&#26469;&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#21387;&#32553;&#24615;&#65292;&#24182;&#21033;&#29992;&#27492;&#25351;&#25968;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#33258;&#36866;&#24212;&#21098;&#26525;&#65288;SAP&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#25903;&#25345;&#20551;&#35774;&#65292;&#23545;&#20110;&#36890;&#29992;&#21098;&#26525;&#31243;&#24207;&#65292;&#24403;&#22823;&#22411;&#27169;&#22411;&#34987;&#26377;&#25928;&#22320;&#27491;&#21017;&#21270;&#26102;&#65292;PQI&#39318;&#20808;&#20943;&#23567;&#65292;&#28982;&#21518;&#22312;&#20854;&#21487;&#21387;&#32553;&#24615;&#36798;&#21040;&#26368;&#23567;&#20540;&#26102;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#36739;&#23567;&#30340;&#39044;&#31639;&#19979;&#21152;&#36895;&#35757;&#32451;&#21644;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#27604;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02013</link><description>&lt;p&gt;
&#22522;&#20110;&#32463;&#27982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;IoT&#20725;&#23608;&#32593;&#32476;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
IoT Botnet Detection Using an Economic Deep Learning Model. (arXiv:2302.02013v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#36739;&#23567;&#30340;&#39044;&#31639;&#19979;&#21152;&#36895;&#35757;&#32451;&#21644;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#27604;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#21019;&#26032;&#30340;&#24555;&#36895;&#36827;&#27493;&#22686;&#21152;&#20102;&#36807;&#21435;&#21313;&#24180;&#30340;&#20351;&#29992;&#21644;&#20998;&#21457;&#12290;&#20840;&#29699;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#24555;&#36895;&#22686;&#38271;&#22686;&#21152;&#20102;&#30001;&#24694;&#24847;&#31532;&#19977;&#26041;&#21019;&#24314;&#30340;&#32593;&#32476;&#23433;&#20840;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#23433;&#20840;&#38382;&#39064;&#21644;&#29289;&#32852;&#32593;&#31995;&#32479;&#38480;&#21046;&#30340;&#21487;&#38752;&#20837;&#20405;&#26816;&#27979;&#21644;&#32593;&#32476;&#21462;&#35777;&#31995;&#32479;&#23545;&#20110;&#20445;&#25252;&#36825;&#20123;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#26159;&#20225;&#19994;&#21644;&#20010;&#20154;&#38754;&#20020;&#30340;&#37325;&#22823;&#23041;&#32961;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#27982;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#36739;&#23567;&#30340;&#23454;&#29616;&#39044;&#31639;&#19979;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#21644;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#33719;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress in technology innovation usage and distribution has increased in the last decade. The rapid growth of the Internet of Things (IoT) systems worldwide has increased network security challenges created by malicious third parties. Thus, reliable intrusion detection and network forensics systems that consider security concerns and IoT systems limitations are essential to protect such systems. IoT botnet attacks are one of the significant threats to enterprises and individuals. Thus, this paper proposed an economic deep learning-based model for detecting IoT botnet attacks along with different types of attacks. The proposed model achieved higher accuracy than the state-of-the-art detection models using a smaller implementation budget and accelerating the training and detecting processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861; SEER&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#32570;&#20047;&#27979;&#35797;&#26029;&#35328;&#25110;&#20854;&#20182;&#31867;&#22411;&#30340;&#27979;&#35797;Oracle&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#21333;&#20803;&#27979;&#35797;&#26159;&#21542;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24182;&#19988;&#21487;&#20197;&#26500;&#24314;&#20934;&#30830;&#30340;Oracle&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#27491;&#30830;&#25110;&#38169;&#35823;&#34892;&#20026;&#30340;&#30830;&#20999;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2302.01488</link><description>&lt;p&gt;
&#23436;&#32654;&#20027;&#20041;&#26159;&#27979;&#35797;Oracle&#30340;&#25932;&#20154;
&lt;/p&gt;
&lt;p&gt;
Perfect is the enemy of test oracle. (arXiv:2302.01488v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861; SEER&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#32570;&#20047;&#27979;&#35797;&#26029;&#35328;&#25110;&#20854;&#20182;&#31867;&#22411;&#30340;&#27979;&#35797;Oracle&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#21333;&#20803;&#27979;&#35797;&#26159;&#21542;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24182;&#19988;&#21487;&#20197;&#26500;&#24314;&#20934;&#30830;&#30340;Oracle&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#27491;&#30830;&#25110;&#38169;&#35823;&#34892;&#20026;&#30340;&#30830;&#20999;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#27979;&#35797;Oracle&#26159;&#36719;&#20214;&#27979;&#35797;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#20043;&#19968;&#65292;&#20294;&#19982;&#33258;&#21160;&#21270;&#27979;&#35797;&#36755;&#20837;&#29983;&#25104;&#30456;&#27604;&#65292;&#20173;&#28982;&#21463;&#21040;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#27979;&#35797;Oracle&#20381;&#36182;&#20110;&#21487;&#20197;&#21306;&#20998;&#27491;&#30830;&#34892;&#20026;&#21644;&#38169;&#35823;&#34892;&#20026;&#30340;&#22522;&#30784;&#30495;&#30456;&#26469;&#30830;&#23450;&#27979;&#35797;&#26159;&#21542;&#22833;&#36133;&#65288;&#26816;&#27979;&#21040;&#38169;&#35823;&#65289;&#25110;&#36890;&#36807;&#12290;&#35753;Oracle&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#19981;&#21487;&#20915;&#23450;&#24615;&#30340;&#26159;&#20551;&#35774;&#36825;&#20010;&#22522;&#30784;&#30495;&#30456;&#38656;&#35201;&#30693;&#36947;&#27491;&#30830;&#34892;&#20026;&#25110;&#38169;&#35823;&#34892;&#20026;&#30340;&#30830;&#20999;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#21363;&#20351;&#19981;&#30693;&#36947;&#30830;&#20999;&#30340;&#27491;&#30830;&#25110;&#38169;&#35823;&#34892;&#20026;&#22914;&#20309;&#19981;&#21516;&#65292;&#20173;&#28982;&#21487;&#20197;&#26500;&#24314;&#20934;&#30830;&#30340;Oracle&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SEER&#65292;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#20047;&#27979;&#35797;&#26029;&#35328;&#25110;&#20854;&#20182;&#31867;&#22411;&#30340;Oracle&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;&#22312;&#32473;&#23450;&#30340;&#27979;&#35797;&#26041;&#27861;&#19979;&#21333;&#20803;&#27979;&#35797;&#26159;&#21542;&#36890;&#36807;&#25110;&#22833;&#36133;&#12290;&#20026;&#20102;&#24314;&#31435;&#22522;&#30784;&#30495;&#30456;&#65292;SEER&#23558;&#21333;&#20803;&#27979;&#35797;&#21644;MUTs&#30340;&#23454;&#29616;&#32852;&#21512;&#23884;&#20837;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#65292;&#20351;&#31070;&#32463;&#34920;&#31034;&#26041;&#24335;&#20855;&#26377;&#21306;&#20998;&#21333;&#20803;&#27979;&#35797;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automation of test oracles is one of the most challenging facets of software testing, but remains comparatively less addressed compared to automated test input generation. Test oracles rely on a ground-truth that can distinguish between the correct and buggy behavior to determine whether a test fails (detects a bug) or passes. What makes the oracle problem challenging and undecidable is the assumption that the ground-truth should know the exact expected, correct, or buggy behavior. However, we argue that one can still build an accurate oracle without knowing the exact correct or buggy behavior, but how these two might differ. This paper presents SEER, a learning-based approach that in the absence of test assertions or other types of oracle, can determine whether a unit test passes or fails on a given method under test (MUT). To build the ground-truth, SEER jointly embeds unit tests and the implementation of MUTs into a unified vector space, in such a way that the neural representation 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#28431;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#40657;&#30418;&#25552;&#21462;&#12289;&#25512;&#26029;&#21644;&#37325;&#24314;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.00539</link><description>&lt;p&gt;
&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#35782;&#21035;&#20449;&#24687;&#27844;&#38706;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Analyzing Leakage of Personally Identifiable Information in Language Models. (arXiv:2302.00539v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#28431;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#40657;&#30418;&#25552;&#21462;&#12289;&#25512;&#26029;&#21644;&#37325;&#24314;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#20250;&#36890;&#36807;&#21477;&#23376;&#32423;&#25104;&#21592;&#25512;&#26029;&#21644;&#37325;&#26500;&#25915;&#20987;&#27844;&#28431;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#27844;&#38706;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#30340;&#39118;&#38505;&#20102;&#35299;&#19981;&#36275;&#12290;&#30446;&#21069;&#24050;&#32463;&#20551;&#35774;&#25968;&#25454;&#38598;&#25972;&#29702;&#25216;&#26415;&#65288;&#22914;&#25968;&#25454;&#28165;&#27927;&#65289;&#36275;&#20197;&#38450;&#27490;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#27844;&#38706;&#65292;&#20294;&#36825;&#19968;&#20551;&#35774;&#26159;&#38169;&#35823;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#25968;&#25454;&#28165;&#27927;&#25216;&#26415;&#21487;&#20197;&#20943;&#23569;Pll&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#20294;&#24182;&#19981;&#33021;&#23436;&#20840;&#32477;&#23545;&#22320;&#38450;&#27490;&#27844;&#38706;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#27844;&#28431;&#30340;&#20005;&#26684;&#22522;&#20110;&#21338;&#24328;&#30340;&#23450;&#20041;&#65292;&#36890;&#36807;API&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#25552;&#21462;&#12289;&#25512;&#26029;&#21644;&#37325;&#24314;&#25915;&#20987;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#31867;TGPSSM&#65292;&#21033;&#29992;&#27491;&#35268;&#21270;&#27969;&#22686;&#21152;&#20102;&#26631;&#20934;GPSSM&#20013;&#30340;GP&#20808;&#39564;&#27010;&#29575;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#65292;&#20026;&#28508;&#22312;&#29366;&#24577;&#30340;&#21464;&#20998;&#20998;&#24067;&#25552;&#20379;&#28789;&#27963;&#21644;&#26368;&#20248;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2301.08843</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Flexibility and Interpretability of Gaussian Process State-Space Model. (arXiv:2301.08843v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#31867;TGPSSM&#65292;&#21033;&#29992;&#27491;&#35268;&#21270;&#27969;&#22686;&#21152;&#20102;&#26631;&#20934;GPSSM&#20013;&#30340;GP&#20808;&#39564;&#27010;&#29575;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#65292;&#20026;&#28508;&#22312;&#29366;&#24577;&#30340;&#21464;&#20998;&#20998;&#24067;&#25552;&#20379;&#28789;&#27963;&#21644;&#26368;&#20248;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;GPSSM&#65289;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;GPSSM&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#36828;&#38750;&#20196;&#20154;&#28385;&#24847;&#12290;&#22823;&#22810;&#25968;GPSSM&#30740;&#31350;&#20381;&#36182;&#20110;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21644;&#39044;&#20808;&#35774;&#32622;&#30340;&#26680;&#24515;&#65292;&#20363;&#22914;&#24179;&#26041;&#25351;&#25968;&#65288;SE&#65289;&#26680;&#24515;&#25110;Matern&#26680;&#24515;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#31867;&#65292;&#31216;&#20026;TGPSSM&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#27491;&#35268;&#21270;&#27969;&#65292;TGPSSM&#22686;&#21152;&#20102;&#26631;&#20934;GPSSM&#20013;&#30340;GP&#20808;&#39564;&#27010;&#29575;&#65292;&#20351;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26356;&#20855;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;TGPSSM&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#20026;&#28508;&#22312;&#29366;&#24577;&#30340;&#21464;&#20998;&#20998;&#24067;&#25552;&#20379;&#28789;&#27963;&#21644;&#26368;&#20248;&#30340;&#32467;&#26500;&#12290;&#30001;&#20110;GP&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#35813;&#31639;&#27861;&#26159;&#21487;&#35299;&#37322;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gaussian process state-space model (GPSSM) has attracted much attention over the past decade. However, the model representation power of the GPSSM is far from satisfactory. Most GPSSM studies rely on the standard Gaussian process (GP) with a preliminary kernel, such as the squared exponential (SE) kernel or Mat\'{e}rn kernel, which limits the model representation power and its application in complex scenarios. To address this issue, this paper proposes a novel class of probabilistic state-space models, called TGPSSMs. By leveraging a parametric normalizing flow, the TGPSSMs enrich the GP priors in the standard GPSSM, rendering the state-space model more flexible and expressive. Additionally, we present a scalable variational inference algorithm for learning and inference in TGPSSMs, which provides a flexible and optimal structure for the variational distribution of latent states. The algorithm is interpretable and computationally efficient owing to the sparse representation of GP a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#31354;&#38388;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.05785</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#23454;&#29616;&#39640;&#25928;&#30340;&#28608;&#27963;&#20989;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Activation Function Optimization through Surrogate Modeling. (arXiv:2301.05785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#31354;&#38388;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24515;&#35774;&#35745;&#30340;&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#24456;&#38590;&#26500;&#24314;&#26368;&#20248;&#28608;&#27963;&#20989;&#25968;&#65292;&#32780;&#24403;&#21069;&#30340;&#28608;&#27963;&#20989;&#25968;&#25628;&#32034;&#31639;&#27861;&#36807;&#20110;&#26114;&#36149;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#26088;&#22312;&#25913;&#36827;&#29616;&#26377;&#25216;&#26415;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;2,913&#20010;&#31995;&#32479;&#29983;&#25104;&#30340;&#28608;&#27963;&#20989;&#25968;&#20174;&#22836;&#35757;&#32451;&#21367;&#31215;&#12289;&#27531;&#24046;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#21019;&#24314; Act-Bench-CNN&#12289;Act-Bench-ResNet &#21644; Act-Bench-ViT &#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#29992;&#20110;&#20248;&#21270;&#22522;&#20934;&#31354;&#38388;&#65292;&#21457;&#29616;&#19982;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#21644;&#28608;&#27963;&#20989;&#25968;&#36755;&#20986;&#20998;&#24067;&#30456;&#20851;&#32852;&#30340; Fisher &#20449;&#24687;&#30697;&#38453;&#30340;&#39057;&#35889;&#23545;&#24615;&#33021;&#30340;&#39044;&#27979;&#24615;&#24456;&#39640;&#12290;&#31532;&#19977;&#65292;&#20351;&#29992;&#20195;&#29702;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#25913;&#36827;&#30340;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#21516;&#26102;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activ
&lt;/p&gt;</description></item><item><title>EmoGator&#26159;&#19968;&#20010;&#21253;&#21547;32,130&#20010;&#26679;&#26412;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#30701;&#26242;&#30340;&#38750;&#35821;&#38899;&#21457;&#22768;&#23545;&#24773;&#24863;&#30340;&#34920;&#36798;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#32447;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.00508</link><description>&lt;p&gt;
EmoGator&#65306;&#19968;&#31181;&#26032;&#30340;&#24320;&#28304;&#35821;&#38899;&#29190;&#21457;&#25968;&#25454;&#38598;&#19982;&#22522;&#32447;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmoGator: A New Open Source Vocal Burst Dataset with Baseline Machine Learning Classification Methodologies. (arXiv:2301.00508v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00508
&lt;/p&gt;
&lt;p&gt;
EmoGator&#26159;&#19968;&#20010;&#21253;&#21547;32,130&#20010;&#26679;&#26412;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#30701;&#26242;&#30340;&#38750;&#35821;&#38899;&#21457;&#22768;&#23545;&#24773;&#24863;&#30340;&#34920;&#36798;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#32447;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vocal Bursts&#26159;&#19968;&#31181;&#30701;&#26242;&#30340;&#38750;&#35821;&#38899;&#21457;&#22768;&#65292;&#21253;&#25324;&#31505;&#22768;&#12289;&#21741;&#22768;&#12289;&#21497;&#24687;&#12289;&#21627;&#21535;&#21644;&#21653;&#21725;&#31561;&#65292;&#20256;&#36882;&#24773;&#24863;&#65292;&#26159;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#24120;&#34987;&#24573;&#35270;&#20294;&#20154;&#31867;&#22768;&#38899;&#20132;&#27969;&#20013;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#20854;&#20013;&#19968;&#20010;&#38590;&#28857;&#26159;&#32570;&#20047;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24456;&#39640;&#20852;&#22320;&#20171;&#32461;EmoGator&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;357&#20010;&#35828;&#35805;&#20154;&#30340;32,130&#20010;&#26679;&#26412;&#65292;16.9654&#20010;&#23567;&#26102;&#30340;&#38899;&#39057;&#65307;&#27599;&#20010;&#26679;&#26412;&#30001;&#35828;&#35805;&#20154;&#20998;&#31867;&#20026;30&#20010;&#19981;&#21516;&#30340;&#24773;&#24863;&#31867;&#21035;&#12290;&#23558;&#35752;&#35770;&#26500;&#24314;&#20998;&#31867;&#22120;&#20197;&#35782;&#21035;&#24773;&#24863;&#31867;&#21035;&#30340;&#20960;&#31181;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#24314;&#35758;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#25968;&#25454;&#38598;&#21487;&#20174;https://github.com/fredbuhl/EmoGator&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vocal Bursts -- short, non-speech vocalizations that convey emotions, such as laughter, cries, sighs, moans, and groans -- are an often-overlooked aspect of speech emotion recognition, but an important aspect of human vocal communication. One barrier to study of these interesting vocalizations is a lack of large datasets. I am pleased to introduce the EmoGator dataset, which consists of 32,130 samples from 357 speakers, 16.9654 hours of audio; each sample classified into one of 30 distinct emotion categories by the speaker. Several different approaches to construct classifiers to identify emotion categories will be discussed, and directions for future research will be suggested. Data set is available for download from https://github.com/fredbuhl/EmoGator.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;GD&#35757;&#32451;&#36807;&#31243;&#20013;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#36817;&#20284;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;Lipschitz&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#33021;&#22815;&#20135;&#29983;&#26368;&#20248;&#36895;&#29575;&#30340;&#23454;&#29992;&#26089;&#20572;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2212.13848</link><description>&lt;p&gt;
&#36890;&#36807;GD&#35757;&#32451;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;Lipschitz&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning Lipschitz Functions by GD-trained Shallow Overparameterized ReLU Neural Networks. (arXiv:2212.13848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;GD&#35757;&#32451;&#36807;&#31243;&#20013;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#36817;&#20284;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;Lipschitz&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#33021;&#22815;&#20135;&#29983;&#26368;&#20248;&#36895;&#29575;&#30340;&#23454;&#29992;&#26089;&#20572;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#27973;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#26102;&#23398;&#20064;&#20855;&#26377;&#21152;&#24615;&#22122;&#22768;&#30340;Lipschitz&#12289;&#19981;&#21487;&#24494;&#20998;&#12289;&#26377;&#30028;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#20026;&#36991;&#20813;&#23384;&#22312;&#22122;&#22768;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21040;&#25509;&#36817;0&#30340;&#35757;&#32451;&#35823;&#24046;&#26102;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20572;&#27490;&#36739;&#26089;&#30340;GD&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#19968;&#33268;&#24615;&#21644;&#26368;&#20248;&#36895;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;GD&#35757;&#32451;&#30340;&#26377;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#36817;&#20284;&#30340;&#35270;&#35282;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#35201;&#22312;ReLU&#28608;&#27963;&#20989;&#25968;&#24341;&#36215;&#30340;&#26680;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#65292;&#26576;&#20123;&#26089;&#20572;&#35268;&#21017;&#20445;&#35777;&#33021;&#22815;&#32473;&#20986;&#26368;&#20248;&#30340;&#36229;&#39069;&#39118;&#38505;&#36895;&#29575;&#65292;&#37027;&#20040;&#30456;&#21516;&#30340;&#35268;&#21017;&#23601;&#21487;&#20197;&#34987;&#29992;&#20110;&#23454;&#29616;&#22312;Lipschitz&#20989;&#25968;&#25152;&#32771;&#34385;&#30340;&#31867;&#20013;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#20540;&#36895;&#29575;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#20010;&#26080;&#38656;&#25968;&#25454;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#23454;&#38469;&#21560;&#24341;&#21147;&#20572;&#27490;&#20934;&#21017;&#65292;&#36825;&#20123;&#20934;&#21017;&#20135;&#29983;&#20102;&#26368;&#20248;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the ability of overparameterized shallow ReLU neural networks to learn Lipschitz, nondifferentiable, bounded functions with additive noise when trained by Gradient Descent (GD). To avoid the problem that in the presence of noise, neural networks trained to nearly zero training error are inconsistent in this class, we focus on the early-stopped GD which allows us to show consistency and optimal rates. In particular, we explore this problem from the viewpoint of the Neural Tangent Kernel (NTK) approximation of a GD-trained finite-width neural network. We show that whenever some early stopping rule is guaranteed to give an optimal rate (of excess risk) on the Hilbert space of the kernel induced by the ReLU activation function, the same rule can be used to achieve minimax optimal rate for learning on the class of considered Lipschitz functions by neural networks. We discuss several data-free and data-dependent practically appealing stopping rules that yield optimal rates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35745;&#31639;&#27888;&#21202;&#20313;&#39033;&#32423;&#25968;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#65292;&#24182;&#24212;&#29992;&#20110;&#21306;&#38388;&#35745;&#31639;&#12289;&#20248;&#21270;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2212.11429</link><description>&lt;p&gt;
&#33258;&#21160;&#35745;&#31639;&#27888;&#21202;&#20313;&#39033;&#32423;&#25968;&#65306;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#21644;&#26032;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Automatically Bounding the Taylor Remainder Series: Tighter Bounds and New Applications. (arXiv:2212.11429v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35745;&#31639;&#27888;&#21202;&#20313;&#39033;&#32423;&#25968;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#65292;&#24182;&#24212;&#29992;&#20110;&#21306;&#38388;&#35745;&#31639;&#12289;&#20248;&#21270;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35745;&#31639;&#27888;&#21202;&#20313;&#39033;&#32423;&#25968;&#30340;&#31639;&#27861;&#12290;&#22312;&#26631;&#37327;&#20989;&#25968; $f:\mathbb{R}\to\mathbb{R}$ &#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20197;&#21442;&#32771;&#28857; $x_0$&#12289;&#20449;&#20219;&#22495; $[a,b]$ &#21644;&#25972;&#25968; $k\geq1$ &#20026;&#36755;&#20837;&#65292;&#24182;&#36820;&#22238;&#19968;&#20010;&#21306;&#38388; $I$&#65292;&#20351;&#24471;&#23545;&#20110;&#25152;&#26377; $x\in[a,b]$, $f(x)\sum_{i=0}^{k-1}\frac{1}{i!}f^{(i)}(x_0)(x-x_0)^i \in I(x-x_0)^k$&#12290;&#19982;&#33258;&#21160;&#24494;&#20998;&#31867;&#20284;&#65292;&#20989;&#25968; $f$ &#30340;&#36755;&#20837;&#24517;&#39035;&#20026;&#24050;&#30693;&#30340;&#21407;&#23376;&#20989;&#25968;&#12290;&#22312;&#31639;&#27861;&#30340;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#22810;&#31181;&#24120;&#29992;&#30340;&#21021;&#31561;&#20989;&#25968;&#65288;&#22914; $\exp$&#65292;$\log$&#65289;&#23548;&#20986;&#27888;&#21202;&#20313;&#39033;&#32423;&#25968;&#30340;&#23574;&#38160;&#22810;&#39033;&#24335;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21306;&#38388;&#31639;&#26415;&#30340;&#27888;&#21202;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#36882;&#24402;&#22320;&#32452;&#21512;&#20803;&#20989;&#25968;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithm for automatically bounding the Taylor remainder series. In the special case of a scalar function $f: \mathbb{R} \to \mathbb{R}$, our algorithm takes as input a reference point $x_0$, trust region $[a, b]$, and integer $k \ge 1$, and returns an interval $I$ such that $f(x) \sum_{i=0}^{k-1} \frac {1} {i!} f^{(i)}(x_0) (x - x_0)^i \in I (x - x_0)^k$ for all $x \in [a, b]$. As in automatic differentiation, the function $f$ is provided to the algorithm in symbolic form, and must be composed of known atomic functions.  At a high level, our algorithm has two steps. First, for a variety of commonly-used elementary functions (e.g., $\exp$, $\log$), we derive sharp polynomial upper and lower bounds on the Taylor remainder series. We then recursively combine the bounds for the elementary functions using an interval arithmetic variant of Taylor-mode automatic differentiation. Our algorithm can make efficient use of machine learning hardware accelerators, and we provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.09849</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#24182;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#23454;&#29616;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#26500;&#24314;&#19979;&#28216;NLP&#27169;&#22411;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#24050;&#32463;&#21487;&#29992;&#65292;&#20294;&#20854;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#30693;&#35782;&#20135;&#26435;&#38382;&#39064;&#12290;&#36825;&#23601;&#36896;&#25104;&#20102;&#36328;&#27169;&#22411;&#34701;&#21512;&#30693;&#35782;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24314;&#31435;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#21512;&#24182;&#30340;&#38382;&#39064;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#21512;&#24182;&#27169;&#22411;&#65292;&#30001;&#26435;&#37325;&#24341;&#23548;&#65292;&#20197;&#26368;&#23567;&#21270;&#21512;&#24182;&#27169;&#22411;&#21644;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#24322;&#12290;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22914;Fisher&#21152;&#26435;&#24179;&#22343;&#25110;&#27169;&#22411;&#38598;&#25104;&#31561;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#22810;&#35821;&#35328;&#24494;&#35843;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#30417;&#30563;&#26041;&#27861;&#23545;&#20110;Vision Transformers&#30340;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#22836;&#31867;&#22411;&#12290;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;Vision Transformers&#32780;&#35328;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2212.03862</link><description>&lt;p&gt;
&#25945;&#32946;&#24456;&#37325;&#35201;&#65306;&#25506;&#31350;&#30417;&#30563;&#23545;Vision Transformers&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teaching Matters: Investigating the Role of Supervision in Vision Transformers. (arXiv:2212.03862v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#30417;&#30563;&#26041;&#27861;&#23545;&#20110;Vision Transformers&#30340;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#22836;&#31867;&#22411;&#12290;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;Vision Transformers&#32780;&#35328;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs)&#22312;&#36817;&#24180;&#20013;&#24191;&#21463;&#27426;&#36814;&#65292;&#24182;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#19979;&#30340;&#34892;&#20026;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#19981;&#21516;&#30417;&#30563;&#26041;&#27861;&#35757;&#32451;&#30340;ViTs&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#27880;&#24847;&#21147;&#12289;&#34920;&#31034;&#21644;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#23398;&#20064;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;ViT&#34892;&#20026;&#22312;&#19981;&#21516;&#35757;&#32451;&#27169;&#24335;&#19979;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26032;&#20986;&#29616;&#30340;Offset Local Attention Heads&#65292;&#36825;&#26159;&#19968;&#31181;&#25105;&#20204;&#20043;&#21069;&#27809;&#26377;&#24847;&#35782;&#21040;&#30340;&#33258;&#27880;&#24847;&#21147;&#22836;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ViTs&#38750;&#24120;&#28789;&#27963;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#27861;&#23398;&#20064;&#22788;&#29702;&#26412;&#22320;&#21644;&#20840;&#23616;&#20449;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#27604;&#33258;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#29305;&#24449;&#19982;&#26174;&#24335;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#29305;&#24449;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#30417;&#30563;&#26041;&#24335;&#23545;&#20110;ViT&#23398;&#20064;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#22836;&#31867;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23545;&#20110;Vision Transformers&#32780;&#35328;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have gained significant popularity in recent years and have proliferated into many applications. However, their behavior under different learning paradigms is not well explored. We compare ViTs trained through different methods of supervision, and show that they learn a diverse range of behaviors in terms of their attention, representations, and downstream performance. We also discover ViT behaviors that are consistent across supervision, including the emergence of Offset Local Attention Heads. These are self-attention heads that attend to a token adjacent to the current token with a fixed directional offset, a phenomenon that to the best of our knowledge has not been highlighted in any prior work. Our analysis shows that ViTs are highly flexible and learn to process local and global information in different orders depending on their training method. We find that contrastive self-supervised methods learn features that are competitive with explicitly supervise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#23545;&#33021;&#28304;&#65288;&#36127;&#33655;&#12289;&#20809;&#20239;&#25110;&#39118;&#21147;&#65289;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.02977</link><description>&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#33021;&#37327;&#27010;&#29575;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models for probabilistic energy forecasting. (arXiv:2212.02977v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#23545;&#33021;&#28304;&#65288;&#36127;&#33655;&#12289;&#20809;&#20239;&#25110;&#39118;&#21147;&#65289;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24773;&#26223;&#30340;&#27010;&#29575;&#39044;&#27979;&#23545;&#20110;&#20915;&#31574;&#32773;&#22312;&#22788;&#29702;&#38388;&#27463;&#24615;&#21487;&#20877;&#29983;&#33021;&#28304;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31867;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#26368;&#36817;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26410;&#26377;&#28436;&#31034;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36127;&#33655;&#12289;&#20809;&#20239;&#25110;&#39118;&#21147;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#32780;&#36825;&#26159;&#24212;&#23545;&#30005;&#21147;&#31995;&#32479;&#24212;&#29992;&#20013;&#30340;&#26032;&#25361;&#25112;&#25152;&#24517;&#38656;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#31181;&#27169;&#22411;&#22312;&#21033;&#29992;&#20840;&#29699;&#33021;&#28304;&#39044;&#27979;&#22823;&#36187;2014&#30340;&#20844;&#24320;&#25968;&#25454;&#36827;&#34892;&#33021;&#28304;&#39044;&#27979;&#30340;&#39318;&#27425;&#23454;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65288;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#27491;&#24120;&#21270;&#27969;&#65289;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scenario-based probabilistic forecasts have become vital for decision-makers in handling intermittent renewable energies. This paper presents a recent promising deep learning generative approach called denoising diffusion probabilistic models. It is a class of latent variable models which have recently demonstrated impressive results in the computer vision community. However, to our knowledge, there has yet to be a demonstration that they can generate high-quality samples of load, PV, or wind power time series, crucial elements to face the new challenges in power systems applications. Thus, we propose the first implementation of this model for energy forecasting using the open data of the Global Energy Forecasting Competition 2014. The results demonstrate this approach is competitive with other state-of-the-art deep learning generative models, including generative adversarial networks, variational autoencoders, and normalizing flows.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;PFL&#26694;&#26550;PHN-HVI&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#30340;&#35299;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#36825;&#20123;&#35299;&#23450;&#20041;&#30340;&#36229;&#20307;&#31215;&#25351;&#26631;&#26469;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2212.01130</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#26412;&#36229;&#32593;&#32476;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Pareto Front Learning via Multi-Sample Hypernetworks. (arXiv:2212.01130v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;PFL&#26694;&#26550;PHN-HVI&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#30340;&#35299;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#36825;&#20123;&#35299;&#23450;&#20041;&#30340;&#36229;&#20307;&#31215;&#25351;&#26631;&#26469;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#32047;&#25176;&#21069;&#27839;&#23398;&#20064;(PFL)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#24471;&#20174;&#32473;&#23450;&#26435;&#34913;&#21521;&#37327;&#21040;&#24085;&#32047;&#25176;&#21069;&#27839;&#35299;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;(MOO)&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PFL&#26041;&#27861;&#24573;&#30053;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#33719;&#24471;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;PFL&#26694;&#26550;&#65292;&#21363;PHN-HVI&#65292;&#23427;&#20351;&#29992;&#36229;&#32593;&#32476;&#20174;&#22810;&#26679;&#30340;&#26435;&#34913;&#20559;&#22909;&#38598;&#29983;&#25104;&#22810;&#20010;&#35299;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#36825;&#20123;&#35299;&#23450;&#20041;&#30340;&#36229;&#20307;&#31215;&#25351;&#26631;&#26469;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#22810;&#20010;MOO&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;PFL&#26041;&#27861;&#65292;PHN-HVI&#22312;&#24085;&#32047;&#25176;&#21069;&#27839;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pareto Front Learning (PFL) was recently introduced as an effective approach to obtain a mapping function from a given trade-off vector to a solution on the Pareto front, which solves the multi-objective optimization (MOO) problem. Due to the inherent trade-off between conflicting objectives, PFL offers a flexible approach in many scenarios in which the decision makers can not specify the preference of one Pareto solution over another, and must switch between them depending on the situation. However, existing PFL methods ignore the relationship between the solutions during the optimization process, which hinders the quality of the obtained front. To overcome this issue, we propose a novel PFL framework namely PHN-HVI, which employs a hypernetwork to generate multiple solutions from a set of diverse trade-off preferences and enhance the quality of the Pareto front by maximizing the Hypervolume indicator defined by these solutions. The experimental results on several MOO machine learning
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;BigTransfer&#65288;BiT&#65289;&#31639;&#27861;&#36827;&#34892;&#40657;&#32032;&#30244;&#22270;&#20687;&#20998;&#31867;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.11872</link><description>&lt;p&gt;
&#20351;&#29992;BigTransfer&#65288;BiT&#65289;&#36827;&#34892;&#40657;&#32032;&#30244;&#22522;&#24213;&#32454;&#32990;&#30179;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Melanocytic Nevus Images using BigTransfer (BiT). (arXiv:2211.11872v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11872
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;BigTransfer&#65288;BiT&#65289;&#31639;&#27861;&#36827;&#34892;&#40657;&#32032;&#30244;&#22270;&#20687;&#20998;&#31867;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30382;&#32932;&#30284;&#26159;&#19968;&#31181;&#33268;&#21629;&#30340;&#30142;&#30149;&#65292;&#27599;&#24180;&#22842;&#21435;&#20102;&#20154;&#31867;&#30340;&#29983;&#21629;&#12290;&#26377;&#33394;&#30382;&#32932;&#22270;&#20687;&#22312;&#19981;&#21516;&#30340;&#30382;&#32932;&#30149;&#21464;&#65288;&#22914;&#40657;&#32032;&#30244;&#21644;&#30179;&#65289;&#20043;&#38388;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#24471;&#35782;&#21035;&#21644;&#35786;&#26029;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#40657;&#33394;&#32032;&#30179;&#21487;&#33021;&#20250;&#25104;&#29087;&#24182;&#23548;&#33268;&#33268;&#21629;&#30340;&#40657;&#33394;&#32032;&#30244;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;&#31649;&#29702;&#21327;&#35758;&#28041;&#21450;&#21040;&#21435;&#38500;&#37027;&#20123;&#30475;&#36215;&#26469;&#20196;&#20154;&#29983;&#30031;&#30340;&#30179;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#20998;&#31867;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#20808;&#21069;&#38024;&#23545;&#19981;&#21516;&#38382;&#39064;&#38472;&#36848;&#32780;&#36827;&#34892;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;ResNet&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;BigTransfer&#65288;BiT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin cancer is a fatal disease that takes a heavy toll over human lives annually. The colored skin images show a significant degree of resemblance between different skin lesions such as melanoma and nevus, making identification and diagnosis more challenging. Melanocytic nevi may mature to cause fatal melanoma. Therefore, the current management protocol involves the removal of those nevi that appear intimidating. However, this necessitates resilient classification paradigms for classifying benign and malignant melanocytic nevi. Early diagnosis necessitates a dependable automated system for melanocytic nevi classification to render diagnosis efficient, timely, and successful. An automated classification algorithm is proposed in the given research. A neural network previously-trained on a separate problem statement is leveraged in this technique for classifying melanocytic nevus images. The suggested method uses BigTransfer (BiT), a ResNet-based transfer learning approach for classifying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#30340;&#27169;&#31946;-&#38160;&#21270;&#36807;&#31243;&#27169;&#22411;&#65288;BSPM&#65289;&#65292;&#24182;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#22312;&#26174;&#24335;&#21644;&#38544;&#24335;&#21453;&#39304;&#20013;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.09324</link><description>&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#30340;&#27169;&#31946;-&#38160;&#21270;&#36807;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Blurring-Sharpening Process Models for Collaborative Filtering. (arXiv:2211.09324v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#30340;&#27169;&#31946;-&#38160;&#21270;&#36807;&#31243;&#27169;&#22411;&#65288;BSPM&#65289;&#65292;&#24182;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#22312;&#26174;&#24335;&#21644;&#38544;&#24335;&#21453;&#39304;&#20013;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#26368;&#22522;&#26412;&#30340;&#20027;&#39064;&#20043;&#19968;&#12290;&#20174;&#30697;&#38453;&#20998;&#35299;&#21040;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#12290;&#22312;&#22270;&#36807;&#28388;&#26041;&#27861;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#31946;-&#38160;&#21270;&#36807;&#31243;&#27169;&#22411;&#65288;BSPM&#65289;&#30340;&#27010;&#24565;&#12290;SGM&#21644;BSPM&#20849;&#20139;&#30456;&#21516;&#30340;&#22788;&#29702;&#21746;&#23398;&#65292;&#21363;&#22312;&#23558;&#21407;&#22987;&#20449;&#24687;&#39318;&#20808;&#25200;&#20081;&#28982;&#21518;&#24674;&#22797;&#21040;&#21407;&#22987;&#24418;&#24335;&#30340;&#36807;&#31243;&#20013;&#21487;&#20197;&#21457;&#29616;&#26032;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#22312;SGM&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26032;&#22270;&#20687;&#65289;&#12290;&#28982;&#32780;&#65292;SGM&#21644;&#25105;&#20204;&#30340;BSPM&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#26368;&#20248;&#25200;&#21160;&#21644;&#24674;&#22797;&#36807;&#31243;&#23384;&#22312;&#26681;&#26412;&#19978;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;BSPM&#19982;SGM&#20855;&#26377;&#19981;&#21516;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27010;&#24565;&#19981;&#20165;&#29702;&#35770;&#19978;&#21253;&#25324;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#26174;&#24335;&#21644;&#38544;&#24335;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21484;&#22238;&#29575;&#21644;NDCG&#26041;&#38754;&#20063;&#20248;&#20110;&#23427;&#20204;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#20855;&#26377;&#19981;&#21516;&#27169;&#31946;&#21644;&#38160;&#21270;&#28388;&#27874;&#22120;&#35774;&#32622;&#30340;BSPM&#65292;&#24182;&#25512;&#23548;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#20197;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#12290;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering is one of the most fundamental topics for recommender systems. Various methods have been proposed for collaborative filtering, ranging from matrix factorization to graph convolutional methods. Being inspired by recent successes of graph filtering-based methods and score-based generative models (SGMs), we present a novel concept of blurring-sharpening process model (BSPM). SGMs and BSPMs share the same processing philosophy that new information can be discovered (e.g., new images are generated in the case of SGMs) while original information is first perturbed and then recovered to its original form. However, SGMs and our BSPMs deal with different types of information, and their optimal perturbation and recovery processes have fundamental discrepancies. Therefore, our BSPMs have different forms from SGMs. In addition, our concept not only theoretically subsumes many existing collaborative filtering models but also outperforms them in terms of Recall and NDCG in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#24037;&#19994;&#27969;&#31243;&#20013;&#33719;&#21462;&#21644;&#20934;&#22791;&#39640;&#36136;&#37327;&#30340;&#25805;&#20316;&#25968;&#25454;&#65292;&#24182;&#24378;&#35843;&#25968;&#25454;&#39044;&#22788;&#29702;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.06440</link><description>&lt;p&gt;
&#25968;&#25454;&#37327;&#19981;&#22914;&#25968;&#25454;&#36136;&#37327;&#65306;&#24037;&#33402;&#20998;&#26512;&#20013;&#30340;&#38519;&#38449;&#21644;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Data Quality Over Quantity: Pitfalls and Guidelines for Process Analytics. (arXiv:2211.06440v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#24037;&#19994;&#27969;&#31243;&#20013;&#33719;&#21462;&#21644;&#20934;&#22791;&#39640;&#36136;&#37327;&#30340;&#25805;&#20316;&#25968;&#25454;&#65292;&#24182;&#24378;&#35843;&#25968;&#25454;&#39044;&#22788;&#29702;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#36807;&#31243;&#25511;&#21046;&#12289;&#24037;&#33402;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#37117;&#28041;&#21450;&#21040;&#33719;&#21462;&#21644;&#20934;&#22791;&#25968;&#25454;&#12290;&#34429;&#28982;&#25991;&#29486;&#32463;&#24120;&#24378;&#35843;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#30340;&#22797;&#26434;&#24314;&#27169;&#25216;&#26415;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#24403;&#24037;&#19994;&#26696;&#20363;&#30740;&#31350;&#34987;&#21457;&#24067;&#26102;&#65292;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#26377;&#20851;&#25968;&#25454;&#33719;&#21462;&#21644;&#20934;&#22791;&#30340;&#37325;&#35201;&#32454;&#33410;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#33719;&#21462;&#21644;&#20934;&#22791;&#25805;&#20316;&#25968;&#25454;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#36861;&#27714;&#24037;&#19994;&#27969;&#31243;&#20013;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant portion of the effort involved in advanced process control, process analytics, and machine learning involves acquiring and preparing data. Literature often emphasizes increasingly complex modelling techniques with incremental performance improvements. However, when industrial case studies are published they often lack important details on data acquisition and preparation. Although data pre-processing is unfairly maligned as trivial and technically uninteresting, in practice it has an out-sized influence on the success of real-world artificial intelligence applications. This work describes best practices for acquiring and preparing operating data to pursue data-driven modelling and control opportunities in industrial processes. We present practical considerations for pre-processing industrial time series data to inform the efficient development of reliable soft sensors that provide valuable process insights.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#21644;&#26102;&#38388;&#24310;&#36831;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#37096;&#20998;&#35266;&#27979;&#31995;&#32479;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.05992</link><description>&lt;p&gt;
&#24310;&#36831;&#23884;&#20837;&#30340;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65306;&#19968;&#31181;&#37096;&#20998;&#35266;&#27979;&#31995;&#32479;&#30340;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Delay Embedded Echo-State Network: A Predictor for Partially Observed Systems. (arXiv:2211.05992v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#21644;&#26102;&#38388;&#24310;&#36831;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#37096;&#20998;&#35266;&#27979;&#31995;&#32479;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#25968;&#25454;&#30340;&#37096;&#20998;&#35266;&#27979;&#31995;&#32479;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#23436;&#25972;&#29366;&#24577;&#35757;&#32451;&#25968;&#25454;&#30340;&#21160;&#24577;&#39044;&#27979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#37096;&#20998;&#35266;&#27979;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20250;&#24102;&#26469;&#26174;&#30528;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#65288;ESN&#65289;&#21644;&#37096;&#20998;&#35266;&#27979;&#29366;&#24577;&#30340;&#26102;&#38388;&#24310;&#36831;&#23884;&#20837;&#26469;&#36827;&#34892;&#37096;&#20998;&#35266;&#27979;&#31995;&#32479;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Taken&#30340;&#23884;&#20837;&#23450;&#29702;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#24378;&#21487;&#35266;&#27979;&#24615;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#19977;&#20010;&#31995;&#32479;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#65306;&#20004;&#20010;&#28151;&#27788;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#19968;&#32452;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of data-driven prediction of partially observed systems using a recurrent neural network. While neural network based dynamic predictors perform well with full-state training data, prediction with partial observation during training phase poses a significant challenge. Here a predictor for partial observations is developed using an echo-state network (ESN) and time delay embedding of the partially observed state. The proposed method is theoretically justified with Taken's embedding theorem and strong observability of a nonlinear system. The efficacy of the proposed method is demonstrated on three systems: two synthetic datasets from chaotic dynamical systems and a set of real-time traffic data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#30417;&#25511;&#26041;&#27861;&#65292;&#31216;&#20026;&#23450;&#37327;&#39044;&#27979;&#30417;&#25511;&#65288;QPM&#65289;&#65292;&#33021;&#22815;&#25903;&#25345;&#20351;&#29992;Signal Temporal Logic&#65288;STL&#65289;&#25551;&#36848;&#30340;&#38543;&#26426;&#36807;&#31243;&#21644;&#20016;&#23500;&#35268;&#33539;&#65292;&#23427;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#28385;&#36275;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#27010;&#29575;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.02375</link><description>&lt;p&gt;
&#22522;&#20110;STL&#30340;&#38543;&#26426;&#36807;&#31243;&#30340;&#20381;&#20174;&#24615;&#37327;&#21270;&#39044;&#27979;&#30417;&#25511;
&lt;/p&gt;
&lt;p&gt;
Conformal Quantitative Predictive Monitoring of STL Requirements for Stochastic Processes. (arXiv:2211.02375v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#30417;&#25511;&#26041;&#27861;&#65292;&#31216;&#20026;&#23450;&#37327;&#39044;&#27979;&#30417;&#25511;&#65288;QPM&#65289;&#65292;&#33021;&#22815;&#25903;&#25345;&#20351;&#29992;Signal Temporal Logic&#65288;STL&#65289;&#25551;&#36848;&#30340;&#38543;&#26426;&#36807;&#31243;&#21644;&#20016;&#23500;&#35268;&#33539;&#65292;&#23427;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#28385;&#36275;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#27010;&#29575;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#39044;&#27979;&#30417;&#25511;&#65288;PM&#65289;&#30340;&#38382;&#39064;&#65292;&#21363;&#20174;&#24403;&#21069;&#31995;&#32479;&#29366;&#24577;&#39044;&#27979;&#25152;&#38656;&#23646;&#24615;&#30340;&#28385;&#36275;&#24773;&#20917;&#12290;&#30001;&#20110;&#20854;&#23545;&#20110;&#36816;&#34892;&#26102;&#23433;&#20840;&#20445;&#38556;&#21644;&#22312;&#32447;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#65292;PM&#26041;&#27861;&#38656;&#35201;&#39640;&#25928;&#65292;&#20197;&#20415;&#33021;&#22815;&#22312;&#39044;&#27979;&#21040;&#36829;&#35268;&#26102;&#21450;&#26102;&#36827;&#34892;&#24178;&#39044;&#65292;&#21516;&#26102;&#25552;&#20379;&#27491;&#30830;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#23450;&#37327;&#39044;&#27979;&#30417;&#25511;&#65288;QPM&#65289;&#8221;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25903;&#25345;STL&#20013;&#30340;&#38543;&#26426;&#36807;&#31243;&#21644;&#20016;&#23500;&#35268;&#33539;&#30340;PM&#26041;&#27861;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;PM&#25216;&#26415;&#36890;&#36807;&#39044;&#27979;&#26159;&#21542;&#28385;&#36275;&#26576;&#20123;&#23646;&#24615;$\phi$&#19981;&#21516;&#65292;QPM&#36890;&#36807;&#39044;&#27979;$\phi$&#30340;&#23450;&#37327;&#65288;&#20063;&#31216;&#20026;&#40065;&#26834;&#24615;&#65289;STL&#35821;&#20041;&#26469;&#37327;&#21270;&#28385;&#36275;&#31243;&#24230;&#12290;QPM&#23548;&#20986;&#20102;&#39640;&#25928;&#35745;&#31639;&#19988;&#20855;&#26377;&#27010;&#29575;&#20445;&#35777;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#22312;&#27010;&#29575;&#19978;&#35206;&#30422;&#20102;&#19982;&#31995;&#32479;&#38543;&#26426;&#28436;&#21270;&#30456;&#20851;&#30340;STL&#40065;&#26834;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of predictive monitoring (PM), i.e., predicting at runtime the satisfaction of a desired property from the current system's state. Due to its relevance for runtime safety assurance and online control, PM methods need to be efficient to enable timely interventions against predicted violations, while providing correctness guarantees. We introduce \textit{quantitative predictive monitoring (QPM)}, the first PM method to support stochastic processes and rich specifications given in Signal Temporal Logic (STL). Unlike most of the existing PM techniques that predict whether or not some property $\phi$ is satisfied, QPM provides a quantitative measure of satisfaction by predicting the quantitative (aka robust) STL semantics of $\phi$. QPM derives prediction intervals that are highly efficient to compute and with probabilistic guarantees, in that the intervals cover with arbitrary probability the STL robustness values relative to the stochastic evolution of the system. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#23884;&#20837;&#27169;&#22411;UniASM&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#30340;&#35757;&#32451;&#20219;&#21153;&#65292;&#20351;&#24471;&#29983;&#25104;&#21521;&#37327;&#30340;&#31354;&#38388;&#20998;&#24067;&#26356;&#21152;&#22343;&#21248;&#65292;&#30452;&#25509;&#21487;&#20197;&#22312;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#29992;&#20110;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#36827;&#21046;&#20989;&#25968;tokenization&#26041;&#27861;&#65292;&#32531;&#35299;&#20102;&#35789;&#27719;&#22806;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#24471;&#21040;&#20102;&#19968;&#20123;&#26032;&#30340;&#26377;&#20215;&#20540;&#30340;&#21457;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;UniASM&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.01144</link><description>&lt;p&gt;
UniASM&#65306;&#26080;&#38656;&#24494;&#35843;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
UniASM: Binary Code Similarity Detection without Fine-tuning. (arXiv:2211.01144v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01144
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#23884;&#20837;&#27169;&#22411;UniASM&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#30340;&#35757;&#32451;&#20219;&#21153;&#65292;&#20351;&#24471;&#29983;&#25104;&#21521;&#37327;&#30340;&#31354;&#38388;&#20998;&#24067;&#26356;&#21152;&#22343;&#21248;&#65292;&#30452;&#25509;&#21487;&#20197;&#22312;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#29992;&#20110;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#36827;&#21046;&#20989;&#25968;tokenization&#26041;&#27861;&#65292;&#32531;&#35299;&#20102;&#35789;&#27719;&#22806;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#24471;&#21040;&#20102;&#19968;&#20123;&#26032;&#30340;&#26377;&#20215;&#20540;&#30340;&#21457;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;UniASM&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#26816;&#27979;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#20108;&#36827;&#21046;&#20998;&#26512;&#20219;&#21153;&#65292;&#22914;&#28431;&#27934;&#25628;&#32034;&#12289;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#12289;&#20811;&#38534;&#26816;&#27979;&#21644;&#34917;&#19969;&#20998;&#26512;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#23884;&#20837;&#27169;&#22411;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26356;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#23884;&#20837;&#27169;&#22411;UniASM&#65292;&#29992;&#20110;&#23398;&#20064;&#20108;&#36827;&#21046;&#20989;&#25968;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#30340;&#35757;&#32451;&#20219;&#21153;&#65292;&#20351;&#24471;&#29983;&#25104;&#21521;&#37327;&#30340;&#31354;&#38388;&#20998;&#24067;&#26356;&#21152;&#22343;&#21248;&#65292;&#30452;&#25509;&#21487;&#20197;&#22312;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#29992;&#20110;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#36827;&#21046;&#20989;&#25968;tokenization&#26041;&#27861;&#65292;&#22686;&#21152;&#20102;tokens&#30340;&#35821;&#20041;&#20449;&#24687;&#24182;&#32531;&#35299;&#20102;&#35789;&#27719;&#22806;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#19968;&#20123;&#26032;&#30340;&#26377;&#20215;&#20540;&#30340;&#21457;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;UniASM&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary code similarity detection (BCSD) is widely used in various binary analysis tasks such as vulnerability search, malware detection, clone detection, and patch analysis. Recent studies have shown that the learning-based binary code embedding models perform better than the traditional feature-based approaches. In this paper, we propose a novel transformer-based binary code embedding model named UniASM to learn representations of the binary functions. We design two new training tasks to make the spatial distribution of the generated vectors more uniform, which can be used directly in BCSD without any fine-tuning. In addition, we present a new tokenization approach for binary functions, which increases the token's semantic information and mitigates the out-of-vocabulary (OOV) problem. We conduct an in-depth analysis of the factors affecting model performance through ablation experiments and obtain some new and valuable findings. The experimental results show that UniASM outperforms th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#24230;&#31227;&#21160;&#30340;&#36830;&#25509;&#36710;&#36742;&#19979;&#65292;&#36793;&#32536;&#26381;&#21153;&#22120;&#21033;&#29992;&#23616;&#37096;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#30340;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#36890;&#36807;&#26435;&#37325;&#32452;&#21512;&#21644;&#23376;&#38598;&#36873;&#25321;&#26469;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#24182;&#26368;&#22823;&#21270;&#25104;&#21151;&#25509;&#25910;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15496</link><description>&lt;p&gt;
&#24102;&#36164;&#28304;&#32422;&#26463;&#30340;&#39640;&#24230;&#31227;&#21160;&#36830;&#25509;&#36710;&#36742;&#19979;&#30340;&#36710;&#32852;&#32593;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resource Constrained Vehicular Edge Federated Learning with Highly Mobile Connected Vehicles. (arXiv:2210.15496v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#24230;&#31227;&#21160;&#30340;&#36830;&#25509;&#36710;&#36742;&#19979;&#65292;&#36793;&#32536;&#26381;&#21153;&#22120;&#21033;&#29992;&#23616;&#37096;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#30340;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#36890;&#36807;&#26435;&#37325;&#32452;&#21512;&#21644;&#23376;&#38598;&#36873;&#25321;&#26469;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#24182;&#26368;&#22823;&#21270;&#25104;&#21151;&#25509;&#25910;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36710;&#32852;&#32593;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#65288;VEFL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#36793;&#32536;&#26381;&#21153;&#22120;&#21033;&#29992;&#39640;&#24230;&#31227;&#21160;&#30340;&#36830;&#25509;&#36710;&#36742;&#65288;CV&#65289;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#21644;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;&#65288;CPU&#65289;&#26469;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#25910;&#25947;&#20998;&#26512;&#34920;&#26126;&#65292;VEFL&#35757;&#32451;&#25439;&#22833;&#21462;&#20915;&#20110;&#25104;&#21151;&#25509;&#25910;CV&#36890;&#36807;&#38388;&#27463;&#24615;&#36710;&#36742;&#21040;&#22522;&#30784;&#35774;&#26045;&#65288;V2I&#65289;&#26080;&#32447;&#38142;&#36335;&#20256;&#36755;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#30001;&#20110;&#39640;&#24230;&#31227;&#21160;&#24615;&#65292;&#22312;&#20840;&#35774;&#22791;&#21442;&#19982;&#24773;&#20917;&#65288;FDPC&#65289;&#19979;&#65292;&#36793;&#32536;&#26381;&#21153;&#22120;&#26681;&#25454;CV&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#36887;&#30041;&#26102;&#38388;&#30340;&#21152;&#26435;&#32452;&#21512;&#32858;&#21512;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#25968;&#65292;&#32780;&#22312;&#37096;&#20998;&#35774;&#22791;&#21442;&#19982;&#24773;&#20917;&#65288;PDPC&#65289;&#19979;&#36873;&#25321;CV&#30340;&#23376;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22312;&#24310;&#36831;&#12289;&#33021;&#37327;&#21644;&#25104;&#26412;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#32852;&#21512;VEFL&#21644;&#26080;&#32447;&#25509;&#20837;&#25216;&#26415;&#65288;RAT&#65289;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#26368;&#22823;&#21270;&#25104;&#21151;&#25509;&#25910;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#27010;&#29575;&#12290;&#32771;&#34385;&#21040;&#20248;&#21270;&#38382;&#39064;&#26159;NP-hard&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#20026;&#24453;&#35299;&#20915;&#30340;&#23376;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a vehicular edge federated learning (VEFL) solution, where an edge server leverages highly mobile connected vehicles' (CVs') onboard central processing units (CPUs) and local datasets to train a global model. Convergence analysis reveals that the VEFL training loss depends on the successful receptions of the CVs' trained models over the intermittent vehicle-to-infrastructure (V2I) wireless links. Owing to high mobility, in the full device participation case (FDPC), the edge server aggregates client model parameters based on a weighted combination according to the CVs' dataset sizes and sojourn periods, while it selects a subset of CVs in the partial device participation case (PDPC). We then devise joint VEFL and radio access technology (RAT) parameters optimization problems under delay, energy and cost constraints to maximize the probability of successful reception of the locally trained models. Considering that the optimization problem is NP-hard, we decompose it i
&lt;/p&gt;</description></item><item><title>&#31526;&#21512;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#24212;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#38598;&#65292;&#21487;&#20197;&#32416;&#27491;&#30001;&#20110;&#27169;&#22411;&#35268;&#33539;&#19981;&#24403;&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#24102;&#26469;&#30340;&#20027;&#35266;&#19978;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#21644;&#34920;&#26684;&#25490;&#21517;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.12496</link><description>&lt;p&gt;
&#24102;&#26377;&#31526;&#21512;&#24615;&#39044;&#27979;&#38598;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Conformal Prediction Sets. (arXiv:2210.12496v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12496
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#24212;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#38598;&#65292;&#21487;&#20197;&#32416;&#27491;&#30001;&#20110;&#27169;&#22411;&#35268;&#33539;&#19981;&#24403;&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#24102;&#26469;&#30340;&#20027;&#35266;&#19978;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#21644;&#34920;&#26684;&#25490;&#21517;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20570;&#20986;&#20915;&#31574;&#30340;&#26222;&#36941;&#26041;&#27861;&#65292;&#24212;&#29992;&#21253;&#25324;&#22810;&#33218;&#32769;&#34382;&#26426;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#40657;&#30418;&#20248;&#21270;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#39044;&#26399;&#25928;&#29992;&#30340;&#20915;&#31574;(&#21363;&#30446;&#26631;&#20989;&#25968;&#26597;&#35810;)&#65292;&#35813;&#21518;&#39564;&#20998;&#24067;&#37327;&#21270;&#20102;&#26597;&#35810;&#32467;&#26524;&#30340;&#21487;&#20943;&#23569;&#30340;&#20808;&#39564;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22240;&#27169;&#22411;&#35268;&#33539;&#19981;&#24403;&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#21407;&#22240;&#65292;&#20027;&#35266;&#19978;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#21487;&#33021;&#32463;&#24120;&#21457;&#29983;&#12290;&#31526;&#21512;&#24615;&#39044;&#27979;&#26159;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#21363;&#20351;&#23545;&#20110;&#35268;&#33539;&#19981;&#33391;&#30340;&#27169;&#22411;&#20063;&#20855;&#26377;&#35206;&#30422;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#32416;&#27491;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#31616;&#21333;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31526;&#21512;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#23558;&#26597;&#35810;&#24341;&#23548;&#21040;&#27169;&#22411;&#39044;&#27979;&#20855;&#26377;&#20445;&#35777;&#26377;&#25928;&#24615;&#30340;&#25628;&#32034;&#31354;&#38388;&#21306;&#22495;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#22312;&#19968;&#32452;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#21644;&#34920;&#26684;&#25490;&#21517;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#20248;&#20110;&#26631;&#20934;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a coherent, ubiquitous approach to decision-making under uncertainty, with applications including multi-arm bandits, active learning, and black-box optimization. Bayesian optimization selects decisions (i.e. objective function queries) with maximal expected utility with respect to the posterior distribution of a Bayesian model, which quantifies reducible, epistemic uncertainty about query outcomes. In practice, subjectively implausible outcomes can occur regularly for two reasons: 1) model misspecification and 2) covariate shift. Conformal prediction is an uncertainty quantification method with coverage guarantees even for misspecified models and a simple mechanism to correct for covariate shift. We propose conformal Bayesian optimization, which directs queries towards regions of search space where the model predictions have guaranteed validity, and investigate its behavior on a suite of black-box optimization tasks and tabular ranking tasks. In many cases we f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ORCHID&#26694;&#26550;&#65292;&#23558;Ollivier-Ricci&#26354;&#29575;&#25512;&#24191;&#21040;&#36229;&#22270;&#39046;&#22495;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ORCHID&#26354;&#29575;&#23545;&#20110;&#36229;&#22270;&#20219;&#21153;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.12048</link><description>&lt;p&gt;
&#36229;&#22270;&#30340;Ollivier-Ricci&#26354;&#29575;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ollivier-Ricci Curvature for Hypergraphs: A Unified Framework. (arXiv:2210.12048v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ORCHID&#26694;&#26550;&#65292;&#23558;Ollivier-Ricci&#26354;&#29575;&#25512;&#24191;&#21040;&#36229;&#22270;&#39046;&#22495;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ORCHID&#26354;&#29575;&#23545;&#20110;&#36229;&#22270;&#20219;&#21153;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26354;&#29575;&#26159;&#19968;&#31181;&#24378;&#22823;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#19981;&#21464;&#37327;&#65292;&#36830;&#25509;&#20102;&#20960;&#20309;&#21644;&#25299;&#25169;&#12290;&#34429;&#28982;&#22312;&#27969;&#24418;&#21644;&#22270;&#30340;&#32972;&#26223;&#19979;&#65292;&#26354;&#29575;&#30340;&#25928;&#29992;&#24050;&#32463;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#24471;&#21040;&#20102;&#35777;&#23454;&#65292;&#20294;&#20854;&#22312;&#26032;&#20852;&#30340;&#36229;&#22270;&#39046;&#22495;&#30340;&#25512;&#24191;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#22270;&#19978;&#65292;Ollivier-Ricci&#26354;&#29575;&#36890;&#36807;Wasserstein&#36317;&#31163;&#24230;&#37327;&#38543;&#26426;&#28216;&#36208;&#20043;&#38388;&#30340;&#19981;&#21516;&#65292;&#20174;&#32780;&#23558;&#20960;&#20309;&#27010;&#24565;&#33853;&#23454;&#21040;&#27010;&#29575;&#35770;&#21644;&#26368;&#20248;&#36755;&#36816;&#30340;&#24605;&#24819;&#20013;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;ORCHID&#65292;&#19968;&#20010;&#23558;Ollivier-Ricci&#26354;&#29575;&#25512;&#24191;&#21040;&#36229;&#22270;&#30340;&#28789;&#27963;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#24471;&#26354;&#29575;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#23646;&#24615;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#36229;&#22270;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;ORCHID&#26354;&#29575;&#26082;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20063;&#26377;&#29992;&#20110;&#36827;&#34892;&#21508;&#31181;&#36229;&#22270;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging geometry and topology, curvature is a powerful and expressive invariant. While the utility of curvature has been theoretically and empirically confirmed in the context of manifolds and graphs, its generalization to the emerging domain of hypergraphs has remained largely unexplored. On graphs, the Ollivier-Ricci curvature measures differences between random walks via Wasserstein distances, thus grounding a geometric concept in ideas from probability theory and optimal transport. We develop ORCHID, a flexible framework generalizing Ollivier-Ricci curvature to hypergraphs, and prove that the resulting curvatures have favorable theoretical properties. Through extensive experiments on synthetic and real-world hypergraphs from different domains, we demonstrate that ORCHID curvatures are both scalable and useful to perform a variety of hypergraph tasks in practice.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#21487;&#21464;&#26657;&#20934;&#30340;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#22312;&#25968;&#25454;&#29305;&#24449;&#26041;&#38754;&#21363;&#20351;&#25317;&#26377;&#33391;&#22909; ECE &#30340;&#27169;&#22411;&#36824;&#26159;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#26657;&#20934;&#22833;&#35823;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#26816;&#27979;&#12289;&#21487;&#35270;&#21270;&#21644;&#23450;&#37327;&#21270;&#21487;&#21464;&#26657;&#20934;&#35823;&#24046;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2209.15154</link><description>&lt;p&gt;
&#21487;&#21464;&#26657;&#20934;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Variable-Based Calibration for Machine Learning Classifiers. (arXiv:2209.15154v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15154
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#21487;&#21464;&#26657;&#20934;&#30340;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#22312;&#25968;&#25454;&#29305;&#24449;&#26041;&#38754;&#21363;&#20351;&#25317;&#26377;&#33391;&#22909; ECE &#30340;&#27169;&#22411;&#36824;&#26159;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#26657;&#20934;&#22833;&#35823;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#26816;&#27979;&#12289;&#21487;&#35270;&#21270;&#21644;&#23450;&#37327;&#21270;&#21487;&#21464;&#26657;&#20934;&#35823;&#24046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#38656;&#35201;&#26377;&#33391;&#22909;&#26657;&#20934;&#20449;&#24515;&#20998;&#25968;&#20197;&#39044;&#27979;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#21464;&#26657;&#20934;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#27169;&#22411;&#22312;&#29305;&#23450;&#20852;&#36259;&#21464;&#37327;&#26041;&#38754;&#30340;&#35843;&#33410;&#23646;&#24615;&#65292;&#23558;&#20256;&#32479;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#25351;&#26631;&#22914;&#39044;&#26399;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#36827;&#34892;&#25512;&#24191;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#36341;&#22312;&#22810;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#21363;&#20351;&#25317;&#26377;&#25509;&#36817;&#23436;&#32654; ECE &#30340;&#27169;&#22411;&#22312;&#25968;&#25454;&#29305;&#24449;&#26041;&#38754;&#36824;&#26159;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#26657;&#20934;&#22833;&#35823;&#65292;&#32780;&#19988;&#36825;&#31181;&#26657;&#20934;&#22833;&#35823;&#21487;&#33021;&#22312;&#24212;&#29992;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#21518;&#36824;&#20250;&#25345;&#32493;&#23384;&#22312;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26816;&#27979;&#12289;&#21487;&#35270;&#21270;&#21644;&#23450;&#37327;&#21270;&#21487;&#21464;&#26657;&#20934;&#35823;&#24046;&#30340;&#31574;&#30053;&#12290;&#25509;&#30528;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#21069;&#24471;&#20998;&#26657;&#20934;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#24182;&#25506;&#35752;&#20102;&#28508;&#22312;&#30340;&#20462;&#25913;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of machine learning classifiers in high-stakes domains requires well-calibrated confidence scores for model predictions. In this paper we introduce the notion of variable-based calibration to characterize calibration properties of a model with respect to a variable of interest, generalizing traditional score-based metrics such as expected calibration error (ECE). In particular, we find that models with near-perfect ECE can exhibit significant miscalibration as a function of features of the data. We demonstrate this phenomenon both theoretically and in practice on multiple well-known datasets, and show that it can persist after the application of existing calibration methods. To mitigate this issue, we propose strategies for detection, visualization, and quantification of variable-based calibration error. We then examine the limitations of current score-based calibration methods and explore potential modifications. Finally, we discuss the implications of these findings, e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20117;&#27979;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#38750;&#23545;&#27604;&#24230;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#25968;&#25454;&#30340;&#26631;&#27880;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14750</link><description>&lt;p&gt;
&#38024;&#23545;&#20117;&#27979;&#25968;&#25454;&#30340;&#38750;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-contrastive representation learning for intervals from well logs. (arXiv:2209.14750v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20117;&#27979;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#38750;&#23545;&#27604;&#24230;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#25968;&#25454;&#30340;&#26631;&#27880;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30707;&#27833;&#21644;&#22825;&#28982;&#27668;&#34892;&#19994;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#26681;&#25454;&#38075;&#20117;&#25968;&#25454;&#20026;&#20117;&#27573;&#25552;&#20379;&#34920;&#31034;&#24418;&#24335;&#12290;&#20197;&#24448;&#30340;&#23581;&#35797;&#20027;&#35201;&#26159;&#26377;&#30417;&#30563;&#30340;&#65292;&#24182;&#19988;&#20851;&#27880;&#20110;&#30456;&#20284;&#24615;&#20219;&#21153;&#65292;&#21363;&#20272;&#35745;&#20117;&#27573;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#12290;&#25105;&#20204;&#24076;&#26395;&#22312;&#19981;&#20351;&#29992;&#24050;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#20854;&#20013;&#19968;&#20010;&#21487;&#33021;&#30340;&#26041;&#27861;&#26159;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#19982;&#26377;&#30417;&#30563;&#33539;&#24335;&#30456;&#21453;&#65292;&#36825;&#20010;&#26041;&#27861;&#23545;&#25968;&#25454;&#38656;&#35201;&#24456;&#23569;&#25110;&#32773;&#27809;&#26377;&#26631;&#31614;&#12290;&#29616;&#20170;&#65292;&#22823;&#22810;&#25968;SSL&#26041;&#27861;&#35201;&#20040;&#26159;&#23545;&#27604;&#30340;&#65292;&#35201;&#20040;&#26159;&#38750;&#23545;&#27604;&#30340;&#12290;&#23545;&#27604;&#26041;&#27861;&#20351;&#30456;&#20284;&#30340;&#65288;&#27491;&#65289;&#23545;&#35937;&#30340;&#34920;&#31034;&#21464;&#24471;&#26356;&#21152;&#25509;&#36817;&#65292;&#24182;&#23558;&#19981;&#21516;&#30340;&#65288;&#36127;&#65289;&#23545;&#35937;&#19982;&#20043;&#36317;&#31163;&#12290;&#30001;&#20110;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#30340;&#27491;&#36127;&#26631;&#27880;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#25552;&#20379;&#26356;&#24046;&#30340;&#24615;&#33021;&#12290;&#38750;&#23545;&#27604;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#27492;&#31867;&#26631;&#27880;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#23427;&#20204;&#20165;&#20351;&#29992;&#23481;&#26131;&#35782;&#21035;&#30340;&#30456;&#20284;&#23545;&#35937;&#23545;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation learning problem in the oil &amp; gas industry aims to construct a model that provides a representation based on logging data for a well interval. Previous attempts are mainly supervised and focus on similarity task, which estimates closeness between intervals. We desire to build informative representations without using supervised (labelled) data. One of the possible approaches is self-supervised learning (SSL). In contrast to the supervised paradigm, this one requires little or no labels for the data. Nowadays, most SSL approaches are either contrastive or non-contrastive. Contrastive methods make representations of similar (positive) objects closer and distancing different (negative) ones. Due to possible wrong marking of positive and negative pairs, these methods can provide an inferior performance. Non-contrastive methods don't rely on such labelling and are widespread in computer vision. They learn using only pairs of similar objects that are easier to identify in 
&lt;/p&gt;</description></item><item><title>TRBoost &#26159;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#65292;&#20351;&#29992;&#32422;&#26463;&#20108;&#27425;&#27169;&#22411;&#26469;&#36817;&#20284;&#30446;&#26631;&#24182;&#24212;&#29992;&#20449;&#36182;&#22495;&#31639;&#27861;&#26469;&#33719;&#24471;&#26032;&#30340;&#23398;&#20064;&#22120;&#65292;&#20855;&#26377;&#36866;&#29992;&#20110;&#20219;&#24847;&#25439;&#22833;&#20989;&#25968;&#30340;&#36890;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.13791</link><description>&lt;p&gt;
TRBoost: &#22522;&#20110;&#20449;&#36182;&#22495;&#26041;&#27861;&#30340;&#36890;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;
&lt;/p&gt;
&lt;p&gt;
TRBoost: A Generic Gradient Boosting Machine based on Trust-region Method. (arXiv:2209.13791v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13791
&lt;/p&gt;
&lt;p&gt;
TRBoost &#26159;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#65292;&#20351;&#29992;&#32422;&#26463;&#20108;&#27425;&#27169;&#22411;&#26469;&#36817;&#20284;&#30446;&#26631;&#24182;&#24212;&#29992;&#20449;&#36182;&#22495;&#31639;&#27861;&#26469;&#33719;&#24471;&#26032;&#30340;&#23398;&#20064;&#22120;&#65292;&#20855;&#26377;&#36866;&#29992;&#20110;&#20219;&#24847;&#25439;&#22833;&#20989;&#25968;&#30340;&#36890;&#29992;&#24615;&#21644;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#25552;&#21319;&#26426; (GBMs) &#21033;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#27888;&#21202;&#23637;&#24320;&#26174;&#33879;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21508;&#31181;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#21644;&#36890;&#29992;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#23545; GBMs &#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#23588;&#20854;&#26159;&#65292;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340; GBMs &#20351;&#29992;&#19968;&#38454;&#27888;&#21202;&#23637;&#24320;&#20197;&#30830;&#20445;&#36866;&#29992;&#20110;&#25152;&#26377;&#25439;&#22833;&#20989;&#25968;&#65292;&#32780;&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#30340; GBMs &#21033;&#29992;&#27491;&#23450;&#30340;&#40657;&#22622;&#30697;&#38453;&#33719;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#20197;&#29306;&#29298;&#36890;&#29992;&#24615;&#20026;&#20195;&#20215;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36890;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#65292;&#31216;&#20026; TRBoost&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;TRBoost &#20351;&#29992;&#19968;&#20010;&#32422;&#26463;&#20108;&#27425;&#27169;&#22411;&#26469;&#36817;&#20284;&#30446;&#26631;&#24182;&#24212;&#29992;&#20449;&#36182;&#22495;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#24182;&#33719;&#24471;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#22120;&#12290;&#19982;&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#30340; GBMs &#19981;&#21516;&#65292;TRBoost &#19981;&#35201;&#27714;&#40657;&#22622;&#30697;&#38453;&#26159;&#27491;&#23450;&#30340;&#65292;&#22240;&#27492;&#20801;&#35768;&#23427;&#29992;&#20110;&#20219;&#24847;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient Boosting Machines (GBMs) have demonstrated remarkable success in solving diverse problems by utilizing Taylor expansions in functional space. However, achieving a balance between performance and generality has posed a challenge for GBMs. In particular, gradient descent-based GBMs employ the first-order Taylor expansion to ensure applicability to all loss functions, while Newton's method-based GBMs use positive Hessian information to achieve superior performance at the expense of generality. To address this issue, this study proposes a new generic Gradient Boosting Machine called Trust-region Boosting (TRBoost). In each iteration, TRBoost uses a constrained quadratic model to approximate the objective and applies the Trust-region algorithm to solve it and obtain a new learner. Unlike Newton's method-based GBMs, TRBoost does not require the Hessian to be positive definite, thereby allowing it to be applied to arbitrary loss functions while still maintaining competitive performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PREF&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#20272;&#35745;&#30340;&#19977;&#32500;&#36816;&#21160;&#22330;&#20026;&#21487;&#39044;&#27979;&#24615;&#12290;&#35813;&#26694;&#26550;&#22312;&#22810;&#35270;&#22270;&#35774;&#32622;&#19979;&#24314;&#27169;&#21160;&#24577;&#22330;&#26223;&#20013;&#25152;&#26377;&#28857;&#30340;&#36816;&#21160;&#65292;&#24182;&#37319;&#29992;&#28508;&#22312;&#23884;&#20837;&#21644;&#39044;&#27979;&#32593;&#32476;&#26469;&#23454;&#29616;&#21487;&#39044;&#27979;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#36816;&#21160;&#22330;&#30340;&#21160;&#24577;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.10691</link><description>&lt;p&gt;
PREF: &#21487;&#39044;&#27979;&#24615;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#36816;&#21160;&#22330;
&lt;/p&gt;
&lt;p&gt;
PREF: Predictability Regularized Neural Motion Fields. (arXiv:2209.10691v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PREF&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#20272;&#35745;&#30340;&#19977;&#32500;&#36816;&#21160;&#22330;&#20026;&#21487;&#39044;&#27979;&#24615;&#12290;&#35813;&#26694;&#26550;&#22312;&#22810;&#35270;&#22270;&#35774;&#32622;&#19979;&#24314;&#27169;&#21160;&#24577;&#22330;&#26223;&#20013;&#25152;&#26377;&#28857;&#30340;&#36816;&#21160;&#65292;&#24182;&#37319;&#29992;&#28508;&#22312;&#23884;&#20837;&#21644;&#39044;&#27979;&#32593;&#32476;&#26469;&#23454;&#29616;&#21487;&#39044;&#27979;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#36816;&#21160;&#22330;&#30340;&#21160;&#24577;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#35270;&#35273;&#24212;&#29992;&#20013;&#65292;&#20102;&#35299;&#21160;&#24577;&#22330;&#26223;&#30340;&#19977;&#32500;&#36816;&#21160;&#23545;&#20110;&#20272;&#35745;&#27963;&#21160;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#29305;&#23450;&#20803;&#32032;&#65288;&#22914;&#20154;&#65289;&#30340;&#27963;&#21160;&#24773;&#20917;&#19978;&#12290;&#26412;&#25991;&#21033;&#29992;&#31070;&#32463;&#36816;&#21160;&#22330;&#20272;&#35745;&#22810;&#35270;&#22270;&#35774;&#32622;&#19979;&#25152;&#26377;&#28857;&#30340;&#36816;&#21160;&#12290;&#30001;&#20110;&#39068;&#33394;&#30456;&#20284;&#30340;&#28857;&#21644;&#39068;&#33394;&#21464;&#21270;&#30340;&#28857;&#30340;&#27169;&#31946;&#24615;&#65292;&#20174;&#21160;&#24577;&#22330;&#26223;&#30340;&#22810;&#35270;&#22270;&#25968;&#25454;&#20013;&#24314;&#27169;&#36816;&#21160;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#20272;&#35745;&#30340;&#36816;&#21160;&#27491;&#21017;&#21270;&#20026;&#21487;&#39044;&#27979;&#12290;&#22914;&#26524;&#24050;&#30693;&#20043;&#21069;&#20960;&#24103;&#30340;&#36816;&#21160;&#65292;&#21017;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#30340;&#36816;&#21160;&#24212;&#35813;&#26159;&#21487;&#39044;&#27979;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#28508;&#22312;&#23884;&#20837;&#19978;&#23545;&#20272;&#35745;&#30340;&#36816;&#21160;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#28982;&#21518;&#36890;&#36807;&#37319;&#29992;&#39044;&#27979;&#32593;&#32476;&#23545;&#23884;&#20837;&#36827;&#34892;&#21487;&#39044;&#27979;&#24615;&#32422;&#26463;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;PREF&#65288;&#21487;&#39044;&#27979;&#24615;&#27491;&#21017;&#21270;&#22330;&#65289;&#23454;&#29616;&#20102;&#19982;&#22522;&#20110;&#31070;&#32463;&#36816;&#21160;&#22330;&#30340;&#21160;&#24577;&#22330;&#26223;&#34920;&#31034;&#26368;&#20808;&#36827;&#30340;&#21516;&#31561;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowing the 3D motions in a dynamic scene is essential to many vision applications. Recent progress is mainly focused on estimating the activity of some specific elements like humans. In this paper, we leverage a neural motion field for estimating the motion of all points in a multiview setting. Modeling the motion from a dynamic scene with multiview data is challenging due to the ambiguities in points of similar color and points with time-varying color. We propose to regularize the estimated motion to be predictable. If the motion from previous frames is known, then the motion in the near future should be predictable. Therefore, we introduce a predictability regularization by first conditioning the estimated motion on latent embeddings, then by adopting a predictor network to enforce predictability on the embeddings. The proposed framework PREF (Predictability REgularized Fields) achieves on par or better results than state-of-the-art neural motion field-based dynamic scene representa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#35813;&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.06261</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#32034;&#39537;&#21160;&#26426;&#22120;&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#30340;&#25511;&#21046;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Real2Sim2Real Transfer for Control of Cable-driven Robots via a Differentiable Physics Engine. (arXiv:2209.06261v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#35813;&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#30001;&#22362;&#30828;&#30340;&#26438;&#21644;&#26580;&#36719;&#30340;&#32518;&#32499;&#32452;&#25104;&#65292;&#20855;&#26377;&#39640;&#24378;&#24230;&#37325;&#37327;&#27604;&#21644;&#26174;&#33879;&#30340;&#21464;&#24418;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#22320;&#24418;&#20013;&#33322;&#34892;&#24182;&#22312;&#20005;&#23803;&#30340;&#25758;&#20987;&#20013;&#23384;&#27963;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32500;&#24230;&#39640;&#12289;&#21160;&#21147;&#22797;&#26434;&#19988;&#32806;&#21512;&#32467;&#26500;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#25511;&#21046;&#12290;&#22522;&#20110;&#29289;&#29702;&#30340;&#20223;&#30495;&#26159;&#24320;&#21457;&#21487;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#23545;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21516;iable&#29289;&#29702;&#24341;&#25806;&#30340;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#30340;&#36716;&#31227;&#31574;&#30053;(R2S2R)&#12290;&#35813;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#21253;&#25324;&#29289;&#29702;&#23646;&#24615;&#30340;&#31163;&#32447;&#27979;&#37327;&#65292;&#22914;&#36136;&#37327;&#21644;&#20960;&#20309;&#20307;&#30340;&#21508;&#31181;&#26426;&#22120;&#20154;&#37096;&#20214;&#65292;&#20197;&#21450;&#20351;&#29992;&#38543;&#26426;&#25511;&#21046;&#31574;&#30053;&#30340;&#36712;&#36857;&#35266;&#23519;&#12290;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#25968;&#25454;&#65292;&#29289;&#29702;&#24341;&#25806;&#21487;&#20197;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#36825;&#31181;R2S2R&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#24320;&#21457;&#21487;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensegrity robots, composed of rigid rods and flexible cables, exhibit high strength-to-weight ratios and significant deformations, which enable them to navigate unstructured terrains and survive harsh impacts. They are hard to control, however, due to high dimensionality, complex dynamics, and a coupled architecture. Physics-based simulation is a promising avenue for developing locomotion policies that can be transferred to real robots. Nevertheless, modeling tensegrity robots is a complex task due to a substantial sim2real gap. To address this issue, this paper describes a Real2Sim2Real (R2S2R) strategy for tensegrity robots. This strategy is based on a differentiable physics engine that can be trained given limited data from a real robot. These data include offline measurements of physical properties, such as mass and geometry for various robot components, and the observation of a trajectory using a random control policy. With the data from the real robot, the engine can be iterativ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bayan&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#25110;&#36817;&#20284;&#20248;&#21270;&#27169;&#22359;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36820;&#22238;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#21306;&#65292;&#24182;&#19988;&#27604;&#20854;&#20182;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#24182;&#33021;&#22815;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#20934;&#30830;&#22320;&#25214;&#21040;&#22320;&#38754;&#30495;&#23454;&#31038;&#21306;&#12290;</title><link>http://arxiv.org/abs/2209.04562</link><description>&lt;p&gt;
Bayan&#31639;&#27861;&#65306;&#36890;&#36807;&#23545;&#27169;&#22359;&#24230;&#30340;&#31934;&#30830;&#21644;&#36817;&#20284;&#20248;&#21270;&#26469;&#26816;&#27979;&#32593;&#32476;&#20013;&#30340;&#31038;&#21306;
&lt;/p&gt;
&lt;p&gt;
The Bayan Algorithm: Detecting Communities in Networks Through Exact and Approximate Optimization of Modularity. (arXiv:2209.04562v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04562
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bayan&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#25110;&#36817;&#20284;&#20248;&#21270;&#27169;&#22359;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36820;&#22238;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#21306;&#65292;&#24182;&#19988;&#27604;&#20854;&#20182;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#24182;&#33021;&#22815;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#20934;&#30830;&#22320;&#25214;&#21040;&#22320;&#38754;&#30495;&#23454;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#32593;&#32476;&#31185;&#23398;&#20013;&#30340;&#32463;&#20856;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#20247;&#22810;&#26041;&#27861;&#20013;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#26368;&#22823;&#21270;&#27169;&#22359;&#24230;&#12290;&#23613;&#31649;&#21551;&#21457;&#24335;&#27169;&#22359;&#24230;&#26368;&#22823;&#21270;&#31639;&#27861;&#35774;&#35745;&#29702;&#24565;&#21644;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#24456;&#23569;&#36820;&#22238;&#26368;&#20339;&#20998;&#21306;&#25110;&#31867;&#20284;&#20998;&#21306;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#31639;&#27861;Bayan&#65292;&#23427;&#36820;&#22238;&#20855;&#26377;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#20998;&#21306;&#20445;&#35777;&#30340;&#20998;&#21306;&#12290;Bayan&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#20998;&#25903;&#38480;&#30028;&#26041;&#26696;&#65292;&#23427;&#35299;&#20915;&#20102;&#38382;&#39064;&#30340;&#25972;&#25968;&#35268;&#21010;&#20844;&#24335;&#20197;&#36798;&#21040;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;Bayan&#22312;&#21512;&#25104;&#22522;&#20934;&#21644;&#30495;&#23454;&#32593;&#32476;&#33410;&#28857;&#26631;&#31614;&#30340;&#26816;&#32034;&#22320;&#38754;&#30495;&#23454;&#31038;&#21306;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#27604;&#20854;&#20182;21&#31181;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#20998;&#21306;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is a classic problem in network science with extensive applications in various fields. Among numerous approaches, the most common method is modularity maximization. Despite their design philosophy and wide adoption, heuristic modularity maximization algorithms rarely return an optimal partition or anything similar. We propose a specialized algorithm, Bayan, which returns partitions with a guarantee of either optimality or proximity to an optimal partition. At the core of the Bayan algorithm is a branch-and-cut scheme that solves an integer programming formulation of the problem to optimality or approximate it within a factor. We demonstrate Bayan's distinctive accuracy and stability over 21 other algorithms in retrieving ground-truth communities in synthetic benchmarks and node labels in real networks. Bayan is several times faster than open-source and commercial solvers for modularity maximization making it capable of finding optimal partitions for instances that c
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#24310;&#36831;&#22870;&#21169;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#20048;&#35266;&#31639;&#27861;&#65292;&#21487;&#23454;&#29616;&#19968;&#20010;&#29420;&#31435;&#20110;&#26102;&#38388;&#30340;&#24809;&#32602;&#20989;&#25968;&#65292;&#38477;&#20302;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#38543;&#30528;&#26102;&#38388;&#22686;&#38271;&#32780;&#22686;&#21152;&#30340;&#24809;&#32602;&#20989;&#25968;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2207.10786</link><description>&lt;p&gt;
&#24310;&#36831;&#21453;&#39304;&#22312;&#24191;&#20041;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#30740;&#31350;&#20877;&#35775;
&lt;/p&gt;
&lt;p&gt;
Delayed Feedback in Generalised Linear Bandits Revisited. (arXiv:2207.10786v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10786
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#24310;&#36831;&#22870;&#21169;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#20048;&#35266;&#31639;&#27861;&#65292;&#21487;&#23454;&#29616;&#19968;&#20010;&#29420;&#31435;&#20110;&#26102;&#38388;&#30340;&#24809;&#32602;&#20989;&#25968;&#65292;&#38477;&#20302;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#38543;&#30528;&#26102;&#38388;&#22686;&#38271;&#32780;&#22686;&#21152;&#30340;&#24809;&#32602;&#20989;&#25968;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#22870;&#21169;&#20960;&#20046;&#24635;&#26159;&#34987;&#24310;&#36831;&#65292;&#23548;&#33268;&#35201;&#27714;&#21363;&#26102;&#22870;&#21169;&#30340;&#27169;&#22411;&#38590;&#20197;&#24212;&#29992;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;&#22312;&#24191;&#20041;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#24310;&#36831;&#22870;&#21169;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#20048;&#35266;&#31639;&#27861;&#36866;&#24212;&#24310;&#36831;&#21453;&#39304;&#39046;&#22495;&#33021;&#22815;&#26377;&#19968;&#20010;&#19982;&#26102;&#38388;&#26080;&#20851;&#30340;&#24809;&#32602;&#20989;&#25968;&#12290;&#36825;&#27604;&#29616;&#26377;&#30340;&#24037;&#20316;&#26174;&#33879;&#30340;&#25552;&#39640;&#20102;&#65292;&#22240;&#20026;&#26368;&#20339;&#30340;&#24050;&#30693;&#30340;&#24809;&#32602;&#20989;&#25968;&#30340;&#30028;&#38480;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic generalised linear bandit is a well-understood model for sequential decision-making problems, with many algorithms achieving near-optimal regret guarantees under immediate feedback. However, the stringent requirement for immediate rewards is unmet in many real-world applications where the reward is almost always delayed. We study the phenomenon of delayed rewards in generalised linear bandits in a theoretical manner. We show that a natural adaptation of an optimistic algorithm to the delayed feedback achieves a regret bound where the penalty for the delays is independent of the horizon. This result significantly improves upon existing work, where the best known regret bound has the delay penalty increasing with the horizon. We verify our theoretical results through experiments on simulated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#25104;&#26412;&#25935;&#24863;&#30340;PEGASOS SVM&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#26680;&#20989;&#25968;&#32435;&#20837;SVM&#20013;&#25193;&#23637;Ding&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2206.09311</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;&#20272;&#35745;&#20122;&#26799;&#24230;&#27714;&#35299;&#22120;&#30340;&#19981;&#24179;&#34913;&#20998;&#31867;SVM
&lt;/p&gt;
&lt;p&gt;
Primal Estimated Subgradient Solver for SVM for Imbalanced Classification. (arXiv:2206.09311v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#25104;&#26412;&#25935;&#24863;&#30340;PEGASOS SVM&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#26680;&#20989;&#25968;&#32435;&#20837;SVM&#20013;&#25193;&#23637;Ding&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#25104;&#26412;&#25935;&#24863;PEGASOS SVM&#22312;&#20027;&#22810;&#27425;&#35201;&#27604;&#20174;8.6&#65306;1&#21040;130&#65306;1&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#21253;&#25324;&#25130;&#36317;&#65288;&#20559;&#35265;&#65289;&#12289;&#27491;&#21017;&#21270;&#21644;&#21442;&#25968;&#26159;&#21542;&#20250;&#24433;&#21709;&#25105;&#20204;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#35768;&#22810;&#20154;&#37319;&#29992;SMOTE&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#26088;&#22312;&#37319;&#29992;&#19968;&#31181;&#35745;&#31639;&#37327;&#36739;&#23567;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26816;&#26597;&#23398;&#20064;&#26354;&#32447;&#26469;&#35780;&#20272;&#24615;&#33021;&#65292;&#36825;&#20123;&#26354;&#32447;&#21487;&#20197;&#35786;&#26029;&#25105;&#20204;&#26159;&#36807;&#24230;&#25311;&#21512;&#36824;&#26159;&#27424;&#25311;&#21512;&#65292;&#25110;&#32773;&#25105;&#20204;&#36873;&#25321;&#20102;&#36807;&#24230;&#20195;&#34920;&#24615;&#25110;&#27424;&#20195;&#34920;&#24615;&#30340;&#35757;&#32451;/&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#23558;&#22312;&#39564;&#35777;&#26354;&#32447;&#20013;&#26597;&#30475;&#36229;&#21442;&#25968;&#30340;&#32972;&#26223;&#19982;&#27979;&#35797;&#21644;&#35757;&#32451;&#35823;&#24046;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#22522;&#20934;&#21270;&#25105;&#20204;&#30340;PEGASOS&#25104;&#26412;&#25935;&#24863;SVM&#19982;Ding&#30340;LINEAR SVM DECIDL&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#20182;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#20102;0.5&#30340;ROC-AUC&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36890;&#36807;&#23558;&#26680;&#20989;&#25968;&#32435;&#20837;SVM&#26469;&#25193;&#23637;Ding&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;Python&#32780;&#19981;&#26159;MATLAB&#65292;&#22240;&#20026;Python&#20855;&#26377;&#26356;&#26377;&#25928;&#22320;&#23384;&#20648;&#25105;&#30340;&#25968;&#25454;&#38598;&#30340;&#23383;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to demonstrate in experiments that our cost sensitive PEGASOS SVM achieves good performance on imbalanced data sets with a Majority to Minority Ratio ranging from 8.6:1 to 130:1 and to ascertain whether the including intercept (bias), regularization and parameters affects performance on our selection of datasets. Although many resort to SMOTE methods, we aim for a less computationally intensive method. We evaluate the performance by examining the learning curves. These curves diagnose whether we overfit or underfit or we choose over representative or under representative training/test data. We will also see the background of the hyperparameters versus the test and train error in validation curves. We benchmark our PEGASOS Cost-Sensitive SVM's results of Ding's LINEAR SVM DECIDL method. He obtained an ROC-AUC of .5 in one dataset. Our work will extend the work of Ding by incorporating kernels into SVM. We will use Python rather than MATLAB as python has dictionaries for storing m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#20934;&#30830;&#33410;&#28857;&#29305;&#24449;&#20272;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21464;&#20998;&#25512;&#26029;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#25552;&#20379;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2206.04516</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#21270;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#20934;&#30830;&#33410;&#28857;&#29305;&#24449;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Accurate Node Feature Estimation with Structured Variational Graph Autoencoder. (arXiv:2206.04516v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#20934;&#30830;&#33410;&#28857;&#29305;&#24449;&#20272;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21464;&#20998;&#25512;&#26029;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#25552;&#20379;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#33410;&#28857;&#29305;&#24449;&#37096;&#20998;&#35266;&#27979;&#30340;&#22270;&#65292;&#22914;&#20309;&#31934;&#30830;&#22320;&#20272;&#35745;&#32570;&#22833;&#30340;&#29305;&#24449;&#65311;&#29305;&#24449;&#20272;&#35745;&#23545;&#20110;&#20998;&#26512;&#23454;&#38469;&#22270;&#24418;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#20250;&#32570;&#22833;&#12290;&#20934;&#30830;&#30340;&#20272;&#35745;&#19981;&#20165;&#25552;&#20379;&#33410;&#28857;&#30340;&#22810;&#26679;&#20449;&#24687;&#65292;&#32780;&#19988;&#25903;&#25345;&#38656;&#35201;&#23436;&#25972;&#35266;&#27979;&#33410;&#28857;&#29305;&#24449;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#39640;&#32500;&#29305;&#24449;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#20272;&#35745;&#22120;&#20855;&#26377;&#22823;&#30340;&#34920;&#29616;&#21147;&#65292;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#30340;&#29305;&#24449;&#20272;&#35745;&#26041;&#27861;SVGA&#65288;Structured Variational Graph Autoencoder&#65289;&#12290;SVGA&#36890;&#36807;&#32467;&#26500;&#21270;&#21464;&#20998;&#25512;&#26029;&#23545;&#28508;&#21464;&#37327;&#30340;&#20998;&#24067;&#36827;&#34892;&#24378;&#22823;&#30340;&#27491;&#21017;&#21270;&#65292;&#35813;&#25512;&#26029;&#22522;&#20110;&#22270;&#24418;&#32467;&#26500;&#23558;&#21464;&#37327;&#30340;&#20808;&#39564;&#27169;&#22411;&#24314;&#27169;&#20026;&#39640;&#26031;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;&#12290;&#22240;&#27492;&#65292;SVGA&#32467;&#21512;&#20102;&#21464;&#20998;&#25512;&#26029;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#25552;&#20379;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a graph with partial observations of node features, how can we estimate the missing features accurately? Feature estimation is a crucial problem for analyzing real-world graphs whose features are commonly missing during the data collection process. Accurate estimation not only provides diverse information of nodes but also supports the inference of graph neural networks that require the full observation of node features. However, designing an effective approach for estimating high-dimensional features is challenging, since it requires an estimator to have large representation power, increasing the risk of overfitting. In this work, we propose SVGA (Structured Variational Graph Autoencoder), an accurate method for feature estimation. SVGA applies strong regularization to the distribution of latent variables by structured variational inference, which models the prior of variables as Gaussian Markov random field based on the graph structure. As a result, SVGA combines the advantages
&lt;/p&gt;</description></item><item><title>AdaSubS &#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#39564;&#35777;&#26426;&#21046;&#24555;&#36895;&#36807;&#28388;&#20986;&#19981;&#21487;&#36798;&#23376;&#30446;&#26631;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#35745;&#21010;&#26356;&#38271;&#30340;&#23376;&#30446;&#26631;&#30340;&#25928;&#29575;&#21644;&#22312;&#35745;&#21010;&#26356;&#30701;&#30340;&#23376;&#30446;&#26631;&#26041;&#38754;&#20855;&#26377;&#31934;&#32454;&#25511;&#21046;&#65292;&#22312; Sokoban&#12289;&#39764;&#26041;&#21644;&#19981;&#31561;&#24335;&#35777;&#26126;&#22522;&#20934; INT &#31561;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2206.00702</link><description>&lt;p&gt;
&#24555;&#36895;&#32780;&#31934;&#30830;&#65306;&#33258;&#36866;&#24212;&#23376;&#30446;&#26631;&#25628;&#32034;&#35843;&#25972;&#35268;&#21010;&#38271;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search. (arXiv:2206.00702v8 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00702
&lt;/p&gt;
&lt;p&gt;
AdaSubS &#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#39564;&#35777;&#26426;&#21046;&#24555;&#36895;&#36807;&#28388;&#20986;&#19981;&#21487;&#36798;&#23376;&#30446;&#26631;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#35745;&#21010;&#26356;&#38271;&#30340;&#23376;&#30446;&#26631;&#30340;&#25928;&#29575;&#21644;&#22312;&#35745;&#21010;&#26356;&#30701;&#30340;&#23376;&#30446;&#26631;&#26041;&#38754;&#20855;&#26377;&#31934;&#32454;&#25511;&#21046;&#65292;&#22312; Sokoban&#12289;&#39764;&#26041;&#21644;&#19981;&#31561;&#24335;&#35777;&#26126;&#22522;&#20934; INT &#31561;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#21253;&#21547;&#38656;&#35201;&#32791;&#36153;&#19981;&#21516;&#35745;&#31639;&#25104;&#26412;&#26469;&#30830;&#23450;&#33391;&#22909;&#34892;&#21160;&#35745;&#21010;&#30340;&#29366;&#24577;&#12290;&#38024;&#23545;&#36825;&#19968;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#23376;&#30446;&#26631;&#25628;&#32034; (AdaSubS) &#30340;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#35268;&#21010;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;AdaSubS &#29983;&#25104;&#19981;&#21516;&#36317;&#31163;&#19979;&#30340;&#22810;&#26679;&#21270;&#23376;&#30446;&#26631;&#38598;&#12290;&#37319;&#29992;&#39564;&#35777;&#26426;&#21046;&#24555;&#36895;&#36807;&#28388;&#20986;&#19981;&#21487;&#36798;&#23376;&#30446;&#26631;&#65292;&#20197;&#20415;&#19987;&#27880;&#20110;&#21487;&#20197;&#23454;&#29616;&#30340;&#21518;&#32493;&#23376;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;AdaSubS &#22312;&#35745;&#21010;&#26356;&#38271;&#30340;&#23376;&#30446;&#26631;&#30340;&#25928;&#29575;&#21644;&#22312;&#35745;&#21010;&#26356;&#30701;&#30340;&#23376;&#30446;&#26631;&#26041;&#38754;&#20855;&#26377;&#31934;&#32454;&#25511;&#21046;&#65292;&#24182;&#22240;&#27492;&#22312;&#38590;&#35299;&#30340;&#35268;&#21010;&#38382;&#39064;&#19978;&#25193;&#23637;&#24471;&#24456;&#22909;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; AdaSubS &#22312;&#19977;&#20010;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153; Sokoban&#12289;&#39764;&#26041;&#21644;&#19981;&#31561;&#24335;&#35777;&#26126;&#22522;&#20934; INT &#19978;&#26174;&#33879;&#36229;&#36807;&#20998;&#23618;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex reasoning problems contain states that vary in the computational cost required to determine a good action plan. Taking advantage of this property, we propose Adaptive Subgoal Search (AdaSubS), a search method that adaptively adjusts the planning horizon. To this end, AdaSubS generates diverse sets of subgoals at different distances. A verification mechanism is employed to filter out unreachable subgoals swiftly, allowing to focus on feasible further subgoals. In this way, AdaSubS benefits from the efficiency of planning with longer subgoals and the fine control with the shorter ones, and thus scales well to difficult planning problems. We show that AdaSubS significantly surpasses hierarchical planning algorithms on three complex reasoning tasks: Sokoban, the Rubik's Cube, and inequality proving benchmark INT.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#24179;&#28369;&#27169;&#22411;&#26816;&#39564;(smMC)&#26041;&#27861;&#30340;&#24605;&#36335;&#65292;&#20174;&#32780;&#20351;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;smMC&#36866;&#29992;&#20110;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.05398</link><description>&lt;p&gt;
&#24102;&#38543;&#26426;&#21464;&#37327;&#30340;&#21442;&#25968;&#21270;&#39564;&#35777;-&#38543;&#26426;&#21464;&#20998;&#20809;&#28369;&#27169;&#22411;&#26816;&#39564;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Stochastic Parametric Verification with Stochastic Variational Smoothed Model Checking. (arXiv:2205.05398v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#24179;&#28369;&#27169;&#22411;&#26816;&#39564;(smMC)&#26041;&#27861;&#30340;&#24605;&#36335;&#65292;&#20174;&#32780;&#20351;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;smMC&#36866;&#29992;&#20110;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#27169;&#22411;&#30340;&#32447;&#24615;&#26102;&#24577;&#24615;&#23646;&#24615;&#30340;&#21442;&#25968;&#21270;&#39564;&#35777;&#21487;&#20197;&#34920;&#31034;&#20026;&#35745;&#31639;&#28385;&#36275;&#19968;&#23450;&#23646;&#24615;&#30340;&#27010;&#29575;&#65292;&#20989;&#25968;&#30340;&#21442;&#25968;&#20026;&#36825;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#24179;&#28369;&#27169;&#22411;&#26816;&#39564;(smMC)&#26088;&#22312;&#20174;&#36890;&#36807;&#27169;&#25311;&#33719;&#24471;&#30340;&#26377;&#38480;&#30340;&#35266;&#27979;&#20540;&#20013;&#25512;&#26029;&#20986;&#25972;&#20010;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#28385;&#36275;&#20989;&#25968;&#12290;&#30001;&#20110;&#35266;&#27979;&#25104;&#26412;&#39640;&#19988;&#22122;&#22768;&#22823;&#65292;&#22240;&#27492;smMC&#34987;&#26500;&#24314;&#20026;&#36125;&#21494;&#26031;&#25512;&#29702;&#38382;&#39064;&#65292;&#20351;&#20272;&#35745;&#20540;&#20855;&#26377;&#39069;&#22806;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#22312;smMC&#20013;&#65292;&#20316;&#32773;&#20351;&#29992;&#30001;&#26399;&#26395;&#20256;&#25773;&#31639;&#27861;&#25512;&#26029;&#20986;&#30340;&#39640;&#26031;&#36807;&#31243;(GP)&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#37325;&#26500;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#23427;&#32487;&#25215;&#20102;GP&#30340;&#33879;&#21517;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21033;&#29992;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;smMC&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#20854;&#36866;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parametric verification of linear temporal properties for stochastic models can be expressed as computing the satisfaction probability of a certain property as a function of the parameters of the model. Smoothed model checking (smMC) aims at inferring the satisfaction function over the entire parameter space from a limited set of observations obtained via simulation. As observations are costly and noisy, smMC is framed as a Bayesian inference problem so that the estimates have an additional quantification of the uncertainty. In smMC the authors use Gaussian Processes (GP), inferred by means of the Expectation Propagation algorithm. This approach provides accurate reconstructions with statistically sound quantification of the uncertainty. However, it inherits the well-known scalability issues of GP. In this paper, we exploit recent advances in probabilistic machine learning to push this limitation forward, making Bayesian inference of smMC scalable to larger datasets and enabling its ap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WOODS&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#27979;&#35797;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#22312;&#31163;&#32676;&#20998;&#24067;&#19979;&#30340;&#27867;&#21270;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36824;&#25913;&#36827;&#20102;&#30446;&#21069;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#30340;&#31163;&#32676;&#20998;&#24067;&#24191;&#20041;&#24615;&#31639;&#27861;&#65292;&#24182;&#34920;&#26126;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2203.09978</link><description>&lt;p&gt;
WOODS: &#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#31163;&#32676;&#20998;&#24067;&#24191;&#20041;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series. (arXiv:2203.09978v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09978
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WOODS&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#27979;&#35797;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#22312;&#31163;&#32676;&#20998;&#24067;&#19979;&#30340;&#27867;&#21270;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36824;&#25913;&#36827;&#20102;&#30446;&#21069;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#30340;&#31163;&#32676;&#20998;&#24067;&#24191;&#20041;&#24615;&#31639;&#27861;&#65292;&#24182;&#34920;&#26126;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#24448;&#24448;&#38590;&#20197;&#36827;&#34892;&#24456;&#22909;&#30340;&#27867;&#21270;&#12290;&#29702;&#35299;&#21644;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#24418;&#25104;&#20102;&#31163;&#32676;&#20998;&#24067;&#24191;&#20041;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#23545;&#20110;&#38745;&#24577;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#31163;&#32676;&#20998;&#24067;&#24191;&#20041;&#24615;&#21364;&#40092;&#26377;&#25506;&#32034;&#12290;&#20026;&#20943;&#23569;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;WOODS&#65306;&#20843;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#28304;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#65292;&#20363;&#22914;&#35270;&#39057;&#12289;&#33041;&#35760;&#24405;&#21644;&#20256;&#24863;&#22120;&#20449;&#21495;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#31163;&#32676;&#20998;&#24067;&#24191;&#20041;&#24615;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#31995;&#32479;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#23545;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#31163;&#32676;&#20998;&#24067;&#24191;&#20041;&#24615;&#31639;&#27861;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#20174;&#32780;&#20984;&#26174;&#20102;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25991;&#26723;&#21487;&#22312;https://woods-benchmarks.github.io&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often fail to generalize well under distributional shifts. Understanding and overcoming these failures have led to a research field of Out-of-Distribution (OOD) generalization. Despite being extensively studied for static computer vision tasks, OOD generalization has been underexplored for time series tasks. To shine light on this gap, we present WOODS: eight challenging open-source time series benchmarks covering a diverse range of data modalities, such as videos, brain recordings, and sensor signals. We revise the existing OOD generalization algorithms for time series tasks and evaluate them using our systematic framework. Our experiments show a large room for improvement for empirical risk minimization and OOD generalization algorithms on our datasets, thus underscoring the new challenges posed by time series tasks. Code and documentation are available at https://woods-benchmarks.github.io .
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#24182;&#20197;&#37327;&#23376;&#25955;&#24230;&#24418;&#24335;&#23558;&#20854;&#35752;&#35770;&#22312;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#19979;&#65292;&#23558;&#24046;&#20998;&#38544;&#31169;&#30340;&#23646;&#24615;&#20174;&#27599;&#20010;&#27979;&#37327;&#26816;&#26597;&#36716;&#20026;&#20165;&#22522;&#20110;&#35745;&#31639;&#36755;&#20986;&#29366;&#24577;&#30340;&#23646;&#24615;&#65292;&#20351;&#24471;&#35777;&#26126;&#26356;&#31616;&#21333;&#12289;&#24615;&#36136;&#24191;&#27867;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2202.10717</link><description>&lt;p&gt;
&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#65306;&#20449;&#24687;&#35770;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Quantum Differential Privacy: An Information Theory Perspective. (arXiv:2202.10717v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#24182;&#20197;&#37327;&#23376;&#25955;&#24230;&#24418;&#24335;&#23558;&#20854;&#35752;&#35770;&#22312;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#19979;&#65292;&#23558;&#24046;&#20998;&#38544;&#31169;&#30340;&#23646;&#24615;&#20174;&#27599;&#20010;&#27979;&#37327;&#26816;&#26597;&#36716;&#20026;&#20165;&#22522;&#20110;&#35745;&#31639;&#36755;&#20986;&#29366;&#24577;&#30340;&#23646;&#24615;&#65292;&#20351;&#24471;&#35777;&#26126;&#26356;&#31616;&#21333;&#12289;&#24615;&#36136;&#24191;&#27867;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22312;&#25552;&#20379;&#32463;&#20856;&#35745;&#31639;&#30340;&#21487;&#35777;&#23433;&#20840;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#38750;&#24120;&#25104;&#21151;&#30340;&#25104;&#26524;&#12290;&#26368;&#36817;&#65292;&#36825;&#20010;&#27010;&#24565;&#34987;&#25512;&#24191;&#21040;&#37327;&#23376;&#35745;&#31639;&#20013;&#12290;&#34429;&#28982;&#32463;&#20856;&#35745;&#31639;&#26412;&#36136;&#19978;&#26159;&#26080;&#22122;&#22768;&#30340;&#65292;&#24046;&#20998;&#38544;&#31169;&#36890;&#24120;&#36890;&#36807;&#20154;&#20026;&#28155;&#21152;&#22122;&#22768;&#26469;&#23454;&#29616;&#65292;&#20294;&#36817;&#26399;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#26412;&#26469;&#23601;&#26159;&#22024;&#26434;&#30340;&#65292;&#22240;&#27492;&#33258;&#28982;&#23384;&#22312;&#24046;&#20998;&#38544;&#31169;&#36825;&#19968;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#37327;&#23376;&#25955;&#24230;&#30340;&#24418;&#24335;&#23558;&#37327;&#23376;&#24046;&#20998;&#38544;&#31169;&#35752;&#35770;&#22312;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#19979;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#65292;&#24046;&#20998;&#38544;&#31169;&#25104;&#20026;&#20165;&#22522;&#20110;&#35745;&#31639;&#36755;&#20986;&#29366;&#24577;&#30340;&#23646;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#27599;&#20010;&#27979;&#37327;&#36827;&#34892;&#26816;&#26597;&#12290;&#36825;&#23548;&#33268;&#35777;&#26126;&#26356;&#31616;&#21333;&#65292;&#24615;&#36136;&#30340;&#27010;&#25324;&#22768;&#26126;&#26356;&#24191;&#27867;&#65292;&#24182;&#20026;&#36890;&#29992;&#22122;&#22768;&#27169;&#22411;&#21644;&#29305;&#23450;&#22122;&#22768;&#27169;&#22411;&#25552;&#20379;&#20102;&#33509;&#24178;&#26032;&#30340;&#38480;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20123;&#38480;&#21046;&#21253;&#25324;&#24120;&#35265;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy has been an exceptionally successful concept when it comes to providing provable security guarantees for classical computations. More recently, the concept was generalized to quantum computations. While classical computations are essentially noiseless and differential privacy is often achieved by artificially adding noise, near-term quantum computers are inherently noisy and it was observed that this leads to natural differential privacy as a feature.  In this work we discuss quantum differential privacy in an information theoretic framework by casting it as a quantum divergence. A main advantage of this approach is that differential privacy becomes a property solely based on the output states of the computation, without the need to check it for every measurement. This leads to simpler proofs and generalized statements of its properties as well as several new bounds for both, general and specific, noise models. In particular, these include common representations of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StratDef&#30340;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#26041;&#27861;&#30340;&#25112;&#30053;&#38450;&#24481;&#31995;&#32479;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#38450;&#24481;&#25514;&#26045;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;StratDef&#21160;&#24577;&#22320;&#21644;&#31574;&#30053;&#22320;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#22686;&#21152;&#25915;&#20987;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#20855;&#26377;&#24456;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.07568</link><description>&lt;p&gt;
StratDef: &#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#23545;&#25239;&#25915;&#20987;&#30340;&#25112;&#30053;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
StratDef: Strategic Defense Against Adversarial Attacks in ML-based Malware Detection. (arXiv:2202.07568v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StratDef&#30340;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#26041;&#27861;&#30340;&#25112;&#30053;&#38450;&#24481;&#31995;&#32479;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#38450;&#24481;&#25514;&#26045;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;StratDef&#21160;&#24577;&#22320;&#21644;&#31574;&#30053;&#22320;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#22686;&#21152;&#25915;&#20987;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#20855;&#26377;&#24456;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#22823;&#22810;&#38598;&#20013;&#22312;&#22270;&#20687;&#35782;&#21035;&#39046;&#22495;&#12290;&#23613;&#31649;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#24456;&#39640;&#65292;&#20294;&#23427;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#32780;&#19988;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#19968;&#20123;&#26041;&#27861;&#19978;&#65292;&#32780;&#27809;&#26377;&#20855;&#20307;&#30340;&#24212;&#29992;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StratDef&#30340;&#25112;&#30053;&#38450;&#24481;&#31995;&#32479;&#65292;&#23427;&#22522;&#20110;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#28041;&#21450;&#27169;&#22411;&#31995;&#32479;&#21270;&#26500;&#24314;&#12289;&#36873;&#25321;&#21644;&#25112;&#30053;&#20351;&#29992;&#30340;&#25361;&#25112;&#65292;&#20197;&#26368;&#22823;&#21270;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#22914;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#65292;StratDef&#21160;&#24577;&#22320;&#21644;&#31574;&#30053;&#22320;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#22686;&#21152;&#25915;&#20987;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20026;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#38450;&#24481;&#25514;&#26045;&#39318;&#27425;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#23041;&#32961;&#27169;&#22411;&#25506;&#32034;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#23041;&#32961;&#12289;&#25915;&#20987;&#32773;&#30340;&#30693;&#35782;&#27700;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, most research towards defenses against adversarial attacks on machine learning models has been in the image recognition domain. The malware detection domain has received less attention despite its importance. Moreover, most work exploring these defenses has focused on several methods but with no strategy when applying them. In this paper, we introduce StratDef, which is a strategic defense system based on a moving target defense approach. We overcome challenges related to the systematic construction, selection, and strategic use of models to maximize adversarial robustness. StratDef dynamically and strategically chooses the best models to increase the uncertainty for the attacker while minimizing critical aspects in the adversarial ML domain, like attack transferability. We provide the first comprehensive evaluation of defenses against adversarial attacks on machine learning for malware detection, where our threat model explores different levels of threat, attacker know
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24207;&#21015;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#21644;&#26631;&#20934;&#21270;&#27969;&#21464;&#20998;&#25512;&#26029;&#30340;&#36830;&#32493;&#37325;&#22797;&#36864;&#28779;&#27969;&#36755;&#36816;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#21270;&#27969;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#19981;&#21516;&#28201;&#24230;&#19979;&#30340;&#20256;&#36755;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#20363;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2201.13117</link><description>&lt;p&gt;
&#36830;&#32493;&#37325;&#22797;&#36864;&#28779;&#27969;&#36755;&#36816;&#33945;&#29305;&#21345;&#32599;
&lt;/p&gt;
&lt;p&gt;
Continual Repeated Annealed Flow Transport Monte Carlo. (arXiv:2201.13117v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13117
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24207;&#21015;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#21644;&#26631;&#20934;&#21270;&#27969;&#21464;&#20998;&#25512;&#26029;&#30340;&#36830;&#32493;&#37325;&#22797;&#36864;&#28779;&#27969;&#36755;&#36816;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#21270;&#27969;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#19981;&#21516;&#28201;&#24230;&#19979;&#30340;&#20256;&#36755;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#20363;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36830;&#32493;&#37325;&#22797;&#36864;&#28779;&#27969;&#36755;&#36816;&#33945;&#29305;&#21345;&#32599;&#65288;CRAFT&#65289;&#26041;&#27861;&#65292;&#23558;&#24207;&#21015;&#33945;&#29305;&#21345;&#32599;&#65288;SMC&#65289;&#37319;&#26679;&#22120;&#65288;&#33258;&#36523;&#26159;&#36880;&#27493;&#37325;&#35201;&#37319;&#26679;&#30340;&#25512;&#24191;&#65289;&#19982;&#20351;&#29992;&#26631;&#20934;&#21270;&#27969;&#30340;&#21464;&#20998;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;&#26631;&#20934;&#21270;&#27969;&#30452;&#25509;&#35757;&#32451;&#20197;&#22312;&#27599;&#20010;&#36716;&#25442;&#20043;&#38388;&#20256;&#36755;&#36864;&#28779;&#28201;&#24230;&#65292;&#20351;&#29992;KL&#25955;&#24230;&#36827;&#34892;&#20248;&#21270;&#30446;&#26631;&#65292;&#27492;&#20248;&#21270;&#30446;&#26631;&#26412;&#36523;&#20351;&#29992;&#26631;&#20934;&#21270;&#27969;/ SMC&#36817;&#20284;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#27010;&#24565;&#19978;&#21644;&#22810;&#20010;&#32463;&#39564;&#23454;&#20363;&#20013;&#23637;&#31034;&#20102;CRAFT&#20248;&#20110;Annealed Flow Transport Monte Carlo&#65288;Arbel&#31561;&#20154;&#65292;2021&#65289;&#65292;&#24182;&#22312;&#20854;&#22522;&#30784;&#19978;&#25913;&#36827;&#20102;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#30340;&#38543;&#26426;&#26631;&#20934;&#21270;&#27969;&#65288;Wu&#31561;&#20154;&#65292;2020&#65289;&#12290;&#36890;&#36807;&#22312;&#31890;&#23376;MCMC&#20013;&#32467;&#21512;CRAFT&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#23398;&#20064;&#30340;&#37319;&#26679;&#22120;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26230;&#26684;&#22330;&#35770;&#23454;&#20363;&#19978;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31934;&#30830;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Continual Repeated Annealed Flow Transport Monte Carlo (CRAFT), a method that combines a sequential Monte Carlo (SMC) sampler (itself a generalization of Annealed Importance Sampling) with variational inference using normalizing flows. The normalizing flows are directly trained to transport between annealing temperatures using a KL divergence for each transition. This optimization objective is itself estimated using the normalizing flow/SMC approximation. We show conceptually and using multiple empirical examples that CRAFT improves on Annealed Flow Transport Monte Carlo (Arbel et al., 2021), on which it builds and also on Markov chain Monte Carlo (MCMC) based Stochastic Normalizing Flows (Wu et al., 2020). By incorporating CRAFT within particle MCMC, we show that such learnt samplers can achieve impressively accurate results on a challenging lattice field theory example.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#30340;&#22797;&#26434;&#24230;&#65292;&#35777;&#26126;&#20102;&#22312;&#37096;&#20998;&#21160;&#24577;&#35774;&#32622;&#20013;&#39640;&#31934;&#24230;&#21644;&#20302;&#31934;&#24230;LSR&#30340;&#24179;&#25674;&#26356;&#26032;&#26102;&#38388;&#26377;&#23574;&#38160;&#20998;&#31163;&#65292;&#24182;&#21033;&#29992;&#22312;&#32447;&#30697;&#38453;&#21521;&#37327;&#29468;&#24819;&#36827;&#34892;&#20102;&#38388;&#38553;&#25918;&#22823;&#32422;&#31616;&#12290;</title><link>http://arxiv.org/abs/2201.00228</link><description>&lt;p&gt;
&#21160;&#24577;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#30340;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Complexity of Dynamic Least-Squares Regression. (arXiv:2201.00228v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.00228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#30340;&#22797;&#26434;&#24230;&#65292;&#35777;&#26126;&#20102;&#22312;&#37096;&#20998;&#21160;&#24577;&#35774;&#32622;&#20013;&#39640;&#31934;&#24230;&#21644;&#20302;&#31934;&#24230;LSR&#30340;&#24179;&#25674;&#26356;&#26032;&#26102;&#38388;&#26377;&#23574;&#38160;&#20998;&#31163;&#65292;&#24182;&#21033;&#29992;&#22312;&#32447;&#30697;&#38453;&#21521;&#37327;&#29468;&#24819;&#36827;&#34892;&#20102;&#38388;&#38553;&#25918;&#22823;&#32422;&#31616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21160;&#24577;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#65288;LSR&#65289;&#30340;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#34892;&#21644;&#26631;&#31614; $(\mathbf{A}^{(t)},\mathbf{b}^{(t)})$ &#21487;&#20197;&#34987;&#33258;&#36866;&#24212;&#22320;&#25554;&#20837;&#21644;/&#25110;&#21024;&#38500;&#65292;&#30446;&#26631;&#26159;&#26377;&#25928;&#22320;&#32500;&#25252; $\min_{\mathbf{x}^{(t)}} \| \mathbf{A}^{(t)} \mathbf{x}^{(t)} - \mathbf{b}^{(t)} \|_2$ &#30340; $\epsilon$-&#36817;&#20284;&#35299;&#23545;&#20110;&#25152;&#26377; $t\in [T]$&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65288;i&#65289;&#23436;&#20840;&#21160;&#24577; vs &#37096;&#20998;&#21160;&#24577; $0.01$-LSR&#65307;&#65288;ii&#65289;&#39640;&#31934;&#24230; vs &#20302;&#31934;&#24230;LSR&#22312;&#37096;&#20998;&#21160;&#24577;&#65288;&#20165;&#25554;&#20837;&#65289;&#35774;&#32622;&#20013;&#30340;&#24179;&#25674;&#26356;&#26032;&#26102;&#38388;&#30340;&#23574;&#38160;&#20998;&#31163;($d^{2-o(1)}$ vs. $\sim d$)&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#26469;&#33258;&#19968;&#20010;&#38388;&#38553;&#25918;&#22823;&#32422;&#31616;&#65288;&#31867;&#20284;&#20110;&#36845;&#20195;&#37325;&#26500;&#65289;&#8212;&#8212;&#20174;&#20934;&#30830;&#30340;&#22312;&#32447;&#30697;&#38453;&#21521;&#37327;&#29468;&#24819;&#65288;OMv&#65289;[HKNS15]&#21040;&#23454;&#25968;&#19978;&#30340;&#24120;&#25968;&#36817;&#20284;OMv&#65292;&#20854;&#20013;&#31532; $i$ &#20010;&#22312;&#32447;&#31215; $\mathbf{H}\mathbf{v}^{(i)}$ &#21482;&#38656;&#35201;&#35745;&#31639;&#21040; $0.1$-&#30456;&#23545;&#35823;&#24046;&#12290;&#25152;&#26377;&#20197;&#21069;&#30340;&#30001;OMv&#21040;&#23427;&#30340;&#36817;&#20284;&#29256;&#26412;&#30340;&#31934;&#32454;&#32422;&#31616;&#37117;&#21482;&#34920;&#29616;&#20986;&#24378;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We settle the complexity of dynamic least-squares regression (LSR), where rows and labels $(\mathbf{A}^{(t)}, \mathbf{b}^{(t)})$ can be adaptively inserted and/or deleted, and the goal is to efficiently maintain an $\epsilon$-approximate solution to $\min_{\mathbf{x}^{(t)}} \| \mathbf{A}^{(t)} \mathbf{x}^{(t)} - \mathbf{b}^{(t)} \|_2$ for all $t\in [T]$. We prove sharp separations ($d^{2-o(1)}$ vs. $\sim d$) between the amortized update time of: (i) Fully vs. Partially dynamic $0.01$-LSR; (ii) High vs. low-accuracy LSR in the partially-dynamic (insertion-only) setting.  Our lower bounds follow from a gap-amplification reduction -- reminiscent of iterative refinement -- rom the exact version of the Online Matrix Vector Conjecture (OMv) [HKNS15], to constant approximate OMv over the reals, where the $i$-th online product $\mathbf{H}\mathbf{v}^{(i)}$ only needs to be computed to $0.1$-relative error. All previous fine-grained reductions from OMv to its approximate versions only show hardn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;&#32593;&#32476;&#30340;&#25972;&#20307;&#23637;&#24320;&#30340;&#23545;&#25239;&#25439;&#22833;&#19978;&#30028;&#26469;&#23454;&#29616;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#26032;&#30340;&#31283;&#20581;&#20248;&#21270;&#39046;&#22495;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#20445;&#35777;&#36755;&#20986;&#23618;&#32465;&#23450;&#32039;&#23494;&#24615;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2112.09279</link><description>&lt;p&gt;
&#31283;&#20581;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#24378;&#21147;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Robust Upper Bounds for Adversarial Training. (arXiv:2112.09279v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09279
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;&#32593;&#32476;&#30340;&#25972;&#20307;&#23637;&#24320;&#30340;&#23545;&#25239;&#25439;&#22833;&#19978;&#30028;&#26469;&#23454;&#29616;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#26032;&#30340;&#31283;&#20581;&#20248;&#21270;&#39046;&#22495;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#20445;&#35777;&#36755;&#20986;&#23618;&#32465;&#23450;&#32039;&#23494;&#24615;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#20379;&#23545;&#25239;&#25915;&#20987;&#30340;&#23433;&#20840;&#20445;&#35777;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#25439;&#22833;&#30340;&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#20984;&#26494;&#24347;&#26469;&#20256;&#25773;&#20013;&#38388;&#23618;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#65292;&#36825;&#20250;&#24433;&#21709;&#36755;&#20986;&#23618;&#32465;&#23450;&#30340;&#32039;&#23494;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;&#32593;&#32476;&#30340;&#25972;&#20307;&#23637;&#24320;&#30340;&#23545;&#25239;&#25439;&#22833;&#19978;&#30028;&#26469;&#23454;&#29616;&#12290;&#35813;&#19978;&#30028;&#21033;&#29992;&#20102;&#31283;&#20581;&#20248;&#21270;&#39046;&#22495;&#30340;&#26368;&#26032;&#24037;&#20855;&#65292;&#20855;&#26377;&#38381;&#21512;&#24418;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#65288;&#36817;&#20284;&#31283;&#20581;&#19978;&#30028;&#25110;aRUB&#65289;&#20351;&#29992;&#32593;&#32476;&#30340;&#19968;&#38454;&#36817;&#20284;&#21644;&#32447;&#24615;&#31283;&#20581;&#20248;&#21270;&#30340;&#22522;&#26412;&#24037;&#20855;&#65292;&#33719;&#24471;&#23545;&#25239;&#25439;&#22833;&#30340;&#32463;&#39564;&#19978;&#30028;&#65292;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many state-of-the-art adversarial training methods for deep learning leverage upper bounds of the adversarial loss to provide security guarantees against adversarial attacks. Yet, these methods rely on convex relaxations to propagate lower and upper bounds for intermediate layers, which affect the tightness of the bound at the output layer. We introduce a new approach to adversarial training by minimizing an upper bound of the adversarial loss that is based on a holistic expansion of the network instead of separate bounds for each layer. This bound is facilitated by state-of-the-art tools from Robust Optimization; it has closed-form and can be effectively trained using backpropagation. We derive two new methods with the proposed approach. The first method (Approximated Robust Upper Bound or aRUB) uses the first order approximation of the network as well as basic tools from Linear Robust Optimization to obtain an empirical upper bound of the adversarial loss that can be easily implement
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25340;&#25509;&#30340;&#28608;&#20809;&#38647;&#36798;&#20998;&#21106;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;ConDA&#65292;&#36890;&#36807;&#26500;&#24314;&#21253;&#21547;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#32454;&#31890;&#24230;&#20114;&#25442;&#20449;&#21495;&#30340;&#20013;&#38388;&#22495;&#65292;&#23454;&#29616;&#22312;&#19981;&#30772;&#22351;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2111.15242</link><description>&lt;p&gt;
ConDA: &#36890;&#36807;&#35268;&#33539;&#21270;&#22495;&#25340;&#25509;&#36827;&#34892;&#28608;&#20809;&#38647;&#36798;&#20998;&#21106;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
ConDA: Unsupervised Domain Adaptation for LiDAR Segmentation via Regularized Domain Concatenation. (arXiv:2111.15242v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.15242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25340;&#25509;&#30340;&#28608;&#20809;&#38647;&#36798;&#20998;&#21106;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;ConDA&#65292;&#36890;&#36807;&#26500;&#24314;&#21253;&#21547;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#32454;&#31890;&#24230;&#20114;&#25442;&#20449;&#21495;&#30340;&#20013;&#38388;&#22495;&#65292;&#23454;&#29616;&#22312;&#19981;&#30772;&#22351;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26377;&#26631;&#31614;&#30340;&#28304;&#22495;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#21407;&#22987;&#30340;&#30446;&#26631;&#22495;&#20013;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#37096;&#32626;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25340;&#25509;&#30340;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;ConDA&#65292;&#29992;&#20110;&#28608;&#20809;&#38647;&#36798;&#20998;&#21106;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#32454;&#31890;&#24230;&#20114;&#25442;&#20449;&#21495;&#30340;&#20013;&#38388;&#22495;&#65292;&#32780;&#19981;&#30772;&#22351;&#33258;&#36710;&#21608;&#22260;&#29289;&#20307;&#21644;&#32972;&#26223;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#24182;&#21033;&#29992;&#35813;&#20013;&#38388;&#22495;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring knowledge learned from the labeled source domain to the raw target domain for unsupervised domain adaptation (UDA) is essential to the scalable deployment of autonomous driving systems. State-of-the-art methods in UDA often employ a key idea: utilizing joint supervision signals from both source and target domains for self-training. In this work, we improve and extend this aspect. We present ConDA, a concatenation-based domain adaptation framework for LiDAR segmentation that: 1) constructs an intermediate domain consisting of fine-grained interchange signals from both source and target domains without destabilizing the semantic coherency of objects and background around the ego-vehicle; and 2) utilizes the intermediate domain for self-training. To improve the network training on the source domain and self-training on the intermediate domain, we propose an anti-aliasing regularizer and an entropy aggregator to reduce the negative effect caused by the aliasing artifacts and n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#25506;&#35752;&#20102;&#24310;&#36831;&#21453;&#39304;&#23545;&#21608;&#26399;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#22788;&#29702;&#24310;&#36831;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#31181;&#26041;&#27861;&#38024;&#23545;&#20048;&#35266;&#31639;&#27861;&#31867;&#21518;&#24724;&#20250;&#22686;&#21152;&#19968;&#20010;&#19982;&#29366;&#24577;&#25968;&#37327;&#12289;&#21160;&#20316;&#25968;&#37327;&#12289;&#24773;&#33410;&#38271;&#24230;&#12289;&#39044;&#26399;&#24310;&#36831;&#21644;&#31639;&#27861;&#30456;&#20851;&#24120;&#25968;&#30340;&#21152;&#24615;&#39033;&#12290;</title><link>http://arxiv.org/abs/2111.07615</link><description>&lt;p&gt;
&#20048;&#35266;&#31639;&#27861;&#19982;&#24310;&#36831;&#30340;&#21608;&#26399;&#24615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimism and Delays in Episodic Reinforcement Learning. (arXiv:2111.07615v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#25506;&#35752;&#20102;&#24310;&#36831;&#21453;&#39304;&#23545;&#21608;&#26399;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#22788;&#29702;&#24310;&#36831;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#31181;&#26041;&#27861;&#38024;&#23545;&#20048;&#35266;&#31639;&#27861;&#31867;&#21518;&#24724;&#20250;&#22686;&#21152;&#19968;&#20010;&#19982;&#29366;&#24577;&#25968;&#37327;&#12289;&#21160;&#20316;&#25968;&#37327;&#12289;&#24773;&#33410;&#38271;&#24230;&#12289;&#39044;&#26399;&#24310;&#36831;&#21644;&#31639;&#27861;&#30456;&#20851;&#24120;&#25968;&#30340;&#21152;&#24615;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21608;&#26399;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#24456;&#22810;&#29992;&#20110;&#20943;&#23569;&#21518;&#24724;&#30340;&#31639;&#27861;&#12290;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#21482;&#35201;&#19982;&#27599;&#27425;&#19982;&#29615;&#22659;&#20132;&#20114;&#21518;&#31435;&#21363;&#26356;&#26032;&#31574;&#30053;&#30340;&#31639;&#27861;&#21487;&#20197;&#33719;&#21462;&#19982;&#27599;&#20010;&#24773;&#33410;&#30456;&#20851;&#30340;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#21453;&#39304;&#20960;&#20046;&#24635;&#26159;&#24310;&#36831;&#30340;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#30740;&#31350;&#20102;&#24310;&#36831;&#21453;&#39304;&#23545;&#21608;&#26399;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#22788;&#29702;&#24310;&#36831;&#12290;&#31532;&#19968;&#20010;&#26041;&#27861;&#28041;&#21450;&#21040;&#22312;&#26032;&#20449;&#24687;&#21487;&#29992;&#26102;&#31435;&#21363;&#26356;&#26032;&#65292;&#32780;&#31532;&#20108;&#20010;&#26041;&#27861;&#21017;&#31561;&#24453;&#20351;&#29992;&#26032;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#26469;&#26356;&#26032;&#31574;&#30053;&#12290;&#38024;&#23545;&#20048;&#35266;&#31639;&#27861;&#31867;&#21644;&#20219;&#19968;&#26041;&#27861;&#65292;&#25105;&#20204;&#34920;&#26126;&#21518;&#24724;&#20250;&#22686;&#21152;&#19968;&#20010;&#19982;&#29366;&#24577;&#25968;&#37327;&#12289;&#21160;&#20316;&#25968;&#37327;&#12289;&#24773;&#33410;&#38271;&#24230;&#12289;&#39044;&#26399;&#24310;&#36831;&#21644;&#31639;&#27861;&#30456;&#20851;&#24120;&#25968;&#30340;&#21152;&#24615;&#39033;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are many algorithms for regret minimisation in episodic reinforcement learning. This problem is well-understood from a theoretical perspective, providing that the sequences of states, actions and rewards associated with each episode are available to the algorithm updating the policy immediately after every interaction with the environment. However, feedback is almost always delayed in practice. In this paper, we study the impact of delayed feedback in episodic reinforcement learning from a theoretical perspective and propose two general-purpose approaches to handling the delays. The first involves updating as soon as new information becomes available, whereas the second waits before using newly observed information to update the policy. For the class of optimistic algorithms and either approach, we show that the regret increases by an additive term involving the number of states, actions, episode length, the expected delay and an algorithm-dependent constant. We empirically inves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#20132;&#36890;&#27969;&#37327;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#65292;&#26222;&#36941;&#36866;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#27969;&#37327;&#30417;&#27979;&#21644;&#35843;&#25511;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2109.14251</link><description>&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#24341;&#23548;&#30340;&#22478;&#24066;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.14251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#20132;&#36890;&#27969;&#37327;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#65292;&#26222;&#36941;&#36866;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#27969;&#37327;&#30417;&#27979;&#21644;&#35843;&#25511;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#25512;&#26029;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#37327;&#26159;&#19968;&#20010;&#26032;&#20852;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#26497;&#22823;&#22320;&#20943;&#23569;&#25152;&#38656;&#20132;&#36890;&#30417;&#27979;&#20256;&#24863;&#22120;&#30340;&#25968;&#37327;&#20197;&#33410;&#30465;&#25104;&#26412;&#12290;&#26412;&#25991;&#21457;&#29616;&#20132;&#36890;&#27969;&#37327;&#19982;&#36947;&#36335;&#32593;&#32476;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#23436;&#20840;&#24573;&#30053;&#20102;&#36825;&#19968;&#28857;&#65292;&#25110;&#32773;&#20165;&#23558;&#20854;&#35270;&#20026;&#22806;&#37096;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#32593;&#24863;&#30693;&#20132;&#36890;&#27969;&#37327;&#25918;&#22823;&#22120;&#65288;RATFM&#65289;&#65292;&#23427;&#26126;&#30830;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26041;&#21521;1D&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#36947;&#36335;&#32593;&#32476;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#21644;&#31895;&#31890;&#24230;&#27969;&#37327;&#29305;&#24449;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#35268;&#33539;&#21270;&#36947;&#36335;&#30456;&#20851;&#20132;&#36890;&#27969;&#30340;&#30701;&#36317;&#31163;&#31354;&#38388;&#20998;&#24067;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#20316;&#20026;&#26597;&#35810;&#26469;&#25429;&#33719;&#38271;&#36317;&#31163;&#36335;&#27573;&#20132;&#36890;&#27969;&#37327;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate inference of fine-grained traffic flow from coarse-grained one is an emerging yet crucial problem, which can help greatly reduce the number of the required traffic monitoring sensors for cost savings. In this work, we notice that traffic flow has a high correlation with road network, which was either completely ignored or simply treated as an external factor in previous works.To facilitate this problem, we propose a novel Road-Aware Traffic Flow Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks to fully learn the road-aware spatial distribution of fine-grained traffic flow. Specifically, a multi-directional 1D convolutional layer is first introduced to extract the semantic feature of the road network. Subsequently, we incorporate the road network feature and coarse-grained flow feature to regularize the short-range spatial distribution modeling of road-relative traffic flow. Furthermore, we take the road network feature as a query to capture the l
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;&#32570;&#22833;&#25968;&#25454;&#22686;&#24378;&#65288;MisA&#65289;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#24335;&#22635;&#34917;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#20135;&#29983;&#19981;&#23436;&#25972;&#26679;&#26412;&#65292;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#37325;&#26500;&#25439;&#22833;&#23545;&#22686;&#24378;&#26679;&#26412;&#36827;&#34892;&#32422;&#26463;&#65292;&#20026;&#29983;&#25104;&#24335;&#22635;&#34917;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#39640;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2108.02566</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#22686;&#24378;&#65306;&#25552;&#39640;&#29983;&#25104;&#24335;&#22635;&#34917;&#27169;&#22411;&#24615;&#33021;&#30340;&#36890;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Missingness Augmentation: A General Approach for Improving Generative Imputation Models. (arXiv:2108.02566v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.02566
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;&#32570;&#22833;&#25968;&#25454;&#22686;&#24378;&#65288;MisA&#65289;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#24335;&#22635;&#34917;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#20135;&#29983;&#19981;&#23436;&#25972;&#26679;&#26412;&#65292;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#37325;&#26500;&#25439;&#22833;&#23545;&#22686;&#24378;&#26679;&#26412;&#36827;&#34892;&#32422;&#26463;&#65292;&#20026;&#29983;&#25104;&#24335;&#22635;&#34917;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#39640;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#22635;&#34917;&#26159;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#27169;&#22411;&#32467;&#26500;&#21644;&#23398;&#20064;&#36807;&#31243;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#22686;&#24378;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#37324;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;&#32570;&#22833;&#25968;&#25454;&#22686;&#24378;&#65288;MisA&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#24335;&#22635;&#34917;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#27599;&#19968;&#36718;&#21160;&#24577;&#20135;&#29983;&#19981;&#23436;&#25972;&#26679;&#26412;&#65292;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#37325;&#26500;&#25439;&#22833;&#23545;&#22686;&#24378;&#26679;&#26412;&#36827;&#34892;&#32422;&#26463;&#65292;&#24182;&#23558;&#27492;&#25439;&#22833;&#19982;&#21407;&#22987;&#25439;&#22833;&#32452;&#21512;&#25104;&#26368;&#32456;&#30340;&#20248;&#21270;&#30446;&#26631;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;MisA&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#29983;&#25104;&#24335;&#22635;&#34917;&#26694;&#26550;&#20013;&#65292;&#20026;&#25552;&#39640;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MisA&#26174;&#33879;&#25552;&#39640;&#20102;&#35768;&#22810;&#26368;&#36817;&#25552;&#20986;&#30340;&#29983;&#25104;&#24335;&#22635;&#34917;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data imputation is a fundamental problem in data analysis, and many studies have been conducted to improve its performance by exploring model structures and learning procedures. However, data augmentation, as a simple yet effective method, has not received enough attention in this area. In this paper, we propose a novel data augmentation method called Missingness Augmentation (MisA) for generative imputation models. Our approach dynamically produces incomplete samples at each epoch by utilizing the generator's output, constraining the augmented samples using a simple reconstruction loss, and combining this loss with the original loss to form the final optimization objective. As a general augmentation technique, MisA can be easily integrated into generative imputation frameworks, providing a simple yet effective way to enhance their performance. Experimental results demonstrate that MisA significantly improves the performance of many recently proposed generative imputation model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#25193;&#23637;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#23450;&#20041;&#19968;&#32452;&#20855;&#26377;&#26080;&#38480;&#32500;&#21442;&#25968;&#30340;GP&#30340;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36890;&#36807;&#20803;&#23398;&#20064;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2107.07115</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Principal component analysis for Gaussian process posteriors. (arXiv:2107.07115v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.07115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#25193;&#23637;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#23450;&#20041;&#19968;&#32452;&#20855;&#26377;&#26080;&#38480;&#32500;&#21442;&#25968;&#30340;GP&#30340;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36890;&#36807;&#20803;&#23398;&#20064;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;GP-PCA&#65289;&#25193;&#23637;&#12290;&#30001;&#20110;GP-PCA&#20272;&#35745;&#20102;&#19968;&#20010;&#20302;&#32500;&#24230;&#30340;GP&#21518;&#39564;&#31354;&#38388;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#20110;&#20803;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#19968;&#32452;&#20219;&#21153;&#30340;&#32467;&#26500;&#26469;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#24615;&#33021;&#30340;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#30456;&#21516;&#20808;&#39564;&#30340;GP&#21518;&#39564;&#31354;&#38388;&#65292;&#22312;&#20449;&#24687;&#20960;&#20309;&#26694;&#26550;&#19979;&#23558;GP&#30340;&#26080;&#38480;&#32500;&#24230;&#38382;&#39064;&#32553;&#20943;&#20026;&#26377;&#38480;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22914;&#20309;&#23450;&#20041;&#19968;&#32452;&#20855;&#26377;&#26080;&#38480;&#32500;&#21442;&#25968;&#65288;&#22914;&#22352;&#26631;&#31995;&#21644;&#21457;&#25955;&#65289;&#30340;GP&#30340;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;GP-PCA&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;GP-PCA&#20316;&#20026;&#20803;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an extension of principal component analysis for Gaussian process (GP) posteriors, denoted by GP-PCA. Since GP-PCA estimates a low-dimensional space of GP posteriors, it can be used for meta-learning, which is a framework for improving the performance of target tasks by estimating a structure of a set of tasks. The issue is how to define a structure of a set of GPs with an infinite-dimensional parameter, such as coordinate system and a divergence. In this study, we reduce the infiniteness of GP to the finite-dimensional case under the information geometrical framework by considering a space of GP posteriors that have the same prior. In addition, we propose an approximation method of GP-PCA based on variational inference and demonstrate the effectiveness of GP-PCA as meta-learning through experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#35889;&#31354;&#38388;&#22270;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#35757;&#32451;&#20013;&#29983;&#25104;&#36229;&#20687;&#32032;&#24182;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#20449;&#24687;&#36827;&#34892;&#22270;&#21367;&#31215;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#22522;&#20934;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.13952</link><description>&lt;p&gt;
&#20840;&#23616;&#35889;&#31354;&#38388;&#22270;&#25512;&#29702;&#29992;&#20110;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spectral-Spatial Global Graph Reasoning for Hyperspectral Image Classification. (arXiv:2106.13952v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.13952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#35889;&#31354;&#38388;&#22270;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#35757;&#32451;&#20013;&#29983;&#25104;&#36229;&#20687;&#32032;&#24182;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#20449;&#24687;&#36827;&#34892;&#22270;&#21367;&#31215;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#22522;&#20934;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#21367;&#31215;&#22312;&#25552;&#21462;&#19981;&#35268;&#21017;&#20998;&#24067;&#29289;&#20307;&#30340;&#29305;&#24449;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#36817;&#26399;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#22312;&#31354;&#38388;&#25299;&#25169;&#19978;&#25191;&#34892;&#22270;&#21367;&#31215;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22266;&#23450;&#30340;&#22270;&#32467;&#26500;&#21644;&#23616;&#37096;&#24863;&#30693;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#20013;&#38388;&#29305;&#24449;&#36827;&#34892;&#36229;&#20687;&#32032;&#29983;&#25104;&#26469;&#36866;&#24212;&#24615;&#29983;&#25104;&#22343;&#36136;&#21306;&#22495;&#12289;&#33719;&#24471;&#22270;&#32467;&#26500;&#24182;&#36827;&#19968;&#27493;&#29983;&#25104;&#31354;&#38388;&#25551;&#36848;&#31526;&#65292;&#36825;&#20123;&#25551;&#36848;&#31526;&#34987;&#29992;&#20316;&#22270;&#33410;&#28857;&#12290;&#38500;&#20102;&#31354;&#38388;&#23545;&#35937;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#21512;&#29702;&#22320;&#32858;&#21512;&#36890;&#36947;&#26469;&#29983;&#25104;&#20809;&#35889;&#25551;&#36848;&#31526;&#26469;&#25506;&#32034;&#36890;&#36947;&#20043;&#38388;&#30340;&#22270;&#20851;&#31995;&#12290;&#36825;&#20123;&#22270;&#21367;&#31215;&#20013;&#30340;&#37051;&#25509;&#30697;&#38453;&#36890;&#36807;&#32771;&#34385;&#25152;&#26377;&#25551;&#36848;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23454;&#29616;&#20840;&#23616;&#24863;&#30693;&#12290;&#22312;&#23558;&#20809;&#35889;&#21644;&#31354;&#38388;&#20449;&#24687;&#32467;&#21512;&#21040;&#20840;&#23616;&#22270;&#25512;&#29702;&#26694;&#26550;&#20013;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#22522;&#20934;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks have been widely applied to hyperspectral image classification. However, traditional convolutions can not effectively extract features for objects with irregular distributions. Recent methods attempt to address this issue by performing graph convolutions on spatial topologies, but fixed graph structures and local perceptions limit their performances. To tackle these problems, in this paper, different from previous approaches, we perform the superpixel generation on intermediate features during network training to adaptively produce homogeneous regions, obtain graph structures, and further generate spatial descriptors, which are served as graph nodes. Besides spatial objects, we also explore the graph relationships between channels by reasonably aggregating channels to generate spectral descriptors. The adjacent matrices in these graph convolutions are obtained by considering the relationships among all descriptors to realize global perceptions. By combinin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21333;&#24352;&#22270;&#20687;&#30340; 3D CT &#25110; MRI &#25195;&#25551;&#36229;&#20998;&#36776;&#29575;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24182;&#24341;&#20837;&#20013;&#38388;&#25439;&#22833;&#20989;&#25968;&#20197;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#37327;&#21270;&#24230;&#37327;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2001.01330</link><description>&lt;p&gt;
&#24102;&#26377;&#20013;&#38388;&#25439;&#22833;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110; CT &#21644; MRI &#25195;&#25551;&#30340; 3D &#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks with Intermediate Loss for 3D Super-Resolution of CT and MRI Scans. (arXiv:2001.01330v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.01330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21333;&#24352;&#22270;&#20687;&#30340; 3D CT &#25110; MRI &#25195;&#25551;&#36229;&#20998;&#36776;&#29575;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24182;&#24341;&#20837;&#20013;&#38388;&#25439;&#22833;&#20989;&#25968;&#20197;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#37327;&#21270;&#24230;&#37327;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#21307;&#38498;&#24120;&#29992;&#30340; CT &#25195;&#25551;&#20202;&#21487;&#20135;&#29983;&#26368;&#22823; 512 &#20687;&#32032;&#30340;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#22270;&#20687;&#20013;&#30340;&#19968;&#20010;&#20687;&#32032;&#23545;&#24212;&#32452;&#32455;&#30340;&#19968;&#27627;&#31859;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#20998;&#21106;&#32959;&#30244;&#24182;&#21046;&#23450;&#27835;&#30103;&#35745;&#21010;&#65292;&#21307;&#29983;&#38656;&#35201;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340; CT &#25195;&#25551;&#12290;&#21516;&#26679;&#30340;&#38382;&#39064;&#20063;&#20986;&#29616;&#22312; MRI &#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21333;&#24352;&#22270;&#20687;&#30340; 3D CT &#25110; MRI &#25195;&#25551;&#36229;&#20998;&#36776;&#29575;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#21253;&#25324; 10 &#20010;&#21367;&#31215;&#23618;&#21644;&#19968;&#20010;&#20013;&#38388;&#19978;&#37319;&#26679;&#23618;&#65292;&#35813;&#23618;&#25918;&#32622;&#22312;&#21069; 6 &#20010;&#21367;&#31215;&#23618;&#20043;&#21518;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010; CNN &#36890;&#36807;&#20004;&#20010;&#36724;&#65288;&#23485;&#24230;&#21644;&#39640;&#24230;&#65289;&#22686;&#21152;&#20998;&#36776;&#29575;&#65292;&#32039;&#25509;&#30528;&#26159;&#31532;&#20108;&#20010; CNN&#65292;&#23427;&#22686;&#21152;&#20102;&#31532;&#19977;&#20010;&#36724;&#65288;&#28145;&#24230;&#65289;&#30340;&#20998;&#36776;&#29575;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19978;&#37319;&#26679;&#23618;&#21518;&#31435;&#21363;&#35745;&#31639;&#19982;&#39640;&#20998;&#36776;&#29575;&#36755;&#20986;&#30340;&#30495;&#20540;&#30456;&#20851;&#30340;&#25439;&#22833;&#65292;&#38500;&#20102;&#26368;&#21518;&#19968;&#20010;&#21367;&#31215;&#23618;&#21518;&#35745;&#31639;&#30340;&#25439;&#22833;&#12290;&#20013;&#38388;&#25439;&#22833;&#26377;&#21161;&#20110;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#37327;&#21270;&#24230;&#37327;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
CT scanners that are commonly-used in hospitals nowadays produce low-resolution images, up to 512 pixels in size. One pixel in the image corresponds to a one millimeter piece of tissue. In order to accurately segment tumors and make treatment plans, doctors need CT scans of higher resolution. The same problem appears in MRI. In this paper, we propose an approach for the single-image super-resolution of 3D CT or MRI scans. Our method is based on deep convolutional neural networks (CNNs) composed of 10 convolutional layers and an intermediate upscaling layer that is placed after the first 6 convolutional layers. Our first CNN, which increases the resolution on two axes (width and height), is followed by a second CNN, which increases the resolution on the third axis (depth). Different from other methods, we compute the loss with respect to the ground-truth high-resolution output right after the upscaling layer, in addition to computing the loss after the last convolutional layer. The inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#39034;&#24207;&#26816;&#27979;&#22120;&#65292;&#20351;&#29992;&#24102;&#26631;&#35760;&#30340;&#28857;&#36807;&#31243;&#27169;&#22411;&#25429;&#25417;&#24207;&#21015;&#20107;&#20214;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#24207;&#21015;&#65292;&#36890;&#36807;&#35299;&#20915;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#65292;&#38024;&#23545;&#26368;&#22351;&#24773;&#20917;&#30340;&#29983;&#25104;&#22120;&#65292;&#25214;&#21040;&#26368;&#20339;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/1910.09161</link><description>&lt;p&gt;
&#29992;&#20110;&#19968;&#31867;&#20107;&#20214;&#25968;&#25454;&#30340;&#39034;&#24207;&#23545;&#25239;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sequential Adversarial Anomaly Detection for One-Class Event Data. (arXiv:1910.09161v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.09161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#39034;&#24207;&#26816;&#27979;&#22120;&#65292;&#20351;&#29992;&#24102;&#26631;&#35760;&#30340;&#28857;&#36807;&#31243;&#27169;&#22411;&#25429;&#25417;&#24207;&#21015;&#20107;&#20214;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#24207;&#21015;&#65292;&#36890;&#36807;&#35299;&#20915;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#65292;&#38024;&#23545;&#26368;&#22351;&#24773;&#20917;&#30340;&#29983;&#25104;&#22120;&#65292;&#25214;&#21040;&#26368;&#20339;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#21333;&#31867;&#22330;&#26223;&#19979;&#30340;&#39034;&#24207;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#20165;&#22312;&#24322;&#24120;&#24207;&#21015;&#21487;&#29992;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#39034;&#24207;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#35299;&#20915;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#65292;&#38024;&#23545;&#26368;&#22351;&#24773;&#20917;&#30340;&#29983;&#25104;&#22120;&#65292;&#25214;&#21040;&#26368;&#20339;&#26816;&#27979;&#22120;&#12290;&#29983;&#25104;&#22120;&#20351;&#29992;&#24102;&#26631;&#35760;&#30340;&#28857;&#36807;&#31243;&#27169;&#22411;&#25429;&#25417;&#24207;&#21015;&#20107;&#20214;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;&#26816;&#27979;&#22120;&#39034;&#24207;&#35780;&#20272;&#27979;&#35797;&#24207;&#21015;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#23398;&#20064;&#33258;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#30340;&#26102;&#38388;&#21464;&#21270;&#38408;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#19987;&#26377;&#30340;&#22823;&#35268;&#27169;&#20449;&#29992;&#21345;&#27450;&#35784;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25552;&#20986;&#26041;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#24120;&#36866;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the sequential anomaly detection problem in the one-class setting when only the anomalous sequences are available and propose an adversarial sequential detector by solving a minimax problem to find an optimal detector against the worst-case sequences from a generator. The generator captures the dependence in sequential events using the marked point process model. The detector sequentially evaluates the likelihood of a test sequence and compares it with a time-varying threshold, also learned from data through the minimax problem. We demonstrate our proposed method's good performance using numerical experiments on simulations and proprietary large-scale credit card fraud datasets. The proposed method can generally apply to detecting anomalous sequences.
&lt;/p&gt;</description></item></channel></rss>